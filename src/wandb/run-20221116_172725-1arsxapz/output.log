11/16/2022 17:27:26 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:27:26 - INFO - __main__ - Cached results:
11/16/2022 17:27:26 - INFO - __main__ - Task #1: VQAv2 - best score = 67.06
11/16/2022 17:27:26 - INFO - __main__ - Task #2: SNLI-VE - best score = 75.29
11/16/2022 17:27:26 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:27:26 - INFO - __main__ - Training models on Vision-Language continual learning tasks...
11/16/2022 17:27:26 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:27:26 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 17:27:26 - INFO - __main__ - Found checkpoint for task VQAv2!
11/16/2022 17:27:30 - INFO - __main__ - Loaded model checkpoint from task VQAv2! Moving on to next task...
11/16/2022 17:27:31 - INFO - data.image_datasets.cocoimages_dataset - in the MSCOCOImagesDatset
11/16/2022 17:27:32 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/16/2022 17:27:35 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 443757 examples
11/16/2022 17:27:35 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/16/2022 17:27:36 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 214354 examples
11/16/2022 17:27:36 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:27:36 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 17:27:36 - INFO - __main__ - Found checkpoint for task SNLI-VE!
11/16/2022 17:27:40 - INFO - __main__ - Loaded model checkpoint from task SNLI-VE! Moving on to next task...
11/16/2022 17:27:40 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE train dataloader with batch size of 32
11/16/2022 17:27:42 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE train dataset, with 529527 examples
11/16/2022 17:27:42 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE dev dataloader with batch size of 32
11/16/2022 17:27:42 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE dev dataset, with 17858 examples
11/16/2022 17:27:42 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:27:42 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 17:27:42 - INFO - __main__ - Training vilt model on task #1: VQAv2
11/16/2022 17:27:42 - INFO - data.image_datasets.cocoimages_dataset - in the MSCOCOImagesDatset
11/16/2022 17:27:43 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/16/2022 17:27:50 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 43522 examples
11/16/2022 17:27:51 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/16/2022 17:27:54 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 21519 examples
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.cls_token
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.position_embeddings
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.word_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.position_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.vqa.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.norm1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.norm1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.attn.q.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.attn.k.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.attn.v.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.attn.proj.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.attn.proj.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.norm2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.norm2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - task_tokens.0
11/16/2022 17:27:55 - INFO - train.train_vqa - task_tokens.1
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.cls_token
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.position_embeddings
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.word_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.position_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.layernorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.layernorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.pooler.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.vilt_encoder.vilt.pooler.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.vqa.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.task_layer.snli-ve.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.norm1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.norm1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.attn.q.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.attn.k.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.attn.v.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.attn.proj.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.attn.proj.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.norm2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.norm2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.mlp.fc1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.mlp.fc1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.mlp.fc2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.transformer.TAB.mlp.fc2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.task_tokens.0
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.task_tokens.1
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.cls_token
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.position_embeddings
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.word_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.position_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.embeddings.token_type_embeddings.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.layernorm.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.layernorm.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.pooler.dense.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.vilt_encoder.vilt.pooler.dense.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.vqa.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.0.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.0.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.3.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.task_layer.snli-ve.3.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.norm1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.norm1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.attn.q.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.attn.k.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.attn.v.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.attn.proj.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.attn.proj.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.norm2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.norm2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.mlp.fc1.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.mlp.fc1.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.mlp.fc2.weight
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.transformer.TAB.mlp.fc2.bias
11/16/2022 17:27:55 - INFO - train.train_vqa - teacher_model.teacher_model.task_tokens.0
Creating DyTox!
/project/rostamim_919/caiyulia/data/vqav2/v2_mscoco_train2014_annotations.json
/project/rostamim_919/caiyulia/data/vqav2/v2_mscoco_val2014_annotations.json
Training epoch 1:   0% 0/1361 [00:00<?, ?it/s]Training epoch 1:   0% 1/1361 [00:02<1:03:22,  2.80s/it]Training epoch 1:   0% 2/1361 [00:03<34:43,  1.53s/it]  Training epoch 1:   0% 3/1361 [00:04<25:40,  1.13s/it]Training epoch 1:   0% 4/1361 [00:04<20:45,  1.09it/s]Training epoch 1:   0% 5/1361 [00:05<21:38,  1.04it/s]Training epoch 1:   0% 6/1361 [00:06<18:44,  1.21it/s]Training epoch 1:   1% 7/1361 [00:06<17:40,  1.28it/s]Training epoch 1:   1% 8/1361 [00:07<16:20,  1.38it/s]Training epoch 1:   1% 9/1361 [00:08<15:57,  1.41it/s]Training epoch 1:   1% 10/1361 [00:08<15:05,  1.49it/s]Training epoch 1:   1% 11/1361 [00:09<14:49,  1.52it/s]Training epoch 1:   1% 12/1361 [00:10<14:20,  1.57it/s]Training epoch 1:   1% 13/1361 [00:10<14:28,  1.55it/s]Training epoch 1:   1% 14/1361 [00:11<14:13,  1.58it/s]Training epoch 1:   1% 15/1361 [00:11<14:20,  1.56it/s]Training epoch 1:   1% 16/1361 [00:12<14:05,  1.59it/s]Training epoch 1:   1% 17/1361 [00:13<14:04,  1.59it/s]Training epoch 1:   1% 18/1361 [00:13<13:44,  1.63it/s]Training epoch 1:   1% 19/1361 [00:14<13:48,  1.62it/s]Training epoch 1:   1% 20/1361 [00:15<13:52,  1.61it/s]Training epoch 1:   2% 21/1361 [00:15<14:07,  1.58it/s]Training epoch 1:   2% 22/1361 [00:16<13:46,  1.62it/s]Training epoch 1:   2% 23/1361 [00:16<13:49,  1.61it/s]Training epoch 1:   2% 24/1361 [00:17<13:26,  1.66it/s]Training epoch 1:   2% 25/1361 [00:18<13:56,  1.60it/s]Training epoch 1:   2% 26/1361 [00:18<13:33,  1.64it/s]Training epoch 1:   2% 27/1361 [00:19<13:53,  1.60it/s]Training epoch 1:   2% 28/1361 [00:19<13:36,  1.63it/s]Training epoch 1:   2% 29/1361 [00:20<13:41,  1.62it/s]Training epoch 1:   2% 30/1361 [00:21<13:31,  1.64it/s]Training epoch 1:   2% 31/1361 [00:21<13:51,  1.60it/s]Training epoch 1:   2% 32/1361 [00:22<13:50,  1.60it/s]Training epoch 1:   2% 33/1361 [00:23<13:50,  1.60it/s]Training epoch 1:   2% 34/1361 [00:23<13:28,  1.64it/s]Training epoch 1:   3% 35/1361 [00:24<13:25,  1.65it/s]Training epoch 1:   3% 36/1361 [00:24<13:32,  1.63it/s]Training epoch 1:   3% 37/1361 [00:25<13:40,  1.61it/s]