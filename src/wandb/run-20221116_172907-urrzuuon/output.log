11/16/2022 17:29:08 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:29:08 - INFO - __main__ - Training models on Vision-Language continual learning tasks...
11/16/2022 17:29:08 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 17:29:08 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 17:29:08 - INFO - __main__ - Training vilt model on task #1: VQAv2
11/16/2022 17:29:08 - INFO - data.image_datasets.cocoimages_dataset - in the MSCOCOImagesDatset
11/16/2022 17:29:09 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/16/2022 17:29:12 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 443757 examples
11/16/2022 17:29:12 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/16/2022 17:29:13 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 214354 examples
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.cls_token
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.position_embeddings
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.word_embeddings.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.position_embeddings.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.token_type_embeddings.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.token_type_embeddings.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.0.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.0.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.1.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.1.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.3.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.vqa.3.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.norm1.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.norm1.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.attn.q.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.attn.k.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.attn.v.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.attn.proj.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.attn.proj.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.norm2.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.norm2.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.weight
11/16/2022 17:29:16 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.bias
11/16/2022 17:29:16 - INFO - train.train_vqa - task_tokens.0
Creating DyTox!
Training epoch 1:   0% 0/13868 [00:00<?, ?it/s]Training epoch 1:   0% 0/13868 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/project/rostamim_919/caiyulia/Multi-Dytox/src/run.py", line 396, in <module>
    main()
  File "/project/rostamim_919/caiyulia/Multi-Dytox/src/run.py", line 213, in main
    ewc=None)
  File "./train/train_vqa.py", line 272, in train
    for step, batch in enumerate(tqdm(self.vqa_train_dataloader, desc='Training epoch {}'.format(epoch+1))):
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/tqdm/std.py", line 1195, in __iter__
    for obj in iterable:
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
AssertionError: Caught AssertionError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "./data/visionlanguage_datasets/vqa_dataset.py", line 161, in __getitem__
    image = self.images_dataset.get_image_data(image_id)
  File "./data/image_datasets/cocoimages_dataset.py", line 78, in get_image_data
    return self.get_pil_image(image_id)
  File "./data/image_datasets/cocoimages_dataset.py", line 89, in get_pil_image
    assert image_id in self.imageid2filename.keys()
AssertionError

