11/15/2022 12:53:19 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 12:53:19 - INFO - __main__ - Training models on Vision-Language continual learning tasks...
11/15/2022 12:53:19 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 12:53:19 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/15/2022 12:53:19 - INFO - __main__ - Training vilt model on task #1: VQAv2
11/15/2022 12:53:20 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/15/2022 12:53:23 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 443757 examples
11/15/2022 12:53:23 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/15/2022 12:53:24 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 214354 examples
Creating DyTox!
Training epoch 1:   0% 0/13868 [00:00<?, ?it/s]Training epoch 1:   0% 1/13868 [00:02<8:34:10,  2.22s/it]Training epoch 1:   0% 2/13868 [00:02<5:08:55,  1.34s/it]Training epoch 1:   0% 3/13868 [00:03<4:10:18,  1.08s/it]Training epoch 1:   0% 4/13868 [00:04<3:31:54,  1.09it/s]Training epoch 1:   0% 5/13868 [00:05<3:15:25,  1.18it/s]Training epoch 1:   0% 6/13868 [00:05<3:07:13,  1.23it/s]Training epoch 1:   0% 7/13868 [00:06<3:02:53,  1.26it/s]Training epoch 1:   0% 8/13868 [00:07<3:00:59,  1.28it/s]Training epoch 1:   0% 9/13868 [00:08<2:56:31,  1.31it/s]Training epoch 1:   0% 10/13868 [00:08<2:55:15,  1.32it/s]Training epoch 1:   0% 11/13868 [00:09<2:55:44,  1.31it/s]Training epoch 1:   0% 12/13868 [00:10<2:56:04,  1.31it/s]Training epoch 1:   0% 13/13868 [00:11<2:54:21,  1.32it/s]Training epoch 1:   0% 14/13868 [00:11<2:53:48,  1.33it/s]Training epoch 1:   0% 15/13868 [00:12<2:52:39,  1.34it/s]Training epoch 1:   0% 16/13868 [00:13<2:50:31,  1.35it/s]Training epoch 1:   0% 17/13868 [00:14<2:51:41,  1.34it/s]Training epoch 1:   0% 18/13868 [00:14<2:51:34,  1.35it/s]Training epoch 1:   0% 19/13868 [00:15<2:49:09,  1.36it/s]Training epoch 1:   0% 20/13868 [00:16<2:50:22,  1.35it/s]Training epoch 1:   0% 21/13868 [00:17<2:52:22,  1.34it/s]Training epoch 1:   0% 22/13868 [00:17<2:51:48,  1.34it/s]Training epoch 1:   0% 23/13868 [00:18<2:51:57,  1.34it/s]Training epoch 1:   0% 24/13868 [00:19<2:49:18,  1.36it/s]Training epoch 1:   0% 25/13868 [00:19<2:47:50,  1.37it/s]Training epoch 1:   0% 26/13868 [00:20<2:48:02,  1.37it/s]Training epoch 1:   0% 27/13868 [00:21<2:50:20,  1.35it/s]Training epoch 1:   0% 28/13868 [00:22<2:50:25,  1.35it/s]Training epoch 1:   0% 29/13868 [00:22<2:49:42,  1.36it/s]Training epoch 1:   0% 30/13868 [00:23<2:48:32,  1.37it/s]Training epoch 1:   0% 31/13868 [00:24<2:47:29,  1.38it/s]Training epoch 1:   0% 32/13868 [00:25<2:48:08,  1.37it/s]Training epoch 1:   0% 33/13868 [00:25<2:50:08,  1.36it/s]Training epoch 1:   0% 34/13868 [00:26<2:49:22,  1.36it/s]Training epoch 1:   0% 35/13868 [00:27<2:49:43,  1.36it/s]Training epoch 1:   0% 36/13868 [00:27<2:47:04,  1.38it/s]Training epoch 1:   0% 37/13868 [00:28<2:47:14,  1.38it/s]Training epoch 1:   0% 38/13868 [00:29<2:48:09,  1.37it/s]Training epoch 1:   0% 39/13868 [00:30<2:48:10,  1.37it/s]Training epoch 1:   0% 40/13868 [00:30<2:46:47,  1.38it/s]Training epoch 1:   0% 41/13868 [00:31<2:50:58,  1.35it/s]Training epoch 1:   0% 42/13868 [00:32<2:48:35,  1.37it/s]Training epoch 1:   0% 43/13868 [00:33<2:48:02,  1.37it/s]Training epoch 1:   0% 44/13868 [00:33<2:48:37,  1.37it/s]Training epoch 1:   0% 45/13868 [00:34<2:49:45,  1.36it/s]Training epoch 1:   0% 46/13868 [00:35<2:49:22,  1.36it/s]Training epoch 1:   0% 47/13868 [00:36<2:50:58,  1.35it/s]Training epoch 1:   0% 48/13868 [00:36<2:50:04,  1.35it/s]Training epoch 1:   0% 49/13868 [00:37<2:47:32,  1.37it/s]Training epoch 1:   0% 50/13868 [00:38<2:47:21,  1.38it/s]Training epoch 1:   0% 51/13868 [00:38<2:47:57,  1.37it/s]Training epoch 1:   0% 52/13868 [00:39<2:45:53,  1.39it/s]Training epoch 1:   0% 53/13868 [00:40<2:44:30,  1.40it/s]Training epoch 1:   0% 54/13868 [00:41<2:43:27,  1.41it/s]Training epoch 1:   0% 55/13868 [00:41<2:43:45,  1.41it/s]Training epoch 1:   0% 56/13868 [00:42<2:45:42,  1.39it/s]Training epoch 1:   0% 57/13868 [00:43<2:47:22,  1.38it/s]Training epoch 1:   0% 58/13868 [00:43<2:45:27,  1.39it/s]Training epoch 1:   0% 59/13868 [00:44<2:47:25,  1.37it/s]Training epoch 1:   0% 60/13868 [00:45<2:47:33,  1.37it/s]Training epoch 1:   0% 61/13868 [00:46<2:46:42,  1.38it/s]Training epoch 1:   0% 62/13868 [00:46<2:45:32,  1.39it/s]Training epoch 1:   0% 63/13868 [00:47<2:46:31,  1.38it/s]Training epoch 1:   0% 64/13868 [00:48<2:45:41,  1.39it/s]Training epoch 1:   0% 65/13868 [00:49<2:46:46,  1.38it/s]Training epoch 1:   0% 66/13868 [00:49<2:48:28,  1.37it/s]Training epoch 1:   0% 67/13868 [00:50<2:48:36,  1.36it/s]Training epoch 1:   0% 68/13868 [00:51<2:48:28,  1.37it/s]Training epoch 1:   0% 69/13868 [00:52<2:49:37,  1.36it/s]Training epoch 1:   1% 70/13868 [00:52<2:46:35,  1.38it/s]Training epoch 1:   1% 71/13868 [00:53<2:47:26,  1.37it/s]Training epoch 1:   1% 72/13868 [00:54<2:48:07,  1.37it/s]Training epoch 1:   1% 73/13868 [00:54<2:47:43,  1.37it/s]Training epoch 1:   1% 74/13868 [00:55<2:47:50,  1.37it/s]Training epoch 1:   1% 75/13868 [00:56<2:47:05,  1.38it/s]Training epoch 1:   1% 76/13868 [00:57<2:48:23,  1.37it/s]Training epoch 1:   1% 77/13868 [00:57<2:47:34,  1.37it/s]Training epoch 1:   1% 78/13868 [00:58<2:49:29,  1.36it/s]Training epoch 1:   1% 79/13868 [00:59<2:49:43,  1.35it/s]Training epoch 1:   1% 80/13868 [01:00<2:50:35,  1.35it/s]Training epoch 1:   1% 81/13868 [01:00<2:50:45,  1.35it/s]Training epoch 1:   1% 82/13868 [01:01<2:47:59,  1.37it/s]Training epoch 1:   1% 83/13868 [01:02<2:45:28,  1.39it/s]Training epoch 1:   1% 84/13868 [01:02<2:45:40,  1.39it/s]Training epoch 1:   1% 85/13868 [01:03<2:45:55,  1.38it/s]Training epoch 1:   1% 86/13868 [01:04<2:46:06,  1.38it/s]Training epoch 1:   1% 87/13868 [01:05<2:46:11,  1.38it/s]Training epoch 1:   1% 88/13868 [01:05<2:46:24,  1.38it/s]Training epoch 1:   1% 89/13868 [01:06<2:47:26,  1.37it/s]Training epoch 1:   1% 90/13868 [01:07<2:49:27,  1.36it/s]Training epoch 1:   1% 91/13868 [01:08<2:52:27,  1.33it/s]Training epoch 1:   1% 92/13868 [01:08<2:52:15,  1.33it/s]Training epoch 1:   1% 93/13868 [01:09<2:51:27,  1.34it/s]Training epoch 1:   1% 94/13868 [01:10<2:49:03,  1.36it/s]Training epoch 1:   1% 95/13868 [01:11<2:50:51,  1.34it/s]Training epoch 1:   1% 96/13868 [01:11<2:49:46,  1.35it/s]Training epoch 1:   1% 97/13868 [01:12<2:48:28,  1.36it/s]Training epoch 1:   1% 98/13868 [01:13<2:48:46,  1.36it/s]Training epoch 1:   1% 99/13868 [01:14<2:51:51,  1.34it/s]Training epoch 1:   1% 100/13868 [01:15<3:14:30,  1.18it/s]Training epoch 1:   1% 101/13868 [01:15<3:04:15,  1.25it/s]Training epoch 1:   1% 102/13868 [01:16<3:00:52,  1.27it/s]Training epoch 1:   1% 103/13868 [01:17<2:57:58,  1.29it/s]Training epoch 1:   1% 104/13868 [01:18<2:54:16,  1.32it/s]Training epoch 1:   1% 105/13868 [01:18<2:53:38,  1.32it/s]Training epoch 1:   1% 106/13868 [01:19<2:49:34,  1.35it/s]Training epoch 1:   1% 107/13868 [01:20<2:48:31,  1.36it/s]Training epoch 1:   1% 108/13868 [01:20<2:49:00,  1.36it/s]Training epoch 1:   1% 109/13868 [01:21<2:46:04,  1.38it/s]Training epoch 1:   1% 110/13868 [01:22<2:45:50,  1.38it/s]Training epoch 1:   1% 111/13868 [01:23<2:47:30,  1.37it/s]Training epoch 1:   1% 112/13868 [01:23<2:47:31,  1.37it/s]Training epoch 1:   1% 113/13868 [01:24<2:47:30,  1.37it/s]Training epoch 1:   1% 114/13868 [01:25<2:49:09,  1.36it/s]Training epoch 1:   1% 115/13868 [01:26<2:47:52,  1.37it/s]Training epoch 1:   1% 116/13868 [01:26<2:48:41,  1.36it/s]Training epoch 1:   1% 117/13868 [01:27<2:48:14,  1.36it/s]Training epoch 1:   1% 118/13868 [01:28<2:49:14,  1.35it/s]Training epoch 1:   1% 119/13868 [01:29<2:48:38,  1.36it/s]Training epoch 1:   1% 120/13868 [01:29<2:49:55,  1.35it/s]Training epoch 1:   1% 121/13868 [01:30<2:51:34,  1.34it/s]Training epoch 1:   1% 122/13868 [01:31<2:53:23,  1.32it/s]Training epoch 1:   1% 123/13868 [01:32<2:52:13,  1.33it/s]Training epoch 1:   1% 124/13868 [01:32<2:53:08,  1.32it/s]Training epoch 1:   1% 125/13868 [01:33<2:52:18,  1.33it/s]Training epoch 1:   1% 126/13868 [01:34<2:52:21,  1.33it/s]Training epoch 1:   1% 127/13868 [01:35<2:51:34,  1.33it/s]Training epoch 1:   1% 128/13868 [01:35<2:49:17,  1.35it/s]Training epoch 1:   1% 129/13868 [01:36<2:48:36,  1.36it/s]Training epoch 1:   1% 130/13868 [01:37<2:49:27,  1.35it/s]Training epoch 1:   1% 131/13868 [01:37<2:47:25,  1.37it/s]Training epoch 1:   1% 132/13868 [01:38<2:45:23,  1.38it/s]Training epoch 1:   1% 133/13868 [01:39<2:46:36,  1.37it/s]Training epoch 1:   1% 134/13868 [01:40<2:46:28,  1.38it/s]Training epoch 1:   1% 135/13868 [01:40<2:47:04,  1.37it/s]Training epoch 1:   1% 136/13868 [01:41<2:45:30,  1.38it/s]Training epoch 1:   1% 137/13868 [01:42<2:45:11,  1.39it/s]Training epoch 1:   1% 138/13868 [01:43<2:46:34,  1.37it/s]Training epoch 1:   1% 139/13868 [01:43<2:46:23,  1.38it/s]Training epoch 1:   1% 140/13868 [01:44<2:45:50,  1.38it/s]Training epoch 1:   1% 141/13868 [01:45<2:44:44,  1.39it/s]Training epoch 1:   1% 142/13868 [01:45<2:43:47,  1.40it/s]Training epoch 1:   1% 143/13868 [01:46<2:46:08,  1.38it/s]Training epoch 1:   1% 144/13868 [01:47<2:45:08,  1.39it/s]Training epoch 1:   1% 145/13868 [01:48<2:46:08,  1.38it/s]Training epoch 1:   1% 146/13868 [01:48<2:46:46,  1.37it/s]Training epoch 1:   1% 147/13868 [01:49<2:47:58,  1.36it/s]Training epoch 1:   1% 148/13868 [01:50<2:46:12,  1.38it/s]Training epoch 1:   1% 149/13868 [01:51<2:46:51,  1.37it/s]Training epoch 1:   1% 150/13868 [01:51<2:47:34,  1.36it/s]Training epoch 1:   1% 151/13868 [01:52<2:48:24,  1.36it/s]Training epoch 1:   1% 152/13868 [01:53<2:48:20,  1.36it/s]Training epoch 1:   1% 153/13868 [01:54<2:48:54,  1.35it/s]Training epoch 1:   1% 154/13868 [01:54<2:46:07,  1.38it/s]Training epoch 1:   1% 155/13868 [01:55<2:46:18,  1.37it/s]Training epoch 1:   1% 156/13868 [01:56<2:45:03,  1.38it/s]Training epoch 1:   1% 157/13868 [01:56<2:43:09,  1.40it/s]Training epoch 1:   1% 158/13868 [01:57<2:45:56,  1.38it/s]Training epoch 1:   1% 159/13868 [01:58<2:45:56,  1.38it/s]Training epoch 1:   1% 160/13868 [01:59<2:44:49,  1.39it/s]Training epoch 1:   1% 161/13868 [01:59<2:44:36,  1.39it/s]Training epoch 1:   1% 162/13868 [02:00<2:44:16,  1.39it/s]Training epoch 1:   1% 163/13868 [02:01<2:45:42,  1.38it/s]Training epoch 1:   1% 164/13868 [02:01<2:46:25,  1.37it/s]Training epoch 1:   1% 165/13868 [02:02<2:46:55,  1.37it/s]Training epoch 1:   1% 166/13868 [02:03<2:46:22,  1.37it/s]Training epoch 1:   1% 167/13868 [02:04<2:43:29,  1.40it/s]Training epoch 1:   1% 168/13868 [02:04<2:45:09,  1.38it/s]Training epoch 1:   1% 169/13868 [02:05<2:45:56,  1.38it/s]Training epoch 1:   1% 170/13868 [02:06<2:46:28,  1.37it/s]Training epoch 1:   1% 171/13868 [02:07<2:47:20,  1.36it/s]Training epoch 1:   1% 172/13868 [02:07<2:46:03,  1.37it/s]Training epoch 1:   1% 173/13868 [02:08<2:45:20,  1.38it/s]Training epoch 1:   1% 174/13868 [02:09<2:45:32,  1.38it/s]Training epoch 1:   1% 175/13868 [02:09<2:42:58,  1.40it/s]Training epoch 1:   1% 176/13868 [02:10<2:43:32,  1.40it/s]Training epoch 1:   1% 177/13868 [02:11<2:44:58,  1.38it/s]Training epoch 1:   1% 178/13868 [02:12<2:46:26,  1.37it/s]Training epoch 1:   1% 179/13868 [02:12<2:47:09,  1.36it/s]Training epoch 1:   1% 180/13868 [02:13<2:47:56,  1.36it/s]Training epoch 1:   1% 181/13868 [02:14<2:47:45,  1.36it/s]Training epoch 1:   1% 182/13868 [02:15<2:47:37,  1.36it/s]Training epoch 1:   1% 183/13868 [02:15<2:45:21,  1.38it/s]Training epoch 1:   1% 184/13868 [02:16<2:47:02,  1.37it/s]Training epoch 1:   1% 185/13868 [02:17<2:45:36,  1.38it/s]Training epoch 1:   1% 186/13868 [02:17<2:47:40,  1.36it/s]Training epoch 1:   1% 187/13868 [02:18<2:47:24,  1.36it/s]Training epoch 1:   1% 188/13868 [02:19<2:47:14,  1.36it/s]Training epoch 1:   1% 189/13868 [02:20<2:45:29,  1.38it/s]Training epoch 1:   1% 190/13868 [02:20<2:46:18,  1.37it/s]Training epoch 1:   1% 191/13868 [02:21<2:47:23,  1.36it/s]Training epoch 1:   1% 192/13868 [02:22<2:49:41,  1.34it/s]Training epoch 1:   1% 193/13868 [02:23<2:46:25,  1.37it/s]Training epoch 1:   1% 194/13868 [02:23<2:43:36,  1.39it/s]Training epoch 1:   1% 195/13868 [02:24<2:41:56,  1.41it/s]Training epoch 1:   1% 196/13868 [02:25<2:42:10,  1.41it/s]Training epoch 1:   1% 197/13868 [02:25<2:45:07,  1.38it/s]Training epoch 1:   1% 198/13868 [02:26<2:46:57,  1.36it/s]Training epoch 1:   1% 199/13868 [02:27<2:44:21,  1.39it/s]Training epoch 1:   1% 200/13868 [02:28<2:55:47,  1.30it/s]Training epoch 1:   1% 201/13868 [02:29<2:53:55,  1.31it/s]Training epoch 1:   1% 202/13868 [02:29<2:51:02,  1.33it/s]Training epoch 1:   1% 203/13868 [02:30<2:48:09,  1.35it/s]Training epoch 1:   1% 204/13868 [02:31<2:48:46,  1.35it/s]Training epoch 1:   1% 205/13868 [02:31<2:44:11,  1.39it/s]Training epoch 1:   1% 206/13868 [02:32<2:42:13,  1.40it/s]Training epoch 1:   1% 207/13868 [02:33<2:42:34,  1.40it/s]Training epoch 1:   1% 208/13868 [02:34<2:44:46,  1.38it/s]Training epoch 1:   2% 209/13868 [02:34<2:43:09,  1.40it/s]Training epoch 1:   2% 210/13868 [02:35<2:43:46,  1.39it/s]Training epoch 1:   2% 211/13868 [02:36<2:44:03,  1.39it/s]Training epoch 1:   2% 212/13868 [02:36<2:46:57,  1.36it/s]Training epoch 1:   2% 213/13868 [02:37<2:45:59,  1.37it/s]Training epoch 1:   2% 214/13868 [02:38<2:46:07,  1.37it/s]Training epoch 1:   2% 215/13868 [02:39<2:46:10,  1.37it/s]Training epoch 1:   2% 216/13868 [02:39<2:46:50,  1.36it/s]Training epoch 1:   2% 217/13868 [02:40<2:44:25,  1.38it/s]Training epoch 1:   2% 218/13868 [02:41<2:45:53,  1.37it/s]Training epoch 1:   2% 219/13868 [02:42<2:46:40,  1.36it/s]Training epoch 1:   2% 220/13868 [02:42<2:47:02,  1.36it/s]Training epoch 1:   2% 221/13868 [02:43<2:45:28,  1.37it/s]Training epoch 1:   2% 222/13868 [02:44<2:47:41,  1.36it/s]Training epoch 1:   2% 223/13868 [02:45<2:49:56,  1.34it/s]Training epoch 1:   2% 224/13868 [02:45<2:49:21,  1.34it/s]Training epoch 1:   2% 225/13868 [02:46<2:46:13,  1.37it/s]Training epoch 1:   2% 226/13868 [02:47<2:46:00,  1.37it/s]Training epoch 1:   2% 227/13868 [02:47<2:47:30,  1.36it/s]Training epoch 1:   2% 228/13868 [02:48<2:48:19,  1.35it/s]Training epoch 1:   2% 229/13868 [02:49<2:45:36,  1.37it/s]Training epoch 1:   2% 230/13868 [02:50<2:45:51,  1.37it/s]Training epoch 1:   2% 231/13868 [02:50<2:44:13,  1.38it/s]Training epoch 1:   2% 232/13868 [02:51<2:45:04,  1.38it/s]Training epoch 1:   2% 233/13868 [02:52<2:44:10,  1.38it/s]Training epoch 1:   2% 234/13868 [02:53<2:47:52,  1.35it/s]Training epoch 1:   2% 235/13868 [02:53<2:48:21,  1.35it/s]Training epoch 1:   2% 236/13868 [02:54<2:46:09,  1.37it/s]Training epoch 1:   2% 237/13868 [02:55<2:47:09,  1.36it/s]Training epoch 1:   2% 238/13868 [02:55<2:47:43,  1.35it/s]Training epoch 1:   2% 239/13868 [02:56<2:47:43,  1.35it/s]Training epoch 1:   2% 240/13868 [02:57<2:47:31,  1.36it/s]Training epoch 1:   2% 241/13868 [02:58<2:45:44,  1.37it/s]Training epoch 1:   2% 242/13868 [02:58<2:46:14,  1.37it/s]Training epoch 1:   2% 243/13868 [02:59<2:45:11,  1.37it/s]Training epoch 1:   2% 244/13868 [03:00<2:48:01,  1.35it/s]Training epoch 1:   2% 245/13868 [03:01<2:47:28,  1.36it/s]Training epoch 1:   2% 246/13868 [03:01<2:49:11,  1.34it/s]Training epoch 1:   2% 247/13868 [03:02<2:46:51,  1.36it/s]Training epoch 1:   2% 248/13868 [03:03<2:46:59,  1.36it/s]Training epoch 1:   2% 249/13868 [03:04<2:45:38,  1.37it/s]Training epoch 1:   2% 250/13868 [03:04<2:46:31,  1.36it/s]Training epoch 1:   2% 251/13868 [03:05<2:44:12,  1.38it/s]Training epoch 1:   2% 252/13868 [03:06<2:47:11,  1.36it/s]Training epoch 1:   2% 253/13868 [03:06<2:45:04,  1.37it/s]Training epoch 1:   2% 254/13868 [03:07<2:45:33,  1.37it/s]Training epoch 1:   2% 255/13868 [03:08<2:46:05,  1.37it/s]Training epoch 1:   2% 256/13868 [03:09<2:47:38,  1.35it/s]Training epoch 1:   2% 257/13868 [03:09<2:45:35,  1.37it/s]Training epoch 1:   2% 258/13868 [03:10<2:45:40,  1.37it/s]Training epoch 1:   2% 259/13868 [03:11<2:45:47,  1.37it/s]Training epoch 1:   2% 260/13868 [03:12<2:45:33,  1.37it/s]Training epoch 1:   2% 261/13868 [03:12<2:42:54,  1.39it/s]Training epoch 1:   2% 262/13868 [03:13<2:43:09,  1.39it/s]Training epoch 1:   2% 263/13868 [03:14<2:43:13,  1.39it/s]Training epoch 1:   2% 264/13868 [03:14<2:43:59,  1.38it/s]Training epoch 1:   2% 265/13868 [03:15<2:43:51,  1.38it/s]Training epoch 1:   2% 266/13868 [03:16<2:42:41,  1.39it/s]Training epoch 1:   2% 267/13868 [03:17<2:44:56,  1.37it/s]Training epoch 1:   2% 268/13868 [03:17<2:50:12,  1.33it/s]Training epoch 1:   2% 269/13868 [03:18<2:49:01,  1.34it/s]Training epoch 1:   2% 270/13868 [03:19<2:49:33,  1.34it/s]Training epoch 1:   2% 271/13868 [03:20<2:47:03,  1.36it/s]Training epoch 1:   2% 272/13868 [03:20<2:47:34,  1.35it/s]Training epoch 1:   2% 273/13868 [03:21<2:43:53,  1.38it/s]Training epoch 1:   2% 274/13868 [03:22<2:43:47,  1.38it/s]Training epoch 1:   2% 275/13868 [03:23<2:43:51,  1.38it/s]Training epoch 1:   2% 276/13868 [03:23<2:45:05,  1.37it/s]Training epoch 1:   2% 277/13868 [03:24<2:44:54,  1.37it/s]Training epoch 1:   2% 278/13868 [03:25<2:44:47,  1.37it/s]Training epoch 1:   2% 279/13868 [03:25<2:44:40,  1.38it/s]Training epoch 1:   2% 280/13868 [03:26<2:46:28,  1.36it/s]Training epoch 1:   2% 281/13868 [03:27<2:45:54,  1.36it/s]Training epoch 1:   2% 282/13868 [03:28<2:45:57,  1.36it/s]Training epoch 1:   2% 283/13868 [03:28<2:47:10,  1.35it/s]Training epoch 1:   2% 284/13868 [03:29<2:46:58,  1.36it/s]Training epoch 1:   2% 285/13868 [03:30<2:48:43,  1.34it/s]Training epoch 1:   2% 286/13868 [03:31<2:47:13,  1.35it/s]Training epoch 1:   2% 287/13868 [03:31<2:47:33,  1.35it/s]Training epoch 1:   2% 288/13868 [03:32<2:47:15,  1.35it/s]Training epoch 1:   2% 289/13868 [03:33<2:46:21,  1.36it/s]Training epoch 1:   2% 290/13868 [03:34<2:46:37,  1.36it/s]Training epoch 1:   2% 291/13868 [03:34<2:46:16,  1.36it/s]Training epoch 1:   2% 292/13868 [03:35<2:44:24,  1.38it/s]Training epoch 1:   2% 293/13868 [03:36<2:44:26,  1.38it/s]Training epoch 1:   2% 294/13868 [03:36<2:45:08,  1.37it/s]Training epoch 1:   2% 295/13868 [03:37<2:46:12,  1.36it/s]Training epoch 1:   2% 296/13868 [03:38<2:45:36,  1.37it/s]Training epoch 1:   2% 297/13868 [03:39<2:45:41,  1.37it/s]Training epoch 1:   2% 298/13868 [03:39<2:46:44,  1.36it/s]Training epoch 1:   2% 299/13868 [03:40<2:47:25,  1.35it/s]Training epoch 1:   2% 300/13868 [03:41<2:57:18,  1.28it/s]Training epoch 1:   2% 301/13868 [03:42<2:52:34,  1.31it/s]Training epoch 1:   2% 302/13868 [03:43<2:51:47,  1.32it/s]Training epoch 1:   2% 303/13868 [03:43<2:49:33,  1.33it/s]Training epoch 1:   2% 304/13868 [03:44<2:48:55,  1.34it/s]Training epoch 1:   2% 305/13868 [03:45<2:44:36,  1.37it/s]Training epoch 1:   2% 306/13868 [03:45<2:45:14,  1.37it/s]Training epoch 1:   2% 307/13868 [03:46<2:47:56,  1.35it/s]Training epoch 1:   2% 308/13868 [03:47<2:48:21,  1.34it/s]Training epoch 1:   2% 309/13868 [03:48<2:45:29,  1.37it/s]Training epoch 1:   2% 310/13868 [03:48<2:47:44,  1.35it/s]Training epoch 1:   2% 311/13868 [03:49<2:46:55,  1.35it/s]Training epoch 1:   2% 312/13868 [03:50<2:47:12,  1.35it/s]Training epoch 1:   2% 313/13868 [03:51<2:46:04,  1.36it/s]Training epoch 1:   2% 314/13868 [03:51<2:43:30,  1.38it/s]Training epoch 1:   2% 315/13868 [03:52<2:42:30,  1.39it/s]Training epoch 1:   2% 316/13868 [03:53<2:43:27,  1.38it/s]Training epoch 1:   2% 317/13868 [03:53<2:42:50,  1.39it/s]Training epoch 1:   2% 318/13868 [03:54<2:44:50,  1.37it/s]Training epoch 1:   2% 319/13868 [03:55<2:42:46,  1.39it/s]Training epoch 1:   2% 320/13868 [03:56<2:44:36,  1.37it/s]Training epoch 1:   2% 321/13868 [03:56<2:43:38,  1.38it/s]Training epoch 1:   2% 322/13868 [03:57<2:45:31,  1.36it/s]Training epoch 1:   2% 323/13868 [03:58<2:44:30,  1.37it/s]Training epoch 1:   2% 324/13868 [03:59<2:44:31,  1.37it/s]Training epoch 1:   2% 325/13868 [03:59<2:43:00,  1.38it/s]Training epoch 1:   2% 326/13868 [04:00<2:42:59,  1.38it/s]Training epoch 1:   2% 327/13868 [04:01<2:42:09,  1.39it/s]Training epoch 1:   2% 328/13868 [04:01<2:45:11,  1.37it/s]Training epoch 1:   2% 329/13868 [04:02<2:43:59,  1.38it/s]Training epoch 1:   2% 330/13868 [04:03<2:43:09,  1.38it/s]Training epoch 1:   2% 331/13868 [04:04<2:42:29,  1.39it/s]Training epoch 1:   2% 332/13868 [04:04<2:42:27,  1.39it/s]Training epoch 1:   2% 333/13868 [04:05<2:43:12,  1.38it/s]Training epoch 1:   2% 334/13868 [04:06<2:42:12,  1.39it/s]Training epoch 1:   2% 335/13868 [04:07<2:42:23,  1.39it/s]Training epoch 1:   2% 336/13868 [04:07<2:46:18,  1.36it/s]Training epoch 1:   2% 337/13868 [04:08<2:46:51,  1.35it/s]Training epoch 1:   2% 338/13868 [04:09<2:46:11,  1.36it/s]Training epoch 1:   2% 339/13868 [04:10<2:47:01,  1.35it/s]Training epoch 1:   2% 340/13868 [04:10<2:47:04,  1.35it/s]Training epoch 1:   2% 341/13868 [04:11<2:47:22,  1.35it/s]Training epoch 1:   2% 342/13868 [04:12<2:45:20,  1.36it/s]Training epoch 1:   2% 343/13868 [04:13<2:49:42,  1.33it/s]Training epoch 1:   2% 344/13868 [04:13<2:47:49,  1.34it/s]Training epoch 1:   2% 345/13868 [04:14<2:47:16,  1.35it/s]Training epoch 1:   2% 346/13868 [04:15<2:48:23,  1.34it/s]Training epoch 1:   3% 347/13868 [04:15<2:47:24,  1.35it/s]Training epoch 1:   3% 348/13868 [04:16<2:46:33,  1.35it/s]Training epoch 1:   3% 349/13868 [04:17<2:47:10,  1.35it/s]Training epoch 1:   3% 350/13868 [04:18<2:45:32,  1.36it/s]Training epoch 1:   3% 351/13868 [04:18<2:45:03,  1.36it/s]Training epoch 1:   3% 352/13868 [04:19<2:45:39,  1.36it/s]Training epoch 1:   3% 353/13868 [04:20<2:48:41,  1.34it/s]Training epoch 1:   3% 354/13868 [04:21<2:48:42,  1.34it/s]Training epoch 1:   3% 355/13868 [04:21<2:47:29,  1.34it/s]Training epoch 1:   3% 356/13868 [04:22<2:48:02,  1.34it/s]Training epoch 1:   3% 357/13868 [04:23<2:47:55,  1.34it/s]Training epoch 1:   3% 358/13868 [04:24<2:44:56,  1.37it/s]Training epoch 1:   3% 359/13868 [04:24<2:43:41,  1.38it/s]Training epoch 1:   3% 360/13868 [04:25<2:43:40,  1.38it/s]Training epoch 1:   3% 361/13868 [04:26<2:43:14,  1.38it/s]Training epoch 1:   3% 362/13868 [04:26<2:42:16,  1.39it/s]Training epoch 1:   3% 363/13868 [04:27<2:38:59,  1.42it/s]Training epoch 1:   3% 364/13868 [04:28<2:40:23,  1.40it/s]Training epoch 1:   3% 365/13868 [04:29<2:42:03,  1.39it/s]Training epoch 1:   3% 366/13868 [04:29<2:41:50,  1.39it/s]Training epoch 1:   3% 367/13868 [04:30<2:43:21,  1.38it/s]Training epoch 1:   3% 368/13868 [04:31<2:45:06,  1.36it/s]Training epoch 1:   3% 369/13868 [04:32<2:46:38,  1.35it/s]Training epoch 1:   3% 370/13868 [04:32<2:47:01,  1.35it/s]Training epoch 1:   3% 371/13868 [04:33<2:45:10,  1.36it/s]Training epoch 1:   3% 372/13868 [04:34<2:46:19,  1.35it/s]Training epoch 1:   3% 373/13868 [04:35<2:45:31,  1.36it/s]Training epoch 1:   3% 374/13868 [04:35<2:46:17,  1.35it/s]Training epoch 1:   3% 375/13868 [04:36<2:45:59,  1.35it/s]Training epoch 1:   3% 376/13868 [04:37<2:48:37,  1.33it/s]Training epoch 1:   3% 377/13868 [04:38<2:47:31,  1.34it/s]Training epoch 1:   3% 378/13868 [04:38<2:45:09,  1.36it/s]Training epoch 1:   3% 379/13868 [04:39<2:44:58,  1.36it/s]Training epoch 1:   3% 380/13868 [04:40<2:44:06,  1.37it/s]Training epoch 1:   3% 381/13868 [04:40<2:42:51,  1.38it/s]Training epoch 1:   3% 382/13868 [04:41<2:42:50,  1.38it/s]Training epoch 1:   3% 383/13868 [04:42<2:41:53,  1.39it/s]Training epoch 1:   3% 384/13868 [04:43<2:41:15,  1.39it/s]Training epoch 1:   3% 385/13868 [04:43<2:44:51,  1.36it/s]Training epoch 1:   3% 386/13868 [04:44<2:40:48,  1.40it/s]Training epoch 1:   3% 387/13868 [04:45<2:42:15,  1.38it/s]Training epoch 1:   3% 388/13868 [04:45<2:42:05,  1.39it/s]Training epoch 1:   3% 389/13868 [04:46<2:42:15,  1.38it/s]Training epoch 1:   3% 390/13868 [04:47<2:40:18,  1.40it/s]Training epoch 1:   3% 391/13868 [04:48<2:42:51,  1.38it/s]Training epoch 1:   3% 392/13868 [04:48<2:41:58,  1.39it/s]Training epoch 1:   3% 393/13868 [04:49<2:44:30,  1.37it/s]Training epoch 1:   3% 394/13868 [04:50<2:44:25,  1.37it/s]Training epoch 1:   3% 395/13868 [04:51<2:44:30,  1.37it/s]Training epoch 1:   3% 396/13868 [04:51<2:41:23,  1.39it/s]Training epoch 1:   3% 397/13868 [04:52<2:44:59,  1.36it/s]Training epoch 1:   3% 398/13868 [04:53<2:41:41,  1.39it/s]Training epoch 1:   3% 399/13868 [04:53<2:43:26,  1.37it/s]Training epoch 1:   3% 400/13868 [04:54<2:54:30,  1.29it/s]Training epoch 1:   3% 401/13868 [04:55<2:51:33,  1.31it/s]Training epoch 1:   3% 402/13868 [04:56<2:49:09,  1.33it/s]Training epoch 1:   3% 403/13868 [04:57<2:48:29,  1.33it/s]Training epoch 1:   3% 404/13868 [04:57<2:46:36,  1.35it/s]Training epoch 1:   3% 405/13868 [04:58<2:48:16,  1.33it/s]Training epoch 1:   3% 406/13868 [04:59<2:49:05,  1.33it/s]Training epoch 1:   3% 407/13868 [05:00<2:48:59,  1.33it/s]Training epoch 1:   3% 408/13868 [05:00<2:47:41,  1.34it/s]Training epoch 1:   3% 409/13868 [05:01<2:47:16,  1.34it/s]Training epoch 1:   3% 410/13868 [05:02<2:46:48,  1.34it/s]Training epoch 1:   3% 411/13868 [05:03<2:48:11,  1.33it/s]Training epoch 1:   3% 412/13868 [05:03<2:45:21,  1.36it/s]Training epoch 1:   3% 413/13868 [05:04<2:42:41,  1.38it/s]Training epoch 1:   3% 414/13868 [05:05<2:45:12,  1.36it/s]Training epoch 1:   3% 415/13868 [05:05<2:45:49,  1.35it/s]Training epoch 1:   3% 416/13868 [05:06<2:41:33,  1.39it/s]Training epoch 1:   3% 417/13868 [05:07<2:41:56,  1.38it/s]Training epoch 1:   3% 418/13868 [05:08<2:42:08,  1.38it/s]Training epoch 1:   3% 419/13868 [05:08<2:40:45,  1.39it/s]Training epoch 1:   3% 420/13868 [05:09<2:40:41,  1.39it/s]Training epoch 1:   3% 421/13868 [05:10<2:42:05,  1.38it/s]Training epoch 1:   3% 422/13868 [05:10<2:43:31,  1.37it/s]Training epoch 1:   3% 423/13868 [05:11<2:44:18,  1.36it/s]Training epoch 1:   3% 424/13868 [05:12<2:44:50,  1.36it/s]Training epoch 1:   3% 425/13868 [05:13<2:46:31,  1.35it/s]Training epoch 1:   3% 426/13868 [05:13<2:44:20,  1.36it/s]Training epoch 1:   3% 427/13868 [05:14<2:46:03,  1.35it/s]Training epoch 1:   3% 428/13868 [05:15<2:45:58,  1.35it/s]Training epoch 1:   3% 429/13868 [05:16<2:43:46,  1.37it/s]Training epoch 1:   3% 430/13868 [05:16<2:42:53,  1.37it/s]Training epoch 1:   3% 431/13868 [05:17<2:44:06,  1.36it/s]Training epoch 1:   3% 432/13868 [05:18<2:45:09,  1.36it/s]Training epoch 1:   3% 433/13868 [05:19<2:44:23,  1.36it/s]Training epoch 1:   3% 434/13868 [05:19<2:45:17,  1.35it/s]Training epoch 1:   3% 435/13868 [05:20<2:44:10,  1.36it/s]Training epoch 1:   3% 436/13868 [05:21<2:45:00,  1.36it/s]Training epoch 1:   3% 437/13868 [05:21<2:42:43,  1.38it/s]Training epoch 1:   3% 438/13868 [05:22<2:41:33,  1.39it/s]Training epoch 1:   3% 439/13868 [05:23<2:44:10,  1.36it/s]Training epoch 1:   3% 440/13868 [05:24<2:41:24,  1.39it/s]Training epoch 1:   3% 441/13868 [05:24<2:44:20,  1.36it/s]Training epoch 1:   3% 442/13868 [05:25<2:44:09,  1.36it/s]Training epoch 1:   3% 443/13868 [05:26<2:47:54,  1.33it/s]Training epoch 1:   3% 444/13868 [05:27<2:46:42,  1.34it/s]Training epoch 1:   3% 445/13868 [05:27<2:46:05,  1.35it/s]Training epoch 1:   3% 446/13868 [05:28<2:43:36,  1.37it/s]Training epoch 1:   3% 447/13868 [05:29<2:43:13,  1.37it/s]Training epoch 1:   3% 448/13868 [05:30<2:43:43,  1.37it/s]Training epoch 1:   3% 449/13868 [05:30<2:43:57,  1.36it/s]Training epoch 1:   3% 450/13868 [05:31<2:42:31,  1.38it/s]Training epoch 1:   3% 451/13868 [05:32<2:42:53,  1.37it/s]Training epoch 1:   3% 452/13868 [05:32<2:41:56,  1.38it/s]Training epoch 1:   3% 453/13868 [05:33<2:43:04,  1.37it/s]Training epoch 1:   3% 454/13868 [05:34<2:41:46,  1.38it/s]Training epoch 1:   3% 455/13868 [05:35<2:42:41,  1.37it/s]Training epoch 1:   3% 456/13868 [05:35<2:42:16,  1.38it/s]Training epoch 1:   3% 457/13868 [05:36<2:43:30,  1.37it/s]Training epoch 1:   3% 458/13868 [05:37<2:41:15,  1.39it/s]Training epoch 1:   3% 459/13868 [05:38<2:41:37,  1.38it/s]Training epoch 1:   3% 460/13868 [05:38<2:41:02,  1.39it/s]Training epoch 1:   3% 461/13868 [05:39<2:43:04,  1.37it/s]Training epoch 1:   3% 462/13868 [05:40<2:43:01,  1.37it/s]Training epoch 1:   3% 463/13868 [05:40<2:44:46,  1.36it/s]Training epoch 1:   3% 464/13868 [05:41<2:41:09,  1.39it/s]Training epoch 1:   3% 465/13868 [05:42<2:42:27,  1.38it/s]Training epoch 1:   3% 466/13868 [05:43<2:44:17,  1.36it/s]Training epoch 1:   3% 467/13868 [05:43<2:43:06,  1.37it/s]Training epoch 1:   3% 468/13868 [05:44<2:44:25,  1.36it/s]Training epoch 1:   3% 469/13868 [05:45<2:45:16,  1.35it/s]Training epoch 1:   3% 470/13868 [05:46<2:42:50,  1.37it/s]Training epoch 1:   3% 471/13868 [05:46<2:41:40,  1.38it/s]Training epoch 1:   3% 472/13868 [05:47<2:41:37,  1.38it/s]Training epoch 1:   3% 473/13868 [05:48<2:40:39,  1.39it/s]Training epoch 1:   3% 474/13868 [05:48<2:38:55,  1.40it/s]Training epoch 1:   3% 475/13868 [05:49<2:40:27,  1.39it/s]Training epoch 1:   3% 476/13868 [05:50<2:41:18,  1.38it/s]Training epoch 1:   3% 477/13868 [05:51<2:42:54,  1.37it/s]Training epoch 1:   3% 478/13868 [05:51<2:41:35,  1.38it/s]Training epoch 1:   3% 479/13868 [05:52<2:43:00,  1.37it/s]Training epoch 1:   3% 480/13868 [05:53<2:42:32,  1.37it/s]Training epoch 1:   3% 481/13868 [05:54<2:44:27,  1.36it/s]Training epoch 1:   3% 482/13868 [05:54<2:43:28,  1.36it/s]Training epoch 1:   3% 483/13868 [05:55<2:44:05,  1.36it/s]Training epoch 1:   3% 484/13868 [05:56<2:44:57,  1.35it/s]Training epoch 1:   3% 485/13868 [05:57<2:44:01,  1.36it/s]Training epoch 1:   4% 486/13868 [05:57<2:41:54,  1.38it/s]Training epoch 1:   4% 487/13868 [05:58<2:44:04,  1.36it/s]Training epoch 1:   4% 488/13868 [05:59<2:44:07,  1.36it/s]Training epoch 1:   4% 489/13868 [05:59<2:44:05,  1.36it/s]Training epoch 1:   4% 490/13868 [06:00<2:45:05,  1.35it/s]Training epoch 1:   4% 491/13868 [06:01<2:44:42,  1.35it/s]Training epoch 1:   4% 492/13868 [06:02<2:44:02,  1.36it/s]Training epoch 1:   4% 493/13868 [06:02<2:44:41,  1.35it/s]Training epoch 1:   4% 494/13868 [06:03<2:42:48,  1.37it/s]Training epoch 1:   4% 495/13868 [06:04<2:43:35,  1.36it/s]Training epoch 1:   4% 496/13868 [06:05<2:43:42,  1.36it/s]Training epoch 1:   4% 497/13868 [06:05<2:43:50,  1.36it/s]Training epoch 1:   4% 498/13868 [06:06<2:41:45,  1.38it/s]Training epoch 1:   4% 499/13868 [06:07<2:41:56,  1.38it/s]Training epoch 1:   4% 500/13868 [06:08<2:52:45,  1.29it/s]Training epoch 1:   4% 501/13868 [06:08<2:48:48,  1.32it/s]Training epoch 1:   4% 502/13868 [06:09<2:47:54,  1.33it/s]Training epoch 1:   4% 503/13868 [06:10<2:46:51,  1.33it/s]Training epoch 1:   4% 504/13868 [06:11<2:44:57,  1.35it/s]Training epoch 1:   4% 505/13868 [06:11<2:46:04,  1.34it/s]Training epoch 1:   4% 506/13868 [06:12<2:46:16,  1.34it/s]Training epoch 1:   4% 507/13868 [06:13<2:47:09,  1.33it/s]Training epoch 1:   4% 508/13868 [06:14<2:46:25,  1.34it/s]Training epoch 1:   4% 509/13868 [06:14<2:43:56,  1.36it/s]Training epoch 1:   4% 510/13868 [06:15<2:42:58,  1.37it/s]Training epoch 1:   4% 511/13868 [06:16<2:44:12,  1.36it/s]Training epoch 1:   4% 512/13868 [06:16<2:43:08,  1.36it/s]Training epoch 1:   4% 513/13868 [06:17<2:42:39,  1.37it/s]Training epoch 1:   4% 514/13868 [06:18<2:42:00,  1.37it/s]Training epoch 1:   4% 515/13868 [06:19<2:43:53,  1.36it/s]Training epoch 1:   4% 516/13868 [06:19<2:45:32,  1.34it/s]Training epoch 1:   4% 517/13868 [06:20<2:41:40,  1.38it/s]Training epoch 1:   4% 518/13868 [06:21<2:39:38,  1.39it/s]Training epoch 1:   4% 519/13868 [06:22<2:38:02,  1.41it/s]Training epoch 1:   4% 520/13868 [06:22<2:40:46,  1.38it/s]Training epoch 1:   4% 521/13868 [06:23<2:42:16,  1.37it/s]Training epoch 1:   4% 522/13868 [06:24<2:42:50,  1.37it/s]Training epoch 1:   4% 523/13868 [06:24<2:42:36,  1.37it/s]Training epoch 1:   4% 524/13868 [06:25<2:41:57,  1.37it/s]Training epoch 1:   4% 525/13868 [06:26<2:41:03,  1.38it/s]Training epoch 1:   4% 526/13868 [06:27<2:39:59,  1.39it/s]Training epoch 1:   4% 527/13868 [06:27<2:39:48,  1.39it/s]Training epoch 1:   4% 528/13868 [06:28<2:38:37,  1.40it/s]Training epoch 1:   4% 529/13868 [06:29<2:39:16,  1.40it/s]Training epoch 1:   4% 530/13868 [06:30<2:39:21,  1.40it/s]Training epoch 1:   4% 531/13868 [06:30<2:41:35,  1.38it/s]Training epoch 1:   4% 532/13868 [06:31<2:41:06,  1.38it/s]Training epoch 1:   4% 533/13868 [06:32<2:41:29,  1.38it/s]Training epoch 1:   4% 534/13868 [06:32<2:42:25,  1.37it/s]Training epoch 1:   4% 535/13868 [06:33<2:42:25,  1.37it/s]Training epoch 1:   4% 536/13868 [06:34<2:41:57,  1.37it/s]Training epoch 1:   4% 537/13868 [06:35<2:40:50,  1.38it/s]Training epoch 1:   4% 538/13868 [06:35<2:39:48,  1.39it/s]Training epoch 1:   4% 539/13868 [06:36<2:42:37,  1.37it/s]Training epoch 1:   4% 540/13868 [06:37<2:41:09,  1.38it/s]Training epoch 1:   4% 541/13868 [06:37<2:38:53,  1.40it/s]Training epoch 1:   4% 542/13868 [06:38<2:39:45,  1.39it/s]Training epoch 1:   4% 543/13868 [06:39<2:40:53,  1.38it/s]Training epoch 1:   4% 544/13868 [06:40<2:38:24,  1.40it/s]Training epoch 1:   4% 545/13868 [06:40<2:38:41,  1.40it/s]Training epoch 1:   4% 546/13868 [06:41<2:38:51,  1.40it/s]Training epoch 1:   4% 547/13868 [06:42<2:40:58,  1.38it/s]Training epoch 1:   4% 548/13868 [06:43<2:43:37,  1.36it/s]Training epoch 1:   4% 549/13868 [06:43<2:43:32,  1.36it/s]Training epoch 1:   4% 550/13868 [06:44<2:42:54,  1.36it/s]Training epoch 1:   4% 551/13868 [06:45<2:42:04,  1.37it/s]Training epoch 1:   4% 552/13868 [06:46<2:48:20,  1.32it/s]Training epoch 1:   4% 553/13868 [06:46<2:46:35,  1.33it/s]Training epoch 1:   4% 554/13868 [06:47<2:45:47,  1.34it/s]Training epoch 1:   4% 555/13868 [06:48<2:46:45,  1.33it/s]Training epoch 1:   4% 556/13868 [06:49<2:43:47,  1.35it/s]Training epoch 1:   4% 557/13868 [06:49<2:46:33,  1.33it/s]Training epoch 1:   4% 558/13868 [06:50<2:44:10,  1.35it/s]Training epoch 1:   4% 559/13868 [06:51<2:44:16,  1.35it/s]Training epoch 1:   4% 560/13868 [06:51<2:40:15,  1.38it/s]Training epoch 1:   4% 561/13868 [06:52<2:40:25,  1.38it/s]Training epoch 1:   4% 562/13868 [06:53<2:40:46,  1.38it/s]Training epoch 1:   4% 563/13868 [06:54<2:40:26,  1.38it/s]Training epoch 1:   4% 564/13868 [06:54<2:40:48,  1.38it/s]Training epoch 1:   4% 565/13868 [06:55<2:43:02,  1.36it/s]Training epoch 1:   4% 566/13868 [06:56<2:43:44,  1.35it/s]Training epoch 1:   4% 567/13868 [06:57<2:41:41,  1.37it/s]Training epoch 1:   4% 568/13868 [06:57<2:41:15,  1.37it/s]Training epoch 1:   4% 569/13868 [06:58<2:42:39,  1.36it/s]Training epoch 1:   4% 570/13868 [06:59<2:42:46,  1.36it/s]Training epoch 1:   4% 571/13868 [07:00<2:42:38,  1.36it/s]Training epoch 1:   4% 572/13868 [07:00<2:43:21,  1.36it/s]Training epoch 1:   4% 573/13868 [07:01<2:45:13,  1.34it/s]Training epoch 1:   4% 574/13868 [07:02<2:44:29,  1.35it/s]Training epoch 1:   4% 575/13868 [07:03<2:44:51,  1.34it/s]Training epoch 1:   4% 576/13868 [07:03<2:44:40,  1.35it/s]Training epoch 1:   4% 577/13868 [07:04<2:41:57,  1.37it/s]Training epoch 1:   4% 578/13868 [07:05<2:43:56,  1.35it/s]Training epoch 1:   4% 579/13868 [07:05<2:41:47,  1.37it/s]Training epoch 1:   4% 580/13868 [07:06<2:41:05,  1.37it/s]Training epoch 1:   4% 581/13868 [07:07<2:43:01,  1.36it/s]Training epoch 1:   4% 582/13868 [07:08<2:40:57,  1.38it/s]Training epoch 1:   4% 583/13868 [07:08<2:42:29,  1.36it/s]Training epoch 1:   4% 584/13868 [07:09<2:43:29,  1.35it/s]Training epoch 1:   4% 585/13868 [07:10<2:41:33,  1.37it/s]Training epoch 1:   4% 586/13868 [07:11<2:40:08,  1.38it/s]Training epoch 1:   4% 587/13868 [07:11<2:41:00,  1.37it/s]Training epoch 1:   4% 588/13868 [07:12<2:38:51,  1.39it/s]Training epoch 1:   4% 589/13868 [07:13<2:38:52,  1.39it/s]Training epoch 1:   4% 590/13868 [07:13<2:38:16,  1.40it/s]Training epoch 1:   4% 591/13868 [07:14<2:40:19,  1.38it/s]Training epoch 1:   4% 592/13868 [07:15<2:37:18,  1.41it/s]Training epoch 1:   4% 593/13868 [07:16<2:39:33,  1.39it/s]Training epoch 1:   4% 594/13868 [07:16<2:37:20,  1.41it/s]Training epoch 1:   4% 595/13868 [07:17<2:38:58,  1.39it/s]Training epoch 1:   4% 596/13868 [07:18<2:40:09,  1.38it/s]Training epoch 1:   4% 597/13868 [07:18<2:40:48,  1.38it/s]Training epoch 1:   4% 598/13868 [07:19<2:39:23,  1.39it/s]Training epoch 1:   4% 599/13868 [07:20<2:41:19,  1.37it/s]Training epoch 1:   4% 600/13868 [07:21<2:49:59,  1.30it/s]Training epoch 1:   4% 601/13868 [07:22<2:48:45,  1.31it/s]Training epoch 1:   4% 602/13868 [07:22<2:45:56,  1.33it/s]Training epoch 1:   4% 603/13868 [07:23<2:45:23,  1.34it/s]Training epoch 1:   4% 604/13868 [07:24<2:44:53,  1.34it/s]Training epoch 1:   4% 605/13868 [07:24<2:45:37,  1.33it/s]Training epoch 1:   4% 606/13868 [07:25<2:45:42,  1.33it/s]Training epoch 1:   4% 607/13868 [07:26<2:46:13,  1.33it/s]Training epoch 1:   4% 608/13868 [07:27<2:47:12,  1.32it/s]Training epoch 1:   4% 609/13868 [07:27<2:44:58,  1.34it/s]Training epoch 1:   4% 610/13868 [07:28<2:45:31,  1.34it/s]Training epoch 1:   4% 611/13868 [07:29<2:45:46,  1.33it/s]Training epoch 1:   4% 612/13868 [07:30<2:41:59,  1.36it/s]Training epoch 1:   4% 613/13868 [07:30<2:44:31,  1.34it/s]Training epoch 1:   4% 614/13868 [07:31<2:42:42,  1.36it/s]Training epoch 1:   4% 615/13868 [07:32<2:44:00,  1.35it/s]Training epoch 1:   4% 616/13868 [07:33<2:43:18,  1.35it/s]Training epoch 1:   4% 617/13868 [07:33<2:43:17,  1.35it/s]Training epoch 1:   4% 618/13868 [07:34<2:42:08,  1.36it/s]Training epoch 1:   4% 619/13868 [07:35<2:42:54,  1.36it/s]Training epoch 1:   4% 620/13868 [07:36<2:43:15,  1.35it/s]Training epoch 1:   4% 621/13868 [07:36<2:43:03,  1.35it/s]Training epoch 1:   4% 622/13868 [07:37<2:43:43,  1.35it/s]Training epoch 1:   4% 623/13868 [07:38<2:44:01,  1.35it/s]Training epoch 1:   4% 624/13868 [07:39<2:42:11,  1.36it/s]Training epoch 1:   5% 625/13868 [07:39<2:43:11,  1.35it/s]Training epoch 1:   5% 626/13868 [07:40<2:43:40,  1.35it/s]Training epoch 1:   5% 627/13868 [07:41<2:42:59,  1.35it/s]Training epoch 1:   5% 628/13868 [07:41<2:41:37,  1.37it/s]Training epoch 1:   5% 629/13868 [07:42<2:43:24,  1.35it/s]Training epoch 1:   5% 630/13868 [07:43<2:45:44,  1.33it/s]Training epoch 1:   5% 631/13868 [07:44<2:47:03,  1.32it/s]Training epoch 1:   5% 632/13868 [07:44<2:42:39,  1.36it/s]Training epoch 1:   5% 633/13868 [07:45<2:44:42,  1.34it/s]Training epoch 1:   5% 634/13868 [07:46<2:45:38,  1.33it/s]Training epoch 1:   5% 635/13868 [07:47<2:43:46,  1.35it/s]Training epoch 1:   5% 636/13868 [07:47<2:42:31,  1.36it/s]Training epoch 1:   5% 637/13868 [07:48<2:42:33,  1.36it/s]Training epoch 1:   5% 638/13868 [07:49<2:44:18,  1.34it/s]Training epoch 1:   5% 639/13868 [07:50<2:45:15,  1.33it/s]Training epoch 1:   5% 640/13868 [07:50<2:43:53,  1.35it/s]Training epoch 1:   5% 641/13868 [07:51<2:40:45,  1.37it/s]Training epoch 1:   5% 642/13868 [07:52<2:43:15,  1.35it/s]Training epoch 1:   5% 643/13868 [07:53<2:41:28,  1.36it/s]Training epoch 1:   5% 644/13868 [07:53<2:41:17,  1.37it/s]Training epoch 1:   5% 645/13868 [07:54<2:39:25,  1.38it/s]Training epoch 1:   5% 646/13868 [07:55<2:40:35,  1.37it/s]Training epoch 1:   5% 647/13868 [07:56<2:39:58,  1.38it/s]Training epoch 1:   5% 648/13868 [07:56<2:41:59,  1.36it/s]Training epoch 1:   5% 649/13868 [07:57<2:40:40,  1.37it/s]Training epoch 1:   5% 650/13868 [07:58<2:40:51,  1.37it/s]Training epoch 1:   5% 651/13868 [07:58<2:41:19,  1.37it/s]Training epoch 1:   5% 652/13868 [07:59<2:41:58,  1.36it/s]Training epoch 1:   5% 653/13868 [08:00<2:43:01,  1.35it/s]Training epoch 1:   5% 654/13868 [08:01<2:44:09,  1.34it/s]Training epoch 1:   5% 655/13868 [08:01<2:42:55,  1.35it/s]Training epoch 1:   5% 656/13868 [08:02<2:43:44,  1.34it/s]Training epoch 1:   5% 657/13868 [08:03<2:42:40,  1.35it/s]Training epoch 1:   5% 658/13868 [08:04<2:42:57,  1.35it/s]Training epoch 1:   5% 659/13868 [08:04<2:42:39,  1.35it/s]Training epoch 1:   5% 660/13868 [08:05<2:42:53,  1.35it/s]Training epoch 1:   5% 661/13868 [08:06<2:44:05,  1.34it/s]Training epoch 1:   5% 662/13868 [08:07<2:43:05,  1.35it/s]Training epoch 1:   5% 663/13868 [08:07<2:40:28,  1.37it/s]Training epoch 1:   5% 664/13868 [08:08<2:41:11,  1.37it/s]Training epoch 1:   5% 665/13868 [08:09<2:41:26,  1.36it/s]Training epoch 1:   5% 666/13868 [08:10<2:42:20,  1.36it/s]Training epoch 1:   5% 667/13868 [08:10<2:43:12,  1.35it/s]Training epoch 1:   5% 668/13868 [08:11<2:42:30,  1.35it/s]Training epoch 1:   5% 669/13868 [08:12<2:43:21,  1.35it/s]Training epoch 1:   5% 670/13868 [08:12<2:40:52,  1.37it/s]Training epoch 1:   5% 671/13868 [08:13<2:43:27,  1.35it/s]Training epoch 1:   5% 672/13868 [08:14<2:43:29,  1.35it/s]Training epoch 1:   5% 673/13868 [08:15<2:43:39,  1.34it/s]Training epoch 1:   5% 674/13868 [08:16<2:44:59,  1.33it/s]Training epoch 1:   5% 675/13868 [08:16<2:41:14,  1.36it/s]Training epoch 1:   5% 676/13868 [08:17<2:39:44,  1.38it/s]Training epoch 1:   5% 677/13868 [08:18<2:40:14,  1.37it/s]Training epoch 1:   5% 678/13868 [08:18<2:41:00,  1.37it/s]Training epoch 1:   5% 679/13868 [08:19<2:41:11,  1.36it/s]Training epoch 1:   5% 680/13868 [08:20<2:39:35,  1.38it/s]Training epoch 1:   5% 681/13868 [08:21<2:37:48,  1.39it/s]Training epoch 1:   5% 682/13868 [08:21<2:39:16,  1.38it/s]Training epoch 1:   5% 683/13868 [08:22<2:40:44,  1.37it/s]Training epoch 1:   5% 684/13868 [08:23<2:41:41,  1.36it/s]Training epoch 1:   5% 685/13868 [08:24<2:41:47,  1.36it/s]Training epoch 1:   5% 686/13868 [08:24<2:41:50,  1.36it/s]Training epoch 1:   5% 687/13868 [08:25<2:42:21,  1.35it/s]Training epoch 1:   5% 688/13868 [08:26<2:41:48,  1.36it/s]Training epoch 1:   5% 689/13868 [08:26<2:42:16,  1.35it/s]Training epoch 1:   5% 690/13868 [08:27<2:42:34,  1.35it/s]Training epoch 1:   5% 691/13868 [08:28<2:42:31,  1.35it/s]Training epoch 1:   5% 692/13868 [08:29<2:41:49,  1.36it/s]Training epoch 1:   5% 693/13868 [08:29<2:44:27,  1.34it/s]Training epoch 1:   5% 694/13868 [08:30<2:42:26,  1.35it/s]Training epoch 1:   5% 695/13868 [08:31<2:40:32,  1.37it/s]Training epoch 1:   5% 696/13868 [08:32<2:41:40,  1.36it/s]Training epoch 1:   5% 697/13868 [08:32<2:40:39,  1.37it/s]Training epoch 1:   5% 698/13868 [08:33<2:40:31,  1.37it/s]Training epoch 1:   5% 699/13868 [08:34<2:41:27,  1.36it/s]Training epoch 1:   5% 700/13868 [08:35<2:49:59,  1.29it/s]Training epoch 1:   5% 701/13868 [08:35<2:45:40,  1.32it/s]Training epoch 1:   5% 702/13868 [08:36<2:45:09,  1.33it/s]Training epoch 1:   5% 703/13868 [08:37<2:45:04,  1.33it/s]Training epoch 1:   5% 704/13868 [08:38<2:43:29,  1.34it/s]Training epoch 1:   5% 705/13868 [08:38<2:44:04,  1.34it/s]Training epoch 1:   5% 706/13868 [08:39<2:45:03,  1.33it/s]Training epoch 1:   5% 707/13868 [08:40<2:44:24,  1.33it/s]Training epoch 1:   5% 708/13868 [08:41<2:45:03,  1.33it/s]Training epoch 1:   5% 709/13868 [08:41<2:44:22,  1.33it/s]Training epoch 1:   5% 710/13868 [08:42<2:42:50,  1.35it/s]Training epoch 1:   5% 711/13868 [08:43<2:44:29,  1.33it/s]Training epoch 1:   5% 712/13868 [08:44<2:41:40,  1.36it/s]Training epoch 1:   5% 713/13868 [08:44<2:40:53,  1.36it/s]Training epoch 1:   5% 714/13868 [08:45<2:42:28,  1.35it/s]Training epoch 1:   5% 715/13868 [08:46<2:41:24,  1.36it/s]Training epoch 1:   5% 716/13868 [08:47<2:40:06,  1.37it/s]Training epoch 1:   5% 717/13868 [08:47<2:41:04,  1.36it/s]Training epoch 1:   5% 718/13868 [08:48<2:39:01,  1.38it/s]Training epoch 1:   5% 719/13868 [08:49<2:39:04,  1.38it/s]Training epoch 1:   5% 720/13868 [08:49<2:41:18,  1.36it/s]Training epoch 1:   5% 721/13868 [08:50<2:39:34,  1.37it/s]Training epoch 1:   5% 722/13868 [08:51<2:39:18,  1.38it/s]Training epoch 1:   5% 723/13868 [08:52<2:37:20,  1.39it/s]Training epoch 1:   5% 724/13868 [08:52<2:37:47,  1.39it/s]Training epoch 1:   5% 725/13868 [08:53<2:37:52,  1.39it/s]Training epoch 1:   5% 726/13868 [08:54<2:38:11,  1.38it/s]Training epoch 1:   5% 727/13868 [08:55<2:38:45,  1.38it/s]Training epoch 1:   5% 728/13868 [08:55<2:41:04,  1.36it/s]Training epoch 1:   5% 729/13868 [08:56<2:40:48,  1.36it/s]Training epoch 1:   5% 730/13868 [08:57<2:40:01,  1.37it/s]Training epoch 1:   5% 731/13868 [08:57<2:41:26,  1.36it/s]Training epoch 1:   5% 732/13868 [08:58<2:41:33,  1.36it/s]Training epoch 1:   5% 733/13868 [08:59<2:40:59,  1.36it/s]Training epoch 1:   5% 734/13868 [09:00<2:40:17,  1.37it/s]Training epoch 1:   5% 735/13868 [09:00<2:41:07,  1.36it/s]Training epoch 1:   5% 736/13868 [09:01<2:40:36,  1.36it/s]Training epoch 1:   5% 737/13868 [09:02<2:40:52,  1.36it/s]Training epoch 1:   5% 738/13868 [09:03<2:41:49,  1.35it/s]Training epoch 1:   5% 739/13868 [09:03<2:40:51,  1.36it/s]Training epoch 1:   5% 740/13868 [09:04<2:39:56,  1.37it/s]Training epoch 1:   5% 741/13868 [09:05<2:40:23,  1.36it/s]Training epoch 1:   5% 742/13868 [09:06<2:42:02,  1.35it/s]Training epoch 1:   5% 743/13868 [09:06<2:42:03,  1.35it/s]Training epoch 1:   5% 744/13868 [09:07<2:42:26,  1.35it/s]Training epoch 1:   5% 745/13868 [09:08<2:41:10,  1.36it/s]Training epoch 1:   5% 746/13868 [09:09<2:40:03,  1.37it/s]Training epoch 1:   5% 747/13868 [09:09<2:40:23,  1.36it/s]Training epoch 1:   5% 748/13868 [09:10<2:39:01,  1.38it/s]Training epoch 1:   5% 749/13868 [09:11<2:39:20,  1.37it/s]Training epoch 1:   5% 750/13868 [09:11<2:40:41,  1.36it/s]Training epoch 1:   5% 751/13868 [09:12<2:38:03,  1.38it/s]Training epoch 1:   5% 752/13868 [09:13<2:40:38,  1.36it/s]Training epoch 1:   5% 753/13868 [09:14<2:41:00,  1.36it/s]Training epoch 1:   5% 754/13868 [09:14<2:37:36,  1.39it/s]Training epoch 1:   5% 755/13868 [09:15<2:37:56,  1.38it/s]Training epoch 1:   5% 756/13868 [09:16<2:38:30,  1.38it/s]Training epoch 1:   5% 757/13868 [09:16<2:37:34,  1.39it/s]Training epoch 1:   5% 758/13868 [09:17<2:37:39,  1.39it/s]Training epoch 1:   5% 759/13868 [09:18<2:38:17,  1.38it/s]Training epoch 1:   5% 760/13868 [09:19<2:37:47,  1.38it/s]Training epoch 1:   5% 761/13868 [09:19<2:38:16,  1.38it/s]Training epoch 1:   5% 762/13868 [09:20<2:39:12,  1.37it/s]Training epoch 1:   6% 763/13868 [09:21<2:39:31,  1.37it/s]Training epoch 1:   6% 764/13868 [09:22<2:42:03,  1.35it/s]Training epoch 1:   6% 765/13868 [09:22<2:40:41,  1.36it/s]Training epoch 1:   6% 766/13868 [09:23<2:40:21,  1.36it/s]Training epoch 1:   6% 767/13868 [09:24<2:41:42,  1.35it/s]Training epoch 1:   6% 768/13868 [09:25<2:42:12,  1.35it/s]Training epoch 1:   6% 769/13868 [09:25<2:42:36,  1.34it/s]Training epoch 1:   6% 770/13868 [09:26<2:39:51,  1.37it/s]Training epoch 1:   6% 771/13868 [09:27<2:39:47,  1.37it/s]Training epoch 1:   6% 772/13868 [09:27<2:38:29,  1.38it/s]Training epoch 1:   6% 773/13868 [09:28<2:37:53,  1.38it/s]Training epoch 1:   6% 774/13868 [09:29<2:36:56,  1.39it/s]Training epoch 1:   6% 775/13868 [09:30<2:38:19,  1.38it/s]Training epoch 1:   6% 776/13868 [09:30<2:40:08,  1.36it/s]Training epoch 1:   6% 777/13868 [09:31<2:39:30,  1.37it/s]Training epoch 1:   6% 778/13868 [09:32<2:40:37,  1.36it/s]Training epoch 1:   6% 779/13868 [09:33<2:39:36,  1.37it/s]Training epoch 1:   6% 780/13868 [09:33<2:39:24,  1.37it/s]Training epoch 1:   6% 781/13868 [09:34<2:39:03,  1.37it/s]Training epoch 1:   6% 782/13868 [09:35<2:40:12,  1.36it/s]Training epoch 1:   6% 783/13868 [09:36<2:40:40,  1.36it/s]Training epoch 1:   6% 784/13868 [09:36<2:39:51,  1.36it/s]Training epoch 1:   6% 785/13868 [09:37<2:41:46,  1.35it/s]Training epoch 1:   6% 786/13868 [09:38<2:41:39,  1.35it/s]Training epoch 1:   6% 787/13868 [09:38<2:40:37,  1.36it/s]Training epoch 1:   6% 788/13868 [09:39<2:39:39,  1.37it/s]Training epoch 1:   6% 789/13868 [09:40<2:37:10,  1.39it/s]Training epoch 1:   6% 790/13868 [09:41<2:40:11,  1.36it/s]Training epoch 1:   6% 791/13868 [09:41<2:37:39,  1.38it/s]Training epoch 1:   6% 792/13868 [09:42<2:36:31,  1.39it/s]Training epoch 1:   6% 793/13868 [09:43<2:40:18,  1.36it/s]Training epoch 1:   6% 794/13868 [09:44<2:39:23,  1.37it/s]Training epoch 1:   6% 795/13868 [09:44<2:40:14,  1.36it/s]Training epoch 1:   6% 796/13868 [09:45<2:40:14,  1.36it/s]Training epoch 1:   6% 797/13868 [09:46<2:42:54,  1.34it/s]Training epoch 1:   6% 798/13868 [09:47<2:42:37,  1.34it/s]Training epoch 1:   6% 799/13868 [09:47<2:41:27,  1.35it/s]Training epoch 1:   6% 800/13868 [09:48<2:49:30,  1.28it/s]Training epoch 1:   6% 801/13868 [09:49<2:47:27,  1.30it/s]Training epoch 1:   6% 802/13868 [09:50<2:44:07,  1.33it/s]Training epoch 1:   6% 803/13868 [09:50<2:42:10,  1.34it/s]Training epoch 1:   6% 804/13868 [09:51<2:40:40,  1.36it/s]Training epoch 1:   6% 805/13868 [09:52<2:39:39,  1.36it/s]Training epoch 1:   6% 806/13868 [09:53<2:43:27,  1.33it/s]Training epoch 1:   6% 807/13868 [09:53<2:43:12,  1.33it/s]Training epoch 1:   6% 808/13868 [09:54<2:43:33,  1.33it/s]Training epoch 1:   6% 809/13868 [09:55<2:42:23,  1.34it/s]Training epoch 1:   6% 810/13868 [09:56<2:40:50,  1.35it/s]Training epoch 1:   6% 811/13868 [09:56<2:39:54,  1.36it/s]Training epoch 1:   6% 812/13868 [09:57<2:41:16,  1.35it/s]Training epoch 1:   6% 813/13868 [09:58<2:39:05,  1.37it/s]Training epoch 1:   6% 814/13868 [09:58<2:38:11,  1.38it/s]Training epoch 1:   6% 815/13868 [09:59<2:39:13,  1.37it/s]Training epoch 1:   6% 816/13868 [10:00<2:40:33,  1.35it/s]Training epoch 1:   6% 817/13868 [10:01<2:40:17,  1.36it/s]Training epoch 1:   6% 818/13868 [10:01<2:39:06,  1.37it/s]Training epoch 1:   6% 819/13868 [10:02<2:39:24,  1.36it/s]Training epoch 1:   6% 820/13868 [10:03<2:39:10,  1.37it/s]Training epoch 1:   6% 821/13868 [10:04<2:38:59,  1.37it/s]Training epoch 1:   6% 822/13868 [10:04<2:40:39,  1.35it/s]Training epoch 1:   6% 823/13868 [10:05<2:41:25,  1.35it/s]Training epoch 1:   6% 824/13868 [10:06<2:42:19,  1.34it/s]Training epoch 1:   6% 825/13868 [10:07<2:42:29,  1.34it/s]Training epoch 1:   6% 826/13868 [10:07<2:41:42,  1.34it/s]Training epoch 1:   6% 827/13868 [10:08<2:40:03,  1.36it/s]Training epoch 1:   6% 828/13868 [10:09<2:39:04,  1.37it/s]Training epoch 1:   6% 829/13868 [10:10<2:38:59,  1.37it/s]Training epoch 1:   6% 830/13868 [10:10<2:39:14,  1.36it/s]Training epoch 1:   6% 831/13868 [10:11<2:39:31,  1.36it/s]Training epoch 1:   6% 832/13868 [10:12<2:38:25,  1.37it/s]Training epoch 1:   6% 833/13868 [10:12<2:39:08,  1.37it/s]Training epoch 1:   6% 834/13868 [10:13<2:39:35,  1.36it/s]Training epoch 1:   6% 835/13868 [10:14<2:39:39,  1.36it/s]Training epoch 1:   6% 836/13868 [10:15<2:40:21,  1.35it/s]Training epoch 1:   6% 837/13868 [10:15<2:39:41,  1.36it/s]Training epoch 1:   6% 838/13868 [10:16<2:40:35,  1.35it/s]Training epoch 1:   6% 839/13868 [10:17<2:41:16,  1.35it/s]Training epoch 1:   6% 840/13868 [10:18<2:40:31,  1.35it/s]Training epoch 1:   6% 841/13868 [10:18<2:39:32,  1.36it/s]Training epoch 1:   6% 842/13868 [10:19<2:37:30,  1.38it/s]Training epoch 1:   6% 843/13868 [10:20<2:37:39,  1.38it/s]Training epoch 1:   6% 844/13868 [10:21<2:39:01,  1.36it/s]Training epoch 1:   6% 845/13868 [10:21<2:36:14,  1.39it/s]Training epoch 1:   6% 846/13868 [10:22<2:38:01,  1.37it/s]Training epoch 1:   6% 847/13868 [10:23<2:39:40,  1.36it/s]Training epoch 1:   6% 848/13868 [10:23<2:38:04,  1.37it/s]Training epoch 1:   6% 849/13868 [10:24<2:40:24,  1.35it/s]Training epoch 1:   6% 850/13868 [10:25<2:41:23,  1.34it/s]Training epoch 1:   6% 851/13868 [10:26<2:43:59,  1.32it/s]Training epoch 1:   6% 852/13868 [10:26<2:41:13,  1.35it/s]Training epoch 1:   6% 853/13868 [10:27<2:39:57,  1.36it/s]Training epoch 1:   6% 854/13868 [10:28<2:42:14,  1.34it/s]Training epoch 1:   6% 855/13868 [10:29<2:40:40,  1.35it/s]Training epoch 1:   6% 856/13868 [10:29<2:41:30,  1.34it/s]Training epoch 1:   6% 857/13868 [10:30<2:39:58,  1.36it/s]Training epoch 1:   6% 858/13868 [10:31<2:39:14,  1.36it/s]Training epoch 1:   6% 859/13868 [10:32<2:37:19,  1.38it/s]Training epoch 1:   6% 860/13868 [10:32<2:39:29,  1.36it/s]Training epoch 1:   6% 861/13868 [10:33<2:38:25,  1.37it/s]Training epoch 1:   6% 862/13868 [10:34<2:39:57,  1.36it/s]Training epoch 1:   6% 863/13868 [10:35<2:38:19,  1.37it/s]Training epoch 1:   6% 864/13868 [10:35<2:38:25,  1.37it/s]Training epoch 1:   6% 865/13868 [10:36<2:38:03,  1.37it/s]Training epoch 1:   6% 866/13868 [10:37<2:40:21,  1.35it/s]Training epoch 1:   6% 867/13868 [10:37<2:40:18,  1.35it/s]Training epoch 1:   6% 868/13868 [10:38<2:39:20,  1.36it/s]Training epoch 1:   6% 869/13868 [10:39<2:37:54,  1.37it/s]Training epoch 1:   6% 870/13868 [10:40<2:40:44,  1.35it/s]Training epoch 1:   6% 871/13868 [10:40<2:41:20,  1.34it/s]Training epoch 1:   6% 872/13868 [10:41<2:40:56,  1.35it/s]Training epoch 1:   6% 873/13868 [10:42<2:40:08,  1.35it/s]Training epoch 1:   6% 874/13868 [10:43<2:39:12,  1.36it/s]Training epoch 1:   6% 875/13868 [10:43<2:37:22,  1.38it/s]Training epoch 1:   6% 876/13868 [10:44<2:36:29,  1.38it/s]Training epoch 1:   6% 877/13868 [10:45<2:38:44,  1.36it/s]Training epoch 1:   6% 878/13868 [10:46<2:38:22,  1.37it/s]Training epoch 1:   6% 879/13868 [10:46<2:37:11,  1.38it/s]Training epoch 1:   6% 880/13868 [10:47<2:36:58,  1.38it/s]Training epoch 1:   6% 881/13868 [10:48<2:36:55,  1.38it/s]Training epoch 1:   6% 882/13868 [10:48<2:38:19,  1.37it/s]Training epoch 1:   6% 883/13868 [10:49<2:38:13,  1.37it/s]Training epoch 1:   6% 884/13868 [10:50<2:36:15,  1.38it/s]Training epoch 1:   6% 885/13868 [10:51<2:34:52,  1.40it/s]Training epoch 1:   6% 886/13868 [10:51<2:35:33,  1.39it/s]Training epoch 1:   6% 887/13868 [10:52<2:34:55,  1.40it/s]Training epoch 1:   6% 888/13868 [10:53<2:36:37,  1.38it/s]Training epoch 1:   6% 889/13868 [10:53<2:36:36,  1.38it/s]Training epoch 1:   6% 890/13868 [10:54<2:35:44,  1.39it/s]Training epoch 1:   6% 891/13868 [10:55<2:37:26,  1.37it/s]Training epoch 1:   6% 892/13868 [10:56<2:37:40,  1.37it/s]Training epoch 1:   6% 893/13868 [10:56<2:36:09,  1.38it/s]Training epoch 1:   6% 894/13868 [10:57<2:37:02,  1.38it/s]Training epoch 1:   6% 895/13868 [10:58<2:36:59,  1.38it/s]Training epoch 1:   6% 896/13868 [10:59<2:37:29,  1.37it/s]Training epoch 1:   6% 897/13868 [10:59<2:37:02,  1.38it/s]Training epoch 1:   6% 898/13868 [11:00<2:37:54,  1.37it/s]Training epoch 1:   6% 899/13868 [11:01<2:39:19,  1.36it/s]Training epoch 1:   6% 900/13868 [11:02<2:47:48,  1.29it/s]Training epoch 1:   6% 901/13868 [11:02<2:42:18,  1.33it/s]Training epoch 1:   7% 902/13868 [11:03<2:40:00,  1.35it/s]Training epoch 1:   7% 903/13868 [11:04<2:38:57,  1.36it/s]Training epoch 1:   7% 904/13868 [11:05<2:38:21,  1.36it/s]Training epoch 1:   7% 905/13868 [11:05<2:39:57,  1.35it/s]Training epoch 1:   7% 906/13868 [11:06<2:38:05,  1.37it/s]Training epoch 1:   7% 907/13868 [11:07<2:38:10,  1.37it/s]Training epoch 1:   7% 908/13868 [11:07<2:38:02,  1.37it/s]Training epoch 1:   7% 909/13868 [11:08<2:37:15,  1.37it/s]Training epoch 1:   7% 910/13868 [11:09<2:38:00,  1.37it/s]Training epoch 1:   7% 911/13868 [11:10<2:38:59,  1.36it/s]Training epoch 1:   7% 912/13868 [11:10<2:38:49,  1.36it/s]Training epoch 1:   7% 913/13868 [11:11<2:36:57,  1.38it/s]Training epoch 1:   7% 914/13868 [11:12<2:37:56,  1.37it/s]Training epoch 1:   7% 915/13868 [11:13<2:37:32,  1.37it/s]Training epoch 1:   7% 916/13868 [11:13<2:38:12,  1.36it/s]Training epoch 1:   7% 917/13868 [11:14<2:38:57,  1.36it/s]Training epoch 1:   7% 918/13868 [11:15<2:40:07,  1.35it/s]Training epoch 1:   7% 919/13868 [11:16<2:40:53,  1.34it/s]Training epoch 1:   7% 920/13868 [11:16<2:38:41,  1.36it/s]Training epoch 1:   7% 921/13868 [11:17<2:37:08,  1.37it/s]Training epoch 1:   7% 922/13868 [11:18<2:37:31,  1.37it/s]Training epoch 1:   7% 923/13868 [11:18<2:36:53,  1.38it/s]Training epoch 1:   7% 924/13868 [11:19<2:38:37,  1.36it/s]Training epoch 1:   7% 925/13868 [11:20<2:38:37,  1.36it/s]Training epoch 1:   7% 926/13868 [11:21<2:37:29,  1.37it/s]Training epoch 1:   7% 927/13868 [11:21<2:39:38,  1.35it/s]Training epoch 1:   7% 928/13868 [11:22<2:38:11,  1.36it/s]Training epoch 1:   7% 929/13868 [11:23<2:37:46,  1.37it/s]Training epoch 1:   7% 930/13868 [11:24<2:37:59,  1.36it/s]Training epoch 1:   7% 931/13868 [11:24<2:36:59,  1.37it/s]Training epoch 1:   7% 932/13868 [11:25<2:36:04,  1.38it/s]Training epoch 1:   7% 933/13868 [11:26<2:36:24,  1.38it/s]Training epoch 1:   7% 934/13868 [11:26<2:36:57,  1.37it/s]Training epoch 1:   7% 935/13868 [11:27<2:38:50,  1.36it/s]Training epoch 1:   7% 936/13868 [11:28<2:40:02,  1.35it/s]Training epoch 1:   7% 937/13868 [11:29<2:40:02,  1.35it/s]Training epoch 1:   7% 938/13868 [11:29<2:40:18,  1.34it/s]Training epoch 1:   7% 939/13868 [11:30<2:38:03,  1.36it/s]Training epoch 1:   7% 940/13868 [11:31<2:34:57,  1.39it/s]Training epoch 1:   7% 941/13868 [11:32<2:37:14,  1.37it/s]Training epoch 1:   7% 942/13868 [11:32<2:36:06,  1.38it/s]Training epoch 1:   7% 943/13868 [11:33<2:35:54,  1.38it/s]Training epoch 1:   7% 944/13868 [11:34<2:36:33,  1.38it/s]Training epoch 1:   7% 945/13868 [11:35<2:37:03,  1.37it/s]Training epoch 1:   7% 946/13868 [11:35<2:35:46,  1.38it/s]Training epoch 1:   7% 947/13868 [11:36<2:37:36,  1.37it/s]Training epoch 1:   7% 948/13868 [11:37<2:37:47,  1.36it/s]Training epoch 1:   7% 949/13868 [11:37<2:38:02,  1.36it/s]Training epoch 1:   7% 950/13868 [11:38<2:35:53,  1.38it/s]Training epoch 1:   7% 951/13868 [11:39<2:37:16,  1.37it/s]Training epoch 1:   7% 952/13868 [11:40<2:36:03,  1.38it/s]Training epoch 1:   7% 953/13868 [11:40<2:35:30,  1.38it/s]Training epoch 1:   7% 954/13868 [11:41<2:38:04,  1.36it/s]Training epoch 1:   7% 955/13868 [11:42<2:40:17,  1.34it/s]Training epoch 1:   7% 956/13868 [11:43<2:39:45,  1.35it/s]Training epoch 1:   7% 957/13868 [11:43<2:38:31,  1.36it/s]Training epoch 1:   7% 958/13868 [11:44<2:37:32,  1.37it/s]Training epoch 1:   7% 959/13868 [11:45<2:36:43,  1.37it/s]Training epoch 1:   7% 960/13868 [11:46<2:35:41,  1.38it/s]Training epoch 1:   7% 961/13868 [11:46<2:36:06,  1.38it/s]Training epoch 1:   7% 962/13868 [11:47<2:34:17,  1.39it/s]Training epoch 1:   7% 963/13868 [11:48<2:37:12,  1.37it/s]Training epoch 1:   7% 964/13868 [11:48<2:36:34,  1.37it/s]Training epoch 1:   7% 965/13868 [11:49<2:37:06,  1.37it/s]Training epoch 1:   7% 966/13868 [11:50<2:36:40,  1.37it/s]Training epoch 1:   7% 967/13868 [11:51<2:36:41,  1.37it/s]Training epoch 1:   7% 968/13868 [11:51<2:37:25,  1.37it/s]Training epoch 1:   7% 969/13868 [11:52<2:34:31,  1.39it/s]Training epoch 1:   7% 970/13868 [11:53<2:34:08,  1.39it/s]Training epoch 1:   7% 971/13868 [11:53<2:35:55,  1.38it/s]Training epoch 1:   7% 972/13868 [11:54<2:36:04,  1.38it/s]Training epoch 1:   7% 973/13868 [11:55<2:36:21,  1.37it/s]Training epoch 1:   7% 974/13868 [11:56<2:35:58,  1.38it/s]Training epoch 1:   7% 975/13868 [11:56<2:34:01,  1.40it/s]Training epoch 1:   7% 976/13868 [11:57<2:34:14,  1.39it/s]Training epoch 1:   7% 977/13868 [11:58<2:33:42,  1.40it/s]Training epoch 1:   7% 978/13868 [11:59<2:33:08,  1.40it/s]Training epoch 1:   7% 979/13868 [11:59<2:33:26,  1.40it/s]Training epoch 1:   7% 980/13868 [12:00<2:34:36,  1.39it/s]Training epoch 1:   7% 981/13868 [12:01<2:34:27,  1.39it/s]Training epoch 1:   7% 982/13868 [12:01<2:36:58,  1.37it/s]Training epoch 1:   7% 983/13868 [12:02<2:39:11,  1.35it/s]Training epoch 1:   7% 984/13868 [12:03<2:36:07,  1.38it/s]Training epoch 1:   7% 985/13868 [12:04<2:37:56,  1.36it/s]Training epoch 1:   7% 986/13868 [12:04<2:36:43,  1.37it/s]Training epoch 1:   7% 987/13868 [12:05<2:39:11,  1.35it/s]Training epoch 1:   7% 988/13868 [12:06<2:38:26,  1.35it/s]Training epoch 1:   7% 989/13868 [12:07<2:38:34,  1.35it/s]Training epoch 1:   7% 990/13868 [12:07<2:36:00,  1.38it/s]Training epoch 1:   7% 991/13868 [12:08<2:35:40,  1.38it/s]Training epoch 1:   7% 992/13868 [12:09<2:35:34,  1.38it/s]Training epoch 1:   7% 993/13868 [12:09<2:36:36,  1.37it/s]Training epoch 1:   7% 994/13868 [12:10<2:38:28,  1.35it/s]Training epoch 1:   7% 995/13868 [12:11<2:40:28,  1.34it/s]Training epoch 1:   7% 996/13868 [12:12<2:37:29,  1.36it/s]Training epoch 1:   7% 997/13868 [12:12<2:37:17,  1.36it/s]Training epoch 1:   7% 998/13868 [12:13<2:38:06,  1.36it/s]Training epoch 1:   7% 999/13868 [12:14<2:37:04,  1.37it/s]Training epoch 1:   7% 1000/13868 [12:15<2:45:07,  1.30it/s]Training epoch 1:   7% 1001/13868 [12:15<2:41:25,  1.33it/s]Training epoch 1:   7% 1002/13868 [12:16<2:39:05,  1.35it/s]Training epoch 1:   7% 1003/13868 [12:17<2:39:34,  1.34it/s]Training epoch 1:   7% 1004/13868 [12:18<2:37:46,  1.36it/s]Training epoch 1:   7% 1005/13868 [12:18<2:37:53,  1.36it/s]Training epoch 1:   7% 1006/13868 [12:19<2:39:16,  1.35it/s]Training epoch 1:   7% 1007/13868 [12:20<2:38:23,  1.35it/s]Training epoch 1:   7% 1008/13868 [12:21<2:39:31,  1.34it/s]Training epoch 1:   7% 1009/13868 [12:21<2:40:04,  1.34it/s]Training epoch 1:   7% 1010/13868 [12:22<2:38:33,  1.35it/s]Training epoch 1:   7% 1011/13868 [12:23<2:36:53,  1.37it/s]Training epoch 1:   7% 1012/13868 [12:24<2:35:13,  1.38it/s]Training epoch 1:   7% 1013/13868 [12:24<2:34:57,  1.38it/s]Training epoch 1:   7% 1014/13868 [12:25<2:36:31,  1.37it/s]Training epoch 1:   7% 1015/13868 [12:26<2:35:09,  1.38it/s]Training epoch 1:   7% 1016/13868 [12:26<2:35:09,  1.38it/s]Training epoch 1:   7% 1017/13868 [12:27<2:33:34,  1.39it/s]Training epoch 1:   7% 1018/13868 [12:28<2:34:22,  1.39it/s]Training epoch 1:   7% 1019/13868 [12:29<2:36:30,  1.37it/s]Training epoch 1:   7% 1020/13868 [12:29<2:35:13,  1.38it/s]Training epoch 1:   7% 1021/13868 [12:30<2:36:31,  1.37it/s]Training epoch 1:   7% 1022/13868 [12:31<2:37:04,  1.36it/s]Training epoch 1:   7% 1023/13868 [12:32<2:38:13,  1.35it/s]Training epoch 1:   7% 1024/13868 [12:32<2:40:13,  1.34it/s]Training epoch 1:   7% 1025/13868 [12:33<2:38:35,  1.35it/s]Training epoch 1:   7% 1026/13868 [12:34<2:40:20,  1.33it/s]Training epoch 1:   7% 1027/13868 [12:35<2:39:27,  1.34it/s]Training epoch 1:   7% 1028/13868 [12:35<2:36:51,  1.36it/s]Training epoch 1:   7% 1029/13868 [12:36<2:35:03,  1.38it/s]Training epoch 1:   7% 1030/13868 [12:37<2:35:29,  1.38it/s]Training epoch 1:   7% 1031/13868 [12:37<2:34:24,  1.39it/s]Training epoch 1:   7% 1032/13868 [12:38<2:33:47,  1.39it/s]Training epoch 1:   7% 1033/13868 [12:39<2:35:14,  1.38it/s]Training epoch 1:   7% 1034/13868 [12:40<2:34:51,  1.38it/s]Training epoch 1:   7% 1035/13868 [12:40<2:34:08,  1.39it/s]Training epoch 1:   7% 1036/13868 [12:41<2:37:31,  1.36it/s]Training epoch 1:   7% 1037/13868 [12:42<2:39:43,  1.34it/s]Training epoch 1:   7% 1038/13868 [12:43<2:39:48,  1.34it/s]Training epoch 1:   7% 1039/13868 [12:43<2:38:29,  1.35it/s]Training epoch 1:   7% 1040/13868 [12:44<2:37:02,  1.36it/s]Training epoch 1:   8% 1041/13868 [12:45<2:36:30,  1.37it/s]Training epoch 1:   8% 1042/13868 [12:46<2:36:36,  1.36it/s]Training epoch 1:   8% 1043/13868 [12:46<2:36:48,  1.36it/s]Training epoch 1:   8% 1044/13868 [12:47<2:34:16,  1.39it/s]Training epoch 1:   8% 1045/13868 [12:48<2:32:13,  1.40it/s]Training epoch 1:   8% 1046/13868 [12:48<2:34:13,  1.39it/s]Training epoch 1:   8% 1047/13868 [12:49<2:35:14,  1.38it/s]Training epoch 1:   8% 1048/13868 [12:50<2:35:24,  1.37it/s]Training epoch 1:   8% 1049/13868 [12:51<2:37:06,  1.36it/s]Training epoch 1:   8% 1050/13868 [12:51<2:36:31,  1.36it/s]Training epoch 1:   8% 1051/13868 [12:52<2:36:27,  1.37it/s]Training epoch 1:   8% 1052/13868 [12:53<2:35:19,  1.38it/s]Training epoch 1:   8% 1053/13868 [12:54<2:37:50,  1.35it/s]Training epoch 1:   8% 1054/13868 [12:54<2:38:15,  1.35it/s]Training epoch 1:   8% 1055/13868 [12:55<2:40:22,  1.33it/s]Training epoch 1:   8% 1056/13868 [12:56<2:40:05,  1.33it/s]Training epoch 1:   8% 1057/13868 [12:57<2:37:37,  1.35it/s]Training epoch 1:   8% 1058/13868 [12:57<2:37:37,  1.35it/s]Training epoch 1:   8% 1059/13868 [12:58<2:36:59,  1.36it/s]Training epoch 1:   8% 1060/13868 [12:59<2:39:56,  1.33it/s]Training epoch 1:   8% 1061/13868 [13:00<2:38:52,  1.34it/s]Training epoch 1:   8% 1062/13868 [13:00<2:37:54,  1.35it/s]Training epoch 1:   8% 1063/13868 [13:01<2:38:40,  1.34it/s]Training epoch 1:   8% 1064/13868 [13:02<2:38:15,  1.35it/s]Training epoch 1:   8% 1065/13868 [13:02<2:36:34,  1.36it/s]Training epoch 1:   8% 1066/13868 [13:03<2:36:15,  1.37it/s]Training epoch 1:   8% 1067/13868 [13:04<2:35:25,  1.37it/s]Training epoch 1:   8% 1068/13868 [13:05<2:32:31,  1.40it/s]Training epoch 1:   8% 1069/13868 [13:05<2:32:26,  1.40it/s]Training epoch 1:   8% 1070/13868 [13:06<2:30:53,  1.41it/s]Training epoch 1:   8% 1071/13868 [13:07<2:31:22,  1.41it/s]Training epoch 1:   8% 1072/13868 [13:07<2:31:19,  1.41it/s]Training epoch 1:   8% 1073/13868 [13:08<2:33:11,  1.39it/s]Training epoch 1:   8% 1074/13868 [13:09<2:33:29,  1.39it/s]Training epoch 1:   8% 1075/13868 [13:10<2:34:12,  1.38it/s]Training epoch 1:   8% 1076/13868 [13:10<2:35:11,  1.37it/s]Training epoch 1:   8% 1077/13868 [13:11<2:34:52,  1.38it/s]Training epoch 1:   8% 1078/13868 [13:12<2:36:06,  1.37it/s]Training epoch 1:   8% 1079/13868 [13:13<2:37:45,  1.35it/s]Training epoch 1:   8% 1080/13868 [13:13<2:36:51,  1.36it/s]Training epoch 1:   8% 1081/13868 [13:14<2:36:57,  1.36it/s]Training epoch 1:   8% 1082/13868 [13:15<2:35:04,  1.37it/s]Training epoch 1:   8% 1083/13868 [13:15<2:34:33,  1.38it/s]Training epoch 1:   8% 1084/13868 [13:16<2:33:17,  1.39it/s]Training epoch 1:   8% 1085/13868 [13:17<2:33:12,  1.39it/s]Training epoch 1:   8% 1086/13868 [13:18<2:32:58,  1.39it/s]Training epoch 1:   8% 1087/13868 [13:18<2:34:10,  1.38it/s]Training epoch 1:   8% 1088/13868 [13:19<2:33:53,  1.38it/s]Training epoch 1:   8% 1089/13868 [13:20<2:36:32,  1.36it/s]Training epoch 1:   8% 1090/13868 [13:21<2:37:20,  1.35it/s]Training epoch 1:   8% 1091/13868 [13:21<2:35:29,  1.37it/s]Training epoch 1:   8% 1092/13868 [13:22<2:35:24,  1.37it/s]Training epoch 1:   8% 1093/13868 [13:23<2:33:35,  1.39it/s]Training epoch 1:   8% 1094/13868 [13:23<2:35:20,  1.37it/s]Training epoch 1:   8% 1095/13868 [13:24<2:38:03,  1.35it/s]Training epoch 1:   8% 1096/13868 [13:25<2:36:35,  1.36it/s]Training epoch 1:   8% 1097/13868 [13:26<2:36:47,  1.36it/s]Training epoch 1:   8% 1098/13868 [13:26<2:36:07,  1.36it/s]Training epoch 1:   8% 1099/13868 [13:27<2:37:23,  1.35it/s]Training epoch 1:   8% 1100/13868 [13:28<2:44:46,  1.29it/s]Training epoch 1:   8% 1101/13868 [13:29<2:44:29,  1.29it/s]Training epoch 1:   8% 1102/13868 [13:29<2:40:21,  1.33it/s]Training epoch 1:   8% 1103/13868 [13:30<2:39:59,  1.33it/s]Training epoch 1:   8% 1104/13868 [13:31<2:39:03,  1.34it/s]Training epoch 1:   8% 1105/13868 [13:32<2:38:11,  1.34it/s]Training epoch 1:   8% 1106/13868 [13:32<2:39:01,  1.34it/s]Training epoch 1:   8% 1107/13868 [13:33<2:37:51,  1.35it/s]Training epoch 1:   8% 1108/13868 [13:34<2:38:00,  1.35it/s]Training epoch 1:   8% 1109/13868 [13:35<2:38:31,  1.34it/s]Training epoch 1:   8% 1110/13868 [13:35<2:38:04,  1.35it/s]Training epoch 1:   8% 1111/13868 [13:36<2:36:21,  1.36it/s]Training epoch 1:   8% 1112/13868 [13:37<2:34:53,  1.37it/s]Training epoch 1:   8% 1113/13868 [13:38<2:33:00,  1.39it/s]Training epoch 1:   8% 1114/13868 [13:38<2:32:42,  1.39it/s]Training epoch 1:   8% 1115/13868 [13:39<2:34:55,  1.37it/s]Training epoch 1:   8% 1116/13868 [13:40<2:35:58,  1.36it/s]Training epoch 1:   8% 1117/13868 [13:40<2:34:25,  1.38it/s]Training epoch 1:   8% 1118/13868 [13:41<2:34:20,  1.38it/s]Training epoch 1:   8% 1119/13868 [13:42<2:35:12,  1.37it/s]Training epoch 1:   8% 1120/13868 [13:43<2:36:57,  1.35it/s]Training epoch 1:   8% 1121/13868 [13:43<2:36:20,  1.36it/s]Training epoch 1:   8% 1122/13868 [13:44<2:36:00,  1.36it/s]Training epoch 1:   8% 1123/13868 [13:45<2:35:44,  1.36it/s]Training epoch 1:   8% 1124/13868 [13:46<2:33:42,  1.38it/s]Training epoch 1:   8% 1125/13868 [13:46<2:34:13,  1.38it/s]Training epoch 1:   8% 1126/13868 [13:47<2:33:41,  1.38it/s]Training epoch 1:   8% 1127/13868 [13:48<2:35:22,  1.37it/s]Training epoch 1:   8% 1128/13868 [13:49<2:35:40,  1.36it/s]Training epoch 1:   8% 1129/13868 [13:49<2:34:40,  1.37it/s]Training epoch 1:   8% 1130/13868 [13:50<2:36:01,  1.36it/s]Training epoch 1:   8% 1131/13868 [13:51<2:36:33,  1.36it/s]Training epoch 1:   8% 1132/13868 [13:51<2:34:40,  1.37it/s]Training epoch 1:   8% 1133/13868 [13:52<2:34:44,  1.37it/s]Training epoch 1:   8% 1134/13868 [13:53<2:36:05,  1.36it/s]Training epoch 1:   8% 1135/13868 [13:54<2:34:44,  1.37it/s]Training epoch 1:   8% 1136/13868 [13:54<2:36:56,  1.35it/s]Training epoch 1:   8% 1137/13868 [13:55<2:37:17,  1.35it/s]Training epoch 1:   8% 1138/13868 [13:56<2:35:40,  1.36it/s]Training epoch 1:   8% 1139/13868 [13:57<2:34:31,  1.37it/s]Training epoch 1:   8% 1140/13868 [13:57<2:34:55,  1.37it/s]Training epoch 1:   8% 1141/13868 [13:58<2:35:14,  1.37it/s]Training epoch 1:   8% 1142/13868 [13:59<2:35:19,  1.37it/s]Training epoch 1:   8% 1143/13868 [14:00<2:36:13,  1.36it/s]Training epoch 1:   8% 1144/13868 [14:00<2:34:36,  1.37it/s]Training epoch 1:   8% 1145/13868 [14:01<2:36:08,  1.36it/s]Training epoch 1:   8% 1146/13868 [14:02<2:36:26,  1.36it/s]Training epoch 1:   8% 1147/13868 [14:02<2:37:06,  1.35it/s]Training epoch 1:   8% 1148/13868 [14:03<2:37:15,  1.35it/s]Training epoch 1:   8% 1149/13868 [14:04<2:35:53,  1.36it/s]Training epoch 1:   8% 1150/13868 [14:05<2:35:21,  1.36it/s]Training epoch 1:   8% 1151/13868 [14:05<2:36:11,  1.36it/s]Training epoch 1:   8% 1152/13868 [14:06<2:36:39,  1.35it/s]Training epoch 1:   8% 1153/13868 [14:07<2:35:41,  1.36it/s]Training epoch 1:   8% 1154/13868 [14:08<2:34:11,  1.37it/s]Training epoch 1:   8% 1155/13868 [14:08<2:34:00,  1.38it/s]Training epoch 1:   8% 1156/13868 [14:09<2:34:52,  1.37it/s]Training epoch 1:   8% 1157/13868 [14:10<2:38:53,  1.33it/s]Training epoch 1:   8% 1158/13868 [14:11<2:36:11,  1.36it/s]Training epoch 1:   8% 1159/13868 [14:11<2:35:48,  1.36it/s]Training epoch 1:   8% 1160/13868 [14:12<2:36:50,  1.35it/s]Training epoch 1:   8% 1161/13868 [14:13<2:37:32,  1.34it/s]Training epoch 1:   8% 1162/13868 [14:14<2:38:16,  1.34it/s]Training epoch 1:   8% 1163/13868 [14:14<2:37:58,  1.34it/s]Training epoch 1:   8% 1164/13868 [14:15<2:38:48,  1.33it/s]Training epoch 1:   8% 1165/13868 [14:16<2:39:00,  1.33it/s]Training epoch 1:   8% 1166/13868 [14:17<2:37:12,  1.35it/s]Training epoch 1:   8% 1167/13868 [14:17<2:36:55,  1.35it/s]Training epoch 1:   8% 1168/13868 [14:18<2:37:00,  1.35it/s]Training epoch 1:   8% 1169/13868 [14:19<2:36:51,  1.35it/s]Training epoch 1:   8% 1170/13868 [14:20<2:36:15,  1.35it/s]Training epoch 1:   8% 1171/13868 [14:20<2:35:29,  1.36it/s]Training epoch 1:   8% 1172/13868 [14:21<2:35:06,  1.36it/s]Training epoch 1:   8% 1173/13868 [14:22<2:34:08,  1.37it/s]Training epoch 1:   8% 1174/13868 [14:22<2:33:17,  1.38it/s]Training epoch 1:   8% 1175/13868 [14:23<2:33:37,  1.38it/s]Training epoch 1:   8% 1176/13868 [14:24<2:35:08,  1.36it/s]Training epoch 1:   8% 1177/13868 [14:25<2:34:03,  1.37it/s]Training epoch 1:   8% 1178/13868 [14:25<2:32:09,  1.39it/s]Training epoch 1:   9% 1179/13868 [14:26<2:31:26,  1.40it/s]Training epoch 1:   9% 1180/13868 [14:27<2:31:06,  1.40it/s]Training epoch 1:   9% 1181/13868 [14:27<2:34:03,  1.37it/s]Training epoch 1:   9% 1182/13868 [14:28<2:33:33,  1.38it/s]Training epoch 1:   9% 1183/13868 [14:29<2:36:14,  1.35it/s]Training epoch 1:   9% 1184/13868 [14:30<2:34:41,  1.37it/s]Training epoch 1:   9% 1185/13868 [14:30<2:36:51,  1.35it/s]Training epoch 1:   9% 1186/13868 [14:31<2:34:32,  1.37it/s]Training epoch 1:   9% 1187/13868 [14:32<2:35:52,  1.36it/s]Training epoch 1:   9% 1188/13868 [14:33<2:34:53,  1.36it/s]Training epoch 1:   9% 1189/13868 [14:33<2:34:35,  1.37it/s]Training epoch 1:   9% 1190/13868 [14:34<2:33:16,  1.38it/s]Training epoch 1:   9% 1191/13868 [14:35<2:35:57,  1.35it/s]Training epoch 1:   9% 1192/13868 [14:36<2:35:22,  1.36it/s]Training epoch 1:   9% 1193/13868 [14:36<2:35:04,  1.36it/s]Training epoch 1:   9% 1194/13868 [14:37<2:34:56,  1.36it/s]Training epoch 1:   9% 1195/13868 [14:38<2:35:32,  1.36it/s]Training epoch 1:   9% 1196/13868 [14:38<2:33:40,  1.37it/s]Training epoch 1:   9% 1197/13868 [14:39<2:34:08,  1.37it/s]Training epoch 1:   9% 1198/13868 [14:40<2:33:47,  1.37it/s]Training epoch 1:   9% 1199/13868 [14:41<2:33:07,  1.38it/s]Training epoch 1:   9% 1200/13868 [14:42<2:45:43,  1.27it/s]Training epoch 1:   9% 1201/13868 [14:42<2:42:59,  1.30it/s]Training epoch 1:   9% 1202/13868 [14:43<2:40:58,  1.31it/s]Training epoch 1:   9% 1203/13868 [14:44<2:38:23,  1.33it/s]Training epoch 1:   9% 1204/13868 [14:45<2:39:01,  1.33it/s]Training epoch 1:   9% 1205/13868 [14:45<2:36:09,  1.35it/s]Training epoch 1:   9% 1206/13868 [14:46<2:36:48,  1.35it/s]Training epoch 1:   9% 1207/13868 [14:47<2:36:52,  1.35it/s]Training epoch 1:   9% 1208/13868 [14:47<2:34:39,  1.36it/s]Training epoch 1:   9% 1209/13868 [14:48<2:32:52,  1.38it/s]Training epoch 1:   9% 1210/13868 [14:49<2:33:35,  1.37it/s]Training epoch 1:   9% 1211/13868 [14:50<2:33:29,  1.37it/s]Training epoch 1:   9% 1212/13868 [14:50<2:32:47,  1.38it/s]Training epoch 1:   9% 1213/13868 [14:51<2:33:39,  1.37it/s]Training epoch 1:   9% 1214/13868 [14:52<2:35:27,  1.36it/s]Training epoch 1:   9% 1215/13868 [14:53<2:36:42,  1.35it/s]Training epoch 1:   9% 1216/13868 [14:53<2:36:39,  1.35it/s]Training epoch 1:   9% 1217/13868 [14:54<2:36:07,  1.35it/s]Training epoch 1:   9% 1218/13868 [14:55<2:37:12,  1.34it/s]Training epoch 1:   9% 1219/13868 [14:56<2:37:12,  1.34it/s]Training epoch 1:   9% 1220/13868 [14:56<2:37:31,  1.34it/s]Training epoch 1:   9% 1221/13868 [14:57<2:36:49,  1.34it/s]Training epoch 1:   9% 1222/13868 [14:58<2:36:39,  1.35it/s]Training epoch 1:   9% 1223/13868 [14:59<2:35:28,  1.36it/s]Training epoch 1:   9% 1224/13868 [14:59<2:33:32,  1.37it/s]Training epoch 1:   9% 1225/13868 [15:00<2:33:49,  1.37it/s]Training epoch 1:   9% 1226/13868 [15:01<2:33:30,  1.37it/s]Training epoch 1:   9% 1227/13868 [15:01<2:34:42,  1.36it/s]Training epoch 1:   9% 1228/13868 [15:02<2:33:01,  1.38it/s]Training epoch 1:   9% 1229/13868 [15:03<2:34:00,  1.37it/s]Training epoch 1:   9% 1230/13868 [15:04<2:32:06,  1.38it/s]Training epoch 1:   9% 1231/13868 [15:04<2:32:51,  1.38it/s]Training epoch 1:   9% 1232/13868 [15:05<2:34:45,  1.36it/s]Training epoch 1:   9% 1233/13868 [15:06<2:35:26,  1.35it/s]Training epoch 1:   9% 1234/13868 [15:07<2:35:12,  1.36it/s]Training epoch 1:   9% 1235/13868 [15:07<2:34:02,  1.37it/s]Training epoch 1:   9% 1236/13868 [15:08<2:32:24,  1.38it/s]Training epoch 1:   9% 1237/13868 [15:09<2:33:37,  1.37it/s]Training epoch 1:   9% 1238/13868 [15:09<2:36:10,  1.35it/s]Training epoch 1:   9% 1239/13868 [15:10<2:35:18,  1.36it/s]Training epoch 1:   9% 1240/13868 [15:11<2:34:21,  1.36it/s]Training epoch 1:   9% 1241/13868 [15:12<2:34:28,  1.36it/s]Training epoch 1:   9% 1242/13868 [15:12<2:34:39,  1.36it/s]Training epoch 1:   9% 1243/13868 [15:13<2:34:41,  1.36it/s]Training epoch 1:   9% 1244/13868 [15:14<2:32:58,  1.38it/s]Training epoch 1:   9% 1245/13868 [15:15<2:34:59,  1.36it/s]Training epoch 1:   9% 1246/13868 [15:15<2:35:24,  1.35it/s]Training epoch 1:   9% 1247/13868 [15:16<2:32:54,  1.38it/s]Training epoch 1:   9% 1248/13868 [15:17<2:31:43,  1.39it/s]Training epoch 1:   9% 1249/13868 [15:18<2:32:22,  1.38it/s]Training epoch 1:   9% 1250/13868 [15:18<2:32:06,  1.38it/s]Training epoch 1:   9% 1251/13868 [15:19<2:33:43,  1.37it/s]Training epoch 1:   9% 1252/13868 [15:20<2:33:38,  1.37it/s]Training epoch 1:   9% 1253/13868 [15:20<2:34:09,  1.36it/s]Training epoch 1:   9% 1254/13868 [15:21<2:33:15,  1.37it/s]Training epoch 1:   9% 1255/13868 [15:22<2:34:57,  1.36it/s]Training epoch 1:   9% 1256/13868 [15:23<2:34:57,  1.36it/s]Training epoch 1:   9% 1257/13868 [15:23<2:34:44,  1.36it/s]Training epoch 1:   9% 1258/13868 [15:24<2:32:40,  1.38it/s]Training epoch 1:   9% 1259/13868 [15:25<2:29:45,  1.40it/s]Training epoch 1:   9% 1260/13868 [15:26<2:30:56,  1.39it/s]Training epoch 1:   9% 1261/13868 [15:26<2:31:30,  1.39it/s]Training epoch 1:   9% 1262/13868 [15:27<2:34:30,  1.36it/s]Training epoch 1:   9% 1263/13868 [15:28<2:35:11,  1.35it/s]Training epoch 1:   9% 1264/13868 [15:28<2:34:06,  1.36it/s]Training epoch 1:   9% 1265/13868 [15:29<2:34:50,  1.36it/s]Training epoch 1:   9% 1266/13868 [15:30<2:35:13,  1.35it/s]Training epoch 1:   9% 1267/13868 [15:31<2:34:38,  1.36it/s]Training epoch 1:   9% 1268/13868 [15:31<2:33:53,  1.36it/s]Training epoch 1:   9% 1269/13868 [15:32<2:34:18,  1.36it/s]Training epoch 1:   9% 1270/13868 [15:33<2:33:52,  1.36it/s]Training epoch 1:   9% 1271/13868 [15:34<2:33:27,  1.37it/s]Training epoch 1:   9% 1272/13868 [15:34<2:34:47,  1.36it/s]Training epoch 1:   9% 1273/13868 [15:35<2:33:52,  1.36it/s]Training epoch 1:   9% 1274/13868 [15:36<2:34:22,  1.36it/s]Training epoch 1:   9% 1275/13868 [15:37<2:35:13,  1.35it/s]Training epoch 1:   9% 1276/13868 [15:37<2:34:25,  1.36it/s]Training epoch 1:   9% 1277/13868 [15:38<2:35:20,  1.35it/s]Training epoch 1:   9% 1278/13868 [15:39<2:34:39,  1.36it/s]Training epoch 1:   9% 1279/13868 [15:40<2:33:39,  1.37it/s]Training epoch 1:   9% 1280/13868 [15:40<2:34:38,  1.36it/s]Training epoch 1:   9% 1281/13868 [15:41<2:33:08,  1.37it/s]Training epoch 1:   9% 1282/13868 [15:42<2:33:42,  1.36it/s]Training epoch 1:   9% 1283/13868 [15:42<2:31:47,  1.38it/s]Training epoch 1:   9% 1284/13868 [15:43<2:31:23,  1.39it/s]Training epoch 1:   9% 1285/13868 [15:44<2:29:07,  1.41it/s]Training epoch 1:   9% 1286/13868 [15:45<2:32:49,  1.37it/s]Training epoch 1:   9% 1287/13868 [15:45<2:32:40,  1.37it/s]Training epoch 1:   9% 1288/13868 [15:46<2:34:03,  1.36it/s]Training epoch 1:   9% 1289/13868 [15:47<2:34:57,  1.35it/s]Training epoch 1:   9% 1290/13868 [15:48<2:34:39,  1.36it/s]Training epoch 1:   9% 1291/13868 [15:48<2:34:58,  1.35it/s]Training epoch 1:   9% 1292/13868 [15:49<2:33:41,  1.36it/s]Training epoch 1:   9% 1293/13868 [15:50<2:34:03,  1.36it/s]Training epoch 1:   9% 1294/13868 [15:50<2:32:11,  1.38it/s]Training epoch 1:   9% 1295/13868 [15:51<2:33:29,  1.37it/s]Training epoch 1:   9% 1296/13868 [15:52<2:33:22,  1.37it/s]Training epoch 1:   9% 1297/13868 [15:53<2:34:45,  1.35it/s]Training epoch 1:   9% 1298/13868 [15:53<2:32:46,  1.37it/s]Training epoch 1:   9% 1299/13868 [15:54<2:33:30,  1.36it/s]Training epoch 1:   9% 1300/13868 [15:55<2:42:02,  1.29it/s]Training epoch 1:   9% 1301/13868 [15:56<2:41:44,  1.29it/s]Training epoch 1:   9% 1302/13868 [15:57<2:40:29,  1.30it/s]Training epoch 1:   9% 1303/13868 [15:57<2:40:32,  1.30it/s]Training epoch 1:   9% 1304/13868 [15:58<2:37:16,  1.33it/s]Training epoch 1:   9% 1305/13868 [15:59<2:35:12,  1.35it/s]Training epoch 1:   9% 1306/13868 [15:59<2:34:29,  1.36it/s]Training epoch 1:   9% 1307/13868 [16:00<2:33:57,  1.36it/s]Training epoch 1:   9% 1308/13868 [16:01<2:33:21,  1.36it/s]Training epoch 1:   9% 1309/13868 [16:02<2:33:15,  1.37it/s]Training epoch 1:   9% 1310/13868 [16:02<2:33:49,  1.36it/s]Training epoch 1:   9% 1311/13868 [16:03<2:33:11,  1.37it/s]Training epoch 1:   9% 1312/13868 [16:04<2:30:39,  1.39it/s]Training epoch 1:   9% 1313/13868 [16:05<2:31:55,  1.38it/s]Training epoch 1:   9% 1314/13868 [16:05<2:29:47,  1.40it/s]Training epoch 1:   9% 1315/13868 [16:06<2:33:03,  1.37it/s]Training epoch 1:   9% 1316/13868 [16:07<2:33:10,  1.37it/s]Training epoch 1:   9% 1317/13868 [16:07<2:32:27,  1.37it/s]Training epoch 1:  10% 1318/13868 [16:08<2:32:48,  1.37it/s]Training epoch 1:  10% 1319/13868 [16:09<2:33:55,  1.36it/s]Training epoch 1:  10% 1320/13868 [16:10<2:34:23,  1.35it/s]Training epoch 1:  10% 1321/13868 [16:10<2:34:12,  1.36it/s]Training epoch 1:  10% 1322/13868 [16:11<2:33:46,  1.36it/s]Training epoch 1:  10% 1323/13868 [16:12<2:32:47,  1.37it/s]Training epoch 1:  10% 1324/13868 [16:13<2:32:58,  1.37it/s]Training epoch 1:  10% 1325/13868 [16:13<2:31:28,  1.38it/s]Training epoch 1:  10% 1326/13868 [16:14<2:30:09,  1.39it/s]Training epoch 1:  10% 1327/13868 [16:15<2:30:40,  1.39it/s]Training epoch 1:  10% 1328/13868 [16:15<2:32:39,  1.37it/s]Training epoch 1:  10% 1329/13868 [16:16<2:32:46,  1.37it/s]Training epoch 1:  10% 1330/13868 [16:17<2:31:10,  1.38it/s]Training epoch 1:  10% 1331/13868 [16:18<2:31:34,  1.38it/s]Training epoch 1:  10% 1332/13868 [16:18<2:32:27,  1.37it/s]Training epoch 1:  10% 1333/13868 [16:19<2:33:51,  1.36it/s]Training epoch 1:  10% 1334/13868 [16:20<2:32:04,  1.37it/s]Training epoch 1:  10% 1335/13868 [16:21<2:30:48,  1.39it/s]Training epoch 1:  10% 1336/13868 [16:21<2:31:24,  1.38it/s]Training epoch 1:  10% 1337/13868 [16:22<2:32:11,  1.37it/s]Training epoch 1:  10% 1338/13868 [16:23<2:31:21,  1.38it/s]Training epoch 1:  10% 1339/13868 [16:23<2:31:56,  1.37it/s]Training epoch 1:  10% 1340/13868 [16:24<2:31:26,  1.38it/s]Training epoch 1:  10% 1341/13868 [16:25<2:33:10,  1.36it/s]Training epoch 1:  10% 1342/13868 [16:26<2:33:23,  1.36it/s]Training epoch 1:  10% 1343/13868 [16:26<2:35:43,  1.34it/s]Training epoch 1:  10% 1344/13868 [16:27<2:34:41,  1.35it/s]Training epoch 1:  10% 1345/13868 [16:28<2:34:08,  1.35it/s]Training epoch 1:  10% 1346/13868 [16:29<2:31:15,  1.38it/s]Training epoch 1:  10% 1347/13868 [16:29<2:29:23,  1.40it/s]Training epoch 1:  10% 1348/13868 [16:30<2:29:39,  1.39it/s]Training epoch 1:  10% 1349/13868 [16:31<2:30:55,  1.38it/s]Training epoch 1:  10% 1350/13868 [16:31<2:28:36,  1.40it/s]Training epoch 1:  10% 1351/13868 [16:32<2:29:33,  1.39it/s]Training epoch 1:  10% 1352/13868 [16:33<2:30:34,  1.39it/s]Training epoch 1:  10% 1353/13868 [16:34<2:32:08,  1.37it/s]Training epoch 1:  10% 1354/13868 [16:34<2:30:39,  1.38it/s]Training epoch 1:  10% 1355/13868 [16:35<2:29:41,  1.39it/s]Training epoch 1:  10% 1356/13868 [16:36<2:29:47,  1.39it/s]Training epoch 1:  10% 1357/13868 [16:37<2:30:14,  1.39it/s]Training epoch 1:  10% 1358/13868 [16:37<2:31:44,  1.37it/s]Training epoch 1:  10% 1359/13868 [16:38<2:32:11,  1.37it/s]Training epoch 1:  10% 1360/13868 [16:39<2:32:15,  1.37it/s]Training epoch 1:  10% 1361/13868 [16:39<2:31:56,  1.37it/s]Training epoch 1:  10% 1362/13868 [16:40<2:31:49,  1.37it/s]Training epoch 1:  10% 1363/13868 [16:41<2:32:11,  1.37it/s]Training epoch 1:  10% 1364/13868 [16:42<2:31:49,  1.37it/s]Training epoch 1:  10% 1365/13868 [16:42<2:30:01,  1.39it/s]Training epoch 1:  10% 1366/13868 [16:43<2:29:47,  1.39it/s]Training epoch 1:  10% 1367/13868 [16:44<2:30:03,  1.39it/s]Training epoch 1:  10% 1368/13868 [16:45<2:30:40,  1.38it/s]Training epoch 1:  10% 1369/13868 [16:45<2:30:31,  1.38it/s]Training epoch 1:  10% 1370/13868 [16:46<2:29:32,  1.39it/s]Training epoch 1:  10% 1371/13868 [16:47<2:29:53,  1.39it/s]Training epoch 1:  10% 1372/13868 [16:47<2:28:24,  1.40it/s]Training epoch 1:  10% 1373/13868 [16:48<2:26:34,  1.42it/s]Training epoch 1:  10% 1374/13868 [16:49<2:27:00,  1.42it/s]Training epoch 1:  10% 1375/13868 [16:49<2:27:07,  1.42it/s]Training epoch 1:  10% 1376/13868 [16:50<2:28:45,  1.40it/s]Training epoch 1:  10% 1377/13868 [16:51<2:28:06,  1.41it/s]Training epoch 1:  10% 1378/13868 [16:52<2:29:57,  1.39it/s]Training epoch 1:  10% 1379/13868 [16:52<2:29:31,  1.39it/s]Training epoch 1:  10% 1380/13868 [16:53<2:29:08,  1.40it/s]Training epoch 1:  10% 1381/13868 [16:54<2:27:48,  1.41it/s]Training epoch 1:  10% 1382/13868 [16:54<2:28:29,  1.40it/s]Training epoch 1:  10% 1383/13868 [16:55<2:27:32,  1.41it/s]Training epoch 1:  10% 1384/13868 [16:56<2:28:12,  1.40it/s]Training epoch 1:  10% 1385/13868 [16:57<2:32:18,  1.37it/s]Training epoch 1:  10% 1386/13868 [16:57<2:32:21,  1.37it/s]Training epoch 1:  10% 1387/13868 [16:58<2:30:54,  1.38it/s]Training epoch 1:  10% 1388/13868 [16:59<2:31:00,  1.38it/s]Training epoch 1:  10% 1389/13868 [17:00<2:30:25,  1.38it/s]Training epoch 1:  10% 1390/13868 [17:00<2:28:55,  1.40it/s]Training epoch 1:  10% 1391/13868 [17:01<2:28:22,  1.40it/s]Training epoch 1:  10% 1392/13868 [17:02<2:27:58,  1.41it/s]Training epoch 1:  10% 1393/13868 [17:02<2:29:28,  1.39it/s]Training epoch 1:  10% 1394/13868 [17:03<2:28:37,  1.40it/s]Training epoch 1:  10% 1395/13868 [17:04<2:28:29,  1.40it/s]Training epoch 1:  10% 1396/13868 [17:05<2:30:31,  1.38it/s]Training epoch 1:  10% 1397/13868 [17:05<2:30:44,  1.38it/s]Training epoch 1:  10% 1398/13868 [17:06<2:32:21,  1.36it/s]Training epoch 1:  10% 1399/13868 [17:07<2:31:54,  1.37it/s]Training epoch 1:  10% 1400/13868 [17:08<2:40:39,  1.29it/s]Training epoch 1:  10% 1401/13868 [17:08<2:39:00,  1.31it/s]Training epoch 1:  10% 1402/13868 [17:09<2:34:35,  1.34it/s]Training epoch 1:  10% 1403/13868 [17:10<2:34:12,  1.35it/s]Training epoch 1:  10% 1404/13868 [17:11<2:35:26,  1.34it/s]Training epoch 1:  10% 1405/13868 [17:11<2:31:59,  1.37it/s]Training epoch 1:  10% 1406/13868 [17:12<2:31:32,  1.37it/s]Training epoch 1:  10% 1407/13868 [17:13<2:33:13,  1.36it/s]Training epoch 1:  10% 1408/13868 [17:14<2:32:30,  1.36it/s]Training epoch 1:  10% 1409/13868 [17:14<2:29:57,  1.38it/s]Training epoch 1:  10% 1410/13868 [17:15<2:29:41,  1.39it/s]Training epoch 1:  10% 1411/13868 [17:16<2:31:49,  1.37it/s]Training epoch 1:  10% 1412/13868 [17:16<2:31:28,  1.37it/s]Training epoch 1:  10% 1413/13868 [17:17<2:32:12,  1.36it/s]Training epoch 1:  10% 1414/13868 [17:18<2:31:08,  1.37it/s]Training epoch 1:  10% 1415/13868 [17:19<2:30:59,  1.37it/s]Training epoch 1:  10% 1416/13868 [17:19<2:31:10,  1.37it/s]Training epoch 1:  10% 1417/13868 [17:20<2:29:39,  1.39it/s]Training epoch 1:  10% 1418/13868 [17:21<2:29:22,  1.39it/s]Training epoch 1:  10% 1419/13868 [17:21<2:28:51,  1.39it/s]Training epoch 1:  10% 1420/13868 [17:22<2:28:18,  1.40it/s]Training epoch 1:  10% 1421/13868 [17:23<2:30:20,  1.38it/s]Training epoch 1:  10% 1422/13868 [17:24<2:31:01,  1.37it/s]Training epoch 1:  10% 1423/13868 [17:24<2:29:28,  1.39it/s]Training epoch 1:  10% 1424/13868 [17:25<2:30:09,  1.38it/s]Training epoch 1:  10% 1425/13868 [17:26<2:29:01,  1.39it/s]Training epoch 1:  10% 1426/13868 [17:26<2:28:44,  1.39it/s]Training epoch 1:  10% 1427/13868 [17:27<2:29:23,  1.39it/s]Training epoch 1:  10% 1428/13868 [17:28<2:29:00,  1.39it/s]Training epoch 1:  10% 1429/13868 [17:29<2:28:35,  1.40it/s]Training epoch 1:  10% 1430/13868 [17:29<2:29:56,  1.38it/s]Training epoch 1:  10% 1431/13868 [17:30<2:32:00,  1.36it/s]Training epoch 1:  10% 1432/13868 [17:31<2:32:05,  1.36it/s]Training epoch 1:  10% 1433/13868 [17:32<2:31:11,  1.37it/s]Training epoch 1:  10% 1434/13868 [17:32<2:31:38,  1.37it/s]Training epoch 1:  10% 1435/13868 [17:33<2:30:32,  1.38it/s]Training epoch 1:  10% 1436/13868 [17:34<2:31:21,  1.37it/s]Training epoch 1:  10% 1437/13868 [17:35<2:30:59,  1.37it/s]Training epoch 1:  10% 1438/13868 [17:35<2:28:55,  1.39it/s]Training epoch 1:  10% 1439/13868 [17:36<2:31:09,  1.37it/s]Training epoch 1:  10% 1440/13868 [17:37<2:31:39,  1.37it/s]Training epoch 1:  10% 1441/13868 [17:37<2:29:33,  1.38it/s]Training epoch 1:  10% 1442/13868 [17:38<2:28:57,  1.39it/s]Training epoch 1:  10% 1443/13868 [17:39<2:29:41,  1.38it/s]Training epoch 1:  10% 1444/13868 [17:40<2:29:16,  1.39it/s]Training epoch 1:  10% 1445/13868 [17:40<2:29:27,  1.39it/s]Training epoch 1:  10% 1446/13868 [17:41<2:29:25,  1.39it/s]Training epoch 1:  10% 1447/13868 [17:42<2:27:24,  1.40it/s]Training epoch 1:  10% 1448/13868 [17:42<2:28:18,  1.40it/s]Training epoch 1:  10% 1449/13868 [17:43<2:28:19,  1.40it/s]Training epoch 1:  10% 1450/13868 [17:44<2:31:09,  1.37it/s]Training epoch 1:  10% 1451/13868 [17:45<2:30:25,  1.38it/s]Training epoch 1:  10% 1452/13868 [17:45<2:30:13,  1.38it/s]Training epoch 1:  10% 1453/13868 [17:46<2:28:51,  1.39it/s]Training epoch 1:  10% 1454/13868 [17:47<2:29:49,  1.38it/s]Training epoch 1:  10% 1455/13868 [17:48<2:30:49,  1.37it/s]Training epoch 1:  10% 1456/13868 [17:48<2:28:13,  1.40it/s]Training epoch 1:  11% 1457/13868 [17:49<2:29:31,  1.38it/s]Training epoch 1:  11% 1458/13868 [17:50<2:29:33,  1.38it/s]Training epoch 1:  11% 1459/13868 [17:50<2:29:45,  1.38it/s]Training epoch 1:  11% 1460/13868 [17:51<2:31:48,  1.36it/s]Training epoch 1:  11% 1461/13868 [17:52<2:33:09,  1.35it/s]Training epoch 1:  11% 1462/13868 [17:53<2:32:40,  1.35it/s]Training epoch 1:  11% 1463/13868 [17:53<2:31:41,  1.36it/s]Training epoch 1:  11% 1464/13868 [17:54<2:31:13,  1.37it/s]Training epoch 1:  11% 1465/13868 [17:55<2:32:52,  1.35it/s]Training epoch 1:  11% 1466/13868 [17:56<2:31:52,  1.36it/s]Training epoch 1:  11% 1467/13868 [17:56<2:30:14,  1.38it/s]Training epoch 1:  11% 1468/13868 [17:57<2:28:48,  1.39it/s]Training epoch 1:  11% 1469/13868 [17:58<2:30:32,  1.37it/s]Training epoch 1:  11% 1470/13868 [17:58<2:31:49,  1.36it/s]Training epoch 1:  11% 1471/13868 [17:59<2:32:33,  1.35it/s]Training epoch 1:  11% 1472/13868 [18:00<2:31:36,  1.36it/s]Training epoch 1:  11% 1473/13868 [18:01<2:29:27,  1.38it/s]Training epoch 1:  11% 1474/13868 [18:01<2:28:05,  1.39it/s]Training epoch 1:  11% 1475/13868 [18:02<2:28:37,  1.39it/s]Training epoch 1:  11% 1476/13868 [18:03<2:28:15,  1.39it/s]Training epoch 1:  11% 1477/13868 [18:04<2:28:35,  1.39it/s]Training epoch 1:  11% 1478/13868 [18:04<2:29:16,  1.38it/s]Training epoch 1:  11% 1479/13868 [18:05<2:29:25,  1.38it/s]Training epoch 1:  11% 1480/13868 [18:06<2:30:21,  1.37it/s]Training epoch 1:  11% 1481/13868 [18:06<2:29:42,  1.38it/s]Training epoch 1:  11% 1482/13868 [18:07<2:30:01,  1.38it/s]Training epoch 1:  11% 1483/13868 [18:08<2:30:42,  1.37it/s]Training epoch 1:  11% 1484/13868 [18:09<2:31:22,  1.36it/s]Training epoch 1:  11% 1485/13868 [18:09<2:30:40,  1.37it/s]Training epoch 1:  11% 1486/13868 [18:10<2:33:13,  1.35it/s]Training epoch 1:  11% 1487/13868 [18:11<2:32:42,  1.35it/s]Training epoch 1:  11% 1488/13868 [18:12<2:31:00,  1.37it/s]Training epoch 1:  11% 1489/13868 [18:12<2:31:29,  1.36it/s]Training epoch 1:  11% 1490/13868 [18:13<2:30:22,  1.37it/s]Training epoch 1:  11% 1491/13868 [18:14<2:28:46,  1.39it/s]Training epoch 1:  11% 1492/13868 [18:14<2:29:52,  1.38it/s]Training epoch 1:  11% 1493/13868 [18:15<2:29:46,  1.38it/s]Training epoch 1:  11% 1494/13868 [18:16<2:28:30,  1.39it/s]Training epoch 1:  11% 1495/13868 [18:17<2:28:12,  1.39it/s]Training epoch 1:  11% 1496/13868 [18:17<2:29:19,  1.38it/s]Training epoch 1:  11% 1497/13868 [18:18<2:31:02,  1.37it/s]Training epoch 1:  11% 1498/13868 [18:19<2:30:23,  1.37it/s]Training epoch 1:  11% 1499/13868 [18:20<2:28:44,  1.39it/s]Training epoch 1:  11% 1500/13868 [18:20<2:37:05,  1.31it/s]Training epoch 1:  11% 1501/13868 [18:21<2:35:07,  1.33it/s]Training epoch 1:  11% 1502/13868 [18:22<2:33:05,  1.35it/s]Training epoch 1:  11% 1503/13868 [18:23<2:29:42,  1.38it/s]Training epoch 1:  11% 1504/13868 [18:23<2:29:41,  1.38it/s]Training epoch 1:  11% 1505/13868 [18:24<2:30:13,  1.37it/s]Training epoch 1:  11% 1506/13868 [18:25<2:31:08,  1.36it/s]Training epoch 1:  11% 1507/13868 [18:25<2:29:25,  1.38it/s]Training epoch 1:  11% 1508/13868 [18:26<2:27:54,  1.39it/s]Training epoch 1:  11% 1509/13868 [18:27<2:27:31,  1.40it/s]Training epoch 1:  11% 1510/13868 [18:28<2:28:14,  1.39it/s]Training epoch 1:  11% 1511/13868 [18:28<2:28:51,  1.38it/s]Training epoch 1:  11% 1512/13868 [18:29<2:29:35,  1.38it/s]Training epoch 1:  11% 1513/13868 [18:30<2:28:04,  1.39it/s]Training epoch 1:  11% 1514/13868 [18:30<2:28:29,  1.39it/s]Training epoch 1:  11% 1515/13868 [18:31<2:30:32,  1.37it/s]Training epoch 1:  11% 1516/13868 [18:32<2:30:16,  1.37it/s]Training epoch 1:  11% 1517/13868 [18:33<2:28:50,  1.38it/s]Training epoch 1:  11% 1518/13868 [18:33<2:28:52,  1.38it/s]Training epoch 1:  11% 1519/13868 [18:34<2:27:13,  1.40it/s]Training epoch 1:  11% 1520/13868 [18:35<2:25:38,  1.41it/s]Training epoch 1:  11% 1521/13868 [18:35<2:25:24,  1.42it/s]Training epoch 1:  11% 1522/13868 [18:36<2:24:40,  1.42it/s]Training epoch 1:  11% 1523/13868 [18:37<2:24:04,  1.43it/s]Training epoch 1:  11% 1524/13868 [18:38<2:25:20,  1.42it/s]Training epoch 1:  11% 1525/13868 [18:38<2:24:45,  1.42it/s]Training epoch 1:  11% 1526/13868 [18:39<2:27:32,  1.39it/s]Training epoch 1:  11% 1527/13868 [18:40<2:28:31,  1.38it/s]Training epoch 1:  11% 1528/13868 [18:41<2:28:20,  1.39it/s]Training epoch 1:  11% 1529/13868 [18:41<2:27:56,  1.39it/s]Training epoch 1:  11% 1530/13868 [18:42<2:26:55,  1.40it/s]Training epoch 1:  11% 1531/13868 [18:43<2:26:40,  1.40it/s]Training epoch 1:  11% 1532/13868 [18:43<2:25:07,  1.42it/s]Training epoch 1:  11% 1533/13868 [18:44<2:25:10,  1.42it/s]Training epoch 1:  11% 1534/13868 [18:45<2:26:16,  1.41it/s]Training epoch 1:  11% 1535/13868 [18:45<2:26:35,  1.40it/s]Training epoch 1:  11% 1536/13868 [18:46<2:28:23,  1.39it/s]Training epoch 1:  11% 1537/13868 [18:47<2:27:15,  1.40it/s]Training epoch 1:  11% 1538/13868 [18:48<2:26:42,  1.40it/s]Training epoch 1:  11% 1539/13868 [18:48<2:28:02,  1.39it/s]Training epoch 1:  11% 1540/13868 [18:49<2:29:11,  1.38it/s]Training epoch 1:  11% 1541/13868 [18:50<2:29:52,  1.37it/s]Training epoch 1:  11% 1542/13868 [18:51<2:28:12,  1.39it/s]Training epoch 1:  11% 1543/13868 [18:51<2:27:19,  1.39it/s]Training epoch 1:  11% 1544/13868 [18:52<2:29:17,  1.38it/s]Training epoch 1:  11% 1545/13868 [18:53<2:29:43,  1.37it/s]Training epoch 1:  11% 1546/13868 [18:53<2:28:51,  1.38it/s]Training epoch 1:  11% 1547/13868 [18:54<2:27:10,  1.40it/s]Training epoch 1:  11% 1548/13868 [18:55<2:27:53,  1.39it/s]Training epoch 1:  11% 1549/13868 [18:56<2:29:36,  1.37it/s]Training epoch 1:  11% 1550/13868 [18:56<2:28:18,  1.38it/s]Training epoch 1:  11% 1551/13868 [18:57<2:28:11,  1.39it/s]Training epoch 1:  11% 1552/13868 [18:58<2:29:45,  1.37it/s]Training epoch 1:  11% 1553/13868 [18:59<2:29:16,  1.37it/s]Training epoch 1:  11% 1554/13868 [18:59<2:30:09,  1.37it/s]Training epoch 1:  11% 1555/13868 [19:00<2:31:30,  1.35it/s]Training epoch 1:  11% 1556/13868 [19:01<2:29:51,  1.37it/s]Training epoch 1:  11% 1557/13868 [19:01<2:30:33,  1.36it/s]Training epoch 1:  11% 1558/13868 [19:02<2:31:37,  1.35it/s]Training epoch 1:  11% 1559/13868 [19:03<2:30:30,  1.36it/s]Training epoch 1:  11% 1560/13868 [19:04<2:30:10,  1.37it/s]Training epoch 1:  11% 1561/13868 [19:04<2:29:26,  1.37it/s]Training epoch 1:  11% 1562/13868 [19:05<2:29:21,  1.37it/s]Training epoch 1:  11% 1563/13868 [19:06<2:29:51,  1.37it/s]Training epoch 1:  11% 1564/13868 [19:07<2:29:26,  1.37it/s]Training epoch 1:  11% 1565/13868 [19:07<2:28:33,  1.38it/s]Training epoch 1:  11% 1566/13868 [19:08<2:27:11,  1.39it/s]Training epoch 1:  11% 1567/13868 [19:09<2:27:18,  1.39it/s]Training epoch 1:  11% 1568/13868 [19:09<2:28:21,  1.38it/s]Training epoch 1:  11% 1569/13868 [19:10<2:26:54,  1.40it/s]Training epoch 1:  11% 1570/13868 [19:11<2:29:57,  1.37it/s]Training epoch 1:  11% 1571/13868 [19:12<2:28:52,  1.38it/s]Training epoch 1:  11% 1572/13868 [19:12<2:28:13,  1.38it/s]Training epoch 1:  11% 1573/13868 [19:13<2:27:56,  1.39it/s]Training epoch 1:  11% 1574/13868 [19:14<2:27:58,  1.38it/s]Training epoch 1:  11% 1575/13868 [19:15<2:28:53,  1.38it/s]Training epoch 1:  11% 1576/13868 [19:15<2:30:27,  1.36it/s]Training epoch 1:  11% 1577/13868 [19:16<2:28:19,  1.38it/s]Training epoch 1:  11% 1578/13868 [19:17<2:29:09,  1.37it/s]Training epoch 1:  11% 1579/13868 [19:17<2:28:55,  1.38it/s]Training epoch 1:  11% 1580/13868 [19:18<2:28:48,  1.38it/s]Training epoch 1:  11% 1581/13868 [19:19<2:28:19,  1.38it/s]Training epoch 1:  11% 1582/13868 [19:20<2:28:36,  1.38it/s]Training epoch 1:  11% 1583/13868 [19:20<2:26:07,  1.40it/s]Training epoch 1:  11% 1584/13868 [19:21<2:27:13,  1.39it/s]Training epoch 1:  11% 1585/13868 [19:22<2:27:11,  1.39it/s]Training epoch 1:  11% 1586/13868 [19:22<2:28:53,  1.37it/s]Training epoch 1:  11% 1587/13868 [19:23<2:30:21,  1.36it/s]Training epoch 1:  11% 1588/13868 [19:24<2:29:05,  1.37it/s]Training epoch 1:  11% 1589/13868 [19:25<2:28:46,  1.38it/s]Training epoch 1:  11% 1590/13868 [19:25<2:28:26,  1.38it/s]Training epoch 1:  11% 1591/13868 [19:26<2:29:54,  1.36it/s]Training epoch 1:  11% 1592/13868 [19:27<2:29:29,  1.37it/s]Training epoch 1:  11% 1593/13868 [19:28<2:28:51,  1.37it/s]Training epoch 1:  11% 1594/13868 [19:28<2:30:44,  1.36it/s]Training epoch 1:  12% 1595/13868 [19:29<2:30:29,  1.36it/s]Training epoch 1:  12% 1596/13868 [19:30<2:31:17,  1.35it/s]Training epoch 1:  12% 1597/13868 [19:31<2:29:58,  1.36it/s]Training epoch 1:  12% 1598/13868 [19:31<2:28:54,  1.37it/s]Training epoch 1:  12% 1599/13868 [19:32<2:29:07,  1.37it/s]Training epoch 1:  12% 1600/13868 [19:33<2:40:02,  1.28it/s]Training epoch 1:  12% 1601/13868 [19:34<2:37:05,  1.30it/s]Training epoch 1:  12% 1602/13868 [19:34<2:33:47,  1.33it/s]Training epoch 1:  12% 1603/13868 [19:35<2:34:01,  1.33it/s]Training epoch 1:  12% 1604/13868 [19:36<2:33:01,  1.34it/s]Training epoch 1:  12% 1605/13868 [19:37<2:31:58,  1.34it/s]Training epoch 1:  12% 1606/13868 [19:37<2:30:04,  1.36it/s]Training epoch 1:  12% 1607/13868 [19:38<2:32:11,  1.34it/s]Training epoch 1:  12% 1608/13868 [19:39<2:31:55,  1.34it/s]Training epoch 1:  12% 1609/13868 [19:40<2:32:29,  1.34it/s]Training epoch 1:  12% 1610/13868 [19:40<2:32:15,  1.34it/s]Training epoch 1:  12% 1611/13868 [19:41<2:33:35,  1.33it/s]Training epoch 1:  12% 1612/13868 [19:42<2:32:08,  1.34it/s]Training epoch 1:  12% 1613/13868 [19:43<2:32:01,  1.34it/s]Training epoch 1:  12% 1614/13868 [19:43<2:30:20,  1.36it/s]Training epoch 1:  12% 1615/13868 [19:44<2:33:16,  1.33it/s]Training epoch 1:  12% 1616/13868 [19:45<2:32:50,  1.34it/s]Training epoch 1:  12% 1617/13868 [19:46<2:31:26,  1.35it/s]Training epoch 1:  12% 1618/13868 [19:46<2:30:28,  1.36it/s]Training epoch 1:  12% 1619/13868 [19:47<2:30:50,  1.35it/s]Training epoch 1:  12% 1620/13868 [19:48<2:30:09,  1.36it/s]Training epoch 1:  12% 1621/13868 [19:48<2:30:36,  1.36it/s]Training epoch 1:  12% 1622/13868 [19:49<2:29:46,  1.36it/s]Training epoch 1:  12% 1623/13868 [19:50<2:31:26,  1.35it/s]Training epoch 1:  12% 1624/13868 [19:51<2:31:58,  1.34it/s]Training epoch 1:  12% 1625/13868 [19:51<2:30:59,  1.35it/s]Training epoch 1:  12% 1626/13868 [19:52<2:32:53,  1.33it/s]Training epoch 1:  12% 1627/13868 [19:53<2:32:41,  1.34it/s]Training epoch 1:  12% 1628/13868 [19:54<2:32:03,  1.34it/s]Training epoch 1:  12% 1629/13868 [19:54<2:31:46,  1.34it/s]Training epoch 1:  12% 1630/13868 [19:55<2:32:00,  1.34it/s]Training epoch 1:  12% 1631/13868 [19:56<2:31:39,  1.34it/s]Training epoch 1:  12% 1632/13868 [19:57<2:32:02,  1.34it/s]Training epoch 1:  12% 1633/13868 [19:57<2:32:23,  1.34it/s]Training epoch 1:  12% 1634/13868 [19:58<2:33:00,  1.33it/s]Training epoch 1:  12% 1635/13868 [19:59<2:31:56,  1.34it/s]Training epoch 1:  12% 1636/13868 [20:00<2:30:12,  1.36it/s]Training epoch 1:  12% 1637/13868 [20:00<2:30:35,  1.35it/s]Training epoch 1:  12% 1638/13868 [20:01<2:33:32,  1.33it/s]Training epoch 1:  12% 1639/13868 [20:02<2:34:33,  1.32it/s]Training epoch 1:  12% 1640/13868 [20:03<2:33:03,  1.33it/s]Training epoch 1:  12% 1641/13868 [20:03<2:35:55,  1.31it/s]Training epoch 1:  12% 1642/13868 [20:04<2:37:29,  1.29it/s]Training epoch 1:  12% 1643/13868 [20:05<2:35:08,  1.31it/s]Training epoch 1:  12% 1644/13868 [20:06<2:34:27,  1.32it/s]Training epoch 1:  12% 1645/13868 [20:06<2:32:51,  1.33it/s]Training epoch 1:  12% 1646/13868 [20:07<2:32:27,  1.34it/s]Training epoch 1:  12% 1647/13868 [20:08<2:33:16,  1.33it/s]Training epoch 1:  12% 1648/13868 [20:09<2:33:52,  1.32it/s]Training epoch 1:  12% 1649/13868 [20:09<2:32:50,  1.33it/s]Training epoch 1:  12% 1650/13868 [20:10<2:32:15,  1.34it/s]Training epoch 1:  12% 1651/13868 [20:11<2:29:32,  1.36it/s]Training epoch 1:  12% 1652/13868 [20:12<2:32:07,  1.34it/s]Training epoch 1:  12% 1653/13868 [20:12<2:31:44,  1.34it/s]Training epoch 1:  12% 1654/13868 [20:13<2:31:33,  1.34it/s]Training epoch 1:  12% 1655/13868 [20:14<2:31:51,  1.34it/s]Training epoch 1:  12% 1656/13868 [20:15<2:30:48,  1.35it/s]Training epoch 1:  12% 1657/13868 [20:15<2:30:20,  1.35it/s]Training epoch 1:  12% 1658/13868 [20:16<2:29:08,  1.36it/s]Training epoch 1:  12% 1659/13868 [20:17<2:29:14,  1.36it/s]Training epoch 1:  12% 1660/13868 [20:18<2:30:47,  1.35it/s]Training epoch 1:  12% 1661/13868 [20:18<2:33:09,  1.33it/s]Training epoch 1:  12% 1662/13868 [20:19<2:35:51,  1.31it/s]Training epoch 1:  12% 1663/13868 [20:20<2:34:29,  1.32it/s]Training epoch 1:  12% 1664/13868 [20:21<2:32:05,  1.34it/s]Training epoch 1:  12% 1665/13868 [20:21<2:31:37,  1.34it/s]Training epoch 1:  12% 1666/13868 [20:22<2:31:22,  1.34it/s]Training epoch 1:  12% 1667/13868 [20:23<2:32:20,  1.33it/s]Training epoch 1:  12% 1668/13868 [20:24<2:33:11,  1.33it/s]Training epoch 1:  12% 1669/13868 [20:24<2:34:09,  1.32it/s]Training epoch 1:  12% 1670/13868 [20:25<2:34:51,  1.31it/s]Training epoch 1:  12% 1671/13868 [20:26<2:33:45,  1.32it/s]Training epoch 1:  12% 1672/13868 [20:27<2:35:07,  1.31it/s]Training epoch 1:  12% 1673/13868 [20:27<2:35:46,  1.30it/s]Training epoch 1:  12% 1674/13868 [20:28<2:36:43,  1.30it/s]Training epoch 1:  12% 1675/13868 [20:29<2:35:58,  1.30it/s]Training epoch 1:  12% 1676/13868 [20:30<2:35:05,  1.31it/s]Training epoch 1:  12% 1677/13868 [20:31<2:37:38,  1.29it/s]Training epoch 1:  12% 1678/13868 [20:31<2:37:01,  1.29it/s]Training epoch 1:  12% 1679/13868 [20:32<2:35:47,  1.30it/s]Training epoch 1:  12% 1680/13868 [20:33<2:33:24,  1.32it/s]Training epoch 1:  12% 1681/13868 [20:34<2:33:11,  1.33it/s]Training epoch 1:  12% 1682/13868 [20:34<2:36:06,  1.30it/s]Training epoch 1:  12% 1683/13868 [20:35<2:33:56,  1.32it/s]Training epoch 1:  12% 1684/13868 [20:36<2:33:21,  1.32it/s]Training epoch 1:  12% 1685/13868 [20:37<2:33:43,  1.32it/s]Training epoch 1:  12% 1686/13868 [20:37<2:32:15,  1.33it/s]Training epoch 1:  12% 1687/13868 [20:38<2:30:53,  1.35it/s]Training epoch 1:  12% 1688/13868 [20:39<2:29:59,  1.35it/s]Training epoch 1:  12% 1689/13868 [20:40<2:32:58,  1.33it/s]Training epoch 1:  12% 1690/13868 [20:40<2:32:38,  1.33it/s]Training epoch 1:  12% 1691/13868 [20:41<2:33:11,  1.32it/s]Training epoch 1:  12% 1692/13868 [20:42<2:35:00,  1.31it/s]Training epoch 1:  12% 1693/13868 [20:43<2:34:31,  1.31it/s]Training epoch 1:  12% 1694/13868 [20:43<2:34:39,  1.31it/s]Training epoch 1:  12% 1695/13868 [20:44<2:33:47,  1.32it/s]Training epoch 1:  12% 1696/13868 [20:45<2:32:08,  1.33it/s]Training epoch 1:  12% 1697/13868 [20:46<2:35:07,  1.31it/s]Training epoch 1:  12% 1698/13868 [20:46<2:33:23,  1.32it/s]Training epoch 1:  12% 1699/13868 [20:47<2:34:18,  1.31it/s]Training epoch 1:  12% 1700/13868 [20:48<2:41:25,  1.26it/s]Training epoch 1:  12% 1701/13868 [20:49<2:35:31,  1.30it/s]Training epoch 1:  12% 1702/13868 [20:50<2:33:49,  1.32it/s]Training epoch 1:  12% 1703/13868 [20:50<2:32:31,  1.33it/s]Training epoch 1:  12% 1704/13868 [20:51<2:35:09,  1.31it/s]Training epoch 1:  12% 1705/13868 [20:52<2:35:21,  1.30it/s]Training epoch 1:  12% 1706/13868 [20:53<2:34:51,  1.31it/s]Training epoch 1:  12% 1707/13868 [20:53<2:36:03,  1.30it/s]Training epoch 1:  12% 1708/13868 [20:54<2:33:26,  1.32it/s]Training epoch 1:  12% 1709/13868 [20:55<2:33:53,  1.32it/s]Training epoch 1:  12% 1710/13868 [20:56<2:31:53,  1.33it/s]Training epoch 1:  12% 1711/13868 [20:56<2:32:06,  1.33it/s]Training epoch 1:  12% 1712/13868 [20:57<2:31:50,  1.33it/s]Training epoch 1:  12% 1713/13868 [20:58<2:31:54,  1.33it/s]Training epoch 1:  12% 1714/13868 [20:59<2:31:41,  1.34it/s]Training epoch 1:  12% 1715/13868 [20:59<2:32:23,  1.33it/s]Training epoch 1:  12% 1716/13868 [21:00<2:32:08,  1.33it/s]Training epoch 1:  12% 1717/13868 [21:01<2:29:33,  1.35it/s]Training epoch 1:  12% 1718/13868 [21:02<2:29:56,  1.35it/s]Training epoch 1:  12% 1719/13868 [21:02<2:29:32,  1.35it/s]Training epoch 1:  12% 1720/13868 [21:03<2:32:39,  1.33it/s]Training epoch 1:  12% 1721/13868 [21:04<2:32:14,  1.33it/s]Training epoch 1:  12% 1722/13868 [21:05<2:30:00,  1.35it/s]Training epoch 1:  12% 1723/13868 [21:05<2:30:08,  1.35it/s]Training epoch 1:  12% 1724/13868 [21:06<2:31:15,  1.34it/s]Training epoch 1:  12% 1725/13868 [21:07<2:30:22,  1.35it/s]Training epoch 1:  12% 1726/13868 [21:08<2:31:01,  1.34it/s]Training epoch 1:  12% 1727/13868 [21:08<2:32:38,  1.33it/s]Training epoch 1:  12% 1728/13868 [21:09<2:35:54,  1.30it/s]Training epoch 1:  12% 1729/13868 [21:10<2:34:52,  1.31it/s]Training epoch 1:  12% 1730/13868 [21:11<2:36:11,  1.30it/s]Training epoch 1:  12% 1731/13868 [21:11<2:37:01,  1.29it/s]Training epoch 1:  12% 1732/13868 [21:12<2:35:38,  1.30it/s]Training epoch 1:  12% 1733/13868 [21:13<2:36:20,  1.29it/s]Training epoch 1:  13% 1734/13868 [21:14<2:37:42,  1.28it/s]Training epoch 1:  13% 1735/13868 [21:15<2:36:27,  1.29it/s]Training epoch 1:  13% 1736/13868 [21:15<2:36:38,  1.29it/s]Training epoch 1:  13% 1737/13868 [21:16<2:35:19,  1.30it/s]Training epoch 1:  13% 1738/13868 [21:17<2:34:14,  1.31it/s]Training epoch 1:  13% 1739/13868 [21:18<2:34:52,  1.31it/s]Training epoch 1:  13% 1740/13868 [21:18<2:34:37,  1.31it/s]Training epoch 1:  13% 1741/13868 [21:19<2:33:12,  1.32it/s]Training epoch 1:  13% 1742/13868 [21:20<2:33:16,  1.32it/s]Training epoch 1:  13% 1743/13868 [21:21<2:33:46,  1.31it/s]Training epoch 1:  13% 1744/13868 [21:21<2:32:44,  1.32it/s]Training epoch 1:  13% 1745/13868 [21:22<2:31:59,  1.33it/s]Training epoch 1:  13% 1746/13868 [21:23<2:30:21,  1.34it/s]Training epoch 1:  13% 1747/13868 [21:24<2:29:20,  1.35it/s]Training epoch 1:  13% 1748/13868 [21:24<2:29:42,  1.35it/s]Training epoch 1:  13% 1749/13868 [21:25<2:29:49,  1.35it/s]Training epoch 1:  13% 1750/13868 [21:26<2:31:35,  1.33it/s]Training epoch 1:  13% 1751/13868 [21:27<2:28:58,  1.36it/s]Training epoch 1:  13% 1752/13868 [21:27<2:30:38,  1.34it/s]Training epoch 1:  13% 1753/13868 [21:28<2:30:57,  1.34it/s]Training epoch 1:  13% 1754/13868 [21:29<2:31:49,  1.33it/s]Training epoch 1:  13% 1755/13868 [21:30<2:30:27,  1.34it/s]Training epoch 1:  13% 1756/13868 [21:30<2:32:37,  1.32it/s]Training epoch 1:  13% 1757/13868 [21:31<2:33:07,  1.32it/s]Training epoch 1:  13% 1758/13868 [21:32<2:33:21,  1.32it/s]Training epoch 1:  13% 1759/13868 [21:33<2:32:43,  1.32it/s]Training epoch 1:  13% 1760/13868 [21:33<2:32:59,  1.32it/s]Training epoch 1:  13% 1761/13868 [21:34<2:32:20,  1.32it/s]Training epoch 1:  13% 1762/13868 [21:35<2:31:19,  1.33it/s]Training epoch 1:  13% 1763/13868 [21:36<2:31:54,  1.33it/s]Training epoch 1:  13% 1764/13868 [21:36<2:33:11,  1.32it/s]Training epoch 1:  13% 1765/13868 [21:37<2:33:01,  1.32it/s]Training epoch 1:  13% 1766/13868 [21:38<2:31:40,  1.33it/s]Training epoch 1:  13% 1767/13868 [21:39<2:28:16,  1.36it/s]Training epoch 1:  13% 1768/13868 [21:39<2:27:32,  1.37it/s]Training epoch 1:  13% 1769/13868 [21:40<2:28:16,  1.36it/s]Training epoch 1:  13% 1770/13868 [21:41<2:29:03,  1.35it/s]Training epoch 1:  13% 1771/13868 [21:42<2:29:00,  1.35it/s]Training epoch 1:  13% 1772/13868 [21:42<2:28:41,  1.36it/s]Training epoch 1:  13% 1773/13868 [21:43<2:30:34,  1.34it/s]Training epoch 1:  13% 1774/13868 [21:44<2:32:02,  1.33it/s]Training epoch 1:  13% 1775/13868 [21:45<2:32:11,  1.32it/s]Training epoch 1:  13% 1776/13868 [21:45<2:32:46,  1.32it/s]Training epoch 1:  13% 1777/13868 [21:46<2:33:26,  1.31it/s]Training epoch 1:  13% 1778/13868 [21:47<2:33:08,  1.32it/s]Training epoch 1:  13% 1779/13868 [21:48<2:35:12,  1.30it/s]Training epoch 1:  13% 1780/13868 [21:48<2:35:35,  1.29it/s]Training epoch 1:  13% 1781/13868 [21:49<2:33:26,  1.31it/s]Training epoch 1:  13% 1782/13868 [21:50<2:34:21,  1.30it/s]Training epoch 1:  13% 1783/13868 [21:51<2:34:06,  1.31it/s]Training epoch 1:  13% 1784/13868 [21:51<2:34:30,  1.30it/s]Training epoch 1:  13% 1785/13868 [21:52<2:34:26,  1.30it/s]Training epoch 1:  13% 1786/13868 [21:53<2:35:22,  1.30it/s]Training epoch 1:  13% 1787/13868 [21:54<2:32:42,  1.32it/s]Training epoch 1:  13% 1788/13868 [21:54<2:33:02,  1.32it/s]Training epoch 1:  13% 1789/13868 [21:55<2:32:50,  1.32it/s]Training epoch 1:  13% 1790/13868 [21:56<2:33:04,  1.32it/s]Training epoch 1:  13% 1791/13868 [21:57<2:32:20,  1.32it/s]Training epoch 1:  13% 1792/13868 [21:58<2:33:31,  1.31it/s]Training epoch 1:  13% 1793/13868 [21:58<2:34:29,  1.30it/s]Training epoch 1:  13% 1794/13868 [21:59<2:34:07,  1.31it/s]Training epoch 1:  13% 1795/13868 [22:00<2:33:23,  1.31it/s]Training epoch 1:  13% 1796/13868 [22:01<2:33:17,  1.31it/s]Training epoch 1:  13% 1797/13868 [22:01<2:31:39,  1.33it/s]Training epoch 1:  13% 1798/13868 [22:02<2:32:03,  1.32it/s]Training epoch 1:  13% 1799/13868 [22:03<2:33:57,  1.31it/s]Training epoch 1:  13% 1800/13868 [22:04<2:42:40,  1.24it/s]Training epoch 1:  13% 1801/13868 [22:05<2:40:37,  1.25it/s]Training epoch 1:  13% 1802/13868 [22:05<2:38:19,  1.27it/s]Training epoch 1:  13% 1803/13868 [22:06<2:36:39,  1.28it/s]Training epoch 1:  13% 1804/13868 [22:07<2:33:32,  1.31it/s]Training epoch 1:  13% 1805/13868 [22:08<2:34:01,  1.31it/s]Training epoch 1:  13% 1806/13868 [22:08<2:36:01,  1.29it/s]Training epoch 1:  13% 1807/13868 [22:09<2:35:22,  1.29it/s]Training epoch 1:  13% 1808/13868 [22:10<2:33:11,  1.31it/s]Training epoch 1:  13% 1809/13868 [22:11<2:32:38,  1.32it/s]Training epoch 1:  13% 1810/13868 [22:11<2:32:59,  1.31it/s]Training epoch 1:  13% 1811/13868 [22:12<2:32:15,  1.32it/s]Training epoch 1:  13% 1812/13868 [22:13<2:32:23,  1.32it/s]Training epoch 1:  13% 1813/13868 [22:14<2:31:24,  1.33it/s]Training epoch 1:  13% 1814/13868 [22:14<2:32:02,  1.32it/s]Training epoch 1:  13% 1815/13868 [22:15<2:32:50,  1.31it/s]Training epoch 1:  13% 1816/13868 [22:16<2:31:45,  1.32it/s]Training epoch 1:  13% 1817/13868 [22:17<2:30:48,  1.33it/s]Training epoch 1:  13% 1818/13868 [22:17<2:31:09,  1.33it/s]Training epoch 1:  13% 1819/13868 [22:18<2:31:01,  1.33it/s]Training epoch 1:  13% 1820/13868 [22:19<2:31:37,  1.32it/s]Training epoch 1:  13% 1821/13868 [22:20<2:32:06,  1.32it/s]Training epoch 1:  13% 1822/13868 [22:20<2:30:46,  1.33it/s]Training epoch 1:  13% 1823/13868 [22:21<2:31:49,  1.32it/s]Training epoch 1:  13% 1824/13868 [22:22<2:32:23,  1.32it/s]Training epoch 1:  13% 1825/13868 [22:23<2:31:54,  1.32it/s]Training epoch 1:  13% 1826/13868 [22:24<2:34:14,  1.30it/s]Training epoch 1:  13% 1827/13868 [22:24<2:32:33,  1.32it/s]Training epoch 1:  13% 1828/13868 [22:25<2:31:38,  1.32it/s]Training epoch 1:  13% 1829/13868 [22:26<2:31:13,  1.33it/s]Training epoch 1:  13% 1830/13868 [22:26<2:30:17,  1.34it/s]Training epoch 1:  13% 1831/13868 [22:27<2:29:39,  1.34it/s]Training epoch 1:  13% 1832/13868 [22:28<2:28:22,  1.35it/s]Training epoch 1:  13% 1833/13868 [22:29<2:27:39,  1.36it/s]Training epoch 1:  13% 1834/13868 [22:29<2:30:07,  1.34it/s]Training epoch 1:  13% 1835/13868 [22:30<2:29:54,  1.34it/s]Training epoch 1:  13% 1836/13868 [22:31<2:31:51,  1.32it/s]Training epoch 1:  13% 1837/13868 [22:32<2:34:36,  1.30it/s]Training epoch 1:  13% 1838/13868 [22:32<2:29:59,  1.34it/s]Training epoch 1:  13% 1839/13868 [22:33<2:30:59,  1.33it/s]Training epoch 1:  13% 1840/13868 [22:34<2:31:38,  1.32it/s]Training epoch 1:  13% 1841/13868 [22:35<2:30:44,  1.33it/s]Training epoch 1:  13% 1842/13868 [22:36<2:30:57,  1.33it/s]Training epoch 1:  13% 1843/13868 [22:36<2:32:12,  1.32it/s]Training epoch 1:  13% 1844/13868 [22:37<2:33:29,  1.31it/s]Training epoch 1:  13% 1845/13868 [22:38<2:34:50,  1.29it/s]Training epoch 1:  13% 1846/13868 [22:39<2:33:49,  1.30it/s]Training epoch 1:  13% 1847/13868 [22:39<2:33:48,  1.30it/s]Training epoch 1:  13% 1848/13868 [22:40<2:31:38,  1.32it/s]Training epoch 1:  13% 1849/13868 [22:41<2:30:56,  1.33it/s]Training epoch 1:  13% 1850/13868 [22:42<2:32:16,  1.32it/s]Training epoch 1:  13% 1851/13868 [22:42<2:30:56,  1.33it/s]Training epoch 1:  13% 1852/13868 [22:43<2:29:53,  1.34it/s]Training epoch 1:  13% 1853/13868 [22:44<2:30:51,  1.33it/s]Training epoch 1:  13% 1854/13868 [22:45<2:32:53,  1.31it/s]Training epoch 1:  13% 1855/13868 [22:45<2:31:17,  1.32it/s]Training epoch 1:  13% 1856/13868 [22:46<2:32:16,  1.31it/s]Training epoch 1:  13% 1857/13868 [22:47<2:30:26,  1.33it/s]Training epoch 1:  13% 1858/13868 [22:48<2:31:11,  1.32it/s]Training epoch 1:  13% 1859/13868 [22:48<2:30:57,  1.33it/s]Training epoch 1:  13% 1860/13868 [22:49<2:30:43,  1.33it/s]Training epoch 1:  13% 1861/13868 [22:50<2:28:49,  1.34it/s]Training epoch 1:  13% 1862/13868 [22:51<2:30:52,  1.33it/s]Training epoch 1:  13% 1863/13868 [22:51<2:31:49,  1.32it/s]Training epoch 1:  13% 1864/13868 [22:52<2:31:25,  1.32it/s]Training epoch 1:  13% 1865/13868 [22:53<2:31:39,  1.32it/s]Training epoch 1:  13% 1866/13868 [22:54<2:31:47,  1.32it/s]Training epoch 1:  13% 1867/13868 [22:54<2:32:11,  1.31it/s]Training epoch 1:  13% 1868/13868 [22:55<2:34:23,  1.30it/s]Training epoch 1:  13% 1869/13868 [22:56<2:32:14,  1.31it/s]Training epoch 1:  13% 1870/13868 [22:57<2:34:37,  1.29it/s]Training epoch 1:  13% 1871/13868 [22:58<2:34:14,  1.30it/s]Training epoch 1:  13% 1872/13868 [22:58<2:33:16,  1.30it/s]Training epoch 1:  14% 1873/13868 [22:59<2:30:55,  1.32it/s]Training epoch 1:  14% 1874/13868 [23:00<2:32:22,  1.31it/s]Training epoch 1:  14% 1875/13868 [23:01<2:33:14,  1.30it/s]Training epoch 1:  14% 1876/13868 [23:01<2:33:01,  1.31it/s]Training epoch 1:  14% 1877/13868 [23:02<2:31:35,  1.32it/s]Training epoch 1:  14% 1878/13868 [23:03<2:29:37,  1.34it/s]Training epoch 1:  14% 1879/13868 [23:04<2:29:56,  1.33it/s]Training epoch 1:  14% 1880/13868 [23:04<2:27:36,  1.35it/s]Training epoch 1:  14% 1881/13868 [23:05<2:28:59,  1.34it/s]Training epoch 1:  14% 1882/13868 [23:06<2:30:40,  1.33it/s]Training epoch 1:  14% 1883/13868 [23:07<2:28:16,  1.35it/s]Training epoch 1:  14% 1884/13868 [23:07<2:29:21,  1.34it/s]Training epoch 1:  14% 1885/13868 [23:08<2:28:31,  1.34it/s]Training epoch 1:  14% 1886/13868 [23:09<2:28:02,  1.35it/s]Training epoch 1:  14% 1887/13868 [23:10<2:28:52,  1.34it/s]Training epoch 1:  14% 1888/13868 [23:10<2:27:57,  1.35it/s]Training epoch 1:  14% 1889/13868 [23:11<2:25:52,  1.37it/s]Training epoch 1:  14% 1890/13868 [23:12<2:27:55,  1.35it/s]Training epoch 1:  14% 1891/13868 [23:13<2:28:56,  1.34it/s]Training epoch 1:  14% 1892/13868 [23:13<2:29:26,  1.34it/s]Training epoch 1:  14% 1893/13868 [23:14<2:26:26,  1.36it/s]Training epoch 1:  14% 1894/13868 [23:15<2:28:01,  1.35it/s]Training epoch 1:  14% 1895/13868 [23:15<2:26:35,  1.36it/s]Training epoch 1:  14% 1896/13868 [23:16<2:28:42,  1.34it/s]Training epoch 1:  14% 1897/13868 [23:17<2:30:31,  1.33it/s]Training epoch 1:  14% 1898/13868 [23:18<2:30:55,  1.32it/s]Training epoch 1:  14% 1899/13868 [23:18<2:29:58,  1.33it/s]Training epoch 1:  14% 1900/13868 [23:20<2:46:40,  1.20it/s]Training epoch 1:  14% 1901/13868 [23:20<2:40:00,  1.25it/s]Training epoch 1:  14% 1902/13868 [23:21<2:37:06,  1.27it/s]Training epoch 1:  14% 1903/13868 [23:22<2:32:54,  1.30it/s]Training epoch 1:  14% 1904/13868 [23:22<2:29:31,  1.33it/s]Training epoch 1:  14% 1905/13868 [23:23<2:28:40,  1.34it/s]Training epoch 1:  14% 1906/13868 [23:24<2:28:27,  1.34it/s]Training epoch 1:  14% 1907/13868 [23:25<2:27:10,  1.35it/s]Training epoch 1:  14% 1908/13868 [23:25<2:28:34,  1.34it/s]Training epoch 1:  14% 1909/13868 [23:26<2:28:25,  1.34it/s]Training epoch 1:  14% 1910/13868 [23:27<2:25:16,  1.37it/s]Training epoch 1:  14% 1911/13868 [23:28<2:26:14,  1.36it/s]Training epoch 1:  14% 1912/13868 [23:28<2:28:10,  1.34it/s]Training epoch 1:  14% 1913/13868 [23:29<2:27:34,  1.35it/s]Training epoch 1:  14% 1914/13868 [23:30<2:26:32,  1.36it/s]Training epoch 1:  14% 1915/13868 [23:31<2:27:44,  1.35it/s]Training epoch 1:  14% 1916/13868 [23:31<2:29:32,  1.33it/s]Training epoch 1:  14% 1917/13868 [23:32<2:28:29,  1.34it/s]Training epoch 1:  14% 1918/13868 [23:33<2:27:43,  1.35it/s]Training epoch 1:  14% 1919/13868 [23:34<2:27:10,  1.35it/s]Training epoch 1:  14% 1920/13868 [23:34<2:26:30,  1.36it/s]Training epoch 1:  14% 1921/13868 [23:35<2:27:23,  1.35it/s]Training epoch 1:  14% 1922/13868 [23:36<2:28:49,  1.34it/s]Training epoch 1:  14% 1923/13868 [23:36<2:27:49,  1.35it/s]Training epoch 1:  14% 1924/13868 [23:37<2:26:56,  1.35it/s]Training epoch 1:  14% 1925/13868 [23:38<2:29:22,  1.33it/s]Training epoch 1:  14% 1926/13868 [23:39<2:30:42,  1.32it/s]Training epoch 1:  14% 1927/13868 [23:40<2:30:17,  1.32it/s]Training epoch 1:  14% 1928/13868 [23:40<2:32:33,  1.30it/s]Training epoch 1:  14% 1929/13868 [23:41<2:30:50,  1.32it/s]Training epoch 1:  14% 1930/13868 [23:42<2:29:53,  1.33it/s]Training epoch 1:  14% 1931/13868 [23:43<2:29:13,  1.33it/s]Training epoch 1:  14% 1932/13868 [23:43<2:29:38,  1.33it/s]Training epoch 1:  14% 1933/13868 [23:44<2:30:08,  1.32it/s]Training epoch 1:  14% 1934/13868 [23:45<2:30:14,  1.32it/s]Training epoch 1:  14% 1935/13868 [23:46<2:28:00,  1.34it/s]Training epoch 1:  14% 1936/13868 [23:46<2:28:14,  1.34it/s]Training epoch 1:  14% 1937/13868 [23:47<2:29:06,  1.33it/s]Training epoch 1:  14% 1938/13868 [23:48<2:27:41,  1.35it/s]Training epoch 1:  14% 1939/13868 [23:49<2:28:55,  1.34it/s]Training epoch 1:  14% 1940/13868 [23:49<2:27:03,  1.35it/s]Training epoch 1:  14% 1941/13868 [23:50<2:27:10,  1.35it/s]Training epoch 1:  14% 1942/13868 [23:51<2:29:50,  1.33it/s]Training epoch 1:  14% 1943/13868 [23:52<2:29:44,  1.33it/s]Training epoch 1:  14% 1944/13868 [23:52<2:29:39,  1.33it/s]Training epoch 1:  14% 1945/13868 [23:53<2:30:41,  1.32it/s]Training epoch 1:  14% 1946/13868 [23:54<2:31:04,  1.32it/s]Training epoch 1:  14% 1947/13868 [23:55<2:30:32,  1.32it/s]Training epoch 1:  14% 1948/13868 [23:55<2:30:26,  1.32it/s]Training epoch 1:  14% 1949/13868 [23:56<2:32:50,  1.30it/s]Training epoch 1:  14% 1950/13868 [23:57<2:30:53,  1.32it/s]Training epoch 1:  14% 1951/13868 [23:58<2:29:48,  1.33it/s]Training epoch 1:  14% 1952/13868 [23:58<2:28:54,  1.33it/s]Training epoch 1:  14% 1953/13868 [23:59<2:28:57,  1.33it/s]Training epoch 1:  14% 1954/13868 [24:00<2:28:48,  1.33it/s]Training epoch 1:  14% 1955/13868 [24:01<2:28:52,  1.33it/s]Training epoch 1:  14% 1956/13868 [24:01<2:29:06,  1.33it/s]Training epoch 1:  14% 1957/13868 [24:02<2:29:25,  1.33it/s]Training epoch 1:  14% 1958/13868 [24:03<2:29:41,  1.33it/s]Training epoch 1:  14% 1959/13868 [24:04<2:27:49,  1.34it/s]Training epoch 1:  14% 1960/13868 [24:04<2:29:36,  1.33it/s]Training epoch 1:  14% 1961/13868 [24:05<2:28:01,  1.34it/s]Training epoch 1:  14% 1962/13868 [24:06<2:28:34,  1.34it/s]Training epoch 1:  14% 1963/13868 [24:07<2:28:30,  1.34it/s]Training epoch 1:  14% 1964/13868 [24:07<2:26:40,  1.35it/s]Training epoch 1:  14% 1965/13868 [24:08<2:28:24,  1.34it/s]Training epoch 1:  14% 1966/13868 [24:09<2:29:03,  1.33it/s]Training epoch 1:  14% 1967/13868 [24:10<2:27:49,  1.34it/s]Training epoch 1:  14% 1968/13868 [24:10<2:27:08,  1.35it/s]Training epoch 1:  14% 1969/13868 [24:11<2:26:15,  1.36it/s]Training epoch 1:  14% 1970/13868 [24:12<2:25:38,  1.36it/s]Training epoch 1:  14% 1971/13868 [24:12<2:22:25,  1.39it/s]Training epoch 1:  14% 1972/13868 [24:13<2:24:13,  1.37it/s]Training epoch 1:  14% 1973/13868 [24:14<2:24:26,  1.37it/s]Training epoch 1:  14% 1974/13868 [24:15<2:24:43,  1.37it/s]Training epoch 1:  14% 1975/13868 [24:15<2:25:57,  1.36it/s]Training epoch 1:  14% 1976/13868 [24:16<2:27:10,  1.35it/s]Training epoch 1:  14% 1977/13868 [24:17<2:26:38,  1.35it/s]Training epoch 1:  14% 1978/13868 [24:18<2:28:38,  1.33it/s]Training epoch 1:  14% 1979/13868 [24:18<2:27:50,  1.34it/s]Training epoch 1:  14% 1980/13868 [24:19<2:28:05,  1.34it/s]Training epoch 1:  14% 1981/13868 [24:20<2:27:55,  1.34it/s]Training epoch 1:  14% 1982/13868 [24:21<2:27:37,  1.34it/s]Training epoch 1:  14% 1983/13868 [24:21<2:27:37,  1.34it/s]Training epoch 1:  14% 1984/13868 [24:22<2:25:58,  1.36it/s]Training epoch 1:  14% 1985/13868 [24:23<2:27:11,  1.35it/s]Training epoch 1:  14% 1986/13868 [24:24<2:28:53,  1.33it/s]Training epoch 1:  14% 1987/13868 [24:24<2:29:29,  1.32it/s]Training epoch 1:  14% 1988/13868 [24:25<2:28:50,  1.33it/s]Training epoch 1:  14% 1989/13868 [24:26<2:27:22,  1.34it/s]Training epoch 1:  14% 1990/13868 [24:27<2:25:24,  1.36it/s]Training epoch 1:  14% 1991/13868 [24:27<2:25:38,  1.36it/s]Training epoch 1:  14% 1992/13868 [24:28<2:24:49,  1.37it/s]Training epoch 1:  14% 1993/13868 [24:29<2:25:00,  1.36it/s]Training epoch 1:  14% 1994/13868 [24:29<2:23:17,  1.38it/s]Training epoch 1:  14% 1995/13868 [24:30<2:24:02,  1.37it/s]Training epoch 1:  14% 1996/13868 [24:31<2:25:02,  1.36it/s]Training epoch 1:  14% 1997/13868 [24:32<2:24:15,  1.37it/s]Training epoch 1:  14% 1998/13868 [24:32<2:25:11,  1.36it/s]Training epoch 1:  14% 1999/13868 [24:33<2:27:32,  1.34it/s]Training epoch 1:  14% 2000/13868 [24:34<2:37:07,  1.26it/s]Training epoch 1:  14% 2001/13868 [24:35<2:32:50,  1.29it/s]Training epoch 1:  14% 2002/13868 [24:36<2:32:54,  1.29it/s]Training epoch 1:  14% 2003/13868 [24:36<2:30:42,  1.31it/s]Training epoch 1:  14% 2004/13868 [24:37<2:31:58,  1.30it/s]Training epoch 1:  14% 2005/13868 [24:38<2:28:56,  1.33it/s]Training epoch 1:  14% 2006/13868 [24:39<2:27:23,  1.34it/s]Training epoch 1:  14% 2007/13868 [24:39<2:26:43,  1.35it/s]Training epoch 1:  14% 2008/13868 [24:40<2:27:50,  1.34it/s]Training epoch 1:  14% 2009/13868 [24:41<2:25:54,  1.35it/s]Training epoch 1:  14% 2010/13868 [24:42<2:27:10,  1.34it/s]Training epoch 1:  15% 2011/13868 [24:42<2:28:03,  1.33it/s]Training epoch 1:  15% 2012/13868 [24:43<2:25:23,  1.36it/s]Training epoch 1:  15% 2013/13868 [24:44<2:24:50,  1.36it/s]Training epoch 1:  15% 2014/13868 [24:44<2:25:35,  1.36it/s]Training epoch 1:  15% 2015/13868 [24:45<2:25:42,  1.36it/s]Training epoch 1:  15% 2016/13868 [24:46<2:28:02,  1.33it/s]Training epoch 1:  15% 2017/13868 [24:47<2:26:48,  1.35it/s]Training epoch 1:  15% 2018/13868 [24:47<2:28:22,  1.33it/s]Training epoch 1:  15% 2019/13868 [24:48<2:30:26,  1.31it/s]Training epoch 1:  15% 2020/13868 [24:49<2:29:17,  1.32it/s]Training epoch 1:  15% 2021/13868 [24:50<2:27:34,  1.34it/s]Training epoch 1:  15% 2022/13868 [24:50<2:26:51,  1.34it/s]Training epoch 1:  15% 2023/13868 [24:51<2:28:17,  1.33it/s]Training epoch 1:  15% 2024/13868 [24:52<2:29:23,  1.32it/s]Training epoch 1:  15% 2025/13868 [24:53<2:29:04,  1.32it/s]Training epoch 1:  15% 2026/13868 [24:54<2:28:49,  1.33it/s]Training epoch 1:  15% 2027/13868 [24:54<2:30:20,  1.31it/s]Training epoch 1:  15% 2028/13868 [24:55<2:29:44,  1.32it/s]Training epoch 1:  15% 2029/13868 [24:56<2:27:35,  1.34it/s]Training epoch 1:  15% 2030/13868 [24:57<2:28:09,  1.33it/s]Training epoch 1:  15% 2031/13868 [24:57<2:28:19,  1.33it/s]Training epoch 1:  15% 2032/13868 [24:58<2:29:55,  1.32it/s]Training epoch 1:  15% 2033/13868 [24:59<2:28:26,  1.33it/s]Training epoch 1:  15% 2034/13868 [25:00<2:29:07,  1.32it/s]Training epoch 1:  15% 2035/13868 [25:00<2:28:17,  1.33it/s]Training epoch 1:  15% 2036/13868 [25:01<2:26:02,  1.35it/s]Training epoch 1:  15% 2037/13868 [25:02<2:23:51,  1.37it/s]Training epoch 1:  15% 2038/13868 [25:02<2:24:22,  1.37it/s]Training epoch 1:  15% 2039/13868 [25:03<2:27:33,  1.34it/s]Training epoch 1:  15% 2040/13868 [25:04<2:30:06,  1.31it/s]Training epoch 1:  15% 2041/13868 [25:05<2:29:11,  1.32it/s]Training epoch 1:  15% 2042/13868 [25:06<2:31:14,  1.30it/s]Training epoch 1:  15% 2043/13868 [25:06<2:30:45,  1.31it/s]Training epoch 1:  15% 2044/13868 [25:07<2:30:09,  1.31it/s]Training epoch 1:  15% 2045/13868 [25:08<2:28:45,  1.32it/s]Training epoch 1:  15% 2046/13868 [25:09<2:28:48,  1.32it/s]Training epoch 1:  15% 2047/13868 [25:09<2:27:45,  1.33it/s]Training epoch 1:  15% 2048/13868 [25:10<2:27:24,  1.34it/s]Training epoch 1:  15% 2049/13868 [25:11<2:26:35,  1.34it/s]Training epoch 1:  15% 2050/13868 [25:12<2:25:37,  1.35it/s]Training epoch 1:  15% 2051/13868 [25:12<2:24:50,  1.36it/s]Training epoch 1:  15% 2052/13868 [25:13<2:25:47,  1.35it/s]Training epoch 1:  15% 2053/13868 [25:14<2:25:56,  1.35it/s]Training epoch 1:  15% 2054/13868 [25:15<2:26:26,  1.34it/s]Training epoch 1:  15% 2055/13868 [25:15<2:24:52,  1.36it/s]Training epoch 1:  15% 2056/13868 [25:16<2:24:52,  1.36it/s]Training epoch 1:  15% 2057/13868 [25:17<2:24:52,  1.36it/s]Training epoch 1:  15% 2058/13868 [25:17<2:25:18,  1.35it/s]Training epoch 1:  15% 2059/13868 [25:18<2:25:27,  1.35it/s]Training epoch 1:  15% 2060/13868 [25:19<2:26:29,  1.34it/s]Training epoch 1:  15% 2061/13868 [25:20<2:28:02,  1.33it/s]Training epoch 1:  15% 2062/13868 [25:20<2:25:49,  1.35it/s]Training epoch 1:  15% 2063/13868 [25:21<2:28:35,  1.32it/s]Training epoch 1:  15% 2064/13868 [25:22<2:27:07,  1.34it/s]Training epoch 1:  15% 2065/13868 [25:23<2:28:32,  1.32it/s]Training epoch 1:  15% 2066/13868 [25:23<2:29:58,  1.31it/s]Training epoch 1:  15% 2067/13868 [25:24<2:30:06,  1.31it/s]Training epoch 1:  15% 2068/13868 [25:25<2:30:27,  1.31it/s]Training epoch 1:  15% 2069/13868 [25:26<2:30:16,  1.31it/s]Training epoch 1:  15% 2070/13868 [25:27<2:27:21,  1.33it/s]Training epoch 1:  15% 2071/13868 [25:27<2:27:26,  1.33it/s]Training epoch 1:  15% 2072/13868 [25:28<2:26:08,  1.35it/s]Training epoch 1:  15% 2073/13868 [25:29<2:24:52,  1.36it/s]Training epoch 1:  15% 2074/13868 [25:29<2:26:21,  1.34it/s]Training epoch 1:  15% 2075/13868 [25:30<2:27:53,  1.33it/s]Training epoch 1:  15% 2076/13868 [25:31<2:26:34,  1.34it/s]Training epoch 1:  15% 2077/13868 [25:32<2:26:47,  1.34it/s]Training epoch 1:  15% 2078/13868 [25:32<2:25:23,  1.35it/s]Training epoch 1:  15% 2079/13868 [25:33<2:25:23,  1.35it/s]Training epoch 1:  15% 2080/13868 [25:34<2:25:24,  1.35it/s]Training epoch 1:  15% 2081/13868 [25:35<2:26:37,  1.34it/s]Training epoch 1:  15% 2082/13868 [25:35<2:24:27,  1.36it/s]Training epoch 1:  15% 2083/13868 [25:36<2:26:12,  1.34it/s]Training epoch 1:  15% 2084/13868 [25:37<2:26:16,  1.34it/s]Training epoch 1:  15% 2085/13868 [25:38<2:26:18,  1.34it/s]Training epoch 1:  15% 2086/13868 [25:38<2:26:44,  1.34it/s]Training epoch 1:  15% 2087/13868 [25:39<2:25:46,  1.35it/s]Training epoch 1:  15% 2088/13868 [25:40<2:24:56,  1.35it/s]Training epoch 1:  15% 2089/13868 [25:41<2:25:30,  1.35it/s]Training epoch 1:  15% 2090/13868 [25:41<2:25:51,  1.35it/s]Training epoch 1:  15% 2091/13868 [25:42<2:27:18,  1.33it/s]Training epoch 1:  15% 2092/13868 [25:43<2:27:07,  1.33it/s]Training epoch 1:  15% 2093/13868 [25:44<2:28:45,  1.32it/s]Training epoch 1:  15% 2094/13868 [25:44<2:28:54,  1.32it/s]Training epoch 1:  15% 2095/13868 [25:45<2:28:04,  1.33it/s]Training epoch 1:  15% 2096/13868 [25:46<2:27:35,  1.33it/s]Training epoch 1:  15% 2097/13868 [25:47<2:25:37,  1.35it/s]Training epoch 1:  15% 2098/13868 [25:47<2:25:32,  1.35it/s]Training epoch 1:  15% 2099/13868 [25:48<2:26:03,  1.34it/s]Training epoch 1:  15% 2100/13868 [25:49<2:33:19,  1.28it/s]Training epoch 1:  15% 2101/13868 [25:50<2:33:30,  1.28it/s]Training epoch 1:  15% 2102/13868 [25:51<2:30:54,  1.30it/s]Training epoch 1:  15% 2103/13868 [25:51<2:30:16,  1.30it/s]Training epoch 1:  15% 2104/13868 [25:52<2:30:15,  1.30it/s]Training epoch 1:  15% 2105/13868 [25:53<2:27:49,  1.33it/s]Training epoch 1:  15% 2106/13868 [25:54<2:27:58,  1.32it/s]Training epoch 1:  15% 2107/13868 [25:54<2:25:45,  1.34it/s]Training epoch 1:  15% 2108/13868 [25:55<2:27:18,  1.33it/s]Training epoch 1:  15% 2109/13868 [25:56<2:24:44,  1.35it/s]Training epoch 1:  15% 2110/13868 [25:56<2:24:44,  1.35it/s]Training epoch 1:  15% 2111/13868 [25:57<2:23:17,  1.37it/s]Training epoch 1:  15% 2112/13868 [25:58<2:22:33,  1.37it/s]Training epoch 1:  15% 2113/13868 [25:59<2:24:44,  1.35it/s]Training epoch 1:  15% 2114/13868 [25:59<2:23:05,  1.37it/s]Training epoch 1:  15% 2115/13868 [26:00<2:23:20,  1.37it/s]Training epoch 1:  15% 2116/13868 [26:01<2:24:29,  1.36it/s]Training epoch 1:  15% 2117/13868 [26:02<2:24:40,  1.35it/s]Training epoch 1:  15% 2118/13868 [26:02<2:26:01,  1.34it/s]Training epoch 1:  15% 2119/13868 [26:03<2:26:54,  1.33it/s]Training epoch 1:  15% 2120/13868 [26:04<2:27:33,  1.33it/s]Training epoch 1:  15% 2121/13868 [26:05<2:24:20,  1.36it/s]Training epoch 1:  15% 2122/13868 [26:05<2:25:03,  1.35it/s]Training epoch 1:  15% 2123/13868 [26:06<2:26:02,  1.34it/s]Training epoch 1:  15% 2124/13868 [26:07<2:24:04,  1.36it/s]Training epoch 1:  15% 2125/13868 [26:07<2:22:27,  1.37it/s]Training epoch 1:  15% 2126/13868 [26:08<2:23:41,  1.36it/s]Training epoch 1:  15% 2127/13868 [26:09<2:22:51,  1.37it/s]Training epoch 1:  15% 2128/13868 [26:10<2:23:40,  1.36it/s]Training epoch 1:  15% 2129/13868 [26:10<2:24:47,  1.35it/s]Training epoch 1:  15% 2130/13868 [26:11<2:24:21,  1.36it/s]Training epoch 1:  15% 2131/13868 [26:12<2:24:43,  1.35it/s]Training epoch 1:  15% 2132/13868 [26:13<2:24:19,  1.36it/s]Training epoch 1:  15% 2133/13868 [26:13<2:24:01,  1.36it/s]Training epoch 1:  15% 2134/13868 [26:14<2:22:27,  1.37it/s]Training epoch 1:  15% 2135/13868 [26:15<2:22:49,  1.37it/s]Training epoch 1:  15% 2136/13868 [26:16<2:26:02,  1.34it/s]Training epoch 1:  15% 2137/13868 [26:16<2:25:11,  1.35it/s]Training epoch 1:  15% 2138/13868 [26:17<2:23:20,  1.36it/s]Training epoch 1:  15% 2139/13868 [26:18<2:22:38,  1.37it/s]Training epoch 1:  15% 2140/13868 [26:19<2:24:01,  1.36it/s]Training epoch 1:  15% 2141/13868 [26:19<2:24:50,  1.35it/s]Training epoch 1:  15% 2142/13868 [26:20<2:26:28,  1.33it/s]Training epoch 1:  15% 2143/13868 [26:21<2:26:46,  1.33it/s]Training epoch 1:  15% 2144/13868 [26:22<2:26:41,  1.33it/s]Training epoch 1:  15% 2145/13868 [26:22<2:27:18,  1.33it/s]Training epoch 1:  15% 2146/13868 [26:23<2:28:36,  1.31it/s]Training epoch 1:  15% 2147/13868 [26:24<2:28:06,  1.32it/s]Training epoch 1:  15% 2148/13868 [26:25<2:27:10,  1.33it/s]Training epoch 1:  15% 2149/13868 [26:25<2:26:48,  1.33it/s]Training epoch 1:  16% 2150/13868 [26:26<2:27:03,  1.33it/s]Training epoch 1:  16% 2151/13868 [26:27<2:26:42,  1.33it/s]Training epoch 1:  16% 2152/13868 [26:28<2:26:41,  1.33it/s]Training epoch 1:  16% 2153/13868 [26:28<2:25:36,  1.34it/s]Training epoch 1:  16% 2154/13868 [26:29<2:26:16,  1.33it/s]Training epoch 1:  16% 2155/13868 [26:30<2:27:42,  1.32it/s]Training epoch 1:  16% 2156/13868 [26:31<2:28:06,  1.32it/s]Training epoch 1:  16% 2157/13868 [26:31<2:26:52,  1.33it/s]Training epoch 1:  16% 2158/13868 [26:32<2:28:44,  1.31it/s]Training epoch 1:  16% 2159/13868 [26:33<2:27:27,  1.32it/s]Training epoch 1:  16% 2160/13868 [26:34<2:27:34,  1.32it/s]Training epoch 1:  16% 2161/13868 [26:34<2:27:21,  1.32it/s]Training epoch 1:  16% 2162/13868 [26:35<2:27:36,  1.32it/s]Training epoch 1:  16% 2163/13868 [26:36<2:27:26,  1.32it/s]Training epoch 1:  16% 2164/13868 [26:37<2:28:13,  1.32it/s]Training epoch 1:  16% 2165/13868 [26:37<2:28:26,  1.31it/s]Training epoch 1:  16% 2166/13868 [26:38<2:29:22,  1.31it/s]Training epoch 1:  16% 2167/13868 [26:39<2:28:24,  1.31it/s]Training epoch 1:  16% 2168/13868 [26:40<2:28:57,  1.31it/s]Training epoch 1:  16% 2169/13868 [26:41<2:28:51,  1.31it/s]Training epoch 1:  16% 2170/13868 [26:41<2:27:18,  1.32it/s]Training epoch 1:  16% 2171/13868 [26:42<2:25:59,  1.34it/s]Training epoch 1:  16% 2172/13868 [26:43<2:25:57,  1.34it/s]Training epoch 1:  16% 2173/13868 [26:43<2:25:29,  1.34it/s]Training epoch 1:  16% 2174/13868 [26:44<2:26:54,  1.33it/s]Training epoch 1:  16% 2175/13868 [26:45<2:26:26,  1.33it/s]Training epoch 1:  16% 2176/13868 [26:46<2:25:59,  1.33it/s]Training epoch 1:  16% 2177/13868 [26:47<2:28:34,  1.31it/s]Training epoch 1:  16% 2178/13868 [26:47<2:29:08,  1.31it/s]Training epoch 1:  16% 2179/13868 [26:48<2:29:38,  1.30it/s]Training epoch 1:  16% 2180/13868 [26:49<2:28:35,  1.31it/s]Training epoch 1:  16% 2181/13868 [26:50<2:28:35,  1.31it/s]Training epoch 1:  16% 2182/13868 [26:50<2:29:38,  1.30it/s]Training epoch 1:  16% 2183/13868 [26:51<2:28:22,  1.31it/s]Training epoch 1:  16% 2184/13868 [26:52<2:27:27,  1.32it/s]Training epoch 1:  16% 2185/13868 [26:53<2:27:10,  1.32it/s]Training epoch 1:  16% 2186/13868 [26:53<2:26:31,  1.33it/s]Training epoch 1:  16% 2187/13868 [26:54<2:26:43,  1.33it/s]Training epoch 1:  16% 2188/13868 [26:55<2:27:26,  1.32it/s]Training epoch 1:  16% 2189/13868 [26:56<2:24:46,  1.34it/s]Training epoch 1:  16% 2190/13868 [26:56<2:25:02,  1.34it/s]Training epoch 1:  16% 2191/13868 [26:57<2:25:28,  1.34it/s]Training epoch 1:  16% 2192/13868 [26:58<2:24:58,  1.34it/s]Training epoch 1:  16% 2193/13868 [26:59<2:25:01,  1.34it/s]Training epoch 1:  16% 2194/13868 [26:59<2:25:38,  1.34it/s]Training epoch 1:  16% 2195/13868 [27:00<2:22:37,  1.36it/s]Training epoch 1:  16% 2196/13868 [27:01<2:24:39,  1.34it/s]Training epoch 1:  16% 2197/13868 [27:02<2:26:23,  1.33it/s]Training epoch 1:  16% 2198/13868 [27:02<2:26:00,  1.33it/s]Training epoch 1:  16% 2199/13868 [27:03<2:24:06,  1.35it/s]Training epoch 1:  16% 2200/13868 [27:04<2:33:12,  1.27it/s]Training epoch 1:  16% 2201/13868 [27:05<2:29:57,  1.30it/s]Training epoch 1:  16% 2202/13868 [27:05<2:29:20,  1.30it/s]Training epoch 1:  16% 2203/13868 [27:06<2:27:43,  1.32it/s]Training epoch 1:  16% 2204/13868 [27:07<2:26:52,  1.32it/s]Training epoch 1:  16% 2205/13868 [27:08<2:27:41,  1.32it/s]Training epoch 1:  16% 2206/13868 [27:08<2:27:12,  1.32it/s]Training epoch 1:  16% 2207/13868 [27:09<2:27:50,  1.31it/s]Training epoch 1:  16% 2208/13868 [27:10<2:29:08,  1.30it/s]Training epoch 1:  16% 2209/13868 [27:11<2:25:54,  1.33it/s]Training epoch 1:  16% 2210/13868 [27:11<2:26:15,  1.33it/s]Training epoch 1:  16% 2211/13868 [27:12<2:27:55,  1.31it/s]Training epoch 1:  16% 2212/13868 [27:13<2:27:30,  1.32it/s]Training epoch 1:  16% 2213/13868 [27:14<2:26:31,  1.33it/s]Training epoch 1:  16% 2214/13868 [27:15<2:27:09,  1.32it/s]Training epoch 1:  16% 2215/13868 [27:15<2:26:13,  1.33it/s]Training epoch 1:  16% 2216/13868 [27:16<2:28:33,  1.31it/s]Training epoch 1:  16% 2217/13868 [27:17<2:27:02,  1.32it/s]Training epoch 1:  16% 2218/13868 [27:18<2:26:35,  1.32it/s]Training epoch 1:  16% 2219/13868 [27:18<2:24:48,  1.34it/s]Training epoch 1:  16% 2220/13868 [27:19<2:27:14,  1.32it/s]Training epoch 1:  16% 2221/13868 [27:20<2:26:53,  1.32it/s]Training epoch 1:  16% 2222/13868 [27:21<2:26:15,  1.33it/s]Training epoch 1:  16% 2223/13868 [27:21<2:25:18,  1.34it/s]Training epoch 1:  16% 2224/13868 [27:22<2:22:19,  1.36it/s]Training epoch 1:  16% 2225/13868 [27:23<2:25:22,  1.33it/s]Training epoch 1:  16% 2226/13868 [27:24<2:26:54,  1.32it/s]Training epoch 1:  16% 2227/13868 [27:24<2:26:56,  1.32it/s]Training epoch 1:  16% 2228/13868 [27:25<2:25:41,  1.33it/s]Training epoch 1:  16% 2229/13868 [27:26<2:25:23,  1.33it/s]Training epoch 1:  16% 2230/13868 [27:27<2:24:05,  1.35it/s]Training epoch 1:  16% 2231/13868 [27:27<2:24:04,  1.35it/s]Training epoch 1:  16% 2232/13868 [27:28<2:26:00,  1.33it/s]Training epoch 1:  16% 2233/13868 [27:29<2:27:05,  1.32it/s]Training epoch 1:  16% 2234/13868 [27:30<2:27:15,  1.32it/s]Training epoch 1:  16% 2235/13868 [27:30<2:26:08,  1.33it/s]Training epoch 1:  16% 2236/13868 [27:31<2:27:51,  1.31it/s]Training epoch 1:  16% 2237/13868 [27:32<2:27:55,  1.31it/s]Training epoch 1:  16% 2238/13868 [27:33<2:27:37,  1.31it/s]Training epoch 1:  16% 2239/13868 [27:33<2:24:32,  1.34it/s]Training epoch 1:  16% 2240/13868 [27:34<2:24:29,  1.34it/s]Training epoch 1:  16% 2241/13868 [27:35<2:23:44,  1.35it/s]Training epoch 1:  16% 2242/13868 [27:36<2:23:57,  1.35it/s]Training epoch 1:  16% 2243/13868 [27:36<2:23:48,  1.35it/s]Training epoch 1:  16% 2244/13868 [27:37<2:27:42,  1.31it/s]Training epoch 1:  16% 2245/13868 [27:38<2:28:13,  1.31it/s]Training epoch 1:  16% 2246/13868 [27:39<2:25:12,  1.33it/s]Training epoch 1:  16% 2247/13868 [27:39<2:26:13,  1.32it/s]Training epoch 1:  16% 2248/13868 [27:40<2:26:36,  1.32it/s]Training epoch 1:  16% 2249/13868 [27:41<2:25:19,  1.33it/s]Training epoch 1:  16% 2250/13868 [27:42<2:26:37,  1.32it/s]Training epoch 1:  16% 2251/13868 [27:42<2:25:34,  1.33it/s]Training epoch 1:  16% 2252/13868 [27:43<2:26:03,  1.33it/s]Training epoch 1:  16% 2253/13868 [27:44<2:26:33,  1.32it/s]Training epoch 1:  16% 2254/13868 [27:45<2:26:04,  1.33it/s]Training epoch 1:  16% 2255/13868 [27:45<2:24:57,  1.34it/s]Training epoch 1:  16% 2256/13868 [27:46<2:26:08,  1.32it/s]Training epoch 1:  16% 2257/13868 [27:47<2:25:33,  1.33it/s]Training epoch 1:  16% 2258/13868 [27:48<2:27:09,  1.31it/s]Training epoch 1:  16% 2259/13868 [27:48<2:28:08,  1.31it/s]Training epoch 1:  16% 2260/13868 [27:49<2:29:06,  1.30it/s]Training epoch 1:  16% 2261/13868 [27:50<2:25:54,  1.33it/s]Training epoch 1:  16% 2262/13868 [27:51<2:27:51,  1.31it/s]Training epoch 1:  16% 2263/13868 [27:51<2:26:14,  1.32it/s]Training epoch 1:  16% 2264/13868 [27:52<2:27:41,  1.31it/s]Training epoch 1:  16% 2265/13868 [27:53<2:26:29,  1.32it/s]Training epoch 1:  16% 2266/13868 [27:54<2:24:01,  1.34it/s]Training epoch 1:  16% 2267/13868 [27:54<2:23:48,  1.34it/s]Training epoch 1:  16% 2268/13868 [27:55<2:24:50,  1.33it/s]Training epoch 1:  16% 2269/13868 [27:56<2:26:16,  1.32it/s]Training epoch 1:  16% 2270/13868 [27:57<2:24:33,  1.34it/s]Training epoch 1:  16% 2271/13868 [27:57<2:24:29,  1.34it/s]Training epoch 1:  16% 2272/13868 [27:58<2:24:17,  1.34it/s]Training epoch 1:  16% 2273/13868 [27:59<2:25:19,  1.33it/s]Training epoch 1:  16% 2274/13868 [28:00<2:26:06,  1.32it/s]Training epoch 1:  16% 2275/13868 [28:01<2:27:32,  1.31it/s]Training epoch 1:  16% 2276/13868 [28:01<2:28:14,  1.30it/s]Training epoch 1:  16% 2277/13868 [28:02<2:27:44,  1.31it/s]Training epoch 1:  16% 2278/13868 [28:03<2:28:37,  1.30it/s]Training epoch 1:  16% 2279/13868 [28:04<2:26:56,  1.31it/s]Training epoch 1:  16% 2280/13868 [28:04<2:27:58,  1.31it/s]Training epoch 1:  16% 2281/13868 [28:05<2:25:23,  1.33it/s]Training epoch 1:  16% 2282/13868 [28:06<2:24:16,  1.34it/s]Training epoch 1:  16% 2283/13868 [28:07<2:24:26,  1.34it/s]Training epoch 1:  16% 2284/13868 [28:07<2:26:21,  1.32it/s]Training epoch 1:  16% 2285/13868 [28:08<2:25:52,  1.32it/s]Training epoch 1:  16% 2286/13868 [28:09<2:26:08,  1.32it/s]Training epoch 1:  16% 2287/13868 [28:10<2:25:23,  1.33it/s]Training epoch 1:  16% 2288/13868 [28:10<2:25:10,  1.33it/s]Training epoch 1:  17% 2289/13868 [28:11<2:25:20,  1.33it/s]Training epoch 1:  17% 2290/13868 [28:12<2:26:41,  1.32it/s]Training epoch 1:  17% 2291/13868 [28:13<2:24:48,  1.33it/s]Training epoch 1:  17% 2292/13868 [28:13<2:26:08,  1.32it/s]Training epoch 1:  17% 2293/13868 [28:14<2:23:07,  1.35it/s]Training epoch 1:  17% 2294/13868 [28:15<2:23:33,  1.34it/s]Training epoch 1:  17% 2295/13868 [28:16<2:24:27,  1.34it/s]Training epoch 1:  17% 2296/13868 [28:16<2:24:34,  1.33it/s]Training epoch 1:  17% 2297/13868 [28:17<2:23:41,  1.34it/s]Training epoch 1:  17% 2298/13868 [28:18<2:25:17,  1.33it/s]Training epoch 1:  17% 2299/13868 [28:19<2:25:43,  1.32it/s]Training epoch 1:  17% 2300/13868 [28:20<2:39:56,  1.21it/s]Training epoch 1:  17% 2301/13868 [28:20<2:35:21,  1.24it/s]Training epoch 1:  17% 2302/13868 [28:21<2:34:07,  1.25it/s]Training epoch 1:  17% 2303/13868 [28:22<2:29:34,  1.29it/s]Training epoch 1:  17% 2304/13868 [28:23<2:28:17,  1.30it/s]Training epoch 1:  17% 2305/13868 [28:23<2:27:28,  1.31it/s]Training epoch 1:  17% 2306/13868 [28:24<2:27:22,  1.31it/s]Training epoch 1:  17% 2307/13868 [28:25<2:27:25,  1.31it/s]Training epoch 1:  17% 2308/13868 [28:26<2:27:42,  1.30it/s]Training epoch 1:  17% 2309/13868 [28:26<2:27:41,  1.30it/s]Training epoch 1:  17% 2310/13868 [28:27<2:28:04,  1.30it/s]Training epoch 1:  17% 2311/13868 [28:28<2:27:10,  1.31it/s]Training epoch 1:  17% 2312/13868 [28:29<2:24:48,  1.33it/s]Training epoch 1:  17% 2313/13868 [28:29<2:24:39,  1.33it/s]Training epoch 1:  17% 2314/13868 [28:30<2:26:11,  1.32it/s]Training epoch 1:  17% 2315/13868 [28:31<2:22:52,  1.35it/s]Training epoch 1:  17% 2316/13868 [28:32<2:24:05,  1.34it/s]Training epoch 1:  17% 2317/13868 [28:32<2:22:49,  1.35it/s]Training epoch 1:  17% 2318/13868 [28:33<2:21:26,  1.36it/s]Training epoch 1:  17% 2319/13868 [28:34<2:21:48,  1.36it/s]Training epoch 1:  17% 2320/13868 [28:35<2:22:48,  1.35it/s]Training epoch 1:  17% 2321/13868 [28:35<2:22:05,  1.35it/s]Training epoch 1:  17% 2322/13868 [28:36<2:23:45,  1.34it/s]Training epoch 1:  17% 2323/13868 [28:37<2:25:49,  1.32it/s]Training epoch 1:  17% 2324/13868 [28:38<2:24:30,  1.33it/s]Training epoch 1:  17% 2325/13868 [28:38<2:23:43,  1.34it/s]Training epoch 1:  17% 2326/13868 [28:39<2:22:22,  1.35it/s]Training epoch 1:  17% 2327/13868 [28:40<2:22:37,  1.35it/s]Training epoch 1:  17% 2328/13868 [28:41<2:23:10,  1.34it/s]Training epoch 1:  17% 2329/13868 [28:41<2:23:52,  1.34it/s]Training epoch 1:  17% 2330/13868 [28:42<2:23:43,  1.34it/s]Training epoch 1:  17% 2331/13868 [28:43<2:24:25,  1.33it/s]Training epoch 1:  17% 2332/13868 [28:44<2:25:49,  1.32it/s]Training epoch 1:  17% 2333/13868 [28:44<2:22:58,  1.34it/s]Training epoch 1:  17% 2334/13868 [28:45<2:22:59,  1.34it/s]Training epoch 1:  17% 2335/13868 [28:46<2:22:28,  1.35it/s]Training epoch 1:  17% 2336/13868 [28:47<2:21:46,  1.36it/s]Training epoch 1:  17% 2337/13868 [28:47<2:22:18,  1.35it/s]Training epoch 1:  17% 2338/13868 [28:48<2:22:23,  1.35it/s]Training epoch 1:  17% 2339/13868 [28:49<2:21:04,  1.36it/s]Training epoch 1:  17% 2340/13868 [28:50<2:22:59,  1.34it/s]Training epoch 1:  17% 2341/13868 [28:50<2:23:43,  1.34it/s]Training epoch 1:  17% 2342/13868 [28:51<2:24:21,  1.33it/s]Training epoch 1:  17% 2343/13868 [28:52<2:24:12,  1.33it/s]Training epoch 1:  17% 2344/13868 [28:53<2:26:27,  1.31it/s]Training epoch 1:  17% 2345/13868 [28:53<2:26:22,  1.31it/s]Training epoch 1:  17% 2346/13868 [28:54<2:25:16,  1.32it/s]Training epoch 1:  17% 2347/13868 [28:55<2:25:01,  1.32it/s]Training epoch 1:  17% 2348/13868 [28:56<2:25:16,  1.32it/s]Training epoch 1:  17% 2349/13868 [28:56<2:25:45,  1.32it/s]Training epoch 1:  17% 2350/13868 [28:57<2:24:24,  1.33it/s]Training epoch 1:  17% 2351/13868 [28:58<2:23:57,  1.33it/s]Training epoch 1:  17% 2352/13868 [28:59<2:22:31,  1.35it/s]Training epoch 1:  17% 2353/13868 [28:59<2:22:51,  1.34it/s]Training epoch 1:  17% 2354/13868 [29:00<2:22:57,  1.34it/s]Training epoch 1:  17% 2355/13868 [29:01<2:23:54,  1.33it/s]Training epoch 1:  17% 2356/13868 [29:02<2:24:16,  1.33it/s]Training epoch 1:  17% 2357/13868 [29:02<2:24:06,  1.33it/s]Training epoch 1:  17% 2358/13868 [29:03<2:22:45,  1.34it/s]Training epoch 1:  17% 2359/13868 [29:04<2:22:45,  1.34it/s]Training epoch 1:  17% 2360/13868 [29:05<2:21:24,  1.36it/s]Training epoch 1:  17% 2361/13868 [29:05<2:20:42,  1.36it/s]Training epoch 1:  17% 2362/13868 [29:06<2:21:56,  1.35it/s]Training epoch 1:  17% 2363/13868 [29:07<2:22:37,  1.34it/s]Training epoch 1:  17% 2364/13868 [29:07<2:21:53,  1.35it/s]Training epoch 1:  17% 2365/13868 [29:08<2:22:55,  1.34it/s]Training epoch 1:  17% 2366/13868 [29:09<2:24:09,  1.33it/s]Training epoch 1:  17% 2367/13868 [29:10<2:22:06,  1.35it/s]Training epoch 1:  17% 2368/13868 [29:10<2:21:38,  1.35it/s]Training epoch 1:  17% 2369/13868 [29:11<2:21:02,  1.36it/s]Training epoch 1:  17% 2370/13868 [29:12<2:20:48,  1.36it/s]Training epoch 1:  17% 2371/13868 [29:13<2:22:27,  1.35it/s]Training epoch 1:  17% 2372/13868 [29:13<2:23:58,  1.33it/s]Training epoch 1:  17% 2373/13868 [29:14<2:23:01,  1.34it/s]Training epoch 1:  17% 2374/13868 [29:15<2:23:31,  1.33it/s]Training epoch 1:  17% 2375/13868 [29:16<2:22:13,  1.35it/s]Training epoch 1:  17% 2376/13868 [29:16<2:22:29,  1.34it/s]Training epoch 1:  17% 2377/13868 [29:17<2:24:30,  1.33it/s]Training epoch 1:  17% 2378/13868 [29:18<2:23:37,  1.33it/s]Training epoch 1:  17% 2379/13868 [29:19<2:23:35,  1.33it/s]Training epoch 1:  17% 2380/13868 [29:19<2:23:43,  1.33it/s]Training epoch 1:  17% 2381/13868 [29:20<2:23:41,  1.33it/s]Training epoch 1:  17% 2382/13868 [29:21<2:25:50,  1.31it/s]Training epoch 1:  17% 2383/13868 [29:22<2:25:17,  1.32it/s]Training epoch 1:  17% 2384/13868 [29:22<2:25:06,  1.32it/s]Training epoch 1:  17% 2385/13868 [29:23<2:24:38,  1.32it/s]Training epoch 1:  17% 2386/13868 [29:24<2:24:40,  1.32it/s]Training epoch 1:  17% 2387/13868 [29:25<2:22:25,  1.34it/s]Training epoch 1:  17% 2388/13868 [29:25<2:24:49,  1.32it/s]Training epoch 1:  17% 2389/13868 [29:26<2:25:16,  1.32it/s]Training epoch 1:  17% 2390/13868 [29:27<2:24:49,  1.32it/s]Training epoch 1:  17% 2391/13868 [29:28<2:23:30,  1.33it/s]Training epoch 1:  17% 2392/13868 [29:28<2:23:17,  1.33it/s]Training epoch 1:  17% 2393/13868 [29:29<2:21:33,  1.35it/s]Training epoch 1:  17% 2394/13868 [29:30<2:22:51,  1.34it/s]Training epoch 1:  17% 2395/13868 [29:31<2:22:28,  1.34it/s]Training epoch 1:  17% 2396/13868 [29:31<2:20:38,  1.36it/s]Training epoch 1:  17% 2397/13868 [29:32<2:21:51,  1.35it/s]Training epoch 1:  17% 2398/13868 [29:33<2:22:29,  1.34it/s]Training epoch 1:  17% 2399/13868 [29:34<2:22:16,  1.34it/s]Training epoch 1:  17% 2400/13868 [29:35<2:36:27,  1.22it/s]Training epoch 1:  17% 2401/13868 [29:35<2:32:56,  1.25it/s]Training epoch 1:  17% 2402/13868 [29:36<2:28:19,  1.29it/s]Training epoch 1:  17% 2403/13868 [29:37<2:24:43,  1.32it/s]Training epoch 1:  17% 2404/13868 [29:38<2:25:52,  1.31it/s]Training epoch 1:  17% 2405/13868 [29:38<2:23:16,  1.33it/s]Training epoch 1:  17% 2406/13868 [29:39<2:21:54,  1.35it/s]Training epoch 1:  17% 2407/13868 [29:40<2:20:30,  1.36it/s]Training epoch 1:  17% 2408/13868 [29:41<2:20:12,  1.36it/s]Training epoch 1:  17% 2409/13868 [29:41<2:20:15,  1.36it/s]Training epoch 1:  17% 2410/13868 [29:42<2:21:32,  1.35it/s]Training epoch 1:  17% 2411/13868 [29:43<2:21:27,  1.35it/s]Training epoch 1:  17% 2412/13868 [29:43<2:20:40,  1.36it/s]Training epoch 1:  17% 2413/13868 [29:44<2:20:12,  1.36it/s]Training epoch 1:  17% 2414/13868 [29:45<2:21:12,  1.35it/s]Training epoch 1:  17% 2415/13868 [29:46<2:22:05,  1.34it/s]Training epoch 1:  17% 2416/13868 [29:46<2:21:46,  1.35it/s]Training epoch 1:  17% 2417/13868 [29:47<2:20:58,  1.35it/s]Training epoch 1:  17% 2418/13868 [29:48<2:21:42,  1.35it/s]Training epoch 1:  17% 2419/13868 [29:49<2:22:34,  1.34it/s]Training epoch 1:  17% 2420/13868 [29:49<2:23:39,  1.33it/s]Training epoch 1:  17% 2421/13868 [29:50<2:23:55,  1.33it/s]Training epoch 1:  17% 2422/13868 [29:51<2:22:14,  1.34it/s]Training epoch 1:  17% 2423/13868 [29:52<2:20:48,  1.35it/s]Training epoch 1:  17% 2424/13868 [29:52<2:21:32,  1.35it/s]Training epoch 1:  17% 2425/13868 [29:53<2:19:43,  1.36it/s]Training epoch 1:  17% 2426/13868 [29:54<2:23:14,  1.33it/s]Training epoch 1:  18% 2427/13868 [29:55<2:23:17,  1.33it/s]Training epoch 1:  18% 2428/13868 [29:55<2:23:04,  1.33it/s]Training epoch 1:  18% 2429/13868 [29:56<2:24:22,  1.32it/s]Training epoch 1:  18% 2430/13868 [29:57<2:23:01,  1.33it/s]Training epoch 1:  18% 2431/13868 [29:58<2:22:46,  1.34it/s]Training epoch 1:  18% 2432/13868 [29:58<2:23:22,  1.33it/s]Training epoch 1:  18% 2433/13868 [29:59<2:21:01,  1.35it/s]Training epoch 1:  18% 2434/13868 [30:00<2:20:45,  1.35it/s]Training epoch 1:  18% 2435/13868 [30:01<2:20:48,  1.35it/s]Training epoch 1:  18% 2436/13868 [30:01<2:21:05,  1.35it/s]Training epoch 1:  18% 2437/13868 [30:02<2:21:31,  1.35it/s]Training epoch 1:  18% 2438/13868 [30:03<2:24:14,  1.32it/s]Training epoch 1:  18% 2439/13868 [30:04<2:23:04,  1.33it/s]Training epoch 1:  18% 2440/13868 [30:04<2:24:32,  1.32it/s]Training epoch 1:  18% 2441/13868 [30:05<2:22:59,  1.33it/s]Training epoch 1:  18% 2442/13868 [30:06<2:21:48,  1.34it/s]Training epoch 1:  18% 2443/13868 [30:07<2:22:14,  1.34it/s]Training epoch 1:  18% 2444/13868 [30:07<2:22:38,  1.33it/s]Training epoch 1:  18% 2445/13868 [30:08<2:21:24,  1.35it/s]Training epoch 1:  18% 2446/13868 [30:09<2:21:49,  1.34it/s]Training epoch 1:  18% 2447/13868 [30:10<2:22:49,  1.33it/s]Training epoch 1:  18% 2448/13868 [30:10<2:21:44,  1.34it/s]Training epoch 1:  18% 2449/13868 [30:11<2:21:38,  1.34it/s]Training epoch 1:  18% 2450/13868 [30:12<2:23:06,  1.33it/s]Training epoch 1:  18% 2451/13868 [30:13<2:20:36,  1.35it/s]Training epoch 1:  18% 2452/13868 [30:13<2:22:42,  1.33it/s]Training epoch 1:  18% 2453/13868 [30:14<2:23:08,  1.33it/s]Training epoch 1:  18% 2454/13868 [30:15<2:22:23,  1.34it/s]Training epoch 1:  18% 2455/13868 [30:16<2:21:07,  1.35it/s]Training epoch 1:  18% 2456/13868 [30:16<2:21:32,  1.34it/s]Training epoch 1:  18% 2457/13868 [30:17<2:21:34,  1.34it/s]Training epoch 1:  18% 2458/13868 [30:18<2:23:59,  1.32it/s]Training epoch 1:  18% 2459/13868 [30:19<2:23:48,  1.32it/s]Training epoch 1:  18% 2460/13868 [30:19<2:22:10,  1.34it/s]Training epoch 1:  18% 2461/13868 [30:20<2:20:10,  1.36it/s]Training epoch 1:  18% 2462/13868 [30:21<2:21:42,  1.34it/s]Training epoch 1:  18% 2463/13868 [30:22<2:21:21,  1.34it/s]Training epoch 1:  18% 2464/13868 [30:22<2:22:10,  1.34it/s]Training epoch 1:  18% 2465/13868 [30:23<2:22:44,  1.33it/s]Training epoch 1:  18% 2466/13868 [30:24<2:23:56,  1.32it/s]Training epoch 1:  18% 2467/13868 [30:25<2:24:01,  1.32it/s]Training epoch 1:  18% 2468/13868 [30:25<2:25:37,  1.30it/s]Training epoch 1:  18% 2469/13868 [30:26<2:23:05,  1.33it/s]Training epoch 1:  18% 2470/13868 [30:27<2:22:52,  1.33it/s]Training epoch 1:  18% 2471/13868 [30:28<2:20:40,  1.35it/s]Training epoch 1:  18% 2472/13868 [30:28<2:21:41,  1.34it/s]Training epoch 1:  18% 2473/13868 [30:29<2:22:00,  1.34it/s]Training epoch 1:  18% 2474/13868 [30:30<2:21:57,  1.34it/s]Training epoch 1:  18% 2475/13868 [30:31<2:21:25,  1.34it/s]Training epoch 1:  18% 2476/13868 [30:31<2:20:51,  1.35it/s]Training epoch 1:  18% 2477/13868 [30:32<2:20:11,  1.35it/s]Training epoch 1:  18% 2478/13868 [30:33<2:21:10,  1.34it/s]Training epoch 1:  18% 2479/13868 [30:34<2:22:07,  1.34it/s]Training epoch 1:  18% 2480/13868 [30:34<2:24:19,  1.32it/s]Training epoch 1:  18% 2481/13868 [30:35<2:22:28,  1.33it/s]Training epoch 1:  18% 2482/13868 [30:36<2:23:52,  1.32it/s]Training epoch 1:  18% 2483/13868 [30:37<2:23:14,  1.32it/s]Training epoch 1:  18% 2484/13868 [30:37<2:22:15,  1.33it/s]Training epoch 1:  18% 2485/13868 [30:38<2:21:33,  1.34it/s]Training epoch 1:  18% 2486/13868 [30:39<2:22:10,  1.33it/s]Training epoch 1:  18% 2487/13868 [30:40<2:20:20,  1.35it/s]Training epoch 1:  18% 2488/13868 [30:40<2:21:41,  1.34it/s]Training epoch 1:  18% 2489/13868 [30:41<2:23:46,  1.32it/s]Training epoch 1:  18% 2490/13868 [30:42<2:24:50,  1.31it/s]Training epoch 1:  18% 2491/13868 [30:43<2:23:23,  1.32it/s]Training epoch 1:  18% 2492/13868 [30:43<2:23:50,  1.32it/s]Training epoch 1:  18% 2493/13868 [30:44<2:21:52,  1.34it/s]Training epoch 1:  18% 2494/13868 [30:45<2:23:15,  1.32it/s]Training epoch 1:  18% 2495/13868 [30:46<2:24:53,  1.31it/s]Training epoch 1:  18% 2496/13868 [30:46<2:24:47,  1.31it/s]Training epoch 1:  18% 2497/13868 [30:47<2:23:13,  1.32it/s]Training epoch 1:  18% 2498/13868 [30:48<2:22:16,  1.33it/s]Training epoch 1:  18% 2499/13868 [30:49<2:22:56,  1.33it/s]Training epoch 1:  18% 2500/13868 [30:50<2:33:04,  1.24it/s]Training epoch 1:  18% 2501/13868 [30:50<2:30:23,  1.26it/s]Training epoch 1:  18% 2502/13868 [30:51<2:27:45,  1.28it/s]Training epoch 1:  18% 2503/13868 [30:52<2:26:49,  1.29it/s]Training epoch 1:  18% 2504/13868 [30:53<2:26:34,  1.29it/s]Training epoch 1:  18% 2505/13868 [30:53<2:23:55,  1.32it/s]Training epoch 1:  18% 2506/13868 [30:54<2:25:26,  1.30it/s]Training epoch 1:  18% 2507/13868 [30:55<2:24:43,  1.31it/s]Training epoch 1:  18% 2508/13868 [30:56<2:24:23,  1.31it/s]Training epoch 1:  18% 2509/13868 [30:56<2:23:23,  1.32it/s]Training epoch 1:  18% 2510/13868 [30:57<2:24:10,  1.31it/s]Training epoch 1:  18% 2511/13868 [30:58<2:23:56,  1.32it/s]Training epoch 1:  18% 2512/13868 [30:59<2:23:21,  1.32it/s]Training epoch 1:  18% 2513/13868 [30:59<2:21:45,  1.34it/s]Training epoch 1:  18% 2514/13868 [31:00<2:23:07,  1.32it/s]Training epoch 1:  18% 2515/13868 [31:01<2:20:54,  1.34it/s]Training epoch 1:  18% 2516/13868 [31:02<2:20:57,  1.34it/s]Training epoch 1:  18% 2517/13868 [31:02<2:19:37,  1.35it/s]Training epoch 1:  18% 2518/13868 [31:03<2:20:00,  1.35it/s]Training epoch 1:  18% 2519/13868 [31:04<2:19:51,  1.35it/s]Training epoch 1:  18% 2520/13868 [31:05<2:20:03,  1.35it/s]Training epoch 1:  18% 2521/13868 [31:05<2:20:14,  1.35it/s]Training epoch 1:  18% 2522/13868 [31:06<2:21:31,  1.34it/s]Training epoch 1:  18% 2523/13868 [31:07<2:22:13,  1.33it/s]Training epoch 1:  18% 2524/13868 [31:08<2:21:26,  1.34it/s]Training epoch 1:  18% 2525/13868 [31:08<2:20:34,  1.34it/s]Training epoch 1:  18% 2526/13868 [31:09<2:23:01,  1.32it/s]Training epoch 1:  18% 2527/13868 [31:10<2:22:20,  1.33it/s]Training epoch 1:  18% 2528/13868 [31:11<2:23:10,  1.32it/s]Training epoch 1:  18% 2529/13868 [31:11<2:22:16,  1.33it/s]Training epoch 1:  18% 2530/13868 [31:12<2:20:51,  1.34it/s]Training epoch 1:  18% 2531/13868 [31:13<2:21:43,  1.33it/s]Training epoch 1:  18% 2532/13868 [31:14<2:24:00,  1.31it/s]Training epoch 1:  18% 2533/13868 [31:14<2:24:36,  1.31it/s]Training epoch 1:  18% 2534/13868 [31:15<2:24:18,  1.31it/s]Training epoch 1:  18% 2535/13868 [31:16<2:22:23,  1.33it/s]Training epoch 1:  18% 2536/13868 [31:17<2:21:44,  1.33it/s]Training epoch 1:  18% 2537/13868 [31:17<2:22:14,  1.33it/s]Training epoch 1:  18% 2538/13868 [31:18<2:25:36,  1.30it/s]Training epoch 1:  18% 2539/13868 [31:19<2:24:13,  1.31it/s]Training epoch 1:  18% 2540/13868 [31:20<2:23:14,  1.32it/s]Training epoch 1:  18% 2541/13868 [31:20<2:20:48,  1.34it/s]Training epoch 1:  18% 2542/13868 [31:21<2:21:31,  1.33it/s]Training epoch 1:  18% 2543/13868 [31:22<2:20:04,  1.35it/s]Training epoch 1:  18% 2544/13868 [31:23<2:21:10,  1.34it/s]Training epoch 1:  18% 2545/13868 [31:23<2:20:11,  1.35it/s]Training epoch 1:  18% 2546/13868 [31:24<2:20:50,  1.34it/s]Training epoch 1:  18% 2547/13868 [31:25<2:20:04,  1.35it/s]Training epoch 1:  18% 2548/13868 [31:26<2:20:18,  1.34it/s]Training epoch 1:  18% 2549/13868 [31:26<2:20:15,  1.34it/s]Training epoch 1:  18% 2550/13868 [31:27<2:23:24,  1.32it/s]Training epoch 1:  18% 2551/13868 [31:28<2:22:20,  1.33it/s]Training epoch 1:  18% 2552/13868 [31:29<2:21:08,  1.34it/s]Training epoch 1:  18% 2553/13868 [31:29<2:23:08,  1.32it/s]Training epoch 1:  18% 2554/13868 [31:30<2:22:07,  1.33it/s]Training epoch 1:  18% 2555/13868 [31:31<2:19:54,  1.35it/s]Training epoch 1:  18% 2556/13868 [31:32<2:19:50,  1.35it/s]Training epoch 1:  18% 2557/13868 [31:32<2:21:13,  1.33it/s]Training epoch 1:  18% 2558/13868 [31:33<2:21:15,  1.33it/s]Training epoch 1:  18% 2559/13868 [31:34<2:22:20,  1.32it/s]Training epoch 1:  18% 2560/13868 [31:35<2:23:31,  1.31it/s]Training epoch 1:  18% 2561/13868 [31:35<2:21:29,  1.33it/s]Training epoch 1:  18% 2562/13868 [31:36<2:23:29,  1.31it/s]Training epoch 1:  18% 2563/13868 [31:37<2:23:00,  1.32it/s]Training epoch 1:  18% 2564/13868 [31:38<2:23:32,  1.31it/s]Training epoch 1:  18% 2565/13868 [31:39<2:22:46,  1.32it/s]Training epoch 1:  19% 2566/13868 [31:39<2:23:49,  1.31it/s]Training epoch 1:  19% 2567/13868 [31:40<2:23:52,  1.31it/s]Training epoch 1:  19% 2568/13868 [31:41<2:24:24,  1.30it/s]Training epoch 1:  19% 2569/13868 [31:42<2:22:28,  1.32it/s]Training epoch 1:  19% 2570/13868 [31:42<2:22:58,  1.32it/s]Training epoch 1:  19% 2571/13868 [31:43<2:23:38,  1.31it/s]Training epoch 1:  19% 2572/13868 [31:44<2:25:35,  1.29it/s]Training epoch 1:  19% 2573/13868 [31:45<2:25:49,  1.29it/s]Training epoch 1:  19% 2574/13868 [31:45<2:24:30,  1.30it/s]Training epoch 1:  19% 2575/13868 [31:46<2:23:40,  1.31it/s]Training epoch 1:  19% 2576/13868 [31:47<2:22:46,  1.32it/s]Training epoch 1:  19% 2577/13868 [31:48<2:21:27,  1.33it/s]Training epoch 1:  19% 2578/13868 [31:48<2:23:53,  1.31it/s]Training epoch 1:  19% 2579/13868 [31:49<2:24:16,  1.30it/s]Training epoch 1:  19% 2580/13868 [31:50<2:23:29,  1.31it/s]Training epoch 1:  19% 2581/13868 [31:51<2:20:39,  1.34it/s]Training epoch 1:  19% 2582/13868 [31:51<2:18:59,  1.35it/s]Training epoch 1:  19% 2583/13868 [31:52<2:18:35,  1.36it/s]Training epoch 1:  19% 2584/13868 [31:53<2:22:30,  1.32it/s]Training epoch 1:  19% 2585/13868 [31:54<2:22:36,  1.32it/s]Training epoch 1:  19% 2586/13868 [31:55<2:24:17,  1.30it/s]Training epoch 1:  19% 2587/13868 [31:55<2:22:23,  1.32it/s]Training epoch 1:  19% 2588/13868 [31:56<2:21:00,  1.33it/s]Training epoch 1:  19% 2589/13868 [31:57<2:20:57,  1.33it/s]Training epoch 1:  19% 2590/13868 [31:57<2:20:32,  1.34it/s]Training epoch 1:  19% 2591/13868 [31:58<2:20:27,  1.34it/s]Training epoch 1:  19% 2592/13868 [31:59<2:21:00,  1.33it/s]Training epoch 1:  19% 2593/13868 [32:00<2:20:28,  1.34it/s]Training epoch 1:  19% 2594/13868 [32:00<2:21:19,  1.33it/s]Training epoch 1:  19% 2595/13868 [32:01<2:20:46,  1.33it/s]Training epoch 1:  19% 2596/13868 [32:02<2:23:19,  1.31it/s]Training epoch 1:  19% 2597/13868 [32:03<2:24:23,  1.30it/s]Training epoch 1:  19% 2598/13868 [32:04<2:23:47,  1.31it/s]Training epoch 1:  19% 2599/13868 [32:04<2:22:22,  1.32it/s]Training epoch 1:  19% 2600/13868 [32:05<2:30:47,  1.25it/s]Training epoch 1:  19% 2601/13868 [32:06<2:26:27,  1.28it/s]Training epoch 1:  19% 2602/13868 [32:07<2:24:10,  1.30it/s]Training epoch 1:  19% 2603/13868 [32:07<2:22:25,  1.32it/s]Training epoch 1:  19% 2604/13868 [32:08<2:22:52,  1.31it/s]Training epoch 1:  19% 2605/13868 [32:09<2:22:24,  1.32it/s]Training epoch 1:  19% 2606/13868 [32:10<2:22:33,  1.32it/s]Training epoch 1:  19% 2607/13868 [32:10<2:23:26,  1.31it/s]Training epoch 1:  19% 2608/13868 [32:11<2:22:04,  1.32it/s]Training epoch 1:  19% 2609/13868 [32:12<2:21:40,  1.32it/s]Training epoch 1:  19% 2610/13868 [32:13<2:22:48,  1.31it/s]Training epoch 1:  19% 2611/13868 [32:13<2:19:53,  1.34it/s]Training epoch 1:  19% 2612/13868 [32:14<2:21:01,  1.33it/s]Training epoch 1:  19% 2613/13868 [32:15<2:21:10,  1.33it/s]Training epoch 1:  19% 2614/13868 [32:16<2:21:03,  1.33it/s]Training epoch 1:  19% 2615/13868 [32:16<2:19:29,  1.34it/s]Training epoch 1:  19% 2616/13868 [32:17<2:20:04,  1.34it/s]Training epoch 1:  19% 2617/13868 [32:18<2:18:46,  1.35it/s]Training epoch 1:  19% 2618/13868 [32:19<2:19:55,  1.34it/s]Training epoch 1:  19% 2619/13868 [32:19<2:22:13,  1.32it/s]Training epoch 1:  19% 2620/13868 [32:20<2:22:05,  1.32it/s]Training epoch 1:  19% 2621/13868 [32:21<2:21:03,  1.33it/s]Training epoch 1:  19% 2622/13868 [32:22<2:22:34,  1.31it/s]Training epoch 1:  19% 2623/13868 [32:22<2:22:09,  1.32it/s]Training epoch 1:  19% 2624/13868 [32:23<2:22:18,  1.32it/s]Training epoch 1:  19% 2625/13868 [32:24<2:22:45,  1.31it/s]Training epoch 1:  19% 2626/13868 [32:25<2:24:27,  1.30it/s]Training epoch 1:  19% 2627/13868 [32:26<2:25:09,  1.29it/s]Training epoch 1:  19% 2628/13868 [32:26<2:25:33,  1.29it/s]Training epoch 1:  19% 2629/13868 [32:27<2:23:26,  1.31it/s]Training epoch 1:  19% 2630/13868 [32:28<2:22:34,  1.31it/s]Training epoch 1:  19% 2631/13868 [32:29<2:21:40,  1.32it/s]Training epoch 1:  19% 2632/13868 [32:29<2:22:03,  1.32it/s]Training epoch 1:  19% 2633/13868 [32:30<2:18:56,  1.35it/s]Training epoch 1:  19% 2634/13868 [32:31<2:20:05,  1.34it/s]Training epoch 1:  19% 2635/13868 [32:32<2:20:24,  1.33it/s]Training epoch 1:  19% 2636/13868 [32:32<2:22:03,  1.32it/s]Training epoch 1:  19% 2637/13868 [32:33<2:20:37,  1.33it/s]Training epoch 1:  19% 2638/13868 [32:34<2:18:51,  1.35it/s]Training epoch 1:  19% 2639/13868 [32:35<2:21:02,  1.33it/s]Training epoch 1:  19% 2640/13868 [32:35<2:20:14,  1.33it/s]Training epoch 1:  19% 2641/13868 [32:36<2:19:49,  1.34it/s]Training epoch 1:  19% 2642/13868 [32:37<2:18:28,  1.35it/s]Training epoch 1:  19% 2643/13868 [32:38<2:19:14,  1.34it/s]Training epoch 1:  19% 2644/13868 [32:38<2:18:47,  1.35it/s]Training epoch 1:  19% 2645/13868 [32:39<2:18:07,  1.35it/s]Training epoch 1:  19% 2646/13868 [32:40<2:17:27,  1.36it/s]Training epoch 1:  19% 2647/13868 [32:40<2:15:13,  1.38it/s]Training epoch 1:  19% 2648/13868 [32:41<2:16:44,  1.37it/s]Training epoch 1:  19% 2649/13868 [32:42<2:19:17,  1.34it/s]Training epoch 1:  19% 2650/13868 [32:43<2:21:25,  1.32it/s]Training epoch 1:  19% 2651/13868 [32:43<2:20:12,  1.33it/s]Training epoch 1:  19% 2652/13868 [32:44<2:20:00,  1.34it/s]Training epoch 1:  19% 2653/13868 [32:45<2:22:39,  1.31it/s]Training epoch 1:  19% 2654/13868 [32:46<2:22:48,  1.31it/s]Training epoch 1:  19% 2655/13868 [32:47<2:21:45,  1.32it/s]Training epoch 1:  19% 2656/13868 [32:47<2:23:13,  1.30it/s]Training epoch 1:  19% 2657/13868 [32:48<2:21:18,  1.32it/s]Training epoch 1:  19% 2658/13868 [32:49<2:22:15,  1.31it/s]Training epoch 1:  19% 2659/13868 [32:50<2:20:12,  1.33it/s]Training epoch 1:  19% 2660/13868 [32:50<2:21:16,  1.32it/s]Training epoch 1:  19% 2661/13868 [32:51<2:22:15,  1.31it/s]Training epoch 1:  19% 2662/13868 [32:52<2:22:46,  1.31it/s]Training epoch 1:  19% 2663/13868 [32:53<2:22:39,  1.31it/s]Training epoch 1:  19% 2664/13868 [32:53<2:23:13,  1.30it/s]Training epoch 1:  19% 2665/13868 [32:54<2:19:08,  1.34it/s]Training epoch 1:  19% 2666/13868 [32:55<2:20:26,  1.33it/s]Training epoch 1:  19% 2667/13868 [32:56<2:20:28,  1.33it/s]Training epoch 1:  19% 2668/13868 [32:56<2:20:14,  1.33it/s]Training epoch 1:  19% 2669/13868 [32:57<2:19:20,  1.34it/s]Training epoch 1:  19% 2670/13868 [32:58<2:19:21,  1.34it/s]Training epoch 1:  19% 2671/13868 [32:59<2:20:28,  1.33it/s]Training epoch 1:  19% 2672/13868 [32:59<2:19:41,  1.34it/s]Training epoch 1:  19% 2673/13868 [33:00<2:19:29,  1.34it/s]Training epoch 1:  19% 2674/13868 [33:01<2:20:32,  1.33it/s]Training epoch 1:  19% 2675/13868 [33:02<2:22:05,  1.31it/s]Training epoch 1:  19% 2676/13868 [33:02<2:22:25,  1.31it/s]Training epoch 1:  19% 2677/13868 [33:03<2:22:06,  1.31it/s]Training epoch 1:  19% 2678/13868 [33:04<2:23:01,  1.30it/s]Training epoch 1:  19% 2679/13868 [33:05<2:22:26,  1.31it/s]Training epoch 1:  19% 2680/13868 [33:05<2:20:46,  1.32it/s]Training epoch 1:  19% 2681/13868 [33:06<2:20:33,  1.33it/s]Training epoch 1:  19% 2682/13868 [33:07<2:21:03,  1.32it/s]Training epoch 1:  19% 2683/13868 [33:08<2:21:51,  1.31it/s]Training epoch 1:  19% 2684/13868 [33:09<2:21:18,  1.32it/s]Training epoch 1:  19% 2685/13868 [33:09<2:22:49,  1.31it/s]Training epoch 1:  19% 2686/13868 [33:10<2:22:38,  1.31it/s]Training epoch 1:  19% 2687/13868 [33:11<2:21:13,  1.32it/s]Training epoch 1:  19% 2688/13868 [33:12<2:21:09,  1.32it/s]Training epoch 1:  19% 2689/13868 [33:12<2:20:05,  1.33it/s]Training epoch 1:  19% 2690/13868 [33:13<2:20:22,  1.33it/s]Training epoch 1:  19% 2691/13868 [33:14<2:20:00,  1.33it/s]Training epoch 1:  19% 2692/13868 [33:15<2:20:54,  1.32it/s]Training epoch 1:  19% 2693/13868 [33:15<2:22:17,  1.31it/s]Training epoch 1:  19% 2694/13868 [33:16<2:23:28,  1.30it/s]Training epoch 1:  19% 2695/13868 [33:17<2:23:06,  1.30it/s]Training epoch 1:  19% 2696/13868 [33:18<2:23:26,  1.30it/s]Training epoch 1:  19% 2697/13868 [33:18<2:23:15,  1.30it/s]Training epoch 1:  19% 2698/13868 [33:19<2:21:53,  1.31it/s]Training epoch 1:  19% 2699/13868 [33:20<2:20:53,  1.32it/s]Training epoch 1:  19% 2700/13868 [33:21<2:28:13,  1.26it/s]Training epoch 1:  19% 2701/13868 [33:22<2:26:32,  1.27it/s]Training epoch 1:  19% 2702/13868 [33:22<2:23:39,  1.30it/s]Training epoch 1:  19% 2703/13868 [33:23<2:24:17,  1.29it/s]Training epoch 1:  19% 2704/13868 [33:24<2:23:47,  1.29it/s]Training epoch 1:  20% 2705/13868 [33:25<2:21:45,  1.31it/s]Training epoch 1:  20% 2706/13868 [33:25<2:20:50,  1.32it/s]Training epoch 1:  20% 2707/13868 [33:26<2:20:22,  1.33it/s]Training epoch 1:  20% 2708/13868 [33:27<2:19:56,  1.33it/s]Training epoch 1:  20% 2709/13868 [33:28<2:21:02,  1.32it/s]Training epoch 1:  20% 2710/13868 [33:28<2:20:58,  1.32it/s]Training epoch 1:  20% 2711/13868 [33:29<2:21:13,  1.32it/s]Training epoch 1:  20% 2712/13868 [33:30<2:19:09,  1.34it/s]Training epoch 1:  20% 2713/13868 [33:31<2:20:25,  1.32it/s]Training epoch 1:  20% 2714/13868 [33:31<2:22:35,  1.30it/s]Training epoch 1:  20% 2715/13868 [33:32<2:23:51,  1.29it/s]Training epoch 1:  20% 2716/13868 [33:33<2:22:32,  1.30it/s]Training epoch 1:  20% 2717/13868 [33:34<2:23:09,  1.30it/s]Training epoch 1:  20% 2718/13868 [33:34<2:20:49,  1.32it/s]Training epoch 1:  20% 2719/13868 [33:35<2:19:43,  1.33it/s]Training epoch 1:  20% 2720/13868 [33:36<2:20:25,  1.32it/s]Training epoch 1:  20% 2721/13868 [33:37<2:20:20,  1.32it/s]Training epoch 1:  20% 2722/13868 [33:37<2:20:41,  1.32it/s]Training epoch 1:  20% 2723/13868 [33:38<2:19:55,  1.33it/s]Training epoch 1:  20% 2724/13868 [33:39<2:21:21,  1.31it/s]Training epoch 1:  20% 2725/13868 [33:40<2:21:34,  1.31it/s]Training epoch 1:  20% 2726/13868 [33:41<2:21:56,  1.31it/s]Training epoch 1:  20% 2727/13868 [33:41<2:20:19,  1.32it/s]Training epoch 1:  20% 2728/13868 [33:42<2:21:35,  1.31it/s]Training epoch 1:  20% 2729/13868 [33:43<2:21:41,  1.31it/s]Training epoch 1:  20% 2730/13868 [33:44<2:21:09,  1.32it/s]Training epoch 1:  20% 2731/13868 [33:44<2:20:11,  1.32it/s]Training epoch 1:  20% 2732/13868 [33:45<2:19:00,  1.34it/s]Training epoch 1:  20% 2733/13868 [33:46<2:19:29,  1.33it/s]Training epoch 1:  20% 2734/13868 [33:47<2:20:53,  1.32it/s]Training epoch 1:  20% 2735/13868 [33:47<2:22:21,  1.30it/s]Training epoch 1:  20% 2736/13868 [33:48<2:22:09,  1.31it/s]Training epoch 1:  20% 2737/13868 [33:49<2:20:16,  1.32it/s]Training epoch 1:  20% 2738/13868 [33:50<2:19:05,  1.33it/s]Training epoch 1:  20% 2739/13868 [33:50<2:19:35,  1.33it/s]Training epoch 1:  20% 2740/13868 [33:51<2:18:08,  1.34it/s]Training epoch 1:  20% 2741/13868 [33:52<2:20:29,  1.32it/s]Training epoch 1:  20% 2742/13868 [33:53<2:19:53,  1.33it/s]Training epoch 1:  20% 2743/13868 [33:53<2:20:36,  1.32it/s]Training epoch 1:  20% 2744/13868 [33:54<2:21:26,  1.31it/s]Training epoch 1:  20% 2745/13868 [33:55<2:19:13,  1.33it/s]Training epoch 1:  20% 2746/13868 [33:56<2:18:56,  1.33it/s]Training epoch 1:  20% 2747/13868 [33:56<2:19:07,  1.33it/s]Training epoch 1:  20% 2748/13868 [33:57<2:16:54,  1.35it/s]Training epoch 1:  20% 2749/13868 [33:58<2:17:52,  1.34it/s]Training epoch 1:  20% 2750/13868 [33:59<2:17:43,  1.35it/s]Training epoch 1:  20% 2751/13868 [33:59<2:18:03,  1.34it/s]Training epoch 1:  20% 2752/13868 [34:00<2:19:43,  1.33it/s]Training epoch 1:  20% 2753/13868 [34:01<2:20:22,  1.32it/s]Training epoch 1:  20% 2754/13868 [34:02<2:19:09,  1.33it/s]Training epoch 1:  20% 2755/13868 [34:02<2:19:44,  1.33it/s]Training epoch 1:  20% 2756/13868 [34:03<2:19:12,  1.33it/s]Training epoch 1:  20% 2757/13868 [34:04<2:18:40,  1.34it/s]Training epoch 1:  20% 2758/13868 [34:05<2:19:06,  1.33it/s]Training epoch 1:  20% 2759/13868 [34:05<2:18:01,  1.34it/s]Training epoch 1:  20% 2760/13868 [34:06<2:17:49,  1.34it/s]Training epoch 1:  20% 2761/13868 [34:07<2:17:22,  1.35it/s]Training epoch 1:  20% 2762/13868 [34:08<2:17:21,  1.35it/s]Training epoch 1:  20% 2763/13868 [34:08<2:18:37,  1.34it/s]Training epoch 1:  20% 2764/13868 [34:09<2:18:31,  1.34it/s]Training epoch 1:  20% 2765/13868 [34:10<2:18:54,  1.33it/s]Training epoch 1:  20% 2766/13868 [34:11<2:20:31,  1.32it/s]Training epoch 1:  20% 2767/13868 [34:11<2:19:37,  1.33it/s]Training epoch 1:  20% 2768/13868 [34:12<2:21:09,  1.31it/s]Training epoch 1:  20% 2769/13868 [34:13<2:23:05,  1.29it/s]Training epoch 1:  20% 2770/13868 [34:14<2:21:38,  1.31it/s]Training epoch 1:  20% 2771/13868 [34:14<2:23:09,  1.29it/s]Training epoch 1:  20% 2772/13868 [34:15<2:22:15,  1.30it/s]Training epoch 1:  20% 2773/13868 [34:16<2:22:55,  1.29it/s]Training epoch 1:  20% 2774/13868 [34:17<2:22:26,  1.30it/s]Training epoch 1:  20% 2775/13868 [34:18<2:20:13,  1.32it/s]Training epoch 1:  20% 2776/13868 [34:18<2:18:14,  1.34it/s]Training epoch 1:  20% 2777/13868 [34:19<2:18:42,  1.33it/s]Training epoch 1:  20% 2778/13868 [34:20<2:18:39,  1.33it/s]Training epoch 1:  20% 2779/13868 [34:20<2:17:49,  1.34it/s]Training epoch 1:  20% 2780/13868 [34:21<2:18:05,  1.34it/s]Training epoch 1:  20% 2781/13868 [34:22<2:18:11,  1.34it/s]Training epoch 1:  20% 2782/13868 [34:23<2:18:35,  1.33it/s]Training epoch 1:  20% 2783/13868 [34:24<2:21:17,  1.31it/s]Training epoch 1:  20% 2784/13868 [34:24<2:21:23,  1.31it/s]Training epoch 1:  20% 2785/13868 [34:25<2:21:01,  1.31it/s]Training epoch 1:  20% 2786/13868 [34:26<2:21:06,  1.31it/s]Training epoch 1:  20% 2787/13868 [34:27<2:21:19,  1.31it/s]Training epoch 1:  20% 2788/13868 [34:27<2:21:50,  1.30it/s]Training epoch 1:  20% 2789/13868 [34:28<2:19:49,  1.32it/s]Training epoch 1:  20% 2790/13868 [34:29<2:19:00,  1.33it/s]Training epoch 1:  20% 2791/13868 [34:30<2:17:47,  1.34it/s]Training epoch 1:  20% 2792/13868 [34:30<2:19:10,  1.33it/s]Training epoch 1:  20% 2793/13868 [34:31<2:19:20,  1.32it/s]Training epoch 1:  20% 2794/13868 [34:32<2:16:40,  1.35it/s]Training epoch 1:  20% 2795/13868 [34:33<2:16:54,  1.35it/s]Training epoch 1:  20% 2796/13868 [34:33<2:16:37,  1.35it/s]Training epoch 1:  20% 2797/13868 [34:34<2:17:30,  1.34it/s]Training epoch 1:  20% 2798/13868 [34:35<2:20:19,  1.31it/s]Training epoch 1:  20% 2799/13868 [34:36<2:19:56,  1.32it/s]Training epoch 1:  20% 2800/13868 [34:37<2:35:44,  1.18it/s]Training epoch 1:  20% 2801/13868 [34:37<2:30:03,  1.23it/s]Training epoch 1:  20% 2802/13868 [34:38<2:25:02,  1.27it/s]Training epoch 1:  20% 2803/13868 [34:39<2:21:52,  1.30it/s]Training epoch 1:  20% 2804/13868 [34:40<2:21:28,  1.30it/s]Training epoch 1:  20% 2805/13868 [34:40<2:21:00,  1.31it/s]Training epoch 1:  20% 2806/13868 [34:41<2:22:32,  1.29it/s]Training epoch 1:  20% 2807/13868 [34:42<2:23:12,  1.29it/s]Training epoch 1:  20% 2808/13868 [34:43<2:20:32,  1.31it/s]Training epoch 1:  20% 2809/13868 [34:43<2:21:40,  1.30it/s]Training epoch 1:  20% 2810/13868 [34:44<2:21:17,  1.30it/s]Training epoch 1:  20% 2811/13868 [34:45<2:20:07,  1.32it/s]Training epoch 1:  20% 2812/13868 [34:46<2:20:28,  1.31it/s]Training epoch 1:  20% 2813/13868 [34:47<2:21:55,  1.30it/s]Training epoch 1:  20% 2814/13868 [34:47<2:20:10,  1.31it/s]Training epoch 1:  20% 2815/13868 [34:48<2:21:14,  1.30it/s]Training epoch 1:  20% 2816/13868 [34:49<2:23:37,  1.28it/s]Training epoch 1:  20% 2817/13868 [34:50<2:22:07,  1.30it/s]Training epoch 1:  20% 2818/13868 [34:50<2:22:32,  1.29it/s]Training epoch 1:  20% 2819/13868 [34:51<2:21:59,  1.30it/s]Training epoch 1:  20% 2820/13868 [34:52<2:19:26,  1.32it/s]Training epoch 1:  20% 2821/13868 [34:53<2:18:49,  1.33it/s]Training epoch 1:  20% 2822/13868 [34:53<2:19:07,  1.32it/s]Training epoch 1:  20% 2823/13868 [34:54<2:18:02,  1.33it/s]Training epoch 1:  20% 2824/13868 [34:55<2:18:10,  1.33it/s]Training epoch 1:  20% 2825/13868 [34:56<2:21:12,  1.30it/s]Training epoch 1:  20% 2826/13868 [34:56<2:18:41,  1.33it/s]Training epoch 1:  20% 2827/13868 [34:57<2:20:33,  1.31it/s]Training epoch 1:  20% 2828/13868 [34:58<2:20:29,  1.31it/s]Training epoch 1:  20% 2829/13868 [34:59<2:21:01,  1.30it/s]Training epoch 1:  20% 2830/13868 [35:00<2:21:54,  1.30it/s]Training epoch 1:  20% 2831/13868 [35:00<2:20:50,  1.31it/s]Training epoch 1:  20% 2832/13868 [35:01<2:20:50,  1.31it/s]Training epoch 1:  20% 2833/13868 [35:02<2:20:27,  1.31it/s]Training epoch 1:  20% 2834/13868 [35:03<2:18:54,  1.32it/s]Training epoch 1:  20% 2835/13868 [35:03<2:18:48,  1.32it/s]Training epoch 1:  20% 2836/13868 [35:04<2:20:38,  1.31it/s]Training epoch 1:  20% 2837/13868 [35:05<2:21:12,  1.30it/s]Training epoch 1:  20% 2838/13868 [35:06<2:20:40,  1.31it/s]Training epoch 1:  20% 2839/13868 [35:06<2:20:02,  1.31it/s]Training epoch 1:  20% 2840/13868 [35:07<2:19:45,  1.32it/s]Training epoch 1:  20% 2841/13868 [35:08<2:19:24,  1.32it/s]Training epoch 1:  20% 2842/13868 [35:09<2:18:45,  1.32it/s]Training epoch 1:  21% 2843/13868 [35:09<2:17:45,  1.33it/s]Training epoch 1:  21% 2844/13868 [35:10<2:17:45,  1.33it/s]Training epoch 1:  21% 2845/13868 [35:11<2:18:20,  1.33it/s]Training epoch 1:  21% 2846/13868 [35:12<2:19:35,  1.32it/s]Training epoch 1:  21% 2847/13868 [35:12<2:19:04,  1.32it/s]Training epoch 1:  21% 2848/13868 [35:13<2:18:13,  1.33it/s]Training epoch 1:  21% 2849/13868 [35:14<2:17:03,  1.34it/s]Training epoch 1:  21% 2850/13868 [35:15<2:15:29,  1.36it/s]Training epoch 1:  21% 2851/13868 [35:15<2:15:26,  1.36it/s]Training epoch 1:  21% 2852/13868 [35:16<2:13:53,  1.37it/s]Training epoch 1:  21% 2853/13868 [35:17<2:14:43,  1.36it/s]Training epoch 1:  21% 2854/13868 [35:18<2:16:25,  1.35it/s]Training epoch 1:  21% 2855/13868 [35:18<2:17:31,  1.33it/s]Training epoch 1:  21% 2856/13868 [35:19<2:19:42,  1.31it/s]Training epoch 1:  21% 2857/13868 [35:20<2:19:26,  1.32it/s]Training epoch 1:  21% 2858/13868 [35:21<2:20:28,  1.31it/s]Training epoch 1:  21% 2859/13868 [35:21<2:19:02,  1.32it/s]Training epoch 1:  21% 2860/13868 [35:22<2:19:14,  1.32it/s]Training epoch 1:  21% 2861/13868 [35:23<2:20:05,  1.31it/s]Training epoch 1:  21% 2862/13868 [35:24<2:19:08,  1.32it/s]Training epoch 1:  21% 2863/13868 [35:24<2:17:50,  1.33it/s]Training epoch 1:  21% 2864/13868 [35:25<2:18:08,  1.33it/s]Training epoch 1:  21% 2865/13868 [35:26<2:17:51,  1.33it/s]Training epoch 1:  21% 2866/13868 [35:27<2:17:38,  1.33it/s]Training epoch 1:  21% 2867/13868 [35:27<2:16:49,  1.34it/s]Training epoch 1:  21% 2868/13868 [35:28<2:17:28,  1.33it/s]Training epoch 1:  21% 2869/13868 [35:29<2:18:31,  1.32it/s]Training epoch 1:  21% 2870/13868 [35:30<2:18:28,  1.32it/s]Training epoch 1:  21% 2871/13868 [35:30<2:19:38,  1.31it/s]Training epoch 1:  21% 2872/13868 [35:31<2:20:42,  1.30it/s]Training epoch 1:  21% 2873/13868 [35:32<2:22:34,  1.29it/s]Training epoch 1:  21% 2874/13868 [35:33<2:21:49,  1.29it/s]Training epoch 1:  21% 2875/13868 [35:34<2:22:12,  1.29it/s]Training epoch 1:  21% 2876/13868 [35:34<2:20:29,  1.30it/s]Training epoch 1:  21% 2877/13868 [35:35<2:19:54,  1.31it/s]Training epoch 1:  21% 2878/13868 [35:36<2:16:44,  1.34it/s]Training epoch 1:  21% 2879/13868 [35:37<2:17:37,  1.33it/s]Training epoch 1:  21% 2880/13868 [35:37<2:16:47,  1.34it/s]Training epoch 1:  21% 2881/13868 [35:38<2:16:35,  1.34it/s]Training epoch 1:  21% 2882/13868 [35:39<2:18:24,  1.32it/s]Training epoch 1:  21% 2883/13868 [35:40<2:19:05,  1.32it/s]Training epoch 1:  21% 2884/13868 [35:40<2:18:25,  1.32it/s]Training epoch 1:  21% 2885/13868 [35:41<2:20:08,  1.31it/s]Training epoch 1:  21% 2886/13868 [35:42<2:21:17,  1.30it/s]Training epoch 1:  21% 2887/13868 [35:43<2:19:09,  1.32it/s]Training epoch 1:  21% 2888/13868 [35:43<2:19:26,  1.31it/s]Training epoch 1:  21% 2889/13868 [35:44<2:18:46,  1.32it/s]Training epoch 1:  21% 2890/13868 [35:45<2:18:07,  1.32it/s]Training epoch 1:  21% 2891/13868 [35:46<2:16:32,  1.34it/s]Training epoch 1:  21% 2892/13868 [35:46<2:16:13,  1.34it/s]Training epoch 1:  21% 2893/13868 [35:47<2:15:56,  1.35it/s]Training epoch 1:  21% 2894/13868 [35:48<2:17:19,  1.33it/s]Training epoch 1:  21% 2895/13868 [35:49<2:17:37,  1.33it/s]Training epoch 1:  21% 2896/13868 [35:49<2:16:54,  1.34it/s]Training epoch 1:  21% 2897/13868 [35:50<2:18:56,  1.32it/s]Training epoch 1:  21% 2898/13868 [35:51<2:18:36,  1.32it/s]Training epoch 1:  21% 2899/13868 [35:52<2:16:42,  1.34it/s]Training epoch 1:  21% 2900/13868 [35:52<2:24:21,  1.27it/s]Training epoch 1:  21% 2901/13868 [35:53<2:23:46,  1.27it/s]Training epoch 1:  21% 2902/13868 [35:54<2:19:43,  1.31it/s]Training epoch 1:  21% 2903/13868 [35:55<2:20:25,  1.30it/s]Training epoch 1:  21% 2904/13868 [35:56<2:19:08,  1.31it/s]Training epoch 1:  21% 2905/13868 [35:56<2:19:29,  1.31it/s]Training epoch 1:  21% 2906/13868 [35:57<2:18:34,  1.32it/s]Training epoch 1:  21% 2907/13868 [35:58<2:19:17,  1.31it/s]Training epoch 1:  21% 2908/13868 [35:59<2:17:23,  1.33it/s]Training epoch 1:  21% 2909/13868 [35:59<2:18:03,  1.32it/s]Training epoch 1:  21% 2910/13868 [36:00<2:20:40,  1.30it/s]Training epoch 1:  21% 2911/13868 [36:01<2:21:22,  1.29it/s]Training epoch 1:  21% 2912/13868 [36:02<2:19:52,  1.31it/s]Training epoch 1:  21% 2913/13868 [36:02<2:17:31,  1.33it/s]Training epoch 1:  21% 2914/13868 [36:03<2:16:48,  1.33it/s]Training epoch 1:  21% 2915/13868 [36:04<2:17:34,  1.33it/s]Training epoch 1:  21% 2916/13868 [36:05<2:18:04,  1.32it/s]Training epoch 1:  21% 2917/13868 [36:05<2:17:57,  1.32it/s]Training epoch 1:  21% 2918/13868 [36:06<2:16:49,  1.33it/s]Training epoch 1:  21% 2919/13868 [36:07<2:16:27,  1.34it/s]Training epoch 1:  21% 2920/13868 [36:08<2:16:27,  1.34it/s]Training epoch 1:  21% 2921/13868 [36:08<2:16:49,  1.33it/s]Training epoch 1:  21% 2922/13868 [36:09<2:16:08,  1.34it/s]Training epoch 1:  21% 2923/13868 [36:10<2:16:29,  1.34it/s]Training epoch 1:  21% 2924/13868 [36:11<2:16:48,  1.33it/s]Training epoch 1:  21% 2925/13868 [36:11<2:16:57,  1.33it/s]Training epoch 1:  21% 2926/13868 [36:12<2:16:13,  1.34it/s]Training epoch 1:  21% 2927/13868 [36:13<2:15:40,  1.34it/s]Training epoch 1:  21% 2928/13868 [36:14<2:16:11,  1.34it/s]Training epoch 1:  21% 2929/13868 [36:14<2:15:23,  1.35it/s]Training epoch 1:  21% 2930/13868 [36:15<2:17:30,  1.33it/s]Training epoch 1:  21% 2931/13868 [36:16<2:17:15,  1.33it/s]Training epoch 1:  21% 2932/13868 [36:17<2:18:17,  1.32it/s]Training epoch 1:  21% 2933/13868 [36:17<2:16:39,  1.33it/s]Training epoch 1:  21% 2934/13868 [36:18<2:14:50,  1.35it/s]Training epoch 1:  21% 2935/13868 [36:19<2:13:24,  1.37it/s]Training epoch 1:  21% 2936/13868 [36:20<2:13:35,  1.36it/s]Training epoch 1:  21% 2937/13868 [36:20<2:15:29,  1.34it/s]Training epoch 1:  21% 2938/13868 [36:21<2:15:25,  1.35it/s]Training epoch 1:  21% 2939/13868 [36:22<2:16:12,  1.34it/s]Training epoch 1:  21% 2940/13868 [36:23<2:15:47,  1.34it/s]Training epoch 1:  21% 2941/13868 [36:23<2:16:03,  1.34it/s]Training epoch 1:  21% 2942/13868 [36:24<2:14:09,  1.36it/s]Training epoch 1:  21% 2943/13868 [36:25<2:13:32,  1.36it/s]Training epoch 1:  21% 2944/13868 [36:25<2:12:26,  1.37it/s]Training epoch 1:  21% 2945/13868 [36:26<2:13:33,  1.36it/s]Training epoch 1:  21% 2946/13868 [36:27<2:13:51,  1.36it/s]Training epoch 1:  21% 2947/13868 [36:28<2:14:19,  1.36it/s]Training epoch 1:  21% 2948/13868 [36:28<2:13:37,  1.36it/s]Training epoch 1:  21% 2949/13868 [36:29<2:14:36,  1.35it/s]Training epoch 1:  21% 2950/13868 [36:30<2:12:22,  1.37it/s]Training epoch 1:  21% 2951/13868 [36:31<2:15:03,  1.35it/s]Training epoch 1:  21% 2952/13868 [36:31<2:13:48,  1.36it/s]Training epoch 1:  21% 2953/13868 [36:32<2:17:23,  1.32it/s]Training epoch 1:  21% 2954/13868 [36:33<2:18:01,  1.32it/s]Training epoch 1:  21% 2955/13868 [36:34<2:18:56,  1.31it/s]Training epoch 1:  21% 2956/13868 [36:34<2:15:28,  1.34it/s]Training epoch 1:  21% 2957/13868 [36:35<2:16:37,  1.33it/s]Training epoch 1:  21% 2958/13868 [36:36<2:18:30,  1.31it/s]Training epoch 1:  21% 2959/13868 [36:37<2:19:25,  1.30it/s]Training epoch 1:  21% 2960/13868 [36:37<2:17:08,  1.33it/s]Training epoch 1:  21% 2961/13868 [36:38<2:17:12,  1.32it/s]Training epoch 1:  21% 2962/13868 [36:39<2:18:03,  1.32it/s]Training epoch 1:  21% 2963/13868 [36:40<2:18:43,  1.31it/s]Training epoch 1:  21% 2964/13868 [36:40<2:17:01,  1.33it/s]Training epoch 1:  21% 2965/13868 [36:41<2:19:44,  1.30it/s]Training epoch 1:  21% 2966/13868 [36:42<2:18:04,  1.32it/s]Training epoch 1:  21% 2967/13868 [36:43<2:19:43,  1.30it/s]Training epoch 1:  21% 2968/13868 [36:44<2:18:25,  1.31it/s]Training epoch 1:  21% 2969/13868 [36:44<2:19:17,  1.30it/s]Training epoch 1:  21% 2970/13868 [36:45<2:17:25,  1.32it/s]Training epoch 1:  21% 2971/13868 [36:46<2:16:58,  1.33it/s]Training epoch 1:  21% 2972/13868 [36:47<2:16:17,  1.33it/s]Training epoch 1:  21% 2973/13868 [36:47<2:16:34,  1.33it/s]Training epoch 1:  21% 2974/13868 [36:48<2:14:54,  1.35it/s]Training epoch 1:  21% 2975/13868 [36:49<2:16:03,  1.33it/s]Training epoch 1:  21% 2976/13868 [36:50<2:15:39,  1.34it/s]Training epoch 1:  21% 2977/13868 [36:50<2:15:36,  1.34it/s]Training epoch 1:  21% 2978/13868 [36:51<2:15:05,  1.34it/s]Training epoch 1:  21% 2979/13868 [36:52<2:15:23,  1.34it/s]Training epoch 1:  21% 2980/13868 [36:52<2:14:36,  1.35it/s]Training epoch 1:  21% 2981/13868 [36:53<2:15:28,  1.34it/s]Training epoch 1:  22% 2982/13868 [36:54<2:15:28,  1.34it/s]Training epoch 1:  22% 2983/13868 [36:55<2:14:15,  1.35it/s]Training epoch 1:  22% 2984/13868 [36:55<2:14:28,  1.35it/s]Training epoch 1:  22% 2985/13868 [36:56<2:14:58,  1.34it/s]Training epoch 1:  22% 2986/13868 [36:57<2:14:49,  1.35it/s]Training epoch 1:  22% 2987/13868 [36:58<2:16:55,  1.32it/s]Training epoch 1:  22% 2988/13868 [36:58<2:15:48,  1.34it/s]Training epoch 1:  22% 2989/13868 [36:59<2:15:56,  1.33it/s]Training epoch 1:  22% 2990/13868 [37:00<2:15:32,  1.34it/s]Training epoch 1:  22% 2991/13868 [37:01<2:15:51,  1.33it/s]Training epoch 1:  22% 2992/13868 [37:01<2:15:12,  1.34it/s]Training epoch 1:  22% 2993/13868 [37:02<2:16:12,  1.33it/s]Training epoch 1:  22% 2994/13868 [37:03<2:15:45,  1.33it/s]Training epoch 1:  22% 2995/13868 [37:04<2:15:24,  1.34it/s]Training epoch 1:  22% 2996/13868 [37:04<2:13:52,  1.35it/s]Training epoch 1:  22% 2997/13868 [37:05<2:16:37,  1.33it/s]Training epoch 1:  22% 2998/13868 [37:06<2:15:04,  1.34it/s]Training epoch 1:  22% 2999/13868 [37:07<2:17:28,  1.32it/s]Training epoch 1:  22% 3000/13868 [37:08<2:24:27,  1.25it/s]Training epoch 1:  22% 3001/13868 [37:08<2:22:18,  1.27it/s]Training epoch 1:  22% 3002/13868 [37:09<2:18:20,  1.31it/s]Training epoch 1:  22% 3003/13868 [37:10<2:18:13,  1.31it/s]Training epoch 1:  22% 3004/13868 [37:11<2:17:34,  1.32it/s]Training epoch 1:  22% 3005/13868 [37:11<2:18:02,  1.31it/s]Training epoch 1:  22% 3006/13868 [37:12<2:17:24,  1.32it/s]Training epoch 1:  22% 3007/13868 [37:13<2:19:24,  1.30it/s]Training epoch 1:  22% 3008/13868 [37:14<2:17:51,  1.31it/s]Training epoch 1:  22% 3009/13868 [37:14<2:18:15,  1.31it/s]Training epoch 1:  22% 3010/13868 [37:15<2:16:00,  1.33it/s]Training epoch 1:  22% 3011/13868 [37:16<2:15:14,  1.34it/s]Training epoch 1:  22% 3012/13868 [37:17<2:14:51,  1.34it/s]Training epoch 1:  22% 3013/13868 [37:17<2:14:28,  1.35it/s]Training epoch 1:  22% 3014/13868 [37:18<2:12:36,  1.36it/s]Training epoch 1:  22% 3015/13868 [37:19<2:14:08,  1.35it/s]Training epoch 1:  22% 3016/13868 [37:20<2:14:42,  1.34it/s]Training epoch 1:  22% 3017/13868 [37:20<2:15:19,  1.34it/s]Training epoch 1:  22% 3018/13868 [37:21<2:14:23,  1.35it/s]Training epoch 1:  22% 3019/13868 [37:22<2:15:54,  1.33it/s]Training epoch 1:  22% 3020/13868 [37:23<2:17:28,  1.32it/s]Training epoch 1:  22% 3021/13868 [37:23<2:16:33,  1.32it/s]Training epoch 1:  22% 3022/13868 [37:24<2:15:12,  1.34it/s]Training epoch 1:  22% 3023/13868 [37:25<2:14:31,  1.34it/s]Training epoch 1:  22% 3024/13868 [37:26<2:13:14,  1.36it/s]Training epoch 1:  22% 3025/13868 [37:26<2:15:13,  1.34it/s]Training epoch 1:  22% 3026/13868 [37:27<2:15:16,  1.34it/s]Training epoch 1:  22% 3027/13868 [37:28<2:14:49,  1.34it/s]Training epoch 1:  22% 3028/13868 [37:29<2:12:24,  1.36it/s]Training epoch 1:  22% 3029/13868 [37:29<2:13:13,  1.36it/s]Training epoch 1:  22% 3030/13868 [37:30<2:14:05,  1.35it/s]Training epoch 1:  22% 3031/13868 [37:31<2:15:22,  1.33it/s]Training epoch 1:  22% 3032/13868 [37:32<2:14:41,  1.34it/s]Training epoch 1:  22% 3033/13868 [37:32<2:16:58,  1.32it/s]Training epoch 1:  22% 3034/13868 [37:33<2:15:57,  1.33it/s]Training epoch 1:  22% 3035/13868 [37:34<2:16:40,  1.32it/s]Training epoch 1:  22% 3036/13868 [37:35<2:17:48,  1.31it/s]Training epoch 1:  22% 3037/13868 [37:35<2:15:24,  1.33it/s]Training epoch 1:  22% 3038/13868 [37:36<2:14:57,  1.34it/s]Training epoch 1:  22% 3039/13868 [37:37<2:15:19,  1.33it/s]Training epoch 1:  22% 3040/13868 [37:38<2:13:34,  1.35it/s]Training epoch 1:  22% 3041/13868 [37:38<2:13:26,  1.35it/s]Training epoch 1:  22% 3042/13868 [37:39<2:14:00,  1.35it/s]Training epoch 1:  22% 3043/13868 [37:40<2:16:26,  1.32it/s]Training epoch 1:  22% 3044/13868 [37:41<2:16:59,  1.32it/s]Training epoch 1:  22% 3045/13868 [37:41<2:14:58,  1.34it/s]Training epoch 1:  22% 3046/13868 [37:42<2:13:52,  1.35it/s]Training epoch 1:  22% 3047/13868 [37:43<2:15:47,  1.33it/s]Training epoch 1:  22% 3048/13868 [37:44<2:15:33,  1.33it/s]Training epoch 1:  22% 3049/13868 [37:44<2:17:29,  1.31it/s]Training epoch 1:  22% 3050/13868 [37:45<2:16:39,  1.32it/s]Training epoch 1:  22% 3051/13868 [37:46<2:18:59,  1.30it/s]Training epoch 1:  22% 3052/13868 [37:47<2:16:45,  1.32it/s]Training epoch 1:  22% 3053/13868 [37:47<2:14:51,  1.34it/s]Training epoch 1:  22% 3054/13868 [37:48<2:13:32,  1.35it/s]Training epoch 1:  22% 3055/13868 [37:49<2:15:13,  1.33it/s]Training epoch 1:  22% 3056/13868 [37:50<2:13:35,  1.35it/s]Training epoch 1:  22% 3057/13868 [37:50<2:14:47,  1.34it/s]Training epoch 1:  22% 3058/13868 [37:51<2:14:56,  1.34it/s]Training epoch 1:  22% 3059/13868 [37:52<2:14:52,  1.34it/s]Training epoch 1:  22% 3060/13868 [37:53<2:14:08,  1.34it/s]Training epoch 1:  22% 3061/13868 [37:53<2:15:17,  1.33it/s]Training epoch 1:  22% 3062/13868 [37:54<2:15:10,  1.33it/s]Training epoch 1:  22% 3063/13868 [37:55<2:14:23,  1.34it/s]Training epoch 1:  22% 3064/13868 [37:56<2:13:39,  1.35it/s]Training epoch 1:  22% 3065/13868 [37:56<2:12:20,  1.36it/s]Training epoch 1:  22% 3066/13868 [37:57<2:12:19,  1.36it/s]Training epoch 1:  22% 3067/13868 [37:58<2:12:31,  1.36it/s]Training epoch 1:  22% 3068/13868 [37:58<2:13:02,  1.35it/s]Training epoch 1:  22% 3069/13868 [37:59<2:15:52,  1.32it/s]Training epoch 1:  22% 3070/13868 [38:00<2:16:09,  1.32it/s]Training epoch 1:  22% 3071/13868 [38:01<2:15:33,  1.33it/s]Training epoch 1:  22% 3072/13868 [38:02<2:15:55,  1.32it/s]Training epoch 1:  22% 3073/13868 [38:02<2:14:45,  1.34it/s]Training epoch 1:  22% 3074/13868 [38:03<2:13:26,  1.35it/s]Training epoch 1:  22% 3075/13868 [38:04<2:13:14,  1.35it/s]Training epoch 1:  22% 3076/13868 [38:04<2:11:49,  1.36it/s]Training epoch 1:  22% 3077/13868 [38:05<2:13:34,  1.35it/s]Training epoch 1:  22% 3078/13868 [38:06<2:12:29,  1.36it/s]Training epoch 1:  22% 3079/13868 [38:07<2:13:21,  1.35it/s]Training epoch 1:  22% 3080/13868 [38:07<2:14:30,  1.34it/s]Training epoch 1:  22% 3081/13868 [38:08<2:15:19,  1.33it/s]Training epoch 1:  22% 3082/13868 [38:09<2:14:45,  1.33it/s]Training epoch 1:  22% 3083/13868 [38:10<2:16:19,  1.32it/s]Training epoch 1:  22% 3084/13868 [38:11<2:16:46,  1.31it/s]Training epoch 1:  22% 3085/13868 [38:11<2:17:15,  1.31it/s]Training epoch 1:  22% 3086/13868 [38:12<2:17:06,  1.31it/s]Training epoch 1:  22% 3087/13868 [38:13<2:15:29,  1.33it/s]Training epoch 1:  22% 3088/13868 [38:14<2:15:42,  1.32it/s]Training epoch 1:  22% 3089/13868 [38:14<2:14:56,  1.33it/s]Training epoch 1:  22% 3090/13868 [38:15<2:14:56,  1.33it/s]Training epoch 1:  22% 3091/13868 [38:16<2:14:55,  1.33it/s]Training epoch 1:  22% 3092/13868 [38:17<2:14:40,  1.33it/s]Training epoch 1:  22% 3093/13868 [38:17<2:15:12,  1.33it/s]Training epoch 1:  22% 3094/13868 [38:18<2:13:48,  1.34it/s]Training epoch 1:  22% 3095/13868 [38:19<2:14:45,  1.33it/s]Training epoch 1:  22% 3096/13868 [38:20<2:14:42,  1.33it/s]Training epoch 1:  22% 3097/13868 [38:20<2:13:20,  1.35it/s]Training epoch 1:  22% 3098/13868 [38:21<2:15:16,  1.33it/s]Training epoch 1:  22% 3099/13868 [38:22<2:13:54,  1.34it/s]Training epoch 1:  22% 3100/13868 [38:23<2:22:52,  1.26it/s]Training epoch 1:  22% 3101/13868 [38:23<2:21:43,  1.27it/s]Training epoch 1:  22% 3102/13868 [38:24<2:21:46,  1.27it/s]Training epoch 1:  22% 3103/13868 [38:25<2:19:40,  1.28it/s]Training epoch 1:  22% 3104/13868 [38:26<2:17:34,  1.30it/s]Training epoch 1:  22% 3105/13868 [38:26<2:15:15,  1.33it/s]Training epoch 1:  22% 3106/13868 [38:27<2:13:04,  1.35it/s]Training epoch 1:  22% 3107/13868 [38:28<2:13:13,  1.35it/s]Training epoch 1:  22% 3108/13868 [38:29<2:13:40,  1.34it/s]Training epoch 1:  22% 3109/13868 [38:29<2:14:39,  1.33it/s]Training epoch 1:  22% 3110/13868 [38:30<2:14:54,  1.33it/s]Training epoch 1:  22% 3111/13868 [38:31<2:14:55,  1.33it/s]Training epoch 1:  22% 3112/13868 [38:32<2:15:52,  1.32it/s]Training epoch 1:  22% 3113/13868 [38:32<2:13:30,  1.34it/s]Training epoch 1:  22% 3114/13868 [38:33<2:13:45,  1.34it/s]Training epoch 1:  22% 3115/13868 [38:34<2:14:54,  1.33it/s]Training epoch 1:  22% 3116/13868 [38:35<2:14:05,  1.34it/s]Training epoch 1:  22% 3117/13868 [38:35<2:17:01,  1.31it/s]Training epoch 1:  22% 3118/13868 [38:36<2:17:05,  1.31it/s]Training epoch 1:  22% 3119/13868 [38:37<2:17:30,  1.30it/s]Training epoch 1:  22% 3120/13868 [38:38<2:15:28,  1.32it/s]Training epoch 1:  23% 3121/13868 [38:39<2:16:06,  1.32it/s]Training epoch 1:  23% 3122/13868 [38:39<2:13:27,  1.34it/s]Training epoch 1:  23% 3123/13868 [38:40<2:13:34,  1.34it/s]Training epoch 1:  23% 3124/13868 [38:41<2:11:51,  1.36it/s]Training epoch 1:  23% 3125/13868 [38:41<2:12:53,  1.35it/s]Training epoch 1:  23% 3126/13868 [38:42<2:11:38,  1.36it/s]Training epoch 1:  23% 3127/13868 [38:43<2:14:02,  1.34it/s]Training epoch 1:  23% 3128/13868 [38:44<2:11:42,  1.36it/s]Training epoch 1:  23% 3129/13868 [38:44<2:12:14,  1.35it/s]Training epoch 1:  23% 3130/13868 [38:45<2:12:55,  1.35it/s]Training epoch 1:  23% 3131/13868 [38:46<2:11:43,  1.36it/s]Training epoch 1:  23% 3132/13868 [38:47<2:11:23,  1.36it/s]Training epoch 1:  23% 3133/13868 [38:47<2:14:39,  1.33it/s]Training epoch 1:  23% 3134/13868 [38:48<2:13:09,  1.34it/s]Training epoch 1:  23% 3135/13868 [38:49<2:14:23,  1.33it/s]Training epoch 1:  23% 3136/13868 [38:50<2:14:01,  1.33it/s]Training epoch 1:  23% 3137/13868 [38:50<2:16:05,  1.31it/s]Training epoch 1:  23% 3138/13868 [38:51<2:16:32,  1.31it/s]Training epoch 1:  23% 3139/13868 [38:52<2:16:19,  1.31it/s]Training epoch 1:  23% 3140/13868 [38:53<2:12:58,  1.34it/s]Training epoch 1:  23% 3141/13868 [38:53<2:13:48,  1.34it/s]Training epoch 1:  23% 3142/13868 [38:54<2:12:33,  1.35it/s]Training epoch 1:  23% 3143/13868 [38:55<2:11:16,  1.36it/s]Training epoch 1:  23% 3144/13868 [38:56<2:13:05,  1.34it/s]Training epoch 1:  23% 3145/13868 [38:56<2:15:03,  1.32it/s]Training epoch 1:  23% 3146/13868 [38:57<2:16:00,  1.31it/s]Training epoch 1:  23% 3147/13868 [38:58<2:17:00,  1.30it/s]Training epoch 1:  23% 3148/13868 [38:59<2:16:02,  1.31it/s]Training epoch 1:  23% 3149/13868 [38:59<2:15:06,  1.32it/s]Training epoch 1:  23% 3150/13868 [39:00<2:15:38,  1.32it/s]Training epoch 1:  23% 3151/13868 [39:01<2:15:31,  1.32it/s]Training epoch 1:  23% 3152/13868 [39:02<2:15:12,  1.32it/s]Training epoch 1:  23% 3153/13868 [39:02<2:14:20,  1.33it/s]Training epoch 1:  23% 3154/13868 [39:03<2:13:46,  1.33it/s]Training epoch 1:  23% 3155/13868 [39:04<2:12:12,  1.35it/s]Training epoch 1:  23% 3156/13868 [39:05<2:13:24,  1.34it/s]Training epoch 1:  23% 3157/13868 [39:05<2:14:58,  1.32it/s]Training epoch 1:  23% 3158/13868 [39:06<2:14:03,  1.33it/s]Training epoch 1:  23% 3159/13868 [39:07<2:14:26,  1.33it/s]Training epoch 1:  23% 3160/13868 [39:08<2:13:26,  1.34it/s]Training epoch 1:  23% 3161/13868 [39:08<2:12:17,  1.35it/s]Training epoch 1:  23% 3162/13868 [39:09<2:11:05,  1.36it/s]Training epoch 1:  23% 3163/13868 [39:10<2:12:15,  1.35it/s]Training epoch 1:  23% 3164/13868 [39:11<2:14:57,  1.32it/s]Training epoch 1:  23% 3165/13868 [39:11<2:13:43,  1.33it/s]Training epoch 1:  23% 3166/13868 [39:12<2:13:36,  1.34it/s]Training epoch 1:  23% 3167/13868 [39:13<2:12:17,  1.35it/s]Training epoch 1:  23% 3168/13868 [39:14<2:13:41,  1.33it/s]Training epoch 1:  23% 3169/13868 [39:14<2:13:50,  1.33it/s]Training epoch 1:  23% 3170/13868 [39:15<2:13:02,  1.34it/s]Training epoch 1:  23% 3171/13868 [39:16<2:12:16,  1.35it/s]Training epoch 1:  23% 3172/13868 [39:17<2:12:10,  1.35it/s]Training epoch 1:  23% 3173/13868 [39:17<2:11:55,  1.35it/s]Training epoch 1:  23% 3174/13868 [39:18<2:12:07,  1.35it/s]Training epoch 1:  23% 3175/13868 [39:19<2:14:28,  1.33it/s]Training epoch 1:  23% 3176/13868 [39:20<2:14:14,  1.33it/s]Training epoch 1:  23% 3177/13868 [39:20<2:14:22,  1.33it/s]Training epoch 1:  23% 3178/13868 [39:21<2:14:36,  1.32it/s]Training epoch 1:  23% 3179/13868 [39:22<2:16:23,  1.31it/s]Training epoch 1:  23% 3180/13868 [39:23<2:14:23,  1.33it/s]Training epoch 1:  23% 3181/13868 [39:23<2:12:40,  1.34it/s]Training epoch 1:  23% 3182/13868 [39:24<2:13:37,  1.33it/s]Training epoch 1:  23% 3183/13868 [39:25<2:14:20,  1.33it/s]Training epoch 1:  23% 3184/13868 [39:26<2:14:40,  1.32it/s]Training epoch 1:  23% 3185/13868 [39:26<2:15:54,  1.31it/s]Training epoch 1:  23% 3186/13868 [39:27<2:14:34,  1.32it/s]Training epoch 1:  23% 3187/13868 [39:28<2:15:41,  1.31it/s]Training epoch 1:  23% 3188/13868 [39:29<2:14:56,  1.32it/s]Training epoch 1:  23% 3189/13868 [39:29<2:13:42,  1.33it/s]Training epoch 1:  23% 3190/13868 [39:30<2:13:14,  1.34it/s]Training epoch 1:  23% 3191/13868 [39:31<2:14:21,  1.32it/s]Training epoch 1:  23% 3192/13868 [39:32<2:13:18,  1.33it/s]Training epoch 1:  23% 3193/13868 [39:32<2:13:32,  1.33it/s]Training epoch 1:  23% 3194/13868 [39:33<2:11:48,  1.35it/s]Training epoch 1:  23% 3195/13868 [39:34<2:11:57,  1.35it/s]Training epoch 1:  23% 3196/13868 [39:35<2:10:21,  1.36it/s]Training epoch 1:  23% 3197/13868 [39:35<2:10:01,  1.37it/s]Training epoch 1:  23% 3198/13868 [39:36<2:12:06,  1.35it/s]Training epoch 1:  23% 3199/13868 [39:37<2:13:56,  1.33it/s]Training epoch 1:  23% 3200/13868 [39:38<2:21:20,  1.26it/s]Training epoch 1:  23% 3201/13868 [39:39<2:20:13,  1.27it/s]Training epoch 1:  23% 3202/13868 [39:39<2:18:54,  1.28it/s]Training epoch 1:  23% 3203/13868 [39:40<2:16:49,  1.30it/s]Training epoch 1:  23% 3204/13868 [39:41<2:15:15,  1.31it/s]Training epoch 1:  23% 3205/13868 [39:42<2:15:54,  1.31it/s]Training epoch 1:  23% 3206/13868 [39:42<2:15:08,  1.31it/s]Training epoch 1:  23% 3207/13868 [39:43<2:14:37,  1.32it/s]Training epoch 1:  23% 3208/13868 [39:44<2:14:16,  1.32it/s]Training epoch 1:  23% 3209/13868 [39:45<2:14:38,  1.32it/s]Training epoch 1:  23% 3210/13868 [39:45<2:15:38,  1.31it/s]Training epoch 1:  23% 3211/13868 [39:46<2:12:51,  1.34it/s]Training epoch 1:  23% 3212/13868 [39:47<2:13:07,  1.33it/s]Training epoch 1:  23% 3213/13868 [39:48<2:14:30,  1.32it/s]Training epoch 1:  23% 3214/13868 [39:48<2:13:34,  1.33it/s]Training epoch 1:  23% 3215/13868 [39:49<2:13:43,  1.33it/s]Training epoch 1:  23% 3216/13868 [39:50<2:14:39,  1.32it/s]Training epoch 1:  23% 3217/13868 [39:51<2:14:33,  1.32it/s]Training epoch 1:  23% 3218/13868 [39:51<2:14:02,  1.32it/s]Training epoch 1:  23% 3219/13868 [39:52<2:14:33,  1.32it/s]Training epoch 1:  23% 3220/13868 [39:53<2:13:51,  1.33it/s]Training epoch 1:  23% 3221/13868 [39:54<2:14:46,  1.32it/s]Training epoch 1:  23% 3222/13868 [39:54<2:14:28,  1.32it/s]Training epoch 1:  23% 3223/13868 [39:55<2:15:20,  1.31it/s]Training epoch 1:  23% 3224/13868 [39:56<2:14:56,  1.31it/s]Training epoch 1:  23% 3225/13868 [39:57<2:14:18,  1.32it/s]Training epoch 1:  23% 3226/13868 [39:57<2:14:16,  1.32it/s]Training epoch 1:  23% 3227/13868 [39:58<2:13:01,  1.33it/s]Training epoch 1:  23% 3228/13868 [39:59<2:12:57,  1.33it/s]Training epoch 1:  23% 3229/13868 [40:00<2:12:13,  1.34it/s]Training epoch 1:  23% 3230/13868 [40:00<2:11:51,  1.34it/s]Training epoch 1:  23% 3231/13868 [40:01<2:12:21,  1.34it/s]Training epoch 1:  23% 3232/13868 [40:02<2:11:19,  1.35it/s]Training epoch 1:  23% 3233/13868 [40:03<2:11:48,  1.34it/s]Training epoch 1:  23% 3234/13868 [40:03<2:11:41,  1.35it/s]Training epoch 1:  23% 3235/13868 [40:04<2:12:20,  1.34it/s]Training epoch 1:  23% 3236/13868 [40:05<2:13:05,  1.33it/s]Training epoch 1:  23% 3237/13868 [40:06<2:13:24,  1.33it/s]Training epoch 1:  23% 3238/13868 [40:06<2:13:50,  1.32it/s]Training epoch 1:  23% 3239/13868 [40:07<2:13:30,  1.33it/s]Training epoch 1:  23% 3240/13868 [40:08<2:11:30,  1.35it/s]Training epoch 1:  23% 3241/13868 [40:09<2:12:20,  1.34it/s]Training epoch 1:  23% 3242/13868 [40:09<2:12:08,  1.34it/s]Training epoch 1:  23% 3243/13868 [40:10<2:12:10,  1.34it/s]Training epoch 1:  23% 3244/13868 [40:11<2:10:59,  1.35it/s]Training epoch 1:  23% 3245/13868 [40:12<2:13:06,  1.33it/s]Training epoch 1:  23% 3246/13868 [40:12<2:12:19,  1.34it/s]Training epoch 1:  23% 3247/13868 [40:13<2:11:54,  1.34it/s]Training epoch 1:  23% 3248/13868 [40:14<2:11:18,  1.35it/s]Training epoch 1:  23% 3249/13868 [40:15<2:11:24,  1.35it/s]Training epoch 1:  23% 3250/13868 [40:15<2:10:57,  1.35it/s]Training epoch 1:  23% 3251/13868 [40:16<2:12:16,  1.34it/s]Training epoch 1:  23% 3252/13868 [40:17<2:10:53,  1.35it/s]Training epoch 1:  23% 3253/13868 [40:18<2:11:23,  1.35it/s]Training epoch 1:  23% 3254/13868 [40:18<2:11:56,  1.34it/s]Training epoch 1:  23% 3255/13868 [40:19<2:14:14,  1.32it/s]Training epoch 1:  23% 3256/13868 [40:20<2:12:10,  1.34it/s]Training epoch 1:  23% 3257/13868 [40:21<2:12:43,  1.33it/s]Training epoch 1:  23% 3258/13868 [40:21<2:12:08,  1.34it/s]Training epoch 1:  24% 3259/13868 [40:22<2:12:12,  1.34it/s]Training epoch 1:  24% 3260/13868 [40:23<2:12:13,  1.34it/s]Training epoch 1:  24% 3261/13868 [40:24<2:12:40,  1.33it/s]Training epoch 1:  24% 3262/13868 [40:24<2:12:06,  1.34it/s]Training epoch 1:  24% 3263/13868 [40:25<2:14:16,  1.32it/s]Training epoch 1:  24% 3264/13868 [40:26<2:13:37,  1.32it/s]Training epoch 1:  24% 3265/13868 [40:27<2:12:12,  1.34it/s]Training epoch 1:  24% 3266/13868 [40:27<2:11:12,  1.35it/s]Training epoch 1:  24% 3267/13868 [40:28<2:12:14,  1.34it/s]Training epoch 1:  24% 3268/13868 [40:29<2:12:37,  1.33it/s]Training epoch 1:  24% 3269/13868 [40:30<2:14:07,  1.32it/s]Training epoch 1:  24% 3270/13868 [40:30<2:13:05,  1.33it/s]Training epoch 1:  24% 3271/13868 [40:31<2:12:00,  1.34it/s]Training epoch 1:  24% 3272/13868 [40:32<2:11:48,  1.34it/s]Training epoch 1:  24% 3273/13868 [40:33<2:12:27,  1.33it/s]Training epoch 1:  24% 3274/13868 [40:33<2:10:52,  1.35it/s]Training epoch 1:  24% 3275/13868 [40:34<2:11:26,  1.34it/s]Training epoch 1:  24% 3276/13868 [40:35<2:09:44,  1.36it/s]Training epoch 1:  24% 3277/13868 [40:36<2:11:20,  1.34it/s]Training epoch 1:  24% 3278/13868 [40:36<2:10:55,  1.35it/s]Training epoch 1:  24% 3279/13868 [40:37<2:10:26,  1.35it/s]Training epoch 1:  24% 3280/13868 [40:38<2:10:36,  1.35it/s]Training epoch 1:  24% 3281/13868 [40:39<2:11:40,  1.34it/s]Training epoch 1:  24% 3282/13868 [40:39<2:12:54,  1.33it/s]Training epoch 1:  24% 3283/13868 [40:40<2:14:07,  1.32it/s]Training epoch 1:  24% 3284/13868 [40:41<2:11:33,  1.34it/s]Training epoch 1:  24% 3285/13868 [40:42<2:10:30,  1.35it/s]Training epoch 1:  24% 3286/13868 [40:42<2:10:35,  1.35it/s]Training epoch 1:  24% 3287/13868 [40:43<2:11:08,  1.34it/s]Training epoch 1:  24% 3288/13868 [40:44<2:10:44,  1.35it/s]Training epoch 1:  24% 3289/13868 [40:45<2:12:00,  1.34it/s]Training epoch 1:  24% 3290/13868 [40:45<2:10:57,  1.35it/s]Training epoch 1:  24% 3291/13868 [40:46<2:10:07,  1.35it/s]Training epoch 1:  24% 3292/13868 [40:47<2:11:32,  1.34it/s]Training epoch 1:  24% 3293/13868 [40:48<2:13:08,  1.32it/s]Training epoch 1:  24% 3294/13868 [40:48<2:13:36,  1.32it/s]Training epoch 1:  24% 3295/13868 [40:49<2:12:58,  1.33it/s]Training epoch 1:  24% 3296/13868 [40:50<2:11:02,  1.34it/s]Training epoch 1:  24% 3297/13868 [40:51<2:11:17,  1.34it/s]Training epoch 1:  24% 3298/13868 [40:51<2:11:06,  1.34it/s]Training epoch 1:  24% 3299/13868 [40:52<2:09:39,  1.36it/s]Training epoch 1:  24% 3300/13868 [40:53<2:18:46,  1.27it/s]Training epoch 1:  24% 3301/13868 [40:54<2:17:40,  1.28it/s]Training epoch 1:  24% 3302/13868 [40:54<2:15:29,  1.30it/s]Training epoch 1:  24% 3303/13868 [40:55<2:14:36,  1.31it/s]Training epoch 1:  24% 3304/13868 [40:56<2:13:54,  1.31it/s]Training epoch 1:  24% 3305/13868 [40:57<2:14:13,  1.31it/s]Training epoch 1:  24% 3306/13868 [40:57<2:13:22,  1.32it/s]Training epoch 1:  24% 3307/13868 [40:58<2:12:29,  1.33it/s]Training epoch 1:  24% 3308/13868 [40:59<2:12:55,  1.32it/s]Training epoch 1:  24% 3309/13868 [41:00<2:10:34,  1.35it/s]Training epoch 1:  24% 3310/13868 [41:00<2:09:37,  1.36it/s]Training epoch 1:  24% 3311/13868 [41:01<2:09:52,  1.35it/s]Training epoch 1:  24% 3312/13868 [41:02<2:10:34,  1.35it/s]Training epoch 1:  24% 3313/13868 [41:03<2:10:26,  1.35it/s]Training epoch 1:  24% 3314/13868 [41:03<2:10:33,  1.35it/s]Training epoch 1:  24% 3315/13868 [41:04<2:11:59,  1.33it/s]Training epoch 1:  24% 3316/13868 [41:05<2:10:05,  1.35it/s]Training epoch 1:  24% 3317/13868 [41:06<2:11:28,  1.34it/s]Training epoch 1:  24% 3318/13868 [41:06<2:11:59,  1.33it/s]Training epoch 1:  24% 3319/13868 [41:07<2:12:05,  1.33it/s]Training epoch 1:  24% 3320/13868 [41:08<2:11:20,  1.34it/s]Training epoch 1:  24% 3321/13868 [41:09<2:11:01,  1.34it/s]Training epoch 1:  24% 3322/13868 [41:09<2:09:45,  1.35it/s]Training epoch 1:  24% 3323/13868 [41:10<2:10:33,  1.35it/s]Training epoch 1:  24% 3324/13868 [41:11<2:10:29,  1.35it/s]Training epoch 1:  24% 3325/13868 [41:12<2:12:30,  1.33it/s]Training epoch 1:  24% 3326/13868 [41:12<2:12:55,  1.32it/s]Training epoch 1:  24% 3327/13868 [41:13<2:12:45,  1.32it/s]Training epoch 1:  24% 3328/13868 [41:14<2:11:12,  1.34it/s]Training epoch 1:  24% 3329/13868 [41:15<2:11:50,  1.33it/s]Training epoch 1:  24% 3330/13868 [41:15<2:11:02,  1.34it/s]Training epoch 1:  24% 3331/13868 [41:16<2:09:26,  1.36it/s]Training epoch 1:  24% 3332/13868 [41:17<2:10:28,  1.35it/s]Training epoch 1:  24% 3333/13868 [41:17<2:10:28,  1.35it/s]Training epoch 1:  24% 3334/13868 [41:18<2:09:47,  1.35it/s]Training epoch 1:  24% 3335/13868 [41:19<2:09:34,  1.35it/s]Training epoch 1:  24% 3336/13868 [41:20<2:10:06,  1.35it/s]Training epoch 1:  24% 3337/13868 [41:20<2:11:52,  1.33it/s]Training epoch 1:  24% 3338/13868 [41:21<2:10:37,  1.34it/s]Training epoch 1:  24% 3339/13868 [41:22<2:10:38,  1.34it/s]Training epoch 1:  24% 3340/13868 [41:23<2:09:36,  1.35it/s]Training epoch 1:  24% 3341/13868 [41:23<2:09:45,  1.35it/s]Training epoch 1:  24% 3342/13868 [41:24<2:09:21,  1.36it/s]Training epoch 1:  24% 3343/13868 [41:25<2:09:27,  1.35it/s]Training epoch 1:  24% 3344/13868 [41:26<2:11:54,  1.33it/s]Training epoch 1:  24% 3345/13868 [41:26<2:11:53,  1.33it/s]Training epoch 1:  24% 3346/13868 [41:27<2:10:45,  1.34it/s]Training epoch 1:  24% 3347/13868 [41:28<2:12:19,  1.33it/s]Training epoch 1:  24% 3348/13868 [41:29<2:11:48,  1.33it/s]Training epoch 1:  24% 3349/13868 [41:29<2:10:35,  1.34it/s]Training epoch 1:  24% 3350/13868 [41:30<2:11:26,  1.33it/s]Training epoch 1:  24% 3351/13868 [41:31<2:10:32,  1.34it/s]Training epoch 1:  24% 3352/13868 [41:32<2:11:28,  1.33it/s]Training epoch 1:  24% 3353/13868 [41:32<2:12:10,  1.33it/s]Training epoch 1:  24% 3354/13868 [41:33<2:11:54,  1.33it/s]Training epoch 1:  24% 3355/13868 [41:34<2:09:47,  1.35it/s]Training epoch 1:  24% 3356/13868 [41:35<2:10:49,  1.34it/s]Training epoch 1:  24% 3357/13868 [41:35<2:09:31,  1.35it/s]Training epoch 1:  24% 3358/13868 [41:36<2:09:58,  1.35it/s]Training epoch 1:  24% 3359/13868 [41:37<2:10:17,  1.34it/s]Training epoch 1:  24% 3360/13868 [41:38<2:11:08,  1.34it/s]Training epoch 1:  24% 3361/13868 [41:38<2:10:50,  1.34it/s]Training epoch 1:  24% 3362/13868 [41:39<2:09:42,  1.35it/s]Training epoch 1:  24% 3363/13868 [41:40<2:09:53,  1.35it/s]Training epoch 1:  24% 3364/13868 [41:41<2:10:30,  1.34it/s]Training epoch 1:  24% 3365/13868 [41:41<2:10:33,  1.34it/s]Training epoch 1:  24% 3366/13868 [41:42<2:10:18,  1.34it/s]Training epoch 1:  24% 3367/13868 [41:43<2:11:03,  1.34it/s]Training epoch 1:  24% 3368/13868 [41:44<2:11:17,  1.33it/s]Training epoch 1:  24% 3369/13868 [41:44<2:11:31,  1.33it/s]Training epoch 1:  24% 3370/13868 [41:45<2:10:55,  1.34it/s]Training epoch 1:  24% 3371/13868 [41:46<2:12:28,  1.32it/s]Training epoch 1:  24% 3372/13868 [41:47<2:11:41,  1.33it/s]Training epoch 1:  24% 3373/13868 [41:47<2:12:29,  1.32it/s]Training epoch 1:  24% 3374/13868 [41:48<2:10:42,  1.34it/s]Training epoch 1:  24% 3375/13868 [41:49<2:12:13,  1.32it/s]Training epoch 1:  24% 3376/13868 [41:50<2:10:47,  1.34it/s]Training epoch 1:  24% 3377/13868 [41:50<2:13:20,  1.31it/s]Training epoch 1:  24% 3378/13868 [41:51<2:13:14,  1.31it/s]Training epoch 1:  24% 3379/13868 [41:52<2:13:17,  1.31it/s]Training epoch 1:  24% 3380/13868 [41:53<2:12:33,  1.32it/s]Training epoch 1:  24% 3381/13868 [41:53<2:13:38,  1.31it/s]Training epoch 1:  24% 3382/13868 [41:54<2:12:33,  1.32it/s]Training epoch 1:  24% 3383/13868 [41:55<2:10:19,  1.34it/s]Training epoch 1:  24% 3384/13868 [41:56<2:10:33,  1.34it/s]Training epoch 1:  24% 3385/13868 [41:56<2:10:43,  1.34it/s]Training epoch 1:  24% 3386/13868 [41:57<2:12:12,  1.32it/s]Training epoch 1:  24% 3387/13868 [41:58<2:11:50,  1.32it/s]Training epoch 1:  24% 3388/13868 [41:59<2:10:57,  1.33it/s]Training epoch 1:  24% 3389/13868 [41:59<2:11:46,  1.33it/s]Training epoch 1:  24% 3390/13868 [42:00<2:10:07,  1.34it/s]Training epoch 1:  24% 3391/13868 [42:01<2:10:28,  1.34it/s]Training epoch 1:  24% 3392/13868 [42:02<2:11:02,  1.33it/s]Training epoch 1:  24% 3393/13868 [42:02<2:11:44,  1.33it/s]Training epoch 1:  24% 3394/13868 [42:03<2:13:07,  1.31it/s]Training epoch 1:  24% 3395/13868 [42:04<2:11:32,  1.33it/s]Training epoch 1:  24% 3396/13868 [42:05<2:09:43,  1.35it/s]Training epoch 1:  24% 3397/13868 [42:05<2:11:06,  1.33it/s]Training epoch 1:  25% 3398/13868 [42:06<2:10:49,  1.33it/s]Training epoch 1:  25% 3399/13868 [42:07<2:12:27,  1.32it/s]Training epoch 1:  25% 3400/13868 [42:08<2:18:29,  1.26it/s]Training epoch 1:  25% 3401/13868 [42:09<2:16:42,  1.28it/s]Training epoch 1:  25% 3402/13868 [42:09<2:16:00,  1.28it/s]Training epoch 1:  25% 3403/13868 [42:10<2:14:43,  1.29it/s]Training epoch 1:  25% 3404/13868 [42:11<2:12:06,  1.32it/s]Training epoch 1:  25% 3405/13868 [42:12<2:12:16,  1.32it/s]Training epoch 1:  25% 3406/13868 [42:12<2:10:45,  1.33it/s]Training epoch 1:  25% 3407/13868 [42:13<2:10:42,  1.33it/s]Training epoch 1:  25% 3408/13868 [42:14<2:10:26,  1.34it/s]Training epoch 1:  25% 3409/13868 [42:15<2:10:41,  1.33it/s]Training epoch 1:  25% 3410/13868 [42:15<2:09:26,  1.35it/s]Training epoch 1:  25% 3411/13868 [42:16<2:12:04,  1.32it/s]Training epoch 1:  25% 3412/13868 [42:17<2:12:02,  1.32it/s]Training epoch 1:  25% 3413/13868 [42:18<2:12:44,  1.31it/s]Training epoch 1:  25% 3414/13868 [42:18<2:12:31,  1.31it/s]Training epoch 1:  25% 3415/13868 [42:19<2:11:39,  1.32it/s]Training epoch 1:  25% 3416/13868 [42:20<2:10:55,  1.33it/s]Training epoch 1:  25% 3417/13868 [42:21<2:11:20,  1.33it/s]Training epoch 1:  25% 3418/13868 [42:21<2:10:07,  1.34it/s]Training epoch 1:  25% 3419/13868 [42:22<2:11:25,  1.33it/s]Training epoch 1:  25% 3420/13868 [42:23<2:11:39,  1.32it/s]Training epoch 1:  25% 3421/13868 [42:24<2:10:33,  1.33it/s]Training epoch 1:  25% 3422/13868 [42:24<2:11:07,  1.33it/s]Training epoch 1:  25% 3423/13868 [42:25<2:11:45,  1.32it/s]Training epoch 1:  25% 3424/13868 [42:26<2:09:28,  1.34it/s]Training epoch 1:  25% 3425/13868 [42:27<2:09:55,  1.34it/s]Training epoch 1:  25% 3426/13868 [42:27<2:11:00,  1.33it/s]Training epoch 1:  25% 3427/13868 [42:28<2:11:52,  1.32it/s]Training epoch 1:  25% 3428/13868 [42:29<2:12:18,  1.32it/s]Training epoch 1:  25% 3429/13868 [42:30<2:11:28,  1.32it/s]Training epoch 1:  25% 3430/13868 [42:30<2:10:31,  1.33it/s]Training epoch 1:  25% 3431/13868 [42:31<2:11:21,  1.32it/s]Training epoch 1:  25% 3432/13868 [42:32<2:12:26,  1.31it/s]Training epoch 1:  25% 3433/13868 [42:33<2:11:58,  1.32it/s]Training epoch 1:  25% 3434/13868 [42:33<2:10:01,  1.34it/s]Training epoch 1:  25% 3435/13868 [42:34<2:11:03,  1.33it/s]Training epoch 1:  25% 3436/13868 [42:35<2:12:17,  1.31it/s]Training epoch 1:  25% 3437/13868 [42:36<2:12:09,  1.32it/s]Training epoch 1:  25% 3438/13868 [42:36<2:10:54,  1.33it/s]Training epoch 1:  25% 3439/13868 [42:37<2:10:24,  1.33it/s]Training epoch 1:  25% 3440/13868 [42:38<2:09:01,  1.35it/s]Training epoch 1:  25% 3441/13868 [42:39<2:11:00,  1.33it/s]Training epoch 1:  25% 3442/13868 [42:39<2:09:43,  1.34it/s]Training epoch 1:  25% 3443/13868 [42:40<2:09:35,  1.34it/s]Training epoch 1:  25% 3444/13868 [42:41<2:10:40,  1.33it/s]Training epoch 1:  25% 3445/13868 [42:42<2:11:46,  1.32it/s]Training epoch 1:  25% 3446/13868 [42:42<2:10:29,  1.33it/s]Training epoch 1:  25% 3447/13868 [42:43<2:09:40,  1.34it/s]Training epoch 1:  25% 3448/13868 [42:44<2:09:26,  1.34it/s]Training epoch 1:  25% 3449/13868 [42:45<2:10:21,  1.33it/s]Training epoch 1:  25% 3450/13868 [42:46<2:11:22,  1.32it/s]Training epoch 1:  25% 3451/13868 [42:46<2:10:19,  1.33it/s]Training epoch 1:  25% 3452/13868 [42:47<2:08:33,  1.35it/s]Training epoch 1:  25% 3453/13868 [42:48<2:10:43,  1.33it/s]Training epoch 1:  25% 3454/13868 [42:48<2:09:35,  1.34it/s]Training epoch 1:  25% 3455/13868 [42:49<2:09:46,  1.34it/s]Training epoch 1:  25% 3456/13868 [42:50<2:09:03,  1.34it/s]Training epoch 1:  25% 3457/13868 [42:51<2:10:06,  1.33it/s]Training epoch 1:  25% 3458/13868 [42:51<2:10:19,  1.33it/s]Training epoch 1:  25% 3459/13868 [42:52<2:08:16,  1.35it/s]Training epoch 1:  25% 3460/13868 [42:53<2:07:25,  1.36it/s]Training epoch 1:  25% 3461/13868 [42:54<2:08:51,  1.35it/s]Training epoch 1:  25% 3462/13868 [42:54<2:10:13,  1.33it/s]Training epoch 1:  25% 3463/13868 [42:55<2:12:27,  1.31it/s]Training epoch 1:  25% 3464/13868 [42:56<2:10:52,  1.32it/s]Training epoch 1:  25% 3465/13868 [42:57<2:09:10,  1.34it/s]Training epoch 1:  25% 3466/13868 [42:57<2:10:16,  1.33it/s]Training epoch 1:  25% 3467/13868 [42:58<2:09:32,  1.34it/s]Training epoch 1:  25% 3468/13868 [42:59<2:11:12,  1.32it/s]Training epoch 1:  25% 3469/13868 [43:00<2:12:55,  1.30it/s]Training epoch 1:  25% 3470/13868 [43:01<2:11:25,  1.32it/s]Training epoch 1:  25% 3471/13868 [43:01<2:12:22,  1.31it/s]Training epoch 1:  25% 3472/13868 [43:02<2:13:15,  1.30it/s]Training epoch 1:  25% 3473/13868 [43:03<2:13:31,  1.30it/s]Training epoch 1:  25% 3474/13868 [43:04<2:12:17,  1.31it/s]Training epoch 1:  25% 3475/13868 [43:04<2:13:25,  1.30it/s]Training epoch 1:  25% 3476/13868 [43:05<2:13:35,  1.30it/s]Training epoch 1:  25% 3477/13868 [43:06<2:13:42,  1.30it/s]Training epoch 1:  25% 3478/13868 [43:07<2:12:54,  1.30it/s]Training epoch 1:  25% 3479/13868 [43:07<2:10:41,  1.32it/s]Training epoch 1:  25% 3480/13868 [43:08<2:09:33,  1.34it/s]Training epoch 1:  25% 3481/13868 [43:09<2:11:24,  1.32it/s]Training epoch 1:  25% 3482/13868 [43:10<2:10:04,  1.33it/s]Training epoch 1:  25% 3483/13868 [43:10<2:11:40,  1.31it/s]Training epoch 1:  25% 3484/13868 [43:11<2:10:26,  1.33it/s]Training epoch 1:  25% 3485/13868 [43:12<2:11:14,  1.32it/s]Training epoch 1:  25% 3486/13868 [43:13<2:10:23,  1.33it/s]Training epoch 1:  25% 3487/13868 [43:13<2:10:34,  1.33it/s]Training epoch 1:  25% 3488/13868 [43:14<2:11:09,  1.32it/s]Training epoch 1:  25% 3489/13868 [43:15<2:09:49,  1.33it/s]Training epoch 1:  25% 3490/13868 [43:16<2:10:35,  1.32it/s]Training epoch 1:  25% 3491/13868 [43:16<2:10:52,  1.32it/s]Training epoch 1:  25% 3492/13868 [43:17<2:10:31,  1.32it/s]Training epoch 1:  25% 3493/13868 [43:18<2:08:58,  1.34it/s]Training epoch 1:  25% 3494/13868 [43:19<2:10:46,  1.32it/s]Training epoch 1:  25% 3495/13868 [43:19<2:11:20,  1.32it/s]Training epoch 1:  25% 3496/13868 [43:20<2:12:27,  1.31it/s]Training epoch 1:  25% 3497/13868 [43:21<2:14:25,  1.29it/s]Training epoch 1:  25% 3498/13868 [43:22<2:13:11,  1.30it/s]Training epoch 1:  25% 3499/13868 [43:23<2:13:55,  1.29it/s]Training epoch 1:  25% 3500/13868 [43:24<2:21:15,  1.22it/s]Training epoch 1:  25% 3501/13868 [43:24<2:19:19,  1.24it/s]Training epoch 1:  25% 3502/13868 [43:25<2:18:07,  1.25it/s]Training epoch 1:  25% 3503/13868 [43:26<2:15:47,  1.27it/s]Training epoch 1:  25% 3504/13868 [43:27<2:14:37,  1.28it/s]Training epoch 1:  25% 3505/13868 [43:27<2:12:21,  1.30it/s]Training epoch 1:  25% 3506/13868 [43:28<2:10:49,  1.32it/s]Training epoch 1:  25% 3507/13868 [43:29<2:09:33,  1.33it/s]Training epoch 1:  25% 3508/13868 [43:30<2:09:10,  1.34it/s]Training epoch 1:  25% 3509/13868 [43:30<2:11:02,  1.32it/s]Training epoch 1:  25% 3510/13868 [43:31<2:10:23,  1.32it/s]Training epoch 1:  25% 3511/13868 [43:32<2:09:57,  1.33it/s]Training epoch 1:  25% 3512/13868 [43:33<2:10:14,  1.33it/s]Training epoch 1:  25% 3513/13868 [43:33<2:10:56,  1.32it/s]Training epoch 1:  25% 3514/13868 [43:34<2:12:41,  1.30it/s]Training epoch 1:  25% 3515/13868 [43:35<2:11:35,  1.31it/s]Training epoch 1:  25% 3516/13868 [43:36<2:09:38,  1.33it/s]Training epoch 1:  25% 3517/13868 [43:36<2:09:22,  1.33it/s]Training epoch 1:  25% 3518/13868 [43:37<2:09:32,  1.33it/s]Training epoch 1:  25% 3519/13868 [43:38<2:08:45,  1.34it/s]Training epoch 1:  25% 3520/13868 [43:39<2:11:08,  1.32it/s]Training epoch 1:  25% 3521/13868 [43:39<2:09:11,  1.33it/s]Training epoch 1:  25% 3522/13868 [43:40<2:09:53,  1.33it/s]Training epoch 1:  25% 3523/13868 [43:41<2:10:32,  1.32it/s]Training epoch 1:  25% 3524/13868 [43:42<2:09:20,  1.33it/s]Training epoch 1:  25% 3525/13868 [43:42<2:09:32,  1.33it/s]Training epoch 1:  25% 3526/13868 [43:43<2:11:42,  1.31it/s]Training epoch 1:  25% 3527/13868 [43:44<2:12:46,  1.30it/s]Training epoch 1:  25% 3528/13868 [43:45<2:10:36,  1.32it/s]Training epoch 1:  25% 3529/13868 [43:45<2:08:34,  1.34it/s]Training epoch 1:  25% 3530/13868 [43:46<2:09:40,  1.33it/s]Training epoch 1:  25% 3531/13868 [43:47<2:11:41,  1.31it/s]Training epoch 1:  25% 3532/13868 [43:48<2:09:54,  1.33it/s]Training epoch 1:  25% 3533/13868 [43:49<2:12:02,  1.30it/s]Training epoch 1:  25% 3534/13868 [43:49<2:10:44,  1.32it/s]Training epoch 1:  25% 3535/13868 [43:50<2:11:11,  1.31it/s]Training epoch 1:  25% 3536/13868 [43:51<2:12:19,  1.30it/s]Training epoch 1:  26% 3537/13868 [43:52<2:12:04,  1.30it/s]Training epoch 1:  26% 3538/13868 [43:52<2:10:37,  1.32it/s]Training epoch 1:  26% 3539/13868 [43:53<2:11:07,  1.31it/s]Training epoch 1:  26% 3540/13868 [43:54<2:11:17,  1.31it/s]Training epoch 1:  26% 3541/13868 [43:55<2:11:18,  1.31it/s]Training epoch 1:  26% 3542/13868 [43:55<2:10:17,  1.32it/s]Training epoch 1:  26% 3543/13868 [43:56<2:09:28,  1.33it/s]Training epoch 1:  26% 3544/13868 [43:57<2:09:55,  1.32it/s]Training epoch 1:  26% 3545/13868 [43:58<2:09:42,  1.33it/s]Training epoch 1:  26% 3546/13868 [43:58<2:09:46,  1.33it/s]Training epoch 1:  26% 3547/13868 [43:59<2:10:00,  1.32it/s]Training epoch 1:  26% 3548/13868 [44:00<2:09:01,  1.33it/s]Training epoch 1:  26% 3549/13868 [44:01<2:09:27,  1.33it/s]Training epoch 1:  26% 3550/13868 [44:01<2:08:02,  1.34it/s]Training epoch 1:  26% 3551/13868 [44:02<2:06:46,  1.36it/s]Training epoch 1:  26% 3552/13868 [44:03<2:09:32,  1.33it/s]Training epoch 1:  26% 3553/13868 [44:04<2:09:46,  1.32it/s]Training epoch 1:  26% 3554/13868 [44:04<2:09:47,  1.32it/s]Training epoch 1:  26% 3555/13868 [44:05<2:08:27,  1.34it/s]Training epoch 1:  26% 3556/13868 [44:06<2:07:14,  1.35it/s]Training epoch 1:  26% 3557/13868 [44:07<2:10:27,  1.32it/s]Training epoch 1:  26% 3558/13868 [44:07<2:07:41,  1.35it/s]Training epoch 1:  26% 3559/13868 [44:08<2:09:29,  1.33it/s]Training epoch 1:  26% 3560/13868 [44:09<2:10:48,  1.31it/s]Training epoch 1:  26% 3561/13868 [44:10<2:11:28,  1.31it/s]Training epoch 1:  26% 3562/13868 [44:10<2:10:30,  1.32it/s]Training epoch 1:  26% 3563/13868 [44:11<2:09:49,  1.32it/s]Training epoch 1:  26% 3564/13868 [44:12<2:08:41,  1.33it/s]Training epoch 1:  26% 3565/13868 [44:13<2:08:04,  1.34it/s]Training epoch 1:  26% 3566/13868 [44:13<2:07:08,  1.35it/s]Training epoch 1:  26% 3567/13868 [44:14<2:07:42,  1.34it/s]Training epoch 1:  26% 3568/13868 [44:15<2:10:04,  1.32it/s]Training epoch 1:  26% 3569/13868 [44:16<2:10:28,  1.32it/s]Training epoch 1:  26% 3570/13868 [44:16<2:08:32,  1.34it/s]Training epoch 1:  26% 3571/13868 [44:17<2:07:45,  1.34it/s]Training epoch 1:  26% 3572/13868 [44:18<2:10:13,  1.32it/s]Training epoch 1:  26% 3573/13868 [44:19<2:11:01,  1.31it/s]Training epoch 1:  26% 3574/13868 [44:19<2:11:20,  1.31it/s]Training epoch 1:  26% 3575/13868 [44:20<2:10:19,  1.32it/s]Training epoch 1:  26% 3576/13868 [44:21<2:10:31,  1.31it/s]Training epoch 1:  26% 3577/13868 [44:22<2:10:25,  1.32it/s]Training epoch 1:  26% 3578/13868 [44:22<2:09:45,  1.32it/s]Training epoch 1:  26% 3579/13868 [44:23<2:08:23,  1.34it/s]Training epoch 1:  26% 3580/13868 [44:24<2:08:40,  1.33it/s]Training epoch 1:  26% 3581/13868 [44:25<2:09:29,  1.32it/s]Training epoch 1:  26% 3582/13868 [44:25<2:09:40,  1.32it/s]Training epoch 1:  26% 3583/13868 [44:26<2:09:17,  1.33it/s]Training epoch 1:  26% 3584/13868 [44:27<2:07:07,  1.35it/s]Training epoch 1:  26% 3585/13868 [44:28<2:05:58,  1.36it/s]Training epoch 1:  26% 3586/13868 [44:28<2:07:01,  1.35it/s]Training epoch 1:  26% 3587/13868 [44:29<2:08:49,  1.33it/s]Training epoch 1:  26% 3588/13868 [44:30<2:07:12,  1.35it/s]Training epoch 1:  26% 3589/13868 [44:31<2:07:35,  1.34it/s]Training epoch 1:  26% 3590/13868 [44:31<2:06:40,  1.35it/s]Training epoch 1:  26% 3591/13868 [44:32<2:07:56,  1.34it/s]Training epoch 1:  26% 3592/13868 [44:33<2:07:41,  1.34it/s]Training epoch 1:  26% 3593/13868 [44:34<2:08:04,  1.34it/s]Training epoch 1:  26% 3594/13868 [44:34<2:08:56,  1.33it/s]Training epoch 1:  26% 3595/13868 [44:35<2:08:36,  1.33it/s]Training epoch 1:  26% 3596/13868 [44:36<2:10:12,  1.31it/s]Training epoch 1:  26% 3597/13868 [44:37<2:10:35,  1.31it/s]Training epoch 1:  26% 3598/13868 [44:37<2:09:11,  1.32it/s]Training epoch 1:  26% 3599/13868 [44:38<2:08:35,  1.33it/s]Training epoch 1:  26% 3600/13868 [44:39<2:15:08,  1.27it/s]Training epoch 1:  26% 3601/13868 [44:40<2:14:26,  1.27it/s]Training epoch 1:  26% 3602/13868 [44:41<2:13:40,  1.28it/s]Training epoch 1:  26% 3603/13868 [44:41<2:14:37,  1.27it/s]Training epoch 1:  26% 3604/13868 [44:42<2:14:04,  1.28it/s]Training epoch 1:  26% 3605/13868 [44:43<2:11:00,  1.31it/s]Training epoch 1:  26% 3606/13868 [44:44<2:09:40,  1.32it/s]Training epoch 1:  26% 3607/13868 [44:44<2:10:49,  1.31it/s]Training epoch 1:  26% 3608/13868 [44:45<2:09:48,  1.32it/s]Training epoch 1:  26% 3609/13868 [44:46<2:11:16,  1.30it/s]Training epoch 1:  26% 3610/13868 [44:47<2:09:14,  1.32it/s]Training epoch 1:  26% 3611/13868 [44:47<2:07:56,  1.34it/s]Training epoch 1:  26% 3612/13868 [44:48<2:07:31,  1.34it/s]Training epoch 1:  26% 3613/13868 [44:49<2:10:44,  1.31it/s]Training epoch 1:  26% 3614/13868 [44:50<2:10:06,  1.31it/s]Training epoch 1:  26% 3615/13868 [44:51<2:10:43,  1.31it/s]Training epoch 1:  26% 3616/13868 [44:51<2:08:16,  1.33it/s]Training epoch 1:  26% 3617/13868 [44:52<2:09:49,  1.32it/s]Training epoch 1:  26% 3618/13868 [44:53<2:11:32,  1.30it/s]Training epoch 1:  26% 3619/13868 [44:54<2:09:39,  1.32it/s]Training epoch 1:  26% 3620/13868 [44:54<2:09:30,  1.32it/s]Training epoch 1:  26% 3621/13868 [44:55<2:07:50,  1.34it/s]Training epoch 1:  26% 3622/13868 [44:56<2:09:26,  1.32it/s]Training epoch 1:  26% 3623/13868 [44:57<2:09:11,  1.32it/s]Training epoch 1:  26% 3624/13868 [44:57<2:08:45,  1.33it/s]Training epoch 1:  26% 3625/13868 [44:58<2:09:04,  1.32it/s]Training epoch 1:  26% 3626/13868 [44:59<2:08:30,  1.33it/s]Training epoch 1:  26% 3627/13868 [45:00<2:07:47,  1.34it/s]Training epoch 1:  26% 3628/13868 [45:00<2:07:50,  1.33it/s]Training epoch 1:  26% 3629/13868 [45:01<2:08:32,  1.33it/s]Training epoch 1:  26% 3630/13868 [45:02<2:07:28,  1.34it/s]Training epoch 1:  26% 3631/13868 [45:03<2:07:38,  1.34it/s]Training epoch 1:  26% 3632/13868 [45:03<2:05:50,  1.36it/s]Training epoch 1:  26% 3633/13868 [45:04<2:07:21,  1.34it/s]Training epoch 1:  26% 3634/13868 [45:05<2:07:19,  1.34it/s]Training epoch 1:  26% 3635/13868 [45:06<2:08:04,  1.33it/s]Training epoch 1:  26% 3636/13868 [45:06<2:08:37,  1.33it/s]Training epoch 1:  26% 3637/13868 [45:07<2:10:44,  1.30it/s]Training epoch 1:  26% 3638/13868 [45:08<2:10:12,  1.31it/s]Training epoch 1:  26% 3639/13868 [45:09<2:11:17,  1.30it/s]Training epoch 1:  26% 3640/13868 [45:09<2:10:02,  1.31it/s]Training epoch 1:  26% 3641/13868 [45:10<2:09:49,  1.31it/s]Training epoch 1:  26% 3642/13868 [45:11<2:11:11,  1.30it/s]Training epoch 1:  26% 3643/13868 [45:12<2:11:41,  1.29it/s]Training epoch 1:  26% 3644/13868 [45:12<2:11:27,  1.30it/s]Training epoch 1:  26% 3645/13868 [45:13<2:09:33,  1.32it/s]Training epoch 1:  26% 3646/13868 [45:14<2:08:56,  1.32it/s]Training epoch 1:  26% 3647/13868 [45:15<2:08:57,  1.32it/s]Training epoch 1:  26% 3648/13868 [45:15<2:09:23,  1.32it/s]Training epoch 1:  26% 3649/13868 [45:16<2:10:53,  1.30it/s]Training epoch 1:  26% 3650/13868 [45:17<2:10:55,  1.30it/s]Training epoch 1:  26% 3651/13868 [45:18<2:08:59,  1.32it/s]Training epoch 1:  26% 3652/13868 [45:19<2:08:24,  1.33it/s]Training epoch 1:  26% 3653/13868 [45:19<2:07:20,  1.34it/s]Training epoch 1:  26% 3654/13868 [45:20<2:06:22,  1.35it/s]Training epoch 1:  26% 3655/13868 [45:21<2:08:41,  1.32it/s]Training epoch 1:  26% 3656/13868 [45:22<2:09:15,  1.32it/s]Training epoch 1:  26% 3657/13868 [45:22<2:08:58,  1.32it/s]Training epoch 1:  26% 3658/13868 [45:23<2:10:54,  1.30it/s]Training epoch 1:  26% 3659/13868 [45:24<2:10:33,  1.30it/s]Training epoch 1:  26% 3660/13868 [45:25<2:09:42,  1.31it/s]Training epoch 1:  26% 3661/13868 [45:25<2:08:43,  1.32it/s]Training epoch 1:  26% 3662/13868 [45:26<2:10:55,  1.30it/s]Training epoch 1:  26% 3663/13868 [45:27<2:09:43,  1.31it/s]Training epoch 1:  26% 3664/13868 [45:28<2:10:26,  1.30it/s]Training epoch 1:  26% 3665/13868 [45:28<2:09:50,  1.31it/s]Training epoch 1:  26% 3666/13868 [45:29<2:05:57,  1.35it/s]Training epoch 1:  26% 3667/13868 [45:30<2:06:50,  1.34it/s]Training epoch 1:  26% 3668/13868 [45:31<2:06:36,  1.34it/s]Training epoch 1:  26% 3669/13868 [45:31<2:06:05,  1.35it/s]Training epoch 1:  26% 3670/13868 [45:32<2:07:38,  1.33it/s]Training epoch 1:  26% 3671/13868 [45:33<2:07:33,  1.33it/s]Training epoch 1:  26% 3672/13868 [45:34<2:03:52,  1.37it/s]Training epoch 1:  26% 3673/13868 [45:34<2:06:57,  1.34it/s]Training epoch 1:  26% 3674/13868 [45:35<2:06:15,  1.35it/s]Training epoch 1:  26% 3675/13868 [45:36<2:05:07,  1.36it/s]Training epoch 1:  27% 3676/13868 [45:37<2:04:35,  1.36it/s]Training epoch 1:  27% 3677/13868 [45:37<2:06:03,  1.35it/s]Training epoch 1:  27% 3678/13868 [45:38<2:06:46,  1.34it/s]Training epoch 1:  27% 3679/13868 [45:39<2:07:52,  1.33it/s]Training epoch 1:  27% 3680/13868 [45:40<2:06:31,  1.34it/s]Training epoch 1:  27% 3681/13868 [45:40<2:08:17,  1.32it/s]Training epoch 1:  27% 3682/13868 [45:41<2:06:19,  1.34it/s]Training epoch 1:  27% 3683/13868 [45:42<2:07:03,  1.34it/s]Training epoch 1:  27% 3684/13868 [45:43<2:07:51,  1.33it/s]Training epoch 1:  27% 3685/13868 [45:43<2:08:32,  1.32it/s]Training epoch 1:  27% 3686/13868 [45:44<2:07:51,  1.33it/s]Training epoch 1:  27% 3687/13868 [45:45<2:07:07,  1.33it/s]Training epoch 1:  27% 3688/13868 [45:46<2:06:20,  1.34it/s]Training epoch 1:  27% 3689/13868 [45:46<2:07:05,  1.33it/s]Training epoch 1:  27% 3690/13868 [45:47<2:06:59,  1.34it/s]Training epoch 1:  27% 3691/13868 [45:48<2:06:38,  1.34it/s]Training epoch 1:  27% 3692/13868 [45:49<2:05:11,  1.35it/s]Training epoch 1:  27% 3693/13868 [45:49<2:06:30,  1.34it/s]Training epoch 1:  27% 3694/13868 [45:50<2:06:52,  1.34it/s]Training epoch 1:  27% 3695/13868 [45:51<2:07:15,  1.33it/s]Training epoch 1:  27% 3696/13868 [45:52<2:06:53,  1.34it/s]Training epoch 1:  27% 3697/13868 [45:52<2:07:46,  1.33it/s]Training epoch 1:  27% 3698/13868 [45:53<2:06:00,  1.35it/s]Training epoch 1:  27% 3699/13868 [45:54<2:07:15,  1.33it/s]Training epoch 1:  27% 3700/13868 [45:55<2:13:14,  1.27it/s]Training epoch 1:  27% 3701/13868 [45:55<2:11:40,  1.29it/s]Training epoch 1:  27% 3702/13868 [45:56<2:10:44,  1.30it/s]Training epoch 1:  27% 3703/13868 [45:57<2:09:56,  1.30it/s]Training epoch 1:  27% 3704/13868 [45:58<2:07:57,  1.32it/s]Training epoch 1:  27% 3705/13868 [45:58<2:08:37,  1.32it/s]Training epoch 1:  27% 3706/13868 [45:59<2:08:11,  1.32it/s]Training epoch 1:  27% 3707/13868 [46:00<2:08:06,  1.32it/s]Training epoch 1:  27% 3708/13868 [46:01<2:07:36,  1.33it/s]Training epoch 1:  27% 3709/13868 [46:01<2:07:04,  1.33it/s]Training epoch 1:  27% 3710/13868 [46:02<2:05:51,  1.35it/s]Training epoch 1:  27% 3711/13868 [46:03<2:06:23,  1.34it/s]Training epoch 1:  27% 3712/13868 [46:04<2:06:24,  1.34it/s]Training epoch 1:  27% 3713/13868 [46:04<2:06:07,  1.34it/s]Training epoch 1:  27% 3714/13868 [46:05<2:05:29,  1.35it/s]Training epoch 1:  27% 3715/13868 [46:06<2:04:54,  1.35it/s]Training epoch 1:  27% 3716/13868 [46:07<2:06:06,  1.34it/s]Training epoch 1:  27% 3717/13868 [46:07<2:06:29,  1.34it/s]Training epoch 1:  27% 3718/13868 [46:08<2:04:14,  1.36it/s]Training epoch 1:  27% 3719/13868 [46:09<2:05:29,  1.35it/s]Training epoch 1:  27% 3720/13868 [46:10<2:04:55,  1.35it/s]Training epoch 1:  27% 3721/13868 [46:10<2:04:57,  1.35it/s]Training epoch 1:  27% 3722/13868 [46:11<2:03:40,  1.37it/s]Training epoch 1:  27% 3723/13868 [46:12<2:05:02,  1.35it/s]Training epoch 1:  27% 3724/13868 [46:12<2:04:12,  1.36it/s]Training epoch 1:  27% 3725/13868 [46:13<2:04:53,  1.35it/s]Training epoch 1:  27% 3726/13868 [46:14<2:03:03,  1.37it/s]Training epoch 1:  27% 3727/13868 [46:15<2:04:54,  1.35it/s]Training epoch 1:  27% 3728/13868 [46:15<2:05:21,  1.35it/s]Training epoch 1:  27% 3729/13868 [46:16<2:05:06,  1.35it/s]Training epoch 1:  27% 3730/13868 [46:17<2:04:07,  1.36it/s]Training epoch 1:  27% 3731/13868 [46:18<2:05:42,  1.34it/s]Training epoch 1:  27% 3732/13868 [46:18<2:06:07,  1.34it/s]Training epoch 1:  27% 3733/13868 [46:19<2:04:59,  1.35it/s]Training epoch 1:  27% 3734/13868 [46:20<2:05:21,  1.35it/s]Training epoch 1:  27% 3735/13868 [46:21<2:07:04,  1.33it/s]Training epoch 1:  27% 3736/13868 [46:21<2:05:23,  1.35it/s]Training epoch 1:  27% 3737/13868 [46:22<2:07:03,  1.33it/s]Training epoch 1:  27% 3738/13868 [46:23<2:06:26,  1.34it/s]Training epoch 1:  27% 3739/13868 [46:24<2:08:59,  1.31it/s]Training epoch 1:  27% 3740/13868 [46:24<2:08:23,  1.31it/s]Training epoch 1:  27% 3741/13868 [46:25<2:08:28,  1.31it/s]Training epoch 1:  27% 3742/13868 [46:26<2:06:17,  1.34it/s]Training epoch 1:  27% 3743/13868 [46:27<2:05:42,  1.34it/s]Training epoch 1:  27% 3744/13868 [46:27<2:05:15,  1.35it/s]Training epoch 1:  27% 3745/13868 [46:28<2:06:00,  1.34it/s]Training epoch 1:  27% 3746/13868 [46:29<2:06:10,  1.34it/s]Training epoch 1:  27% 3747/13868 [46:30<2:07:45,  1.32it/s]Training epoch 1:  27% 3748/13868 [46:30<2:05:25,  1.34it/s]Training epoch 1:  27% 3749/13868 [46:31<2:05:04,  1.35it/s]Training epoch 1:  27% 3750/13868 [46:32<2:07:24,  1.32it/s]Training epoch 1:  27% 3751/13868 [46:33<2:08:58,  1.31it/s]Training epoch 1:  27% 3752/13868 [46:34<2:09:48,  1.30it/s]Training epoch 1:  27% 3753/13868 [46:34<2:09:36,  1.30it/s]Training epoch 1:  27% 3754/13868 [46:35<2:07:28,  1.32it/s]Training epoch 1:  27% 3755/13868 [46:36<2:08:04,  1.32it/s]Training epoch 1:  27% 3756/13868 [46:37<2:06:46,  1.33it/s]Training epoch 1:  27% 3757/13868 [46:37<2:08:24,  1.31it/s]Training epoch 1:  27% 3758/13868 [46:38<2:08:00,  1.32it/s]Training epoch 1:  27% 3759/13868 [46:39<2:08:42,  1.31it/s]Training epoch 1:  27% 3760/13868 [46:40<2:09:11,  1.30it/s]Training epoch 1:  27% 3761/13868 [46:40<2:09:30,  1.30it/s]Training epoch 1:  27% 3762/13868 [46:41<2:07:47,  1.32it/s]Training epoch 1:  27% 3763/13868 [46:42<2:09:35,  1.30it/s]Training epoch 1:  27% 3764/13868 [46:43<2:07:42,  1.32it/s]Training epoch 1:  27% 3765/13868 [46:43<2:07:09,  1.32it/s]Training epoch 1:  27% 3766/13868 [46:44<2:04:43,  1.35it/s]Training epoch 1:  27% 3767/13868 [46:45<2:05:49,  1.34it/s]Training epoch 1:  27% 3768/13868 [46:46<2:06:52,  1.33it/s]Training epoch 1:  27% 3769/13868 [46:46<2:06:13,  1.33it/s]Training epoch 1:  27% 3770/13868 [46:47<2:05:48,  1.34it/s]Training epoch 1:  27% 3771/13868 [46:48<2:06:20,  1.33it/s]Training epoch 1:  27% 3772/13868 [46:49<2:05:40,  1.34it/s]Training epoch 1:  27% 3773/13868 [46:49<2:05:50,  1.34it/s]Training epoch 1:  27% 3774/13868 [46:50<2:05:32,  1.34it/s]Training epoch 1:  27% 3775/13868 [46:51<2:06:33,  1.33it/s]Training epoch 1:  27% 3776/13868 [46:52<2:05:52,  1.34it/s]Training epoch 1:  27% 3777/13868 [46:52<2:07:56,  1.31it/s]Training epoch 1:  27% 3778/13868 [46:53<2:06:57,  1.32it/s]Training epoch 1:  27% 3779/13868 [46:54<2:07:16,  1.32it/s]Training epoch 1:  27% 3780/13868 [46:55<2:07:18,  1.32it/s]Training epoch 1:  27% 3781/13868 [46:55<2:08:10,  1.31it/s]Training epoch 1:  27% 3782/13868 [46:56<2:05:30,  1.34it/s]Training epoch 1:  27% 3783/13868 [46:57<2:05:38,  1.34it/s]Training epoch 1:  27% 3784/13868 [46:58<2:04:58,  1.34it/s]Training epoch 1:  27% 3785/13868 [46:58<2:04:47,  1.35it/s]Training epoch 1:  27% 3786/13868 [46:59<2:03:34,  1.36it/s]Training epoch 1:  27% 3787/13868 [47:00<2:03:12,  1.36it/s]Training epoch 1:  27% 3788/13868 [47:01<2:03:20,  1.36it/s]Training epoch 1:  27% 3789/13868 [47:01<2:05:42,  1.34it/s]Training epoch 1:  27% 3790/13868 [47:02<2:04:37,  1.35it/s]Training epoch 1:  27% 3791/13868 [47:03<2:06:15,  1.33it/s]Training epoch 1:  27% 3792/13868 [47:04<2:05:45,  1.34it/s]Training epoch 1:  27% 3793/13868 [47:04<2:06:11,  1.33it/s]Training epoch 1:  27% 3794/13868 [47:05<2:06:01,  1.33it/s]Training epoch 1:  27% 3795/13868 [47:06<2:07:36,  1.32it/s]Training epoch 1:  27% 3796/13868 [47:07<2:06:19,  1.33it/s]Training epoch 1:  27% 3797/13868 [47:07<2:05:51,  1.33it/s]Training epoch 1:  27% 3798/13868 [47:08<2:04:14,  1.35it/s]Training epoch 1:  27% 3799/13868 [47:09<2:04:48,  1.34it/s]Training epoch 1:  27% 3800/13868 [47:10<2:13:39,  1.26it/s]Training epoch 1:  27% 3801/13868 [47:10<2:10:57,  1.28it/s]Training epoch 1:  27% 3802/13868 [47:11<2:09:16,  1.30it/s]Training epoch 1:  27% 3803/13868 [47:12<2:09:07,  1.30it/s]Training epoch 1:  27% 3804/13868 [47:13<2:08:22,  1.31it/s]Training epoch 1:  27% 3805/13868 [47:13<2:06:59,  1.32it/s]Training epoch 1:  27% 3806/13868 [47:14<2:06:19,  1.33it/s]Training epoch 1:  27% 3807/13868 [47:15<2:06:16,  1.33it/s]Training epoch 1:  27% 3808/13868 [47:16<2:04:04,  1.35it/s]Training epoch 1:  27% 3809/13868 [47:16<2:04:22,  1.35it/s]Training epoch 1:  27% 3810/13868 [47:17<2:04:30,  1.35it/s]Training epoch 1:  27% 3811/13868 [47:18<2:03:46,  1.35it/s]Training epoch 1:  27% 3812/13868 [47:19<2:04:43,  1.34it/s]Training epoch 1:  27% 3813/13868 [47:19<2:05:50,  1.33it/s]Training epoch 1:  28% 3814/13868 [47:20<2:05:44,  1.33it/s]Training epoch 1:  28% 3815/13868 [47:21<2:06:26,  1.33it/s]Training epoch 1:  28% 3816/13868 [47:22<2:05:52,  1.33it/s]Training epoch 1:  28% 3817/13868 [47:22<2:05:07,  1.34it/s]Training epoch 1:  28% 3818/13868 [47:23<2:04:58,  1.34it/s]Training epoch 1:  28% 3819/13868 [47:24<2:04:26,  1.35it/s]Training epoch 1:  28% 3820/13868 [47:25<2:05:41,  1.33it/s]Training epoch 1:  28% 3821/13868 [47:25<2:04:22,  1.35it/s]Training epoch 1:  28% 3822/13868 [47:26<2:04:07,  1.35it/s]Training epoch 1:  28% 3823/13868 [47:27<2:06:58,  1.32it/s]Training epoch 1:  28% 3824/13868 [47:28<2:06:23,  1.32it/s]Training epoch 1:  28% 3825/13868 [47:28<2:05:40,  1.33it/s]Training epoch 1:  28% 3826/13868 [47:29<2:07:30,  1.31it/s]Training epoch 1:  28% 3827/13868 [47:30<2:06:54,  1.32it/s]Training epoch 1:  28% 3828/13868 [47:31<2:05:40,  1.33it/s]Training epoch 1:  28% 3829/13868 [47:31<2:06:03,  1.33it/s]Training epoch 1:  28% 3830/13868 [47:32<2:03:59,  1.35it/s]Training epoch 1:  28% 3831/13868 [47:33<2:03:16,  1.36it/s]Training epoch 1:  28% 3832/13868 [47:34<2:01:56,  1.37it/s]Training epoch 1:  28% 3833/13868 [47:34<2:03:36,  1.35it/s]Training epoch 1:  28% 3834/13868 [47:35<2:03:43,  1.35it/s]Training epoch 1:  28% 3835/13868 [47:36<2:03:38,  1.35it/s]Training epoch 1:  28% 3836/13868 [47:37<2:03:59,  1.35it/s]Training epoch 1:  28% 3837/13868 [47:37<2:03:29,  1.35it/s]Training epoch 1:  28% 3838/13868 [47:38<2:03:42,  1.35it/s]Training epoch 1:  28% 3839/13868 [47:39<2:04:35,  1.34it/s]Training epoch 1:  28% 3840/13868 [47:40<2:04:08,  1.35it/s]Training epoch 1:  28% 3841/13868 [47:40<2:04:16,  1.34it/s]Training epoch 1:  28% 3842/13868 [47:41<2:05:25,  1.33it/s]Training epoch 1:  28% 3843/13868 [47:42<2:05:02,  1.34it/s]Training epoch 1:  28% 3844/13868 [47:43<2:04:16,  1.34it/s]Training epoch 1:  28% 3845/13868 [47:43<2:05:35,  1.33it/s]Training epoch 1:  28% 3846/13868 [47:44<2:03:42,  1.35it/s]Training epoch 1:  28% 3847/13868 [47:45<2:03:35,  1.35it/s]Training epoch 1:  28% 3848/13868 [47:46<2:06:05,  1.32it/s]Training epoch 1:  28% 3849/13868 [47:46<2:05:38,  1.33it/s]Training epoch 1:  28% 3850/13868 [47:47<2:05:45,  1.33it/s]Training epoch 1:  28% 3851/13868 [47:48<2:04:48,  1.34it/s]Training epoch 1:  28% 3852/13868 [47:49<2:05:29,  1.33it/s]Training epoch 1:  28% 3853/13868 [47:49<2:05:40,  1.33it/s]Training epoch 1:  28% 3854/13868 [47:50<2:05:36,  1.33it/s]Training epoch 1:  28% 3855/13868 [47:51<2:07:20,  1.31it/s]Training epoch 1:  28% 3856/13868 [47:52<2:06:58,  1.31it/s]Training epoch 1:  28% 3857/13868 [47:52<2:06:43,  1.32it/s]Training epoch 1:  28% 3858/13868 [47:53<2:05:42,  1.33it/s]Training epoch 1:  28% 3859/13868 [47:54<2:05:40,  1.33it/s]Training epoch 1:  28% 3860/13868 [47:55<2:03:55,  1.35it/s]Training epoch 1:  28% 3861/13868 [47:55<2:04:04,  1.34it/s]Training epoch 1:  28% 3862/13868 [47:56<2:03:52,  1.35it/s]Training epoch 1:  28% 3863/13868 [47:57<2:05:20,  1.33it/s]Training epoch 1:  28% 3864/13868 [47:58<2:03:25,  1.35it/s]Training epoch 1:  28% 3865/13868 [47:58<2:03:09,  1.35it/s]Training epoch 1:  28% 3866/13868 [47:59<2:04:14,  1.34it/s]Training epoch 1:  28% 3867/13868 [48:00<2:06:04,  1.32it/s]Training epoch 1:  28% 3868/13868 [48:01<2:06:19,  1.32it/s]Training epoch 1:  28% 3869/13868 [48:01<2:06:38,  1.32it/s]Training epoch 1:  28% 3870/13868 [48:02<2:06:37,  1.32it/s]Training epoch 1:  28% 3871/13868 [48:03<2:07:35,  1.31it/s]Training epoch 1:  28% 3872/13868 [48:04<2:05:42,  1.33it/s]Training epoch 1:  28% 3873/13868 [48:04<2:06:22,  1.32it/s]Training epoch 1:  28% 3874/13868 [48:05<2:03:38,  1.35it/s]Training epoch 1:  28% 3875/13868 [48:06<2:03:15,  1.35it/s]Training epoch 1:  28% 3876/13868 [48:07<2:04:04,  1.34it/s]Training epoch 1:  28% 3877/13868 [48:07<2:04:19,  1.34it/s]Training epoch 1:  28% 3878/13868 [48:08<2:03:28,  1.35it/s]Training epoch 1:  28% 3879/13868 [48:09<2:04:54,  1.33it/s]Training epoch 1:  28% 3880/13868 [48:10<2:05:51,  1.32it/s]Training epoch 1:  28% 3881/13868 [48:10<2:06:11,  1.32it/s]Training epoch 1:  28% 3882/13868 [48:11<2:06:37,  1.31it/s]Training epoch 1:  28% 3883/13868 [48:12<2:06:47,  1.31it/s]Training epoch 1:  28% 3884/13868 [48:13<2:07:03,  1.31it/s]Training epoch 1:  28% 3885/13868 [48:13<2:06:16,  1.32it/s]Training epoch 1:  28% 3886/13868 [48:14<2:05:26,  1.33it/s]Training epoch 1:  28% 3887/13868 [48:15<2:06:50,  1.31it/s]Training epoch 1:  28% 3888/13868 [48:16<2:05:57,  1.32it/s]Training epoch 1:  28% 3889/13868 [48:16<2:07:23,  1.31it/s]Training epoch 1:  28% 3890/13868 [48:17<2:05:48,  1.32it/s]Training epoch 1:  28% 3891/13868 [48:18<2:05:20,  1.33it/s]Training epoch 1:  28% 3892/13868 [48:19<2:06:06,  1.32it/s]Training epoch 1:  28% 3893/13868 [48:19<2:05:18,  1.33it/s]Training epoch 1:  28% 3894/13868 [48:20<2:02:38,  1.36it/s]Training epoch 1:  28% 3895/13868 [48:21<2:03:31,  1.35it/s]Training epoch 1:  28% 3896/13868 [48:22<2:04:09,  1.34it/s]Training epoch 1:  28% 3897/13868 [48:22<2:04:17,  1.34it/s]Training epoch 1:  28% 3898/13868 [48:23<2:04:39,  1.33it/s]Training epoch 1:  28% 3899/13868 [48:24<2:05:26,  1.32it/s]Training epoch 1:  28% 3900/13868 [48:25<2:12:01,  1.26it/s]Training epoch 1:  28% 3901/13868 [48:26<2:10:01,  1.28it/s]Training epoch 1:  28% 3902/13868 [48:26<2:05:56,  1.32it/s]Training epoch 1:  28% 3903/13868 [48:27<2:06:17,  1.32it/s]Training epoch 1:  28% 3904/13868 [48:28<2:05:25,  1.32it/s]Training epoch 1:  28% 3905/13868 [48:29<2:04:09,  1.34it/s]Training epoch 1:  28% 3906/13868 [48:29<2:05:09,  1.33it/s]Training epoch 1:  28% 3907/13868 [48:30<2:05:17,  1.32it/s]Training epoch 1:  28% 3908/13868 [48:31<2:04:04,  1.34it/s]Training epoch 1:  28% 3909/13868 [48:31<2:02:30,  1.35it/s]Training epoch 1:  28% 3910/13868 [48:32<2:01:13,  1.37it/s]Training epoch 1:  28% 3911/13868 [48:33<2:02:43,  1.35it/s]Training epoch 1:  28% 3912/13868 [48:34<2:03:50,  1.34it/s]Training epoch 1:  28% 3913/13868 [48:34<2:04:54,  1.33it/s]Training epoch 1:  28% 3914/13868 [48:35<2:05:41,  1.32it/s]Training epoch 1:  28% 3915/13868 [48:36<2:06:59,  1.31it/s]Training epoch 1:  28% 3916/13868 [48:37<2:05:32,  1.32it/s]Training epoch 1:  28% 3917/13868 [48:38<2:05:51,  1.32it/s]Training epoch 1:  28% 3918/13868 [48:38<2:08:05,  1.29it/s]Training epoch 1:  28% 3919/13868 [48:39<2:06:59,  1.31it/s]Training epoch 1:  28% 3920/13868 [48:40<2:06:33,  1.31it/s]Training epoch 1:  28% 3921/13868 [48:41<2:06:54,  1.31it/s]Training epoch 1:  28% 3922/13868 [48:41<2:05:04,  1.33it/s]Training epoch 1:  28% 3923/13868 [48:42<2:02:59,  1.35it/s]Training epoch 1:  28% 3924/13868 [48:43<2:02:46,  1.35it/s]Training epoch 1:  28% 3925/13868 [48:44<2:04:04,  1.34it/s]Training epoch 1:  28% 3926/13868 [48:44<2:07:08,  1.30it/s]Training epoch 1:  28% 3927/13868 [48:45<2:07:29,  1.30it/s]Training epoch 1:  28% 3928/13868 [48:46<2:08:17,  1.29it/s]Training epoch 1:  28% 3929/13868 [48:47<2:06:20,  1.31it/s]Training epoch 1:  28% 3930/13868 [48:47<2:06:02,  1.31it/s]Training epoch 1:  28% 3931/13868 [48:48<2:05:34,  1.32it/s]Training epoch 1:  28% 3932/13868 [48:49<2:06:31,  1.31it/s]Training epoch 1:  28% 3933/13868 [48:50<2:04:27,  1.33it/s]Training epoch 1:  28% 3934/13868 [48:50<2:06:16,  1.31it/s]Training epoch 1:  28% 3935/13868 [48:51<2:06:30,  1.31it/s]Training epoch 1:  28% 3936/13868 [48:52<2:07:01,  1.30it/s]Training epoch 1:  28% 3937/13868 [48:53<2:07:12,  1.30it/s]Training epoch 1:  28% 3938/13868 [48:54<2:07:15,  1.30it/s]Training epoch 1:  28% 3939/13868 [48:54<2:07:48,  1.29it/s]Training epoch 1:  28% 3940/13868 [48:55<2:09:18,  1.28it/s]Training epoch 1:  28% 3941/13868 [48:56<2:07:47,  1.29it/s]Training epoch 1:  28% 3942/13868 [48:57<2:08:08,  1.29it/s]Training epoch 1:  28% 3943/13868 [48:57<2:06:36,  1.31it/s]Training epoch 1:  28% 3944/13868 [48:58<2:06:59,  1.30it/s]Training epoch 1:  28% 3945/13868 [48:59<2:05:19,  1.32it/s]Training epoch 1:  28% 3946/13868 [49:00<2:06:12,  1.31it/s]Training epoch 1:  28% 3947/13868 [49:00<2:06:26,  1.31it/s]Training epoch 1:  28% 3948/13868 [49:01<2:08:09,  1.29it/s]Training epoch 1:  28% 3949/13868 [49:02<2:07:15,  1.30it/s]Training epoch 1:  28% 3950/13868 [49:03<2:05:07,  1.32it/s]Training epoch 1:  28% 3951/13868 [49:04<2:04:44,  1.33it/s]Training epoch 1:  28% 3952/13868 [49:04<2:05:33,  1.32it/s]Training epoch 1:  29% 3953/13868 [49:05<2:04:50,  1.32it/s]Training epoch 1:  29% 3954/13868 [49:06<2:04:12,  1.33it/s]Training epoch 1:  29% 3955/13868 [49:07<2:06:02,  1.31it/s]Training epoch 1:  29% 3956/13868 [49:07<2:06:22,  1.31it/s]Training epoch 1:  29% 3957/13868 [49:08<2:03:51,  1.33it/s]Training epoch 1:  29% 3958/13868 [49:09<2:04:31,  1.33it/s]Training epoch 1:  29% 3959/13868 [49:10<2:05:13,  1.32it/s]Training epoch 1:  29% 3960/13868 [49:10<2:04:10,  1.33it/s]Training epoch 1:  29% 3961/13868 [49:11<2:03:37,  1.34it/s]Training epoch 1:  29% 3962/13868 [49:12<2:03:59,  1.33it/s]Training epoch 1:  29% 3963/13868 [49:13<2:04:18,  1.33it/s]Training epoch 1:  29% 3964/13868 [49:13<2:05:25,  1.32it/s]Training epoch 1:  29% 3965/13868 [49:14<2:05:40,  1.31it/s]Training epoch 1:  29% 3966/13868 [49:15<2:04:04,  1.33it/s]Training epoch 1:  29% 3967/13868 [49:16<2:02:16,  1.35it/s]Training epoch 1:  29% 3968/13868 [49:16<2:03:37,  1.33it/s]Training epoch 1:  29% 3969/13868 [49:17<2:02:03,  1.35it/s]Training epoch 1:  29% 3970/13868 [49:18<2:02:34,  1.35it/s]Training epoch 1:  29% 3971/13868 [49:19<2:02:31,  1.35it/s]Training epoch 1:  29% 3972/13868 [49:19<2:03:40,  1.33it/s]Training epoch 1:  29% 3973/13868 [49:20<2:03:24,  1.34it/s]Training epoch 1:  29% 3974/13868 [49:21<2:03:38,  1.33it/s]Training epoch 1:  29% 3975/13868 [49:22<2:03:01,  1.34it/s]Training epoch 1:  29% 3976/13868 [49:22<2:02:23,  1.35it/s]Training epoch 1:  29% 3977/13868 [49:23<2:03:57,  1.33it/s]Training epoch 1:  29% 3978/13868 [49:24<2:04:55,  1.32it/s]Training epoch 1:  29% 3979/13868 [49:25<2:03:56,  1.33it/s]Training epoch 1:  29% 3980/13868 [49:25<2:04:49,  1.32it/s]Training epoch 1:  29% 3981/13868 [49:26<2:05:40,  1.31it/s]Training epoch 1:  29% 3982/13868 [49:27<2:04:18,  1.33it/s]Training epoch 1:  29% 3983/13868 [49:28<2:05:00,  1.32it/s]Training epoch 1:  29% 3984/13868 [49:28<2:03:56,  1.33it/s]Training epoch 1:  29% 3985/13868 [49:29<2:04:07,  1.33it/s]Training epoch 1:  29% 3986/13868 [49:30<2:06:13,  1.30it/s]Training epoch 1:  29% 3987/13868 [49:31<2:05:46,  1.31it/s]Training epoch 1:  29% 3988/13868 [49:31<2:06:48,  1.30it/s]Training epoch 1:  29% 3989/13868 [49:32<2:05:08,  1.32it/s]Training epoch 1:  29% 3990/13868 [49:33<2:04:49,  1.32it/s]Training epoch 1:  29% 3991/13868 [49:34<2:05:57,  1.31it/s]Training epoch 1:  29% 3992/13868 [49:34<2:06:36,  1.30it/s]Training epoch 1:  29% 3993/13868 [49:35<2:03:55,  1.33it/s]Training epoch 1:  29% 3994/13868 [49:36<2:03:51,  1.33it/s]Training epoch 1:  29% 3995/13868 [49:37<2:03:16,  1.33it/s]Training epoch 1:  29% 3996/13868 [49:37<2:04:26,  1.32it/s]Training epoch 1:  29% 3997/13868 [49:38<2:03:42,  1.33it/s]Training epoch 1:  29% 3998/13868 [49:39<2:03:25,  1.33it/s]Training epoch 1:  29% 3999/13868 [49:40<2:03:45,  1.33it/s]Training epoch 1:  29% 4000/13868 [49:41<2:11:50,  1.25it/s]Training epoch 1:  29% 4001/13868 [49:41<2:09:18,  1.27it/s]Training epoch 1:  29% 4002/13868 [49:42<2:06:06,  1.30it/s]Training epoch 1:  29% 4003/13868 [49:43<2:04:59,  1.32it/s]Training epoch 1:  29% 4004/13868 [49:44<2:06:57,  1.29it/s]Training epoch 1:  29% 4005/13868 [49:44<2:05:18,  1.31it/s]Training epoch 1:  29% 4006/13868 [49:45<2:05:20,  1.31it/s]Training epoch 1:  29% 4007/13868 [49:46<2:05:45,  1.31it/s]Training epoch 1:  29% 4008/13868 [49:47<2:04:40,  1.32it/s]Training epoch 1:  29% 4009/13868 [49:47<2:03:58,  1.33it/s]Training epoch 1:  29% 4010/13868 [49:48<2:02:03,  1.35it/s]Training epoch 1:  29% 4011/13868 [49:49<2:01:42,  1.35it/s]Training epoch 1:  29% 4012/13868 [49:50<2:03:11,  1.33it/s]Training epoch 1:  29% 4013/13868 [49:50<2:03:41,  1.33it/s]Training epoch 1:  29% 4014/13868 [49:51<2:02:53,  1.34it/s]Training epoch 1:  29% 4015/13868 [49:52<2:03:37,  1.33it/s]Training epoch 1:  29% 4016/13868 [49:53<2:04:23,  1.32it/s]Training epoch 1:  29% 4017/13868 [49:53<2:03:28,  1.33it/s]Training epoch 1:  29% 4018/13868 [49:54<2:04:02,  1.32it/s]Training epoch 1:  29% 4019/13868 [49:55<2:01:47,  1.35it/s]Training epoch 1:  29% 4020/13868 [49:56<2:01:22,  1.35it/s]Training epoch 1:  29% 4021/13868 [49:56<2:01:21,  1.35it/s]Training epoch 1:  29% 4022/13868 [49:57<2:03:01,  1.33it/s]Training epoch 1:  29% 4023/13868 [49:58<2:04:04,  1.32it/s]Training epoch 1:  29% 4024/13868 [49:59<2:03:21,  1.33it/s]Training epoch 1:  29% 4025/13868 [49:59<2:03:47,  1.33it/s]Training epoch 1:  29% 4026/13868 [50:00<2:02:44,  1.34it/s]Training epoch 1:  29% 4027/13868 [50:01<2:03:18,  1.33it/s]Training epoch 1:  29% 4028/13868 [50:02<2:05:03,  1.31it/s]Training epoch 1:  29% 4029/13868 [50:02<2:05:58,  1.30it/s]Training epoch 1:  29% 4030/13868 [50:03<2:04:50,  1.31it/s]Training epoch 1:  29% 4031/13868 [50:04<2:04:21,  1.32it/s]Training epoch 1:  29% 4032/13868 [50:05<2:03:04,  1.33it/s]Training epoch 1:  29% 4033/13868 [50:05<2:02:33,  1.34it/s]Training epoch 1:  29% 4034/13868 [50:06<2:05:13,  1.31it/s]Training epoch 1:  29% 4035/13868 [50:07<2:06:14,  1.30it/s]Training epoch 1:  29% 4036/13868 [50:08<2:06:00,  1.30it/s]Training epoch 1:  29% 4037/13868 [50:09<2:05:16,  1.31it/s]Training epoch 1:  29% 4038/13868 [50:09<2:03:05,  1.33it/s]Training epoch 1:  29% 4039/13868 [50:10<2:04:27,  1.32it/s]Training epoch 1:  29% 4040/13868 [50:11<2:03:17,  1.33it/s]Training epoch 1:  29% 4041/13868 [50:11<2:02:11,  1.34it/s]Training epoch 1:  29% 4042/13868 [50:12<2:02:33,  1.34it/s]Training epoch 1:  29% 4043/13868 [50:13<2:04:30,  1.32it/s]Training epoch 1:  29% 4044/13868 [50:14<2:02:36,  1.34it/s]Training epoch 1:  29% 4045/13868 [50:15<2:02:16,  1.34it/s]Training epoch 1:  29% 4046/13868 [50:15<2:03:03,  1.33it/s]Training epoch 1:  29% 4047/13868 [50:16<2:04:02,  1.32it/s]Training epoch 1:  29% 4048/13868 [50:17<2:04:11,  1.32it/s]Training epoch 1:  29% 4049/13868 [50:18<2:03:51,  1.32it/s]Training epoch 1:  29% 4050/13868 [50:18<2:03:09,  1.33it/s]Training epoch 1:  29% 4051/13868 [50:20<2:28:21,  1.10it/s]Training epoch 1:  29% 4052/13868 [50:20<2:19:56,  1.17it/s]Training epoch 1:  29% 4053/13868 [50:21<2:16:16,  1.20it/s]Training epoch 1:  29% 4054/13868 [50:22<2:11:33,  1.24it/s]Training epoch 1:  29% 4055/13868 [50:23<2:12:40,  1.23it/s]Training epoch 1:  29% 4056/13868 [50:23<2:07:25,  1.28it/s]Training epoch 1:  29% 4057/13868 [50:24<2:09:50,  1.26it/s]Training epoch 1:  29% 4058/13868 [50:25<2:05:52,  1.30it/s]Training epoch 1:  29% 4059/13868 [50:26<2:08:26,  1.27it/s]Training epoch 1:  29% 4060/13868 [50:26<2:03:48,  1.32it/s]Training epoch 1:  29% 4061/13868 [50:27<2:06:20,  1.29it/s]Training epoch 1:  29% 4062/13868 [50:28<2:04:08,  1.32it/s]Training epoch 1:  29% 4063/13868 [50:29<2:02:03,  1.34it/s]Training epoch 1:  29% 4064/13868 [50:29<2:02:07,  1.34it/s]Training epoch 1:  29% 4065/13868 [50:30<2:02:49,  1.33it/s]Training epoch 1:  29% 4066/13868 [50:31<2:04:54,  1.31it/s]Training epoch 1:  29% 4067/13868 [50:32<2:04:06,  1.32it/s]Training epoch 1:  29% 4068/13868 [50:32<2:04:32,  1.31it/s]Training epoch 1:  29% 4069/13868 [50:33<2:03:16,  1.32it/s]Training epoch 1:  29% 4070/13868 [50:34<2:04:27,  1.31it/s]Training epoch 1:  29% 4071/13868 [50:35<2:04:49,  1.31it/s]Training epoch 1:  29% 4072/13868 [50:36<2:04:01,  1.32it/s]Training epoch 1:  29% 4073/13868 [50:36<2:03:36,  1.32it/s]Training epoch 1:  29% 4074/13868 [50:37<2:04:35,  1.31it/s]Training epoch 1:  29% 4075/13868 [50:38<2:03:39,  1.32it/s]Training epoch 1:  29% 4076/13868 [50:39<2:04:32,  1.31it/s]Training epoch 1:  29% 4077/13868 [50:39<2:03:51,  1.32it/s]Training epoch 1:  29% 4078/13868 [50:40<2:03:47,  1.32it/s]Training epoch 1:  29% 4079/13868 [50:41<2:05:12,  1.30it/s]Training epoch 1:  29% 4080/13868 [50:42<2:06:04,  1.29it/s]Training epoch 1:  29% 4081/13868 [50:42<2:04:21,  1.31it/s]Training epoch 1:  29% 4082/13868 [50:43<2:05:48,  1.30it/s]Training epoch 1:  29% 4083/13868 [50:44<2:05:40,  1.30it/s]Training epoch 1:  29% 4084/13868 [50:45<2:05:07,  1.30it/s]Training epoch 1:  29% 4085/13868 [50:45<2:03:59,  1.32it/s]Training epoch 1:  29% 4086/13868 [50:46<2:04:41,  1.31it/s]Training epoch 1:  29% 4087/13868 [50:47<2:03:06,  1.32it/s]Training epoch 1:  29% 4088/13868 [50:48<2:04:53,  1.31it/s]Training epoch 1:  29% 4089/13868 [50:48<2:03:59,  1.31it/s]Training epoch 1:  29% 4090/13868 [50:49<2:04:06,  1.31it/s]Training epoch 1:  29% 4091/13868 [50:50<2:02:36,  1.33it/s]Training epoch 1:  30% 4092/13868 [50:51<2:03:38,  1.32it/s]Training epoch 1:  30% 4093/13868 [50:52<2:03:28,  1.32it/s]Training epoch 1:  30% 4094/13868 [50:52<2:03:42,  1.32it/s]Training epoch 1:  30% 4095/13868 [50:53<2:03:20,  1.32it/s]Training epoch 1:  30% 4096/13868 [50:54<2:03:50,  1.32it/s]Training epoch 1:  30% 4097/13868 [50:55<2:05:35,  1.30it/s]Training epoch 1:  30% 4098/13868 [50:55<2:05:16,  1.30it/s]Training epoch 1:  30% 4099/13868 [50:56<2:03:49,  1.31it/s]Training epoch 1:  30% 4100/13868 [50:57<2:10:50,  1.24it/s]Training epoch 1:  30% 4101/13868 [50:58<2:06:52,  1.28it/s]Training epoch 1:  30% 4102/13868 [50:58<2:03:56,  1.31it/s]Training epoch 1:  30% 4103/13868 [50:59<2:02:42,  1.33it/s]Training epoch 1:  30% 4104/13868 [51:00<2:03:57,  1.31it/s]Training epoch 1:  30% 4105/13868 [51:01<2:04:55,  1.30it/s]Training epoch 1:  30% 4106/13868 [51:02<2:04:47,  1.30it/s]Training epoch 1:  30% 4107/13868 [51:02<2:03:04,  1.32it/s]Training epoch 1:  30% 4108/13868 [51:03<2:01:55,  1.33it/s]Training epoch 1:  30% 4109/13868 [51:04<2:01:24,  1.34it/s]Training epoch 1:  30% 4110/13868 [51:04<1:58:59,  1.37it/s]Training epoch 1:  30% 4111/13868 [51:05<1:59:52,  1.36it/s]Training epoch 1:  30% 4112/13868 [51:06<2:01:41,  1.34it/s]Training epoch 1:  30% 4113/13868 [51:07<2:02:18,  1.33it/s]Training epoch 1:  30% 4114/13868 [51:07<2:02:44,  1.32it/s]Training epoch 1:  30% 4115/13868 [51:08<2:02:19,  1.33it/s]Training epoch 1:  30% 4116/13868 [51:09<2:04:36,  1.30it/s]Training epoch 1:  30% 4117/13868 [51:10<2:02:44,  1.32it/s]Training epoch 1:  30% 4118/13868 [51:11<2:03:20,  1.32it/s]Training epoch 1:  30% 4119/13868 [51:11<2:04:48,  1.30it/s]Training epoch 1:  30% 4120/13868 [51:12<2:04:59,  1.30it/s]Training epoch 1:  30% 4121/13868 [51:13<2:05:02,  1.30it/s]Training epoch 1:  30% 4122/13868 [51:14<2:04:06,  1.31it/s]Training epoch 1:  30% 4123/13868 [51:14<2:03:56,  1.31it/s]Training epoch 1:  30% 4124/13868 [51:15<2:04:06,  1.31it/s]Training epoch 1:  30% 4125/13868 [51:16<2:04:20,  1.31it/s]Training epoch 1:  30% 4126/13868 [51:17<2:02:35,  1.32it/s]Training epoch 1:  30% 4127/13868 [51:17<2:05:07,  1.30it/s]Training epoch 1:  30% 4128/13868 [51:18<2:03:13,  1.32it/s]Training epoch 1:  30% 4129/13868 [51:19<2:04:07,  1.31it/s]Training epoch 1:  30% 4130/13868 [51:20<2:05:35,  1.29it/s]Training epoch 1:  30% 4131/13868 [51:20<2:03:39,  1.31it/s]Training epoch 1:  30% 4132/13868 [51:21<2:04:07,  1.31it/s]Training epoch 1:  30% 4133/13868 [51:22<2:03:55,  1.31it/s]Training epoch 1:  30% 4134/13868 [51:23<2:04:59,  1.30it/s]Training epoch 1:  30% 4135/13868 [51:24<2:05:14,  1.30it/s]Training epoch 1:  30% 4136/13868 [51:24<2:06:20,  1.28it/s]Training epoch 1:  30% 4137/13868 [51:25<2:04:27,  1.30it/s]Training epoch 1:  30% 4138/13868 [51:26<2:03:48,  1.31it/s]Training epoch 1:  30% 4139/13868 [51:27<2:02:50,  1.32it/s]Training epoch 1:  30% 4140/13868 [51:27<2:03:03,  1.32it/s]Training epoch 1:  30% 4141/13868 [51:28<2:02:08,  1.33it/s]Training epoch 1:  30% 4142/13868 [51:29<2:02:43,  1.32it/s]Training epoch 1:  30% 4143/13868 [51:30<2:02:58,  1.32it/s]Training epoch 1:  30% 4144/13868 [51:30<2:03:57,  1.31it/s]Training epoch 1:  30% 4145/13868 [51:31<2:02:23,  1.32it/s]Training epoch 1:  30% 4146/13868 [51:32<2:04:14,  1.30it/s]Training epoch 1:  30% 4147/13868 [51:33<2:06:36,  1.28it/s]Training epoch 1:  30% 4148/13868 [51:34<2:06:44,  1.28it/s]Training epoch 1:  30% 4149/13868 [51:34<2:06:12,  1.28it/s]Training epoch 1:  30% 4150/13868 [51:35<2:06:23,  1.28it/s]Training epoch 1:  30% 4151/13868 [51:36<2:05:11,  1.29it/s]Training epoch 1:  30% 4152/13868 [51:37<2:04:33,  1.30it/s]Training epoch 1:  30% 4153/13868 [51:37<2:04:11,  1.30it/s]Training epoch 1:  30% 4154/13868 [51:38<2:04:01,  1.31it/s]Training epoch 1:  30% 4155/13868 [51:39<2:04:11,  1.30it/s]Training epoch 1:  30% 4156/13868 [51:40<2:03:30,  1.31it/s]Training epoch 1:  30% 4157/13868 [51:40<2:04:20,  1.30it/s]Training epoch 1:  30% 4158/13868 [51:41<2:05:17,  1.29it/s]Training epoch 1:  30% 4159/13868 [51:42<2:04:32,  1.30it/s]Training epoch 1:  30% 4160/13868 [51:43<2:03:02,  1.31it/s]Training epoch 1:  30% 4161/13868 [51:43<2:03:22,  1.31it/s]Training epoch 1:  30% 4162/13868 [51:44<2:00:51,  1.34it/s]Training epoch 1:  30% 4163/13868 [51:45<2:03:08,  1.31it/s]Training epoch 1:  30% 4164/13868 [51:46<2:02:56,  1.32it/s]Training epoch 1:  30% 4165/13868 [51:46<2:00:14,  1.34it/s]Training epoch 1:  30% 4166/13868 [51:47<2:01:35,  1.33it/s]Training epoch 1:  30% 4167/13868 [51:48<2:01:12,  1.33it/s]Training epoch 1:  30% 4168/13868 [51:49<2:01:11,  1.33it/s]Training epoch 1:  30% 4169/13868 [51:50<2:03:18,  1.31it/s]Training epoch 1:  30% 4170/13868 [51:50<2:03:24,  1.31it/s]Training epoch 1:  30% 4171/13868 [51:51<2:03:04,  1.31it/s]Training epoch 1:  30% 4172/13868 [51:52<2:01:45,  1.33it/s]Training epoch 1:  30% 4173/13868 [51:53<2:01:24,  1.33it/s]Training epoch 1:  30% 4174/13868 [51:53<2:02:40,  1.32it/s]Training epoch 1:  30% 4175/13868 [51:54<2:02:26,  1.32it/s]Training epoch 1:  30% 4176/13868 [51:55<2:02:32,  1.32it/s]Training epoch 1:  30% 4177/13868 [51:56<2:02:35,  1.32it/s]Training epoch 1:  30% 4178/13868 [51:56<2:02:08,  1.32it/s]Training epoch 1:  30% 4179/13868 [51:57<2:02:11,  1.32it/s]Training epoch 1:  30% 4180/13868 [51:58<2:00:15,  1.34it/s]Training epoch 1:  30% 4181/13868 [51:59<1:59:55,  1.35it/s]Training epoch 1:  30% 4182/13868 [51:59<1:58:30,  1.36it/s]Training epoch 1:  30% 4183/13868 [52:00<1:58:58,  1.36it/s]Training epoch 1:  30% 4184/13868 [52:01<2:00:57,  1.33it/s]Training epoch 1:  30% 4185/13868 [52:02<2:02:16,  1.32it/s]Training epoch 1:  30% 4186/13868 [52:02<2:01:50,  1.32it/s]Training epoch 1:  30% 4187/13868 [52:03<2:03:19,  1.31it/s]Training epoch 1:  30% 4188/13868 [52:04<2:04:01,  1.30it/s]Training epoch 1:  30% 4189/13868 [52:05<2:04:13,  1.30it/s]Training epoch 1:  30% 4190/13868 [52:05<2:01:55,  1.32it/s]Training epoch 1:  30% 4191/13868 [52:06<2:01:55,  1.32it/s]Training epoch 1:  30% 4192/13868 [52:07<1:59:59,  1.34it/s]Training epoch 1:  30% 4193/13868 [52:08<2:01:20,  1.33it/s]Training epoch 1:  30% 4194/13868 [52:08<2:01:21,  1.33it/s]Training epoch 1:  30% 4195/13868 [52:09<2:00:43,  1.34it/s]Training epoch 1:  30% 4196/13868 [52:10<2:00:39,  1.34it/s]Training epoch 1:  30% 4197/13868 [52:11<2:02:19,  1.32it/s]Training epoch 1:  30% 4198/13868 [52:11<2:02:23,  1.32it/s]Training epoch 1:  30% 4199/13868 [52:12<2:04:07,  1.30it/s]Training epoch 1:  30% 4200/13868 [52:13<2:09:40,  1.24it/s]Training epoch 1:  30% 4201/13868 [52:14<2:07:43,  1.26it/s]Training epoch 1:  30% 4202/13868 [52:15<2:04:04,  1.30it/s]Training epoch 1:  30% 4203/13868 [52:15<2:02:36,  1.31it/s]Training epoch 1:  30% 4204/13868 [52:16<1:59:54,  1.34it/s]Training epoch 1:  30% 4205/13868 [52:17<2:00:04,  1.34it/s]Training epoch 1:  30% 4206/13868 [52:17<2:00:04,  1.34it/s]Training epoch 1:  30% 4207/13868 [52:18<2:02:27,  1.31it/s]Training epoch 1:  30% 4208/13868 [52:19<2:02:30,  1.31it/s]Training epoch 1:  30% 4209/13868 [52:20<2:03:35,  1.30it/s]Training epoch 1:  30% 4210/13868 [52:21<2:03:25,  1.30it/s]Training epoch 1:  30% 4211/13868 [52:21<2:02:43,  1.31it/s]Training epoch 1:  30% 4212/13868 [52:22<2:01:31,  1.32it/s]Training epoch 1:  30% 4213/13868 [52:23<2:00:43,  1.33it/s]Training epoch 1:  30% 4214/13868 [52:24<2:00:12,  1.34it/s]Training epoch 1:  30% 4215/13868 [52:24<2:01:20,  1.33it/s]Training epoch 1:  30% 4216/13868 [52:25<1:59:59,  1.34it/s]Training epoch 1:  30% 4217/13868 [52:26<2:00:38,  1.33it/s]Training epoch 1:  30% 4218/13868 [52:27<2:00:26,  1.34it/s]Training epoch 1:  30% 4219/13868 [52:27<2:01:06,  1.33it/s]Training epoch 1:  30% 4220/13868 [52:28<2:00:00,  1.34it/s]Training epoch 1:  30% 4221/13868 [52:29<1:59:16,  1.35it/s]Training epoch 1:  30% 4222/13868 [52:30<1:59:21,  1.35it/s]Training epoch 1:  30% 4223/13868 [52:30<1:59:26,  1.35it/s]Training epoch 1:  30% 4224/13868 [52:31<1:58:23,  1.36it/s]Training epoch 1:  30% 4225/13868 [52:32<1:58:01,  1.36it/s]Training epoch 1:  30% 4226/13868 [52:32<1:58:40,  1.35it/s]Training epoch 1:  30% 4227/13868 [52:33<1:57:54,  1.36it/s]Training epoch 1:  30% 4228/13868 [52:34<1:58:33,  1.36it/s]Training epoch 1:  30% 4229/13868 [52:35<2:01:34,  1.32it/s]Training epoch 1:  31% 4230/13868 [52:36<2:02:38,  1.31it/s]Training epoch 1:  31% 4231/13868 [52:36<2:00:58,  1.33it/s]Training epoch 1:  31% 4232/13868 [52:37<1:59:58,  1.34it/s]Training epoch 1:  31% 4233/13868 [52:38<1:59:34,  1.34it/s]Training epoch 1:  31% 4234/13868 [52:39<2:01:47,  1.32it/s]Training epoch 1:  31% 4235/13868 [52:39<2:01:53,  1.32it/s]Training epoch 1:  31% 4236/13868 [52:40<2:01:23,  1.32it/s]Training epoch 1:  31% 4237/13868 [52:41<1:59:59,  1.34it/s]Training epoch 1:  31% 4238/13868 [52:41<1:59:33,  1.34it/s]Training epoch 1:  31% 4239/13868 [52:42<1:59:46,  1.34it/s]Training epoch 1:  31% 4240/13868 [52:43<2:00:11,  1.34it/s]Training epoch 1:  31% 4241/13868 [52:44<1:59:39,  1.34it/s]Training epoch 1:  31% 4242/13868 [52:44<1:56:41,  1.37it/s]Training epoch 1:  31% 4243/13868 [52:45<1:57:51,  1.36it/s]Training epoch 1:  31% 4244/13868 [52:46<1:58:19,  1.36it/s]Training epoch 1:  31% 4245/13868 [52:47<1:58:59,  1.35it/s]Training epoch 1:  31% 4246/13868 [52:47<1:59:42,  1.34it/s]Training epoch 1:  31% 4247/13868 [52:48<2:01:17,  1.32it/s]Training epoch 1:  31% 4248/13868 [52:49<2:01:19,  1.32it/s]Training epoch 1:  31% 4249/13868 [52:50<2:01:56,  1.31it/s]Training epoch 1:  31% 4250/13868 [52:51<2:02:52,  1.30it/s]Training epoch 1:  31% 4251/13868 [52:51<2:01:04,  1.32it/s]Training epoch 1:  31% 4252/13868 [52:52<2:00:58,  1.32it/s]Training epoch 1:  31% 4253/13868 [52:53<1:59:54,  1.34it/s]Training epoch 1:  31% 4254/13868 [52:53<1:59:20,  1.34it/s]Training epoch 1:  31% 4255/13868 [52:54<2:01:07,  1.32it/s]Training epoch 1:  31% 4256/13868 [52:55<2:01:02,  1.32it/s]Training epoch 1:  31% 4257/13868 [52:56<2:02:46,  1.30it/s]Training epoch 1:  31% 4258/13868 [52:57<2:02:22,  1.31it/s]Training epoch 1:  31% 4259/13868 [52:57<2:01:38,  1.32it/s]Training epoch 1:  31% 4260/13868 [52:58<2:01:23,  1.32it/s]Training epoch 1:  31% 4261/13868 [52:59<2:01:36,  1.32it/s]Training epoch 1:  31% 4262/13868 [53:00<2:02:13,  1.31it/s]Training epoch 1:  31% 4263/13868 [53:00<2:02:39,  1.31it/s]Training epoch 1:  31% 4264/13868 [53:01<2:02:42,  1.30it/s]Training epoch 1:  31% 4265/13868 [53:02<2:03:30,  1.30it/s]Training epoch 1:  31% 4266/13868 [53:03<2:02:11,  1.31it/s]Training epoch 1:  31% 4267/13868 [53:03<2:01:31,  1.32it/s]Training epoch 1:  31% 4268/13868 [53:04<2:00:26,  1.33it/s]Training epoch 1:  31% 4269/13868 [53:05<1:59:37,  1.34it/s]Training epoch 1:  31% 4270/13868 [53:06<1:59:59,  1.33it/s]Training epoch 1:  31% 4271/13868 [53:06<1:59:53,  1.33it/s]Training epoch 1:  31% 4272/13868 [53:07<1:57:22,  1.36it/s]Training epoch 1:  31% 4273/13868 [53:08<1:57:57,  1.36it/s]Training epoch 1:  31% 4274/13868 [53:09<1:57:43,  1.36it/s]Training epoch 1:  31% 4275/13868 [53:09<1:58:47,  1.35it/s]Training epoch 1:  31% 4276/13868 [53:10<1:59:18,  1.34it/s]Training epoch 1:  31% 4277/13868 [53:11<1:59:07,  1.34it/s]Training epoch 1:  31% 4278/13868 [53:12<1:57:54,  1.36it/s]Training epoch 1:  31% 4279/13868 [53:12<1:57:09,  1.36it/s]Training epoch 1:  31% 4280/13868 [53:13<1:58:50,  1.34it/s]Training epoch 1:  31% 4281/13868 [53:14<1:59:47,  1.33it/s]Training epoch 1:  31% 4282/13868 [53:15<1:59:13,  1.34it/s]Training epoch 1:  31% 4283/13868 [53:15<2:00:12,  1.33it/s]Training epoch 1:  31% 4284/13868 [53:16<2:01:25,  1.32it/s]Training epoch 1:  31% 4285/13868 [53:17<2:02:43,  1.30it/s]Training epoch 1:  31% 4286/13868 [53:18<2:02:41,  1.30it/s]Training epoch 1:  31% 4287/13868 [53:18<2:04:30,  1.28it/s]Training epoch 1:  31% 4288/13868 [53:19<2:02:38,  1.30it/s]Training epoch 1:  31% 4289/13868 [53:20<2:02:46,  1.30it/s]Training epoch 1:  31% 4290/13868 [53:21<2:01:42,  1.31it/s]Training epoch 1:  31% 4291/13868 [53:21<2:01:08,  1.32it/s]Training epoch 1:  31% 4292/13868 [53:22<2:01:49,  1.31it/s]Training epoch 1:  31% 4293/13868 [53:23<2:02:10,  1.31it/s]Training epoch 1:  31% 4294/13868 [53:24<2:01:27,  1.31it/s]Training epoch 1:  31% 4295/13868 [53:25<2:02:29,  1.30it/s]Training epoch 1:  31% 4296/13868 [53:25<2:01:48,  1.31it/s]Training epoch 1:  31% 4297/13868 [53:26<2:01:14,  1.32it/s]Training epoch 1:  31% 4298/13868 [53:27<2:00:14,  1.33it/s]Training epoch 1:  31% 4299/13868 [53:28<2:01:22,  1.31it/s]Training epoch 1:  31% 4300/13868 [53:28<2:08:02,  1.25it/s]Training epoch 1:  31% 4301/13868 [53:29<2:05:55,  1.27it/s]Training epoch 1:  31% 4302/13868 [53:30<2:03:20,  1.29it/s]Training epoch 1:  31% 4303/13868 [53:31<2:03:43,  1.29it/s]Training epoch 1:  31% 4304/13868 [53:31<2:01:16,  1.31it/s]Training epoch 1:  31% 4305/13868 [53:32<2:01:27,  1.31it/s]Training epoch 1:  31% 4306/13868 [53:33<2:01:48,  1.31it/s]Training epoch 1:  31% 4307/13868 [53:34<2:01:49,  1.31it/s]Training epoch 1:  31% 4308/13868 [53:34<2:00:46,  1.32it/s]Training epoch 1:  31% 4309/13868 [53:35<2:00:23,  1.32it/s]Training epoch 1:  31% 4310/13868 [53:36<1:59:45,  1.33it/s]Training epoch 1:  31% 4311/13868 [53:37<1:59:28,  1.33it/s]Training epoch 1:  31% 4312/13868 [53:37<1:58:02,  1.35it/s]Training epoch 1:  31% 4313/13868 [53:38<1:57:27,  1.36it/s]Training epoch 1:  31% 4314/13868 [53:39<1:58:41,  1.34it/s]Training epoch 1:  31% 4315/13868 [53:40<2:00:01,  1.33it/s]Training epoch 1:  31% 4316/13868 [53:40<1:59:34,  1.33it/s]Training epoch 1:  31% 4317/13868 [53:41<1:59:57,  1.33it/s]Training epoch 1:  31% 4318/13868 [53:42<2:00:00,  1.33it/s]Training epoch 1:  31% 4319/13868 [53:43<2:00:08,  1.32it/s]Training epoch 1:  31% 4320/13868 [53:43<1:59:56,  1.33it/s]Training epoch 1:  31% 4321/13868 [53:44<2:00:54,  1.32it/s]Training epoch 1:  31% 4322/13868 [53:45<1:57:34,  1.35it/s]Training epoch 1:  31% 4323/13868 [53:46<1:58:09,  1.35it/s]Training epoch 1:  31% 4324/13868 [53:46<1:57:56,  1.35it/s]Training epoch 1:  31% 4325/13868 [53:47<1:59:30,  1.33it/s]Training epoch 1:  31% 4326/13868 [53:48<1:59:18,  1.33it/s]Training epoch 1:  31% 4327/13868 [53:49<1:58:43,  1.34it/s]Training epoch 1:  31% 4328/13868 [53:49<1:57:39,  1.35it/s]Training epoch 1:  31% 4329/13868 [53:50<1:56:37,  1.36it/s]Training epoch 1:  31% 4330/13868 [53:51<1:58:37,  1.34it/s]Training epoch 1:  31% 4331/13868 [53:52<1:58:58,  1.34it/s]Training epoch 1:  31% 4332/13868 [53:52<1:59:40,  1.33it/s]Training epoch 1:  31% 4333/13868 [53:53<2:00:33,  1.32it/s]Training epoch 1:  31% 4334/13868 [53:54<2:01:44,  1.31it/s]Training epoch 1:  31% 4335/13868 [53:55<2:00:33,  1.32it/s]Training epoch 1:  31% 4336/13868 [53:55<1:59:40,  1.33it/s]Training epoch 1:  31% 4337/13868 [53:56<1:59:49,  1.33it/s]Training epoch 1:  31% 4338/13868 [53:57<1:59:54,  1.32it/s]Training epoch 1:  31% 4339/13868 [53:58<2:00:03,  1.32it/s]Training epoch 1:  31% 4340/13868 [53:59<2:00:10,  1.32it/s]Training epoch 1:  31% 4341/13868 [53:59<1:59:31,  1.33it/s]Training epoch 1:  31% 4342/13868 [54:00<1:58:42,  1.34it/s]Training epoch 1:  31% 4343/13868 [54:01<1:59:35,  1.33it/s]Training epoch 1:  31% 4344/13868 [54:01<1:58:57,  1.33it/s]Training epoch 1:  31% 4345/13868 [54:02<1:58:59,  1.33it/s]Training epoch 1:  31% 4346/13868 [54:03<1:57:28,  1.35it/s]Training epoch 1:  31% 4347/13868 [54:04<1:58:31,  1.34it/s]Training epoch 1:  31% 4348/13868 [54:04<1:59:33,  1.33it/s]Training epoch 1:  31% 4349/13868 [54:05<2:00:29,  1.32it/s]Training epoch 1:  31% 4350/13868 [54:06<2:00:53,  1.31it/s]Training epoch 1:  31% 4351/13868 [54:07<2:00:24,  1.32it/s]Training epoch 1:  31% 4352/13868 [54:08<1:59:47,  1.32it/s]Training epoch 1:  31% 4353/13868 [54:08<1:58:24,  1.34it/s]Training epoch 1:  31% 4354/13868 [54:09<1:57:41,  1.35it/s]Training epoch 1:  31% 4355/13868 [54:10<1:58:04,  1.34it/s]Training epoch 1:  31% 4356/13868 [54:10<1:58:47,  1.33it/s]Training epoch 1:  31% 4357/13868 [54:11<1:59:07,  1.33it/s]Training epoch 1:  31% 4358/13868 [54:12<1:59:29,  1.33it/s]Training epoch 1:  31% 4359/13868 [54:13<1:58:29,  1.34it/s]Training epoch 1:  31% 4360/13868 [54:13<1:58:12,  1.34it/s]Training epoch 1:  31% 4361/13868 [54:14<1:59:58,  1.32it/s]Training epoch 1:  31% 4362/13868 [54:15<2:00:25,  1.32it/s]Training epoch 1:  31% 4363/13868 [54:16<1:58:45,  1.33it/s]Training epoch 1:  31% 4364/13868 [54:17<1:59:58,  1.32it/s]Training epoch 1:  31% 4365/13868 [54:17<2:00:33,  1.31it/s]Training epoch 1:  31% 4366/13868 [54:18<1:59:54,  1.32it/s]Training epoch 1:  31% 4367/13868 [54:19<1:59:28,  1.33it/s]Training epoch 1:  31% 4368/13868 [54:20<2:00:13,  1.32it/s]Training epoch 1:  32% 4369/13868 [54:20<2:00:32,  1.31it/s]Training epoch 1:  32% 4370/13868 [54:21<1:58:48,  1.33it/s]Training epoch 1:  32% 4371/13868 [54:22<1:57:55,  1.34it/s]Training epoch 1:  32% 4372/13868 [54:23<1:57:42,  1.34it/s]Training epoch 1:  32% 4373/13868 [54:23<1:58:33,  1.33it/s]Training epoch 1:  32% 4374/13868 [54:24<2:00:39,  1.31it/s]Training epoch 1:  32% 4375/13868 [54:25<1:59:54,  1.32it/s]Training epoch 1:  32% 4376/13868 [54:26<1:59:28,  1.32it/s]Training epoch 1:  32% 4377/13868 [54:26<1:57:34,  1.35it/s]Training epoch 1:  32% 4378/13868 [54:27<1:58:13,  1.34it/s]Training epoch 1:  32% 4379/13868 [54:28<1:57:53,  1.34it/s]Training epoch 1:  32% 4380/13868 [54:29<1:57:10,  1.35it/s]Training epoch 1:  32% 4381/13868 [54:29<1:58:29,  1.33it/s]Training epoch 1:  32% 4382/13868 [54:30<1:58:49,  1.33it/s]Training epoch 1:  32% 4383/13868 [54:31<1:59:34,  1.32it/s]Training epoch 1:  32% 4384/13868 [54:32<1:59:13,  1.33it/s]Training epoch 1:  32% 4385/13868 [54:32<1:59:51,  1.32it/s]Training epoch 1:  32% 4386/13868 [54:33<1:59:36,  1.32it/s]Training epoch 1:  32% 4387/13868 [54:34<2:00:58,  1.31it/s]Training epoch 1:  32% 4388/13868 [54:35<1:59:40,  1.32it/s]Training epoch 1:  32% 4389/13868 [54:35<1:58:09,  1.34it/s]Training epoch 1:  32% 4390/13868 [54:36<1:57:22,  1.35it/s]Training epoch 1:  32% 4391/13868 [54:37<1:58:03,  1.34it/s]Training epoch 1:  32% 4392/13868 [54:38<1:58:08,  1.34it/s]Training epoch 1:  32% 4393/13868 [54:38<1:57:40,  1.34it/s]Training epoch 1:  32% 4394/13868 [54:39<1:58:50,  1.33it/s]Training epoch 1:  32% 4395/13868 [54:40<2:01:25,  1.30it/s]Training epoch 1:  32% 4396/13868 [54:41<2:01:07,  1.30it/s]Training epoch 1:  32% 4397/13868 [54:41<2:00:17,  1.31it/s]Training epoch 1:  32% 4398/13868 [54:42<2:00:13,  1.31it/s]Training epoch 1:  32% 4399/13868 [54:43<2:00:42,  1.31it/s]Training epoch 1:  32% 4400/13868 [54:44<2:06:11,  1.25it/s]Training epoch 1:  32% 4401/13868 [54:45<2:04:28,  1.27it/s]Training epoch 1:  32% 4402/13868 [54:45<2:01:52,  1.29it/s]Training epoch 1:  32% 4403/13868 [54:46<2:01:24,  1.30it/s]Training epoch 1:  32% 4404/13868 [54:47<1:58:56,  1.33it/s]Training epoch 1:  32% 4405/13868 [54:48<1:58:53,  1.33it/s]Training epoch 1:  32% 4406/13868 [54:48<1:59:15,  1.32it/s]Training epoch 1:  32% 4407/13868 [54:49<1:58:36,  1.33it/s]Training epoch 1:  32% 4408/13868 [54:50<1:58:55,  1.33it/s]Training epoch 1:  32% 4409/13868 [54:51<1:58:39,  1.33it/s]Training epoch 1:  32% 4410/13868 [54:51<1:59:07,  1.32it/s]Training epoch 1:  32% 4411/13868 [54:52<1:59:31,  1.32it/s]Training epoch 1:  32% 4412/13868 [54:53<1:59:14,  1.32it/s]Training epoch 1:  32% 4413/13868 [54:54<2:00:15,  1.31it/s]Training epoch 1:  32% 4414/13868 [54:54<1:59:35,  1.32it/s]Training epoch 1:  32% 4415/13868 [54:55<1:58:57,  1.32it/s]Training epoch 1:  32% 4416/13868 [54:56<1:58:06,  1.33it/s]Training epoch 1:  32% 4417/13868 [54:57<1:57:58,  1.34it/s]Training epoch 1:  32% 4418/13868 [54:57<1:56:01,  1.36it/s]Training epoch 1:  32% 4419/13868 [54:58<1:58:17,  1.33it/s]Training epoch 1:  32% 4420/13868 [54:59<1:57:34,  1.34it/s]Training epoch 1:  32% 4421/13868 [55:00<1:57:35,  1.34it/s]Training epoch 1:  32% 4422/13868 [55:00<1:57:23,  1.34it/s]Training epoch 1:  32% 4423/13868 [55:01<1:56:43,  1.35it/s]Training epoch 1:  32% 4424/13868 [55:02<1:57:27,  1.34it/s]Training epoch 1:  32% 4425/13868 [55:03<1:57:09,  1.34it/s]Training epoch 1:  32% 4426/13868 [55:03<1:57:01,  1.34it/s]Training epoch 1:  32% 4427/13868 [55:04<1:56:14,  1.35it/s]Training epoch 1:  32% 4428/13868 [55:05<1:57:21,  1.34it/s]Training epoch 1:  32% 4429/13868 [55:06<1:57:50,  1.33it/s]Training epoch 1:  32% 4430/13868 [55:06<1:57:46,  1.34it/s]Training epoch 1:  32% 4431/13868 [55:07<1:56:59,  1.34it/s]Training epoch 1:  32% 4432/13868 [55:08<1:57:33,  1.34it/s]Training epoch 1:  32% 4433/13868 [55:09<1:59:25,  1.32it/s]Training epoch 1:  32% 4434/13868 [55:09<1:58:33,  1.33it/s]Training epoch 1:  32% 4435/13868 [55:10<1:58:27,  1.33it/s]Training epoch 1:  32% 4436/13868 [55:11<1:57:07,  1.34it/s]Training epoch 1:  32% 4437/13868 [55:12<1:57:11,  1.34it/s]Training epoch 1:  32% 4438/13868 [55:12<1:57:00,  1.34it/s]Training epoch 1:  32% 4439/13868 [55:13<1:58:05,  1.33it/s]Training epoch 1:  32% 4440/13868 [55:14<1:58:28,  1.33it/s]Training epoch 1:  32% 4441/13868 [55:15<1:57:23,  1.34it/s]Training epoch 1:  32% 4442/13868 [55:15<1:58:32,  1.33it/s]Training epoch 1:  32% 4443/13868 [55:16<1:58:12,  1.33it/s]Training epoch 1:  32% 4444/13868 [55:17<1:57:58,  1.33it/s]Training epoch 1:  32% 4445/13868 [55:18<1:56:53,  1.34it/s]Training epoch 1:  32% 4446/13868 [55:18<1:57:59,  1.33it/s]Training epoch 1:  32% 4447/13868 [55:19<1:59:14,  1.32it/s]Training epoch 1:  32% 4448/13868 [55:20<2:01:12,  1.30it/s]Training epoch 1:  32% 4449/13868 [55:21<1:59:53,  1.31it/s]Training epoch 1:  32% 4450/13868 [55:21<2:01:21,  1.29it/s]Training epoch 1:  32% 4451/13868 [55:22<2:01:13,  1.29it/s]Training epoch 1:  32% 4452/13868 [55:23<2:01:15,  1.29it/s]Training epoch 1:  32% 4453/13868 [55:24<1:58:41,  1.32it/s]Training epoch 1:  32% 4454/13868 [55:24<1:58:19,  1.33it/s]Training epoch 1:  32% 4455/13868 [55:25<1:57:19,  1.34it/s]Training epoch 1:  32% 4456/13868 [55:26<1:59:47,  1.31it/s]Training epoch 1:  32% 4457/13868 [55:27<1:58:43,  1.32it/s]Training epoch 1:  32% 4458/13868 [55:27<1:57:34,  1.33it/s]Training epoch 1:  32% 4459/13868 [55:28<1:56:09,  1.35it/s]Training epoch 1:  32% 4460/13868 [55:29<1:57:16,  1.34it/s]Training epoch 1:  32% 4461/13868 [55:30<1:57:26,  1.33it/s]Training epoch 1:  32% 4462/13868 [55:30<1:56:06,  1.35it/s]Training epoch 1:  32% 4463/13868 [55:31<1:56:24,  1.35it/s]Training epoch 1:  32% 4464/13868 [55:32<1:55:54,  1.35it/s]Training epoch 1:  32% 4465/13868 [55:33<1:57:24,  1.33it/s]Training epoch 1:  32% 4466/13868 [55:33<1:57:40,  1.33it/s]Training epoch 1:  32% 4467/13868 [55:34<1:59:19,  1.31it/s]Training epoch 1:  32% 4468/13868 [55:35<1:58:16,  1.32it/s]Training epoch 1:  32% 4469/13868 [55:36<1:57:53,  1.33it/s]Training epoch 1:  32% 4470/13868 [55:36<1:57:51,  1.33it/s]Training epoch 1:  32% 4471/13868 [55:37<1:56:32,  1.34it/s]Training epoch 1:  32% 4472/13868 [55:38<1:58:55,  1.32it/s]Training epoch 1:  32% 4473/13868 [55:39<1:57:59,  1.33it/s]Training epoch 1:  32% 4474/13868 [55:39<1:57:53,  1.33it/s]Training epoch 1:  32% 4475/13868 [55:40<1:56:29,  1.34it/s]Training epoch 1:  32% 4476/13868 [55:41<2:00:14,  1.30it/s]Training epoch 1:  32% 4477/13868 [55:42<1:58:59,  1.32it/s]Training epoch 1:  32% 4478/13868 [55:42<1:58:44,  1.32it/s]Training epoch 1:  32% 4479/13868 [55:43<1:57:19,  1.33it/s]Training epoch 1:  32% 4480/13868 [55:44<1:57:35,  1.33it/s]Training epoch 1:  32% 4481/13868 [55:45<1:56:46,  1.34it/s]Training epoch 1:  32% 4482/13868 [55:45<1:57:22,  1.33it/s]Training epoch 1:  32% 4483/13868 [55:46<1:55:30,  1.35it/s]Training epoch 1:  32% 4484/13868 [55:47<1:55:40,  1.35it/s]Training epoch 1:  32% 4485/13868 [55:48<1:56:05,  1.35it/s]Training epoch 1:  32% 4486/13868 [55:48<1:55:56,  1.35it/s]Training epoch 1:  32% 4487/13868 [55:49<1:56:58,  1.34it/s]Training epoch 1:  32% 4488/13868 [55:50<1:57:58,  1.33it/s]Training epoch 1:  32% 4489/13868 [55:51<1:57:58,  1.32it/s]Training epoch 1:  32% 4490/13868 [55:51<1:57:11,  1.33it/s]Training epoch 1:  32% 4491/13868 [55:52<1:56:42,  1.34it/s]Training epoch 1:  32% 4492/13868 [55:53<1:58:47,  1.32it/s]Training epoch 1:  32% 4493/13868 [55:54<1:59:56,  1.30it/s]Training epoch 1:  32% 4494/13868 [55:55<2:01:20,  1.29it/s]Training epoch 1:  32% 4495/13868 [55:55<2:00:11,  1.30it/s]Training epoch 1:  32% 4496/13868 [55:56<2:00:47,  1.29it/s]Training epoch 1:  32% 4497/13868 [55:57<2:01:08,  1.29it/s]Training epoch 1:  32% 4498/13868 [55:58<1:59:49,  1.30it/s]Training epoch 1:  32% 4499/13868 [55:58<1:57:26,  1.33it/s]Training epoch 1:  32% 4500/13868 [55:59<2:04:21,  1.26it/s]Training epoch 1:  32% 4501/13868 [56:00<2:02:24,  1.28it/s]Training epoch 1:  32% 4502/13868 [56:01<2:00:21,  1.30it/s]Training epoch 1:  32% 4503/13868 [56:01<1:57:49,  1.32it/s]Training epoch 1:  32% 4504/13868 [56:02<1:57:47,  1.32it/s]Training epoch 1:  32% 4505/13868 [56:03<1:58:34,  1.32it/s]Training epoch 1:  32% 4506/13868 [56:04<1:58:40,  1.31it/s]Training epoch 1:  32% 4507/13868 [56:04<1:58:57,  1.31it/s]Training epoch 1:  33% 4508/13868 [56:05<1:59:11,  1.31it/s]Training epoch 1:  33% 4509/13868 [56:06<1:57:53,  1.32it/s]Training epoch 1:  33% 4510/13868 [56:07<1:55:44,  1.35it/s]Training epoch 1:  33% 4511/13868 [56:07<1:55:22,  1.35it/s]Training epoch 1:  33% 4512/13868 [56:08<1:56:22,  1.34it/s]Training epoch 1:  33% 4513/13868 [56:09<1:54:46,  1.36it/s]Training epoch 1:  33% 4514/13868 [56:10<1:55:18,  1.35it/s]Training epoch 1:  33% 4515/13868 [56:10<1:56:51,  1.33it/s]Training epoch 1:  33% 4516/13868 [56:11<1:56:58,  1.33it/s]Training epoch 1:  33% 4517/13868 [56:12<1:57:02,  1.33it/s]Training epoch 1:  33% 4518/13868 [56:13<1:57:51,  1.32it/s]Training epoch 1:  33% 4519/13868 [56:13<1:57:36,  1.32it/s]Training epoch 1:  33% 4520/13868 [56:14<1:55:02,  1.35it/s]Training epoch 1:  33% 4521/13868 [56:15<1:55:09,  1.35it/s]Training epoch 1:  33% 4522/13868 [56:16<1:57:19,  1.33it/s]Training epoch 1:  33% 4523/13868 [56:16<1:55:52,  1.34it/s]Training epoch 1:  33% 4524/13868 [56:17<1:55:14,  1.35it/s]Training epoch 1:  33% 4525/13868 [56:18<1:54:45,  1.36it/s]Training epoch 1:  33% 4526/13868 [56:19<1:56:16,  1.34it/s]Training epoch 1:  33% 4527/13868 [56:19<1:56:37,  1.33it/s]Training epoch 1:  33% 4528/13868 [56:20<1:55:26,  1.35it/s]Training epoch 1:  33% 4529/13868 [56:21<1:54:07,  1.36it/s]Training epoch 1:  33% 4530/13868 [56:22<1:56:27,  1.34it/s]Training epoch 1:  33% 4531/13868 [56:22<1:57:43,  1.32it/s]Training epoch 1:  33% 4532/13868 [56:23<1:57:35,  1.32it/s]Training epoch 1:  33% 4533/13868 [56:24<1:55:43,  1.34it/s]Training epoch 1:  33% 4534/13868 [56:25<1:57:06,  1.33it/s]Training epoch 1:  33% 4535/13868 [56:25<1:56:08,  1.34it/s]Training epoch 1:  33% 4536/13868 [56:26<1:57:07,  1.33it/s]Training epoch 1:  33% 4537/13868 [56:27<1:55:41,  1.34it/s]Training epoch 1:  33% 4538/13868 [56:28<1:55:43,  1.34it/s]Training epoch 1:  33% 4539/13868 [56:28<1:55:02,  1.35it/s]Training epoch 1:  33% 4540/13868 [56:29<1:54:56,  1.35it/s]Training epoch 1:  33% 4541/13868 [56:30<1:54:54,  1.35it/s]Training epoch 1:  33% 4542/13868 [56:31<1:56:13,  1.34it/s]Training epoch 1:  33% 4543/13868 [56:31<1:56:07,  1.34it/s]Training epoch 1:  33% 4544/13868 [56:32<1:57:40,  1.32it/s]Training epoch 1:  33% 4545/13868 [56:33<1:56:57,  1.33it/s]Training epoch 1:  33% 4546/13868 [56:34<1:57:24,  1.32it/s]Training epoch 1:  33% 4547/13868 [56:34<1:57:10,  1.33it/s]Training epoch 1:  33% 4548/13868 [56:35<1:57:57,  1.32it/s]Training epoch 1:  33% 4549/13868 [56:36<1:57:14,  1.32it/s]Training epoch 1:  33% 4550/13868 [56:37<1:58:42,  1.31it/s]Training epoch 1:  33% 4551/13868 [56:37<1:58:59,  1.31it/s]Training epoch 1:  33% 4552/13868 [56:38<1:58:35,  1.31it/s]Training epoch 1:  33% 4553/13868 [56:39<1:57:59,  1.32it/s]Training epoch 1:  33% 4554/13868 [56:40<1:57:31,  1.32it/s]Training epoch 1:  33% 4555/13868 [56:40<1:56:23,  1.33it/s]Training epoch 1:  33% 4556/13868 [56:41<1:58:06,  1.31it/s]Training epoch 1:  33% 4557/13868 [56:42<1:57:46,  1.32it/s]Training epoch 1:  33% 4558/13868 [56:43<1:58:32,  1.31it/s]Training epoch 1:  33% 4559/13868 [56:44<1:58:24,  1.31it/s]Training epoch 1:  33% 4560/13868 [56:44<1:57:36,  1.32it/s]Training epoch 1:  33% 4561/13868 [56:45<1:58:00,  1.31it/s]Training epoch 1:  33% 4562/13868 [56:46<1:56:39,  1.33it/s]Training epoch 1:  33% 4563/13868 [56:47<1:57:17,  1.32it/s]Training epoch 1:  33% 4564/13868 [56:47<1:58:39,  1.31it/s]Training epoch 1:  33% 4565/13868 [56:48<1:57:59,  1.31it/s]Training epoch 1:  33% 4566/13868 [56:49<1:58:54,  1.30it/s]Training epoch 1:  33% 4567/13868 [56:50<1:57:11,  1.32it/s]Training epoch 1:  33% 4568/13868 [56:50<1:56:42,  1.33it/s]Training epoch 1:  33% 4569/13868 [56:51<1:56:00,  1.34it/s]Training epoch 1:  33% 4570/13868 [56:52<1:54:19,  1.36it/s]Training epoch 1:  33% 4571/13868 [56:52<1:52:41,  1.37it/s]Training epoch 1:  33% 4572/13868 [56:53<1:54:52,  1.35it/s]Training epoch 1:  33% 4573/13868 [56:54<1:54:13,  1.36it/s]Training epoch 1:  33% 4574/13868 [56:55<1:56:05,  1.33it/s]Training epoch 1:  33% 4575/13868 [56:56<1:56:26,  1.33it/s]Training epoch 1:  33% 4576/13868 [56:56<1:57:13,  1.32it/s]Training epoch 1:  33% 4577/13868 [56:57<1:56:19,  1.33it/s]Training epoch 1:  33% 4578/13868 [56:58<1:56:36,  1.33it/s]Training epoch 1:  33% 4579/13868 [56:59<1:57:09,  1.32it/s]Training epoch 1:  33% 4580/13868 [56:59<1:56:21,  1.33it/s]Training epoch 1:  33% 4581/13868 [57:00<1:55:50,  1.34it/s]Training epoch 1:  33% 4582/13868 [57:01<1:55:58,  1.33it/s]Training epoch 1:  33% 4583/13868 [57:02<1:56:26,  1.33it/s]Training epoch 1:  33% 4584/13868 [57:02<1:55:18,  1.34it/s]Training epoch 1:  33% 4585/13868 [57:03<1:54:50,  1.35it/s]Training epoch 1:  33% 4586/13868 [57:04<1:55:09,  1.34it/s]Training epoch 1:  33% 4587/13868 [57:04<1:53:46,  1.36it/s]Training epoch 1:  33% 4588/13868 [57:05<1:55:51,  1.33it/s]Training epoch 1:  33% 4589/13868 [57:06<1:56:49,  1.32it/s]Training epoch 1:  33% 4590/13868 [57:07<1:57:56,  1.31it/s]Training epoch 1:  33% 4591/13868 [57:08<1:58:51,  1.30it/s]Training epoch 1:  33% 4592/13868 [57:08<1:57:07,  1.32it/s]Training epoch 1:  33% 4593/13868 [57:09<1:57:23,  1.32it/s]Training epoch 1:  33% 4594/13868 [57:10<1:59:28,  1.29it/s]Training epoch 1:  33% 4595/13868 [57:11<1:57:59,  1.31it/s]Training epoch 1:  33% 4596/13868 [57:11<1:57:22,  1.32it/s]Training epoch 1:  33% 4597/13868 [57:12<1:56:15,  1.33it/s]Training epoch 1:  33% 4598/13868 [57:13<1:57:17,  1.32it/s]Training epoch 1:  33% 4599/13868 [57:14<1:56:08,  1.33it/s]Training epoch 1:  33% 4600/13868 [57:15<2:07:10,  1.21it/s]Training epoch 1:  33% 4601/13868 [57:15<2:02:54,  1.26it/s]Training epoch 1:  33% 4602/13868 [57:16<1:59:52,  1.29it/s]Training epoch 1:  33% 4603/13868 [57:17<1:58:22,  1.30it/s]Training epoch 1:  33% 4604/13868 [57:18<1:56:37,  1.32it/s]Training epoch 1:  33% 4605/13868 [57:18<1:55:10,  1.34it/s]Training epoch 1:  33% 4606/13868 [57:19<1:55:51,  1.33it/s]Training epoch 1:  33% 4607/13868 [57:20<1:54:58,  1.34it/s]Training epoch 1:  33% 4608/13868 [57:21<1:55:57,  1.33it/s]Training epoch 1:  33% 4609/13868 [57:21<1:56:24,  1.33it/s]Training epoch 1:  33% 4610/13868 [57:22<1:57:13,  1.32it/s]Training epoch 1:  33% 4611/13868 [57:23<1:56:34,  1.32it/s]Training epoch 1:  33% 4612/13868 [57:24<1:58:08,  1.31it/s]Training epoch 1:  33% 4613/13868 [57:24<1:58:07,  1.31it/s]Training epoch 1:  33% 4614/13868 [57:25<1:56:25,  1.32it/s]Training epoch 1:  33% 4615/13868 [57:26<1:56:13,  1.33it/s]Training epoch 1:  33% 4616/13868 [57:27<1:55:55,  1.33it/s]Training epoch 1:  33% 4617/13868 [57:27<1:56:52,  1.32it/s]Training epoch 1:  33% 4618/13868 [57:28<1:58:41,  1.30it/s]Training epoch 1:  33% 4619/13868 [57:29<1:57:05,  1.32it/s]Training epoch 1:  33% 4620/13868 [57:30<1:56:00,  1.33it/s]Training epoch 1:  33% 4621/13868 [57:30<1:54:15,  1.35it/s]Training epoch 1:  33% 4622/13868 [57:31<1:54:27,  1.35it/s]Training epoch 1:  33% 4623/13868 [57:32<1:53:27,  1.36it/s]Training epoch 1:  33% 4624/13868 [57:33<1:54:22,  1.35it/s]Training epoch 1:  33% 4625/13868 [57:33<1:53:25,  1.36it/s]Training epoch 1:  33% 4626/13868 [57:34<1:53:21,  1.36it/s]Training epoch 1:  33% 4627/13868 [57:35<1:54:38,  1.34it/s]Training epoch 1:  33% 4628/13868 [57:36<1:54:46,  1.34it/s]Training epoch 1:  33% 4629/13868 [57:36<1:54:59,  1.34it/s]Training epoch 1:  33% 4630/13868 [57:37<1:56:18,  1.32it/s]Training epoch 1:  33% 4631/13868 [57:38<1:55:15,  1.34it/s]Training epoch 1:  33% 4632/13868 [57:39<1:57:11,  1.31it/s]Training epoch 1:  33% 4633/13868 [57:39<1:55:38,  1.33it/s]Training epoch 1:  33% 4634/13868 [57:40<1:54:49,  1.34it/s]Training epoch 1:  33% 4635/13868 [57:41<1:54:42,  1.34it/s]Training epoch 1:  33% 4636/13868 [57:42<1:55:52,  1.33it/s]Training epoch 1:  33% 4637/13868 [57:42<1:55:34,  1.33it/s]Training epoch 1:  33% 4638/13868 [57:43<1:57:11,  1.31it/s]Training epoch 1:  33% 4639/13868 [57:44<1:55:53,  1.33it/s]Training epoch 1:  33% 4640/13868 [57:45<1:56:41,  1.32it/s]Training epoch 1:  33% 4641/13868 [57:45<1:56:30,  1.32it/s]Training epoch 1:  33% 4642/13868 [57:46<1:56:32,  1.32it/s]Training epoch 1:  33% 4643/13868 [57:47<1:56:14,  1.32it/s]Training epoch 1:  33% 4644/13868 [57:48<1:56:59,  1.31it/s]Training epoch 1:  33% 4645/13868 [57:48<1:56:58,  1.31it/s]Training epoch 1:  34% 4646/13868 [57:49<1:56:55,  1.31it/s]Training epoch 1:  34% 4647/13868 [57:50<1:56:35,  1.32it/s]Training epoch 1:  34% 4648/13868 [57:51<1:56:43,  1.32it/s]Training epoch 1:  34% 4649/13868 [57:51<1:55:23,  1.33it/s]Training epoch 1:  34% 4650/13868 [57:52<1:55:01,  1.34it/s]Training epoch 1:  34% 4651/13868 [57:53<1:54:22,  1.34it/s]Training epoch 1:  34% 4652/13868 [57:54<1:54:08,  1.35it/s]Training epoch 1:  34% 4653/13868 [57:54<1:54:15,  1.34it/s]Training epoch 1:  34% 4654/13868 [57:55<1:55:33,  1.33it/s]Training epoch 1:  34% 4655/13868 [57:56<1:54:50,  1.34it/s]Training epoch 1:  34% 4656/13868 [57:57<1:54:29,  1.34it/s]Training epoch 1:  34% 4657/13868 [57:57<1:56:07,  1.32it/s]Training epoch 1:  34% 4658/13868 [57:58<1:54:53,  1.34it/s]Training epoch 1:  34% 4659/13868 [57:59<1:55:22,  1.33it/s]Training epoch 1:  34% 4660/13868 [58:00<1:54:53,  1.34it/s]Training epoch 1:  34% 4661/13868 [58:00<1:53:46,  1.35it/s]Training epoch 1:  34% 4662/13868 [58:01<1:55:39,  1.33it/s]Training epoch 1:  34% 4663/13868 [58:02<1:55:20,  1.33it/s]Training epoch 1:  34% 4664/13868 [58:03<1:55:06,  1.33it/s]Training epoch 1:  34% 4665/13868 [58:03<1:54:26,  1.34it/s]Training epoch 1:  34% 4666/13868 [58:04<1:53:56,  1.35it/s]Training epoch 1:  34% 4667/13868 [58:05<1:53:50,  1.35it/s]Training epoch 1:  34% 4668/13868 [58:06<1:55:56,  1.32it/s]Training epoch 1:  34% 4669/13868 [58:06<1:55:41,  1.33it/s]Training epoch 1:  34% 4670/13868 [58:07<1:56:24,  1.32it/s]Training epoch 1:  34% 4671/13868 [58:08<1:55:52,  1.32it/s]Training epoch 1:  34% 4672/13868 [58:09<1:55:22,  1.33it/s]Training epoch 1:  34% 4673/13868 [58:09<1:53:47,  1.35it/s]Training epoch 1:  34% 4674/13868 [58:10<1:52:35,  1.36it/s]Training epoch 1:  34% 4675/13868 [58:11<1:53:42,  1.35it/s]Training epoch 1:  34% 4676/13868 [58:12<1:55:40,  1.32it/s]Training epoch 1:  34% 4677/13868 [58:12<1:55:49,  1.32it/s]Training epoch 1:  34% 4678/13868 [58:13<1:56:32,  1.31it/s]Training epoch 1:  34% 4679/13868 [58:14<1:55:59,  1.32it/s]Training epoch 1:  34% 4680/13868 [58:15<1:57:09,  1.31it/s]Training epoch 1:  34% 4681/13868 [58:15<1:55:02,  1.33it/s]Training epoch 1:  34% 4682/13868 [58:16<1:55:48,  1.32it/s]Training epoch 1:  34% 4683/13868 [58:17<1:56:48,  1.31it/s]Training epoch 1:  34% 4684/13868 [58:18<1:56:07,  1.32it/s]Training epoch 1:  34% 4685/13868 [58:18<1:55:01,  1.33it/s]Training epoch 1:  34% 4686/13868 [58:19<1:56:03,  1.32it/s]Training epoch 1:  34% 4687/13868 [58:20<1:55:49,  1.32it/s]Training epoch 1:  34% 4688/13868 [58:21<1:55:36,  1.32it/s]Training epoch 1:  34% 4689/13868 [58:21<1:55:55,  1.32it/s]Training epoch 1:  34% 4690/13868 [58:22<1:57:18,  1.30it/s]Training epoch 1:  34% 4691/13868 [58:23<1:58:52,  1.29it/s]Training epoch 1:  34% 4692/13868 [58:24<1:59:47,  1.28it/s]Training epoch 1:  34% 4693/13868 [58:25<1:58:59,  1.29it/s]Training epoch 1:  34% 4694/13868 [58:25<1:59:02,  1.28it/s]Training epoch 1:  34% 4695/13868 [58:26<1:57:08,  1.31it/s]Training epoch 1:  34% 4696/13868 [58:27<1:56:34,  1.31it/s]Training epoch 1:  34% 4697/13868 [58:28<1:55:28,  1.32it/s]Training epoch 1:  34% 4698/13868 [58:28<1:55:45,  1.32it/s]Training epoch 1:  34% 4699/13868 [58:29<1:58:04,  1.29it/s]Training epoch 1:  34% 4700/13868 [58:30<2:04:03,  1.23it/s]Training epoch 1:  34% 4701/13868 [58:31<2:00:54,  1.26it/s]Training epoch 1:  34% 4702/13868 [58:32<1:59:12,  1.28it/s]Training epoch 1:  34% 4703/13868 [58:32<1:58:16,  1.29it/s]Training epoch 1:  34% 4704/13868 [58:33<1:56:27,  1.31it/s]Training epoch 1:  34% 4705/13868 [58:34<1:56:07,  1.32it/s]Training epoch 1:  34% 4706/13868 [58:35<1:56:26,  1.31it/s]Training epoch 1:  34% 4707/13868 [58:35<1:54:53,  1.33it/s]Training epoch 1:  34% 4708/13868 [58:36<1:55:32,  1.32it/s]Training epoch 1:  34% 4709/13868 [58:37<1:55:18,  1.32it/s]Training epoch 1:  34% 4710/13868 [58:38<1:55:20,  1.32it/s]Training epoch 1:  34% 4711/13868 [58:38<1:54:52,  1.33it/s]Training epoch 1:  34% 4712/13868 [58:39<1:54:46,  1.33it/s]Training epoch 1:  34% 4713/13868 [58:40<1:55:23,  1.32it/s]Training epoch 1:  34% 4714/13868 [58:41<1:55:21,  1.32it/s]Training epoch 1:  34% 4715/13868 [58:41<1:55:34,  1.32it/s]Training epoch 1:  34% 4716/13868 [58:42<1:54:49,  1.33it/s]Training epoch 1:  34% 4717/13868 [58:43<1:56:23,  1.31it/s]Training epoch 1:  34% 4718/13868 [58:44<1:56:27,  1.31it/s]Training epoch 1:  34% 4719/13868 [58:44<1:57:18,  1.30it/s]Training epoch 1:  34% 4720/13868 [58:45<1:55:24,  1.32it/s]Training epoch 1:  34% 4721/13868 [58:46<1:55:46,  1.32it/s]Training epoch 1:  34% 4722/13868 [58:47<1:54:46,  1.33it/s]Training epoch 1:  34% 4723/13868 [58:47<1:55:20,  1.32it/s]Training epoch 1:  34% 4724/13868 [58:48<1:55:02,  1.32it/s]Training epoch 1:  34% 4725/13868 [58:49<1:54:37,  1.33it/s]Training epoch 1:  34% 4726/13868 [58:50<1:53:53,  1.34it/s]Training epoch 1:  34% 4727/13868 [58:50<1:53:42,  1.34it/s]Training epoch 1:  34% 4728/13868 [58:51<1:56:10,  1.31it/s]Training epoch 1:  34% 4729/13868 [58:52<1:56:13,  1.31it/s]Training epoch 1:  34% 4730/13868 [58:53<1:53:32,  1.34it/s]Training epoch 1:  34% 4731/13868 [58:53<1:54:11,  1.33it/s]Training epoch 1:  34% 4732/13868 [58:54<1:55:38,  1.32it/s]Training epoch 1:  34% 4733/13868 [58:55<1:55:41,  1.32it/s]Training epoch 1:  34% 4734/13868 [58:56<1:55:52,  1.31it/s]Training epoch 1:  34% 4735/13868 [58:57<1:55:51,  1.31it/s]Training epoch 1:  34% 4736/13868 [58:57<1:55:02,  1.32it/s]Training epoch 1:  34% 4737/13868 [58:58<1:54:22,  1.33it/s]Training epoch 1:  34% 4738/13868 [58:59<1:55:09,  1.32it/s]Training epoch 1:  34% 4739/13868 [59:00<1:56:10,  1.31it/s]Training epoch 1:  34% 4740/13868 [59:00<1:54:43,  1.33it/s]Training epoch 1:  34% 4741/13868 [59:01<1:54:58,  1.32it/s]Training epoch 1:  34% 4742/13868 [59:02<1:55:15,  1.32it/s]Training epoch 1:  34% 4743/13868 [59:03<1:54:22,  1.33it/s]Training epoch 1:  34% 4744/13868 [59:03<1:55:08,  1.32it/s]Training epoch 1:  34% 4745/13868 [59:04<1:56:03,  1.31it/s]Training epoch 1:  34% 4746/13868 [59:05<1:55:22,  1.32it/s]Training epoch 1:  34% 4747/13868 [59:06<1:55:33,  1.32it/s]Training epoch 1:  34% 4748/13868 [59:06<1:56:17,  1.31it/s]Training epoch 1:  34% 4749/13868 [59:07<1:54:47,  1.32it/s]Training epoch 1:  34% 4750/13868 [59:08<1:55:40,  1.31it/s]Training epoch 1:  34% 4751/13868 [59:09<1:54:33,  1.33it/s]Training epoch 1:  34% 4752/13868 [59:09<1:54:50,  1.32it/s]Training epoch 1:  34% 4753/13868 [59:10<1:55:09,  1.32it/s]Training epoch 1:  34% 4754/13868 [59:11<1:55:32,  1.31it/s]Training epoch 1:  34% 4755/13868 [59:12<1:54:44,  1.32it/s]Training epoch 1:  34% 4756/13868 [59:12<1:54:32,  1.33it/s]Training epoch 1:  34% 4757/13868 [59:13<1:53:52,  1.33it/s]Training epoch 1:  34% 4758/13868 [59:14<1:54:16,  1.33it/s]Training epoch 1:  34% 4759/13868 [59:15<1:55:01,  1.32it/s]Training epoch 1:  34% 4760/13868 [59:15<1:54:50,  1.32it/s]Training epoch 1:  34% 4761/13868 [59:16<1:55:04,  1.32it/s]Training epoch 1:  34% 4762/13868 [59:17<1:54:40,  1.32it/s]Training epoch 1:  34% 4763/13868 [59:18<1:56:10,  1.31it/s]Training epoch 1:  34% 4764/13868 [59:19<1:54:38,  1.32it/s]Training epoch 1:  34% 4765/13868 [59:19<1:55:20,  1.32it/s]Training epoch 1:  34% 4766/13868 [59:20<1:55:47,  1.31it/s]Training epoch 1:  34% 4767/13868 [59:21<1:55:16,  1.32it/s]Training epoch 1:  34% 4768/13868 [59:22<1:55:01,  1.32it/s]Training epoch 1:  34% 4769/13868 [59:22<1:54:46,  1.32it/s]Training epoch 1:  34% 4770/13868 [59:23<1:54:14,  1.33it/s]Training epoch 1:  34% 4771/13868 [59:24<1:52:59,  1.34it/s]Training epoch 1:  34% 4772/13868 [59:25<1:53:05,  1.34it/s]Training epoch 1:  34% 4773/13868 [59:25<1:52:26,  1.35it/s]Training epoch 1:  34% 4774/13868 [59:26<1:53:04,  1.34it/s]Training epoch 1:  34% 4775/13868 [59:27<1:52:47,  1.34it/s]Training epoch 1:  34% 4776/13868 [59:28<1:54:19,  1.33it/s]Training epoch 1:  34% 4777/13868 [59:28<1:53:00,  1.34it/s]Training epoch 1:  34% 4778/13868 [59:29<1:50:59,  1.37it/s]Training epoch 1:  34% 4779/13868 [59:30<1:51:36,  1.36it/s]Training epoch 1:  34% 4780/13868 [59:30<1:52:21,  1.35it/s]Training epoch 1:  34% 4781/13868 [59:31<1:53:00,  1.34it/s]Training epoch 1:  34% 4782/13868 [59:32<1:53:18,  1.34it/s]Training epoch 1:  34% 4783/13868 [59:33<1:54:27,  1.32it/s]Training epoch 1:  34% 4784/13868 [59:33<1:54:19,  1.32it/s]Training epoch 1:  35% 4785/13868 [59:34<1:53:16,  1.34it/s]Training epoch 1:  35% 4786/13868 [59:35<1:54:44,  1.32it/s]Training epoch 1:  35% 4787/13868 [59:36<1:53:19,  1.34it/s]Training epoch 1:  35% 4788/13868 [59:36<1:51:12,  1.36it/s]Training epoch 1:  35% 4789/13868 [59:37<1:51:01,  1.36it/s]Training epoch 1:  35% 4790/13868 [59:38<1:54:00,  1.33it/s]Training epoch 1:  35% 4791/13868 [59:39<1:52:38,  1.34it/s]Training epoch 1:  35% 4792/13868 [59:39<1:53:33,  1.33it/s]Training epoch 1:  35% 4793/13868 [59:40<1:53:47,  1.33it/s]Training epoch 1:  35% 4794/13868 [59:41<1:53:58,  1.33it/s]Training epoch 1:  35% 4795/13868 [59:42<1:52:47,  1.34it/s]Training epoch 1:  35% 4796/13868 [59:42<1:53:02,  1.34it/s]Training epoch 1:  35% 4797/13868 [59:43<1:53:40,  1.33it/s]Training epoch 1:  35% 4798/13868 [59:44<1:54:40,  1.32it/s]Training epoch 1:  35% 4799/13868 [59:45<1:54:29,  1.32it/s]Training epoch 1:  35% 4800/13868 [59:46<2:01:25,  1.24it/s]Training epoch 1:  35% 4801/13868 [59:46<1:58:58,  1.27it/s]Training epoch 1:  35% 4802/13868 [59:47<1:58:53,  1.27it/s]Training epoch 1:  35% 4803/13868 [59:48<1:56:40,  1.29it/s]Training epoch 1:  35% 4804/13868 [59:49<1:55:58,  1.30it/s]Training epoch 1:  35% 4805/13868 [59:49<1:55:39,  1.31it/s]Training epoch 1:  35% 4806/13868 [59:50<1:55:21,  1.31it/s]Training epoch 1:  35% 4807/13868 [59:51<1:55:24,  1.31it/s]Training epoch 1:  35% 4808/13868 [59:52<1:55:12,  1.31it/s]Training epoch 1:  35% 4809/13868 [59:52<1:53:05,  1.34it/s]Training epoch 1:  35% 4810/13868 [59:53<1:54:19,  1.32it/s]Training epoch 1:  35% 4811/13868 [59:54<1:54:56,  1.31it/s]Training epoch 1:  35% 4812/13868 [59:55<1:55:41,  1.30it/s]Training epoch 1:  35% 4813/13868 [59:56<1:56:39,  1.29it/s]Training epoch 1:  35% 4814/13868 [59:56<1:55:11,  1.31it/s]Training epoch 1:  35% 4815/13868 [59:57<1:53:23,  1.33it/s]Training epoch 1:  35% 4816/13868 [59:58<1:53:37,  1.33it/s]Training epoch 1:  35% 4817/13868 [59:59<1:53:49,  1.33it/s]Training epoch 1:  35% 4818/13868 [59:59<1:54:39,  1.32it/s]Training epoch 1:  35% 4819/13868 [1:00:00<1:55:01,  1.31it/s]Training epoch 1:  35% 4820/13868 [1:00:01<1:54:00,  1.32it/s]Training epoch 1:  35% 4821/13868 [1:00:02<1:53:23,  1.33it/s]Training epoch 1:  35% 4822/13868 [1:00:02<1:52:49,  1.34it/s]Training epoch 1:  35% 4823/13868 [1:00:03<1:52:31,  1.34it/s]Training epoch 1:  35% 4824/13868 [1:00:04<1:52:28,  1.34it/s]Training epoch 1:  35% 4825/13868 [1:00:05<1:51:35,  1.35it/s]Training epoch 1:  35% 4826/13868 [1:00:05<1:52:12,  1.34it/s]Training epoch 1:  35% 4827/13868 [1:00:06<1:51:16,  1.35it/s]Training epoch 1:  35% 4828/13868 [1:00:07<1:53:10,  1.33it/s]Training epoch 1:  35% 4829/13868 [1:00:08<1:54:08,  1.32it/s]Training epoch 1:  35% 4830/13868 [1:00:08<1:53:59,  1.32it/s]Training epoch 1:  35% 4831/13868 [1:00:09<1:52:44,  1.34it/s]Training epoch 1:  35% 4832/13868 [1:00:10<1:51:29,  1.35it/s]Training epoch 1:  35% 4833/13868 [1:00:10<1:51:23,  1.35it/s]Training epoch 1:  35% 4834/13868 [1:00:11<1:51:17,  1.35it/s]Training epoch 1:  35% 4835/13868 [1:00:12<1:52:42,  1.34it/s]Training epoch 1:  35% 4836/13868 [1:00:13<1:52:32,  1.34it/s]Training epoch 1:  35% 4837/13868 [1:00:13<1:50:25,  1.36it/s]Training epoch 1:  35% 4838/13868 [1:00:14<1:52:20,  1.34it/s]Training epoch 1:  35% 4839/13868 [1:00:15<1:52:45,  1.33it/s]Training epoch 1:  35% 4840/13868 [1:00:16<1:54:41,  1.31it/s]Training epoch 1:  35% 4841/13868 [1:00:17<1:55:03,  1.31it/s]Training epoch 1:  35% 4842/13868 [1:00:17<1:53:44,  1.32it/s]Training epoch 1:  35% 4843/13868 [1:00:18<1:54:04,  1.32it/s]Training epoch 1:  35% 4844/13868 [1:00:19<1:53:18,  1.33it/s]Training epoch 1:  35% 4845/13868 [1:00:20<1:53:54,  1.32it/s]Training epoch 1:  35% 4846/13868 [1:00:20<1:54:00,  1.32it/s]Training epoch 1:  35% 4847/13868 [1:00:21<1:54:23,  1.31it/s]Training epoch 1:  35% 4848/13868 [1:00:22<1:54:37,  1.31it/s]Training epoch 1:  35% 4849/13868 [1:00:23<1:54:39,  1.31it/s]Training epoch 1:  35% 4850/13868 [1:00:23<1:56:45,  1.29it/s]Training epoch 1:  35% 4851/13868 [1:00:24<1:54:25,  1.31it/s]Training epoch 1:  35% 4852/13868 [1:00:25<1:54:02,  1.32it/s]Training epoch 1:  35% 4853/13868 [1:00:26<1:55:00,  1.31it/s]Training epoch 1:  35% 4854/13868 [1:00:26<1:53:29,  1.32it/s]Training epoch 1:  35% 4855/13868 [1:00:27<1:56:19,  1.29it/s]Training epoch 1:  35% 4856/13868 [1:00:28<1:55:54,  1.30it/s]Training epoch 1:  35% 4857/13868 [1:00:29<1:54:10,  1.32it/s]Training epoch 1:  35% 4858/13868 [1:00:29<1:52:23,  1.34it/s]Training epoch 1:  35% 4859/13868 [1:00:30<1:52:15,  1.34it/s]Training epoch 1:  35% 4860/13868 [1:00:31<1:52:56,  1.33it/s]Training epoch 1:  35% 4861/13868 [1:00:32<1:51:37,  1.34it/s]Training epoch 1:  35% 4862/13868 [1:00:32<1:53:08,  1.33it/s]Training epoch 1:  35% 4863/13868 [1:00:33<1:52:51,  1.33it/s]Training epoch 1:  35% 4864/13868 [1:00:34<1:50:52,  1.35it/s]Training epoch 1:  35% 4865/13868 [1:00:35<1:51:24,  1.35it/s]Training epoch 1:  35% 4866/13868 [1:00:35<1:51:11,  1.35it/s]Training epoch 1:  35% 4867/13868 [1:00:36<1:53:42,  1.32it/s]Training epoch 1:  35% 4868/13868 [1:00:37<1:52:36,  1.33it/s]Training epoch 1:  35% 4869/13868 [1:00:38<1:51:34,  1.34it/s]Training epoch 1:  35% 4870/13868 [1:00:38<1:52:02,  1.34it/s]Training epoch 1:  35% 4871/13868 [1:00:39<1:52:41,  1.33it/s]Training epoch 1:  35% 4872/13868 [1:00:40<1:52:30,  1.33it/s]Training epoch 1:  35% 4873/13868 [1:00:41<1:52:29,  1.33it/s]Training epoch 1:  35% 4874/13868 [1:00:41<1:53:41,  1.32it/s]Training epoch 1:  35% 4875/13868 [1:00:42<1:54:24,  1.31it/s]Training epoch 1:  35% 4876/13868 [1:00:43<1:53:23,  1.32it/s]Training epoch 1:  35% 4877/13868 [1:00:44<1:51:44,  1.34it/s]Training epoch 1:  35% 4878/13868 [1:00:44<1:52:59,  1.33it/s]Training epoch 1:  35% 4879/13868 [1:00:45<1:51:10,  1.35it/s]Training epoch 1:  35% 4880/13868 [1:00:46<1:50:59,  1.35it/s]Training epoch 1:  35% 4881/13868 [1:00:47<1:52:30,  1.33it/s]Training epoch 1:  35% 4882/13868 [1:00:47<1:53:07,  1.32it/s]Training epoch 1:  35% 4883/13868 [1:00:48<1:52:07,  1.34it/s]Training epoch 1:  35% 4884/13868 [1:00:49<1:51:00,  1.35it/s]Training epoch 1:  35% 4885/13868 [1:00:50<1:50:20,  1.36it/s]Training epoch 1:  35% 4886/13868 [1:00:50<1:49:45,  1.36it/s]Training epoch 1:  35% 4887/13868 [1:00:51<1:49:35,  1.37it/s]Training epoch 1:  35% 4888/13868 [1:00:52<1:50:46,  1.35it/s]Training epoch 1:  35% 4889/13868 [1:00:53<1:50:58,  1.35it/s]Training epoch 1:  35% 4890/13868 [1:00:53<1:51:29,  1.34it/s]Training epoch 1:  35% 4891/13868 [1:00:54<1:51:31,  1.34it/s]Training epoch 1:  35% 4892/13868 [1:00:55<1:52:25,  1.33it/s]Training epoch 1:  35% 4893/13868 [1:00:56<1:51:33,  1.34it/s]Training epoch 1:  35% 4894/13868 [1:00:56<1:51:24,  1.34it/s]Training epoch 1:  35% 4895/13868 [1:00:57<1:52:09,  1.33it/s]Training epoch 1:  35% 4896/13868 [1:00:58<1:54:18,  1.31it/s]Training epoch 1:  35% 4897/13868 [1:00:59<1:53:47,  1.31it/s]Training epoch 1:  35% 4898/13868 [1:00:59<1:54:39,  1.30it/s]Training epoch 1:  35% 4899/13868 [1:01:00<1:53:17,  1.32it/s]Training epoch 1:  35% 4900/13868 [1:01:01<1:57:37,  1.27it/s]Training epoch 1:  35% 4901/13868 [1:01:02<1:55:43,  1.29it/s]Training epoch 1:  35% 4902/13868 [1:01:03<1:55:05,  1.30it/s]Training epoch 1:  35% 4903/13868 [1:01:03<1:54:49,  1.30it/s]Training epoch 1:  35% 4904/13868 [1:01:04<1:53:40,  1.31it/s]Training epoch 1:  35% 4905/13868 [1:01:05<1:53:22,  1.32it/s]Training epoch 1:  35% 4906/13868 [1:01:06<1:51:46,  1.34it/s]Training epoch 1:  35% 4907/13868 [1:01:06<1:51:32,  1.34it/s]Training epoch 1:  35% 4908/13868 [1:01:07<1:50:29,  1.35it/s]Training epoch 1:  35% 4909/13868 [1:01:08<1:50:59,  1.35it/s]Training epoch 1:  35% 4910/13868 [1:01:08<1:50:01,  1.36it/s]Training epoch 1:  35% 4911/13868 [1:01:09<1:49:46,  1.36it/s]Training epoch 1:  35% 4912/13868 [1:01:10<1:49:31,  1.36it/s]Training epoch 1:  35% 4913/13868 [1:01:11<1:50:26,  1.35it/s]Training epoch 1:  35% 4914/13868 [1:01:11<1:52:16,  1.33it/s]Training epoch 1:  35% 4915/13868 [1:01:12<1:52:03,  1.33it/s]Training epoch 1:  35% 4916/13868 [1:01:13<1:52:17,  1.33it/s]Training epoch 1:  35% 4917/13868 [1:01:14<1:51:44,  1.33it/s]Training epoch 1:  35% 4918/13868 [1:01:14<1:50:04,  1.36it/s]Training epoch 1:  35% 4919/13868 [1:01:15<1:49:19,  1.36it/s]Training epoch 1:  35% 4920/13868 [1:01:16<1:50:09,  1.35it/s]Training epoch 1:  35% 4921/13868 [1:01:17<1:51:05,  1.34it/s]Training epoch 1:  35% 4922/13868 [1:01:17<1:51:25,  1.34it/s]Training epoch 1:  35% 4923/13868 [1:01:18<1:50:15,  1.35it/s]Training epoch 1:  36% 4924/13868 [1:01:19<1:51:04,  1.34it/s]Training epoch 1:  36% 4925/13868 [1:01:20<1:51:23,  1.34it/s]Training epoch 1:  36% 4926/13868 [1:01:20<1:53:01,  1.32it/s]Training epoch 1:  36% 4927/13868 [1:01:21<1:53:36,  1.31it/s]Training epoch 1:  36% 4928/13868 [1:01:22<1:53:52,  1.31it/s]Training epoch 1:  36% 4929/13868 [1:01:23<1:52:38,  1.32it/s]Training epoch 1:  36% 4930/13868 [1:01:23<1:52:53,  1.32it/s]Training epoch 1:  36% 4931/13868 [1:01:24<1:52:07,  1.33it/s]Training epoch 1:  36% 4932/13868 [1:01:25<1:52:05,  1.33it/s]Training epoch 1:  36% 4933/13868 [1:01:26<1:50:08,  1.35it/s]Training epoch 1:  36% 4934/13868 [1:01:26<1:49:40,  1.36it/s]Training epoch 1:  36% 4935/13868 [1:01:27<1:50:41,  1.35it/s]Training epoch 1:  36% 4936/13868 [1:01:28<1:51:24,  1.34it/s]Training epoch 1:  36% 4937/13868 [1:01:29<1:50:07,  1.35it/s]Training epoch 1:  36% 4938/13868 [1:01:29<1:51:23,  1.34it/s]Training epoch 1:  36% 4939/13868 [1:01:30<1:50:51,  1.34it/s]Training epoch 1:  36% 4940/13868 [1:01:31<1:52:55,  1.32it/s]Training epoch 1:  36% 4941/13868 [1:01:32<1:53:21,  1.31it/s]Training epoch 1:  36% 4942/13868 [1:01:32<1:51:57,  1.33it/s]Training epoch 1:  36% 4943/13868 [1:01:33<1:50:13,  1.35it/s]Training epoch 1:  36% 4944/13868 [1:01:34<1:49:57,  1.35it/s]Training epoch 1:  36% 4945/13868 [1:01:35<1:49:33,  1.36it/s]Training epoch 1:  36% 4946/13868 [1:01:35<1:51:02,  1.34it/s]Training epoch 1:  36% 4947/13868 [1:01:36<1:51:06,  1.34it/s]Training epoch 1:  36% 4948/13868 [1:01:37<1:49:56,  1.35it/s]Training epoch 1:  36% 4949/13868 [1:01:38<1:49:59,  1.35it/s]Training epoch 1:  36% 4950/13868 [1:01:38<1:51:02,  1.34it/s]Training epoch 1:  36% 4951/13868 [1:01:39<1:50:40,  1.34it/s]Training epoch 1:  36% 4952/13868 [1:01:40<1:50:31,  1.34it/s]Training epoch 1:  36% 4953/13868 [1:01:41<1:53:06,  1.31it/s]Training epoch 1:  36% 4954/13868 [1:01:41<1:54:13,  1.30it/s]Training epoch 1:  36% 4955/13868 [1:01:42<1:54:40,  1.30it/s]Training epoch 1:  36% 4956/13868 [1:01:43<1:56:02,  1.28it/s]Training epoch 1:  36% 4957/13868 [1:01:44<1:54:36,  1.30it/s]Training epoch 1:  36% 4958/13868 [1:01:44<1:53:26,  1.31it/s]Training epoch 1:  36% 4959/13868 [1:01:45<1:53:39,  1.31it/s]Training epoch 1:  36% 4960/13868 [1:01:46<1:53:26,  1.31it/s]Training epoch 1:  36% 4961/13868 [1:01:47<1:52:01,  1.33it/s]Training epoch 1:  36% 4962/13868 [1:01:48<1:51:50,  1.33it/s]Training epoch 1:  36% 4963/13868 [1:01:48<1:52:45,  1.32it/s]Training epoch 1:  36% 4964/13868 [1:01:49<1:50:40,  1.34it/s]Training epoch 1:  36% 4965/13868 [1:01:50<1:50:23,  1.34it/s]Training epoch 1:  36% 4966/13868 [1:01:50<1:49:41,  1.35it/s]Training epoch 1:  36% 4967/13868 [1:01:51<1:49:50,  1.35it/s]Training epoch 1:  36% 4968/13868 [1:01:52<1:49:41,  1.35it/s]Training epoch 1:  36% 4969/13868 [1:01:53<1:50:09,  1.35it/s]Training epoch 1:  36% 4970/13868 [1:01:53<1:50:53,  1.34it/s]Training epoch 1:  36% 4971/13868 [1:01:54<1:50:04,  1.35it/s]Training epoch 1:  36% 4972/13868 [1:01:55<1:52:01,  1.32it/s]Training epoch 1:  36% 4973/13868 [1:01:56<1:51:27,  1.33it/s]Training epoch 1:  36% 4974/13868 [1:01:56<1:52:26,  1.32it/s]Training epoch 1:  36% 4975/13868 [1:01:57<1:52:48,  1.31it/s]Training epoch 1:  36% 4976/13868 [1:01:58<1:52:06,  1.32it/s]Training epoch 1:  36% 4977/13868 [1:01:59<1:51:14,  1.33it/s]Training epoch 1:  36% 4978/13868 [1:01:59<1:49:45,  1.35it/s]Training epoch 1:  36% 4979/13868 [1:02:00<1:50:14,  1.34it/s]Training epoch 1:  36% 4980/13868 [1:02:01<1:51:14,  1.33it/s]Training epoch 1:  36% 4981/13868 [1:02:02<1:52:17,  1.32it/s]Training epoch 1:  36% 4982/13868 [1:02:02<1:51:12,  1.33it/s]Training epoch 1:  36% 4983/13868 [1:02:03<1:50:04,  1.35it/s]Training epoch 1:  36% 4984/13868 [1:02:04<1:50:14,  1.34it/s]Training epoch 1:  36% 4985/13868 [1:02:05<1:51:23,  1.33it/s]Training epoch 1:  36% 4986/13868 [1:02:05<1:51:52,  1.32it/s]Training epoch 1:  36% 4987/13868 [1:02:06<1:53:00,  1.31it/s]Training epoch 1:  36% 4988/13868 [1:02:07<1:54:45,  1.29it/s]Training epoch 1:  36% 4989/13868 [1:02:08<1:53:57,  1.30it/s]Training epoch 1:  36% 4990/13868 [1:02:09<1:53:59,  1.30it/s]Training epoch 1:  36% 4991/13868 [1:02:09<1:52:09,  1.32it/s]Training epoch 1:  36% 4992/13868 [1:02:10<1:52:00,  1.32it/s]Training epoch 1:  36% 4993/13868 [1:02:11<1:51:12,  1.33it/s]Training epoch 1:  36% 4994/13868 [1:02:12<1:50:34,  1.34it/s]Training epoch 1:  36% 4995/13868 [1:02:12<1:51:22,  1.33it/s]Training epoch 1:  36% 4996/13868 [1:02:13<1:51:51,  1.32it/s]Training epoch 1:  36% 4997/13868 [1:02:14<1:51:07,  1.33it/s]Training epoch 1:  36% 4998/13868 [1:02:15<1:50:59,  1.33it/s]Training epoch 1:  36% 4999/13868 [1:02:15<1:51:42,  1.32it/s]Training epoch 1:  36% 5000/13868 [1:02:16<1:59:44,  1.23it/s]Training epoch 1:  36% 5001/13868 [1:02:17<1:58:12,  1.25it/s]Training epoch 1:  36% 5002/13868 [1:02:18<1:57:39,  1.26it/s]Training epoch 1:  36% 5003/13868 [1:02:19<1:56:15,  1.27it/s]Training epoch 1:  36% 5004/13868 [1:02:19<1:55:26,  1.28it/s]Training epoch 1:  36% 5005/13868 [1:02:20<1:54:54,  1.29it/s]Training epoch 1:  36% 5006/13868 [1:02:21<1:54:11,  1.29it/s]Training epoch 1:  36% 5007/13868 [1:02:22<1:52:33,  1.31it/s]Training epoch 1:  36% 5008/13868 [1:02:22<1:51:15,  1.33it/s]Training epoch 1:  36% 5009/13868 [1:02:23<1:53:18,  1.30it/s]Training epoch 1:  36% 5010/13868 [1:02:24<1:53:16,  1.30it/s]Training epoch 1:  36% 5011/13868 [1:02:25<1:52:35,  1.31it/s]Training epoch 1:  36% 5012/13868 [1:02:25<1:51:50,  1.32it/s]Training epoch 1:  36% 5013/13868 [1:02:26<1:51:09,  1.33it/s]Training epoch 1:  36% 5014/13868 [1:02:27<1:51:49,  1.32it/s]Training epoch 1:  36% 5015/13868 [1:02:28<1:50:34,  1.33it/s]Training epoch 1:  36% 5016/13868 [1:02:28<1:52:15,  1.31it/s]Training epoch 1:  36% 5017/13868 [1:02:29<1:52:49,  1.31it/s]Training epoch 1:  36% 5018/13868 [1:02:30<1:54:22,  1.29it/s]Training epoch 1:  36% 5019/13868 [1:02:31<1:53:34,  1.30it/s]Training epoch 1:  36% 5020/13868 [1:02:32<1:54:42,  1.29it/s]Training epoch 1:  36% 5021/13868 [1:02:32<1:53:45,  1.30it/s]Training epoch 1:  36% 5022/13868 [1:02:33<1:53:11,  1.30it/s]Training epoch 1:  36% 5023/13868 [1:02:34<1:53:33,  1.30it/s]Training epoch 1:  36% 5024/13868 [1:02:35<1:53:13,  1.30it/s]Training epoch 1:  36% 5025/13868 [1:02:35<1:52:30,  1.31it/s]Training epoch 1:  36% 5026/13868 [1:02:36<1:53:57,  1.29it/s]Training epoch 1:  36% 5027/13868 [1:02:37<1:53:08,  1.30it/s]Training epoch 1:  36% 5028/13868 [1:02:38<1:54:22,  1.29it/s]Training epoch 1:  36% 5029/13868 [1:02:39<1:54:03,  1.29it/s]Training epoch 1:  36% 5030/13868 [1:02:39<1:53:39,  1.30it/s]Training epoch 1:  36% 5031/13868 [1:02:40<1:51:40,  1.32it/s]Training epoch 1:  36% 5032/13868 [1:02:41<1:52:16,  1.31it/s]Training epoch 1:  36% 5033/13868 [1:02:42<1:51:43,  1.32it/s]Training epoch 1:  36% 5034/13868 [1:02:42<1:53:20,  1.30it/s]Training epoch 1:  36% 5035/13868 [1:02:43<1:54:07,  1.29it/s]Training epoch 1:  36% 5036/13868 [1:02:44<1:53:45,  1.29it/s]Training epoch 1:  36% 5037/13868 [1:02:45<1:53:35,  1.30it/s]Training epoch 1:  36% 5038/13868 [1:02:45<1:53:47,  1.29it/s]Training epoch 1:  36% 5039/13868 [1:02:46<1:52:52,  1.30it/s]Training epoch 1:  36% 5040/13868 [1:02:47<1:52:12,  1.31it/s]Training epoch 1:  36% 5041/13868 [1:02:48<1:52:00,  1.31it/s]Training epoch 1:  36% 5042/13868 [1:02:48<1:51:12,  1.32it/s]Training epoch 1:  36% 5043/13868 [1:02:49<1:52:08,  1.31it/s]Training epoch 1:  36% 5044/13868 [1:02:50<1:52:26,  1.31it/s]Training epoch 1:  36% 5045/13868 [1:02:51<1:51:35,  1.32it/s]Training epoch 1:  36% 5046/13868 [1:02:52<1:51:52,  1.31it/s]Training epoch 1:  36% 5047/13868 [1:02:52<1:51:21,  1.32it/s]Training epoch 1:  36% 5048/13868 [1:02:53<1:51:15,  1.32it/s]Training epoch 1:  36% 5049/13868 [1:02:54<1:52:07,  1.31it/s]Training epoch 1:  36% 5050/13868 [1:02:55<1:52:48,  1.30it/s]Training epoch 1:  36% 5051/13868 [1:02:55<1:51:15,  1.32it/s]Training epoch 1:  36% 5052/13868 [1:02:56<1:49:46,  1.34it/s]Training epoch 1:  36% 5053/13868 [1:02:57<1:50:48,  1.33it/s]Training epoch 1:  36% 5054/13868 [1:02:58<1:50:12,  1.33it/s]Training epoch 1:  36% 5055/13868 [1:02:58<1:49:18,  1.34it/s]Training epoch 1:  36% 5056/13868 [1:02:59<1:50:35,  1.33it/s]Training epoch 1:  36% 5057/13868 [1:03:00<1:50:32,  1.33it/s]Training epoch 1:  36% 5058/13868 [1:03:01<1:50:04,  1.33it/s]Training epoch 1:  36% 5059/13868 [1:03:01<1:50:22,  1.33it/s]Training epoch 1:  36% 5060/13868 [1:03:02<1:50:07,  1.33it/s]Training epoch 1:  36% 5061/13868 [1:03:03<1:49:42,  1.34it/s]Training epoch 1:  37% 5062/13868 [1:03:04<1:50:15,  1.33it/s]Training epoch 1:  37% 5063/13868 [1:03:04<1:50:41,  1.33it/s]Training epoch 1:  37% 5064/13868 [1:03:05<1:49:36,  1.34it/s]Training epoch 1:  37% 5065/13868 [1:03:06<1:51:07,  1.32it/s]Training epoch 1:  37% 5066/13868 [1:03:07<1:51:50,  1.31it/s]Training epoch 1:  37% 5067/13868 [1:03:07<1:50:41,  1.33it/s]Training epoch 1:  37% 5068/13868 [1:03:08<1:51:53,  1.31it/s]Training epoch 1:  37% 5069/13868 [1:03:09<1:50:24,  1.33it/s]Training epoch 1:  37% 5070/13868 [1:03:10<1:50:39,  1.33it/s]Training epoch 1:  37% 5071/13868 [1:03:10<1:51:11,  1.32it/s]Training epoch 1:  37% 5072/13868 [1:03:11<1:50:29,  1.33it/s]Training epoch 1:  37% 5073/13868 [1:03:12<1:50:14,  1.33it/s]Training epoch 1:  37% 5074/13868 [1:03:13<1:49:51,  1.33it/s]Training epoch 1:  37% 5075/13868 [1:03:13<1:50:08,  1.33it/s]Training epoch 1:  37% 5076/13868 [1:03:14<1:49:37,  1.34it/s]Training epoch 1:  37% 5077/13868 [1:03:15<1:49:30,  1.34it/s]Training epoch 1:  37% 5078/13868 [1:03:16<1:49:40,  1.34it/s]Training epoch 1:  37% 5079/13868 [1:03:16<1:50:15,  1.33it/s]Training epoch 1:  37% 5080/13868 [1:03:17<1:50:54,  1.32it/s]Training epoch 1:  37% 5081/13868 [1:03:18<1:51:19,  1.32it/s]Training epoch 1:  37% 5082/13868 [1:03:19<1:51:06,  1.32it/s]Training epoch 1:  37% 5083/13868 [1:03:19<1:52:55,  1.30it/s]Training epoch 1:  37% 5084/13868 [1:03:20<1:53:47,  1.29it/s]Training epoch 1:  37% 5085/13868 [1:03:21<1:52:29,  1.30it/s]Training epoch 1:  37% 5086/13868 [1:03:22<1:53:12,  1.29it/s]Training epoch 1:  37% 5087/13868 [1:03:23<1:53:25,  1.29it/s]Training epoch 1:  37% 5088/13868 [1:03:23<1:51:48,  1.31it/s]Training epoch 1:  37% 5089/13868 [1:03:24<1:51:47,  1.31it/s]Training epoch 1:  37% 5090/13868 [1:03:25<1:52:17,  1.30it/s]Training epoch 1:  37% 5091/13868 [1:03:26<1:50:27,  1.32it/s]Training epoch 1:  37% 5092/13868 [1:03:26<1:50:19,  1.33it/s]Training epoch 1:  37% 5093/13868 [1:03:27<1:51:00,  1.32it/s]Training epoch 1:  37% 5094/13868 [1:03:28<1:50:38,  1.32it/s]Training epoch 1:  37% 5095/13868 [1:03:29<1:51:04,  1.32it/s]Training epoch 1:  37% 5096/13868 [1:03:29<1:52:29,  1.30it/s]Training epoch 1:  37% 5097/13868 [1:03:30<1:53:28,  1.29it/s]Training epoch 1:  37% 5098/13868 [1:03:31<1:54:29,  1.28it/s]Training epoch 1:  37% 5099/13868 [1:03:32<1:54:18,  1.28it/s]Training epoch 1:  37% 5100/13868 [1:03:33<1:58:39,  1.23it/s]Training epoch 1:  37% 5101/13868 [1:03:33<1:56:25,  1.25it/s]Training epoch 1:  37% 5102/13868 [1:03:34<1:54:34,  1.28it/s]Training epoch 1:  37% 5103/13868 [1:03:35<1:53:40,  1.29it/s]Training epoch 1:  37% 5104/13868 [1:03:36<1:51:39,  1.31it/s]Training epoch 1:  37% 5105/13868 [1:03:36<1:49:45,  1.33it/s]Training epoch 1:  37% 5106/13868 [1:03:37<1:51:55,  1.30it/s]Training epoch 1:  37% 5107/13868 [1:03:38<1:51:42,  1.31it/s]Training epoch 1:  37% 5108/13868 [1:03:39<1:53:02,  1.29it/s]Training epoch 1:  37% 5109/13868 [1:03:39<1:50:36,  1.32it/s]Training epoch 1:  37% 5110/13868 [1:03:40<1:50:10,  1.32it/s]Training epoch 1:  37% 5111/13868 [1:03:41<1:49:36,  1.33it/s]Training epoch 1:  37% 5112/13868 [1:03:42<1:51:00,  1.31it/s]Training epoch 1:  37% 5113/13868 [1:03:42<1:51:54,  1.30it/s]Training epoch 1:  37% 5114/13868 [1:03:43<1:51:29,  1.31it/s]Training epoch 1:  37% 5115/13868 [1:03:44<1:51:24,  1.31it/s]Training epoch 1:  37% 5116/13868 [1:03:45<1:51:29,  1.31it/s]Training epoch 1:  37% 5117/13868 [1:03:46<1:51:05,  1.31it/s]Training epoch 1:  37% 5118/13868 [1:03:46<1:51:25,  1.31it/s]Training epoch 1:  37% 5119/13868 [1:03:47<1:51:46,  1.30it/s]Training epoch 1:  37% 5120/13868 [1:03:48<1:52:49,  1.29it/s]Training epoch 1:  37% 5121/13868 [1:03:49<1:51:33,  1.31it/s]Training epoch 1:  37% 5122/13868 [1:03:49<1:50:39,  1.32it/s]Training epoch 1:  37% 5123/13868 [1:03:50<1:50:47,  1.32it/s]Training epoch 1:  37% 5124/13868 [1:03:51<1:48:50,  1.34it/s]Training epoch 1:  37% 5125/13868 [1:03:52<1:49:35,  1.33it/s]Training epoch 1:  37% 5126/13868 [1:03:52<1:50:09,  1.32it/s]Training epoch 1:  37% 5127/13868 [1:03:53<1:50:36,  1.32it/s]Training epoch 1:  37% 5128/13868 [1:03:54<1:50:38,  1.32it/s]Training epoch 1:  37% 5129/13868 [1:03:55<1:49:16,  1.33it/s]Training epoch 1:  37% 5130/13868 [1:03:55<1:49:27,  1.33it/s]Training epoch 1:  37% 5131/13868 [1:03:56<1:50:42,  1.32it/s]Training epoch 1:  37% 5132/13868 [1:03:57<1:50:41,  1.32it/s]Training epoch 1:  37% 5133/13868 [1:03:58<1:50:38,  1.32it/s]Training epoch 1:  37% 5134/13868 [1:03:58<1:50:07,  1.32it/s]Training epoch 1:  37% 5135/13868 [1:03:59<1:49:34,  1.33it/s]Training epoch 1:  37% 5136/13868 [1:04:00<1:51:25,  1.31it/s]Training epoch 1:  37% 5137/13868 [1:04:01<1:49:10,  1.33it/s]Training epoch 1:  37% 5138/13868 [1:04:01<1:49:07,  1.33it/s]Training epoch 1:  37% 5139/13868 [1:04:02<1:48:41,  1.34it/s]Training epoch 1:  37% 5140/13868 [1:04:03<1:48:53,  1.34it/s]Training epoch 1:  37% 5141/13868 [1:04:04<1:48:58,  1.33it/s]Training epoch 1:  37% 5142/13868 [1:04:04<1:48:41,  1.34it/s]Training epoch 1:  37% 5143/13868 [1:04:05<1:49:54,  1.32it/s]Training epoch 1:  37% 5144/13868 [1:04:06<1:50:19,  1.32it/s]Training epoch 1:  37% 5145/13868 [1:04:07<1:49:07,  1.33it/s]Training epoch 1:  37% 5146/13868 [1:04:07<1:49:39,  1.33it/s]Training epoch 1:  37% 5147/13868 [1:04:08<1:49:28,  1.33it/s]Training epoch 1:  37% 5148/13868 [1:04:09<1:49:14,  1.33it/s]Training epoch 1:  37% 5149/13868 [1:04:10<1:47:49,  1.35it/s]Training epoch 1:  37% 5150/13868 [1:04:10<1:47:51,  1.35it/s]Training epoch 1:  37% 5151/13868 [1:04:11<1:46:43,  1.36it/s]Training epoch 1:  37% 5152/13868 [1:04:12<1:45:29,  1.38it/s]Training epoch 1:  37% 5153/13868 [1:04:13<1:47:07,  1.36it/s]Training epoch 1:  37% 5154/13868 [1:04:13<1:47:57,  1.35it/s]Training epoch 1:  37% 5155/13868 [1:04:14<1:47:22,  1.35it/s]Training epoch 1:  37% 5156/13868 [1:04:15<1:47:25,  1.35it/s]Training epoch 1:  37% 5157/13868 [1:04:16<1:48:34,  1.34it/s]Training epoch 1:  37% 5158/13868 [1:04:16<1:48:55,  1.33it/s]Training epoch 1:  37% 5159/13868 [1:04:17<1:49:51,  1.32it/s]Training epoch 1:  37% 5160/13868 [1:04:18<1:50:31,  1.31it/s]Training epoch 1:  37% 5161/13868 [1:04:19<1:50:30,  1.31it/s]Training epoch 1:  37% 5162/13868 [1:04:19<1:50:29,  1.31it/s]Training epoch 1:  37% 5163/13868 [1:04:20<1:48:29,  1.34it/s]Training epoch 1:  37% 5164/13868 [1:04:21<1:49:57,  1.32it/s]Training epoch 1:  37% 5165/13868 [1:04:22<1:51:01,  1.31it/s]Training epoch 1:  37% 5166/13868 [1:04:22<1:51:10,  1.30it/s]Training epoch 1:  37% 5167/13868 [1:04:23<1:51:32,  1.30it/s]Training epoch 1:  37% 5168/13868 [1:04:24<1:49:18,  1.33it/s]Training epoch 1:  37% 5169/13868 [1:04:25<1:50:24,  1.31it/s]Training epoch 1:  37% 5170/13868 [1:04:26<1:50:21,  1.31it/s]Training epoch 1:  37% 5171/13868 [1:04:26<1:49:57,  1.32it/s]Training epoch 1:  37% 5172/13868 [1:04:27<1:48:57,  1.33it/s]Training epoch 1:  37% 5173/13868 [1:04:28<1:48:32,  1.34it/s]Training epoch 1:  37% 5174/13868 [1:04:28<1:49:14,  1.33it/s]Training epoch 1:  37% 5175/13868 [1:04:29<1:49:07,  1.33it/s]Training epoch 1:  37% 5176/13868 [1:04:30<1:49:10,  1.33it/s]Training epoch 1:  37% 5177/13868 [1:04:31<1:50:11,  1.31it/s]Training epoch 1:  37% 5178/13868 [1:04:32<1:49:38,  1.32it/s]Training epoch 1:  37% 5179/13868 [1:04:32<1:49:22,  1.32it/s]Training epoch 1:  37% 5180/13868 [1:04:33<1:48:47,  1.33it/s]Training epoch 1:  37% 5181/13868 [1:04:34<1:48:32,  1.33it/s]Training epoch 1:  37% 5182/13868 [1:04:35<1:49:04,  1.33it/s]Training epoch 1:  37% 5183/13868 [1:04:35<1:48:20,  1.34it/s]Training epoch 1:  37% 5184/13868 [1:04:36<1:48:11,  1.34it/s]Training epoch 1:  37% 5185/13868 [1:04:37<1:48:36,  1.33it/s]Training epoch 1:  37% 5186/13868 [1:04:38<1:49:29,  1.32it/s]Training epoch 1:  37% 5187/13868 [1:04:38<1:48:44,  1.33it/s]Training epoch 1:  37% 5188/13868 [1:04:39<1:51:20,  1.30it/s]Training epoch 1:  37% 5189/13868 [1:04:40<1:50:30,  1.31it/s]Training epoch 1:  37% 5190/13868 [1:04:41<1:48:28,  1.33it/s]Training epoch 1:  37% 5191/13868 [1:04:41<1:49:37,  1.32it/s]Training epoch 1:  37% 5192/13868 [1:04:42<1:49:58,  1.31it/s]Training epoch 1:  37% 5193/13868 [1:04:43<1:48:57,  1.33it/s]Training epoch 1:  37% 5194/13868 [1:04:44<1:48:34,  1.33it/s]Training epoch 1:  37% 5195/13868 [1:04:44<1:50:56,  1.30it/s]Training epoch 1:  37% 5196/13868 [1:04:45<1:49:08,  1.32it/s]Training epoch 1:  37% 5197/13868 [1:04:46<1:48:52,  1.33it/s]Training epoch 1:  37% 5198/13868 [1:04:47<1:48:36,  1.33it/s]Training epoch 1:  37% 5199/13868 [1:04:47<1:49:25,  1.32it/s]Training epoch 1:  37% 5200/13868 [1:04:48<1:54:58,  1.26it/s]Training epoch 1:  38% 5201/13868 [1:04:49<1:54:11,  1.26it/s]Training epoch 1:  38% 5202/13868 [1:04:50<1:51:39,  1.29it/s]Training epoch 1:  38% 5203/13868 [1:04:51<1:50:19,  1.31it/s]Training epoch 1:  38% 5204/13868 [1:04:51<1:50:16,  1.31it/s]Training epoch 1:  38% 5205/13868 [1:04:52<1:49:44,  1.32it/s]Training epoch 1:  38% 5206/13868 [1:04:53<1:49:41,  1.32it/s]Training epoch 1:  38% 5207/13868 [1:04:54<1:47:35,  1.34it/s]Training epoch 1:  38% 5208/13868 [1:04:54<1:47:51,  1.34it/s]Training epoch 1:  38% 5209/13868 [1:04:55<1:48:36,  1.33it/s]Training epoch 1:  38% 5210/13868 [1:04:56<1:47:59,  1.34it/s]Training epoch 1:  38% 5211/13868 [1:04:56<1:46:45,  1.35it/s]Training epoch 1:  38% 5212/13868 [1:04:57<1:45:08,  1.37it/s]Training epoch 1:  38% 5213/13868 [1:04:58<1:45:49,  1.36it/s]Training epoch 1:  38% 5214/13868 [1:04:59<1:47:13,  1.35it/s]Training epoch 1:  38% 5215/13868 [1:04:59<1:46:27,  1.35it/s]Training epoch 1:  38% 5216/13868 [1:05:00<1:46:18,  1.36it/s]Training epoch 1:  38% 5217/13868 [1:05:01<1:47:35,  1.34it/s]Training epoch 1:  38% 5218/13868 [1:05:02<1:47:10,  1.35it/s]Training epoch 1:  38% 5219/13868 [1:05:02<1:47:09,  1.35it/s]Training epoch 1:  38% 5220/13868 [1:05:03<1:47:32,  1.34it/s]Training epoch 1:  38% 5221/13868 [1:05:04<1:47:42,  1.34it/s]Training epoch 1:  38% 5222/13868 [1:05:05<1:47:22,  1.34it/s]Training epoch 1:  38% 5223/13868 [1:05:05<1:48:15,  1.33it/s]Training epoch 1:  38% 5224/13868 [1:05:06<1:47:57,  1.33it/s]Training epoch 1:  38% 5225/13868 [1:05:07<1:48:32,  1.33it/s]Training epoch 1:  38% 5226/13868 [1:05:08<1:49:05,  1.32it/s]Training epoch 1:  38% 5227/13868 [1:05:08<1:47:18,  1.34it/s]Training epoch 1:  38% 5228/13868 [1:05:09<1:47:02,  1.35it/s]Training epoch 1:  38% 5229/13868 [1:05:10<1:47:02,  1.35it/s]Training epoch 1:  38% 5230/13868 [1:05:11<1:46:09,  1.36it/s]Training epoch 1:  38% 5231/13868 [1:05:11<1:47:35,  1.34it/s]Training epoch 1:  38% 5232/13868 [1:05:12<1:47:31,  1.34it/s]Training epoch 1:  38% 5233/13868 [1:05:13<1:46:18,  1.35it/s]Training epoch 1:  38% 5234/13868 [1:05:14<1:47:45,  1.34it/s]Training epoch 1:  38% 5235/13868 [1:05:14<1:47:25,  1.34it/s]Training epoch 1:  38% 5236/13868 [1:05:15<1:46:56,  1.35it/s]Training epoch 1:  38% 5237/13868 [1:05:16<1:48:57,  1.32it/s]Training epoch 1:  38% 5238/13868 [1:05:17<1:47:41,  1.34it/s]Training epoch 1:  38% 5239/13868 [1:05:17<1:45:17,  1.37it/s]Training epoch 1:  38% 5240/13868 [1:05:18<1:45:24,  1.36it/s]Training epoch 1:  38% 5241/13868 [1:05:19<1:47:03,  1.34it/s]Training epoch 1:  38% 5242/13868 [1:05:20<1:46:57,  1.34it/s]Training epoch 1:  38% 5243/13868 [1:05:20<1:46:50,  1.35it/s]Training epoch 1:  38% 5244/13868 [1:05:21<1:47:20,  1.34it/s]Training epoch 1:  38% 5245/13868 [1:05:22<1:46:36,  1.35it/s]Training epoch 1:  38% 5246/13868 [1:05:23<1:47:36,  1.34it/s]Training epoch 1:  38% 5247/13868 [1:05:23<1:48:08,  1.33it/s]Training epoch 1:  38% 5248/13868 [1:05:24<1:47:46,  1.33it/s]Training epoch 1:  38% 5249/13868 [1:05:25<1:48:16,  1.33it/s]Training epoch 1:  38% 5250/13868 [1:05:26<1:47:08,  1.34it/s]Training epoch 1:  38% 5251/13868 [1:05:26<1:47:36,  1.33it/s]Training epoch 1:  38% 5252/13868 [1:05:27<1:49:28,  1.31it/s]Training epoch 1:  38% 5253/13868 [1:05:28<1:50:09,  1.30it/s]Training epoch 1:  38% 5254/13868 [1:05:29<1:48:38,  1.32it/s]Training epoch 1:  38% 5255/13868 [1:05:29<1:46:53,  1.34it/s]Training epoch 1:  38% 5256/13868 [1:05:30<1:47:09,  1.34it/s]Training epoch 1:  38% 5257/13868 [1:05:31<1:48:13,  1.33it/s]Training epoch 1:  38% 5258/13868 [1:05:32<1:47:32,  1.33it/s]Training epoch 1:  38% 5259/13868 [1:05:32<1:48:38,  1.32it/s]Training epoch 1:  38% 5260/13868 [1:05:33<1:48:23,  1.32it/s]Training epoch 1:  38% 5261/13868 [1:05:34<1:47:59,  1.33it/s]Training epoch 1:  38% 5262/13868 [1:05:35<1:48:04,  1.33it/s]Training epoch 1:  38% 5263/13868 [1:05:35<1:47:07,  1.34it/s]Training epoch 1:  38% 5264/13868 [1:05:36<1:46:53,  1.34it/s]Training epoch 1:  38% 5265/13868 [1:05:37<1:46:40,  1.34it/s]Training epoch 1:  38% 5266/13868 [1:05:38<1:46:55,  1.34it/s]Training epoch 1:  38% 5267/13868 [1:05:38<1:46:11,  1.35it/s]Training epoch 1:  38% 5268/13868 [1:05:39<1:47:18,  1.34it/s]Training epoch 1:  38% 5269/13868 [1:05:40<1:48:22,  1.32it/s]Training epoch 1:  38% 5270/13868 [1:05:41<1:46:41,  1.34it/s]Training epoch 1:  38% 5271/13868 [1:05:41<1:46:59,  1.34it/s]Training epoch 1:  38% 5272/13868 [1:05:42<1:46:12,  1.35it/s]Training epoch 1:  38% 5273/13868 [1:05:43<1:45:24,  1.36it/s]Training epoch 1:  38% 5274/13868 [1:05:44<1:45:51,  1.35it/s]Training epoch 1:  38% 5275/13868 [1:05:44<1:45:43,  1.35it/s]Training epoch 1:  38% 5276/13868 [1:05:45<1:45:47,  1.35it/s]Training epoch 1:  38% 5277/13868 [1:05:46<1:46:54,  1.34it/s]Training epoch 1:  38% 5278/13868 [1:05:47<1:47:02,  1.34it/s]Training epoch 1:  38% 5279/13868 [1:05:47<1:48:00,  1.33it/s]Training epoch 1:  38% 5280/13868 [1:05:48<1:47:45,  1.33it/s]Training epoch 1:  38% 5281/13868 [1:05:49<1:47:45,  1.33it/s]Training epoch 1:  38% 5282/13868 [1:05:49<1:46:00,  1.35it/s]Training epoch 1:  38% 5283/13868 [1:05:50<1:46:31,  1.34it/s]Training epoch 1:  38% 5284/13868 [1:05:51<1:47:12,  1.33it/s]Training epoch 1:  38% 5285/13868 [1:05:52<1:48:11,  1.32it/s]Training epoch 1:  38% 5286/13868 [1:05:53<1:48:07,  1.32it/s]Training epoch 1:  38% 5287/13868 [1:05:53<1:48:08,  1.32it/s]Training epoch 1:  38% 5288/13868 [1:05:54<1:45:59,  1.35it/s]Training epoch 1:  38% 5289/13868 [1:05:55<1:45:32,  1.35it/s]Training epoch 1:  38% 5290/13868 [1:05:55<1:46:31,  1.34it/s]Training epoch 1:  38% 5291/13868 [1:05:56<1:47:26,  1.33it/s]Training epoch 1:  38% 5292/13868 [1:05:57<1:48:54,  1.31it/s]Training epoch 1:  38% 5293/13868 [1:05:58<1:48:26,  1.32it/s]Training epoch 1:  38% 5294/13868 [1:05:59<1:48:19,  1.32it/s]Training epoch 1:  38% 5295/13868 [1:05:59<1:48:03,  1.32it/s]Training epoch 1:  38% 5296/13868 [1:06:00<1:48:05,  1.32it/s]Training epoch 1:  38% 5297/13868 [1:06:01<1:46:55,  1.34it/s]Training epoch 1:  38% 5298/13868 [1:06:02<1:47:03,  1.33it/s]Training epoch 1:  38% 5299/13868 [1:06:02<1:47:28,  1.33it/s]Training epoch 1:  38% 5300/13868 [1:06:03<1:54:19,  1.25it/s]Training epoch 1:  38% 5301/13868 [1:06:04<1:50:20,  1.29it/s]Training epoch 1:  38% 5302/13868 [1:06:05<1:50:13,  1.30it/s]Training epoch 1:  38% 5303/13868 [1:06:05<1:50:31,  1.29it/s]Training epoch 1:  38% 5304/13868 [1:06:06<1:49:11,  1.31it/s]Training epoch 1:  38% 5305/13868 [1:06:07<1:48:31,  1.32it/s]Training epoch 1:  38% 5306/13868 [1:06:08<1:48:32,  1.31it/s]Training epoch 1:  38% 5307/13868 [1:06:08<1:47:06,  1.33it/s]Training epoch 1:  38% 5308/13868 [1:06:09<1:47:42,  1.32it/s]Training epoch 1:  38% 5309/13868 [1:06:10<1:47:25,  1.33it/s]Training epoch 1:  38% 5310/13868 [1:06:11<1:47:41,  1.32it/s]Training epoch 1:  38% 5311/13868 [1:06:11<1:45:45,  1.35it/s]Training epoch 1:  38% 5312/13868 [1:06:12<1:45:41,  1.35it/s]Training epoch 1:  38% 5313/13868 [1:06:13<1:46:23,  1.34it/s]Training epoch 1:  38% 5314/13868 [1:06:14<1:46:21,  1.34it/s]Training epoch 1:  38% 5315/13868 [1:06:14<1:46:46,  1.34it/s]Training epoch 1:  38% 5316/13868 [1:06:15<1:47:51,  1.32it/s]Training epoch 1:  38% 5317/13868 [1:06:16<1:46:47,  1.33it/s]Training epoch 1:  38% 5318/13868 [1:06:17<1:47:05,  1.33it/s]Training epoch 1:  38% 5319/13868 [1:06:17<1:47:52,  1.32it/s]Training epoch 1:  38% 5320/13868 [1:06:18<1:47:32,  1.32it/s]Training epoch 1:  38% 5321/13868 [1:06:19<1:45:37,  1.35it/s]Training epoch 1:  38% 5322/13868 [1:06:20<1:45:58,  1.34it/s]Training epoch 1:  38% 5323/13868 [1:06:20<1:46:10,  1.34it/s]Training epoch 1:  38% 5324/13868 [1:06:21<1:48:35,  1.31it/s]Training epoch 1:  38% 5325/13868 [1:06:22<1:48:38,  1.31it/s]Training epoch 1:  38% 5326/13868 [1:06:23<1:47:17,  1.33it/s]Training epoch 1:  38% 5327/13868 [1:06:23<1:47:02,  1.33it/s]Training epoch 1:  38% 5328/13868 [1:06:24<1:48:04,  1.32it/s]Training epoch 1:  38% 5329/13868 [1:06:25<1:48:51,  1.31it/s]Training epoch 1:  38% 5330/13868 [1:06:26<1:48:28,  1.31it/s]Training epoch 1:  38% 5331/13868 [1:06:27<1:48:33,  1.31it/s]Training epoch 1:  38% 5332/13868 [1:06:27<1:49:27,  1.30it/s]Training epoch 1:  38% 5333/13868 [1:06:28<1:48:26,  1.31it/s]Training epoch 1:  38% 5334/13868 [1:06:29<1:46:27,  1.34it/s]Training epoch 1:  38% 5335/13868 [1:06:30<1:46:38,  1.33it/s]Training epoch 1:  38% 5336/13868 [1:06:30<1:48:33,  1.31it/s]Training epoch 1:  38% 5337/13868 [1:06:31<1:47:19,  1.32it/s]Training epoch 1:  38% 5338/13868 [1:06:32<1:46:47,  1.33it/s]Training epoch 1:  38% 5339/13868 [1:06:33<1:46:10,  1.34it/s]Training epoch 1:  39% 5340/13868 [1:06:33<1:45:02,  1.35it/s]Training epoch 1:  39% 5341/13868 [1:06:34<1:44:59,  1.35it/s]Training epoch 1:  39% 5342/13868 [1:06:35<1:44:51,  1.36it/s]Training epoch 1:  39% 5343/13868 [1:06:36<1:45:18,  1.35it/s]Training epoch 1:  39% 5344/13868 [1:06:36<1:47:25,  1.32it/s]Training epoch 1:  39% 5345/13868 [1:06:37<1:46:32,  1.33it/s]Training epoch 1:  39% 5346/13868 [1:06:38<1:47:48,  1.32it/s]Training epoch 1:  39% 5347/13868 [1:06:39<1:46:59,  1.33it/s]Training epoch 1:  39% 5348/13868 [1:06:39<1:47:32,  1.32it/s]Training epoch 1:  39% 5349/13868 [1:06:40<1:46:49,  1.33it/s]Training epoch 1:  39% 5350/13868 [1:06:41<1:47:30,  1.32it/s]Training epoch 1:  39% 5351/13868 [1:06:42<1:47:26,  1.32it/s]Training epoch 1:  39% 5352/13868 [1:06:42<1:47:42,  1.32it/s]Training epoch 1:  39% 5353/13868 [1:06:43<1:48:21,  1.31it/s]Training epoch 1:  39% 5354/13868 [1:06:44<1:48:27,  1.31it/s]Training epoch 1:  39% 5355/13868 [1:06:45<1:46:19,  1.33it/s]Training epoch 1:  39% 5356/13868 [1:06:45<1:45:52,  1.34it/s]Training epoch 1:  39% 5357/13868 [1:06:46<1:44:42,  1.35it/s]Training epoch 1:  39% 5358/13868 [1:06:47<1:44:38,  1.36it/s]Training epoch 1:  39% 5359/13868 [1:06:48<1:45:55,  1.34it/s]Training epoch 1:  39% 5360/13868 [1:06:48<1:47:06,  1.32it/s]Training epoch 1:  39% 5361/13868 [1:06:49<1:47:04,  1.32it/s]Training epoch 1:  39% 5362/13868 [1:06:50<1:48:29,  1.31it/s]Training epoch 1:  39% 5363/13868 [1:06:51<1:48:08,  1.31it/s]Training epoch 1:  39% 5364/13868 [1:06:51<1:49:34,  1.29it/s]Training epoch 1:  39% 5365/13868 [1:06:52<1:48:33,  1.31it/s]Training epoch 1:  39% 5366/13868 [1:06:53<1:48:07,  1.31it/s]Training epoch 1:  39% 5367/13868 [1:06:54<1:47:47,  1.31it/s]Training epoch 1:  39% 5368/13868 [1:06:54<1:47:42,  1.32it/s]Training epoch 1:  39% 5369/13868 [1:06:55<1:46:41,  1.33it/s]Training epoch 1:  39% 5370/13868 [1:06:56<1:46:46,  1.33it/s]Training epoch 1:  39% 5371/13868 [1:06:57<1:47:51,  1.31it/s]Training epoch 1:  39% 5372/13868 [1:06:57<1:47:32,  1.32it/s]Training epoch 1:  39% 5373/13868 [1:06:58<1:45:25,  1.34it/s]Training epoch 1:  39% 5374/13868 [1:06:59<1:45:52,  1.34it/s]Training epoch 1:  39% 5375/13868 [1:07:00<1:46:39,  1.33it/s]Training epoch 1:  39% 5376/13868 [1:07:00<1:45:02,  1.35it/s]Training epoch 1:  39% 5377/13868 [1:07:01<1:44:44,  1.35it/s]Training epoch 1:  39% 5378/13868 [1:07:02<1:45:56,  1.34it/s]Training epoch 1:  39% 5379/13868 [1:07:03<1:46:13,  1.33it/s]Training epoch 1:  39% 5380/13868 [1:07:03<1:46:11,  1.33it/s]Training epoch 1:  39% 5381/13868 [1:07:04<1:44:53,  1.35it/s]Training epoch 1:  39% 5382/13868 [1:07:05<1:44:08,  1.36it/s]Training epoch 1:  39% 5383/13868 [1:07:06<1:44:14,  1.36it/s]Training epoch 1:  39% 5384/13868 [1:07:06<1:45:17,  1.34it/s]Training epoch 1:  39% 5385/13868 [1:07:07<1:46:52,  1.32it/s]Training epoch 1:  39% 5386/13868 [1:07:08<1:47:51,  1.31it/s]Training epoch 1:  39% 5387/13868 [1:07:09<1:48:12,  1.31it/s]Training epoch 1:  39% 5388/13868 [1:07:09<1:47:03,  1.32it/s]Training epoch 1:  39% 5389/13868 [1:07:10<1:47:00,  1.32it/s]Training epoch 1:  39% 5390/13868 [1:07:11<1:46:23,  1.33it/s]Training epoch 1:  39% 5391/13868 [1:07:12<1:44:57,  1.35it/s]Training epoch 1:  39% 5392/13868 [1:07:12<1:44:41,  1.35it/s]Training epoch 1:  39% 5393/13868 [1:07:13<1:46:10,  1.33it/s]Training epoch 1:  39% 5394/13868 [1:07:14<1:46:37,  1.32it/s]Training epoch 1:  39% 5395/13868 [1:07:15<1:46:57,  1.32it/s]Training epoch 1:  39% 5396/13868 [1:07:16<1:49:11,  1.29it/s]Training epoch 1:  39% 5397/13868 [1:07:16<1:47:40,  1.31it/s]Training epoch 1:  39% 5398/13868 [1:07:17<1:46:17,  1.33it/s]Training epoch 1:  39% 5399/13868 [1:07:18<1:45:14,  1.34it/s]Training epoch 1:  39% 5400/13868 [1:07:19<1:51:25,  1.27it/s]Training epoch 1:  39% 5401/13868 [1:07:19<1:49:39,  1.29it/s]Training epoch 1:  39% 5402/13868 [1:07:20<1:46:59,  1.32it/s]Training epoch 1:  39% 5403/13868 [1:07:21<1:47:57,  1.31it/s]Training epoch 1:  39% 5404/13868 [1:07:22<1:48:48,  1.30it/s]Training epoch 1:  39% 5405/13868 [1:07:22<1:46:14,  1.33it/s]Training epoch 1:  39% 5406/13868 [1:07:23<1:46:05,  1.33it/s]Training epoch 1:  39% 5407/13868 [1:07:24<1:45:31,  1.34it/s]Training epoch 1:  39% 5408/13868 [1:07:25<1:45:22,  1.34it/s]Training epoch 1:  39% 5409/13868 [1:07:25<1:44:36,  1.35it/s]Training epoch 1:  39% 5410/13868 [1:07:26<1:44:22,  1.35it/s]Training epoch 1:  39% 5411/13868 [1:07:27<1:44:50,  1.34it/s]Training epoch 1:  39% 5412/13868 [1:07:28<1:47:17,  1.31it/s]Training epoch 1:  39% 5413/13868 [1:07:28<1:46:19,  1.33it/s]Training epoch 1:  39% 5414/13868 [1:07:29<1:45:11,  1.34it/s]Training epoch 1:  39% 5415/13868 [1:07:30<1:45:58,  1.33it/s]Training epoch 1:  39% 5416/13868 [1:07:31<1:45:20,  1.34it/s]Training epoch 1:  39% 5417/13868 [1:07:31<1:46:19,  1.32it/s]Training epoch 1:  39% 5418/13868 [1:07:32<1:46:25,  1.32it/s]Training epoch 1:  39% 5419/13868 [1:07:33<1:47:21,  1.31it/s]Training epoch 1:  39% 5420/13868 [1:07:34<1:48:08,  1.30it/s]Training epoch 1:  39% 5421/13868 [1:07:34<1:47:06,  1.31it/s]Training epoch 1:  39% 5422/13868 [1:07:35<1:46:45,  1.32it/s]Training epoch 1:  39% 5423/13868 [1:07:36<1:47:07,  1.31it/s]Training epoch 1:  39% 5424/13868 [1:07:37<1:44:57,  1.34it/s]Training epoch 1:  39% 5425/13868 [1:07:37<1:44:35,  1.35it/s]Training epoch 1:  39% 5426/13868 [1:07:38<1:44:57,  1.34it/s]Training epoch 1:  39% 5427/13868 [1:07:39<1:46:01,  1.33it/s]Training epoch 1:  39% 5428/13868 [1:07:40<1:47:42,  1.31it/s]Training epoch 1:  39% 5429/13868 [1:07:40<1:47:48,  1.30it/s]Training epoch 1:  39% 5430/13868 [1:07:41<1:46:47,  1.32it/s]Training epoch 1:  39% 5431/13868 [1:07:42<1:46:06,  1.33it/s]Training epoch 1:  39% 5432/13868 [1:07:43<1:43:35,  1.36it/s]Training epoch 1:  39% 5433/13868 [1:07:43<1:44:19,  1.35it/s]Training epoch 1:  39% 5434/13868 [1:07:44<1:44:20,  1.35it/s]Training epoch 1:  39% 5435/13868 [1:07:45<1:42:51,  1.37it/s]Training epoch 1:  39% 5436/13868 [1:07:46<1:43:42,  1.36it/s]Training epoch 1:  39% 5437/13868 [1:07:46<1:43:22,  1.36it/s]Training epoch 1:  39% 5438/13868 [1:07:47<1:45:32,  1.33it/s]Training epoch 1:  39% 5439/13868 [1:07:48<1:45:23,  1.33it/s]Training epoch 1:  39% 5440/13868 [1:07:49<1:44:19,  1.35it/s]Training epoch 1:  39% 5441/13868 [1:07:49<1:43:47,  1.35it/s]Training epoch 1:  39% 5442/13868 [1:07:50<1:43:40,  1.35it/s]Training epoch 1:  39% 5443/13868 [1:07:51<1:46:05,  1.32it/s]Training epoch 1:  39% 5444/13868 [1:07:52<1:45:48,  1.33it/s]Training epoch 1:  39% 5445/13868 [1:07:52<1:45:34,  1.33it/s]Training epoch 1:  39% 5446/13868 [1:07:53<1:46:08,  1.32it/s]Training epoch 1:  39% 5447/13868 [1:07:54<1:45:24,  1.33it/s]Training epoch 1:  39% 5448/13868 [1:07:55<1:45:52,  1.33it/s]Training epoch 1:  39% 5449/13868 [1:07:55<1:47:33,  1.30it/s]Training epoch 1:  39% 5450/13868 [1:07:56<1:45:29,  1.33it/s]Training epoch 1:  39% 5451/13868 [1:07:57<1:45:16,  1.33it/s]Training epoch 1:  39% 5452/13868 [1:07:58<1:44:35,  1.34it/s]Training epoch 1:  39% 5453/13868 [1:07:58<1:44:05,  1.35it/s]Training epoch 1:  39% 5454/13868 [1:07:59<1:44:50,  1.34it/s]Training epoch 1:  39% 5455/13868 [1:08:00<1:43:33,  1.35it/s]Training epoch 1:  39% 5456/13868 [1:08:01<1:42:39,  1.37it/s]Training epoch 1:  39% 5457/13868 [1:08:01<1:42:33,  1.37it/s]Training epoch 1:  39% 5458/13868 [1:08:02<1:43:07,  1.36it/s]Training epoch 1:  39% 5459/13868 [1:08:03<1:43:14,  1.36it/s]Training epoch 1:  39% 5460/13868 [1:08:04<1:45:04,  1.33it/s]Training epoch 1:  39% 5461/13868 [1:08:04<1:45:08,  1.33it/s]Training epoch 1:  39% 5462/13868 [1:08:05<1:46:32,  1.31it/s]Training epoch 1:  39% 5463/13868 [1:08:06<1:48:01,  1.30it/s]Training epoch 1:  39% 5464/13868 [1:08:07<1:47:58,  1.30it/s]Training epoch 1:  39% 5465/13868 [1:08:07<1:47:35,  1.30it/s]Training epoch 1:  39% 5466/13868 [1:08:08<1:47:41,  1.30it/s]Training epoch 1:  39% 5467/13868 [1:08:09<1:47:13,  1.31it/s]Training epoch 1:  39% 5468/13868 [1:08:10<1:45:56,  1.32it/s]Training epoch 1:  39% 5469/13868 [1:08:10<1:44:53,  1.33it/s]Training epoch 1:  39% 5470/13868 [1:08:11<1:44:32,  1.34it/s]Training epoch 1:  39% 5471/13868 [1:08:12<1:44:57,  1.33it/s]Training epoch 1:  39% 5472/13868 [1:08:13<1:44:51,  1.33it/s]Training epoch 1:  39% 5473/13868 [1:08:13<1:44:29,  1.34it/s]Training epoch 1:  39% 5474/13868 [1:08:14<1:45:29,  1.33it/s]Training epoch 1:  39% 5475/13868 [1:08:15<1:45:19,  1.33it/s]Training epoch 1:  39% 5476/13868 [1:08:16<1:45:31,  1.33it/s]Training epoch 1:  39% 5477/13868 [1:08:16<1:45:46,  1.32it/s]Training epoch 1:  40% 5478/13868 [1:08:17<1:46:18,  1.32it/s]Training epoch 1:  40% 5479/13868 [1:08:18<1:45:44,  1.32it/s]Training epoch 1:  40% 5480/13868 [1:08:19<1:45:27,  1.33it/s]Training epoch 1:  40% 5481/13868 [1:08:19<1:43:42,  1.35it/s]Training epoch 1:  40% 5482/13868 [1:08:20<1:43:56,  1.34it/s]Training epoch 1:  40% 5483/13868 [1:08:21<1:45:54,  1.32it/s]Training epoch 1:  40% 5484/13868 [1:08:22<1:43:39,  1.35it/s]Training epoch 1:  40% 5485/13868 [1:08:22<1:43:01,  1.36it/s]Training epoch 1:  40% 5486/13868 [1:08:23<1:43:05,  1.36it/s]Training epoch 1:  40% 5487/13868 [1:08:24<1:42:52,  1.36it/s]Training epoch 1:  40% 5488/13868 [1:08:25<1:43:32,  1.35it/s]Training epoch 1:  40% 5489/13868 [1:08:25<1:44:52,  1.33it/s]Training epoch 1:  40% 5490/13868 [1:08:26<1:43:21,  1.35it/s]Training epoch 1:  40% 5491/13868 [1:08:27<1:44:55,  1.33it/s]Training epoch 1:  40% 5492/13868 [1:08:28<1:46:48,  1.31it/s]Training epoch 1:  40% 5493/13868 [1:08:28<1:45:23,  1.32it/s]Training epoch 1:  40% 5494/13868 [1:08:29<1:45:53,  1.32it/s]Training epoch 1:  40% 5495/13868 [1:08:30<1:46:24,  1.31it/s]Training epoch 1:  40% 5496/13868 [1:08:31<1:44:58,  1.33it/s]Training epoch 1:  40% 5497/13868 [1:08:31<1:45:16,  1.33it/s]Training epoch 1:  40% 5498/13868 [1:08:32<1:45:27,  1.32it/s]Training epoch 1:  40% 5499/13868 [1:08:33<1:45:26,  1.32it/s]Training epoch 1:  40% 5500/13868 [1:08:34<1:51:56,  1.25it/s]Training epoch 1:  40% 5501/13868 [1:08:35<1:50:39,  1.26it/s]Training epoch 1:  40% 5502/13868 [1:08:35<1:48:18,  1.29it/s]Training epoch 1:  40% 5503/13868 [1:08:36<1:46:38,  1.31it/s]Training epoch 1:  40% 5504/13868 [1:08:37<1:47:12,  1.30it/s]Training epoch 1:  40% 5505/13868 [1:08:38<1:45:49,  1.32it/s]Training epoch 1:  40% 5506/13868 [1:08:38<1:45:07,  1.33it/s]Training epoch 1:  40% 5507/13868 [1:08:39<1:44:56,  1.33it/s]Training epoch 1:  40% 5508/13868 [1:08:40<1:44:45,  1.33it/s]Training epoch 1:  40% 5509/13868 [1:08:41<1:44:50,  1.33it/s]Training epoch 1:  40% 5510/13868 [1:08:41<1:45:23,  1.32it/s]Training epoch 1:  40% 5511/13868 [1:08:42<1:44:48,  1.33it/s]Training epoch 1:  40% 5512/13868 [1:08:43<1:45:50,  1.32it/s]Training epoch 1:  40% 5513/13868 [1:08:44<1:46:41,  1.31it/s]Training epoch 1:  40% 5514/13868 [1:08:44<1:44:58,  1.33it/s]Training epoch 1:  40% 5515/13868 [1:08:45<1:45:16,  1.32it/s]Training epoch 1:  40% 5516/13868 [1:08:46<1:44:17,  1.33it/s]Training epoch 1:  40% 5517/13868 [1:08:47<1:46:19,  1.31it/s]Training epoch 1:  40% 5518/13868 [1:08:47<1:46:43,  1.30it/s]Training epoch 1:  40% 5519/13868 [1:08:48<1:46:12,  1.31it/s]Training epoch 1:  40% 5520/13868 [1:08:49<1:46:21,  1.31it/s]Training epoch 1:  40% 5521/13868 [1:08:50<1:44:59,  1.33it/s]Training epoch 1:  40% 5522/13868 [1:08:51<1:48:44,  1.28it/s]Training epoch 1:  40% 5523/13868 [1:08:51<1:47:16,  1.30it/s]Training epoch 1:  40% 5524/13868 [1:08:52<1:45:57,  1.31it/s]Training epoch 1:  40% 5525/13868 [1:08:53<1:48:25,  1.28it/s]Training epoch 1:  40% 5526/13868 [1:08:54<1:47:17,  1.30it/s]Training epoch 1:  40% 5527/13868 [1:08:54<1:49:14,  1.27it/s]Training epoch 1:  40% 5528/13868 [1:08:55<1:46:44,  1.30it/s]Training epoch 1:  40% 5529/13868 [1:08:56<1:44:57,  1.32it/s]Training epoch 1:  40% 5530/13868 [1:08:57<1:44:12,  1.33it/s]Training epoch 1:  40% 5531/13868 [1:08:57<1:42:51,  1.35it/s]Training epoch 1:  40% 5532/13868 [1:08:58<1:43:23,  1.34it/s]Training epoch 1:  40% 5533/13868 [1:08:59<1:43:42,  1.34it/s]Training epoch 1:  40% 5534/13868 [1:09:00<1:43:39,  1.34it/s]Training epoch 1:  40% 5535/13868 [1:09:00<1:44:54,  1.32it/s]Training epoch 1:  40% 5536/13868 [1:09:01<1:45:21,  1.32it/s]Training epoch 1:  40% 5537/13868 [1:09:02<1:44:56,  1.32it/s]Training epoch 1:  40% 5538/13868 [1:09:03<1:45:26,  1.32it/s]Training epoch 1:  40% 5539/13868 [1:09:03<1:44:44,  1.33it/s]Training epoch 1:  40% 5540/13868 [1:09:04<1:45:41,  1.31it/s]Training epoch 1:  40% 5541/13868 [1:09:05<1:45:55,  1.31it/s]Training epoch 1:  40% 5542/13868 [1:09:06<1:46:02,  1.31it/s]Training epoch 1:  40% 5543/13868 [1:09:06<1:44:17,  1.33it/s]Training epoch 1:  40% 5544/13868 [1:09:07<1:42:23,  1.35it/s]Training epoch 1:  40% 5545/13868 [1:09:08<1:42:25,  1.35it/s]Training epoch 1:  40% 5546/13868 [1:09:09<1:41:33,  1.37it/s]Training epoch 1:  40% 5547/13868 [1:09:09<1:43:29,  1.34it/s]Training epoch 1:  40% 5548/13868 [1:09:10<1:44:38,  1.33it/s]Training epoch 1:  40% 5549/13868 [1:09:11<1:44:59,  1.32it/s]Training epoch 1:  40% 5550/13868 [1:09:12<1:45:43,  1.31it/s]Training epoch 1:  40% 5551/13868 [1:09:12<1:45:43,  1.31it/s]Training epoch 1:  40% 5552/13868 [1:09:13<1:47:22,  1.29it/s]Training epoch 1:  40% 5553/13868 [1:09:14<1:46:05,  1.31it/s]Training epoch 1:  40% 5554/13868 [1:09:15<1:47:04,  1.29it/s]Training epoch 1:  40% 5555/13868 [1:09:16<1:45:39,  1.31it/s]Training epoch 1:  40% 5556/13868 [1:09:16<1:46:08,  1.31it/s]Training epoch 1:  40% 5557/13868 [1:09:17<1:47:19,  1.29it/s]Training epoch 1:  40% 5558/13868 [1:09:18<1:45:01,  1.32it/s]Training epoch 1:  40% 5559/13868 [1:09:19<1:46:41,  1.30it/s]Training epoch 1:  40% 5560/13868 [1:09:19<1:46:50,  1.30it/s]Training epoch 1:  40% 5561/13868 [1:09:20<1:48:22,  1.28it/s]Training epoch 1:  40% 5562/13868 [1:09:21<1:46:09,  1.30it/s]Training epoch 1:  40% 5563/13868 [1:09:22<1:44:52,  1.32it/s]Training epoch 1:  40% 5564/13868 [1:09:22<1:44:33,  1.32it/s]Training epoch 1:  40% 5565/13868 [1:09:23<1:43:10,  1.34it/s]Training epoch 1:  40% 5566/13868 [1:09:24<1:42:29,  1.35it/s]Training epoch 1:  40% 5567/13868 [1:09:25<1:42:08,  1.35it/s]Training epoch 1:  40% 5568/13868 [1:09:25<1:42:20,  1.35it/s]Training epoch 1:  40% 5569/13868 [1:09:26<1:43:40,  1.33it/s]Training epoch 1:  40% 5570/13868 [1:09:27<1:45:23,  1.31it/s]Training epoch 1:  40% 5571/13868 [1:09:28<1:44:06,  1.33it/s]Training epoch 1:  40% 5572/13868 [1:09:28<1:45:33,  1.31it/s]Training epoch 1:  40% 5573/13868 [1:09:29<1:44:51,  1.32it/s]Training epoch 1:  40% 5574/13868 [1:09:30<1:44:43,  1.32it/s]Training epoch 1:  40% 5575/13868 [1:09:31<1:43:59,  1.33it/s]Training epoch 1:  40% 5576/13868 [1:09:31<1:43:36,  1.33it/s]Training epoch 1:  40% 5577/13868 [1:09:32<1:44:49,  1.32it/s]Training epoch 1:  40% 5578/13868 [1:09:33<1:44:50,  1.32it/s]Training epoch 1:  40% 5579/13868 [1:09:34<1:45:31,  1.31it/s]Training epoch 1:  40% 5580/13868 [1:09:35<1:45:58,  1.30it/s]Training epoch 1:  40% 5581/13868 [1:09:35<1:46:35,  1.30it/s]Training epoch 1:  40% 5582/13868 [1:09:36<1:45:11,  1.31it/s]Training epoch 1:  40% 5583/13868 [1:09:37<1:44:29,  1.32it/s]Training epoch 1:  40% 5584/13868 [1:09:38<1:44:16,  1.32it/s]Training epoch 1:  40% 5585/13868 [1:09:38<1:43:26,  1.33it/s]Training epoch 1:  40% 5586/13868 [1:09:39<1:43:01,  1.34it/s]Training epoch 1:  40% 5587/13868 [1:09:40<1:43:35,  1.33it/s]Training epoch 1:  40% 5588/13868 [1:09:41<1:44:25,  1.32it/s]Training epoch 1:  40% 5589/13868 [1:09:41<1:45:54,  1.30it/s]Training epoch 1:  40% 5590/13868 [1:09:42<1:46:03,  1.30it/s]Training epoch 1:  40% 5591/13868 [1:09:43<1:45:49,  1.30it/s]Training epoch 1:  40% 5592/13868 [1:09:44<1:44:21,  1.32it/s]Training epoch 1:  40% 5593/13868 [1:09:44<1:46:17,  1.30it/s]Training epoch 1:  40% 5594/13868 [1:09:45<1:46:27,  1.30it/s]Training epoch 1:  40% 5595/13868 [1:09:46<1:46:54,  1.29it/s]Training epoch 1:  40% 5596/13868 [1:09:47<1:45:50,  1.30it/s]Training epoch 1:  40% 5597/13868 [1:09:47<1:45:29,  1.31it/s]Training epoch 1:  40% 5598/13868 [1:09:48<1:46:28,  1.29it/s]Training epoch 1:  40% 5599/13868 [1:09:49<1:47:17,  1.28it/s]Training epoch 1:  40% 5600/13868 [1:09:50<1:54:15,  1.21it/s]Training epoch 1:  40% 5601/13868 [1:09:51<1:50:30,  1.25it/s]Training epoch 1:  40% 5602/13868 [1:09:52<1:48:58,  1.26it/s]Training epoch 1:  40% 5603/13868 [1:09:52<1:45:40,  1.30it/s]Training epoch 1:  40% 5604/13868 [1:09:53<1:44:30,  1.32it/s]Training epoch 1:  40% 5605/13868 [1:09:54<1:43:28,  1.33it/s]Training epoch 1:  40% 5606/13868 [1:09:54<1:44:23,  1.32it/s]Training epoch 1:  40% 5607/13868 [1:09:55<1:43:16,  1.33it/s]Training epoch 1:  40% 5608/13868 [1:09:56<1:43:37,  1.33it/s]Training epoch 1:  40% 5609/13868 [1:09:57<1:44:54,  1.31it/s]Training epoch 1:  40% 5610/13868 [1:09:57<1:44:53,  1.31it/s]Training epoch 1:  40% 5611/13868 [1:09:58<1:46:29,  1.29it/s]Training epoch 1:  40% 5612/13868 [1:09:59<1:45:58,  1.30it/s]Training epoch 1:  40% 5613/13868 [1:10:00<1:46:24,  1.29it/s]Training epoch 1:  40% 5614/13868 [1:10:01<1:46:40,  1.29it/s]Training epoch 1:  40% 5615/13868 [1:10:01<1:46:36,  1.29it/s]Training epoch 1:  40% 5616/13868 [1:10:02<1:45:49,  1.30it/s]Training epoch 1:  41% 5617/13868 [1:10:03<1:45:40,  1.30it/s]Training epoch 1:  41% 5618/13868 [1:10:04<1:45:25,  1.30it/s]Training epoch 1:  41% 5619/13868 [1:10:04<1:44:41,  1.31it/s]Training epoch 1:  41% 5620/13868 [1:10:05<1:44:28,  1.32it/s]Training epoch 1:  41% 5621/13868 [1:10:06<1:44:17,  1.32it/s]Training epoch 1:  41% 5622/13868 [1:10:07<1:44:30,  1.32it/s]Training epoch 1:  41% 5623/13868 [1:10:07<1:43:30,  1.33it/s]Training epoch 1:  41% 5624/13868 [1:10:08<1:44:24,  1.32it/s]Training epoch 1:  41% 5625/13868 [1:10:09<1:44:21,  1.32it/s]Training epoch 1:  41% 5626/13868 [1:10:10<1:44:00,  1.32it/s]Training epoch 1:  41% 5627/13868 [1:10:10<1:43:58,  1.32it/s]Training epoch 1:  41% 5628/13868 [1:10:11<1:43:42,  1.32it/s]Training epoch 1:  41% 5629/13868 [1:10:12<1:44:12,  1.32it/s]Training epoch 1:  41% 5630/13868 [1:10:13<1:45:09,  1.31it/s]Training epoch 1:  41% 5631/13868 [1:10:14<1:44:46,  1.31it/s]Training epoch 1:  41% 5632/13868 [1:10:14<1:44:22,  1.32it/s]Training epoch 1:  41% 5633/13868 [1:10:15<1:43:13,  1.33it/s]Training epoch 1:  41% 5634/13868 [1:10:16<1:45:16,  1.30it/s]Training epoch 1:  41% 5635/13868 [1:10:17<1:53:29,  1.21it/s]Training epoch 1:  41% 5636/13868 [1:10:18<1:49:12,  1.26it/s]Training epoch 1:  41% 5637/13868 [1:10:18<1:47:27,  1.28it/s]Training epoch 1:  41% 5638/13868 [1:10:19<1:45:16,  1.30it/s]Training epoch 1:  41% 5639/13868 [1:10:20<1:47:05,  1.28it/s]Training epoch 1:  41% 5640/13868 [1:10:21<1:43:58,  1.32it/s]Training epoch 1:  41% 5641/13868 [1:10:21<1:44:02,  1.32it/s]Training epoch 1:  41% 5642/13868 [1:10:22<1:40:14,  1.37it/s]Training epoch 1:  41% 5643/13868 [1:10:23<1:40:10,  1.37it/s]Training epoch 1:  41% 5644/13868 [1:10:23<1:41:44,  1.35it/s]Training epoch 1:  41% 5645/13868 [1:10:25<2:04:04,  1.10it/s]Training epoch 1:  41% 5646/13868 [1:10:25<1:57:20,  1.17it/s]Training epoch 1:  41% 5647/13868 [1:10:26<1:53:50,  1.20it/s]Training epoch 1:  41% 5648/13868 [1:10:27<1:49:13,  1.25it/s]Training epoch 1:  41% 5649/13868 [1:10:28<1:49:55,  1.25it/s]Training epoch 1:  41% 5650/13868 [1:10:28<1:45:37,  1.30it/s]Training epoch 1:  41% 5651/13868 [1:10:29<1:46:29,  1.29it/s]Training epoch 1:  41% 5652/13868 [1:10:30<1:43:32,  1.32it/s]Training epoch 1:  41% 5653/13868 [1:10:31<1:45:53,  1.29it/s]Training epoch 1:  41% 5654/13868 [1:10:32<1:44:14,  1.31it/s]Training epoch 1:  41% 5655/13868 [1:10:32<1:46:01,  1.29it/s]Training epoch 1:  41% 5656/13868 [1:10:33<1:41:43,  1.35it/s]Training epoch 1:  41% 5657/13868 [1:10:34<1:44:08,  1.31it/s]Training epoch 1:  41% 5658/13868 [1:10:35<1:43:02,  1.33it/s]Training epoch 1:  41% 5659/13868 [1:10:35<1:46:30,  1.28it/s]Training epoch 1:  41% 5660/13868 [1:10:36<1:44:09,  1.31it/s]Training epoch 1:  41% 5661/13868 [1:10:37<1:47:07,  1.28it/s]Training epoch 1:  41% 5662/13868 [1:10:38<1:44:32,  1.31it/s]Training epoch 1:  41% 5663/13868 [1:10:38<1:45:48,  1.29it/s]Training epoch 1:  41% 5664/13868 [1:10:39<1:42:55,  1.33it/s]Training epoch 1:  41% 5665/13868 [1:10:40<1:45:40,  1.29it/s]Training epoch 1:  41% 5666/13868 [1:10:41<1:42:58,  1.33it/s]Training epoch 1:  41% 5667/13868 [1:10:41<1:43:29,  1.32it/s]Training epoch 1:  41% 5668/13868 [1:10:42<1:45:06,  1.30it/s]Training epoch 1:  41% 5669/13868 [1:10:43<1:46:32,  1.28it/s]Training epoch 1:  41% 5670/13868 [1:10:44<1:45:45,  1.29it/s]Training epoch 1:  41% 5671/13868 [1:10:45<1:45:53,  1.29it/s]Training epoch 1:  41% 5672/13868 [1:10:45<1:43:46,  1.32it/s]Training epoch 1:  41% 5673/13868 [1:10:46<1:42:39,  1.33it/s]Training epoch 1:  41% 5674/13868 [1:10:47<1:42:32,  1.33it/s]Training epoch 1:  41% 5675/13868 [1:10:48<1:42:00,  1.34it/s]Training epoch 1:  41% 5676/13868 [1:10:48<1:41:56,  1.34it/s]Training epoch 1:  41% 5677/13868 [1:10:49<1:42:30,  1.33it/s]Training epoch 1:  41% 5678/13868 [1:10:50<1:43:38,  1.32it/s]Training epoch 1:  41% 5679/13868 [1:10:51<1:44:17,  1.31it/s]Training epoch 1:  41% 5680/13868 [1:10:51<1:43:15,  1.32it/s]Training epoch 1:  41% 5681/13868 [1:10:52<1:43:52,  1.31it/s]Training epoch 1:  41% 5682/13868 [1:10:53<1:44:53,  1.30it/s]Training epoch 1:  41% 5683/13868 [1:10:54<1:43:44,  1.31it/s]Training epoch 1:  41% 5684/13868 [1:10:54<1:43:08,  1.32it/s]Training epoch 1:  41% 5685/13868 [1:10:55<1:43:41,  1.32it/s]Training epoch 1:  41% 5686/13868 [1:10:56<1:45:02,  1.30it/s]Training epoch 1:  41% 5687/13868 [1:10:57<1:45:28,  1.29it/s]Training epoch 1:  41% 5688/13868 [1:10:58<1:45:54,  1.29it/s]Training epoch 1:  41% 5689/13868 [1:10:58<1:44:12,  1.31it/s]Training epoch 1:  41% 5690/13868 [1:10:59<1:43:43,  1.31it/s]Training epoch 1:  41% 5691/13868 [1:11:00<1:44:11,  1.31it/s]Training epoch 1:  41% 5692/13868 [1:11:00<1:42:37,  1.33it/s]Training epoch 1:  41% 5693/13868 [1:11:01<1:42:03,  1.34it/s]Training epoch 1:  41% 5694/13868 [1:11:02<1:43:04,  1.32it/s]Training epoch 1:  41% 5695/13868 [1:11:03<1:43:59,  1.31it/s]Training epoch 1:  41% 5696/13868 [1:11:04<1:43:07,  1.32it/s]Training epoch 1:  41% 5697/13868 [1:11:04<1:42:10,  1.33it/s]Training epoch 1:  41% 5698/13868 [1:11:05<1:43:31,  1.32it/s]Training epoch 1:  41% 5699/13868 [1:11:06<1:44:31,  1.30it/s]Training epoch 1:  41% 5700/13868 [1:11:07<1:52:24,  1.21it/s]Training epoch 1:  41% 5701/13868 [1:11:08<1:49:14,  1.25it/s]Training epoch 1:  41% 5702/13868 [1:11:08<1:47:07,  1.27it/s]Training epoch 1:  41% 5703/13868 [1:11:09<1:46:47,  1.27it/s]Training epoch 1:  41% 5704/13868 [1:11:10<1:46:41,  1.28it/s]Training epoch 1:  41% 5705/13868 [1:11:11<1:46:09,  1.28it/s]Training epoch 1:  41% 5706/13868 [1:11:11<1:45:58,  1.28it/s]Training epoch 1:  41% 5707/13868 [1:11:12<1:46:01,  1.28it/s]Training epoch 1:  41% 5708/13868 [1:11:13<1:46:21,  1.28it/s]Training epoch 1:  41% 5709/13868 [1:11:14<1:45:48,  1.29it/s]Training epoch 1:  41% 5710/13868 [1:11:14<1:44:47,  1.30it/s]Training epoch 1:  41% 5711/13868 [1:11:15<1:43:52,  1.31it/s]Training epoch 1:  41% 5712/13868 [1:11:16<1:43:55,  1.31it/s]Training epoch 1:  41% 5713/13868 [1:11:17<1:44:08,  1.31it/s]Training epoch 1:  41% 5714/13868 [1:11:18<1:44:39,  1.30it/s]Training epoch 1:  41% 5715/13868 [1:11:18<1:43:32,  1.31it/s]Training epoch 1:  41% 5716/13868 [1:11:19<1:44:10,  1.30it/s]Training epoch 1:  41% 5717/13868 [1:11:20<1:43:55,  1.31it/s]Training epoch 1:  41% 5718/13868 [1:11:21<1:44:23,  1.30it/s]Training epoch 1:  41% 5719/13868 [1:11:21<1:45:08,  1.29it/s]Training epoch 1:  41% 5720/13868 [1:11:22<1:42:55,  1.32it/s]Training epoch 1:  41% 5721/13868 [1:11:23<1:43:50,  1.31it/s]Training epoch 1:  41% 5722/13868 [1:11:24<1:44:15,  1.30it/s]Training epoch 1:  41% 5723/13868 [1:11:24<1:43:53,  1.31it/s]Training epoch 1:  41% 5724/13868 [1:11:25<1:44:27,  1.30it/s]Training epoch 1:  41% 5725/13868 [1:11:26<1:43:19,  1.31it/s]Training epoch 1:  41% 5726/13868 [1:11:27<1:42:54,  1.32it/s]Training epoch 1:  41% 5727/13868 [1:11:27<1:42:35,  1.32it/s]Training epoch 1:  41% 5728/13868 [1:11:28<1:41:26,  1.34it/s]Training epoch 1:  41% 5729/13868 [1:11:29<1:42:20,  1.33it/s]Training epoch 1:  41% 5730/13868 [1:11:30<1:41:54,  1.33it/s]Training epoch 1:  41% 5731/13868 [1:11:30<1:43:24,  1.31it/s]Training epoch 1:  41% 5732/13868 [1:11:31<1:41:46,  1.33it/s]Training epoch 1:  41% 5733/13868 [1:11:32<1:41:44,  1.33it/s]Training epoch 1:  41% 5734/13868 [1:11:33<1:42:04,  1.33it/s]Training epoch 1:  41% 5735/13868 [1:11:33<1:41:15,  1.34it/s]Training epoch 1:  41% 5736/13868 [1:11:34<1:41:59,  1.33it/s]Training epoch 1:  41% 5737/13868 [1:11:35<1:42:01,  1.33it/s]Training epoch 1:  41% 5738/13868 [1:11:36<1:41:14,  1.34it/s]Training epoch 1:  41% 5739/13868 [1:11:36<1:40:52,  1.34it/s]Training epoch 1:  41% 5740/13868 [1:11:37<1:42:05,  1.33it/s]Training epoch 1:  41% 5741/13868 [1:11:38<1:41:38,  1.33it/s]Training epoch 1:  41% 5742/13868 [1:11:39<1:40:32,  1.35it/s]Training epoch 1:  41% 5743/13868 [1:11:39<1:41:50,  1.33it/s]Training epoch 1:  41% 5744/13868 [1:11:40<1:41:21,  1.34it/s]Training epoch 1:  41% 5745/13868 [1:11:41<1:43:59,  1.30it/s]Training epoch 1:  41% 5746/13868 [1:11:42<1:42:35,  1.32it/s]Training epoch 1:  41% 5747/13868 [1:11:43<1:42:46,  1.32it/s]Training epoch 1:  41% 5748/13868 [1:11:43<1:43:21,  1.31it/s]Training epoch 1:  41% 5749/13868 [1:11:44<1:42:58,  1.31it/s]Training epoch 1:  41% 5750/13868 [1:11:45<1:42:21,  1.32it/s]Training epoch 1:  41% 5751/13868 [1:11:46<1:42:55,  1.31it/s]Training epoch 1:  41% 5752/13868 [1:11:46<1:42:16,  1.32it/s]Training epoch 1:  41% 5753/13868 [1:11:47<1:42:31,  1.32it/s]Training epoch 1:  41% 5754/13868 [1:11:48<1:40:56,  1.34it/s]Training epoch 1:  41% 5755/13868 [1:11:49<1:42:44,  1.32it/s]Training epoch 1:  42% 5756/13868 [1:11:49<1:41:07,  1.34it/s]Training epoch 1:  42% 5757/13868 [1:11:50<1:41:11,  1.34it/s]Training epoch 1:  42% 5758/13868 [1:11:51<1:39:35,  1.36it/s]Training epoch 1:  42% 5759/13868 [1:11:52<1:40:50,  1.34it/s]Training epoch 1:  42% 5760/13868 [1:11:52<1:41:15,  1.33it/s]Training epoch 1:  42% 5761/13868 [1:11:53<1:41:05,  1.34it/s]Training epoch 1:  42% 5762/13868 [1:11:54<1:41:28,  1.33it/s]Training epoch 1:  42% 5763/13868 [1:11:55<1:40:57,  1.34it/s]Training epoch 1:  42% 5764/13868 [1:11:55<1:42:09,  1.32it/s]Training epoch 1:  42% 5765/13868 [1:11:56<1:41:40,  1.33it/s]Training epoch 1:  42% 5766/13868 [1:11:57<1:40:09,  1.35it/s]Training epoch 1:  42% 5767/13868 [1:11:58<1:41:11,  1.33it/s]Training epoch 1:  42% 5768/13868 [1:11:58<1:42:54,  1.31it/s]Training epoch 1:  42% 5769/13868 [1:11:59<1:43:11,  1.31it/s]Training epoch 1:  42% 5770/13868 [1:12:00<1:42:37,  1.32it/s]Training epoch 1:  42% 5771/13868 [1:12:01<1:41:37,  1.33it/s]Training epoch 1:  42% 5772/13868 [1:12:01<1:41:08,  1.33it/s]Training epoch 1:  42% 5773/13868 [1:12:02<1:41:53,  1.32it/s]Training epoch 1:  42% 5774/13868 [1:12:03<1:41:26,  1.33it/s]Training epoch 1:  42% 5775/13868 [1:12:04<1:41:46,  1.33it/s]Training epoch 1:  42% 5776/13868 [1:12:04<1:42:26,  1.32it/s]Training epoch 1:  42% 5777/13868 [1:12:05<1:40:29,  1.34it/s]Training epoch 1:  42% 5778/13868 [1:12:06<1:40:33,  1.34it/s]Training epoch 1:  42% 5779/13868 [1:12:07<1:39:48,  1.35it/s]Training epoch 1:  42% 5780/13868 [1:12:07<1:40:20,  1.34it/s]Training epoch 1:  42% 5781/13868 [1:12:08<1:39:28,  1.35it/s]Training epoch 1:  42% 5782/13868 [1:12:09<1:39:53,  1.35it/s]Training epoch 1:  42% 5783/13868 [1:12:10<1:40:00,  1.35it/s]Training epoch 1:  42% 5784/13868 [1:12:10<1:40:51,  1.34it/s]Training epoch 1:  42% 5785/13868 [1:12:11<1:41:23,  1.33it/s]Training epoch 1:  42% 5786/13868 [1:12:12<1:40:30,  1.34it/s]Training epoch 1:  42% 5787/13868 [1:12:13<1:41:58,  1.32it/s]Training epoch 1:  42% 5788/13868 [1:12:13<1:42:53,  1.31it/s]Training epoch 1:  42% 5789/13868 [1:12:14<1:41:11,  1.33it/s]Training epoch 1:  42% 5790/13868 [1:12:15<1:41:38,  1.32it/s]Training epoch 1:  42% 5791/13868 [1:12:16<1:42:09,  1.32it/s]Training epoch 1:  42% 5792/13868 [1:12:16<1:41:32,  1.33it/s]Training epoch 1:  42% 5793/13868 [1:12:17<1:42:04,  1.32it/s]Training epoch 1:  42% 5794/13868 [1:12:18<1:42:35,  1.31it/s]Training epoch 1:  42% 5795/13868 [1:12:19<1:43:19,  1.30it/s]Training epoch 1:  42% 5796/13868 [1:12:19<1:42:19,  1.31it/s]Training epoch 1:  42% 5797/13868 [1:12:20<1:41:28,  1.33it/s]Training epoch 1:  42% 5798/13868 [1:12:21<1:41:03,  1.33it/s]Training epoch 1:  42% 5799/13868 [1:12:22<1:41:16,  1.33it/s]Training epoch 1:  42% 5800/13868 [1:12:23<1:50:14,  1.22it/s]Training epoch 1:  42% 5801/13868 [1:12:23<1:48:17,  1.24it/s]Training epoch 1:  42% 5802/13868 [1:12:24<1:47:09,  1.25it/s]Training epoch 1:  42% 5803/13868 [1:12:25<1:44:33,  1.29it/s]Training epoch 1:  42% 5804/13868 [1:12:26<1:42:22,  1.31it/s]Training epoch 1:  42% 5805/13868 [1:12:26<1:42:21,  1.31it/s]Training epoch 1:  42% 5806/13868 [1:12:27<1:42:13,  1.31it/s]Training epoch 1:  42% 5807/13868 [1:12:28<1:42:23,  1.31it/s]Training epoch 1:  42% 5808/13868 [1:12:29<1:42:43,  1.31it/s]Training epoch 1:  42% 5809/13868 [1:12:29<1:42:56,  1.30it/s]Training epoch 1:  42% 5810/13868 [1:12:30<1:42:19,  1.31it/s]Training epoch 1:  42% 5811/13868 [1:12:31<1:43:26,  1.30it/s]Training epoch 1:  42% 5812/13868 [1:12:32<1:41:59,  1.32it/s]Training epoch 1:  42% 5813/13868 [1:12:32<1:42:29,  1.31it/s]Training epoch 1:  42% 5814/13868 [1:12:33<1:41:33,  1.32it/s]Training epoch 1:  42% 5815/13868 [1:12:34<1:41:41,  1.32it/s]Training epoch 1:  42% 5816/13868 [1:12:35<1:41:43,  1.32it/s]Training epoch 1:  42% 5817/13868 [1:12:35<1:40:56,  1.33it/s]Training epoch 1:  42% 5818/13868 [1:12:36<1:43:34,  1.30it/s]Training epoch 1:  42% 5819/13868 [1:12:37<1:42:07,  1.31it/s]Training epoch 1:  42% 5820/13868 [1:12:38<1:43:40,  1.29it/s]Training epoch 1:  42% 5821/13868 [1:12:39<1:43:16,  1.30it/s]Training epoch 1:  42% 5822/13868 [1:12:39<1:42:23,  1.31it/s]Training epoch 1:  42% 5823/13868 [1:12:40<1:41:21,  1.32it/s]Training epoch 1:  42% 5824/13868 [1:12:41<1:40:48,  1.33it/s]Training epoch 1:  42% 5825/13868 [1:12:42<1:41:51,  1.32it/s]Training epoch 1:  42% 5826/13868 [1:12:42<1:40:34,  1.33it/s]Training epoch 1:  42% 5827/13868 [1:12:43<1:40:33,  1.33it/s]Training epoch 1:  42% 5828/13868 [1:12:44<1:42:12,  1.31it/s]Training epoch 1:  42% 5829/13868 [1:12:45<1:42:58,  1.30it/s]Training epoch 1:  42% 5830/13868 [1:12:45<1:43:12,  1.30it/s]Training epoch 1:  42% 5831/13868 [1:12:46<1:42:03,  1.31it/s]Training epoch 1:  42% 5832/13868 [1:12:47<1:41:08,  1.32it/s]Training epoch 1:  42% 5833/13868 [1:12:48<1:42:40,  1.30it/s]Training epoch 1:  42% 5834/13868 [1:12:48<1:42:26,  1.31it/s]Training epoch 1:  42% 5835/13868 [1:12:49<1:42:08,  1.31it/s]Training epoch 1:  42% 5836/13868 [1:12:50<1:42:33,  1.31it/s]Training epoch 1:  42% 5837/13868 [1:12:51<1:42:52,  1.30it/s]Training epoch 1:  42% 5838/13868 [1:12:52<1:41:46,  1.31it/s]Training epoch 1:  42% 5839/13868 [1:12:52<1:42:49,  1.30it/s]Training epoch 1:  42% 5840/13868 [1:12:53<1:41:33,  1.32it/s]Training epoch 1:  42% 5841/13868 [1:12:54<1:41:57,  1.31it/s]Training epoch 1:  42% 5842/13868 [1:12:55<1:41:29,  1.32it/s]Training epoch 1:  42% 5843/13868 [1:12:55<1:40:56,  1.32it/s]Training epoch 1:  42% 5844/13868 [1:12:56<1:40:25,  1.33it/s]Training epoch 1:  42% 5845/13868 [1:12:57<1:41:36,  1.32it/s]Training epoch 1:  42% 5846/13868 [1:12:58<1:40:44,  1.33it/s]Training epoch 1:  42% 5847/13868 [1:12:58<1:40:28,  1.33it/s]Training epoch 1:  42% 5848/13868 [1:12:59<1:39:53,  1.34it/s]Training epoch 1:  42% 5849/13868 [1:13:00<1:41:05,  1.32it/s]Training epoch 1:  42% 5850/13868 [1:13:01<1:40:49,  1.33it/s]Training epoch 1:  42% 5851/13868 [1:13:01<1:40:33,  1.33it/s]Training epoch 1:  42% 5852/13868 [1:13:02<1:41:23,  1.32it/s]Training epoch 1:  42% 5853/13868 [1:13:03<1:42:21,  1.31it/s]Training epoch 1:  42% 5854/13868 [1:13:04<1:41:59,  1.31it/s]Training epoch 1:  42% 5855/13868 [1:13:04<1:42:12,  1.31it/s]Training epoch 1:  42% 5856/13868 [1:13:05<1:40:50,  1.32it/s]Training epoch 1:  42% 5857/13868 [1:13:06<1:41:30,  1.32it/s]Training epoch 1:  42% 5858/13868 [1:13:07<1:41:01,  1.32it/s]Training epoch 1:  42% 5859/13868 [1:13:07<1:40:54,  1.32it/s]Training epoch 1:  42% 5860/13868 [1:13:08<1:38:38,  1.35it/s]Training epoch 1:  42% 5861/13868 [1:13:09<1:39:42,  1.34it/s]Training epoch 1:  42% 5862/13868 [1:13:10<1:40:03,  1.33it/s]Training epoch 1:  42% 5863/13868 [1:13:10<1:40:39,  1.33it/s]Training epoch 1:  42% 5864/13868 [1:13:11<1:41:03,  1.32it/s]Training epoch 1:  42% 5865/13868 [1:13:12<1:41:45,  1.31it/s]Training epoch 1:  42% 5866/13868 [1:13:13<1:41:25,  1.32it/s]Training epoch 1:  42% 5867/13868 [1:13:13<1:41:53,  1.31it/s]Training epoch 1:  42% 5868/13868 [1:13:14<1:40:11,  1.33it/s]Training epoch 1:  42% 5869/13868 [1:13:15<1:40:45,  1.32it/s]Training epoch 1:  42% 5870/13868 [1:13:16<1:42:15,  1.30it/s]Training epoch 1:  42% 5871/13868 [1:13:17<1:42:00,  1.31it/s]Training epoch 1:  42% 5872/13868 [1:13:17<1:42:23,  1.30it/s]Training epoch 1:  42% 5873/13868 [1:13:18<1:41:57,  1.31it/s]Training epoch 1:  42% 5874/13868 [1:13:19<1:39:59,  1.33it/s]Training epoch 1:  42% 5875/13868 [1:13:20<1:39:59,  1.33it/s]Training epoch 1:  42% 5876/13868 [1:13:20<1:40:17,  1.33it/s]Training epoch 1:  42% 5877/13868 [1:13:21<1:42:11,  1.30it/s]Training epoch 1:  42% 5878/13868 [1:13:22<1:40:18,  1.33it/s]Training epoch 1:  42% 5879/13868 [1:13:23<1:39:22,  1.34it/s]Training epoch 1:  42% 5880/13868 [1:13:23<1:39:00,  1.34it/s]Training epoch 1:  42% 5881/13868 [1:13:24<1:39:50,  1.33it/s]Training epoch 1:  42% 5882/13868 [1:13:25<1:39:42,  1.33it/s]Training epoch 1:  42% 5883/13868 [1:13:26<1:40:12,  1.33it/s]Training epoch 1:  42% 5884/13868 [1:13:26<1:40:57,  1.32it/s]Training epoch 1:  42% 5885/13868 [1:13:27<1:41:22,  1.31it/s]Training epoch 1:  42% 5886/13868 [1:13:28<1:42:14,  1.30it/s]Training epoch 1:  42% 5887/13868 [1:13:29<1:42:03,  1.30it/s]Training epoch 1:  42% 5888/13868 [1:13:29<1:41:56,  1.30it/s]Training epoch 1:  42% 5889/13868 [1:13:30<1:41:25,  1.31it/s]Training epoch 1:  42% 5890/13868 [1:13:31<1:40:51,  1.32it/s]Training epoch 1:  42% 5891/13868 [1:13:32<1:40:42,  1.32it/s]Training epoch 1:  42% 5892/13868 [1:13:32<1:40:03,  1.33it/s]Training epoch 1:  42% 5893/13868 [1:13:33<1:40:43,  1.32it/s]Training epoch 1:  43% 5894/13868 [1:13:34<1:41:43,  1.31it/s]Training epoch 1:  43% 5895/13868 [1:13:35<1:40:58,  1.32it/s]Training epoch 1:  43% 5896/13868 [1:13:35<1:40:49,  1.32it/s]Training epoch 1:  43% 5897/13868 [1:13:36<1:41:33,  1.31it/s]Training epoch 1:  43% 5898/13868 [1:13:37<1:40:17,  1.32it/s]Training epoch 1:  43% 5899/13868 [1:13:38<1:38:41,  1.35it/s]Training epoch 1:  43% 5900/13868 [1:13:39<1:44:44,  1.27it/s]Training epoch 1:  43% 5901/13868 [1:13:39<1:45:16,  1.26it/s]Training epoch 1:  43% 5902/13868 [1:13:40<1:43:40,  1.28it/s]Training epoch 1:  43% 5903/13868 [1:13:41<1:43:54,  1.28it/s]Training epoch 1:  43% 5904/13868 [1:13:42<1:41:59,  1.30it/s]Training epoch 1:  43% 5905/13868 [1:13:42<1:41:29,  1.31it/s]Training epoch 1:  43% 5906/13868 [1:13:43<1:40:29,  1.32it/s]Training epoch 1:  43% 5907/13868 [1:13:44<1:40:23,  1.32it/s]Training epoch 1:  43% 5908/13868 [1:13:45<1:41:35,  1.31it/s]Training epoch 1:  43% 5909/13868 [1:13:45<1:40:52,  1.31it/s]Training epoch 1:  43% 5910/13868 [1:13:46<1:40:37,  1.32it/s]Training epoch 1:  43% 5911/13868 [1:13:47<1:39:33,  1.33it/s]Training epoch 1:  43% 5912/13868 [1:13:48<1:40:02,  1.33it/s]Training epoch 1:  43% 5913/13868 [1:13:48<1:39:45,  1.33it/s]Training epoch 1:  43% 5914/13868 [1:13:49<1:39:04,  1.34it/s]Training epoch 1:  43% 5915/13868 [1:13:50<1:39:01,  1.34it/s]Training epoch 1:  43% 5916/13868 [1:13:51<1:39:43,  1.33it/s]Training epoch 1:  43% 5917/13868 [1:13:51<1:39:59,  1.33it/s]Training epoch 1:  43% 5918/13868 [1:13:52<1:39:54,  1.33it/s]Training epoch 1:  43% 5919/13868 [1:13:53<1:37:59,  1.35it/s]Training epoch 1:  43% 5920/13868 [1:13:54<1:38:11,  1.35it/s]Training epoch 1:  43% 5921/13868 [1:13:54<1:39:57,  1.33it/s]Training epoch 1:  43% 5922/13868 [1:13:55<1:38:47,  1.34it/s]Training epoch 1:  43% 5923/13868 [1:13:56<1:40:09,  1.32it/s]Training epoch 1:  43% 5924/13868 [1:13:57<1:38:12,  1.35it/s]Training epoch 1:  43% 5925/13868 [1:13:57<1:38:15,  1.35it/s]Training epoch 1:  43% 5926/13868 [1:13:58<1:38:34,  1.34it/s]Training epoch 1:  43% 5927/13868 [1:13:59<1:40:35,  1.32it/s]Training epoch 1:  43% 5928/13868 [1:14:00<1:39:38,  1.33it/s]Training epoch 1:  43% 5929/13868 [1:14:00<1:40:28,  1.32it/s]Training epoch 1:  43% 5930/13868 [1:14:01<1:39:20,  1.33it/s]Training epoch 1:  43% 5931/13868 [1:14:02<1:39:33,  1.33it/s]Training epoch 1:  43% 5932/13868 [1:14:03<1:38:19,  1.35it/s]Training epoch 1:  43% 5933/13868 [1:14:03<1:38:13,  1.35it/s]Training epoch 1:  43% 5934/13868 [1:14:04<1:38:01,  1.35it/s]Training epoch 1:  43% 5935/13868 [1:14:05<1:37:53,  1.35it/s]Training epoch 1:  43% 5936/13868 [1:14:06<1:38:40,  1.34it/s]Training epoch 1:  43% 5937/13868 [1:14:06<1:40:30,  1.32it/s]Training epoch 1:  43% 5938/13868 [1:14:07<1:39:17,  1.33it/s]Training epoch 1:  43% 5939/13868 [1:14:08<1:41:00,  1.31it/s]Training epoch 1:  43% 5940/13868 [1:14:09<1:40:06,  1.32it/s]Training epoch 1:  43% 5941/13868 [1:14:09<1:40:03,  1.32it/s]Training epoch 1:  43% 5942/13868 [1:14:10<1:40:14,  1.32it/s]Training epoch 1:  43% 5943/13868 [1:14:11<1:40:06,  1.32it/s]Training epoch 1:  43% 5944/13868 [1:14:12<1:40:02,  1.32it/s]Training epoch 1:  43% 5945/13868 [1:14:13<1:41:31,  1.30it/s]Training epoch 1:  43% 5946/13868 [1:14:13<1:38:31,  1.34it/s]Training epoch 1:  43% 5947/13868 [1:14:14<1:37:45,  1.35it/s]Training epoch 1:  43% 5948/13868 [1:14:15<1:37:06,  1.36it/s]Training epoch 1:  43% 5949/13868 [1:14:15<1:38:15,  1.34it/s]Training epoch 1:  43% 5950/13868 [1:14:16<1:38:07,  1.34it/s]Training epoch 1:  43% 5951/13868 [1:14:17<1:39:10,  1.33it/s]Training epoch 1:  43% 5952/13868 [1:14:18<1:39:30,  1.33it/s]Training epoch 1:  43% 5953/13868 [1:14:18<1:40:00,  1.32it/s]Training epoch 1:  43% 5954/13868 [1:14:19<1:39:38,  1.32it/s]Training epoch 1:  43% 5955/13868 [1:14:20<1:39:00,  1.33it/s]Training epoch 1:  43% 5956/13868 [1:14:21<1:39:21,  1.33it/s]Training epoch 1:  43% 5957/13868 [1:14:22<1:40:25,  1.31it/s]Training epoch 1:  43% 5958/13868 [1:14:22<1:39:54,  1.32it/s]Training epoch 1:  43% 5959/13868 [1:14:23<1:40:17,  1.31it/s]Training epoch 1:  43% 5960/13868 [1:14:24<1:42:01,  1.29it/s]Training epoch 1:  43% 5961/13868 [1:14:25<1:42:10,  1.29it/s]Training epoch 1:  43% 5962/13868 [1:14:25<1:41:31,  1.30it/s]Training epoch 1:  43% 5963/13868 [1:14:26<1:40:34,  1.31it/s]Training epoch 1:  43% 5964/13868 [1:14:27<1:39:51,  1.32it/s]Training epoch 1:  43% 5965/13868 [1:14:28<1:39:08,  1.33it/s]Training epoch 1:  43% 5966/13868 [1:14:28<1:40:36,  1.31it/s]Training epoch 1:  43% 5967/13868 [1:14:29<1:40:03,  1.32it/s]Training epoch 1:  43% 5968/13868 [1:14:30<1:40:12,  1.31it/s]Training epoch 1:  43% 5969/13868 [1:14:31<1:41:10,  1.30it/s]Training epoch 1:  43% 5970/13868 [1:14:31<1:40:36,  1.31it/s]Training epoch 1:  43% 5971/13868 [1:14:32<1:40:11,  1.31it/s]Training epoch 1:  43% 5972/13868 [1:14:33<1:39:56,  1.32it/s]Training epoch 1:  43% 5973/13868 [1:14:34<1:38:45,  1.33it/s]Training epoch 1:  43% 5974/13868 [1:14:34<1:39:37,  1.32it/s]Training epoch 1:  43% 5975/13868 [1:14:35<1:37:44,  1.35it/s]Training epoch 1:  43% 5976/13868 [1:14:36<1:39:35,  1.32it/s]Training epoch 1:  43% 5977/13868 [1:14:37<1:39:10,  1.33it/s]Training epoch 1:  43% 5978/13868 [1:14:37<1:39:10,  1.33it/s]Training epoch 1:  43% 5979/13868 [1:14:38<1:39:45,  1.32it/s]Training epoch 1:  43% 5980/13868 [1:14:39<1:38:26,  1.34it/s]Training epoch 1:  43% 5981/13868 [1:14:40<1:39:58,  1.31it/s]Training epoch 1:  43% 5982/13868 [1:14:40<1:39:36,  1.32it/s]Training epoch 1:  43% 5983/13868 [1:14:41<1:40:15,  1.31it/s]Training epoch 1:  43% 5984/13868 [1:14:42<1:40:22,  1.31it/s]Training epoch 1:  43% 5985/13868 [1:14:43<1:40:58,  1.30it/s]Training epoch 1:  43% 5986/13868 [1:14:44<1:39:54,  1.31it/s]Training epoch 1:  43% 5987/13868 [1:14:44<1:38:57,  1.33it/s]Training epoch 1:  43% 5988/13868 [1:14:45<1:40:31,  1.31it/s]Training epoch 1:  43% 5989/13868 [1:14:46<1:39:25,  1.32it/s]Training epoch 1:  43% 5990/13868 [1:14:47<1:40:08,  1.31it/s]Training epoch 1:  43% 5991/13868 [1:14:47<1:40:02,  1.31it/s]Training epoch 1:  43% 5992/13868 [1:14:48<1:39:09,  1.32it/s]Training epoch 1:  43% 5993/13868 [1:14:49<1:38:55,  1.33it/s]Training epoch 1:  43% 5994/13868 [1:14:50<1:40:22,  1.31it/s]Training epoch 1:  43% 5995/13868 [1:14:50<1:39:47,  1.31it/s]Training epoch 1:  43% 5996/13868 [1:14:51<1:40:11,  1.31it/s]Training epoch 1:  43% 5997/13868 [1:14:52<1:39:23,  1.32it/s]Training epoch 1:  43% 5998/13868 [1:14:53<1:40:49,  1.30it/s]Training epoch 1:  43% 5999/13868 [1:14:53<1:39:38,  1.32it/s]Training epoch 1:  43% 6000/13868 [1:14:54<1:46:18,  1.23it/s]Training epoch 1:  43% 6001/13868 [1:14:55<1:42:46,  1.28it/s]Training epoch 1:  43% 6002/13868 [1:14:56<1:42:15,  1.28it/s]Training epoch 1:  43% 6003/13868 [1:14:57<1:40:42,  1.30it/s]Training epoch 1:  43% 6004/13868 [1:14:57<1:39:09,  1.32it/s]Training epoch 1:  43% 6005/13868 [1:14:58<1:38:55,  1.32it/s]Training epoch 1:  43% 6006/13868 [1:14:59<1:37:31,  1.34it/s]Training epoch 1:  43% 6007/13868 [1:15:00<1:39:34,  1.32it/s]Training epoch 1:  43% 6008/13868 [1:15:00<1:39:25,  1.32it/s]Training epoch 1:  43% 6009/13868 [1:15:01<1:39:36,  1.32it/s]Training epoch 1:  43% 6010/13868 [1:15:02<1:40:26,  1.30it/s]Training epoch 1:  43% 6011/13868 [1:15:03<1:38:52,  1.32it/s]Training epoch 1:  43% 6012/13868 [1:15:03<1:39:11,  1.32it/s]Training epoch 1:  43% 6013/13868 [1:15:04<1:37:04,  1.35it/s]Training epoch 1:  43% 6014/13868 [1:15:05<1:38:07,  1.33it/s]Training epoch 1:  43% 6015/13868 [1:15:06<1:39:02,  1.32it/s]Training epoch 1:  43% 6016/13868 [1:15:06<1:40:07,  1.31it/s]Training epoch 1:  43% 6017/13868 [1:15:07<1:37:52,  1.34it/s]Training epoch 1:  43% 6018/13868 [1:15:08<1:38:02,  1.33it/s]Training epoch 1:  43% 6019/13868 [1:15:09<1:36:49,  1.35it/s]Training epoch 1:  43% 6020/13868 [1:15:09<1:37:20,  1.34it/s]Training epoch 1:  43% 6021/13868 [1:15:10<1:36:54,  1.35it/s]Training epoch 1:  43% 6022/13868 [1:15:11<1:38:18,  1.33it/s]Training epoch 1:  43% 6023/13868 [1:15:12<1:37:41,  1.34it/s]Training epoch 1:  43% 6024/13868 [1:15:12<1:37:43,  1.34it/s]Training epoch 1:  43% 6025/13868 [1:15:13<1:37:51,  1.34it/s]Training epoch 1:  43% 6026/13868 [1:15:14<1:38:52,  1.32it/s]Training epoch 1:  43% 6027/13868 [1:15:15<1:38:44,  1.32it/s]Training epoch 1:  43% 6028/13868 [1:15:15<1:39:47,  1.31it/s]Training epoch 1:  43% 6029/13868 [1:15:16<1:40:06,  1.31it/s]Training epoch 1:  43% 6030/13868 [1:15:17<1:40:49,  1.30it/s]Training epoch 1:  43% 6031/13868 [1:15:18<1:39:25,  1.31it/s]Training epoch 1:  43% 6032/13868 [1:15:19<1:41:09,  1.29it/s]Training epoch 1:  44% 6033/13868 [1:15:19<1:39:13,  1.32it/s]Training epoch 1:  44% 6034/13868 [1:15:20<1:39:47,  1.31it/s]Training epoch 1:  44% 6035/13868 [1:15:21<1:39:28,  1.31it/s]Training epoch 1:  44% 6036/13868 [1:15:22<1:40:52,  1.29it/s]Training epoch 1:  44% 6037/13868 [1:15:22<1:41:14,  1.29it/s]Training epoch 1:  44% 6038/13868 [1:15:23<1:40:56,  1.29it/s]Training epoch 1:  44% 6039/13868 [1:15:24<1:41:07,  1.29it/s]Training epoch 1:  44% 6040/13868 [1:15:25<1:40:16,  1.30it/s]Training epoch 1:  44% 6041/13868 [1:15:25<1:39:53,  1.31it/s]Training epoch 1:  44% 6042/13868 [1:15:26<1:39:38,  1.31it/s]Training epoch 1:  44% 6043/13868 [1:15:27<1:39:49,  1.31it/s]Training epoch 1:  44% 6044/13868 [1:15:28<1:41:20,  1.29it/s]Training epoch 1:  44% 6045/13868 [1:15:29<1:41:40,  1.28it/s]Training epoch 1:  44% 6046/13868 [1:15:29<1:40:35,  1.30it/s]Training epoch 1:  44% 6047/13868 [1:15:30<1:38:01,  1.33it/s]Training epoch 1:  44% 6048/13868 [1:15:31<1:37:48,  1.33it/s]Training epoch 1:  44% 6049/13868 [1:15:31<1:36:23,  1.35it/s]Training epoch 1:  44% 6050/13868 [1:15:32<1:36:51,  1.35it/s]Training epoch 1:  44% 6051/13868 [1:15:33<1:36:53,  1.34it/s]Training epoch 1:  44% 6052/13868 [1:15:34<1:38:23,  1.32it/s]Training epoch 1:  44% 6053/13868 [1:15:34<1:38:53,  1.32it/s]Training epoch 1:  44% 6054/13868 [1:15:35<1:38:23,  1.32it/s]Training epoch 1:  44% 6055/13868 [1:15:36<1:38:10,  1.33it/s]Training epoch 1:  44% 6056/13868 [1:15:37<1:38:58,  1.32it/s]Training epoch 1:  44% 6057/13868 [1:15:38<1:38:03,  1.33it/s]Training epoch 1:  44% 6058/13868 [1:15:38<1:38:41,  1.32it/s]Training epoch 1:  44% 6059/13868 [1:15:39<1:38:21,  1.32it/s]Training epoch 1:  44% 6060/13868 [1:15:40<1:38:19,  1.32it/s]Training epoch 1:  44% 6061/13868 [1:15:41<1:38:47,  1.32it/s]Training epoch 1:  44% 6062/13868 [1:15:41<1:39:54,  1.30it/s]Training epoch 1:  44% 6063/13868 [1:15:42<1:39:33,  1.31it/s]Training epoch 1:  44% 6064/13868 [1:15:43<1:41:05,  1.29it/s]Training epoch 1:  44% 6065/13868 [1:15:44<1:38:59,  1.31it/s]Training epoch 1:  44% 6066/13868 [1:15:44<1:39:38,  1.30it/s]Training epoch 1:  44% 6067/13868 [1:15:45<1:37:51,  1.33it/s]Training epoch 1:  44% 6068/13868 [1:15:46<1:38:26,  1.32it/s]Training epoch 1:  44% 6069/13868 [1:15:47<1:37:38,  1.33it/s]Training epoch 1:  44% 6070/13868 [1:15:47<1:36:37,  1.35it/s]Training epoch 1:  44% 6071/13868 [1:15:48<1:35:23,  1.36it/s]Training epoch 1:  44% 6072/13868 [1:15:49<1:35:34,  1.36it/s]Training epoch 1:  44% 6073/13868 [1:15:50<1:35:30,  1.36it/s]Training epoch 1:  44% 6074/13868 [1:15:50<1:36:06,  1.35it/s]Training epoch 1:  44% 6075/13868 [1:15:51<1:37:52,  1.33it/s]Training epoch 1:  44% 6076/13868 [1:15:52<1:39:57,  1.30it/s]Training epoch 1:  44% 6077/13868 [1:15:53<1:39:49,  1.30it/s]Training epoch 1:  44% 6078/13868 [1:15:53<1:40:37,  1.29it/s]Training epoch 1:  44% 6079/13868 [1:15:54<1:37:45,  1.33it/s]Training epoch 1:  44% 6080/13868 [1:15:55<1:37:12,  1.34it/s]Training epoch 1:  44% 6081/13868 [1:15:56<1:37:05,  1.34it/s]Training epoch 1:  44% 6082/13868 [1:15:56<1:38:02,  1.32it/s]Training epoch 1:  44% 6083/13868 [1:15:57<1:38:37,  1.32it/s]Training epoch 1:  44% 6084/13868 [1:15:58<1:39:22,  1.31it/s]Training epoch 1:  44% 6085/13868 [1:15:59<1:37:01,  1.34it/s]Training epoch 1:  44% 6086/13868 [1:15:59<1:37:58,  1.32it/s]Training epoch 1:  44% 6087/13868 [1:16:00<1:38:39,  1.31it/s]Training epoch 1:  44% 6088/13868 [1:16:01<1:38:47,  1.31it/s]Training epoch 1:  44% 6089/13868 [1:16:02<1:38:59,  1.31it/s]Training epoch 1:  44% 6090/13868 [1:16:03<1:40:00,  1.30it/s]Training epoch 1:  44% 6091/13868 [1:16:03<1:40:23,  1.29it/s]Training epoch 1:  44% 6092/13868 [1:16:04<1:38:54,  1.31it/s]Training epoch 1:  44% 6093/13868 [1:16:05<1:38:22,  1.32it/s]Training epoch 1:  44% 6094/13868 [1:16:06<1:37:58,  1.32it/s]Training epoch 1:  44% 6095/13868 [1:16:06<1:36:35,  1.34it/s]Training epoch 1:  44% 6096/13868 [1:16:07<1:36:05,  1.35it/s]Training epoch 1:  44% 6097/13868 [1:16:08<1:36:22,  1.34it/s]Training epoch 1:  44% 6098/13868 [1:16:08<1:35:52,  1.35it/s]Training epoch 1:  44% 6099/13868 [1:16:09<1:36:28,  1.34it/s]Training epoch 1:  44% 6100/13868 [1:16:10<1:43:01,  1.26it/s]Training epoch 1:  44% 6101/13868 [1:16:11<1:41:02,  1.28it/s]Training epoch 1:  44% 6102/13868 [1:16:12<1:40:04,  1.29it/s]Training epoch 1:  44% 6103/13868 [1:16:12<1:39:03,  1.31it/s]Training epoch 1:  44% 6104/13868 [1:16:13<1:39:21,  1.30it/s]Training epoch 1:  44% 6105/13868 [1:16:14<1:37:05,  1.33it/s]Training epoch 1:  44% 6106/13868 [1:16:15<1:37:35,  1.33it/s]Training epoch 1:  44% 6107/13868 [1:16:15<1:37:39,  1.32it/s]Training epoch 1:  44% 6108/13868 [1:16:16<1:38:59,  1.31it/s]Training epoch 1:  44% 6109/13868 [1:16:17<1:38:50,  1.31it/s]Training epoch 1:  44% 6110/13868 [1:16:18<1:37:43,  1.32it/s]Training epoch 1:  44% 6111/13868 [1:16:18<1:38:13,  1.32it/s]Training epoch 1:  44% 6112/13868 [1:16:19<1:37:04,  1.33it/s]Training epoch 1:  44% 6113/13868 [1:16:20<1:37:19,  1.33it/s]Training epoch 1:  44% 6114/13868 [1:16:21<1:38:07,  1.32it/s]Training epoch 1:  44% 6115/13868 [1:16:21<1:39:05,  1.30it/s]Training epoch 1:  44% 6116/13868 [1:16:22<1:38:02,  1.32it/s]Training epoch 1:  44% 6117/13868 [1:16:23<1:38:59,  1.30it/s]Training epoch 1:  44% 6118/13868 [1:16:24<1:38:46,  1.31it/s]Training epoch 1:  44% 6119/13868 [1:16:25<1:36:55,  1.33it/s]Training epoch 1:  44% 6120/13868 [1:16:25<1:36:36,  1.34it/s]Training epoch 1:  44% 6121/13868 [1:16:26<1:36:14,  1.34it/s]Training epoch 1:  44% 6122/13868 [1:16:27<1:37:00,  1.33it/s]Training epoch 1:  44% 6123/13868 [1:16:28<1:37:34,  1.32it/s]Training epoch 1:  44% 6124/13868 [1:16:28<1:38:12,  1.31it/s]Training epoch 1:  44% 6125/13868 [1:16:29<1:38:27,  1.31it/s]Training epoch 1:  44% 6126/13868 [1:16:30<1:38:42,  1.31it/s]Training epoch 1:  44% 6127/13868 [1:16:31<1:39:33,  1.30it/s]Training epoch 1:  44% 6128/13868 [1:16:31<1:38:48,  1.31it/s]Training epoch 1:  44% 6129/13868 [1:16:32<1:39:10,  1.30it/s]Training epoch 1:  44% 6130/13868 [1:16:33<1:38:48,  1.31it/s]Training epoch 1:  44% 6131/13868 [1:16:34<1:37:54,  1.32it/s]Training epoch 1:  44% 6132/13868 [1:16:34<1:38:39,  1.31it/s]Training epoch 1:  44% 6133/13868 [1:16:35<1:37:35,  1.32it/s]Training epoch 1:  44% 6134/13868 [1:16:36<1:36:52,  1.33it/s]Training epoch 1:  44% 6135/13868 [1:16:37<1:37:08,  1.33it/s]Training epoch 1:  44% 6136/13868 [1:16:37<1:38:11,  1.31it/s]Training epoch 1:  44% 6137/13868 [1:16:38<1:35:51,  1.34it/s]Training epoch 1:  44% 6138/13868 [1:16:39<1:36:33,  1.33it/s]Training epoch 1:  44% 6139/13868 [1:16:40<1:38:03,  1.31it/s]Training epoch 1:  44% 6140/13868 [1:16:40<1:36:32,  1.33it/s]Training epoch 1:  44% 6141/13868 [1:16:41<1:35:57,  1.34it/s]Training epoch 1:  44% 6142/13868 [1:16:42<1:38:07,  1.31it/s]Training epoch 1:  44% 6143/13868 [1:16:43<1:38:58,  1.30it/s]Training epoch 1:  44% 6144/13868 [1:16:44<1:38:58,  1.30it/s]Training epoch 1:  44% 6145/13868 [1:16:44<1:38:44,  1.30it/s]Training epoch 1:  44% 6146/13868 [1:16:45<1:38:40,  1.30it/s]Training epoch 1:  44% 6147/13868 [1:16:46<1:36:43,  1.33it/s]Training epoch 1:  44% 6148/13868 [1:16:47<1:36:42,  1.33it/s]Training epoch 1:  44% 6149/13868 [1:16:47<1:36:13,  1.34it/s]Training epoch 1:  44% 6150/13868 [1:16:48<1:35:48,  1.34it/s]Training epoch 1:  44% 6151/13868 [1:16:49<1:37:51,  1.31it/s]Training epoch 1:  44% 6152/13868 [1:16:50<1:38:02,  1.31it/s]Training epoch 1:  44% 6153/13868 [1:16:50<1:37:28,  1.32it/s]Training epoch 1:  44% 6154/13868 [1:16:51<1:36:45,  1.33it/s]Training epoch 1:  44% 6155/13868 [1:16:52<1:36:10,  1.34it/s]Training epoch 1:  44% 6156/13868 [1:16:53<1:37:22,  1.32it/s]Training epoch 1:  44% 6157/13868 [1:16:53<1:37:19,  1.32it/s]Training epoch 1:  44% 6158/13868 [1:16:54<1:38:24,  1.31it/s]Training epoch 1:  44% 6159/13868 [1:16:55<1:38:09,  1.31it/s]Training epoch 1:  44% 6160/13868 [1:16:56<1:38:14,  1.31it/s]Training epoch 1:  44% 6161/13868 [1:16:56<1:37:36,  1.32it/s]Training epoch 1:  44% 6162/13868 [1:16:57<1:37:51,  1.31it/s]Training epoch 1:  44% 6163/13868 [1:16:58<1:38:30,  1.30it/s]Training epoch 1:  44% 6164/13868 [1:16:59<1:35:46,  1.34it/s]Training epoch 1:  44% 6165/13868 [1:16:59<1:36:34,  1.33it/s]Training epoch 1:  44% 6166/13868 [1:17:00<1:36:54,  1.32it/s]Training epoch 1:  44% 6167/13868 [1:17:01<1:36:27,  1.33it/s]Training epoch 1:  44% 6168/13868 [1:17:02<1:37:21,  1.32it/s]Training epoch 1:  44% 6169/13868 [1:17:02<1:38:01,  1.31it/s]Training epoch 1:  44% 6170/13868 [1:17:03<1:37:10,  1.32it/s]Training epoch 1:  44% 6171/13868 [1:17:04<1:37:41,  1.31it/s]Training epoch 1:  45% 6172/13868 [1:17:05<1:37:19,  1.32it/s]Training epoch 1:  45% 6173/13868 [1:17:05<1:36:06,  1.33it/s]Training epoch 1:  45% 6174/13868 [1:17:06<1:35:33,  1.34it/s]Training epoch 1:  45% 6175/13868 [1:17:07<1:34:21,  1.36it/s]Training epoch 1:  45% 6176/13868 [1:17:08<1:35:51,  1.34it/s]Training epoch 1:  45% 6177/13868 [1:17:08<1:35:51,  1.34it/s]Training epoch 1:  45% 6178/13868 [1:17:09<1:36:04,  1.33it/s]Training epoch 1:  45% 6179/13868 [1:17:10<1:37:16,  1.32it/s]Training epoch 1:  45% 6180/13868 [1:17:11<1:36:51,  1.32it/s]Training epoch 1:  45% 6181/13868 [1:17:11<1:36:06,  1.33it/s]Training epoch 1:  45% 6182/13868 [1:17:12<1:37:11,  1.32it/s]Training epoch 1:  45% 6183/13868 [1:17:13<1:37:53,  1.31it/s]Training epoch 1:  45% 6184/13868 [1:17:14<1:37:43,  1.31it/s]Training epoch 1:  45% 6185/13868 [1:17:14<1:37:14,  1.32it/s]Training epoch 1:  45% 6186/13868 [1:17:15<1:36:43,  1.32it/s]Training epoch 1:  45% 6187/13868 [1:17:16<1:35:52,  1.34it/s]Training epoch 1:  45% 6188/13868 [1:17:17<1:35:37,  1.34it/s]Training epoch 1:  45% 6189/13868 [1:17:17<1:35:45,  1.34it/s]Training epoch 1:  45% 6190/13868 [1:17:18<1:37:01,  1.32it/s]Training epoch 1:  45% 6191/13868 [1:17:19<1:36:28,  1.33it/s]Training epoch 1:  45% 6192/13868 [1:17:20<1:37:28,  1.31it/s]Training epoch 1:  45% 6193/13868 [1:17:21<1:37:38,  1.31it/s]Training epoch 1:  45% 6194/13868 [1:17:21<1:38:36,  1.30it/s]Training epoch 1:  45% 6195/13868 [1:17:22<1:38:03,  1.30it/s]Training epoch 1:  45% 6196/13868 [1:17:23<1:37:56,  1.31it/s]Training epoch 1:  45% 6197/13868 [1:17:24<1:37:41,  1.31it/s]Training epoch 1:  45% 6198/13868 [1:17:24<1:37:15,  1.31it/s]Training epoch 1:  45% 6199/13868 [1:17:25<1:36:54,  1.32it/s]Training epoch 1:  45% 6200/13868 [1:17:26<1:42:37,  1.25it/s]Training epoch 1:  45% 6201/13868 [1:17:27<1:41:01,  1.26it/s]Training epoch 1:  45% 6202/13868 [1:17:28<1:39:48,  1.28it/s]Training epoch 1:  45% 6203/13868 [1:17:28<1:36:35,  1.32it/s]Training epoch 1:  45% 6204/13868 [1:17:29<1:35:36,  1.34it/s]Training epoch 1:  45% 6205/13868 [1:17:30<1:35:07,  1.34it/s]Training epoch 1:  45% 6206/13868 [1:17:30<1:35:52,  1.33it/s]Training epoch 1:  45% 6207/13868 [1:17:31<1:36:05,  1.33it/s]Training epoch 1:  45% 6208/13868 [1:17:32<1:36:51,  1.32it/s]Training epoch 1:  45% 6209/13868 [1:17:33<1:37:43,  1.31it/s]Training epoch 1:  45% 6210/13868 [1:17:34<1:37:58,  1.30it/s]Training epoch 1:  45% 6211/13868 [1:17:34<1:38:09,  1.30it/s]Training epoch 1:  45% 6212/13868 [1:17:35<1:35:52,  1.33it/s]Training epoch 1:  45% 6213/13868 [1:17:36<1:35:16,  1.34it/s]Training epoch 1:  45% 6214/13868 [1:17:37<1:35:43,  1.33it/s]Training epoch 1:  45% 6215/13868 [1:17:37<1:36:16,  1.32it/s]Training epoch 1:  45% 6216/13868 [1:17:38<1:36:42,  1.32it/s]Training epoch 1:  45% 6217/13868 [1:17:39<1:36:17,  1.32it/s]Training epoch 1:  45% 6218/13868 [1:17:40<1:35:34,  1.33it/s]Training epoch 1:  45% 6219/13868 [1:17:40<1:36:34,  1.32it/s]Training epoch 1:  45% 6220/13868 [1:17:41<1:36:57,  1.31it/s]Training epoch 1:  45% 6221/13868 [1:17:42<1:36:14,  1.32it/s]Training epoch 1:  45% 6222/13868 [1:17:43<1:35:29,  1.33it/s]Training epoch 1:  45% 6223/13868 [1:17:43<1:35:30,  1.33it/s]Training epoch 1:  45% 6224/13868 [1:17:44<1:36:00,  1.33it/s]Training epoch 1:  45% 6225/13868 [1:17:45<1:36:44,  1.32it/s]Training epoch 1:  45% 6226/13868 [1:17:46<1:36:41,  1.32it/s]Training epoch 1:  45% 6227/13868 [1:17:46<1:36:30,  1.32it/s]Training epoch 1:  45% 6228/13868 [1:17:47<1:36:57,  1.31it/s]Training epoch 1:  45% 6229/13868 [1:17:48<1:35:41,  1.33it/s]Training epoch 1:  45% 6230/13868 [1:17:49<1:37:05,  1.31it/s]Training epoch 1:  45% 6231/13868 [1:17:49<1:36:41,  1.32it/s]Training epoch 1:  45% 6232/13868 [1:17:50<1:37:19,  1.31it/s]Training epoch 1:  45% 6233/13868 [1:17:51<1:36:31,  1.32it/s]Training epoch 1:  45% 6234/13868 [1:17:52<1:35:32,  1.33it/s]Training epoch 1:  45% 6235/13868 [1:17:52<1:36:01,  1.32it/s]Training epoch 1:  45% 6236/13868 [1:17:53<1:35:12,  1.34it/s]Training epoch 1:  45% 6237/13868 [1:17:54<1:36:59,  1.31it/s]Training epoch 1:  45% 6238/13868 [1:17:55<1:36:42,  1.31it/s]Training epoch 1:  45% 6239/13868 [1:17:55<1:37:24,  1.31it/s]Training epoch 1:  45% 6240/13868 [1:17:56<1:37:27,  1.30it/s]Training epoch 1:  45% 6241/13868 [1:17:57<1:37:59,  1.30it/s]Training epoch 1:  45% 6242/13868 [1:17:58<1:37:13,  1.31it/s]Training epoch 1:  45% 6243/13868 [1:17:59<1:36:38,  1.32it/s]Training epoch 1:  45% 6244/13868 [1:17:59<1:36:38,  1.31it/s]Training epoch 1:  45% 6245/13868 [1:18:00<1:37:25,  1.30it/s]Training epoch 1:  45% 6246/13868 [1:18:01<1:37:03,  1.31it/s]Training epoch 1:  45% 6247/13868 [1:18:02<1:36:06,  1.32it/s]Training epoch 1:  45% 6248/13868 [1:18:02<1:36:30,  1.32it/s]Training epoch 1:  45% 6249/13868 [1:18:03<1:34:55,  1.34it/s]Training epoch 1:  45% 6250/13868 [1:18:04<1:35:02,  1.34it/s]Training epoch 1:  45% 6251/13868 [1:18:05<1:34:12,  1.35it/s]Training epoch 1:  45% 6252/13868 [1:18:05<1:34:04,  1.35it/s]Training epoch 1:  45% 6253/13868 [1:18:06<1:33:53,  1.35it/s]Training epoch 1:  45% 6254/13868 [1:18:07<1:33:03,  1.36it/s]Training epoch 1:  45% 6255/13868 [1:18:07<1:33:43,  1.35it/s]Training epoch 1:  45% 6256/13868 [1:18:08<1:34:35,  1.34it/s]Training epoch 1:  45% 6257/13868 [1:18:09<1:35:34,  1.33it/s]Training epoch 1:  45% 6258/13868 [1:18:10<1:35:52,  1.32it/s]Training epoch 1:  45% 6259/13868 [1:18:11<1:36:04,  1.32it/s]Training epoch 1:  45% 6260/13868 [1:18:11<1:36:47,  1.31it/s]Training epoch 1:  45% 6261/13868 [1:18:12<1:36:34,  1.31it/s]Training epoch 1:  45% 6262/13868 [1:18:13<1:36:13,  1.32it/s]Training epoch 1:  45% 6263/13868 [1:18:14<1:35:51,  1.32it/s]Training epoch 1:  45% 6264/13868 [1:18:14<1:36:24,  1.31it/s]Training epoch 1:  45% 6265/13868 [1:18:15<1:36:30,  1.31it/s]Training epoch 1:  45% 6266/13868 [1:18:16<1:36:52,  1.31it/s]Training epoch 1:  45% 6267/13868 [1:18:17<1:35:21,  1.33it/s]Training epoch 1:  45% 6268/13868 [1:18:17<1:35:17,  1.33it/s]Training epoch 1:  45% 6269/13868 [1:18:18<1:35:07,  1.33it/s]Training epoch 1:  45% 6270/13868 [1:18:19<1:34:55,  1.33it/s]Training epoch 1:  45% 6271/13868 [1:18:20<1:33:55,  1.35it/s]Training epoch 1:  45% 6272/13868 [1:18:20<1:33:51,  1.35it/s]Training epoch 1:  45% 6273/13868 [1:18:21<1:34:55,  1.33it/s]Training epoch 1:  45% 6274/13868 [1:18:22<1:36:19,  1.31it/s]Training epoch 1:  45% 6275/13868 [1:18:23<1:37:04,  1.30it/s]Training epoch 1:  45% 6276/13868 [1:18:23<1:37:05,  1.30it/s]Training epoch 1:  45% 6277/13868 [1:18:24<1:34:54,  1.33it/s]Training epoch 1:  45% 6278/13868 [1:18:25<1:36:02,  1.32it/s]Training epoch 1:  45% 6279/13868 [1:18:26<1:35:33,  1.32it/s]Training epoch 1:  45% 6280/13868 [1:18:26<1:35:06,  1.33it/s]Training epoch 1:  45% 6281/13868 [1:18:27<1:35:50,  1.32it/s]Training epoch 1:  45% 6282/13868 [1:18:28<1:35:39,  1.32it/s]Training epoch 1:  45% 6283/13868 [1:18:29<1:36:41,  1.31it/s]Training epoch 1:  45% 6284/13868 [1:18:29<1:36:53,  1.30it/s]Training epoch 1:  45% 6285/13868 [1:18:30<1:36:40,  1.31it/s]Training epoch 1:  45% 6286/13868 [1:18:31<1:36:35,  1.31it/s]Training epoch 1:  45% 6287/13868 [1:18:32<1:37:22,  1.30it/s]Training epoch 1:  45% 6288/13868 [1:18:33<1:37:10,  1.30it/s]Training epoch 1:  45% 6289/13868 [1:18:33<1:36:46,  1.31it/s]Training epoch 1:  45% 6290/13868 [1:18:34<1:38:31,  1.28it/s]Training epoch 1:  45% 6291/13868 [1:18:35<1:37:18,  1.30it/s]Training epoch 1:  45% 6292/13868 [1:18:36<1:37:47,  1.29it/s]Training epoch 1:  45% 6293/13868 [1:18:36<1:37:01,  1.30it/s]Training epoch 1:  45% 6294/13868 [1:18:37<1:37:26,  1.30it/s]Training epoch 1:  45% 6295/13868 [1:18:38<1:36:54,  1.30it/s]Training epoch 1:  45% 6296/13868 [1:18:39<1:38:36,  1.28it/s]Training epoch 1:  45% 6297/13868 [1:18:39<1:35:03,  1.33it/s]Training epoch 1:  45% 6298/13868 [1:18:40<1:34:54,  1.33it/s]Training epoch 1:  45% 6299/13868 [1:18:41<1:35:44,  1.32it/s]Training epoch 1:  45% 6300/13868 [1:18:42<1:41:40,  1.24it/s]Training epoch 1:  45% 6301/13868 [1:18:43<1:38:46,  1.28it/s]Training epoch 1:  45% 6302/13868 [1:18:43<1:38:59,  1.27it/s]Training epoch 1:  45% 6303/13868 [1:18:44<1:39:38,  1.27it/s]Training epoch 1:  45% 6304/13868 [1:18:45<1:40:41,  1.25it/s]Training epoch 1:  45% 6305/13868 [1:18:46<1:38:59,  1.27it/s]Training epoch 1:  45% 6306/13868 [1:18:47<1:38:11,  1.28it/s]Training epoch 1:  45% 6307/13868 [1:18:47<1:38:25,  1.28it/s]Training epoch 1:  45% 6308/13868 [1:18:48<1:37:30,  1.29it/s]Training epoch 1:  45% 6309/13868 [1:18:49<1:38:24,  1.28it/s]Training epoch 1:  46% 6310/13868 [1:18:50<1:37:57,  1.29it/s]Training epoch 1:  46% 6311/13868 [1:18:50<1:37:47,  1.29it/s]Training epoch 1:  46% 6312/13868 [1:18:51<1:37:32,  1.29it/s]Training epoch 1:  46% 6313/13868 [1:18:52<1:35:38,  1.32it/s]Training epoch 1:  46% 6314/13868 [1:18:53<1:34:55,  1.33it/s]Training epoch 1:  46% 6315/13868 [1:18:53<1:34:02,  1.34it/s]Training epoch 1:  46% 6316/13868 [1:18:54<1:35:21,  1.32it/s]Training epoch 1:  46% 6317/13868 [1:18:55<1:35:48,  1.31it/s]Training epoch 1:  46% 6318/13868 [1:18:56<1:35:20,  1.32it/s]Training epoch 1:  46% 6319/13868 [1:18:56<1:34:55,  1.33it/s]Training epoch 1:  46% 6320/13868 [1:18:57<1:36:21,  1.31it/s]Training epoch 1:  46% 6321/13868 [1:18:58<1:35:34,  1.32it/s]Training epoch 1:  46% 6322/13868 [1:18:59<1:36:00,  1.31it/s]Training epoch 1:  46% 6323/13868 [1:18:59<1:34:31,  1.33it/s]Training epoch 1:  46% 6324/13868 [1:19:00<1:34:44,  1.33it/s]Training epoch 1:  46% 6325/13868 [1:19:01<1:35:06,  1.32it/s]Training epoch 1:  46% 6326/13868 [1:19:02<1:35:41,  1.31it/s]Training epoch 1:  46% 6327/13868 [1:19:03<1:34:34,  1.33it/s]Training epoch 1:  46% 6328/13868 [1:19:03<1:34:38,  1.33it/s]Training epoch 1:  46% 6329/13868 [1:19:04<1:35:16,  1.32it/s]Training epoch 1:  46% 6330/13868 [1:19:05<1:34:44,  1.33it/s]Training epoch 1:  46% 6331/13868 [1:19:06<1:33:37,  1.34it/s]Training epoch 1:  46% 6332/13868 [1:19:06<1:34:31,  1.33it/s]Training epoch 1:  46% 6333/13868 [1:19:07<1:34:54,  1.32it/s]Training epoch 1:  46% 6334/13868 [1:19:08<1:35:23,  1.32it/s]Training epoch 1:  46% 6335/13868 [1:19:09<1:35:25,  1.32it/s]Training epoch 1:  46% 6336/13868 [1:19:09<1:35:56,  1.31it/s]Training epoch 1:  46% 6337/13868 [1:19:10<1:34:54,  1.32it/s]Training epoch 1:  46% 6338/13868 [1:19:11<1:35:26,  1.31it/s]Training epoch 1:  46% 6339/13868 [1:19:12<1:35:24,  1.32it/s]Training epoch 1:  46% 6340/13868 [1:19:12<1:36:39,  1.30it/s]Training epoch 1:  46% 6341/13868 [1:19:13<1:34:51,  1.32it/s]Training epoch 1:  46% 6342/13868 [1:19:14<1:34:48,  1.32it/s]Training epoch 1:  46% 6343/13868 [1:19:15<1:33:53,  1.34it/s]Training epoch 1:  46% 6344/13868 [1:19:15<1:35:20,  1.32it/s]Training epoch 1:  46% 6345/13868 [1:19:16<1:34:53,  1.32it/s]Training epoch 1:  46% 6346/13868 [1:19:17<1:34:55,  1.32it/s]Training epoch 1:  46% 6347/13868 [1:19:18<1:34:17,  1.33it/s]Training epoch 1:  46% 6348/13868 [1:19:18<1:33:14,  1.34it/s]Training epoch 1:  46% 6349/13868 [1:19:19<1:33:35,  1.34it/s]Training epoch 1:  46% 6350/13868 [1:19:20<1:34:50,  1.32it/s]Training epoch 1:  46% 6351/13868 [1:19:21<1:34:28,  1.33it/s]Training epoch 1:  46% 6352/13868 [1:19:21<1:35:35,  1.31it/s]Training epoch 1:  46% 6353/13868 [1:19:22<1:36:14,  1.30it/s]Training epoch 1:  46% 6354/13868 [1:19:23<1:35:13,  1.32it/s]Training epoch 1:  46% 6355/13868 [1:19:24<1:34:20,  1.33it/s]Training epoch 1:  46% 6356/13868 [1:19:24<1:33:36,  1.34it/s]Training epoch 1:  46% 6357/13868 [1:19:25<1:33:03,  1.35it/s]Training epoch 1:  46% 6358/13868 [1:19:26<1:33:46,  1.33it/s]Training epoch 1:  46% 6359/13868 [1:19:27<1:33:56,  1.33it/s]Training epoch 1:  46% 6360/13868 [1:19:27<1:33:17,  1.34it/s]Training epoch 1:  46% 6361/13868 [1:19:28<1:32:34,  1.35it/s]Training epoch 1:  46% 6362/13868 [1:19:29<1:33:49,  1.33it/s]Training epoch 1:  46% 6363/13868 [1:19:30<1:33:57,  1.33it/s]Training epoch 1:  46% 6364/13868 [1:19:30<1:35:19,  1.31it/s]Training epoch 1:  46% 6365/13868 [1:19:31<1:35:45,  1.31it/s]Training epoch 1:  46% 6366/13868 [1:19:32<1:37:03,  1.29it/s]Training epoch 1:  46% 6367/13868 [1:19:33<1:36:17,  1.30it/s]Training epoch 1:  46% 6368/13868 [1:19:34<1:36:08,  1.30it/s]Training epoch 1:  46% 6369/13868 [1:19:34<1:36:52,  1.29it/s]Training epoch 1:  46% 6370/13868 [1:19:35<1:36:31,  1.29it/s]Training epoch 1:  46% 6371/13868 [1:19:36<1:36:10,  1.30it/s]Training epoch 1:  46% 6372/13868 [1:19:37<1:35:11,  1.31it/s]Training epoch 1:  46% 6373/13868 [1:19:37<1:33:51,  1.33it/s]Training epoch 1:  46% 6374/13868 [1:19:38<1:33:48,  1.33it/s]Training epoch 1:  46% 6375/13868 [1:19:39<1:33:02,  1.34it/s]Training epoch 1:  46% 6376/13868 [1:19:40<1:33:24,  1.34it/s]Training epoch 1:  46% 6377/13868 [1:19:40<1:33:52,  1.33it/s]Training epoch 1:  46% 6378/13868 [1:19:41<1:33:57,  1.33it/s]Training epoch 1:  46% 6379/13868 [1:19:42<1:33:44,  1.33it/s]Training epoch 1:  46% 6380/13868 [1:19:43<1:33:42,  1.33it/s]Training epoch 1:  46% 6381/13868 [1:19:43<1:32:59,  1.34it/s]Training epoch 1:  46% 6382/13868 [1:19:44<1:32:28,  1.35it/s]Training epoch 1:  46% 6383/13868 [1:19:45<1:32:58,  1.34it/s]Training epoch 1:  46% 6384/13868 [1:19:46<1:33:31,  1.33it/s]Training epoch 1:  46% 6385/13868 [1:19:46<1:33:06,  1.34it/s]Training epoch 1:  46% 6386/13868 [1:19:47<1:32:29,  1.35it/s]Training epoch 1:  46% 6387/13868 [1:19:48<1:32:54,  1.34it/s]Training epoch 1:  46% 6388/13868 [1:19:49<1:33:34,  1.33it/s]Training epoch 1:  46% 6389/13868 [1:19:49<1:34:29,  1.32it/s]Training epoch 1:  46% 6390/13868 [1:19:50<1:35:09,  1.31it/s]Training epoch 1:  46% 6391/13868 [1:19:51<1:35:57,  1.30it/s]Training epoch 1:  46% 6392/13868 [1:19:52<1:34:34,  1.32it/s]Training epoch 1:  46% 6393/13868 [1:19:52<1:34:27,  1.32it/s]Training epoch 1:  46% 6394/13868 [1:19:53<1:34:38,  1.32it/s]Training epoch 1:  46% 6395/13868 [1:19:54<1:33:31,  1.33it/s]Training epoch 1:  46% 6396/13868 [1:19:55<1:35:05,  1.31it/s]Training epoch 1:  46% 6397/13868 [1:19:55<1:35:32,  1.30it/s]Training epoch 1:  46% 6398/13868 [1:19:56<1:35:45,  1.30it/s]Training epoch 1:  46% 6399/13868 [1:19:57<1:34:39,  1.32it/s]Training epoch 1:  46% 6400/13868 [1:19:58<1:39:32,  1.25it/s]Training epoch 1:  46% 6401/13868 [1:19:59<1:37:50,  1.27it/s]Training epoch 1:  46% 6402/13868 [1:19:59<1:35:41,  1.30it/s]Training epoch 1:  46% 6403/13868 [1:20:00<1:33:16,  1.33it/s]Training epoch 1:  46% 6404/13868 [1:20:01<1:33:09,  1.34it/s]Training epoch 1:  46% 6405/13868 [1:20:02<1:33:46,  1.33it/s]Training epoch 1:  46% 6406/13868 [1:20:02<1:33:28,  1.33it/s]Training epoch 1:  46% 6407/13868 [1:20:03<1:32:45,  1.34it/s]Training epoch 1:  46% 6408/13868 [1:20:04<1:34:43,  1.31it/s]Training epoch 1:  46% 6409/13868 [1:20:05<1:34:25,  1.32it/s]Training epoch 1:  46% 6410/13868 [1:20:05<1:34:10,  1.32it/s]Training epoch 1:  46% 6411/13868 [1:20:06<1:32:47,  1.34it/s]Training epoch 1:  46% 6412/13868 [1:20:07<1:32:54,  1.34it/s]Training epoch 1:  46% 6413/13868 [1:20:08<1:31:39,  1.36it/s]Training epoch 1:  46% 6414/13868 [1:20:08<1:32:47,  1.34it/s]Training epoch 1:  46% 6415/13868 [1:20:09<1:32:59,  1.34it/s]Training epoch 1:  46% 6416/13868 [1:20:10<1:33:38,  1.33it/s]Training epoch 1:  46% 6417/13868 [1:20:11<1:34:28,  1.31it/s]Training epoch 1:  46% 6418/13868 [1:20:11<1:34:07,  1.32it/s]Training epoch 1:  46% 6419/13868 [1:20:12<1:33:39,  1.33it/s]Training epoch 1:  46% 6420/13868 [1:20:13<1:34:34,  1.31it/s]Training epoch 1:  46% 6421/13868 [1:20:14<1:34:01,  1.32it/s]Training epoch 1:  46% 6422/13868 [1:20:14<1:35:24,  1.30it/s]Training epoch 1:  46% 6423/13868 [1:20:15<1:34:35,  1.31it/s]Training epoch 1:  46% 6424/13868 [1:20:16<1:35:24,  1.30it/s]Training epoch 1:  46% 6425/13868 [1:20:17<1:33:59,  1.32it/s]Training epoch 1:  46% 6426/13868 [1:20:17<1:33:02,  1.33it/s]Training epoch 1:  46% 6427/13868 [1:20:18<1:35:01,  1.31it/s]Training epoch 1:  46% 6428/13868 [1:20:19<1:34:42,  1.31it/s]Training epoch 1:  46% 6429/13868 [1:20:20<1:33:03,  1.33it/s]Training epoch 1:  46% 6430/13868 [1:20:20<1:33:26,  1.33it/s]Training epoch 1:  46% 6431/13868 [1:20:21<1:34:47,  1.31it/s]Training epoch 1:  46% 6432/13868 [1:20:22<1:34:48,  1.31it/s]Training epoch 1:  46% 6433/13868 [1:20:23<1:33:50,  1.32it/s]Training epoch 1:  46% 6434/13868 [1:20:23<1:32:24,  1.34it/s]Training epoch 1:  46% 6435/13868 [1:20:24<1:33:16,  1.33it/s]Training epoch 1:  46% 6436/13868 [1:20:25<1:33:50,  1.32it/s]Training epoch 1:  46% 6437/13868 [1:20:26<1:34:00,  1.32it/s]Training epoch 1:  46% 6438/13868 [1:20:27<1:34:50,  1.31it/s]Training epoch 1:  46% 6439/13868 [1:20:27<1:34:04,  1.32it/s]Training epoch 1:  46% 6440/13868 [1:20:28<1:34:37,  1.31it/s]Training epoch 1:  46% 6441/13868 [1:20:29<1:33:00,  1.33it/s]Training epoch 1:  46% 6442/13868 [1:20:30<1:32:35,  1.34it/s]Training epoch 1:  46% 6443/13868 [1:20:30<1:33:45,  1.32it/s]Training epoch 1:  46% 6444/13868 [1:20:31<1:34:24,  1.31it/s]Training epoch 1:  46% 6445/13868 [1:20:32<1:33:06,  1.33it/s]Training epoch 1:  46% 6446/13868 [1:20:33<1:33:07,  1.33it/s]Training epoch 1:  46% 6447/13868 [1:20:33<1:33:47,  1.32it/s]Training epoch 1:  46% 6448/13868 [1:20:34<1:32:58,  1.33it/s]Training epoch 1:  47% 6449/13868 [1:20:35<1:32:26,  1.34it/s]Training epoch 1:  47% 6450/13868 [1:20:36<1:33:05,  1.33it/s]Training epoch 1:  47% 6451/13868 [1:20:36<1:33:30,  1.32it/s]Training epoch 1:  47% 6452/13868 [1:20:37<1:34:17,  1.31it/s]Training epoch 1:  47% 6453/13868 [1:20:38<1:34:49,  1.30it/s]Training epoch 1:  47% 6454/13868 [1:20:39<1:35:26,  1.29it/s]Training epoch 1:  47% 6455/13868 [1:20:39<1:34:36,  1.31it/s]Training epoch 1:  47% 6456/13868 [1:20:40<1:35:00,  1.30it/s]Training epoch 1:  47% 6457/13868 [1:20:41<1:35:07,  1.30it/s]Training epoch 1:  47% 6458/13868 [1:20:42<1:35:45,  1.29it/s]Training epoch 1:  47% 6459/13868 [1:20:43<1:34:36,  1.31it/s]Training epoch 1:  47% 6460/13868 [1:20:43<1:33:59,  1.31it/s]Training epoch 1:  47% 6461/13868 [1:20:44<1:33:28,  1.32it/s]Training epoch 1:  47% 6462/13868 [1:20:45<1:34:37,  1.30it/s]Training epoch 1:  47% 6463/13868 [1:20:46<1:35:42,  1.29it/s]Training epoch 1:  47% 6464/13868 [1:20:46<1:35:08,  1.30it/s]Training epoch 1:  47% 6465/13868 [1:20:47<1:35:16,  1.30it/s]Training epoch 1:  47% 6466/13868 [1:20:48<1:36:17,  1.28it/s]Training epoch 1:  47% 6467/13868 [1:20:49<1:34:00,  1.31it/s]Training epoch 1:  47% 6468/13868 [1:20:49<1:34:52,  1.30it/s]Training epoch 1:  47% 6469/13868 [1:20:50<1:33:15,  1.32it/s]Training epoch 1:  47% 6470/13868 [1:20:51<1:34:25,  1.31it/s]Training epoch 1:  47% 6471/13868 [1:20:52<1:35:34,  1.29it/s]Training epoch 1:  47% 6472/13868 [1:20:53<1:35:15,  1.29it/s]Training epoch 1:  47% 6473/13868 [1:20:53<1:33:36,  1.32it/s]Training epoch 1:  47% 6474/13868 [1:20:54<1:33:41,  1.32it/s]Training epoch 1:  47% 6475/13868 [1:20:55<1:34:29,  1.30it/s]Training epoch 1:  47% 6476/13868 [1:20:56<1:34:25,  1.30it/s]Training epoch 1:  47% 6477/13868 [1:20:56<1:34:36,  1.30it/s]Training epoch 1:  47% 6478/13868 [1:20:57<1:33:33,  1.32it/s]Training epoch 1:  47% 6479/13868 [1:20:58<1:32:18,  1.33it/s]Training epoch 1:  47% 6480/13868 [1:20:59<1:33:14,  1.32it/s]Training epoch 1:  47% 6481/13868 [1:20:59<1:33:55,  1.31it/s]Training epoch 1:  47% 6482/13868 [1:21:00<1:34:27,  1.30it/s]Training epoch 1:  47% 6483/13868 [1:21:01<1:33:28,  1.32it/s]Training epoch 1:  47% 6484/13868 [1:21:02<1:33:42,  1.31it/s]Training epoch 1:  47% 6485/13868 [1:21:02<1:33:13,  1.32it/s]Training epoch 1:  47% 6486/13868 [1:21:03<1:33:02,  1.32it/s]Training epoch 1:  47% 6487/13868 [1:21:04<1:32:41,  1.33it/s]Training epoch 1:  47% 6488/13868 [1:21:05<1:34:42,  1.30it/s]Training epoch 1:  47% 6489/13868 [1:21:05<1:33:48,  1.31it/s]Training epoch 1:  47% 6490/13868 [1:21:06<1:34:50,  1.30it/s]Training epoch 1:  47% 6491/13868 [1:21:07<1:36:14,  1.28it/s]Training epoch 1:  47% 6492/13868 [1:21:08<1:34:49,  1.30it/s]Training epoch 1:  47% 6493/13868 [1:21:09<1:33:56,  1.31it/s]Training epoch 1:  47% 6494/13868 [1:21:09<1:34:15,  1.30it/s]Training epoch 1:  47% 6495/13868 [1:21:10<1:34:53,  1.29it/s]Training epoch 1:  47% 6496/13868 [1:21:11<1:34:37,  1.30it/s]Training epoch 1:  47% 6497/13868 [1:21:12<1:31:52,  1.34it/s]Training epoch 1:  47% 6498/13868 [1:21:12<1:32:57,  1.32it/s]Training epoch 1:  47% 6499/13868 [1:21:13<1:32:53,  1.32it/s]Training epoch 1:  47% 6500/13868 [1:21:14<1:39:10,  1.24it/s]Training epoch 1:  47% 6501/13868 [1:21:15<1:37:43,  1.26it/s]Training epoch 1:  47% 6502/13868 [1:21:16<1:38:17,  1.25it/s]Training epoch 1:  47% 6503/13868 [1:21:16<1:34:35,  1.30it/s]Training epoch 1:  47% 6504/13868 [1:21:17<1:34:13,  1.30it/s]Training epoch 1:  47% 6505/13868 [1:21:18<1:34:36,  1.30it/s]Training epoch 1:  47% 6506/13868 [1:21:19<1:33:35,  1.31it/s]Training epoch 1:  47% 6507/13868 [1:21:19<1:33:23,  1.31it/s]Training epoch 1:  47% 6508/13868 [1:21:20<1:33:26,  1.31it/s]Training epoch 1:  47% 6509/13868 [1:21:21<1:32:56,  1.32it/s]Training epoch 1:  47% 6510/13868 [1:21:22<1:33:33,  1.31it/s]Training epoch 1:  47% 6511/13868 [1:21:22<1:32:51,  1.32it/s]Training epoch 1:  47% 6512/13868 [1:21:23<1:33:24,  1.31it/s]Training epoch 1:  47% 6513/13868 [1:21:24<1:32:42,  1.32it/s]Training epoch 1:  47% 6514/13868 [1:21:25<1:32:38,  1.32it/s]Training epoch 1:  47% 6515/13868 [1:21:25<1:32:38,  1.32it/s]Training epoch 1:  47% 6516/13868 [1:21:26<1:33:00,  1.32it/s]Training epoch 1:  47% 6517/13868 [1:21:27<1:32:46,  1.32it/s]Training epoch 1:  47% 6518/13868 [1:21:28<1:34:07,  1.30it/s]Training epoch 1:  47% 6519/13868 [1:21:28<1:34:04,  1.30it/s]Training epoch 1:  47% 6520/13868 [1:21:29<1:33:39,  1.31it/s]Training epoch 1:  47% 6521/13868 [1:21:30<1:33:28,  1.31it/s]Training epoch 1:  47% 6522/13868 [1:21:31<1:33:43,  1.31it/s]Training epoch 1:  47% 6523/13868 [1:21:32<1:33:16,  1.31it/s]Training epoch 1:  47% 6524/13868 [1:21:32<1:31:24,  1.34it/s]Training epoch 1:  47% 6525/13868 [1:21:33<1:31:39,  1.34it/s]Training epoch 1:  47% 6526/13868 [1:21:34<1:31:00,  1.34it/s]Training epoch 1:  47% 6527/13868 [1:21:34<1:31:28,  1.34it/s]Training epoch 1:  47% 6528/13868 [1:21:35<1:32:46,  1.32it/s]Training epoch 1:  47% 6529/13868 [1:21:36<1:32:54,  1.32it/s]Training epoch 1:  47% 6530/13868 [1:21:37<1:31:44,  1.33it/s]Training epoch 1:  47% 6531/13868 [1:21:38<1:32:36,  1.32it/s]Training epoch 1:  47% 6532/13868 [1:21:38<1:32:22,  1.32it/s]Training epoch 1:  47% 6533/13868 [1:21:39<1:32:32,  1.32it/s]Training epoch 1:  47% 6534/13868 [1:21:40<1:31:48,  1.33it/s]Training epoch 1:  47% 6535/13868 [1:21:41<1:31:51,  1.33it/s]Training epoch 1:  47% 6536/13868 [1:21:41<1:33:32,  1.31it/s]Training epoch 1:  47% 6537/13868 [1:21:42<1:32:58,  1.31it/s]Training epoch 1:  47% 6538/13868 [1:21:43<1:33:01,  1.31it/s]Training epoch 1:  47% 6539/13868 [1:21:44<1:33:36,  1.30it/s]Training epoch 1:  47% 6540/13868 [1:21:44<1:34:44,  1.29it/s]Training epoch 1:  47% 6541/13868 [1:21:45<1:35:26,  1.28it/s]Training epoch 1:  47% 6542/13868 [1:21:46<1:34:43,  1.29it/s]Training epoch 1:  47% 6543/13868 [1:21:47<1:32:54,  1.31it/s]Training epoch 1:  47% 6544/13868 [1:21:47<1:33:02,  1.31it/s]Training epoch 1:  47% 6545/13868 [1:21:48<1:33:28,  1.31it/s]Training epoch 1:  47% 6546/13868 [1:21:49<1:33:31,  1.30it/s]Training epoch 1:  47% 6547/13868 [1:21:50<1:32:56,  1.31it/s]Training epoch 1:  47% 6548/13868 [1:21:51<1:35:09,  1.28it/s]Training epoch 1:  47% 6549/13868 [1:21:51<1:34:46,  1.29it/s]Training epoch 1:  47% 6550/13868 [1:21:52<1:33:36,  1.30it/s]Training epoch 1:  47% 6551/13868 [1:21:53<1:32:20,  1.32it/s]Training epoch 1:  47% 6552/13868 [1:21:54<1:33:02,  1.31it/s]Training epoch 1:  47% 6553/13868 [1:21:54<1:33:55,  1.30it/s]Training epoch 1:  47% 6554/13868 [1:21:55<1:33:24,  1.31it/s]Training epoch 1:  47% 6555/13868 [1:21:56<1:33:14,  1.31it/s]Training epoch 1:  47% 6556/13868 [1:21:57<1:33:54,  1.30it/s]Training epoch 1:  47% 6557/13868 [1:21:57<1:33:05,  1.31it/s]Training epoch 1:  47% 6558/13868 [1:21:58<1:32:56,  1.31it/s]Training epoch 1:  47% 6559/13868 [1:21:59<1:32:54,  1.31it/s]Training epoch 1:  47% 6560/13868 [1:22:00<1:33:18,  1.31it/s]Training epoch 1:  47% 6561/13868 [1:22:00<1:33:50,  1.30it/s]Training epoch 1:  47% 6562/13868 [1:22:01<1:34:01,  1.29it/s]Training epoch 1:  47% 6563/13868 [1:22:02<1:33:13,  1.31it/s]Training epoch 1:  47% 6564/13868 [1:22:03<1:34:00,  1.29it/s]Training epoch 1:  47% 6565/13868 [1:22:04<1:33:16,  1.30it/s]Training epoch 1:  47% 6566/13868 [1:22:04<1:33:01,  1.31it/s]Training epoch 1:  47% 6567/13868 [1:22:05<1:33:19,  1.30it/s]Training epoch 1:  47% 6568/13868 [1:22:06<1:33:38,  1.30it/s]Training epoch 1:  47% 6569/13868 [1:22:07<1:32:49,  1.31it/s]Training epoch 1:  47% 6570/13868 [1:22:07<1:33:16,  1.30it/s]Training epoch 1:  47% 6571/13868 [1:22:08<1:32:48,  1.31it/s]Training epoch 1:  47% 6572/13868 [1:22:09<1:32:20,  1.32it/s]Training epoch 1:  47% 6573/13868 [1:22:10<1:31:37,  1.33it/s]Training epoch 1:  47% 6574/13868 [1:22:10<1:32:00,  1.32it/s]Training epoch 1:  47% 6575/13868 [1:22:11<1:31:16,  1.33it/s]Training epoch 1:  47% 6576/13868 [1:22:12<1:32:18,  1.32it/s]Training epoch 1:  47% 6577/13868 [1:22:13<1:32:36,  1.31it/s]Training epoch 1:  47% 6578/13868 [1:22:13<1:31:56,  1.32it/s]Training epoch 1:  47% 6579/13868 [1:22:14<1:31:23,  1.33it/s]Training epoch 1:  47% 6580/13868 [1:22:15<1:32:32,  1.31it/s]Training epoch 1:  47% 6581/13868 [1:22:16<1:33:46,  1.30it/s]Training epoch 1:  47% 6582/13868 [1:22:17<1:33:21,  1.30it/s]Training epoch 1:  47% 6583/13868 [1:22:17<1:33:02,  1.31it/s]Training epoch 1:  47% 6584/13868 [1:22:18<1:32:11,  1.32it/s]Training epoch 1:  47% 6585/13868 [1:22:19<1:31:45,  1.32it/s]Training epoch 1:  47% 6586/13868 [1:22:20<1:33:06,  1.30it/s]Training epoch 1:  47% 6587/13868 [1:22:20<1:31:49,  1.32it/s]Training epoch 1:  48% 6588/13868 [1:22:21<1:33:04,  1.30it/s]Training epoch 1:  48% 6589/13868 [1:22:22<1:32:28,  1.31it/s]Training epoch 1:  48% 6590/13868 [1:22:23<1:32:29,  1.31it/s]Training epoch 1:  48% 6591/13868 [1:22:23<1:31:48,  1.32it/s]Training epoch 1:  48% 6592/13868 [1:22:24<1:31:34,  1.32it/s]Training epoch 1:  48% 6593/13868 [1:22:25<1:31:40,  1.32it/s]Training epoch 1:  48% 6594/13868 [1:22:26<1:32:07,  1.32it/s]Training epoch 1:  48% 6595/13868 [1:22:26<1:31:41,  1.32it/s]Training epoch 1:  48% 6596/13868 [1:22:27<1:31:37,  1.32it/s]Training epoch 1:  48% 6597/13868 [1:22:28<1:31:24,  1.33it/s]Training epoch 1:  48% 6598/13868 [1:22:29<1:31:54,  1.32it/s]Training epoch 1:  48% 6599/13868 [1:22:29<1:31:30,  1.32it/s]Training epoch 1:  48% 6600/13868 [1:22:30<1:36:15,  1.26it/s]Training epoch 1:  48% 6601/13868 [1:22:31<1:35:36,  1.27it/s]Training epoch 1:  48% 6602/13868 [1:22:32<1:34:34,  1.28it/s]Training epoch 1:  48% 6603/13868 [1:22:33<1:33:20,  1.30it/s]Training epoch 1:  48% 6604/13868 [1:22:33<1:32:50,  1.30it/s]Training epoch 1:  48% 6605/13868 [1:22:34<1:32:20,  1.31it/s]Training epoch 1:  48% 6606/13868 [1:22:35<1:31:12,  1.33it/s]Training epoch 1:  48% 6607/13868 [1:22:36<1:31:20,  1.32it/s]Training epoch 1:  48% 6608/13868 [1:22:36<1:30:56,  1.33it/s]Training epoch 1:  48% 6609/13868 [1:22:37<1:30:52,  1.33it/s]Training epoch 1:  48% 6610/13868 [1:22:38<1:32:15,  1.31it/s]Training epoch 1:  48% 6611/13868 [1:22:39<1:32:31,  1.31it/s]Training epoch 1:  48% 6612/13868 [1:22:39<1:32:32,  1.31it/s]Training epoch 1:  48% 6613/13868 [1:22:40<1:33:26,  1.29it/s]Training epoch 1:  48% 6614/13868 [1:22:41<1:32:32,  1.31it/s]Training epoch 1:  48% 6615/13868 [1:22:42<1:32:52,  1.30it/s]Training epoch 1:  48% 6616/13868 [1:22:42<1:33:37,  1.29it/s]Training epoch 1:  48% 6617/13868 [1:22:43<1:33:42,  1.29it/s]Training epoch 1:  48% 6618/13868 [1:22:44<1:32:28,  1.31it/s]Training epoch 1:  48% 6619/13868 [1:22:45<1:32:08,  1.31it/s]Training epoch 1:  48% 6620/13868 [1:22:46<1:31:50,  1.32it/s]Training epoch 1:  48% 6621/13868 [1:22:46<1:31:16,  1.32it/s]Training epoch 1:  48% 6622/13868 [1:22:47<1:30:18,  1.34it/s]Training epoch 1:  48% 6623/13868 [1:22:48<1:31:24,  1.32it/s]Training epoch 1:  48% 6624/13868 [1:22:49<1:30:52,  1.33it/s]Training epoch 1:  48% 6625/13868 [1:22:49<1:32:29,  1.31it/s]Training epoch 1:  48% 6626/13868 [1:22:50<1:32:19,  1.31it/s]Training epoch 1:  48% 6627/13868 [1:22:51<1:31:41,  1.32it/s]Training epoch 1:  48% 6628/13868 [1:22:52<1:31:25,  1.32it/s]Training epoch 1:  48% 6629/13868 [1:22:52<1:31:19,  1.32it/s]Training epoch 1:  48% 6630/13868 [1:22:53<1:30:50,  1.33it/s]Training epoch 1:  48% 6631/13868 [1:22:54<1:31:03,  1.32it/s]Training epoch 1:  48% 6632/13868 [1:22:55<1:31:58,  1.31it/s]Training epoch 1:  48% 6633/13868 [1:22:55<1:32:19,  1.31it/s]Training epoch 1:  48% 6634/13868 [1:22:56<1:32:45,  1.30it/s]Training epoch 1:  48% 6635/13868 [1:22:57<1:32:51,  1.30it/s]Training epoch 1:  48% 6636/13868 [1:22:58<1:31:39,  1.32it/s]Training epoch 1:  48% 6637/13868 [1:22:58<1:30:13,  1.34it/s]Training epoch 1:  48% 6638/13868 [1:22:59<1:29:56,  1.34it/s]Training epoch 1:  48% 6639/13868 [1:23:00<1:29:19,  1.35it/s]Training epoch 1:  48% 6640/13868 [1:23:01<1:29:39,  1.34it/s]Training epoch 1:  48% 6641/13868 [1:23:01<1:30:44,  1.33it/s]Training epoch 1:  48% 6642/13868 [1:23:02<1:30:05,  1.34it/s]Training epoch 1:  48% 6643/13868 [1:23:03<1:31:07,  1.32it/s]Training epoch 1:  48% 6644/13868 [1:23:04<1:31:33,  1.31it/s]Training epoch 1:  48% 6645/13868 [1:23:04<1:31:26,  1.32it/s]Training epoch 1:  48% 6646/13868 [1:23:05<1:31:12,  1.32it/s]Training epoch 1:  48% 6647/13868 [1:23:06<1:31:01,  1.32it/s]Training epoch 1:  48% 6648/13868 [1:23:07<1:30:53,  1.32it/s]Training epoch 1:  48% 6649/13868 [1:23:07<1:31:16,  1.32it/s]Training epoch 1:  48% 6650/13868 [1:23:08<1:30:44,  1.33it/s]Training epoch 1:  48% 6651/13868 [1:23:09<1:29:26,  1.34it/s]Training epoch 1:  48% 6652/13868 [1:23:10<1:28:53,  1.35it/s]Training epoch 1:  48% 6653/13868 [1:23:10<1:29:55,  1.34it/s]Training epoch 1:  48% 6654/13868 [1:23:11<1:31:48,  1.31it/s]Training epoch 1:  48% 6655/13868 [1:23:12<1:32:07,  1.30it/s]Training epoch 1:  48% 6656/13868 [1:23:13<1:32:46,  1.30it/s]Training epoch 1:  48% 6657/13868 [1:23:14<1:31:18,  1.32it/s]Training epoch 1:  48% 6658/13868 [1:23:14<1:32:16,  1.30it/s]Training epoch 1:  48% 6659/13868 [1:23:15<1:32:07,  1.30it/s]Training epoch 1:  48% 6660/13868 [1:23:16<1:30:04,  1.33it/s]Training epoch 1:  48% 6661/13868 [1:23:17<1:29:37,  1.34it/s]Training epoch 1:  48% 6662/13868 [1:23:17<1:30:24,  1.33it/s]Training epoch 1:  48% 6663/13868 [1:23:18<1:30:52,  1.32it/s]Training epoch 1:  48% 6664/13868 [1:23:19<1:31:57,  1.31it/s]Training epoch 1:  48% 6665/13868 [1:23:20<1:31:55,  1.31it/s]Training epoch 1:  48% 6666/13868 [1:23:20<1:31:53,  1.31it/s]Training epoch 1:  48% 6667/13868 [1:23:21<1:32:42,  1.29it/s]Training epoch 1:  48% 6668/13868 [1:23:22<1:33:16,  1.29it/s]Training epoch 1:  48% 6669/13868 [1:23:23<1:32:52,  1.29it/s]Training epoch 1:  48% 6670/13868 [1:23:23<1:32:45,  1.29it/s]Training epoch 1:  48% 6671/13868 [1:23:24<1:32:32,  1.30it/s]Training epoch 1:  48% 6672/13868 [1:23:25<1:33:06,  1.29it/s]Training epoch 1:  48% 6673/13868 [1:23:26<1:32:38,  1.29it/s]Training epoch 1:  48% 6674/13868 [1:23:27<1:33:01,  1.29it/s]Training epoch 1:  48% 6675/13868 [1:23:27<1:31:35,  1.31it/s]Training epoch 1:  48% 6676/13868 [1:23:28<1:31:41,  1.31it/s]Training epoch 1:  48% 6677/13868 [1:23:29<1:31:56,  1.30it/s]Training epoch 1:  48% 6678/13868 [1:23:30<1:32:40,  1.29it/s]Training epoch 1:  48% 6679/13868 [1:23:30<1:32:45,  1.29it/s]Training epoch 1:  48% 6680/13868 [1:23:31<1:32:27,  1.30it/s]Training epoch 1:  48% 6681/13868 [1:23:32<1:31:50,  1.30it/s]Training epoch 1:  48% 6682/13868 [1:23:33<1:31:11,  1.31it/s]Training epoch 1:  48% 6683/13868 [1:23:33<1:30:25,  1.32it/s]Training epoch 1:  48% 6684/13868 [1:23:34<1:28:31,  1.35it/s]Training epoch 1:  48% 6685/13868 [1:23:35<1:29:34,  1.34it/s]Training epoch 1:  48% 6686/13868 [1:23:36<1:30:10,  1.33it/s]Training epoch 1:  48% 6687/13868 [1:23:36<1:29:22,  1.34it/s]Training epoch 1:  48% 6688/13868 [1:23:37<1:29:47,  1.33it/s]Training epoch 1:  48% 6689/13868 [1:23:38<1:30:20,  1.32it/s]Training epoch 1:  48% 6690/13868 [1:23:39<1:31:30,  1.31it/s]Training epoch 1:  48% 6691/13868 [1:23:39<1:30:36,  1.32it/s]Training epoch 1:  48% 6692/13868 [1:23:40<1:30:59,  1.31it/s]Training epoch 1:  48% 6693/13868 [1:23:41<1:30:01,  1.33it/s]Training epoch 1:  48% 6694/13868 [1:23:42<1:29:54,  1.33it/s]Training epoch 1:  48% 6695/13868 [1:23:42<1:29:39,  1.33it/s]Training epoch 1:  48% 6696/13868 [1:23:43<1:30:09,  1.33it/s]Training epoch 1:  48% 6697/13868 [1:23:44<1:29:51,  1.33it/s]Training epoch 1:  48% 6698/13868 [1:23:45<1:31:30,  1.31it/s]Training epoch 1:  48% 6699/13868 [1:23:45<1:30:44,  1.32it/s]Training epoch 1:  48% 6700/13868 [1:23:47<1:39:49,  1.20it/s]Training epoch 1:  48% 6701/13868 [1:23:47<1:36:59,  1.23it/s]Training epoch 1:  48% 6702/13868 [1:23:48<1:34:59,  1.26it/s]Training epoch 1:  48% 6703/13868 [1:23:49<1:33:59,  1.27it/s]Training epoch 1:  48% 6704/13868 [1:23:50<1:33:12,  1.28it/s]Training epoch 1:  48% 6705/13868 [1:23:50<1:31:37,  1.30it/s]Training epoch 1:  48% 6706/13868 [1:23:51<1:31:12,  1.31it/s]Training epoch 1:  48% 6707/13868 [1:23:52<1:31:01,  1.31it/s]Training epoch 1:  48% 6708/13868 [1:23:53<1:30:58,  1.31it/s]Training epoch 1:  48% 6709/13868 [1:23:53<1:31:37,  1.30it/s]Training epoch 1:  48% 6710/13868 [1:23:54<1:31:06,  1.31it/s]Training epoch 1:  48% 6711/13868 [1:23:55<1:29:44,  1.33it/s]Training epoch 1:  48% 6712/13868 [1:23:56<1:29:43,  1.33it/s]Training epoch 1:  48% 6713/13868 [1:23:56<1:30:21,  1.32it/s]Training epoch 1:  48% 6714/13868 [1:23:57<1:30:46,  1.31it/s]Training epoch 1:  48% 6715/13868 [1:23:58<1:30:40,  1.31it/s]Training epoch 1:  48% 6716/13868 [1:23:59<1:31:02,  1.31it/s]Training epoch 1:  48% 6717/13868 [1:23:59<1:31:16,  1.31it/s]Training epoch 1:  48% 6718/13868 [1:24:00<1:31:54,  1.30it/s]Training epoch 1:  48% 6719/13868 [1:24:01<1:31:12,  1.31it/s]Training epoch 1:  48% 6720/13868 [1:24:02<1:31:38,  1.30it/s]Training epoch 1:  48% 6721/13868 [1:24:03<1:31:37,  1.30it/s]Training epoch 1:  48% 6722/13868 [1:24:03<1:31:49,  1.30it/s]Training epoch 1:  48% 6723/13868 [1:24:04<1:31:06,  1.31it/s]Training epoch 1:  48% 6724/13868 [1:24:05<1:31:04,  1.31it/s]Training epoch 1:  48% 6725/13868 [1:24:06<1:30:33,  1.31it/s]Training epoch 1:  49% 6726/13868 [1:24:06<1:30:41,  1.31it/s]Training epoch 1:  49% 6727/13868 [1:24:07<1:30:50,  1.31it/s]Training epoch 1:  49% 6728/13868 [1:24:08<1:30:42,  1.31it/s]Training epoch 1:  49% 6729/13868 [1:24:09<1:30:36,  1.31it/s]Training epoch 1:  49% 6730/13868 [1:24:09<1:32:22,  1.29it/s]Training epoch 1:  49% 6731/13868 [1:24:10<1:31:54,  1.29it/s]Training epoch 1:  49% 6732/13868 [1:24:11<1:32:01,  1.29it/s]Training epoch 1:  49% 6733/13868 [1:24:12<1:31:14,  1.30it/s]Training epoch 1:  49% 6734/13868 [1:24:12<1:31:29,  1.30it/s]Training epoch 1:  49% 6735/13868 [1:24:13<1:30:23,  1.32it/s]Training epoch 1:  49% 6736/13868 [1:24:14<1:30:07,  1.32it/s]Training epoch 1:  49% 6737/13868 [1:24:15<1:29:40,  1.33it/s]Training epoch 1:  49% 6738/13868 [1:24:16<1:30:44,  1.31it/s]Training epoch 1:  49% 6739/13868 [1:24:16<1:31:03,  1.30it/s]Training epoch 1:  49% 6740/13868 [1:24:17<1:30:55,  1.31it/s]Training epoch 1:  49% 6741/13868 [1:24:18<1:30:53,  1.31it/s]Training epoch 1:  49% 6742/13868 [1:24:19<1:30:48,  1.31it/s]Training epoch 1:  49% 6743/13868 [1:24:19<1:29:51,  1.32it/s]Training epoch 1:  49% 6744/13868 [1:24:20<1:29:52,  1.32it/s]Training epoch 1:  49% 6745/13868 [1:24:21<1:29:05,  1.33it/s]Training epoch 1:  49% 6746/13868 [1:24:22<1:29:16,  1.33it/s]Training epoch 1:  49% 6747/13868 [1:24:22<1:29:32,  1.33it/s]Training epoch 1:  49% 6748/13868 [1:24:23<1:28:50,  1.34it/s]Training epoch 1:  49% 6749/13868 [1:24:24<1:29:20,  1.33it/s]Training epoch 1:  49% 6750/13868 [1:24:25<1:30:08,  1.32it/s]Training epoch 1:  49% 6751/13868 [1:24:25<1:29:24,  1.33it/s]Training epoch 1:  49% 6752/13868 [1:24:26<1:28:20,  1.34it/s]Training epoch 1:  49% 6753/13868 [1:24:27<1:28:47,  1.34it/s]Training epoch 1:  49% 6754/13868 [1:24:28<1:28:12,  1.34it/s]Training epoch 1:  49% 6755/13868 [1:24:28<1:27:04,  1.36it/s]Training epoch 1:  49% 6756/13868 [1:24:29<1:28:36,  1.34it/s]Training epoch 1:  49% 6757/13868 [1:24:30<1:28:49,  1.33it/s]Training epoch 1:  49% 6758/13868 [1:24:31<1:29:52,  1.32it/s]Training epoch 1:  49% 6759/13868 [1:24:31<1:29:50,  1.32it/s]Training epoch 1:  49% 6760/13868 [1:24:32<1:29:46,  1.32it/s]Training epoch 1:  49% 6761/13868 [1:24:33<1:28:52,  1.33it/s]Training epoch 1:  49% 6762/13868 [1:24:34<1:28:21,  1.34it/s]Training epoch 1:  49% 6763/13868 [1:24:34<1:27:29,  1.35it/s]Training epoch 1:  49% 6764/13868 [1:24:35<1:29:26,  1.32it/s]Training epoch 1:  49% 6765/13868 [1:24:36<1:30:22,  1.31it/s]Training epoch 1:  49% 6766/13868 [1:24:37<1:29:30,  1.32it/s]Training epoch 1:  49% 6767/13868 [1:24:37<1:28:19,  1.34it/s]Training epoch 1:  49% 6768/13868 [1:24:38<1:28:03,  1.34it/s]Training epoch 1:  49% 6769/13868 [1:24:39<1:27:53,  1.35it/s]Training epoch 1:  49% 6770/13868 [1:24:40<1:28:12,  1.34it/s]Training epoch 1:  49% 6771/13868 [1:24:40<1:28:53,  1.33it/s]Training epoch 1:  49% 6772/13868 [1:24:41<1:29:36,  1.32it/s]Training epoch 1:  49% 6773/13868 [1:24:42<1:29:39,  1.32it/s]Training epoch 1:  49% 6774/13868 [1:24:43<1:29:29,  1.32it/s]Training epoch 1:  49% 6775/13868 [1:24:43<1:28:31,  1.34it/s]Training epoch 1:  49% 6776/13868 [1:24:44<1:28:14,  1.34it/s]Training epoch 1:  49% 6777/13868 [1:24:45<1:27:21,  1.35it/s]Training epoch 1:  49% 6778/13868 [1:24:46<1:28:05,  1.34it/s]Training epoch 1:  49% 6779/13868 [1:24:46<1:29:08,  1.33it/s]Training epoch 1:  49% 6780/13868 [1:24:47<1:30:14,  1.31it/s]Training epoch 1:  49% 6781/13868 [1:24:48<1:29:22,  1.32it/s]Training epoch 1:  49% 6782/13868 [1:24:49<1:28:19,  1.34it/s]Training epoch 1:  49% 6783/13868 [1:24:49<1:28:15,  1.34it/s]Training epoch 1:  49% 6784/13868 [1:24:50<1:29:06,  1.33it/s]Training epoch 1:  49% 6785/13868 [1:24:51<1:28:57,  1.33it/s]Training epoch 1:  49% 6786/13868 [1:24:52<1:29:16,  1.32it/s]Training epoch 1:  49% 6787/13868 [1:24:52<1:28:53,  1.33it/s]Training epoch 1:  49% 6788/13868 [1:24:53<1:29:20,  1.32it/s]Training epoch 1:  49% 6789/13868 [1:24:54<1:29:41,  1.32it/s]Training epoch 1:  49% 6790/13868 [1:24:55<1:28:30,  1.33it/s]Training epoch 1:  49% 6791/13868 [1:24:55<1:27:20,  1.35it/s]Training epoch 1:  49% 6792/13868 [1:24:56<1:29:08,  1.32it/s]Training epoch 1:  49% 6793/13868 [1:24:57<1:28:34,  1.33it/s]Training epoch 1:  49% 6794/13868 [1:24:58<1:29:17,  1.32it/s]Training epoch 1:  49% 6795/13868 [1:24:58<1:29:38,  1.32it/s]Training epoch 1:  49% 6796/13868 [1:24:59<1:28:49,  1.33it/s]Training epoch 1:  49% 6797/13868 [1:25:00<1:27:20,  1.35it/s]Training epoch 1:  49% 6798/13868 [1:25:01<1:28:09,  1.34it/s]Training epoch 1:  49% 6799/13868 [1:25:01<1:27:03,  1.35it/s]Training epoch 1:  49% 6800/13868 [1:25:02<1:32:57,  1.27it/s]Training epoch 1:  49% 6801/13868 [1:25:03<1:32:39,  1.27it/s]Training epoch 1:  49% 6802/13868 [1:25:04<1:31:32,  1.29it/s]Training epoch 1:  49% 6803/13868 [1:25:05<1:30:37,  1.30it/s]Training epoch 1:  49% 6804/13868 [1:25:05<1:29:39,  1.31it/s]Training epoch 1:  49% 6805/13868 [1:25:06<1:28:05,  1.34it/s]Training epoch 1:  49% 6806/13868 [1:25:07<1:27:55,  1.34it/s]Training epoch 1:  49% 6807/13868 [1:25:08<1:29:49,  1.31it/s]Training epoch 1:  49% 6808/13868 [1:25:08<1:28:59,  1.32it/s]Training epoch 1:  49% 6809/13868 [1:25:09<1:28:57,  1.32it/s]Training epoch 1:  49% 6810/13868 [1:25:10<1:28:52,  1.32it/s]Training epoch 1:  49% 6811/13868 [1:25:11<1:28:51,  1.32it/s]Training epoch 1:  49% 6812/13868 [1:25:11<1:28:27,  1.33it/s]Training epoch 1:  49% 6813/13868 [1:25:12<1:29:06,  1.32it/s]Training epoch 1:  49% 6814/13868 [1:25:13<1:28:44,  1.32it/s]Training epoch 1:  49% 6815/13868 [1:25:14<1:27:42,  1.34it/s]Training epoch 1:  49% 6816/13868 [1:25:14<1:27:53,  1.34it/s]Training epoch 1:  49% 6817/13868 [1:25:15<1:27:49,  1.34it/s]Training epoch 1:  49% 6818/13868 [1:25:16<1:27:38,  1.34it/s]Training epoch 1:  49% 6819/13868 [1:25:17<1:28:04,  1.33it/s]Training epoch 1:  49% 6820/13868 [1:25:17<1:29:02,  1.32it/s]Training epoch 1:  49% 6821/13868 [1:25:18<1:29:52,  1.31it/s]Training epoch 1:  49% 6822/13868 [1:25:19<1:29:42,  1.31it/s]Training epoch 1:  49% 6823/13868 [1:25:20<1:28:22,  1.33it/s]Training epoch 1:  49% 6824/13868 [1:25:20<1:29:35,  1.31it/s]Training epoch 1:  49% 6825/13868 [1:25:21<1:31:17,  1.29it/s]Training epoch 1:  49% 6826/13868 [1:25:22<1:31:21,  1.28it/s]Training epoch 1:  49% 6827/13868 [1:25:23<1:31:19,  1.29it/s]Training epoch 1:  49% 6828/13868 [1:25:24<1:31:26,  1.28it/s]Training epoch 1:  49% 6829/13868 [1:25:24<1:29:43,  1.31it/s]Training epoch 1:  49% 6830/13868 [1:25:25<1:28:56,  1.32it/s]Training epoch 1:  49% 6831/13868 [1:25:26<1:29:01,  1.32it/s]Training epoch 1:  49% 6832/13868 [1:25:26<1:28:43,  1.32it/s]Training epoch 1:  49% 6833/13868 [1:25:27<1:28:29,  1.33it/s]Training epoch 1:  49% 6834/13868 [1:25:28<1:30:36,  1.29it/s]Training epoch 1:  49% 6835/13868 [1:25:29<1:30:01,  1.30it/s]Training epoch 1:  49% 6836/13868 [1:25:30<1:29:54,  1.30it/s]Training epoch 1:  49% 6837/13868 [1:25:30<1:28:11,  1.33it/s]Training epoch 1:  49% 6838/13868 [1:25:31<1:27:25,  1.34it/s]Training epoch 1:  49% 6839/13868 [1:25:32<1:26:48,  1.35it/s]Training epoch 1:  49% 6840/13868 [1:25:33<1:28:38,  1.32it/s]Training epoch 1:  49% 6841/13868 [1:25:33<1:27:51,  1.33it/s]Training epoch 1:  49% 6842/13868 [1:25:34<1:28:25,  1.32it/s]Training epoch 1:  49% 6843/13868 [1:25:35<1:28:56,  1.32it/s]Training epoch 1:  49% 6844/13868 [1:25:36<1:29:05,  1.31it/s]Training epoch 1:  49% 6845/13868 [1:25:36<1:26:08,  1.36it/s]Training epoch 1:  49% 6846/13868 [1:25:37<1:28:16,  1.33it/s]Training epoch 1:  49% 6847/13868 [1:25:38<1:27:13,  1.34it/s]Training epoch 1:  49% 6848/13868 [1:25:39<1:26:45,  1.35it/s]Training epoch 1:  49% 6849/13868 [1:25:39<1:27:18,  1.34it/s]Training epoch 1:  49% 6850/13868 [1:25:40<1:27:54,  1.33it/s]Training epoch 1:  49% 6851/13868 [1:25:41<1:28:37,  1.32it/s]Training epoch 1:  49% 6852/13868 [1:25:42<1:29:03,  1.31it/s]Training epoch 1:  49% 6853/13868 [1:25:42<1:28:20,  1.32it/s]Training epoch 1:  49% 6854/13868 [1:25:43<1:28:33,  1.32it/s]Training epoch 1:  49% 6855/13868 [1:25:44<1:27:17,  1.34it/s]Training epoch 1:  49% 6856/13868 [1:25:45<1:27:43,  1.33it/s]Training epoch 1:  49% 6857/13868 [1:25:45<1:26:47,  1.35it/s]Training epoch 1:  49% 6858/13868 [1:25:46<1:27:10,  1.34it/s]Training epoch 1:  49% 6859/13868 [1:25:47<1:27:37,  1.33it/s]Training epoch 1:  49% 6860/13868 [1:25:48<1:28:45,  1.32it/s]Training epoch 1:  49% 6861/13868 [1:25:48<1:27:26,  1.34it/s]Training epoch 1:  49% 6862/13868 [1:25:49<1:28:20,  1.32it/s]Training epoch 1:  49% 6863/13868 [1:25:50<1:26:33,  1.35it/s]Training epoch 1:  49% 6864/13868 [1:25:51<1:26:43,  1.35it/s]Training epoch 1:  50% 6865/13868 [1:25:51<1:26:22,  1.35it/s]Training epoch 1:  50% 6866/13868 [1:25:52<1:28:07,  1.32it/s]Training epoch 1:  50% 6867/13868 [1:25:53<1:28:33,  1.32it/s]Training epoch 1:  50% 6868/13868 [1:25:54<1:27:48,  1.33it/s]Training epoch 1:  50% 6869/13868 [1:25:54<1:26:33,  1.35it/s]Training epoch 1:  50% 6870/13868 [1:25:55<1:27:35,  1.33it/s]Training epoch 1:  50% 6871/13868 [1:25:56<1:27:13,  1.34it/s]Training epoch 1:  50% 6872/13868 [1:25:57<1:27:19,  1.34it/s]Training epoch 1:  50% 6873/13868 [1:25:57<1:26:39,  1.35it/s]Training epoch 1:  50% 6874/13868 [1:25:58<1:26:44,  1.34it/s]Training epoch 1:  50% 6875/13868 [1:25:59<1:27:12,  1.34it/s]Training epoch 1:  50% 6876/13868 [1:26:00<1:27:49,  1.33it/s]Training epoch 1:  50% 6877/13868 [1:26:00<1:27:58,  1.32it/s]Training epoch 1:  50% 6878/13868 [1:26:01<1:28:42,  1.31it/s]Training epoch 1:  50% 6879/13868 [1:26:02<1:28:39,  1.31it/s]Training epoch 1:  50% 6880/13868 [1:26:03<1:28:25,  1.32it/s]Training epoch 1:  50% 6881/13868 [1:26:03<1:28:01,  1.32it/s]Training epoch 1:  50% 6882/13868 [1:26:04<1:27:18,  1.33it/s]Training epoch 1:  50% 6883/13868 [1:26:05<1:28:19,  1.32it/s]Training epoch 1:  50% 6884/13868 [1:26:06<1:26:57,  1.34it/s]Training epoch 1:  50% 6885/13868 [1:26:06<1:27:42,  1.33it/s]Training epoch 1:  50% 6886/13868 [1:26:07<1:28:20,  1.32it/s]Training epoch 1:  50% 6887/13868 [1:26:08<1:27:51,  1.32it/s]Training epoch 1:  50% 6888/13868 [1:26:09<1:27:41,  1.33it/s]Training epoch 1:  50% 6889/13868 [1:26:09<1:26:41,  1.34it/s]Training epoch 1:  50% 6890/13868 [1:26:10<1:28:10,  1.32it/s]Training epoch 1:  50% 6891/13868 [1:26:11<1:28:55,  1.31it/s]Training epoch 1:  50% 6892/13868 [1:26:12<1:29:10,  1.30it/s]Training epoch 1:  50% 6893/13868 [1:26:12<1:28:13,  1.32it/s]Training epoch 1:  50% 6894/13868 [1:26:13<1:28:18,  1.32it/s]Training epoch 1:  50% 6895/13868 [1:26:14<1:28:43,  1.31it/s]Training epoch 1:  50% 6896/13868 [1:26:15<1:28:45,  1.31it/s]Training epoch 1:  50% 6897/13868 [1:26:15<1:26:59,  1.34it/s]Training epoch 1:  50% 6898/13868 [1:26:16<1:27:15,  1.33it/s]Training epoch 1:  50% 6899/13868 [1:26:17<1:27:06,  1.33it/s]Training epoch 1:  50% 6900/13868 [1:26:18<1:32:10,  1.26it/s]Training epoch 1:  50% 6901/13868 [1:26:19<1:30:52,  1.28it/s]Training epoch 1:  50% 6902/13868 [1:26:19<1:29:28,  1.30it/s]Training epoch 1:  50% 6903/13868 [1:26:20<1:29:15,  1.30it/s]Training epoch 1:  50% 6904/13868 [1:26:21<1:29:42,  1.29it/s]Training epoch 1:  50% 6905/13868 [1:26:22<1:27:48,  1.32it/s]Training epoch 1:  50% 6906/13868 [1:26:22<1:29:14,  1.30it/s]Training epoch 1:  50% 6907/13868 [1:26:23<1:29:15,  1.30it/s]Training epoch 1:  50% 6908/13868 [1:26:24<1:29:36,  1.29it/s]Training epoch 1:  50% 6909/13868 [1:26:25<1:29:26,  1.30it/s]Training epoch 1:  50% 6910/13868 [1:26:26<1:30:08,  1.29it/s]Training epoch 1:  50% 6911/13868 [1:26:26<1:29:09,  1.30it/s]Training epoch 1:  50% 6912/13868 [1:26:27<1:29:21,  1.30it/s]Training epoch 1:  50% 6913/13868 [1:26:28<1:28:53,  1.30it/s]Training epoch 1:  50% 6914/13868 [1:26:29<1:28:19,  1.31it/s]Training epoch 1:  50% 6915/13868 [1:26:29<1:27:25,  1.33it/s]Training epoch 1:  50% 6916/13868 [1:26:30<1:28:16,  1.31it/s]Training epoch 1:  50% 6917/13868 [1:26:31<1:26:58,  1.33it/s]Training epoch 1:  50% 6918/13868 [1:26:32<1:26:35,  1.34it/s]Training epoch 1:  50% 6919/13868 [1:26:32<1:25:20,  1.36it/s]Training epoch 1:  50% 6920/13868 [1:26:33<1:25:07,  1.36it/s]Training epoch 1:  50% 6921/13868 [1:26:34<1:25:47,  1.35it/s]Training epoch 1:  50% 6922/13868 [1:26:35<1:27:01,  1.33it/s]Training epoch 1:  50% 6923/13868 [1:26:35<1:27:15,  1.33it/s]Training epoch 1:  50% 6924/13868 [1:26:36<1:26:43,  1.33it/s]Training epoch 1:  50% 6925/13868 [1:26:37<1:26:28,  1.34it/s]Training epoch 1:  50% 6926/13868 [1:26:38<1:26:59,  1.33it/s]Training epoch 1:  50% 6927/13868 [1:26:38<1:27:29,  1.32it/s]Training epoch 1:  50% 6928/13868 [1:26:39<1:28:22,  1.31it/s]Training epoch 1:  50% 6929/13868 [1:26:40<1:28:27,  1.31it/s]Training epoch 1:  50% 6930/13868 [1:26:41<1:28:50,  1.30it/s]Training epoch 1:  50% 6931/13868 [1:26:41<1:29:33,  1.29it/s]Training epoch 1:  50% 6932/13868 [1:26:42<1:28:59,  1.30it/s]Training epoch 1:  50% 6933/13868 [1:26:43<1:29:03,  1.30it/s]Training epoch 1:  50% 6934/13868 [1:26:44<1:29:17,  1.29it/s]Training epoch 1:  50% 6935/13868 [1:26:44<1:28:49,  1.30it/s]Training epoch 1:  50% 6936/13868 [1:26:45<1:27:47,  1.32it/s]Training epoch 1:  50% 6937/13868 [1:26:46<1:28:21,  1.31it/s]Training epoch 1:  50% 6938/13868 [1:26:47<1:28:03,  1.31it/s]Training epoch 1:  50% 6939/13868 [1:26:47<1:27:41,  1.32it/s]Training epoch 1:  50% 6940/13868 [1:26:48<1:27:15,  1.32it/s]Training epoch 1:  50% 6941/13868 [1:26:49<1:26:33,  1.33it/s]Training epoch 1:  50% 6942/13868 [1:26:50<1:27:05,  1.33it/s]Training epoch 1:  50% 6943/13868 [1:26:50<1:27:25,  1.32it/s]Training epoch 1:  50% 6944/13868 [1:26:51<1:27:39,  1.32it/s]Training epoch 1:  50% 6945/13868 [1:26:52<1:26:10,  1.34it/s]Training epoch 1:  50% 6946/13868 [1:26:53<1:26:20,  1.34it/s]Training epoch 1:  50% 6947/13868 [1:26:53<1:25:42,  1.35it/s]Training epoch 1:  50% 6948/13868 [1:26:54<1:27:51,  1.31it/s]Training epoch 1:  50% 6949/13868 [1:26:55<1:28:24,  1.30it/s]Training epoch 1:  50% 6950/13868 [1:26:56<1:28:20,  1.31it/s]Training epoch 1:  50% 6951/13868 [1:26:57<1:27:40,  1.31it/s]Training epoch 1:  50% 6952/13868 [1:26:57<1:29:18,  1.29it/s]Training epoch 1:  50% 6953/13868 [1:26:58<1:27:41,  1.31it/s]Training epoch 1:  50% 6954/13868 [1:26:59<1:27:59,  1.31it/s]Training epoch 1:  50% 6955/13868 [1:27:00<1:27:22,  1.32it/s]Training epoch 1:  50% 6956/13868 [1:27:00<1:27:28,  1.32it/s]Training epoch 1:  50% 6957/13868 [1:27:01<1:28:37,  1.30it/s]Training epoch 1:  50% 6958/13868 [1:27:02<1:28:40,  1.30it/s]Training epoch 1:  50% 6959/13868 [1:27:03<1:26:40,  1.33it/s]Training epoch 1:  50% 6960/13868 [1:27:03<1:26:32,  1.33it/s]Training epoch 1:  50% 6961/13868 [1:27:04<1:26:28,  1.33it/s]Training epoch 1:  50% 6962/13868 [1:27:05<1:26:13,  1.33it/s]Training epoch 1:  50% 6963/13868 [1:27:06<1:25:37,  1.34it/s]Training epoch 1:  50% 6964/13868 [1:27:06<1:25:44,  1.34it/s]Training epoch 1:  50% 6965/13868 [1:27:07<1:25:29,  1.35it/s]Training epoch 1:  50% 6966/13868 [1:27:08<1:27:11,  1.32it/s]Training epoch 1:  50% 6967/13868 [1:27:09<1:27:12,  1.32it/s]Training epoch 1:  50% 6968/13868 [1:27:09<1:27:32,  1.31it/s]Training epoch 1:  50% 6969/13868 [1:27:10<1:27:43,  1.31it/s]Training epoch 1:  50% 6970/13868 [1:27:11<1:28:48,  1.29it/s]Training epoch 1:  50% 6971/13868 [1:27:12<1:27:47,  1.31it/s]Training epoch 1:  50% 6972/13868 [1:27:13<1:28:39,  1.30it/s]Training epoch 1:  50% 6973/13868 [1:27:13<1:27:18,  1.32it/s]Training epoch 1:  50% 6974/13868 [1:27:14<1:27:13,  1.32it/s]Training epoch 1:  50% 6975/13868 [1:27:15<1:26:57,  1.32it/s]Training epoch 1:  50% 6976/13868 [1:27:15<1:26:00,  1.34it/s]Training epoch 1:  50% 6977/13868 [1:27:16<1:26:51,  1.32it/s]Training epoch 1:  50% 6978/13868 [1:27:17<1:26:16,  1.33it/s]Training epoch 1:  50% 6979/13868 [1:27:18<1:25:12,  1.35it/s]Training epoch 1:  50% 6980/13868 [1:27:18<1:25:44,  1.34it/s]Training epoch 1:  50% 6981/13868 [1:27:19<1:25:53,  1.34it/s]Training epoch 1:  50% 6982/13868 [1:27:20<1:26:46,  1.32it/s]Training epoch 1:  50% 6983/13868 [1:27:21<1:26:20,  1.33it/s]Training epoch 1:  50% 6984/13868 [1:27:22<1:27:35,  1.31it/s]Training epoch 1:  50% 6985/13868 [1:27:22<1:27:32,  1.31it/s]Training epoch 1:  50% 6986/13868 [1:27:23<1:26:56,  1.32it/s]Training epoch 1:  50% 6987/13868 [1:27:24<1:28:04,  1.30it/s]Training epoch 1:  50% 6988/13868 [1:27:25<1:27:11,  1.32it/s]Training epoch 1:  50% 6989/13868 [1:27:25<1:26:50,  1.32it/s]Training epoch 1:  50% 6990/13868 [1:27:26<1:27:23,  1.31it/s]Training epoch 1:  50% 6991/13868 [1:27:27<1:27:20,  1.31it/s]Training epoch 1:  50% 6992/13868 [1:27:28<1:27:02,  1.32it/s]Training epoch 1:  50% 6993/13868 [1:27:28<1:26:03,  1.33it/s]Training epoch 1:  50% 6994/13868 [1:27:29<1:25:41,  1.34it/s]Training epoch 1:  50% 6995/13868 [1:27:30<1:27:06,  1.32it/s]Training epoch 1:  50% 6996/13868 [1:27:31<1:27:10,  1.31it/s]Training epoch 1:  50% 6997/13868 [1:27:31<1:26:37,  1.32it/s]Training epoch 1:  50% 6998/13868 [1:27:32<1:26:56,  1.32it/s]Training epoch 1:  50% 6999/13868 [1:27:33<1:27:03,  1.32it/s]Training epoch 1:  50% 7000/13868 [1:27:34<1:31:30,  1.25it/s]Training epoch 1:  50% 7001/13868 [1:27:35<1:29:23,  1.28it/s]Training epoch 1:  50% 7002/13868 [1:27:35<1:29:06,  1.28it/s]Training epoch 1:  50% 7003/13868 [1:27:36<1:28:00,  1.30it/s]Training epoch 1:  51% 7004/13868 [1:27:37<1:29:14,  1.28it/s]Training epoch 1:  51% 7005/13868 [1:27:38<1:28:40,  1.29it/s]Training epoch 1:  51% 7006/13868 [1:27:38<1:28:12,  1.30it/s]Training epoch 1:  51% 7007/13868 [1:27:39<1:28:54,  1.29it/s]Training epoch 1:  51% 7008/13868 [1:27:40<1:28:16,  1.30it/s]Training epoch 1:  51% 7009/13868 [1:27:41<1:27:02,  1.31it/s]Training epoch 1:  51% 7010/13868 [1:27:41<1:27:34,  1.31it/s]Training epoch 1:  51% 7011/13868 [1:27:42<1:27:21,  1.31it/s]Training epoch 1:  51% 7012/13868 [1:27:43<1:27:56,  1.30it/s]Training epoch 1:  51% 7013/13868 [1:27:44<1:27:40,  1.30it/s]Training epoch 1:  51% 7014/13868 [1:27:45<1:27:07,  1.31it/s]Training epoch 1:  51% 7015/13868 [1:27:45<1:27:27,  1.31it/s]Training epoch 1:  51% 7016/13868 [1:27:46<1:26:53,  1.31it/s]Training epoch 1:  51% 7017/13868 [1:27:47<1:27:18,  1.31it/s]Training epoch 1:  51% 7018/13868 [1:27:48<1:28:23,  1.29it/s]Training epoch 1:  51% 7019/13868 [1:27:48<1:27:38,  1.30it/s]Training epoch 1:  51% 7020/13868 [1:27:49<1:28:15,  1.29it/s]Training epoch 1:  51% 7021/13868 [1:27:50<1:26:41,  1.32it/s]Training epoch 1:  51% 7022/13868 [1:27:51<1:27:23,  1.31it/s]Training epoch 1:  51% 7023/13868 [1:27:51<1:26:51,  1.31it/s]Training epoch 1:  51% 7024/13868 [1:27:52<1:27:44,  1.30it/s]Training epoch 1:  51% 7025/13868 [1:27:53<1:28:33,  1.29it/s]Training epoch 1:  51% 7026/13868 [1:27:54<1:28:22,  1.29it/s]Training epoch 1:  51% 7027/13868 [1:27:55<1:27:24,  1.30it/s]Training epoch 1:  51% 7028/13868 [1:27:55<1:28:01,  1.30it/s]Training epoch 1:  51% 7029/13868 [1:27:56<1:26:16,  1.32it/s]Training epoch 1:  51% 7030/13868 [1:27:57<1:26:26,  1.32it/s]Training epoch 1:  51% 7031/13868 [1:27:58<1:26:11,  1.32it/s]Training epoch 1:  51% 7032/13868 [1:27:58<1:26:14,  1.32it/s]Training epoch 1:  51% 7033/13868 [1:27:59<1:25:38,  1.33it/s]Training epoch 1:  51% 7034/13868 [1:28:00<1:26:16,  1.32it/s]Training epoch 1:  51% 7035/13868 [1:28:01<1:26:49,  1.31it/s]Training epoch 1:  51% 7036/13868 [1:28:01<1:26:42,  1.31it/s]Training epoch 1:  51% 7037/13868 [1:28:02<1:24:52,  1.34it/s]Training epoch 1:  51% 7038/13868 [1:28:03<1:24:55,  1.34it/s]Training epoch 1:  51% 7039/13868 [1:28:04<1:25:49,  1.33it/s]Training epoch 1:  51% 7040/13868 [1:28:04<1:26:17,  1.32it/s]Training epoch 1:  51% 7041/13868 [1:28:05<1:25:02,  1.34it/s]Training epoch 1:  51% 7042/13868 [1:28:06<1:26:45,  1.31it/s]Training epoch 1:  51% 7043/13868 [1:28:07<1:26:22,  1.32it/s]Training epoch 1:  51% 7044/13868 [1:28:07<1:26:00,  1.32it/s]Training epoch 1:  51% 7045/13868 [1:28:08<1:25:45,  1.33it/s]Training epoch 1:  51% 7046/13868 [1:28:09<1:27:32,  1.30it/s]Training epoch 1:  51% 7047/13868 [1:28:10<1:26:57,  1.31it/s]Training epoch 1:  51% 7048/13868 [1:28:10<1:27:25,  1.30it/s]Training epoch 1:  51% 7049/13868 [1:28:11<1:27:01,  1.31it/s]Training epoch 1:  51% 7050/13868 [1:28:12<1:26:58,  1.31it/s]Training epoch 1:  51% 7051/13868 [1:28:13<1:27:59,  1.29it/s]Training epoch 1:  51% 7052/13868 [1:28:14<1:27:19,  1.30it/s]Training epoch 1:  51% 7053/13868 [1:28:14<1:26:14,  1.32it/s]Training epoch 1:  51% 7054/13868 [1:28:15<1:27:28,  1.30it/s]Training epoch 1:  51% 7055/13868 [1:28:16<1:26:11,  1.32it/s]Training epoch 1:  51% 7056/13868 [1:28:17<1:26:49,  1.31it/s]Training epoch 1:  51% 7057/13868 [1:28:17<1:25:45,  1.32it/s]Training epoch 1:  51% 7058/13868 [1:28:18<1:25:33,  1.33it/s]Training epoch 1:  51% 7059/13868 [1:28:19<1:26:20,  1.31it/s]Training epoch 1:  51% 7060/13868 [1:28:20<1:26:16,  1.32it/s]Training epoch 1:  51% 7061/13868 [1:28:20<1:25:51,  1.32it/s]Training epoch 1:  51% 7062/13868 [1:28:21<1:26:23,  1.31it/s]Training epoch 1:  51% 7063/13868 [1:28:22<1:26:52,  1.31it/s]Training epoch 1:  51% 7064/13868 [1:28:23<1:26:02,  1.32it/s]Training epoch 1:  51% 7065/13868 [1:28:23<1:25:08,  1.33it/s]Training epoch 1:  51% 7066/13868 [1:28:24<1:25:19,  1.33it/s]Training epoch 1:  51% 7067/13868 [1:28:25<1:25:11,  1.33it/s]Training epoch 1:  51% 7068/13868 [1:28:26<1:24:41,  1.34it/s]Training epoch 1:  51% 7069/13868 [1:28:26<1:25:19,  1.33it/s]Training epoch 1:  51% 7070/13868 [1:28:27<1:25:27,  1.33it/s]Training epoch 1:  51% 7071/13868 [1:28:28<1:25:40,  1.32it/s]Training epoch 1:  51% 7072/13868 [1:28:29<1:25:58,  1.32it/s]Training epoch 1:  51% 7073/13868 [1:28:29<1:25:34,  1.32it/s]Training epoch 1:  51% 7074/13868 [1:28:30<1:24:33,  1.34it/s]Training epoch 1:  51% 7075/13868 [1:28:31<1:25:21,  1.33it/s]Training epoch 1:  51% 7076/13868 [1:28:32<1:25:59,  1.32it/s]Training epoch 1:  51% 7077/13868 [1:28:32<1:25:25,  1.32it/s]Training epoch 1:  51% 7078/13868 [1:28:33<1:26:21,  1.31it/s]Training epoch 1:  51% 7079/13868 [1:28:34<1:26:47,  1.30it/s]Training epoch 1:  51% 7080/13868 [1:28:35<1:27:47,  1.29it/s]Training epoch 1:  51% 7081/13868 [1:28:36<1:28:04,  1.28it/s]Training epoch 1:  51% 7082/13868 [1:28:36<1:27:52,  1.29it/s]Training epoch 1:  51% 7083/13868 [1:28:37<1:26:19,  1.31it/s]Training epoch 1:  51% 7084/13868 [1:28:38<1:26:56,  1.30it/s]Training epoch 1:  51% 7085/13868 [1:28:39<1:26:54,  1.30it/s]Training epoch 1:  51% 7086/13868 [1:28:39<1:25:26,  1.32it/s]Training epoch 1:  51% 7087/13868 [1:28:40<1:25:07,  1.33it/s]Training epoch 1:  51% 7088/13868 [1:28:41<1:25:05,  1.33it/s]Training epoch 1:  51% 7089/13868 [1:28:42<1:25:29,  1.32it/s]Training epoch 1:  51% 7090/13868 [1:28:42<1:24:41,  1.33it/s]Training epoch 1:  51% 7091/13868 [1:28:43<1:25:42,  1.32it/s]Training epoch 1:  51% 7092/13868 [1:28:44<1:24:24,  1.34it/s]Training epoch 1:  51% 7093/13868 [1:28:45<1:23:54,  1.35it/s]Training epoch 1:  51% 7094/13868 [1:28:45<1:24:39,  1.33it/s]Training epoch 1:  51% 7095/13868 [1:28:46<1:24:16,  1.34it/s]Training epoch 1:  51% 7096/13868 [1:28:47<1:25:09,  1.33it/s]Training epoch 1:  51% 7097/13868 [1:28:48<1:25:57,  1.31it/s]Training epoch 1:  51% 7098/13868 [1:28:48<1:25:48,  1.31it/s]Training epoch 1:  51% 7099/13868 [1:28:49<1:25:34,  1.32it/s]Training epoch 1:  51% 7100/13868 [1:28:50<1:30:43,  1.24it/s]Training epoch 1:  51% 7101/13868 [1:28:51<1:28:55,  1.27it/s]Training epoch 1:  51% 7102/13868 [1:28:52<1:28:39,  1.27it/s]Training epoch 1:  51% 7103/13868 [1:28:52<1:27:15,  1.29it/s]Training epoch 1:  51% 7104/13868 [1:28:53<1:27:29,  1.29it/s]Training epoch 1:  51% 7105/13868 [1:28:54<1:26:39,  1.30it/s]Training epoch 1:  51% 7106/13868 [1:28:55<1:26:19,  1.31it/s]Training epoch 1:  51% 7107/13868 [1:28:55<1:26:08,  1.31it/s]Training epoch 1:  51% 7108/13868 [1:28:56<1:25:20,  1.32it/s]Training epoch 1:  51% 7109/13868 [1:28:57<1:25:50,  1.31it/s]Training epoch 1:  51% 7110/13868 [1:28:58<1:24:33,  1.33it/s]Training epoch 1:  51% 7111/13868 [1:28:58<1:25:25,  1.32it/s]Training epoch 1:  51% 7112/13868 [1:28:59<1:26:34,  1.30it/s]Training epoch 1:  51% 7113/13868 [1:29:00<1:26:09,  1.31it/s]Training epoch 1:  51% 7114/13868 [1:29:01<1:24:01,  1.34it/s]Training epoch 1:  51% 7115/13868 [1:29:01<1:25:02,  1.32it/s]Training epoch 1:  51% 7116/13868 [1:29:02<1:25:56,  1.31it/s]Training epoch 1:  51% 7117/13868 [1:29:03<1:26:20,  1.30it/s]Training epoch 1:  51% 7118/13868 [1:29:04<1:26:19,  1.30it/s]Training epoch 1:  51% 7119/13868 [1:29:04<1:25:53,  1.31it/s]Training epoch 1:  51% 7120/13868 [1:29:05<1:26:23,  1.30it/s]Training epoch 1:  51% 7121/13868 [1:29:06<1:25:04,  1.32it/s]Training epoch 1:  51% 7122/13868 [1:29:07<1:25:55,  1.31it/s]Training epoch 1:  51% 7123/13868 [1:29:08<1:25:09,  1.32it/s]Training epoch 1:  51% 7124/13868 [1:29:08<1:24:21,  1.33it/s]Training epoch 1:  51% 7125/13868 [1:29:09<1:25:24,  1.32it/s]Training epoch 1:  51% 7126/13868 [1:29:10<1:25:57,  1.31it/s]Training epoch 1:  51% 7127/13868 [1:29:11<1:25:57,  1.31it/s]Training epoch 1:  51% 7128/13868 [1:29:11<1:25:44,  1.31it/s]Training epoch 1:  51% 7129/13868 [1:29:12<1:24:38,  1.33it/s]Training epoch 1:  51% 7130/13868 [1:29:13<1:24:36,  1.33it/s]Training epoch 1:  51% 7131/13868 [1:29:14<1:24:44,  1.33it/s]Training epoch 1:  51% 7132/13868 [1:29:14<1:24:13,  1.33it/s]Training epoch 1:  51% 7133/13868 [1:29:15<1:24:33,  1.33it/s]Training epoch 1:  51% 7134/13868 [1:29:16<1:25:24,  1.31it/s]Training epoch 1:  51% 7135/13868 [1:29:17<1:24:44,  1.32it/s]Training epoch 1:  51% 7136/13868 [1:29:17<1:25:03,  1.32it/s]Training epoch 1:  51% 7137/13868 [1:29:18<1:24:41,  1.32it/s]Training epoch 1:  51% 7138/13868 [1:29:19<1:25:00,  1.32it/s]Training epoch 1:  51% 7139/13868 [1:29:20<1:25:52,  1.31it/s]Training epoch 1:  51% 7140/13868 [1:29:20<1:25:43,  1.31it/s]Training epoch 1:  51% 7141/13868 [1:29:21<1:24:59,  1.32it/s]Training epoch 1:  51% 7142/13868 [1:29:22<1:25:49,  1.31it/s]Training epoch 1:  52% 7143/13868 [1:29:23<1:26:45,  1.29it/s]Training epoch 1:  52% 7144/13868 [1:29:23<1:24:57,  1.32it/s]Training epoch 1:  52% 7145/13868 [1:29:24<1:26:19,  1.30it/s]Training epoch 1:  52% 7146/13868 [1:29:25<1:27:48,  1.28it/s]Training epoch 1:  52% 7147/13868 [1:29:26<1:26:24,  1.30it/s]Training epoch 1:  52% 7148/13868 [1:29:27<1:24:25,  1.33it/s]Training epoch 1:  52% 7149/13868 [1:29:27<1:25:02,  1.32it/s]Training epoch 1:  52% 7150/13868 [1:29:28<1:25:43,  1.31it/s]Training epoch 1:  52% 7151/13868 [1:29:29<1:25:49,  1.30it/s]Training epoch 1:  52% 7152/13868 [1:29:30<1:25:43,  1.31it/s]Training epoch 1:  52% 7153/13868 [1:29:30<1:24:57,  1.32it/s]Training epoch 1:  52% 7154/13868 [1:29:31<1:24:49,  1.32it/s]Training epoch 1:  52% 7155/13868 [1:29:32<1:24:39,  1.32it/s]Training epoch 1:  52% 7156/13868 [1:29:33<1:25:31,  1.31it/s]Training epoch 1:  52% 7157/13868 [1:29:33<1:25:36,  1.31it/s]Training epoch 1:  52% 7158/13868 [1:29:34<1:25:42,  1.30it/s]Training epoch 1:  52% 7159/13868 [1:29:35<1:25:51,  1.30it/s]Training epoch 1:  52% 7160/13868 [1:29:36<1:26:08,  1.30it/s]Training epoch 1:  52% 7161/13868 [1:29:36<1:24:55,  1.32it/s]Training epoch 1:  52% 7162/13868 [1:29:37<1:24:45,  1.32it/s]Training epoch 1:  52% 7163/13868 [1:29:38<1:24:24,  1.32it/s]Training epoch 1:  52% 7164/13868 [1:29:39<1:24:01,  1.33it/s]Training epoch 1:  52% 7165/13868 [1:29:39<1:22:59,  1.35it/s]Training epoch 1:  52% 7166/13868 [1:29:40<1:23:07,  1.34it/s]Training epoch 1:  52% 7167/13868 [1:29:41<1:23:06,  1.34it/s]Training epoch 1:  52% 7168/13868 [1:29:42<1:22:00,  1.36it/s]Training epoch 1:  52% 7169/13868 [1:29:42<1:21:05,  1.38it/s]Training epoch 1:  52% 7170/13868 [1:29:43<1:21:22,  1.37it/s]Training epoch 1:  52% 7171/13868 [1:29:44<1:21:57,  1.36it/s]Training epoch 1:  52% 7172/13868 [1:29:45<1:23:50,  1.33it/s]Training epoch 1:  52% 7173/13868 [1:29:45<1:24:41,  1.32it/s]Training epoch 1:  52% 7174/13868 [1:29:46<1:24:17,  1.32it/s]Training epoch 1:  52% 7175/13868 [1:29:47<1:24:54,  1.31it/s]Training epoch 1:  52% 7176/13868 [1:29:48<1:24:58,  1.31it/s]Training epoch 1:  52% 7177/13868 [1:29:48<1:23:42,  1.33it/s]Training epoch 1:  52% 7178/13868 [1:29:49<1:23:34,  1.33it/s]Training epoch 1:  52% 7179/13868 [1:29:50<1:24:08,  1.33it/s]Training epoch 1:  52% 7180/13868 [1:29:51<1:24:30,  1.32it/s]Training epoch 1:  52% 7181/13868 [1:29:51<1:23:33,  1.33it/s]Training epoch 1:  52% 7182/13868 [1:29:52<1:23:45,  1.33it/s]Training epoch 1:  52% 7183/13868 [1:29:53<1:24:49,  1.31it/s]Training epoch 1:  52% 7184/13868 [1:29:54<1:24:07,  1.32it/s]Training epoch 1:  52% 7185/13868 [1:29:54<1:25:04,  1.31it/s]Training epoch 1:  52% 7186/13868 [1:29:55<1:24:07,  1.32it/s]Training epoch 1:  52% 7187/13868 [1:29:56<1:23:05,  1.34it/s]Training epoch 1:  52% 7188/13868 [1:29:57<1:23:01,  1.34it/s]Training epoch 1:  52% 7189/13868 [1:29:57<1:23:40,  1.33it/s]Training epoch 1:  52% 7190/13868 [1:29:58<1:23:59,  1.33it/s]Training epoch 1:  52% 7191/13868 [1:29:59<1:24:02,  1.32it/s]Training epoch 1:  52% 7192/13868 [1:30:00<1:23:28,  1.33it/s]Training epoch 1:  52% 7193/13868 [1:30:00<1:23:34,  1.33it/s]Training epoch 1:  52% 7194/13868 [1:30:01<1:24:40,  1.31it/s]Training epoch 1:  52% 7195/13868 [1:30:02<1:25:09,  1.31it/s]Training epoch 1:  52% 7196/13868 [1:30:03<1:25:39,  1.30it/s]Training epoch 1:  52% 7197/13868 [1:30:04<1:24:37,  1.31it/s]Training epoch 1:  52% 7198/13868 [1:30:04<1:24:14,  1.32it/s]Training epoch 1:  52% 7199/13868 [1:30:05<1:23:03,  1.34it/s]Training epoch 1:  52% 7200/13868 [1:30:06<1:29:57,  1.24it/s]Training epoch 1:  52% 7201/13868 [1:30:07<1:28:08,  1.26it/s]Training epoch 1:  52% 7202/13868 [1:30:08<1:27:22,  1.27it/s]Training epoch 1:  52% 7203/13868 [1:30:08<1:25:52,  1.29it/s]Training epoch 1:  52% 7204/13868 [1:30:09<1:25:10,  1.30it/s]Training epoch 1:  52% 7205/13868 [1:30:10<1:23:58,  1.32it/s]Training epoch 1:  52% 7206/13868 [1:30:10<1:24:08,  1.32it/s]Training epoch 1:  52% 7207/13868 [1:30:11<1:22:57,  1.34it/s]Training epoch 1:  52% 7208/13868 [1:30:12<1:24:00,  1.32it/s]Training epoch 1:  52% 7209/13868 [1:30:13<1:24:10,  1.32it/s]Training epoch 1:  52% 7210/13868 [1:30:14<1:23:36,  1.33it/s]Training epoch 1:  52% 7211/13868 [1:30:14<1:23:01,  1.34it/s]Training epoch 1:  52% 7212/13868 [1:30:15<1:23:02,  1.34it/s]Training epoch 1:  52% 7213/13868 [1:30:16<1:23:44,  1.32it/s]Training epoch 1:  52% 7214/13868 [1:30:17<1:24:36,  1.31it/s]Training epoch 1:  52% 7215/13868 [1:30:17<1:24:12,  1.32it/s]Training epoch 1:  52% 7216/13868 [1:30:18<1:24:29,  1.31it/s]Training epoch 1:  52% 7217/13868 [1:30:19<1:24:12,  1.32it/s]Training epoch 1:  52% 7218/13868 [1:30:20<1:24:58,  1.30it/s]Training epoch 1:  52% 7219/13868 [1:30:20<1:24:26,  1.31it/s]Training epoch 1:  52% 7220/13868 [1:30:21<1:24:13,  1.32it/s]Training epoch 1:  52% 7221/13868 [1:30:22<1:22:56,  1.34it/s]Training epoch 1:  52% 7222/13868 [1:30:23<1:22:52,  1.34it/s]Training epoch 1:  52% 7223/13868 [1:30:23<1:23:34,  1.33it/s]Training epoch 1:  52% 7224/13868 [1:30:24<1:24:26,  1.31it/s]Training epoch 1:  52% 7225/13868 [1:30:25<1:23:59,  1.32it/s]Training epoch 1:  52% 7226/13868 [1:30:26<1:23:48,  1.32it/s]Training epoch 1:  52% 7227/13868 [1:30:26<1:23:13,  1.33it/s]Training epoch 1:  52% 7228/13868 [1:30:27<1:22:14,  1.35it/s]Training epoch 1:  52% 7229/13868 [1:30:28<1:22:19,  1.34it/s]Training epoch 1:  52% 7230/13868 [1:30:29<1:22:07,  1.35it/s]Training epoch 1:  52% 7231/13868 [1:30:29<1:22:39,  1.34it/s]Training epoch 1:  52% 7232/13868 [1:30:30<1:22:17,  1.34it/s]Training epoch 1:  52% 7233/13868 [1:30:31<1:22:04,  1.35it/s]Training epoch 1:  52% 7234/13868 [1:30:32<1:22:37,  1.34it/s]Training epoch 1:  52% 7235/13868 [1:30:32<1:21:50,  1.35it/s]Training epoch 1:  52% 7236/13868 [1:30:33<1:23:18,  1.33it/s]Training epoch 1:  52% 7237/13868 [1:30:34<1:22:32,  1.34it/s]Training epoch 1:  52% 7238/13868 [1:30:35<1:23:16,  1.33it/s]Training epoch 1:  52% 7239/13868 [1:30:35<1:23:05,  1.33it/s]Training epoch 1:  52% 7240/13868 [1:30:36<1:23:22,  1.32it/s]Training epoch 1:  52% 7241/13868 [1:30:37<1:24:12,  1.31it/s]Training epoch 1:  52% 7242/13868 [1:30:38<1:24:56,  1.30it/s]Training epoch 1:  52% 7243/13868 [1:30:38<1:24:02,  1.31it/s]Training epoch 1:  52% 7244/13868 [1:30:39<1:22:53,  1.33it/s]Training epoch 1:  52% 7245/13868 [1:30:40<1:22:43,  1.33it/s]Training epoch 1:  52% 7246/13868 [1:30:41<1:22:01,  1.35it/s]Training epoch 1:  52% 7247/13868 [1:30:41<1:22:25,  1.34it/s]Training epoch 1:  52% 7248/13868 [1:30:42<1:23:32,  1.32it/s]Training epoch 1:  52% 7249/13868 [1:30:43<1:22:46,  1.33it/s]Training epoch 1:  52% 7250/13868 [1:30:44<1:23:16,  1.32it/s]Training epoch 1:  52% 7251/13868 [1:30:44<1:24:04,  1.31it/s]Training epoch 1:  52% 7252/13868 [1:30:45<1:23:43,  1.32it/s]Training epoch 1:  52% 7253/13868 [1:30:46<1:24:06,  1.31it/s]Training epoch 1:  52% 7254/13868 [1:30:47<1:24:44,  1.30it/s]Training epoch 1:  52% 7255/13868 [1:30:47<1:24:13,  1.31it/s]Training epoch 1:  52% 7256/13868 [1:30:48<1:23:40,  1.32it/s]Training epoch 1:  52% 7257/13868 [1:30:49<1:21:58,  1.34it/s]Training epoch 1:  52% 7258/13868 [1:30:50<1:22:42,  1.33it/s]Training epoch 1:  52% 7259/13868 [1:30:50<1:22:47,  1.33it/s]Training epoch 1:  52% 7260/13868 [1:30:51<1:22:53,  1.33it/s]Training epoch 1:  52% 7261/13868 [1:30:52<1:22:33,  1.33it/s]Training epoch 1:  52% 7262/13868 [1:30:53<1:22:03,  1.34it/s]Training epoch 1:  52% 7263/13868 [1:30:53<1:22:05,  1.34it/s]Training epoch 1:  52% 7264/13868 [1:30:54<1:21:19,  1.35it/s]Training epoch 1:  52% 7265/13868 [1:30:55<1:21:40,  1.35it/s]Training epoch 1:  52% 7266/13868 [1:30:56<1:21:57,  1.34it/s]Training epoch 1:  52% 7267/13868 [1:30:56<1:22:16,  1.34it/s]Training epoch 1:  52% 7268/13868 [1:30:57<1:22:41,  1.33it/s]Training epoch 1:  52% 7269/13868 [1:30:58<1:22:20,  1.34it/s]Training epoch 1:  52% 7270/13868 [1:30:59<1:22:38,  1.33it/s]Training epoch 1:  52% 7271/13868 [1:30:59<1:23:04,  1.32it/s]Training epoch 1:  52% 7272/13868 [1:31:00<1:23:43,  1.31it/s]Training epoch 1:  52% 7273/13868 [1:31:01<1:23:57,  1.31it/s]Training epoch 1:  52% 7274/13868 [1:31:02<1:23:37,  1.31it/s]Training epoch 1:  52% 7275/13868 [1:31:02<1:22:48,  1.33it/s]Training epoch 1:  52% 7276/13868 [1:31:03<1:22:48,  1.33it/s]Training epoch 1:  52% 7277/13868 [1:31:04<1:21:25,  1.35it/s]Training epoch 1:  52% 7278/13868 [1:31:05<1:22:07,  1.34it/s]Training epoch 1:  52% 7279/13868 [1:31:05<1:22:57,  1.32it/s]Training epoch 1:  52% 7280/13868 [1:31:06<1:22:59,  1.32it/s]Training epoch 1:  53% 7281/13868 [1:31:07<1:23:41,  1.31it/s]Training epoch 1:  53% 7282/13868 [1:31:08<1:24:01,  1.31it/s]Training epoch 1:  53% 7283/13868 [1:31:09<1:22:56,  1.32it/s]Training epoch 1:  53% 7284/13868 [1:31:09<1:23:27,  1.31it/s]Training epoch 1:  53% 7285/13868 [1:31:10<1:24:03,  1.31it/s]Training epoch 1:  53% 7286/13868 [1:31:11<1:23:45,  1.31it/s]Training epoch 1:  53% 7287/13868 [1:31:12<1:23:50,  1.31it/s]Training epoch 1:  53% 7288/13868 [1:31:12<1:23:59,  1.31it/s]Training epoch 1:  53% 7289/13868 [1:31:13<1:23:12,  1.32it/s]Training epoch 1:  53% 7290/13868 [1:31:14<1:23:06,  1.32it/s]Training epoch 1:  53% 7291/13868 [1:31:15<1:22:16,  1.33it/s]Training epoch 1:  53% 7292/13868 [1:31:15<1:22:26,  1.33it/s]Training epoch 1:  53% 7293/13868 [1:31:16<1:21:52,  1.34it/s]Training epoch 1:  53% 7294/13868 [1:31:17<1:23:04,  1.32it/s]Training epoch 1:  53% 7295/13868 [1:31:18<1:21:53,  1.34it/s]Training epoch 1:  53% 7296/13868 [1:31:18<1:21:55,  1.34it/s]Training epoch 1:  53% 7297/13868 [1:31:19<1:21:35,  1.34it/s]Training epoch 1:  53% 7298/13868 [1:31:20<1:22:55,  1.32it/s]Training epoch 1:  53% 7299/13868 [1:31:21<1:23:19,  1.31it/s]Training epoch 1:  53% 7300/13868 [1:31:22<1:32:38,  1.18it/s]Training epoch 1:  53% 7301/13868 [1:31:22<1:28:55,  1.23it/s]Training epoch 1:  53% 7302/13868 [1:31:23<1:27:51,  1.25it/s]Training epoch 1:  53% 7303/13868 [1:31:24<1:26:01,  1.27it/s]Training epoch 1:  53% 7304/13868 [1:31:25<1:25:06,  1.29it/s]Training epoch 1:  53% 7305/13868 [1:31:25<1:24:12,  1.30it/s]Training epoch 1:  53% 7306/13868 [1:31:26<1:23:34,  1.31it/s]Training epoch 1:  53% 7307/13868 [1:31:27<1:23:27,  1.31it/s]Training epoch 1:  53% 7308/13868 [1:31:28<1:22:25,  1.33it/s]Training epoch 1:  53% 7309/13868 [1:31:28<1:22:14,  1.33it/s]Training epoch 1:  53% 7310/13868 [1:31:29<1:23:06,  1.32it/s]Training epoch 1:  53% 7311/13868 [1:31:30<1:22:35,  1.32it/s]Training epoch 1:  53% 7312/13868 [1:31:31<1:24:17,  1.30it/s]Training epoch 1:  53% 7313/13868 [1:31:31<1:22:13,  1.33it/s]Training epoch 1:  53% 7314/13868 [1:31:32<1:23:05,  1.31it/s]Training epoch 1:  53% 7315/13868 [1:31:33<1:23:29,  1.31it/s]Training epoch 1:  53% 7316/13868 [1:31:34<1:23:50,  1.30it/s]Training epoch 1:  53% 7317/13868 [1:31:35<1:23:29,  1.31it/s]Training epoch 1:  53% 7318/13868 [1:31:35<1:23:35,  1.31it/s]Training epoch 1:  53% 7319/13868 [1:31:36<1:22:07,  1.33it/s]Training epoch 1:  53% 7320/13868 [1:31:37<1:23:13,  1.31it/s]Training epoch 1:  53% 7321/13868 [1:31:38<1:22:02,  1.33it/s]Training epoch 1:  53% 7322/13868 [1:31:38<1:22:27,  1.32it/s]Training epoch 1:  53% 7323/13868 [1:31:39<1:22:26,  1.32it/s]Training epoch 1:  53% 7324/13868 [1:31:40<1:22:55,  1.32it/s]Training epoch 1:  53% 7325/13868 [1:31:41<1:22:37,  1.32it/s]Training epoch 1:  53% 7326/13868 [1:31:41<1:22:16,  1.33it/s]Training epoch 1:  53% 7327/13868 [1:31:42<1:22:11,  1.33it/s]Training epoch 1:  53% 7328/13868 [1:31:43<1:22:26,  1.32it/s]Training epoch 1:  53% 7329/13868 [1:31:44<1:21:05,  1.34it/s]Training epoch 1:  53% 7330/13868 [1:31:44<1:20:49,  1.35it/s]Training epoch 1:  53% 7331/13868 [1:31:45<1:21:25,  1.34it/s]Training epoch 1:  53% 7332/13868 [1:31:46<1:20:53,  1.35it/s]Training epoch 1:  53% 7333/13868 [1:31:47<1:21:01,  1.34it/s]Training epoch 1:  53% 7334/13868 [1:31:47<1:21:41,  1.33it/s]Training epoch 1:  53% 7335/13868 [1:31:48<1:22:46,  1.32it/s]Training epoch 1:  53% 7336/13868 [1:31:49<1:23:31,  1.30it/s]Training epoch 1:  53% 7337/13868 [1:31:50<1:22:02,  1.33it/s]Training epoch 1:  53% 7338/13868 [1:31:50<1:21:47,  1.33it/s]Training epoch 1:  53% 7339/13868 [1:31:51<1:22:18,  1.32it/s]Training epoch 1:  53% 7340/13868 [1:31:52<1:21:55,  1.33it/s]Training epoch 1:  53% 7341/13868 [1:31:53<1:22:42,  1.32it/s]Training epoch 1:  53% 7342/13868 [1:31:53<1:22:54,  1.31it/s]Training epoch 1:  53% 7343/13868 [1:31:54<1:22:44,  1.31it/s]Training epoch 1:  53% 7344/13868 [1:31:55<1:23:16,  1.31it/s]Training epoch 1:  53% 7345/13868 [1:31:56<1:22:50,  1.31it/s]Training epoch 1:  53% 7346/13868 [1:31:56<1:22:54,  1.31it/s]Training epoch 1:  53% 7347/13868 [1:31:57<1:22:10,  1.32it/s]Training epoch 1:  53% 7348/13868 [1:31:58<1:23:07,  1.31it/s]Training epoch 1:  53% 7349/13868 [1:31:59<1:22:01,  1.32it/s]Training epoch 1:  53% 7350/13868 [1:31:59<1:22:01,  1.32it/s]Training epoch 1:  53% 7351/13868 [1:32:00<1:23:12,  1.31it/s]Training epoch 1:  53% 7352/13868 [1:32:01<1:23:16,  1.30it/s]Training epoch 1:  53% 7353/13868 [1:32:02<1:22:53,  1.31it/s]Training epoch 1:  53% 7354/13868 [1:32:03<1:23:23,  1.30it/s]Training epoch 1:  53% 7355/13868 [1:32:03<1:22:37,  1.31it/s]Training epoch 1:  53% 7356/13868 [1:32:04<1:23:47,  1.30it/s]Training epoch 1:  53% 7357/13868 [1:32:05<1:23:07,  1.31it/s]Training epoch 1:  53% 7358/13868 [1:32:06<1:23:11,  1.30it/s]Training epoch 1:  53% 7359/13868 [1:32:06<1:22:30,  1.31it/s]Training epoch 1:  53% 7360/13868 [1:32:07<1:22:23,  1.32it/s]Training epoch 1:  53% 7361/13868 [1:32:08<1:22:11,  1.32it/s]Training epoch 1:  53% 7362/13868 [1:32:09<1:22:32,  1.31it/s]Training epoch 1:  53% 7363/13868 [1:32:09<1:21:35,  1.33it/s]Training epoch 1:  53% 7364/13868 [1:32:10<1:21:41,  1.33it/s]Training epoch 1:  53% 7365/13868 [1:32:11<1:20:55,  1.34it/s]Training epoch 1:  53% 7366/13868 [1:32:12<1:22:21,  1.32it/s]Training epoch 1:  53% 7367/13868 [1:32:12<1:21:06,  1.34it/s]Training epoch 1:  53% 7368/13868 [1:32:13<1:21:09,  1.33it/s]Training epoch 1:  53% 7369/13868 [1:32:14<1:21:12,  1.33it/s]Training epoch 1:  53% 7370/13868 [1:32:15<1:21:40,  1.33it/s]Training epoch 1:  53% 7371/13868 [1:32:15<1:20:18,  1.35it/s]Training epoch 1:  53% 7372/13868 [1:32:16<1:20:37,  1.34it/s]Training epoch 1:  53% 7373/13868 [1:32:17<1:21:16,  1.33it/s]Training epoch 1:  53% 7374/13868 [1:32:18<1:20:50,  1.34it/s]Training epoch 1:  53% 7375/13868 [1:32:18<1:20:01,  1.35it/s]Training epoch 1:  53% 7376/13868 [1:32:19<1:21:30,  1.33it/s]Training epoch 1:  53% 7377/13868 [1:32:20<1:22:34,  1.31it/s]Training epoch 1:  53% 7378/13868 [1:32:21<1:23:13,  1.30it/s]Training epoch 1:  53% 7379/13868 [1:32:21<1:22:07,  1.32it/s]Training epoch 1:  53% 7380/13868 [1:32:22<1:21:38,  1.32it/s]Training epoch 1:  53% 7381/13868 [1:32:23<1:21:48,  1.32it/s]Training epoch 1:  53% 7382/13868 [1:32:24<1:21:30,  1.33it/s]Training epoch 1:  53% 7383/13868 [1:32:24<1:21:35,  1.32it/s]Training epoch 1:  53% 7384/13868 [1:32:25<1:22:31,  1.31it/s]Training epoch 1:  53% 7385/13868 [1:32:26<1:22:49,  1.30it/s]Training epoch 1:  53% 7386/13868 [1:32:27<1:21:44,  1.32it/s]Training epoch 1:  53% 7387/13868 [1:32:27<1:21:10,  1.33it/s]Training epoch 1:  53% 7388/13868 [1:32:28<1:21:08,  1.33it/s]Training epoch 1:  53% 7389/13868 [1:32:29<1:19:56,  1.35it/s]Training epoch 1:  53% 7390/13868 [1:32:30<1:20:53,  1.33it/s]Training epoch 1:  53% 7391/13868 [1:32:30<1:20:30,  1.34it/s]Training epoch 1:  53% 7392/13868 [1:32:31<1:20:45,  1.34it/s]Training epoch 1:  53% 7393/13868 [1:32:32<1:19:46,  1.35it/s]Training epoch 1:  53% 7394/13868 [1:32:33<1:20:11,  1.35it/s]Training epoch 1:  53% 7395/13868 [1:32:33<1:20:58,  1.33it/s]Training epoch 1:  53% 7396/13868 [1:32:34<1:21:30,  1.32it/s]Training epoch 1:  53% 7397/13868 [1:32:35<1:21:51,  1.32it/s]Training epoch 1:  53% 7398/13868 [1:32:36<1:21:16,  1.33it/s]Training epoch 1:  53% 7399/13868 [1:32:36<1:21:20,  1.33it/s]Training epoch 1:  53% 7400/13868 [1:32:37<1:26:57,  1.24it/s]Training epoch 1:  53% 7401/13868 [1:32:38<1:24:00,  1.28it/s]Training epoch 1:  53% 7402/13868 [1:32:39<1:23:24,  1.29it/s]Training epoch 1:  53% 7403/13868 [1:32:40<1:24:09,  1.28it/s]Training epoch 1:  53% 7404/13868 [1:32:40<1:23:59,  1.28it/s]Training epoch 1:  53% 7405/13868 [1:32:41<1:23:14,  1.29it/s]Training epoch 1:  53% 7406/13868 [1:32:42<1:22:26,  1.31it/s]Training epoch 1:  53% 7407/13868 [1:32:43<1:21:48,  1.32it/s]Training epoch 1:  53% 7408/13868 [1:32:44<1:22:42,  1.30it/s]Training epoch 1:  53% 7409/13868 [1:32:44<1:21:50,  1.32it/s]Training epoch 1:  53% 7410/13868 [1:32:45<1:20:12,  1.34it/s]Training epoch 1:  53% 7411/13868 [1:32:46<1:19:38,  1.35it/s]Training epoch 1:  53% 7412/13868 [1:32:46<1:20:34,  1.34it/s]Training epoch 1:  53% 7413/13868 [1:32:47<1:20:22,  1.34it/s]Training epoch 1:  53% 7414/13868 [1:32:48<1:21:07,  1.33it/s]Training epoch 1:  53% 7415/13868 [1:32:49<1:22:13,  1.31it/s]Training epoch 1:  53% 7416/13868 [1:32:50<1:21:51,  1.31it/s]Training epoch 1:  53% 7417/13868 [1:32:50<1:21:45,  1.32it/s]Training epoch 1:  53% 7418/13868 [1:32:51<1:21:57,  1.31it/s]Training epoch 1:  53% 7419/13868 [1:32:52<1:21:04,  1.33it/s]Training epoch 1:  54% 7420/13868 [1:32:53<1:21:52,  1.31it/s]Training epoch 1:  54% 7421/13868 [1:32:53<1:21:43,  1.31it/s]Training epoch 1:  54% 7422/13868 [1:32:54<1:22:45,  1.30it/s]Training epoch 1:  54% 7423/13868 [1:32:55<1:23:38,  1.28it/s]Training epoch 1:  54% 7424/13868 [1:32:56<1:23:27,  1.29it/s]Training epoch 1:  54% 7425/13868 [1:32:56<1:22:47,  1.30it/s]Training epoch 1:  54% 7426/13868 [1:32:57<1:22:58,  1.29it/s]Training epoch 1:  54% 7427/13868 [1:32:58<1:22:03,  1.31it/s]Training epoch 1:  54% 7428/13868 [1:32:59<1:21:57,  1.31it/s]Training epoch 1:  54% 7429/13868 [1:32:59<1:21:15,  1.32it/s]Training epoch 1:  54% 7430/13868 [1:33:00<1:21:56,  1.31it/s]Training epoch 1:  54% 7431/13868 [1:33:01<1:22:26,  1.30it/s]Training epoch 1:  54% 7432/13868 [1:33:02<1:22:35,  1.30it/s]Training epoch 1:  54% 7433/13868 [1:33:03<1:22:38,  1.30it/s]Training epoch 1:  54% 7434/13868 [1:33:03<1:21:47,  1.31it/s]Training epoch 1:  54% 7435/13868 [1:33:04<1:20:38,  1.33it/s]Training epoch 1:  54% 7436/13868 [1:33:05<1:21:26,  1.32it/s]Training epoch 1:  54% 7437/13868 [1:33:06<1:20:19,  1.33it/s]Training epoch 1:  54% 7438/13868 [1:33:06<1:20:14,  1.34it/s]Training epoch 1:  54% 7439/13868 [1:33:07<1:19:58,  1.34it/s]Training epoch 1:  54% 7440/13868 [1:33:08<1:20:38,  1.33it/s]Training epoch 1:  54% 7441/13868 [1:33:09<1:20:01,  1.34it/s]Training epoch 1:  54% 7442/13868 [1:33:09<1:20:06,  1.34it/s]Training epoch 1:  54% 7443/13868 [1:33:10<1:19:46,  1.34it/s]Training epoch 1:  54% 7444/13868 [1:33:11<1:19:56,  1.34it/s]Training epoch 1:  54% 7445/13868 [1:33:12<1:20:30,  1.33it/s]Training epoch 1:  54% 7446/13868 [1:33:12<1:20:06,  1.34it/s]Training epoch 1:  54% 7447/13868 [1:33:13<1:21:05,  1.32it/s]Training epoch 1:  54% 7448/13868 [1:33:14<1:21:44,  1.31it/s]Training epoch 1:  54% 7449/13868 [1:33:15<1:21:41,  1.31it/s]Training epoch 1:  54% 7450/13868 [1:33:15<1:21:35,  1.31it/s]Training epoch 1:  54% 7451/13868 [1:33:16<1:22:23,  1.30it/s]Training epoch 1:  54% 7452/13868 [1:33:17<1:21:30,  1.31it/s]Training epoch 1:  54% 7453/13868 [1:33:18<1:21:17,  1.32it/s]Training epoch 1:  54% 7454/13868 [1:33:18<1:20:52,  1.32it/s]Training epoch 1:  54% 7455/13868 [1:33:19<1:20:18,  1.33it/s]Training epoch 1:  54% 7456/13868 [1:33:20<1:20:19,  1.33it/s]Training epoch 1:  54% 7457/13868 [1:33:21<1:20:13,  1.33it/s]Training epoch 1:  54% 7458/13868 [1:33:21<1:20:40,  1.32it/s]Training epoch 1:  54% 7459/13868 [1:33:22<1:20:25,  1.33it/s]Training epoch 1:  54% 7460/13868 [1:33:23<1:20:58,  1.32it/s]Training epoch 1:  54% 7461/13868 [1:33:24<1:19:47,  1.34it/s]Training epoch 1:  54% 7462/13868 [1:33:24<1:20:41,  1.32it/s]Training epoch 1:  54% 7463/13868 [1:33:25<1:21:14,  1.31it/s]Training epoch 1:  54% 7464/13868 [1:33:26<1:21:09,  1.32it/s]Training epoch 1:  54% 7465/13868 [1:33:27<1:21:22,  1.31it/s]Training epoch 1:  54% 7466/13868 [1:33:27<1:21:28,  1.31it/s]Training epoch 1:  54% 7467/13868 [1:33:28<1:19:49,  1.34it/s]Training epoch 1:  54% 7468/13868 [1:33:29<1:20:46,  1.32it/s]Training epoch 1:  54% 7469/13868 [1:33:30<1:19:40,  1.34it/s]Training epoch 1:  54% 7470/13868 [1:33:30<1:20:24,  1.33it/s]Training epoch 1:  54% 7471/13868 [1:33:31<1:20:33,  1.32it/s]Training epoch 1:  54% 7472/13868 [1:33:32<1:20:07,  1.33it/s]Training epoch 1:  54% 7473/13868 [1:33:33<1:19:58,  1.33it/s]Training epoch 1:  54% 7474/13868 [1:33:33<1:20:06,  1.33it/s]Training epoch 1:  54% 7475/13868 [1:33:34<1:19:58,  1.33it/s]Training epoch 1:  54% 7476/13868 [1:33:35<1:19:51,  1.33it/s]Training epoch 1:  54% 7477/13868 [1:33:36<1:19:53,  1.33it/s]Training epoch 1:  54% 7478/13868 [1:33:36<1:20:03,  1.33it/s]Training epoch 1:  54% 7479/13868 [1:33:37<1:17:43,  1.37it/s]Training epoch 1:  54% 7480/13868 [1:33:38<1:18:42,  1.35it/s]Training epoch 1:  54% 7481/13868 [1:33:39<1:18:22,  1.36it/s]Training epoch 1:  54% 7482/13868 [1:33:39<1:19:25,  1.34it/s]Training epoch 1:  54% 7483/13868 [1:33:40<1:18:42,  1.35it/s]Training epoch 1:  54% 7484/13868 [1:33:41<1:20:33,  1.32it/s]Training epoch 1:  54% 7485/13868 [1:33:42<1:19:17,  1.34it/s]Training epoch 1:  54% 7486/13868 [1:33:42<1:20:07,  1.33it/s]Training epoch 1:  54% 7487/13868 [1:33:43<1:19:47,  1.33it/s]Training epoch 1:  54% 7488/13868 [1:33:44<1:20:42,  1.32it/s]Training epoch 1:  54% 7489/13868 [1:33:45<1:20:13,  1.33it/s]Training epoch 1:  54% 7490/13868 [1:33:45<1:19:52,  1.33it/s]Training epoch 1:  54% 7491/13868 [1:33:46<1:20:09,  1.33it/s]Training epoch 1:  54% 7492/13868 [1:33:47<1:20:33,  1.32it/s]Training epoch 1:  54% 7493/13868 [1:33:48<1:20:56,  1.31it/s]Training epoch 1:  54% 7494/13868 [1:33:49<1:22:04,  1.29it/s]Training epoch 1:  54% 7495/13868 [1:33:49<1:21:27,  1.30it/s]Training epoch 1:  54% 7496/13868 [1:33:50<1:20:53,  1.31it/s]Training epoch 1:  54% 7497/13868 [1:33:51<1:20:15,  1.32it/s]Training epoch 1:  54% 7498/13868 [1:33:52<1:20:26,  1.32it/s]Training epoch 1:  54% 7499/13868 [1:33:52<1:20:57,  1.31it/s]Training epoch 1:  54% 7500/13868 [1:33:53<1:27:24,  1.21it/s]Training epoch 1:  54% 7501/13868 [1:33:54<1:24:55,  1.25it/s]Training epoch 1:  54% 7502/13868 [1:33:55<1:24:04,  1.26it/s]Training epoch 1:  54% 7503/13868 [1:33:56<1:22:13,  1.29it/s]Training epoch 1:  54% 7504/13868 [1:33:56<1:21:53,  1.30it/s]Training epoch 1:  54% 7505/13868 [1:33:57<1:20:46,  1.31it/s]Training epoch 1:  54% 7506/13868 [1:33:58<1:21:19,  1.30it/s]Training epoch 1:  54% 7507/13868 [1:33:59<1:20:26,  1.32it/s]Training epoch 1:  54% 7508/13868 [1:33:59<1:20:52,  1.31it/s]Training epoch 1:  54% 7509/13868 [1:34:00<1:22:11,  1.29it/s]Training epoch 1:  54% 7510/13868 [1:34:01<1:21:42,  1.30it/s]Training epoch 1:  54% 7511/13868 [1:34:02<1:22:26,  1.29it/s]Training epoch 1:  54% 7512/13868 [1:34:02<1:21:03,  1.31it/s]Training epoch 1:  54% 7513/13868 [1:34:03<1:21:07,  1.31it/s]Training epoch 1:  54% 7514/13868 [1:34:04<1:20:21,  1.32it/s]Training epoch 1:  54% 7515/13868 [1:34:05<1:20:23,  1.32it/s]Training epoch 1:  54% 7516/13868 [1:34:05<1:20:07,  1.32it/s]Training epoch 1:  54% 7517/13868 [1:34:06<1:21:06,  1.30it/s]Training epoch 1:  54% 7518/13868 [1:34:07<1:21:39,  1.30it/s]Training epoch 1:  54% 7519/13868 [1:34:08<1:21:21,  1.30it/s]Training epoch 1:  54% 7520/13868 [1:34:09<1:20:20,  1.32it/s]Training epoch 1:  54% 7521/13868 [1:34:09<1:20:47,  1.31it/s]Training epoch 1:  54% 7522/13868 [1:34:10<1:19:25,  1.33it/s]Training epoch 1:  54% 7523/13868 [1:34:11<1:19:33,  1.33it/s]Training epoch 1:  54% 7524/13868 [1:34:12<1:19:58,  1.32it/s]Training epoch 1:  54% 7525/13868 [1:34:12<1:21:12,  1.30it/s]Training epoch 1:  54% 7526/13868 [1:34:13<1:20:35,  1.31it/s]Training epoch 1:  54% 7527/13868 [1:34:14<1:20:35,  1.31it/s]Training epoch 1:  54% 7528/13868 [1:34:15<1:19:59,  1.32it/s]Training epoch 1:  54% 7529/13868 [1:34:15<1:19:31,  1.33it/s]Training epoch 1:  54% 7530/13868 [1:34:16<1:19:43,  1.32it/s]Training epoch 1:  54% 7531/13868 [1:34:17<1:21:16,  1.30it/s]Training epoch 1:  54% 7532/13868 [1:34:18<1:20:13,  1.32it/s]Training epoch 1:  54% 7533/13868 [1:34:18<1:19:45,  1.32it/s]Training epoch 1:  54% 7534/13868 [1:34:19<1:19:41,  1.32it/s]Training epoch 1:  54% 7535/13868 [1:34:20<1:19:52,  1.32it/s]Training epoch 1:  54% 7536/13868 [1:34:21<1:19:29,  1.33it/s]Training epoch 1:  54% 7537/13868 [1:34:21<1:18:55,  1.34it/s]Training epoch 1:  54% 7538/13868 [1:34:22<1:19:37,  1.32it/s]Training epoch 1:  54% 7539/13868 [1:34:23<1:20:26,  1.31it/s]Training epoch 1:  54% 7540/13868 [1:34:24<1:20:14,  1.31it/s]Training epoch 1:  54% 7541/13868 [1:34:24<1:19:19,  1.33it/s]Training epoch 1:  54% 7542/13868 [1:34:25<1:20:19,  1.31it/s]Training epoch 1:  54% 7543/13868 [1:34:26<1:19:41,  1.32it/s]Training epoch 1:  54% 7544/13868 [1:34:27<1:19:27,  1.33it/s]Training epoch 1:  54% 7545/13868 [1:34:27<1:19:24,  1.33it/s]Training epoch 1:  54% 7546/13868 [1:34:28<1:19:03,  1.33it/s]Training epoch 1:  54% 7547/13868 [1:34:29<1:19:20,  1.33it/s]Training epoch 1:  54% 7548/13868 [1:34:30<1:20:17,  1.31it/s]Training epoch 1:  54% 7549/13868 [1:34:30<1:19:30,  1.32it/s]Training epoch 1:  54% 7550/13868 [1:34:31<1:19:22,  1.33it/s]Training epoch 1:  54% 7551/13868 [1:34:32<1:20:11,  1.31it/s]Training epoch 1:  54% 7552/13868 [1:34:33<1:20:38,  1.31it/s]Training epoch 1:  54% 7553/13868 [1:34:34<1:20:19,  1.31it/s]Training epoch 1:  54% 7554/13868 [1:34:34<1:20:31,  1.31it/s]Training epoch 1:  54% 7555/13868 [1:34:35<1:20:37,  1.31it/s]Training epoch 1:  54% 7556/13868 [1:34:36<1:21:14,  1.29it/s]Training epoch 1:  54% 7557/13868 [1:34:37<1:21:52,  1.28it/s]Training epoch 1:  54% 7558/13868 [1:34:37<1:21:33,  1.29it/s]Training epoch 1:  55% 7559/13868 [1:34:38<1:21:46,  1.29it/s]Training epoch 1:  55% 7560/13868 [1:34:39<1:22:20,  1.28it/s]Training epoch 1:  55% 7561/13868 [1:34:40<1:21:14,  1.29it/s]Training epoch 1:  55% 7562/13868 [1:34:40<1:20:53,  1.30it/s]Training epoch 1:  55% 7563/13868 [1:34:41<1:20:39,  1.30it/s]Training epoch 1:  55% 7564/13868 [1:34:42<1:21:36,  1.29it/s]Training epoch 1:  55% 7565/13868 [1:34:43<1:22:19,  1.28it/s]Training epoch 1:  55% 7566/13868 [1:34:44<1:21:54,  1.28it/s]Training epoch 1:  55% 7567/13868 [1:34:44<1:21:02,  1.30it/s]Training epoch 1:  55% 7568/13868 [1:34:45<1:20:51,  1.30it/s]Training epoch 1:  55% 7569/13868 [1:34:46<1:21:04,  1.29it/s]Training epoch 1:  55% 7570/13868 [1:34:47<1:19:59,  1.31it/s]Training epoch 1:  55% 7571/13868 [1:34:47<1:19:21,  1.32it/s]Training epoch 1:  55% 7572/13868 [1:34:48<1:19:25,  1.32it/s]Training epoch 1:  55% 7573/13868 [1:34:49<1:19:06,  1.33it/s]Training epoch 1:  55% 7574/13868 [1:34:50<1:19:29,  1.32it/s]Training epoch 1:  55% 7575/13868 [1:34:50<1:20:51,  1.30it/s]Training epoch 1:  55% 7576/13868 [1:34:51<1:20:15,  1.31it/s]Training epoch 1:  55% 7577/13868 [1:34:52<1:19:15,  1.32it/s]Training epoch 1:  55% 7578/13868 [1:34:53<1:18:54,  1.33it/s]Training epoch 1:  55% 7579/13868 [1:34:53<1:19:30,  1.32it/s]Training epoch 1:  55% 7580/13868 [1:34:54<1:18:59,  1.33it/s]Training epoch 1:  55% 7581/13868 [1:34:55<1:20:10,  1.31it/s]Training epoch 1:  55% 7582/13868 [1:34:56<1:20:31,  1.30it/s]Training epoch 1:  55% 7583/13868 [1:34:57<1:20:16,  1.31it/s]Training epoch 1:  55% 7584/13868 [1:34:57<1:20:29,  1.30it/s]Training epoch 1:  55% 7585/13868 [1:34:58<1:20:19,  1.30it/s]Training epoch 1:  55% 7586/13868 [1:34:59<1:20:19,  1.30it/s]Training epoch 1:  55% 7587/13868 [1:35:00<1:20:04,  1.31it/s]Training epoch 1:  55% 7588/13868 [1:35:00<1:20:01,  1.31it/s]Training epoch 1:  55% 7589/13868 [1:35:01<1:20:24,  1.30it/s]Training epoch 1:  55% 7590/13868 [1:35:02<1:19:56,  1.31it/s]Training epoch 1:  55% 7591/13868 [1:35:03<1:18:32,  1.33it/s]Training epoch 1:  55% 7592/13868 [1:35:03<1:18:12,  1.34it/s]Training epoch 1:  55% 7593/13868 [1:35:04<1:18:09,  1.34it/s]Training epoch 1:  55% 7594/13868 [1:35:05<1:17:48,  1.34it/s]Training epoch 1:  55% 7595/13868 [1:35:06<1:18:38,  1.33it/s]Training epoch 1:  55% 7596/13868 [1:35:06<1:19:13,  1.32it/s]Training epoch 1:  55% 7597/13868 [1:35:07<1:17:42,  1.35it/s]Training epoch 1:  55% 7598/13868 [1:35:08<1:17:58,  1.34it/s]Training epoch 1:  55% 7599/13868 [1:35:09<1:18:04,  1.34it/s]Training epoch 1:  55% 7600/13868 [1:35:10<1:26:37,  1.21it/s]Training epoch 1:  55% 7601/13868 [1:35:10<1:25:07,  1.23it/s]Training epoch 1:  55% 7602/13868 [1:35:11<1:23:31,  1.25it/s]Training epoch 1:  55% 7603/13868 [1:35:12<1:21:32,  1.28it/s]Training epoch 1:  55% 7604/13868 [1:35:13<1:21:14,  1.29it/s]Training epoch 1:  55% 7605/13868 [1:35:13<1:21:12,  1.29it/s]Training epoch 1:  55% 7606/13868 [1:35:14<1:20:26,  1.30it/s]Training epoch 1:  55% 7607/13868 [1:35:15<1:19:12,  1.32it/s]Training epoch 1:  55% 7608/13868 [1:35:16<1:18:43,  1.33it/s]Training epoch 1:  55% 7609/13868 [1:35:16<1:18:27,  1.33it/s]Training epoch 1:  55% 7610/13868 [1:35:17<1:19:11,  1.32it/s]Training epoch 1:  55% 7611/13868 [1:35:18<1:19:42,  1.31it/s]Training epoch 1:  55% 7612/13868 [1:35:19<1:19:05,  1.32it/s]Training epoch 1:  55% 7613/13868 [1:35:19<1:18:45,  1.32it/s]Training epoch 1:  55% 7614/13868 [1:35:20<1:18:18,  1.33it/s]Training epoch 1:  55% 7615/13868 [1:35:21<1:19:29,  1.31it/s]Training epoch 1:  55% 7616/13868 [1:35:22<1:20:20,  1.30it/s]Training epoch 1:  55% 7617/13868 [1:35:23<1:20:57,  1.29it/s]Training epoch 1:  55% 7618/13868 [1:35:23<1:20:17,  1.30it/s]Training epoch 1:  55% 7619/13868 [1:35:24<1:20:33,  1.29it/s]Training epoch 1:  55% 7620/13868 [1:35:25<1:21:05,  1.28it/s]Training epoch 1:  55% 7621/13868 [1:35:26<1:20:21,  1.30it/s]Training epoch 1:  55% 7622/13868 [1:35:26<1:20:37,  1.29it/s]Training epoch 1:  55% 7623/13868 [1:35:27<1:20:02,  1.30it/s]Training epoch 1:  55% 7624/13868 [1:35:28<1:19:31,  1.31it/s]Training epoch 1:  55% 7625/13868 [1:35:29<1:19:00,  1.32it/s]Training epoch 1:  55% 7626/13868 [1:35:29<1:18:57,  1.32it/s]Training epoch 1:  55% 7627/13868 [1:35:30<1:19:28,  1.31it/s]Training epoch 1:  55% 7628/13868 [1:35:31<1:18:54,  1.32it/s]Training epoch 1:  55% 7629/13868 [1:35:32<1:18:33,  1.32it/s]Training epoch 1:  55% 7630/13868 [1:35:32<1:18:11,  1.33it/s]Training epoch 1:  55% 7631/13868 [1:35:33<1:18:04,  1.33it/s]Training epoch 1:  55% 7632/13868 [1:35:34<1:18:39,  1.32it/s]Training epoch 1:  55% 7633/13868 [1:35:35<1:19:08,  1.31it/s]Training epoch 1:  55% 7634/13868 [1:35:36<1:19:19,  1.31it/s]Training epoch 1:  55% 7635/13868 [1:35:36<1:18:57,  1.32it/s]Training epoch 1:  55% 7636/13868 [1:35:37<1:18:53,  1.32it/s]Training epoch 1:  55% 7637/13868 [1:35:38<1:17:58,  1.33it/s]Training epoch 1:  55% 7638/13868 [1:35:39<1:18:51,  1.32it/s]Training epoch 1:  55% 7639/13868 [1:35:39<1:19:14,  1.31it/s]Training epoch 1:  55% 7640/13868 [1:35:40<1:19:27,  1.31it/s]Training epoch 1:  55% 7641/13868 [1:35:41<1:20:00,  1.30it/s]Training epoch 1:  55% 7642/13868 [1:35:42<1:20:25,  1.29it/s]Training epoch 1:  55% 7643/13868 [1:35:42<1:18:48,  1.32it/s]Training epoch 1:  55% 7644/13868 [1:35:43<1:19:41,  1.30it/s]Training epoch 1:  55% 7645/13868 [1:35:44<1:19:34,  1.30it/s]Training epoch 1:  55% 7646/13868 [1:35:45<1:19:32,  1.30it/s]Training epoch 1:  55% 7647/13868 [1:35:45<1:19:53,  1.30it/s]Training epoch 1:  55% 7648/13868 [1:35:46<1:20:15,  1.29it/s]Training epoch 1:  55% 7649/13868 [1:35:47<1:20:03,  1.29it/s]Training epoch 1:  55% 7650/13868 [1:35:48<1:20:00,  1.30it/s]Training epoch 1:  55% 7651/13868 [1:35:49<1:19:02,  1.31it/s]Training epoch 1:  55% 7652/13868 [1:35:49<1:19:15,  1.31it/s]Training epoch 1:  55% 7653/13868 [1:35:50<1:19:14,  1.31it/s]Training epoch 1:  55% 7654/13868 [1:35:51<1:20:00,  1.29it/s]Training epoch 1:  55% 7655/13868 [1:35:52<1:19:12,  1.31it/s]Training epoch 1:  55% 7656/13868 [1:35:52<1:18:20,  1.32it/s]Training epoch 1:  55% 7657/13868 [1:35:53<1:18:59,  1.31it/s]Training epoch 1:  55% 7658/13868 [1:35:54<1:18:20,  1.32it/s]Training epoch 1:  55% 7659/13868 [1:35:55<1:18:08,  1.32it/s]Training epoch 1:  55% 7660/13868 [1:35:55<1:18:50,  1.31it/s]Training epoch 1:  55% 7661/13868 [1:35:56<1:19:18,  1.30it/s]Training epoch 1:  55% 7662/13868 [1:35:57<1:18:30,  1.32it/s]Training epoch 1:  55% 7663/13868 [1:35:58<1:18:17,  1.32it/s]Training epoch 1:  55% 7664/13868 [1:35:58<1:18:25,  1.32it/s]Training epoch 1:  55% 7665/13868 [1:35:59<1:17:37,  1.33it/s]Training epoch 1:  55% 7666/13868 [1:36:00<1:17:30,  1.33it/s]Training epoch 1:  55% 7667/13868 [1:36:01<1:17:06,  1.34it/s]Training epoch 1:  55% 7668/13868 [1:36:01<1:17:37,  1.33it/s]Training epoch 1:  55% 7669/13868 [1:36:02<1:18:44,  1.31it/s]Training epoch 1:  55% 7670/13868 [1:36:03<1:17:41,  1.33it/s]Training epoch 1:  55% 7671/13868 [1:36:04<1:17:25,  1.33it/s]Training epoch 1:  55% 7672/13868 [1:36:04<1:18:56,  1.31it/s]Training epoch 1:  55% 7673/13868 [1:36:05<1:17:44,  1.33it/s]Training epoch 1:  55% 7674/13868 [1:36:06<1:19:15,  1.30it/s]Training epoch 1:  55% 7675/13868 [1:36:07<1:19:10,  1.30it/s]Training epoch 1:  55% 7676/13868 [1:36:08<1:19:40,  1.30it/s]Training epoch 1:  55% 7677/13868 [1:36:08<1:19:47,  1.29it/s]Training epoch 1:  55% 7678/13868 [1:36:09<1:19:23,  1.30it/s]Training epoch 1:  55% 7679/13868 [1:36:10<1:18:17,  1.32it/s]Training epoch 1:  55% 7680/13868 [1:36:11<1:18:45,  1.31it/s]Training epoch 1:  55% 7681/13868 [1:36:11<1:19:19,  1.30it/s]Training epoch 1:  55% 7682/13868 [1:36:12<1:18:38,  1.31it/s]Training epoch 1:  55% 7683/13868 [1:36:13<1:18:50,  1.31it/s]Training epoch 1:  55% 7684/13868 [1:36:14<1:17:58,  1.32it/s]Training epoch 1:  55% 7685/13868 [1:36:14<1:17:32,  1.33it/s]Training epoch 1:  55% 7686/13868 [1:36:15<1:16:13,  1.35it/s]Training epoch 1:  55% 7687/13868 [1:36:16<1:18:06,  1.32it/s]Training epoch 1:  55% 7688/13868 [1:36:17<1:17:40,  1.33it/s]Training epoch 1:  55% 7689/13868 [1:36:17<1:18:08,  1.32it/s]Training epoch 1:  55% 7690/13868 [1:36:18<1:19:26,  1.30it/s]Training epoch 1:  55% 7691/13868 [1:36:19<1:18:30,  1.31it/s]Training epoch 1:  55% 7692/13868 [1:36:20<1:18:37,  1.31it/s]Training epoch 1:  55% 7693/13868 [1:36:21<1:19:15,  1.30it/s]Training epoch 1:  55% 7694/13868 [1:36:21<1:17:51,  1.32it/s]Training epoch 1:  55% 7695/13868 [1:36:22<1:18:25,  1.31it/s]Training epoch 1:  55% 7696/13868 [1:36:23<1:18:53,  1.30it/s]Training epoch 1:  56% 7697/13868 [1:36:24<1:17:58,  1.32it/s]Training epoch 1:  56% 7698/13868 [1:36:24<1:18:12,  1.31it/s]Training epoch 1:  56% 7699/13868 [1:36:25<1:17:05,  1.33it/s]Training epoch 1:  56% 7700/13868 [1:36:26<1:21:48,  1.26it/s]Training epoch 1:  56% 7701/13868 [1:36:27<1:20:31,  1.28it/s]Training epoch 1:  56% 7702/13868 [1:36:27<1:19:12,  1.30it/s]Training epoch 1:  56% 7703/13868 [1:36:28<1:18:04,  1.32it/s]Training epoch 1:  56% 7704/13868 [1:36:29<1:17:18,  1.33it/s]Training epoch 1:  56% 7705/13868 [1:36:30<1:18:00,  1.32it/s]Training epoch 1:  56% 7706/13868 [1:36:30<1:17:32,  1.32it/s]Training epoch 1:  56% 7707/13868 [1:36:31<1:17:19,  1.33it/s]Training epoch 1:  56% 7708/13868 [1:36:32<1:19:06,  1.30it/s]Training epoch 1:  56% 7709/13868 [1:36:33<1:19:44,  1.29it/s]Training epoch 1:  56% 7710/13868 [1:36:34<1:19:41,  1.29it/s]Training epoch 1:  56% 7711/13868 [1:36:34<1:19:56,  1.28it/s]Training epoch 1:  56% 7712/13868 [1:36:35<1:18:14,  1.31it/s]Training epoch 1:  56% 7713/13868 [1:36:36<1:18:06,  1.31it/s]Training epoch 1:  56% 7714/13868 [1:36:37<1:18:36,  1.30it/s]Training epoch 1:  56% 7715/13868 [1:36:37<1:17:53,  1.32it/s]Training epoch 1:  56% 7716/13868 [1:36:38<1:17:34,  1.32it/s]Training epoch 1:  56% 7717/13868 [1:36:39<1:18:12,  1.31it/s]Training epoch 1:  56% 7718/13868 [1:36:40<1:18:44,  1.30it/s]Training epoch 1:  56% 7719/13868 [1:36:40<1:18:58,  1.30it/s]Training epoch 1:  56% 7720/13868 [1:36:41<1:18:51,  1.30it/s]Training epoch 1:  56% 7721/13868 [1:36:42<1:17:09,  1.33it/s]Training epoch 1:  56% 7722/13868 [1:36:43<1:17:13,  1.33it/s]Training epoch 1:  56% 7723/13868 [1:36:43<1:18:32,  1.30it/s]Training epoch 1:  56% 7724/13868 [1:36:44<1:18:40,  1.30it/s]Training epoch 1:  56% 7725/13868 [1:36:45<1:18:12,  1.31it/s]Training epoch 1:  56% 7726/13868 [1:36:46<1:17:35,  1.32it/s]Training epoch 1:  56% 7727/13868 [1:36:46<1:17:36,  1.32it/s]Training epoch 1:  56% 7728/13868 [1:36:47<1:17:30,  1.32it/s]Training epoch 1:  56% 7729/13868 [1:36:48<1:16:57,  1.33it/s]Training epoch 1:  56% 7730/13868 [1:36:49<1:16:02,  1.35it/s]Training epoch 1:  56% 7731/13868 [1:36:49<1:16:33,  1.34it/s]Training epoch 1:  56% 7732/13868 [1:36:50<1:17:07,  1.33it/s]Training epoch 1:  56% 7733/13868 [1:36:51<1:17:02,  1.33it/s]Training epoch 1:  56% 7734/13868 [1:36:52<1:17:14,  1.32it/s]Training epoch 1:  56% 7735/13868 [1:36:52<1:16:27,  1.34it/s]Training epoch 1:  56% 7736/13868 [1:36:53<1:16:49,  1.33it/s]Training epoch 1:  56% 7737/13868 [1:36:54<1:17:12,  1.32it/s]Training epoch 1:  56% 7738/13868 [1:36:55<1:18:45,  1.30it/s]Training epoch 1:  56% 7739/13868 [1:36:56<1:17:41,  1.31it/s]Training epoch 1:  56% 7740/13868 [1:36:56<1:17:18,  1.32it/s]Training epoch 1:  56% 7741/13868 [1:36:57<1:17:48,  1.31it/s]Training epoch 1:  56% 7742/13868 [1:36:58<1:17:50,  1.31it/s]Training epoch 1:  56% 7743/13868 [1:36:59<1:17:56,  1.31it/s]Training epoch 1:  56% 7744/13868 [1:36:59<1:17:35,  1.32it/s]Training epoch 1:  56% 7745/13868 [1:37:00<1:17:34,  1.32it/s]Training epoch 1:  56% 7746/13868 [1:37:01<1:18:32,  1.30it/s]Training epoch 1:  56% 7747/13868 [1:37:02<1:17:57,  1.31it/s]Training epoch 1:  56% 7748/13868 [1:37:02<1:17:41,  1.31it/s]Training epoch 1:  56% 7749/13868 [1:37:03<1:18:34,  1.30it/s]Training epoch 1:  56% 7750/13868 [1:37:04<1:18:17,  1.30it/s]Training epoch 1:  56% 7751/13868 [1:37:05<1:17:58,  1.31it/s]Training epoch 1:  56% 7752/13868 [1:37:05<1:18:08,  1.30it/s]Training epoch 1:  56% 7753/13868 [1:37:06<1:17:29,  1.32it/s]Training epoch 1:  56% 7754/13868 [1:37:07<1:18:15,  1.30it/s]Training epoch 1:  56% 7755/13868 [1:37:08<1:18:25,  1.30it/s]Training epoch 1:  56% 7756/13868 [1:37:09<1:18:46,  1.29it/s]Training epoch 1:  56% 7757/13868 [1:37:09<1:17:59,  1.31it/s]Training epoch 1:  56% 7758/13868 [1:37:10<1:17:20,  1.32it/s]Training epoch 1:  56% 7759/13868 [1:37:11<1:17:52,  1.31it/s]Training epoch 1:  56% 7760/13868 [1:37:12<1:18:22,  1.30it/s]Training epoch 1:  56% 7761/13868 [1:37:12<1:17:38,  1.31it/s]Training epoch 1:  56% 7762/13868 [1:37:13<1:17:23,  1.32it/s]Training epoch 1:  56% 7763/13868 [1:37:14<1:17:05,  1.32it/s]Training epoch 1:  56% 7764/13868 [1:37:15<1:16:29,  1.33it/s]Training epoch 1:  56% 7765/13868 [1:37:15<1:16:36,  1.33it/s]Training epoch 1:  56% 7766/13868 [1:37:16<1:16:24,  1.33it/s]Training epoch 1:  56% 7767/13868 [1:37:17<1:15:54,  1.34it/s]Training epoch 1:  56% 7768/13868 [1:37:18<1:16:06,  1.34it/s]Training epoch 1:  56% 7769/13868 [1:37:18<1:16:30,  1.33it/s]Training epoch 1:  56% 7770/13868 [1:37:19<1:16:28,  1.33it/s]Training epoch 1:  56% 7771/13868 [1:37:20<1:17:16,  1.32it/s]Training epoch 1:  56% 7772/13868 [1:37:21<1:17:32,  1.31it/s]Training epoch 1:  56% 7773/13868 [1:37:21<1:18:10,  1.30it/s]Training epoch 1:  56% 7774/13868 [1:37:22<1:18:11,  1.30it/s]Training epoch 1:  56% 7775/13868 [1:37:23<1:17:35,  1.31it/s]Training epoch 1:  56% 7776/13868 [1:37:24<1:16:57,  1.32it/s]Training epoch 1:  56% 7777/13868 [1:37:24<1:16:12,  1.33it/s]Training epoch 1:  56% 7778/13868 [1:37:25<1:16:50,  1.32it/s]Training epoch 1:  56% 7779/13868 [1:37:26<1:16:40,  1.32it/s]Training epoch 1:  56% 7780/13868 [1:37:27<1:16:15,  1.33it/s]Training epoch 1:  56% 7781/13868 [1:37:27<1:17:02,  1.32it/s]Training epoch 1:  56% 7782/13868 [1:37:28<1:16:30,  1.33it/s]Training epoch 1:  56% 7783/13868 [1:37:29<1:17:14,  1.31it/s]Training epoch 1:  56% 7784/13868 [1:37:30<1:17:34,  1.31it/s]Training epoch 1:  56% 7785/13868 [1:37:31<1:16:13,  1.33it/s]Training epoch 1:  56% 7786/13868 [1:37:31<1:15:15,  1.35it/s]Training epoch 1:  56% 7787/13868 [1:37:32<1:15:54,  1.34it/s]Training epoch 1:  56% 7788/13868 [1:37:33<1:16:11,  1.33it/s]Training epoch 1:  56% 7789/13868 [1:37:33<1:15:48,  1.34it/s]Training epoch 1:  56% 7790/13868 [1:37:34<1:16:08,  1.33it/s]Training epoch 1:  56% 7791/13868 [1:37:35<1:16:53,  1.32it/s]Training epoch 1:  56% 7792/13868 [1:37:36<1:17:15,  1.31it/s]Training epoch 1:  56% 7793/13868 [1:37:37<1:18:30,  1.29it/s]Training epoch 1:  56% 7794/13868 [1:37:37<1:18:04,  1.30it/s]Training epoch 1:  56% 7795/13868 [1:37:38<1:17:32,  1.31it/s]Training epoch 1:  56% 7796/13868 [1:37:39<1:16:51,  1.32it/s]Training epoch 1:  56% 7797/13868 [1:37:40<1:16:54,  1.32it/s]Training epoch 1:  56% 7798/13868 [1:37:40<1:17:39,  1.30it/s]Training epoch 1:  56% 7799/13868 [1:37:41<1:17:27,  1.31it/s]Training epoch 1:  56% 7800/13868 [1:37:42<1:21:53,  1.23it/s]Training epoch 1:  56% 7801/13868 [1:37:43<1:21:31,  1.24it/s]Training epoch 1:  56% 7802/13868 [1:37:44<1:20:38,  1.25it/s]Training epoch 1:  56% 7803/13868 [1:37:44<1:20:11,  1.26it/s]Training epoch 1:  56% 7804/13868 [1:37:45<1:19:45,  1.27it/s]Training epoch 1:  56% 7805/13868 [1:37:46<1:18:49,  1.28it/s]Training epoch 1:  56% 7806/13868 [1:37:47<1:18:08,  1.29it/s]Training epoch 1:  56% 7807/13868 [1:37:48<1:18:02,  1.29it/s]Training epoch 1:  56% 7808/13868 [1:37:48<1:18:27,  1.29it/s]Training epoch 1:  56% 7809/13868 [1:37:49<1:17:57,  1.30it/s]Training epoch 1:  56% 7810/13868 [1:37:50<1:18:13,  1.29it/s]Training epoch 1:  56% 7811/13868 [1:37:51<1:17:29,  1.30it/s]Training epoch 1:  56% 7812/13868 [1:37:51<1:15:42,  1.33it/s]Training epoch 1:  56% 7813/13868 [1:37:52<1:15:32,  1.34it/s]Training epoch 1:  56% 7814/13868 [1:37:53<1:15:39,  1.33it/s]Training epoch 1:  56% 7815/13868 [1:37:54<1:14:51,  1.35it/s]Training epoch 1:  56% 7816/13868 [1:37:54<1:15:34,  1.33it/s]Training epoch 1:  56% 7817/13868 [1:37:55<1:15:40,  1.33it/s]Training epoch 1:  56% 7818/13868 [1:37:56<1:15:41,  1.33it/s]Training epoch 1:  56% 7819/13868 [1:37:57<1:16:16,  1.32it/s]Training epoch 1:  56% 7820/13868 [1:37:57<1:16:29,  1.32it/s]Training epoch 1:  56% 7821/13868 [1:37:58<1:16:59,  1.31it/s]Training epoch 1:  56% 7822/13868 [1:37:59<1:17:01,  1.31it/s]Training epoch 1:  56% 7823/13868 [1:38:00<1:17:16,  1.30it/s]Training epoch 1:  56% 7824/13868 [1:38:00<1:16:23,  1.32it/s]Training epoch 1:  56% 7825/13868 [1:38:01<1:15:42,  1.33it/s]Training epoch 1:  56% 7826/13868 [1:38:02<1:16:09,  1.32it/s]Training epoch 1:  56% 7827/13868 [1:38:03<1:16:22,  1.32it/s]Training epoch 1:  56% 7828/13868 [1:38:03<1:16:59,  1.31it/s]Training epoch 1:  56% 7829/13868 [1:38:04<1:16:39,  1.31it/s]Training epoch 1:  56% 7830/13868 [1:38:05<1:16:43,  1.31it/s]Training epoch 1:  56% 7831/13868 [1:38:06<1:17:18,  1.30it/s]Training epoch 1:  56% 7832/13868 [1:38:07<1:17:59,  1.29it/s]Training epoch 1:  56% 7833/13868 [1:38:07<1:18:31,  1.28it/s]Training epoch 1:  56% 7834/13868 [1:38:08<1:18:31,  1.28it/s]Training epoch 1:  56% 7835/13868 [1:38:09<1:18:16,  1.28it/s]Training epoch 1:  57% 7836/13868 [1:38:10<1:17:35,  1.30it/s]Training epoch 1:  57% 7837/13868 [1:38:10<1:17:11,  1.30it/s]Training epoch 1:  57% 7838/13868 [1:38:11<1:16:32,  1.31it/s]Training epoch 1:  57% 7839/13868 [1:38:12<1:15:55,  1.32it/s]Training epoch 1:  57% 7840/13868 [1:38:13<1:16:30,  1.31it/s]Training epoch 1:  57% 7841/13868 [1:38:13<1:16:44,  1.31it/s]Training epoch 1:  57% 7842/13868 [1:38:14<1:16:17,  1.32it/s]Training epoch 1:  57% 7843/13868 [1:38:15<1:16:27,  1.31it/s]Training epoch 1:  57% 7844/13868 [1:38:16<1:16:57,  1.30it/s]Training epoch 1:  57% 7845/13868 [1:38:16<1:17:05,  1.30it/s]Training epoch 1:  57% 7846/13868 [1:38:17<1:17:19,  1.30it/s]Training epoch 1:  57% 7847/13868 [1:38:18<1:17:14,  1.30it/s]Training epoch 1:  57% 7848/13868 [1:38:19<1:17:12,  1.30it/s]Training epoch 1:  57% 7849/13868 [1:38:20<1:17:54,  1.29it/s]Training epoch 1:  57% 7850/13868 [1:38:20<1:18:13,  1.28it/s]Training epoch 1:  57% 7851/13868 [1:38:21<1:17:29,  1.29it/s]Training epoch 1:  57% 7852/13868 [1:38:22<1:17:01,  1.30it/s]Training epoch 1:  57% 7853/13868 [1:38:23<1:16:24,  1.31it/s]Training epoch 1:  57% 7854/13868 [1:38:23<1:16:25,  1.31it/s]Training epoch 1:  57% 7855/13868 [1:38:24<1:15:53,  1.32it/s]Training epoch 1:  57% 7856/13868 [1:38:25<1:15:55,  1.32it/s]Training epoch 1:  57% 7857/13868 [1:38:26<1:15:32,  1.33it/s]Training epoch 1:  57% 7858/13868 [1:38:26<1:16:57,  1.30it/s]Training epoch 1:  57% 7859/13868 [1:38:27<1:17:32,  1.29it/s]Training epoch 1:  57% 7860/13868 [1:38:28<1:16:26,  1.31it/s]Training epoch 1:  57% 7861/13868 [1:38:29<1:15:13,  1.33it/s]Training epoch 1:  57% 7862/13868 [1:38:29<1:15:53,  1.32it/s]Training epoch 1:  57% 7863/13868 [1:38:30<1:15:39,  1.32it/s]Training epoch 1:  57% 7864/13868 [1:38:31<1:16:44,  1.30it/s]Training epoch 1:  57% 7865/13868 [1:38:32<1:16:31,  1.31it/s]Training epoch 1:  57% 7866/13868 [1:38:33<1:16:46,  1.30it/s]Training epoch 1:  57% 7867/13868 [1:38:33<1:16:18,  1.31it/s]Training epoch 1:  57% 7868/13868 [1:38:34<1:16:59,  1.30it/s]Training epoch 1:  57% 7869/13868 [1:38:35<1:17:11,  1.30it/s]Training epoch 1:  57% 7870/13868 [1:38:36<1:16:38,  1.30it/s]Training epoch 1:  57% 7871/13868 [1:38:36<1:16:01,  1.31it/s]Training epoch 1:  57% 7872/13868 [1:38:37<1:15:19,  1.33it/s]Training epoch 1:  57% 7873/13868 [1:38:38<1:14:51,  1.33it/s]Training epoch 1:  57% 7874/13868 [1:38:39<1:15:27,  1.32it/s]Training epoch 1:  57% 7875/13868 [1:38:39<1:14:45,  1.34it/s]Training epoch 1:  57% 7876/13868 [1:38:40<1:15:21,  1.33it/s]Training epoch 1:  57% 7877/13868 [1:38:41<1:15:13,  1.33it/s]Training epoch 1:  57% 7878/13868 [1:38:42<1:15:27,  1.32it/s]Training epoch 1:  57% 7879/13868 [1:38:42<1:16:06,  1.31it/s]Training epoch 1:  57% 7880/13868 [1:38:43<1:16:23,  1.31it/s]Training epoch 1:  57% 7881/13868 [1:38:44<1:16:03,  1.31it/s]Training epoch 1:  57% 7882/13868 [1:38:45<1:16:23,  1.31it/s]Training epoch 1:  57% 7883/13868 [1:38:45<1:16:37,  1.30it/s]Training epoch 1:  57% 7884/13868 [1:38:46<1:16:02,  1.31it/s]Training epoch 1:  57% 7885/13868 [1:38:47<1:16:37,  1.30it/s]Training epoch 1:  57% 7886/13868 [1:38:48<1:16:06,  1.31it/s]Training epoch 1:  57% 7887/13868 [1:38:48<1:15:35,  1.32it/s]Training epoch 1:  57% 7888/13868 [1:38:49<1:14:41,  1.33it/s]Training epoch 1:  57% 7889/13868 [1:38:50<1:13:58,  1.35it/s]Training epoch 1:  57% 7890/13868 [1:38:51<1:13:47,  1.35it/s]Training epoch 1:  57% 7891/13868 [1:38:51<1:15:06,  1.33it/s]Training epoch 1:  57% 7892/13868 [1:38:52<1:15:21,  1.32it/s]Training epoch 1:  57% 7893/13868 [1:38:53<1:15:43,  1.31it/s]Training epoch 1:  57% 7894/13868 [1:38:54<1:16:01,  1.31it/s]Training epoch 1:  57% 7895/13868 [1:38:55<1:17:14,  1.29it/s]Training epoch 1:  57% 7896/13868 [1:38:55<1:16:38,  1.30it/s]Training epoch 1:  57% 7897/13868 [1:38:56<1:16:15,  1.31it/s]Training epoch 1:  57% 7898/13868 [1:38:57<1:16:26,  1.30it/s]Training epoch 1:  57% 7899/13868 [1:38:58<1:15:54,  1.31it/s]Training epoch 1:  57% 7900/13868 [1:38:59<1:20:24,  1.24it/s]Training epoch 1:  57% 7901/13868 [1:38:59<1:18:44,  1.26it/s]Training epoch 1:  57% 7902/13868 [1:39:00<1:17:54,  1.28it/s]Training epoch 1:  57% 7903/13868 [1:39:01<1:16:46,  1.29it/s]Training epoch 1:  57% 7904/13868 [1:39:02<1:16:47,  1.29it/s]Training epoch 1:  57% 7905/13868 [1:39:02<1:16:26,  1.30it/s]Training epoch 1:  57% 7906/13868 [1:39:03<1:16:13,  1.30it/s]Training epoch 1:  57% 7907/13868 [1:39:04<1:16:26,  1.30it/s]Training epoch 1:  57% 7908/13868 [1:39:05<1:16:08,  1.30it/s]Training epoch 1:  57% 7909/13868 [1:39:05<1:15:23,  1.32it/s]Training epoch 1:  57% 7910/13868 [1:39:06<1:15:32,  1.31it/s]Training epoch 1:  57% 7911/13868 [1:39:07<1:14:59,  1.32it/s]Training epoch 1:  57% 7912/13868 [1:39:08<1:13:55,  1.34it/s]Training epoch 1:  57% 7913/13868 [1:39:08<1:14:41,  1.33it/s]Training epoch 1:  57% 7914/13868 [1:39:09<1:14:34,  1.33it/s]Training epoch 1:  57% 7915/13868 [1:39:10<1:14:07,  1.34it/s]Training epoch 1:  57% 7916/13868 [1:39:11<1:13:31,  1.35it/s]Training epoch 1:  57% 7917/13868 [1:39:11<1:14:15,  1.34it/s]Training epoch 1:  57% 7918/13868 [1:39:12<1:15:04,  1.32it/s]Training epoch 1:  57% 7919/13868 [1:39:13<1:15:27,  1.31it/s]Training epoch 1:  57% 7920/13868 [1:39:14<1:16:12,  1.30it/s]Training epoch 1:  57% 7921/13868 [1:39:14<1:16:10,  1.30it/s]Training epoch 1:  57% 7922/13868 [1:39:15<1:15:48,  1.31it/s]Training epoch 1:  57% 7923/13868 [1:39:16<1:16:18,  1.30it/s]Training epoch 1:  57% 7924/13868 [1:39:17<1:15:37,  1.31it/s]Training epoch 1:  57% 7925/13868 [1:39:17<1:15:07,  1.32it/s]Training epoch 1:  57% 7926/13868 [1:39:18<1:15:35,  1.31it/s]Training epoch 1:  57% 7927/13868 [1:39:19<1:15:29,  1.31it/s]Training epoch 1:  57% 7928/13868 [1:39:20<1:15:46,  1.31it/s]Training epoch 1:  57% 7929/13868 [1:39:21<1:15:58,  1.30it/s]Training epoch 1:  57% 7930/13868 [1:39:21<1:15:35,  1.31it/s]Training epoch 1:  57% 7931/13868 [1:39:22<1:13:26,  1.35it/s]Training epoch 1:  57% 7932/13868 [1:39:23<1:15:12,  1.32it/s]Training epoch 1:  57% 7933/13868 [1:39:24<1:15:00,  1.32it/s]Training epoch 1:  57% 7934/13868 [1:39:24<1:14:20,  1.33it/s]Training epoch 1:  57% 7935/13868 [1:39:25<1:13:46,  1.34it/s]Training epoch 1:  57% 7936/13868 [1:39:26<1:13:53,  1.34it/s]Training epoch 1:  57% 7937/13868 [1:39:27<1:13:02,  1.35it/s]Training epoch 1:  57% 7938/13868 [1:39:27<1:14:52,  1.32it/s]Training epoch 1:  57% 7939/13868 [1:39:28<1:14:47,  1.32it/s]Training epoch 1:  57% 7940/13868 [1:39:29<1:15:54,  1.30it/s]Training epoch 1:  57% 7941/13868 [1:39:30<1:15:42,  1.30it/s]Training epoch 1:  57% 7942/13868 [1:39:30<1:16:14,  1.30it/s]Training epoch 1:  57% 7943/13868 [1:39:31<1:16:09,  1.30it/s]Training epoch 1:  57% 7944/13868 [1:39:32<1:15:50,  1.30it/s]Training epoch 1:  57% 7945/13868 [1:39:33<1:14:34,  1.32it/s]Training epoch 1:  57% 7946/13868 [1:39:33<1:14:52,  1.32it/s]Training epoch 1:  57% 7947/13868 [1:39:34<1:15:18,  1.31it/s]Training epoch 1:  57% 7948/13868 [1:39:35<1:15:34,  1.31it/s]Training epoch 1:  57% 7949/13868 [1:39:36<1:15:12,  1.31it/s]Training epoch 1:  57% 7950/13868 [1:39:36<1:15:30,  1.31it/s]Training epoch 1:  57% 7951/13868 [1:39:37<1:15:35,  1.30it/s]Training epoch 1:  57% 7952/13868 [1:39:38<1:16:02,  1.30it/s]Training epoch 1:  57% 7953/13868 [1:39:39<1:16:39,  1.29it/s]Training epoch 1:  57% 7954/13868 [1:39:40<1:16:08,  1.29it/s]Training epoch 1:  57% 7955/13868 [1:39:40<1:15:47,  1.30it/s]Training epoch 1:  57% 7956/13868 [1:39:41<1:15:38,  1.30it/s]Training epoch 1:  57% 7957/13868 [1:39:42<1:16:51,  1.28it/s]Training epoch 1:  57% 7958/13868 [1:39:43<1:16:16,  1.29it/s]Training epoch 1:  57% 7959/13868 [1:39:43<1:15:40,  1.30it/s]Training epoch 1:  57% 7960/13868 [1:39:44<1:14:37,  1.32it/s]Training epoch 1:  57% 7961/13868 [1:39:45<1:14:37,  1.32it/s]Training epoch 1:  57% 7962/13868 [1:39:46<1:14:14,  1.33it/s]Training epoch 1:  57% 7963/13868 [1:39:46<1:14:10,  1.33it/s]Training epoch 1:  57% 7964/13868 [1:39:47<1:14:34,  1.32it/s]Training epoch 1:  57% 7965/13868 [1:39:48<1:15:23,  1.31it/s]Training epoch 1:  57% 7966/13868 [1:39:49<1:14:25,  1.32it/s]Training epoch 1:  57% 7967/13868 [1:39:49<1:14:32,  1.32it/s]Training epoch 1:  57% 7968/13868 [1:39:50<1:14:25,  1.32it/s]Training epoch 1:  57% 7969/13868 [1:39:51<1:14:08,  1.33it/s]Training epoch 1:  57% 7970/13868 [1:39:52<1:13:52,  1.33it/s]Training epoch 1:  57% 7971/13868 [1:39:52<1:13:46,  1.33it/s]Training epoch 1:  57% 7972/13868 [1:39:53<1:13:40,  1.33it/s]Training epoch 1:  57% 7973/13868 [1:39:54<1:13:59,  1.33it/s]Training epoch 1:  57% 7974/13868 [1:39:55<1:13:42,  1.33it/s]Training epoch 1:  58% 7975/13868 [1:39:55<1:13:24,  1.34it/s]Training epoch 1:  58% 7976/13868 [1:39:56<1:13:47,  1.33it/s]Training epoch 1:  58% 7977/13868 [1:39:57<1:14:28,  1.32it/s]Training epoch 1:  58% 7978/13868 [1:39:58<1:14:19,  1.32it/s]Training epoch 1:  58% 7979/13868 [1:39:59<1:14:45,  1.31it/s]Training epoch 1:  58% 7980/13868 [1:39:59<1:14:28,  1.32it/s]Training epoch 1:  58% 7981/13868 [1:40:00<1:14:48,  1.31it/s]Training epoch 1:  58% 7982/13868 [1:40:01<1:14:50,  1.31it/s]Training epoch 1:  58% 7983/13868 [1:40:02<1:14:13,  1.32it/s]Training epoch 1:  58% 7984/13868 [1:40:02<1:14:06,  1.32it/s]Training epoch 1:  58% 7985/13868 [1:40:03<1:13:50,  1.33it/s]Training epoch 1:  58% 7986/13868 [1:40:04<1:14:37,  1.31it/s]Training epoch 1:  58% 7987/13868 [1:40:05<1:14:31,  1.32it/s]Training epoch 1:  58% 7988/13868 [1:40:05<1:14:52,  1.31it/s]Training epoch 1:  58% 7989/13868 [1:40:06<1:13:48,  1.33it/s]Training epoch 1:  58% 7990/13868 [1:40:07<1:13:43,  1.33it/s]Training epoch 1:  58% 7991/13868 [1:40:08<1:14:29,  1.31it/s]Training epoch 1:  58% 7992/13868 [1:40:08<1:16:15,  1.28it/s]Training epoch 1:  58% 7993/13868 [1:40:09<1:15:30,  1.30it/s]Training epoch 1:  58% 7994/13868 [1:40:10<1:15:05,  1.30it/s]Training epoch 1:  58% 7995/13868 [1:40:11<1:15:03,  1.30it/s]Training epoch 1:  58% 7996/13868 [1:40:12<1:14:59,  1.30it/s]Training epoch 1:  58% 7997/13868 [1:40:12<1:15:13,  1.30it/s]Training epoch 1:  58% 7998/13868 [1:40:13<1:14:45,  1.31it/s]Training epoch 1:  58% 7999/13868 [1:40:14<1:14:50,  1.31it/s]Training epoch 1:  58% 8000/13868 [1:40:15<1:18:59,  1.24it/s]Training epoch 1:  58% 8001/13868 [1:40:15<1:18:17,  1.25it/s]Training epoch 1:  58% 8002/13868 [1:40:16<1:16:23,  1.28it/s]Training epoch 1:  58% 8003/13868 [1:40:17<1:16:16,  1.28it/s]Training epoch 1:  58% 8004/13868 [1:40:18<1:16:05,  1.28it/s]Training epoch 1:  58% 8005/13868 [1:40:19<1:15:30,  1.29it/s]Training epoch 1:  58% 8006/13868 [1:40:19<1:14:45,  1.31it/s]Training epoch 1:  58% 8007/13868 [1:40:20<1:13:25,  1.33it/s]Training epoch 1:  58% 8008/13868 [1:40:21<1:14:51,  1.30it/s]Training epoch 1:  58% 8009/13868 [1:40:22<1:13:28,  1.33it/s]Training epoch 1:  58% 8010/13868 [1:40:22<1:13:37,  1.33it/s]Training epoch 1:  58% 8011/13868 [1:40:23<1:14:48,  1.30it/s]Training epoch 1:  58% 8012/13868 [1:40:24<1:14:10,  1.32it/s]Training epoch 1:  58% 8013/13868 [1:40:25<1:14:29,  1.31it/s]Training epoch 1:  58% 8014/13868 [1:40:25<1:13:55,  1.32it/s]Training epoch 1:  58% 8015/13868 [1:40:26<1:12:41,  1.34it/s]Training epoch 1:  58% 8016/13868 [1:40:27<1:13:15,  1.33it/s]Training epoch 1:  58% 8017/13868 [1:40:28<1:12:37,  1.34it/s]Training epoch 1:  58% 8018/13868 [1:40:28<1:12:09,  1.35it/s]Training epoch 1:  58% 8019/13868 [1:40:29<1:12:42,  1.34it/s]Training epoch 1:  58% 8020/13868 [1:40:30<1:13:03,  1.33it/s]Training epoch 1:  58% 8021/13868 [1:40:31<1:13:25,  1.33it/s]Training epoch 1:  58% 8022/13868 [1:40:31<1:13:36,  1.32it/s]Training epoch 1:  58% 8023/13868 [1:40:32<1:14:11,  1.31it/s]Training epoch 1:  58% 8024/13868 [1:40:33<1:14:25,  1.31it/s]Training epoch 1:  58% 8025/13868 [1:40:34<1:14:29,  1.31it/s]Training epoch 1:  58% 8026/13868 [1:40:34<1:14:47,  1.30it/s]Training epoch 1:  58% 8027/13868 [1:40:35<1:15:23,  1.29it/s]Training epoch 1:  58% 8028/13868 [1:40:36<1:14:36,  1.30it/s]Training epoch 1:  58% 8029/13868 [1:40:37<1:14:20,  1.31it/s]Training epoch 1:  58% 8030/13868 [1:40:37<1:13:39,  1.32it/s]Training epoch 1:  58% 8031/13868 [1:40:38<1:13:38,  1.32it/s]Training epoch 1:  58% 8032/13868 [1:40:39<1:12:14,  1.35it/s]Training epoch 1:  58% 8033/13868 [1:40:40<1:12:32,  1.34it/s]Training epoch 1:  58% 8034/13868 [1:40:40<1:11:52,  1.35it/s]Training epoch 1:  58% 8035/13868 [1:40:41<1:12:38,  1.34it/s]Training epoch 1:  58% 8036/13868 [1:40:42<1:13:00,  1.33it/s]Training epoch 1:  58% 8037/13868 [1:40:43<1:12:10,  1.35it/s]Training epoch 1:  58% 8038/13868 [1:40:43<1:12:31,  1.34it/s]Training epoch 1:  58% 8039/13868 [1:40:44<1:12:20,  1.34it/s]Training epoch 1:  58% 8040/13868 [1:40:45<1:12:50,  1.33it/s]Training epoch 1:  58% 8041/13868 [1:40:46<1:12:40,  1.34it/s]Training epoch 1:  58% 8042/13868 [1:40:46<1:12:55,  1.33it/s]Training epoch 1:  58% 8043/13868 [1:40:47<1:13:04,  1.33it/s]Training epoch 1:  58% 8044/13868 [1:40:48<1:12:20,  1.34it/s]Training epoch 1:  58% 8045/13868 [1:40:49<1:11:50,  1.35it/s]Training epoch 1:  58% 8046/13868 [1:40:49<1:12:01,  1.35it/s]Training epoch 1:  58% 8047/13868 [1:40:50<1:12:56,  1.33it/s]Training epoch 1:  58% 8048/13868 [1:40:51<1:11:52,  1.35it/s]Training epoch 1:  58% 8049/13868 [1:40:52<1:12:22,  1.34it/s]Training epoch 1:  58% 8050/13868 [1:40:52<1:12:13,  1.34it/s]Training epoch 1:  58% 8051/13868 [1:40:53<1:12:42,  1.33it/s]Training epoch 1:  58% 8052/13868 [1:40:54<1:12:58,  1.33it/s]Training epoch 1:  58% 8053/13868 [1:40:55<1:13:07,  1.33it/s]Training epoch 1:  58% 8054/13868 [1:40:55<1:13:18,  1.32it/s]Training epoch 1:  58% 8055/13868 [1:40:56<1:13:03,  1.33it/s]Training epoch 1:  58% 8056/13868 [1:40:57<1:13:15,  1.32it/s]Training epoch 1:  58% 8057/13868 [1:40:58<1:13:08,  1.32it/s]Training epoch 1:  58% 8058/13868 [1:40:58<1:13:20,  1.32it/s]Training epoch 1:  58% 8059/13868 [1:40:59<1:14:14,  1.30it/s]Training epoch 1:  58% 8060/13868 [1:41:00<1:14:38,  1.30it/s]Training epoch 1:  58% 8061/13868 [1:41:01<1:15:01,  1.29it/s]Training epoch 1:  58% 8062/13868 [1:41:02<1:14:55,  1.29it/s]Training epoch 1:  58% 8063/13868 [1:41:02<1:14:05,  1.31it/s]Training epoch 1:  58% 8064/13868 [1:41:03<1:13:45,  1.31it/s]Training epoch 1:  58% 8065/13868 [1:41:04<1:14:05,  1.31it/s]Training epoch 1:  58% 8066/13868 [1:41:05<1:13:46,  1.31it/s]Training epoch 1:  58% 8067/13868 [1:41:05<1:13:56,  1.31it/s]Training epoch 1:  58% 8068/13868 [1:41:06<1:13:37,  1.31it/s]Training epoch 1:  58% 8069/13868 [1:41:07<1:13:24,  1.32it/s]Training epoch 1:  58% 8070/13868 [1:41:08<1:13:22,  1.32it/s]Training epoch 1:  58% 8071/13868 [1:41:08<1:13:42,  1.31it/s]Training epoch 1:  58% 8072/13868 [1:41:09<1:12:35,  1.33it/s]Training epoch 1:  58% 8073/13868 [1:41:10<1:12:48,  1.33it/s]Training epoch 1:  58% 8074/13868 [1:41:11<1:12:52,  1.33it/s]Training epoch 1:  58% 8075/13868 [1:41:11<1:13:28,  1.31it/s]Training epoch 1:  58% 8076/13868 [1:41:12<1:13:46,  1.31it/s]Training epoch 1:  58% 8077/13868 [1:41:13<1:13:01,  1.32it/s]Training epoch 1:  58% 8078/13868 [1:41:14<1:12:24,  1.33it/s]Training epoch 1:  58% 8079/13868 [1:41:14<1:12:38,  1.33it/s]Training epoch 1:  58% 8080/13868 [1:41:15<1:12:44,  1.33it/s]Training epoch 1:  58% 8081/13868 [1:41:16<1:12:15,  1.33it/s]Training epoch 1:  58% 8082/13868 [1:41:17<1:10:25,  1.37it/s]Training epoch 1:  58% 8083/13868 [1:41:17<1:11:08,  1.36it/s]Training epoch 1:  58% 8084/13868 [1:41:18<1:11:44,  1.34it/s]Training epoch 1:  58% 8085/13868 [1:41:19<1:12:45,  1.32it/s]Training epoch 1:  58% 8086/13868 [1:41:20<1:13:04,  1.32it/s]Training epoch 1:  58% 8087/13868 [1:41:20<1:12:48,  1.32it/s]Training epoch 1:  58% 8088/13868 [1:41:21<1:12:40,  1.33it/s]Training epoch 1:  58% 8089/13868 [1:41:22<1:13:06,  1.32it/s]Training epoch 1:  58% 8090/13868 [1:41:23<1:13:02,  1.32it/s]Training epoch 1:  58% 8091/13868 [1:41:23<1:13:13,  1.31it/s]Training epoch 1:  58% 8092/13868 [1:41:24<1:13:49,  1.30it/s]Training epoch 1:  58% 8093/13868 [1:41:25<1:12:11,  1.33it/s]Training epoch 1:  58% 8094/13868 [1:41:26<1:10:57,  1.36it/s]Training epoch 1:  58% 8095/13868 [1:41:26<1:12:14,  1.33it/s]Training epoch 1:  58% 8096/13868 [1:41:27<1:11:46,  1.34it/s]Training epoch 1:  58% 8097/13868 [1:41:28<1:12:25,  1.33it/s]Training epoch 1:  58% 8098/13868 [1:41:29<1:11:44,  1.34it/s]Training epoch 1:  58% 8099/13868 [1:41:29<1:11:33,  1.34it/s]Training epoch 1:  58% 8100/13868 [1:41:30<1:15:34,  1.27it/s]Training epoch 1:  58% 8101/13868 [1:41:31<1:15:10,  1.28it/s]Training epoch 1:  58% 8102/13868 [1:41:32<1:13:48,  1.30it/s]Training epoch 1:  58% 8103/13868 [1:41:33<1:12:43,  1.32it/s]Training epoch 1:  58% 8104/13868 [1:41:33<1:13:14,  1.31it/s]Training epoch 1:  58% 8105/13868 [1:41:34<1:13:32,  1.31it/s]Training epoch 1:  58% 8106/13868 [1:41:35<1:12:38,  1.32it/s]Training epoch 1:  58% 8107/13868 [1:41:36<1:13:09,  1.31it/s]Training epoch 1:  58% 8108/13868 [1:41:36<1:11:17,  1.35it/s]Training epoch 1:  58% 8109/13868 [1:41:37<1:10:33,  1.36it/s]Training epoch 1:  58% 8110/13868 [1:41:38<1:11:02,  1.35it/s]Training epoch 1:  58% 8111/13868 [1:41:39<1:11:44,  1.34it/s]Training epoch 1:  58% 8112/13868 [1:41:39<1:11:40,  1.34it/s]Training epoch 1:  59% 8113/13868 [1:41:40<1:12:03,  1.33it/s]Training epoch 1:  59% 8114/13868 [1:41:41<1:11:50,  1.33it/s]Training epoch 1:  59% 8115/13868 [1:41:42<1:12:27,  1.32it/s]Training epoch 1:  59% 8116/13868 [1:41:42<1:11:52,  1.33it/s]Training epoch 1:  59% 8117/13868 [1:41:43<1:12:08,  1.33it/s]Training epoch 1:  59% 8118/13868 [1:41:44<1:11:34,  1.34it/s]Training epoch 1:  59% 8119/13868 [1:41:45<1:11:53,  1.33it/s]Training epoch 1:  59% 8120/13868 [1:41:45<1:11:24,  1.34it/s]Training epoch 1:  59% 8121/13868 [1:41:46<1:12:01,  1.33it/s]Training epoch 1:  59% 8122/13868 [1:41:47<1:13:04,  1.31it/s]Training epoch 1:  59% 8123/13868 [1:41:48<1:13:14,  1.31it/s]Training epoch 1:  59% 8124/13868 [1:41:48<1:13:38,  1.30it/s]Training epoch 1:  59% 8125/13868 [1:41:49<1:13:15,  1.31it/s]Training epoch 1:  59% 8126/13868 [1:41:50<1:12:19,  1.32it/s]Training epoch 1:  59% 8127/13868 [1:41:51<1:13:38,  1.30it/s]Training epoch 1:  59% 8128/13868 [1:41:51<1:12:42,  1.32it/s]Training epoch 1:  59% 8129/13868 [1:41:52<1:12:32,  1.32it/s]Training epoch 1:  59% 8130/13868 [1:41:53<1:12:50,  1.31it/s]Training epoch 1:  59% 8131/13868 [1:41:54<1:14:03,  1.29it/s]Training epoch 1:  59% 8132/13868 [1:41:54<1:14:12,  1.29it/s]Training epoch 1:  59% 8133/13868 [1:41:55<1:13:53,  1.29it/s]Training epoch 1:  59% 8134/13868 [1:41:56<1:13:51,  1.29it/s]Training epoch 1:  59% 8135/13868 [1:41:57<1:13:37,  1.30it/s]Training epoch 1:  59% 8136/13868 [1:41:58<1:12:17,  1.32it/s]Training epoch 1:  59% 8137/13868 [1:41:58<1:11:48,  1.33it/s]Training epoch 1:  59% 8138/13868 [1:41:59<1:12:13,  1.32it/s]Training epoch 1:  59% 8139/13868 [1:42:00<1:12:17,  1.32it/s]Training epoch 1:  59% 8140/13868 [1:42:01<1:12:13,  1.32it/s]Training epoch 1:  59% 8141/13868 [1:42:01<1:12:52,  1.31it/s]Training epoch 1:  59% 8142/13868 [1:42:02<1:11:29,  1.33it/s]Training epoch 1:  59% 8143/13868 [1:42:03<1:12:10,  1.32it/s]Training epoch 1:  59% 8144/13868 [1:42:04<1:12:10,  1.32it/s]Training epoch 1:  59% 8145/13868 [1:42:04<1:11:54,  1.33it/s]Training epoch 1:  59% 8146/13868 [1:42:05<1:10:31,  1.35it/s]Training epoch 1:  59% 8147/13868 [1:42:06<1:12:07,  1.32it/s]Training epoch 1:  59% 8148/13868 [1:42:07<1:10:45,  1.35it/s]Training epoch 1:  59% 8149/13868 [1:42:07<1:11:41,  1.33it/s]Training epoch 1:  59% 8150/13868 [1:42:08<1:10:55,  1.34it/s]Training epoch 1:  59% 8151/13868 [1:42:09<1:11:24,  1.33it/s]Training epoch 1:  59% 8152/13868 [1:42:10<1:10:44,  1.35it/s]Training epoch 1:  59% 8153/13868 [1:42:10<1:11:15,  1.34it/s]Training epoch 1:  59% 8154/13868 [1:42:11<1:10:32,  1.35it/s]Training epoch 1:  59% 8155/13868 [1:42:12<1:11:20,  1.33it/s]Training epoch 1:  59% 8156/13868 [1:42:12<1:10:50,  1.34it/s]Training epoch 1:  59% 8157/13868 [1:42:13<1:10:47,  1.34it/s]Training epoch 1:  59% 8158/13868 [1:42:14<1:10:34,  1.35it/s]Training epoch 1:  59% 8159/13868 [1:42:15<1:11:03,  1.34it/s]Training epoch 1:  59% 8160/13868 [1:42:15<1:10:38,  1.35it/s]Training epoch 1:  59% 8161/13868 [1:42:16<1:11:11,  1.34it/s]Training epoch 1:  59% 8162/13868 [1:42:17<1:11:52,  1.32it/s]Training epoch 1:  59% 8163/13868 [1:42:18<1:12:31,  1.31it/s]Training epoch 1:  59% 8164/13868 [1:42:19<1:11:51,  1.32it/s]Training epoch 1:  59% 8165/13868 [1:42:19<1:12:03,  1.32it/s]Training epoch 1:  59% 8166/13868 [1:42:20<1:11:12,  1.33it/s]Training epoch 1:  59% 8167/13868 [1:42:21<1:11:43,  1.32it/s]Training epoch 1:  59% 8168/13868 [1:42:22<1:12:34,  1.31it/s]Training epoch 1:  59% 8169/13868 [1:42:22<1:12:29,  1.31it/s]Training epoch 1:  59% 8170/13868 [1:42:23<1:12:48,  1.30it/s]Training epoch 1:  59% 8171/13868 [1:42:24<1:12:29,  1.31it/s]Training epoch 1:  59% 8172/13868 [1:42:25<1:12:25,  1.31it/s]Training epoch 1:  59% 8173/13868 [1:42:25<1:12:37,  1.31it/s]Training epoch 1:  59% 8174/13868 [1:42:26<1:12:26,  1.31it/s]Training epoch 1:  59% 8175/13868 [1:42:27<1:11:56,  1.32it/s]Training epoch 1:  59% 8176/13868 [1:42:28<1:11:59,  1.32it/s]Training epoch 1:  59% 8177/13868 [1:42:28<1:12:12,  1.31it/s]Training epoch 1:  59% 8178/13868 [1:42:29<1:12:20,  1.31it/s]Training epoch 1:  59% 8179/13868 [1:42:30<1:11:57,  1.32it/s]Training epoch 1:  59% 8180/13868 [1:42:31<1:12:31,  1.31it/s]Training epoch 1:  59% 8181/13868 [1:42:32<1:13:17,  1.29it/s]Training epoch 1:  59% 8182/13868 [1:42:32<1:13:18,  1.29it/s]Training epoch 1:  59% 8183/13868 [1:42:33<1:13:14,  1.29it/s]Training epoch 1:  59% 8184/13868 [1:42:34<1:13:11,  1.29it/s]Training epoch 1:  59% 8185/13868 [1:42:35<1:12:22,  1.31it/s]Training epoch 1:  59% 8186/13868 [1:42:35<1:10:56,  1.33it/s]Training epoch 1:  59% 8187/13868 [1:42:36<1:10:53,  1.34it/s]Training epoch 1:  59% 8188/13868 [1:42:37<1:09:41,  1.36it/s]Training epoch 1:  59% 8189/13868 [1:42:37<1:09:42,  1.36it/s]Training epoch 1:  59% 8190/13868 [1:42:38<1:09:57,  1.35it/s]Training epoch 1:  59% 8191/13868 [1:42:39<1:11:34,  1.32it/s]Training epoch 1:  59% 8192/13868 [1:42:40<1:11:58,  1.31it/s]Training epoch 1:  59% 8193/13868 [1:42:41<1:11:57,  1.31it/s]Training epoch 1:  59% 8194/13868 [1:42:41<1:11:41,  1.32it/s]Training epoch 1:  59% 8195/13868 [1:42:42<1:10:44,  1.34it/s]Training epoch 1:  59% 8196/13868 [1:42:43<1:09:47,  1.35it/s]Training epoch 1:  59% 8197/13868 [1:42:44<1:10:29,  1.34it/s]Training epoch 1:  59% 8198/13868 [1:42:44<1:10:31,  1.34it/s]Training epoch 1:  59% 8199/13868 [1:42:45<1:11:30,  1.32it/s]Training epoch 1:  59% 8200/13868 [1:42:46<1:15:42,  1.25it/s]Training epoch 1:  59% 8201/13868 [1:42:47<1:15:19,  1.25it/s]Training epoch 1:  59% 8202/13868 [1:42:47<1:13:06,  1.29it/s]Training epoch 1:  59% 8203/13868 [1:42:48<1:13:00,  1.29it/s]Training epoch 1:  59% 8204/13868 [1:42:49<1:12:46,  1.30it/s]Training epoch 1:  59% 8205/13868 [1:42:50<1:12:40,  1.30it/s]Training epoch 1:  59% 8206/13868 [1:42:50<1:11:41,  1.32it/s]Training epoch 1:  59% 8207/13868 [1:42:51<1:12:06,  1.31it/s]Training epoch 1:  59% 8208/13868 [1:42:52<1:11:17,  1.32it/s]Training epoch 1:  59% 8209/13868 [1:42:53<1:11:10,  1.33it/s]Training epoch 1:  59% 8210/13868 [1:42:54<1:11:36,  1.32it/s]Training epoch 1:  59% 8211/13868 [1:42:54<1:11:32,  1.32it/s]Training epoch 1:  59% 8212/13868 [1:42:55<1:11:23,  1.32it/s]Training epoch 1:  59% 8213/13868 [1:42:56<1:11:21,  1.32it/s]Training epoch 1:  59% 8214/13868 [1:42:57<1:10:41,  1.33it/s]Training epoch 1:  59% 8215/13868 [1:42:57<1:11:07,  1.32it/s]Training epoch 1:  59% 8216/13868 [1:42:58<1:11:20,  1.32it/s]Training epoch 1:  59% 8217/13868 [1:42:59<1:11:56,  1.31it/s]Training epoch 1:  59% 8218/13868 [1:43:00<1:12:31,  1.30it/s]Training epoch 1:  59% 8219/13868 [1:43:00<1:13:20,  1.28it/s]Training epoch 1:  59% 8220/13868 [1:43:01<1:12:23,  1.30it/s]Training epoch 1:  59% 8221/13868 [1:43:02<1:11:07,  1.32it/s]Training epoch 1:  59% 8222/13868 [1:43:03<1:10:14,  1.34it/s]Training epoch 1:  59% 8223/13868 [1:43:03<1:09:42,  1.35it/s]Training epoch 1:  59% 8224/13868 [1:43:04<1:10:06,  1.34it/s]Training epoch 1:  59% 8225/13868 [1:43:05<1:10:44,  1.33it/s]Training epoch 1:  59% 8226/13868 [1:43:06<1:10:49,  1.33it/s]Training epoch 1:  59% 8227/13868 [1:43:06<1:11:28,  1.32it/s]Training epoch 1:  59% 8228/13868 [1:43:07<1:10:24,  1.34it/s]Training epoch 1:  59% 8229/13868 [1:43:08<1:11:29,  1.31it/s]Training epoch 1:  59% 8230/13868 [1:43:09<1:11:17,  1.32it/s]Training epoch 1:  59% 8231/13868 [1:43:09<1:10:57,  1.32it/s]Training epoch 1:  59% 8232/13868 [1:43:10<1:09:13,  1.36it/s]Training epoch 1:  59% 8233/13868 [1:43:11<1:10:14,  1.34it/s]Training epoch 1:  59% 8234/13868 [1:43:12<1:10:25,  1.33it/s]Training epoch 1:  59% 8235/13868 [1:43:12<1:10:24,  1.33it/s]Training epoch 1:  59% 8236/13868 [1:43:13<1:10:49,  1.33it/s]Training epoch 1:  59% 8237/13868 [1:43:14<1:11:20,  1.32it/s]Training epoch 1:  59% 8238/13868 [1:43:15<1:09:46,  1.34it/s]Training epoch 1:  59% 8239/13868 [1:43:15<1:09:14,  1.35it/s]Training epoch 1:  59% 8240/13868 [1:43:16<1:09:37,  1.35it/s]Training epoch 1:  59% 8241/13868 [1:43:17<1:09:55,  1.34it/s]Training epoch 1:  59% 8242/13868 [1:43:18<1:11:03,  1.32it/s]Training epoch 1:  59% 8243/13868 [1:43:18<1:11:42,  1.31it/s]Training epoch 1:  59% 8244/13868 [1:43:19<1:11:14,  1.32it/s]Training epoch 1:  59% 8245/13868 [1:43:20<1:10:31,  1.33it/s]Training epoch 1:  59% 8246/13868 [1:43:21<1:11:18,  1.31it/s]Training epoch 1:  59% 8247/13868 [1:43:21<1:11:22,  1.31it/s]Training epoch 1:  59% 8248/13868 [1:43:22<1:10:53,  1.32it/s]Training epoch 1:  59% 8249/13868 [1:43:23<1:11:35,  1.31it/s]Training epoch 1:  59% 8250/13868 [1:43:24<1:10:17,  1.33it/s]Training epoch 1:  59% 8251/13868 [1:43:24<1:09:28,  1.35it/s]Training epoch 1:  60% 8252/13868 [1:43:25<1:09:12,  1.35it/s]Training epoch 1:  60% 8253/13868 [1:43:26<1:10:12,  1.33it/s]Training epoch 1:  60% 8254/13868 [1:43:27<1:09:58,  1.34it/s]Training epoch 1:  60% 8255/13868 [1:43:27<1:10:54,  1.32it/s]Training epoch 1:  60% 8256/13868 [1:43:28<1:10:24,  1.33it/s]Training epoch 1:  60% 8257/13868 [1:43:29<1:10:52,  1.32it/s]Training epoch 1:  60% 8258/13868 [1:43:30<1:10:31,  1.33it/s]Training epoch 1:  60% 8259/13868 [1:43:31<1:12:12,  1.29it/s]Training epoch 1:  60% 8260/13868 [1:43:31<1:12:28,  1.29it/s]Training epoch 1:  60% 8261/13868 [1:43:32<1:12:12,  1.29it/s]Training epoch 1:  60% 8262/13868 [1:43:33<1:10:35,  1.32it/s]Training epoch 1:  60% 8263/13868 [1:43:34<1:09:33,  1.34it/s]Training epoch 1:  60% 8264/13868 [1:43:34<1:08:50,  1.36it/s]Training epoch 1:  60% 8265/13868 [1:43:35<1:09:58,  1.33it/s]Training epoch 1:  60% 8266/13868 [1:43:36<1:09:59,  1.33it/s]Training epoch 1:  60% 8267/13868 [1:43:37<1:10:58,  1.32it/s]Training epoch 1:  60% 8268/13868 [1:43:37<1:10:05,  1.33it/s]Training epoch 1:  60% 8269/13868 [1:43:38<1:10:11,  1.33it/s]Training epoch 1:  60% 8270/13868 [1:43:39<1:10:30,  1.32it/s]Training epoch 1:  60% 8271/13868 [1:43:40<1:10:47,  1.32it/s]Training epoch 1:  60% 8272/13868 [1:43:40<1:10:00,  1.33it/s]Training epoch 1:  60% 8273/13868 [1:43:41<1:10:35,  1.32it/s]Training epoch 1:  60% 8274/13868 [1:43:42<1:10:12,  1.33it/s]Training epoch 1:  60% 8275/13868 [1:43:43<1:10:12,  1.33it/s]Training epoch 1:  60% 8276/13868 [1:43:43<1:10:13,  1.33it/s]Training epoch 1:  60% 8277/13868 [1:43:44<1:10:39,  1.32it/s]Training epoch 1:  60% 8278/13868 [1:43:45<1:10:24,  1.32it/s]Training epoch 1:  60% 8279/13868 [1:43:46<1:09:49,  1.33it/s]Training epoch 1:  60% 8280/13868 [1:43:46<1:09:59,  1.33it/s]Training epoch 1:  60% 8281/13868 [1:43:47<1:10:21,  1.32it/s]Training epoch 1:  60% 8282/13868 [1:43:48<1:10:18,  1.32it/s]Training epoch 1:  60% 8283/13868 [1:43:49<1:09:45,  1.33it/s]Training epoch 1:  60% 8284/13868 [1:43:49<1:09:51,  1.33it/s]Training epoch 1:  60% 8285/13868 [1:43:50<1:10:06,  1.33it/s]Training epoch 1:  60% 8286/13868 [1:43:51<1:10:01,  1.33it/s]Training epoch 1:  60% 8287/13868 [1:43:52<1:10:09,  1.33it/s]Training epoch 1:  60% 8288/13868 [1:43:52<1:10:26,  1.32it/s]Training epoch 1:  60% 8289/13868 [1:43:53<1:10:29,  1.32it/s]Training epoch 1:  60% 8290/13868 [1:43:54<1:10:27,  1.32it/s]Training epoch 1:  60% 8291/13868 [1:43:55<1:10:59,  1.31it/s]Training epoch 1:  60% 8292/13868 [1:43:55<1:11:13,  1.30it/s]Training epoch 1:  60% 8293/13868 [1:43:56<1:11:28,  1.30it/s]Training epoch 1:  60% 8294/13868 [1:43:57<1:11:34,  1.30it/s]Training epoch 1:  60% 8295/13868 [1:43:58<1:11:28,  1.30it/s]Training epoch 1:  60% 8296/13868 [1:43:59<1:11:31,  1.30it/s]Training epoch 1:  60% 8297/13868 [1:43:59<1:10:26,  1.32it/s]Training epoch 1:  60% 8298/13868 [1:44:00<1:11:21,  1.30it/s]Training epoch 1:  60% 8299/13868 [1:44:01<1:10:47,  1.31it/s]Training epoch 1:  60% 8300/13868 [1:44:02<1:18:18,  1.18it/s]Training epoch 1:  60% 8301/13868 [1:44:03<1:15:33,  1.23it/s]Training epoch 1:  60% 8302/13868 [1:44:03<1:14:29,  1.25it/s]Training epoch 1:  60% 8303/13868 [1:44:04<1:14:01,  1.25it/s]Training epoch 1:  60% 8304/13868 [1:44:05<1:13:38,  1.26it/s]Training epoch 1:  60% 8305/13868 [1:44:06<1:11:56,  1.29it/s]Training epoch 1:  60% 8306/13868 [1:44:06<1:12:29,  1.28it/s]Training epoch 1:  60% 8307/13868 [1:44:07<1:11:46,  1.29it/s]Training epoch 1:  60% 8308/13868 [1:44:08<1:10:45,  1.31it/s]Training epoch 1:  60% 8309/13868 [1:44:09<1:10:28,  1.31it/s]Training epoch 1:  60% 8310/13868 [1:44:09<1:11:07,  1.30it/s]Training epoch 1:  60% 8311/13868 [1:44:10<1:09:40,  1.33it/s]Training epoch 1:  60% 8312/13868 [1:44:11<1:10:23,  1.32it/s]Training epoch 1:  60% 8313/13868 [1:44:12<1:09:31,  1.33it/s]Training epoch 1:  60% 8314/13868 [1:44:13<1:10:51,  1.31it/s]Training epoch 1:  60% 8315/13868 [1:44:13<1:10:35,  1.31it/s]Training epoch 1:  60% 8316/13868 [1:44:14<1:10:32,  1.31it/s]Training epoch 1:  60% 8317/13868 [1:44:15<1:10:27,  1.31it/s]Training epoch 1:  60% 8318/13868 [1:44:16<1:10:31,  1.31it/s]Training epoch 1:  60% 8319/13868 [1:44:16<1:10:27,  1.31it/s]Training epoch 1:  60% 8320/13868 [1:44:17<1:09:43,  1.33it/s]Training epoch 1:  60% 8321/13868 [1:44:18<1:09:05,  1.34it/s]Training epoch 1:  60% 8322/13868 [1:44:19<1:09:54,  1.32it/s]Training epoch 1:  60% 8323/13868 [1:44:19<1:09:53,  1.32it/s]Training epoch 1:  60% 8324/13868 [1:44:20<1:10:31,  1.31it/s]Training epoch 1:  60% 8325/13868 [1:44:21<1:10:05,  1.32it/s]Training epoch 1:  60% 8326/13868 [1:44:22<1:10:22,  1.31it/s]Training epoch 1:  60% 8327/13868 [1:44:22<1:10:07,  1.32it/s]Training epoch 1:  60% 8328/13868 [1:44:23<1:10:14,  1.31it/s]Training epoch 1:  60% 8329/13868 [1:44:24<1:09:15,  1.33it/s]Training epoch 1:  60% 8330/13868 [1:44:25<1:09:29,  1.33it/s]Training epoch 1:  60% 8331/13868 [1:44:25<1:09:54,  1.32it/s]Training epoch 1:  60% 8332/13868 [1:44:26<1:10:14,  1.31it/s]Training epoch 1:  60% 8333/13868 [1:44:27<1:09:56,  1.32it/s]Training epoch 1:  60% 8334/13868 [1:44:28<1:09:46,  1.32it/s]Training epoch 1:  60% 8335/13868 [1:44:28<1:08:43,  1.34it/s]Training epoch 1:  60% 8336/13868 [1:44:29<1:09:09,  1.33it/s]Training epoch 1:  60% 8337/13868 [1:44:30<1:09:08,  1.33it/s]Training epoch 1:  60% 8338/13868 [1:44:31<1:08:52,  1.34it/s]Training epoch 1:  60% 8339/13868 [1:44:31<1:08:34,  1.34it/s]Training epoch 1:  60% 8340/13868 [1:44:32<1:09:33,  1.32it/s]Training epoch 1:  60% 8341/13868 [1:44:33<1:09:15,  1.33it/s]Training epoch 1:  60% 8342/13868 [1:44:34<1:10:17,  1.31it/s]Training epoch 1:  60% 8343/13868 [1:44:34<1:09:53,  1.32it/s]Training epoch 1:  60% 8344/13868 [1:44:35<1:09:16,  1.33it/s]Training epoch 1:  60% 8345/13868 [1:44:36<1:09:30,  1.32it/s]Training epoch 1:  60% 8346/13868 [1:44:37<1:08:42,  1.34it/s]Training epoch 1:  60% 8347/13868 [1:44:37<1:09:25,  1.33it/s]Training epoch 1:  60% 8348/13868 [1:44:38<1:09:52,  1.32it/s]Training epoch 1:  60% 8349/13868 [1:44:39<1:09:20,  1.33it/s]Training epoch 1:  60% 8350/13868 [1:44:40<1:09:54,  1.32it/s]Training epoch 1:  60% 8351/13868 [1:44:40<1:09:31,  1.32it/s]Training epoch 1:  60% 8352/13868 [1:44:41<1:09:19,  1.33it/s]Training epoch 1:  60% 8353/13868 [1:44:42<1:09:58,  1.31it/s]Training epoch 1:  60% 8354/13868 [1:44:43<1:09:55,  1.31it/s]Training epoch 1:  60% 8355/13868 [1:44:44<1:10:25,  1.30it/s]Training epoch 1:  60% 8356/13868 [1:44:44<1:10:30,  1.30it/s]Training epoch 1:  60% 8357/13868 [1:44:45<1:09:33,  1.32it/s]Training epoch 1:  60% 8358/13868 [1:44:46<1:10:05,  1.31it/s]Training epoch 1:  60% 8359/13868 [1:44:47<1:09:13,  1.33it/s]Training epoch 1:  60% 8360/13868 [1:44:47<1:10:15,  1.31it/s]Training epoch 1:  60% 8361/13868 [1:44:48<1:10:20,  1.30it/s]Training epoch 1:  60% 8362/13868 [1:44:49<1:09:04,  1.33it/s]Training epoch 1:  60% 8363/13868 [1:44:50<1:10:00,  1.31it/s]Training epoch 1:  60% 8364/13868 [1:44:50<1:10:55,  1.29it/s]Training epoch 1:  60% 8365/13868 [1:44:51<1:09:56,  1.31it/s]Training epoch 1:  60% 8366/13868 [1:44:52<1:09:46,  1.31it/s]Training epoch 1:  60% 8367/13868 [1:44:53<1:09:23,  1.32it/s]Training epoch 1:  60% 8368/13868 [1:44:53<1:08:43,  1.33it/s]Training epoch 1:  60% 8369/13868 [1:44:54<1:09:29,  1.32it/s]Training epoch 1:  60% 8370/13868 [1:44:55<1:10:09,  1.31it/s]Training epoch 1:  60% 8371/13868 [1:44:56<1:09:28,  1.32it/s]Training epoch 1:  60% 8372/13868 [1:44:56<1:09:49,  1.31it/s]Training epoch 1:  60% 8373/13868 [1:44:57<1:09:11,  1.32it/s]Training epoch 1:  60% 8374/13868 [1:44:58<1:09:03,  1.33it/s]Training epoch 1:  60% 8375/13868 [1:44:59<1:09:39,  1.31it/s]Training epoch 1:  60% 8376/13868 [1:45:00<1:10:30,  1.30it/s]Training epoch 1:  60% 8377/13868 [1:45:00<1:09:17,  1.32it/s]Training epoch 1:  60% 8378/13868 [1:45:01<1:09:34,  1.32it/s]Training epoch 1:  60% 8379/13868 [1:45:02<1:08:28,  1.34it/s]Training epoch 1:  60% 8380/13868 [1:45:03<1:09:17,  1.32it/s]Training epoch 1:  60% 8381/13868 [1:45:03<1:09:27,  1.32it/s]Training epoch 1:  60% 8382/13868 [1:45:04<1:09:48,  1.31it/s]Training epoch 1:  60% 8383/13868 [1:45:05<1:09:31,  1.32it/s]Training epoch 1:  60% 8384/13868 [1:45:06<1:09:44,  1.31it/s]Training epoch 1:  60% 8385/13868 [1:45:06<1:09:34,  1.31it/s]Training epoch 1:  60% 8386/13868 [1:45:07<1:09:27,  1.32it/s]Training epoch 1:  60% 8387/13868 [1:45:08<1:08:40,  1.33it/s]Training epoch 1:  60% 8388/13868 [1:45:09<1:09:30,  1.31it/s]Training epoch 1:  60% 8389/13868 [1:45:09<1:09:04,  1.32it/s]Training epoch 1:  60% 8390/13868 [1:45:10<1:09:34,  1.31it/s]Training epoch 1:  61% 8391/13868 [1:45:11<1:08:37,  1.33it/s]Training epoch 1:  61% 8392/13868 [1:45:12<1:09:26,  1.31it/s]Training epoch 1:  61% 8393/13868 [1:45:12<1:08:18,  1.34it/s]Training epoch 1:  61% 8394/13868 [1:45:13<1:09:10,  1.32it/s]Training epoch 1:  61% 8395/13868 [1:45:14<1:09:48,  1.31it/s]Training epoch 1:  61% 8396/13868 [1:45:15<1:10:23,  1.30it/s]Training epoch 1:  61% 8397/13868 [1:45:15<1:10:24,  1.30it/s]Training epoch 1:  61% 8398/13868 [1:45:16<1:10:15,  1.30it/s]Training epoch 1:  61% 8399/13868 [1:45:17<1:08:58,  1.32it/s]Training epoch 1:  61% 8400/13868 [1:45:18<1:13:35,  1.24it/s]Training epoch 1:  61% 8401/13868 [1:45:19<1:11:47,  1.27it/s]Training epoch 1:  61% 8402/13868 [1:45:19<1:10:37,  1.29it/s]Training epoch 1:  61% 8403/13868 [1:45:20<1:10:30,  1.29it/s]Training epoch 1:  61% 8404/13868 [1:45:21<1:09:59,  1.30it/s]Training epoch 1:  61% 8405/13868 [1:45:22<1:09:53,  1.30it/s]Training epoch 1:  61% 8406/13868 [1:45:22<1:08:45,  1.32it/s]Training epoch 1:  61% 8407/13868 [1:45:23<1:08:37,  1.33it/s]Training epoch 1:  61% 8408/13868 [1:45:24<1:08:40,  1.33it/s]Training epoch 1:  61% 8409/13868 [1:45:25<1:08:40,  1.32it/s]Training epoch 1:  61% 8410/13868 [1:45:25<1:09:05,  1.32it/s]Training epoch 1:  61% 8411/13868 [1:45:26<1:08:04,  1.34it/s]Training epoch 1:  61% 8412/13868 [1:45:27<1:08:34,  1.33it/s]Training epoch 1:  61% 8413/13868 [1:45:28<1:08:45,  1.32it/s]Training epoch 1:  61% 8414/13868 [1:45:28<1:09:04,  1.32it/s]Training epoch 1:  61% 8415/13868 [1:45:29<1:08:14,  1.33it/s]Training epoch 1:  61% 8416/13868 [1:45:30<1:08:20,  1.33it/s]Training epoch 1:  61% 8417/13868 [1:45:31<1:09:09,  1.31it/s]Training epoch 1:  61% 8418/13868 [1:45:31<1:08:20,  1.33it/s]Training epoch 1:  61% 8419/13868 [1:45:32<1:08:23,  1.33it/s]Training epoch 1:  61% 8420/13868 [1:45:33<1:08:18,  1.33it/s]Training epoch 1:  61% 8421/13868 [1:45:34<1:07:35,  1.34it/s]Training epoch 1:  61% 8422/13868 [1:45:34<1:08:00,  1.33it/s]Training epoch 1:  61% 8423/13868 [1:45:35<1:08:40,  1.32it/s]Training epoch 1:  61% 8424/13868 [1:45:36<1:08:45,  1.32it/s]Training epoch 1:  61% 8425/13868 [1:45:37<1:08:59,  1.31it/s]Training epoch 1:  61% 8426/13868 [1:45:37<1:08:04,  1.33it/s]Training epoch 1:  61% 8427/13868 [1:45:38<1:07:38,  1.34it/s]Training epoch 1:  61% 8428/13868 [1:45:39<1:08:48,  1.32it/s]Training epoch 1:  61% 8429/13868 [1:45:40<1:08:43,  1.32it/s]Training epoch 1:  61% 8430/13868 [1:45:41<1:08:53,  1.32it/s]Training epoch 1:  61% 8431/13868 [1:45:41<1:09:11,  1.31it/s]Training epoch 1:  61% 8432/13868 [1:45:42<1:08:45,  1.32it/s]Training epoch 1:  61% 8433/13868 [1:45:43<1:08:26,  1.32it/s]Training epoch 1:  61% 8434/13868 [1:45:44<1:08:40,  1.32it/s]Training epoch 1:  61% 8435/13868 [1:45:44<1:08:43,  1.32it/s]Training epoch 1:  61% 8436/13868 [1:45:45<1:09:18,  1.31it/s]Training epoch 1:  61% 8437/13868 [1:45:46<1:09:16,  1.31it/s]Training epoch 1:  61% 8438/13868 [1:45:47<1:09:38,  1.30it/s]Training epoch 1:  61% 8439/13868 [1:45:47<1:10:23,  1.29it/s]Training epoch 1:  61% 8440/13868 [1:45:48<1:10:38,  1.28it/s]Training epoch 1:  61% 8441/13868 [1:45:49<1:09:15,  1.31it/s]Training epoch 1:  61% 8442/13868 [1:45:50<1:09:01,  1.31it/s]Training epoch 1:  61% 8443/13868 [1:45:50<1:09:40,  1.30it/s]Training epoch 1:  61% 8444/13868 [1:45:51<1:09:52,  1.29it/s]Training epoch 1:  61% 8445/13868 [1:45:52<1:09:10,  1.31it/s]Training epoch 1:  61% 8446/13868 [1:45:53<1:09:44,  1.30it/s]Training epoch 1:  61% 8447/13868 [1:45:54<1:08:38,  1.32it/s]Training epoch 1:  61% 8448/13868 [1:45:54<1:08:39,  1.32it/s]Training epoch 1:  61% 8449/13868 [1:45:55<1:08:43,  1.31it/s]Training epoch 1:  61% 8450/13868 [1:45:56<1:08:08,  1.33it/s]Training epoch 1:  61% 8451/13868 [1:45:57<1:08:39,  1.31it/s]Training epoch 1:  61% 8452/13868 [1:45:57<1:08:54,  1.31it/s]Training epoch 1:  61% 8453/13868 [1:45:58<1:09:06,  1.31it/s]Training epoch 1:  61% 8454/13868 [1:45:59<1:08:48,  1.31it/s]Training epoch 1:  61% 8455/13868 [1:46:00<1:08:09,  1.32it/s]Training epoch 1:  61% 8456/13868 [1:46:00<1:08:33,  1.32it/s]Training epoch 1:  61% 8457/13868 [1:46:01<1:09:30,  1.30it/s]Training epoch 1:  61% 8458/13868 [1:46:02<1:09:47,  1.29it/s]Training epoch 1:  61% 8459/13868 [1:46:03<1:09:05,  1.30it/s]Training epoch 1:  61% 8460/13868 [1:46:03<1:08:58,  1.31it/s]Training epoch 1:  61% 8461/13868 [1:46:04<1:08:25,  1.32it/s]Training epoch 1:  61% 8462/13868 [1:46:05<1:07:36,  1.33it/s]Training epoch 1:  61% 8463/13868 [1:46:06<1:07:41,  1.33it/s]Training epoch 1:  61% 8464/13868 [1:46:06<1:08:01,  1.32it/s]Training epoch 1:  61% 8465/13868 [1:46:07<1:07:43,  1.33it/s]Training epoch 1:  61% 8466/13868 [1:46:08<1:07:58,  1.32it/s]Training epoch 1:  61% 8467/13868 [1:46:09<1:07:48,  1.33it/s]Training epoch 1:  61% 8468/13868 [1:46:09<1:07:21,  1.34it/s]Training epoch 1:  61% 8469/13868 [1:46:10<1:07:36,  1.33it/s]Training epoch 1:  61% 8470/13868 [1:46:11<1:08:52,  1.31it/s]Training epoch 1:  61% 8471/13868 [1:46:12<1:07:19,  1.34it/s]Training epoch 1:  61% 8472/13868 [1:46:12<1:07:02,  1.34it/s]Training epoch 1:  61% 8473/13868 [1:46:13<1:06:34,  1.35it/s]Training epoch 1:  61% 8474/13868 [1:46:14<1:07:14,  1.34it/s]Training epoch 1:  61% 8475/13868 [1:46:15<1:08:18,  1.32it/s]Training epoch 1:  61% 8476/13868 [1:46:15<1:08:09,  1.32it/s]Training epoch 1:  61% 8477/13868 [1:46:16<1:08:12,  1.32it/s]Training epoch 1:  61% 8478/13868 [1:46:17<1:09:03,  1.30it/s]Training epoch 1:  61% 8479/13868 [1:46:18<1:08:47,  1.31it/s]Training epoch 1:  61% 8480/13868 [1:46:19<1:08:45,  1.31it/s]Training epoch 1:  61% 8481/13868 [1:46:19<1:08:45,  1.31it/s]Training epoch 1:  61% 8482/13868 [1:46:20<1:09:22,  1.29it/s]Training epoch 1:  61% 8483/13868 [1:46:21<1:08:56,  1.30it/s]Training epoch 1:  61% 8484/13868 [1:46:22<1:08:50,  1.30it/s]Training epoch 1:  61% 8485/13868 [1:46:22<1:08:19,  1.31it/s]Training epoch 1:  61% 8486/13868 [1:46:23<1:08:02,  1.32it/s]Training epoch 1:  61% 8487/13868 [1:46:24<1:06:59,  1.34it/s]Training epoch 1:  61% 8488/13868 [1:46:25<1:06:50,  1.34it/s]Training epoch 1:  61% 8489/13868 [1:46:25<1:06:32,  1.35it/s]Training epoch 1:  61% 8490/13868 [1:46:26<1:07:26,  1.33it/s]Training epoch 1:  61% 8491/13868 [1:46:27<1:08:31,  1.31it/s]Training epoch 1:  61% 8492/13868 [1:46:28<1:08:53,  1.30it/s]Training epoch 1:  61% 8493/13868 [1:46:28<1:08:20,  1.31it/s]Training epoch 1:  61% 8494/13868 [1:46:29<1:08:39,  1.30it/s]Training epoch 1:  61% 8495/13868 [1:46:30<1:07:43,  1.32it/s]Training epoch 1:  61% 8496/13868 [1:46:31<1:08:21,  1.31it/s]Training epoch 1:  61% 8497/13868 [1:46:31<1:08:12,  1.31it/s]Training epoch 1:  61% 8498/13868 [1:46:32<1:08:30,  1.31it/s]Training epoch 1:  61% 8499/13868 [1:46:33<1:09:19,  1.29it/s]Training epoch 1:  61% 8500/13868 [1:46:34<1:12:46,  1.23it/s]Training epoch 1:  61% 8501/13868 [1:46:35<1:10:40,  1.27it/s]Training epoch 1:  61% 8502/13868 [1:46:35<1:09:46,  1.28it/s]Training epoch 1:  61% 8503/13868 [1:46:36<1:09:15,  1.29it/s]Training epoch 1:  61% 8504/13868 [1:46:37<1:08:43,  1.30it/s]Training epoch 1:  61% 8505/13868 [1:46:38<1:08:18,  1.31it/s]Training epoch 1:  61% 8506/13868 [1:46:38<1:06:50,  1.34it/s]Training epoch 1:  61% 8507/13868 [1:46:39<1:06:53,  1.34it/s]Training epoch 1:  61% 8508/13868 [1:46:40<1:06:29,  1.34it/s]Training epoch 1:  61% 8509/13868 [1:46:41<1:07:14,  1.33it/s]Training epoch 1:  61% 8510/13868 [1:46:41<1:07:45,  1.32it/s]Training epoch 1:  61% 8511/13868 [1:46:42<1:07:42,  1.32it/s]Training epoch 1:  61% 8512/13868 [1:46:43<1:08:04,  1.31it/s]Training epoch 1:  61% 8513/13868 [1:46:44<1:08:45,  1.30it/s]Training epoch 1:  61% 8514/13868 [1:46:45<1:08:03,  1.31it/s]Training epoch 1:  61% 8515/13868 [1:46:45<1:07:55,  1.31it/s]Training epoch 1:  61% 8516/13868 [1:46:46<1:07:28,  1.32it/s]Training epoch 1:  61% 8517/13868 [1:46:47<1:06:55,  1.33it/s]Training epoch 1:  61% 8518/13868 [1:46:48<1:07:36,  1.32it/s]Training epoch 1:  61% 8519/13868 [1:46:48<1:08:07,  1.31it/s]Training epoch 1:  61% 8520/13868 [1:46:49<1:07:17,  1.32it/s]Training epoch 1:  61% 8521/13868 [1:46:50<1:07:35,  1.32it/s]Training epoch 1:  61% 8522/13868 [1:46:51<1:07:15,  1.32it/s]Training epoch 1:  61% 8523/13868 [1:46:51<1:06:57,  1.33it/s]Training epoch 1:  61% 8524/13868 [1:46:52<1:06:22,  1.34it/s]Training epoch 1:  61% 8525/13868 [1:46:53<1:06:11,  1.35it/s]Training epoch 1:  61% 8526/13868 [1:46:54<1:05:51,  1.35it/s]Training epoch 1:  61% 8527/13868 [1:46:54<1:06:39,  1.34it/s]Training epoch 1:  61% 8528/13868 [1:46:55<1:07:07,  1.33it/s]Training epoch 1:  62% 8529/13868 [1:46:56<1:06:50,  1.33it/s]Training epoch 1:  62% 8530/13868 [1:46:57<1:06:22,  1.34it/s]Training epoch 1:  62% 8531/13868 [1:46:57<1:07:01,  1.33it/s]Training epoch 1:  62% 8532/13868 [1:46:58<1:07:01,  1.33it/s]Training epoch 1:  62% 8533/13868 [1:46:59<1:07:47,  1.31it/s]Training epoch 1:  62% 8534/13868 [1:47:00<1:08:29,  1.30it/s]Training epoch 1:  62% 8535/13868 [1:47:00<1:07:52,  1.31it/s]Training epoch 1:  62% 8536/13868 [1:47:01<1:07:26,  1.32it/s]Training epoch 1:  62% 8537/13868 [1:47:02<1:07:53,  1.31it/s]Training epoch 1:  62% 8538/13868 [1:47:03<1:07:26,  1.32it/s]Training epoch 1:  62% 8539/13868 [1:47:03<1:07:10,  1.32it/s]Training epoch 1:  62% 8540/13868 [1:47:04<1:07:18,  1.32it/s]Training epoch 1:  62% 8541/13868 [1:47:05<1:06:42,  1.33it/s]Training epoch 1:  62% 8542/13868 [1:47:06<1:06:38,  1.33it/s]Training epoch 1:  62% 8543/13868 [1:47:06<1:06:32,  1.33it/s]Training epoch 1:  62% 8544/13868 [1:47:07<1:06:15,  1.34it/s]Training epoch 1:  62% 8545/13868 [1:47:08<1:05:55,  1.35it/s]Training epoch 1:  62% 8546/13868 [1:47:09<1:06:02,  1.34it/s]Training epoch 1:  62% 8547/13868 [1:47:09<1:05:32,  1.35it/s]Training epoch 1:  62% 8548/13868 [1:47:10<1:05:28,  1.35it/s]Training epoch 1:  62% 8549/13868 [1:47:11<1:06:10,  1.34it/s]Training epoch 1:  62% 8550/13868 [1:47:12<1:07:00,  1.32it/s]Training epoch 1:  62% 8551/13868 [1:47:12<1:07:34,  1.31it/s]Training epoch 1:  62% 8552/13868 [1:47:13<1:07:50,  1.31it/s]Training epoch 1:  62% 8553/13868 [1:47:14<1:06:15,  1.34it/s]Training epoch 1:  62% 8554/13868 [1:47:15<1:05:41,  1.35it/s]Training epoch 1:  62% 8555/13868 [1:47:15<1:05:24,  1.35it/s]Training epoch 1:  62% 8556/13868 [1:47:16<1:05:18,  1.36it/s]Training epoch 1:  62% 8557/13868 [1:47:17<1:05:23,  1.35it/s]Training epoch 1:  62% 8558/13868 [1:47:18<1:05:29,  1.35it/s]Training epoch 1:  62% 8559/13868 [1:47:18<1:05:36,  1.35it/s]Training epoch 1:  62% 8560/13868 [1:47:19<1:05:59,  1.34it/s]Training epoch 1:  62% 8561/13868 [1:47:20<1:05:38,  1.35it/s]Training epoch 1:  62% 8562/13868 [1:47:21<1:05:46,  1.34it/s]Training epoch 1:  62% 8563/13868 [1:47:21<1:06:30,  1.33it/s]Training epoch 1:  62% 8564/13868 [1:47:22<1:05:44,  1.34it/s]Training epoch 1:  62% 8565/13868 [1:47:23<1:05:42,  1.35it/s]Training epoch 1:  62% 8566/13868 [1:47:23<1:05:21,  1.35it/s]Training epoch 1:  62% 8567/13868 [1:47:24<1:05:19,  1.35it/s]Training epoch 1:  62% 8568/13868 [1:47:25<1:05:33,  1.35it/s]Training epoch 1:  62% 8569/13868 [1:47:26<1:07:06,  1.32it/s]Training epoch 1:  62% 8570/13868 [1:47:27<1:06:28,  1.33it/s]Training epoch 1:  62% 8571/13868 [1:47:27<1:06:16,  1.33it/s]Training epoch 1:  62% 8572/13868 [1:47:28<1:05:47,  1.34it/s]Training epoch 1:  62% 8573/13868 [1:47:29<1:05:22,  1.35it/s]Training epoch 1:  62% 8574/13868 [1:47:29<1:06:02,  1.34it/s]Training epoch 1:  62% 8575/13868 [1:47:30<1:06:15,  1.33it/s]Training epoch 1:  62% 8576/13868 [1:47:31<1:05:54,  1.34it/s]Training epoch 1:  62% 8577/13868 [1:47:32<1:05:33,  1.35it/s]Training epoch 1:  62% 8578/13868 [1:47:32<1:05:08,  1.35it/s]Training epoch 1:  62% 8579/13868 [1:47:33<1:05:48,  1.34it/s]Training epoch 1:  62% 8580/13868 [1:47:34<1:06:59,  1.32it/s]Training epoch 1:  62% 8581/13868 [1:47:35<1:07:09,  1.31it/s]Training epoch 1:  62% 8582/13868 [1:47:36<1:07:22,  1.31it/s]Training epoch 1:  62% 8583/13868 [1:47:36<1:06:50,  1.32it/s]Training epoch 1:  62% 8584/13868 [1:47:37<1:06:34,  1.32it/s]Training epoch 1:  62% 8585/13868 [1:47:38<1:06:23,  1.33it/s]Training epoch 1:  62% 8586/13868 [1:47:39<1:06:16,  1.33it/s]Training epoch 1:  62% 8587/13868 [1:47:39<1:05:55,  1.33it/s]Training epoch 1:  62% 8588/13868 [1:47:40<1:05:37,  1.34it/s]Training epoch 1:  62% 8589/13868 [1:47:41<1:05:45,  1.34it/s]Training epoch 1:  62% 8590/13868 [1:47:42<1:05:48,  1.34it/s]Training epoch 1:  62% 8591/13868 [1:47:42<1:05:16,  1.35it/s]Training epoch 1:  62% 8592/13868 [1:47:43<1:05:23,  1.34it/s]Training epoch 1:  62% 8593/13868 [1:47:44<1:05:24,  1.34it/s]Training epoch 1:  62% 8594/13868 [1:47:44<1:05:02,  1.35it/s]Training epoch 1:  62% 8595/13868 [1:47:45<1:05:07,  1.35it/s]Training epoch 1:  62% 8596/13868 [1:47:46<1:05:15,  1.35it/s]Training epoch 1:  62% 8597/13868 [1:47:47<1:06:13,  1.33it/s]Training epoch 1:  62% 8598/13868 [1:47:48<1:06:31,  1.32it/s]Training epoch 1:  62% 8599/13868 [1:47:48<1:07:20,  1.30it/s]Training epoch 1:  62% 8600/13868 [1:47:49<1:11:11,  1.23it/s]Training epoch 1:  62% 8601/13868 [1:47:50<1:09:29,  1.26it/s]Training epoch 1:  62% 8602/13868 [1:47:51<1:09:14,  1.27it/s]Training epoch 1:  62% 8603/13868 [1:47:52<1:08:43,  1.28it/s]Training epoch 1:  62% 8604/13868 [1:47:52<1:07:39,  1.30it/s]Training epoch 1:  62% 8605/13868 [1:47:53<1:07:27,  1.30it/s]Training epoch 1:  62% 8606/13868 [1:47:54<1:06:36,  1.32it/s]Training epoch 1:  62% 8607/13868 [1:47:54<1:05:58,  1.33it/s]Training epoch 1:  62% 8608/13868 [1:47:55<1:05:49,  1.33it/s]Training epoch 1:  62% 8609/13868 [1:47:56<1:05:16,  1.34it/s]Training epoch 1:  62% 8610/13868 [1:47:57<1:05:19,  1.34it/s]Training epoch 1:  62% 8611/13868 [1:47:57<1:05:37,  1.34it/s]Training epoch 1:  62% 8612/13868 [1:47:58<1:06:29,  1.32it/s]Training epoch 1:  62% 8613/13868 [1:47:59<1:06:46,  1.31it/s]Training epoch 1:  62% 8614/13868 [1:48:00<1:06:21,  1.32it/s]Training epoch 1:  62% 8615/13868 [1:48:01<1:06:12,  1.32it/s]Training epoch 1:  62% 8616/13868 [1:48:01<1:06:04,  1.32it/s]Training epoch 1:  62% 8617/13868 [1:48:02<1:05:57,  1.33it/s]Training epoch 1:  62% 8618/13868 [1:48:03<1:06:02,  1.32it/s]Training epoch 1:  62% 8619/13868 [1:48:04<1:06:06,  1.32it/s]Training epoch 1:  62% 8620/13868 [1:48:04<1:06:03,  1.32it/s]Training epoch 1:  62% 8621/13868 [1:48:05<1:05:16,  1.34it/s]Training epoch 1:  62% 8622/13868 [1:48:06<1:05:35,  1.33it/s]Training epoch 1:  62% 8623/13868 [1:48:07<1:05:38,  1.33it/s]Training epoch 1:  62% 8624/13868 [1:48:07<1:05:50,  1.33it/s]Training epoch 1:  62% 8625/13868 [1:48:08<1:06:01,  1.32it/s]Training epoch 1:  62% 8626/13868 [1:48:09<1:05:51,  1.33it/s]Training epoch 1:  62% 8627/13868 [1:48:10<1:05:31,  1.33it/s]Training epoch 1:  62% 8628/13868 [1:48:10<1:05:37,  1.33it/s]Training epoch 1:  62% 8629/13868 [1:48:11<1:05:23,  1.34it/s]Training epoch 1:  62% 8630/13868 [1:48:12<1:05:59,  1.32it/s]Training epoch 1:  62% 8631/13868 [1:48:13<1:06:24,  1.31it/s]Training epoch 1:  62% 8632/13868 [1:48:13<1:06:20,  1.32it/s]Training epoch 1:  62% 8633/13868 [1:48:14<1:06:37,  1.31it/s]Training epoch 1:  62% 8634/13868 [1:48:15<1:05:42,  1.33it/s]Training epoch 1:  62% 8635/13868 [1:48:16<1:06:34,  1.31it/s]Training epoch 1:  62% 8636/13868 [1:48:16<1:06:16,  1.32it/s]Training epoch 1:  62% 8637/13868 [1:48:17<1:06:27,  1.31it/s]Training epoch 1:  62% 8638/13868 [1:48:18<1:06:16,  1.32it/s]Training epoch 1:  62% 8639/13868 [1:48:19<1:05:50,  1.32it/s]Training epoch 1:  62% 8640/13868 [1:48:19<1:06:14,  1.32it/s]Training epoch 1:  62% 8641/13868 [1:48:20<1:06:12,  1.32it/s]Training epoch 1:  62% 8642/13868 [1:48:21<1:05:09,  1.34it/s]Training epoch 1:  62% 8643/13868 [1:48:22<1:05:38,  1.33it/s]Training epoch 1:  62% 8644/13868 [1:48:22<1:05:15,  1.33it/s]Training epoch 1:  62% 8645/13868 [1:48:23<1:05:31,  1.33it/s]Training epoch 1:  62% 8646/13868 [1:48:24<1:05:35,  1.33it/s]Training epoch 1:  62% 8647/13868 [1:48:25<1:04:15,  1.35it/s]Training epoch 1:  62% 8648/13868 [1:48:25<1:03:44,  1.36it/s]Training epoch 1:  62% 8649/13868 [1:48:26<1:04:15,  1.35it/s]Training epoch 1:  62% 8650/13868 [1:48:27<1:04:19,  1.35it/s]Training epoch 1:  62% 8651/13868 [1:48:28<1:03:42,  1.36it/s]Training epoch 1:  62% 8652/13868 [1:48:28<1:04:35,  1.35it/s]Training epoch 1:  62% 8653/13868 [1:48:29<1:04:46,  1.34it/s]Training epoch 1:  62% 8654/13868 [1:48:30<1:05:03,  1.34it/s]Training epoch 1:  62% 8655/13868 [1:48:31<1:05:16,  1.33it/s]Training epoch 1:  62% 8656/13868 [1:48:31<1:05:09,  1.33it/s]Training epoch 1:  62% 8657/13868 [1:48:32<1:06:12,  1.31it/s]Training epoch 1:  62% 8658/13868 [1:48:33<1:06:09,  1.31it/s]Training epoch 1:  62% 8659/13868 [1:48:34<1:06:08,  1.31it/s]Training epoch 1:  62% 8660/13868 [1:48:34<1:06:41,  1.30it/s]Training epoch 1:  62% 8661/13868 [1:48:35<1:07:30,  1.29it/s]Training epoch 1:  62% 8662/13868 [1:48:36<1:06:19,  1.31it/s]Training epoch 1:  62% 8663/13868 [1:48:37<1:05:33,  1.32it/s]Training epoch 1:  62% 8664/13868 [1:48:37<1:05:08,  1.33it/s]Training epoch 1:  62% 8665/13868 [1:48:38<1:05:27,  1.32it/s]Training epoch 1:  62% 8666/13868 [1:48:39<1:05:41,  1.32it/s]Training epoch 1:  62% 8667/13868 [1:48:40<1:07:23,  1.29it/s]Training epoch 1:  63% 8668/13868 [1:48:41<1:05:51,  1.32it/s]Training epoch 1:  63% 8669/13868 [1:48:41<1:04:55,  1.33it/s]Training epoch 1:  63% 8670/13868 [1:48:42<1:04:45,  1.34it/s]Training epoch 1:  63% 8671/13868 [1:48:43<1:05:01,  1.33it/s]Training epoch 1:  63% 8672/13868 [1:48:43<1:04:58,  1.33it/s]Training epoch 1:  63% 8673/13868 [1:48:44<1:05:21,  1.32it/s]Training epoch 1:  63% 8674/13868 [1:48:45<1:05:29,  1.32it/s]Training epoch 1:  63% 8675/13868 [1:48:46<1:05:06,  1.33it/s]Training epoch 1:  63% 8676/13868 [1:48:47<1:04:54,  1.33it/s]Training epoch 1:  63% 8677/13868 [1:48:47<1:05:34,  1.32it/s]Training epoch 1:  63% 8678/13868 [1:48:48<1:05:24,  1.32it/s]Training epoch 1:  63% 8679/13868 [1:48:49<1:05:41,  1.32it/s]Training epoch 1:  63% 8680/13868 [1:48:50<1:05:15,  1.32it/s]Training epoch 1:  63% 8681/13868 [1:48:50<1:05:09,  1.33it/s]Training epoch 1:  63% 8682/13868 [1:48:51<1:05:45,  1.31it/s]Training epoch 1:  63% 8683/13868 [1:48:52<1:05:15,  1.32it/s]Training epoch 1:  63% 8684/13868 [1:48:53<1:06:03,  1.31it/s]Training epoch 1:  63% 8685/13868 [1:48:53<1:05:37,  1.32it/s]Training epoch 1:  63% 8686/13868 [1:48:54<1:04:53,  1.33it/s]Training epoch 1:  63% 8687/13868 [1:48:55<1:04:38,  1.34it/s]Training epoch 1:  63% 8688/13868 [1:48:56<1:04:18,  1.34it/s]Training epoch 1:  63% 8689/13868 [1:48:56<1:04:06,  1.35it/s]Training epoch 1:  63% 8690/13868 [1:48:57<1:04:26,  1.34it/s]Training epoch 1:  63% 8691/13868 [1:48:58<1:04:44,  1.33it/s]Training epoch 1:  63% 8692/13868 [1:48:59<1:03:46,  1.35it/s]Training epoch 1:  63% 8693/13868 [1:48:59<1:03:34,  1.36it/s]Training epoch 1:  63% 8694/13868 [1:49:00<1:05:03,  1.33it/s]Training epoch 1:  63% 8695/13868 [1:49:01<1:04:50,  1.33it/s]Training epoch 1:  63% 8696/13868 [1:49:02<1:03:54,  1.35it/s]Training epoch 1:  63% 8697/13868 [1:49:02<1:03:43,  1.35it/s]Training epoch 1:  63% 8698/13868 [1:49:03<1:04:07,  1.34it/s]Training epoch 1:  63% 8699/13868 [1:49:04<1:04:02,  1.35it/s]Training epoch 1:  63% 8700/13868 [1:49:05<1:11:00,  1.21it/s]Training epoch 1:  63% 8701/13868 [1:49:06<1:09:00,  1.25it/s]Training epoch 1:  63% 8702/13868 [1:49:06<1:07:39,  1.27it/s]Training epoch 1:  63% 8703/13868 [1:49:07<1:07:04,  1.28it/s]Training epoch 1:  63% 8704/13868 [1:49:08<1:06:09,  1.30it/s]Training epoch 1:  63% 8705/13868 [1:49:08<1:04:57,  1.32it/s]Training epoch 1:  63% 8706/13868 [1:49:09<1:04:52,  1.33it/s]Training epoch 1:  63% 8707/13868 [1:49:10<1:03:51,  1.35it/s]Training epoch 1:  63% 8708/13868 [1:49:11<1:05:00,  1.32it/s]Training epoch 1:  63% 8709/13868 [1:49:12<1:05:37,  1.31it/s]Training epoch 1:  63% 8710/13868 [1:49:12<1:05:23,  1.31it/s]Training epoch 1:  63% 8711/13868 [1:49:13<1:05:13,  1.32it/s]Training epoch 1:  63% 8712/13868 [1:49:14<1:04:58,  1.32it/s]Training epoch 1:  63% 8713/13868 [1:49:15<1:04:15,  1.34it/s]Training epoch 1:  63% 8714/13868 [1:49:15<1:05:13,  1.32it/s]Training epoch 1:  63% 8715/13868 [1:49:16<1:05:49,  1.30it/s]Training epoch 1:  63% 8716/13868 [1:49:17<1:05:52,  1.30it/s]Training epoch 1:  63% 8717/13868 [1:49:18<1:05:04,  1.32it/s]Training epoch 1:  63% 8718/13868 [1:49:18<1:05:24,  1.31it/s]Training epoch 1:  63% 8719/13868 [1:49:19<1:04:54,  1.32it/s]Training epoch 1:  63% 8720/13868 [1:49:20<1:04:53,  1.32it/s]Training epoch 1:  63% 8721/13868 [1:49:21<1:05:29,  1.31it/s]Training epoch 1:  63% 8722/13868 [1:49:21<1:05:21,  1.31it/s]Training epoch 1:  63% 8723/13868 [1:49:22<1:04:58,  1.32it/s]Training epoch 1:  63% 8724/13868 [1:49:23<1:05:12,  1.31it/s]Training epoch 1:  63% 8725/13868 [1:49:24<1:04:35,  1.33it/s]Training epoch 1:  63% 8726/13868 [1:49:24<1:04:42,  1.32it/s]Training epoch 1:  63% 8727/13868 [1:49:25<1:03:55,  1.34it/s]Training epoch 1:  63% 8728/13868 [1:49:26<1:03:41,  1.34it/s]Training epoch 1:  63% 8729/13868 [1:49:27<1:03:26,  1.35it/s]Training epoch 1:  63% 8730/13868 [1:49:27<1:03:58,  1.34it/s]Training epoch 1:  63% 8731/13868 [1:49:28<1:04:14,  1.33it/s]Training epoch 1:  63% 8732/13868 [1:49:29<1:04:27,  1.33it/s]Training epoch 1:  63% 8733/13868 [1:49:30<1:04:53,  1.32it/s]Training epoch 1:  63% 8734/13868 [1:49:30<1:05:01,  1.32it/s]Training epoch 1:  63% 8735/13868 [1:49:31<1:05:06,  1.31it/s]Training epoch 1:  63% 8736/13868 [1:49:32<1:04:35,  1.32it/s]Training epoch 1:  63% 8737/13868 [1:49:33<1:04:49,  1.32it/s]Training epoch 1:  63% 8738/13868 [1:49:33<1:05:26,  1.31it/s]Training epoch 1:  63% 8739/13868 [1:49:34<1:05:42,  1.30it/s]Training epoch 1:  63% 8740/13868 [1:49:35<1:03:57,  1.34it/s]Training epoch 1:  63% 8741/13868 [1:49:36<1:03:52,  1.34it/s]Training epoch 1:  63% 8742/13868 [1:49:36<1:04:56,  1.32it/s]Training epoch 1:  63% 8743/13868 [1:49:37<1:04:57,  1.31it/s]Training epoch 1:  63% 8744/13868 [1:49:38<1:04:24,  1.33it/s]Training epoch 1:  63% 8745/13868 [1:49:39<1:05:14,  1.31it/s]Training epoch 1:  63% 8746/13868 [1:49:40<1:05:27,  1.30it/s]Training epoch 1:  63% 8747/13868 [1:49:40<1:04:22,  1.33it/s]Training epoch 1:  63% 8748/13868 [1:49:41<1:03:47,  1.34it/s]Training epoch 1:  63% 8749/13868 [1:49:42<1:04:55,  1.31it/s]Training epoch 1:  63% 8750/13868 [1:49:43<1:04:47,  1.32it/s]Training epoch 1:  63% 8751/13868 [1:49:43<1:04:24,  1.32it/s]Training epoch 1:  63% 8752/13868 [1:49:44<1:03:37,  1.34it/s]Training epoch 1:  63% 8753/13868 [1:49:45<1:04:07,  1.33it/s]Training epoch 1:  63% 8754/13868 [1:49:46<1:03:53,  1.33it/s]Training epoch 1:  63% 8755/13868 [1:49:46<1:03:39,  1.34it/s]Training epoch 1:  63% 8756/13868 [1:49:47<1:03:15,  1.35it/s]Training epoch 1:  63% 8757/13868 [1:49:48<1:03:29,  1.34it/s]Training epoch 1:  63% 8758/13868 [1:49:49<1:03:45,  1.34it/s]Training epoch 1:  63% 8759/13868 [1:49:49<1:04:13,  1.33it/s]Training epoch 1:  63% 8760/13868 [1:49:50<1:04:10,  1.33it/s]Training epoch 1:  63% 8761/13868 [1:49:51<1:03:46,  1.33it/s]Training epoch 1:  63% 8762/13868 [1:49:52<1:04:38,  1.32it/s]Training epoch 1:  63% 8763/13868 [1:49:52<1:04:43,  1.31it/s]Training epoch 1:  63% 8764/13868 [1:49:53<1:04:51,  1.31it/s]Training epoch 1:  63% 8765/13868 [1:49:54<1:05:14,  1.30it/s]Training epoch 1:  63% 8766/13868 [1:49:55<1:05:39,  1.30it/s]Training epoch 1:  63% 8767/13868 [1:49:55<1:04:41,  1.31it/s]Training epoch 1:  63% 8768/13868 [1:49:56<1:04:52,  1.31it/s]Training epoch 1:  63% 8769/13868 [1:49:57<1:05:25,  1.30it/s]Training epoch 1:  63% 8770/13868 [1:49:58<1:05:13,  1.30it/s]Training epoch 1:  63% 8771/13868 [1:49:58<1:04:07,  1.32it/s]Training epoch 1:  63% 8772/13868 [1:49:59<1:03:04,  1.35it/s]Training epoch 1:  63% 8773/13868 [1:50:00<1:03:06,  1.35it/s]Training epoch 1:  63% 8774/13868 [1:50:01<1:03:33,  1.34it/s]Training epoch 1:  63% 8775/13868 [1:50:01<1:03:37,  1.33it/s]Training epoch 1:  63% 8776/13868 [1:50:02<1:03:25,  1.34it/s]Training epoch 1:  63% 8777/13868 [1:50:03<1:03:38,  1.33it/s]Training epoch 1:  63% 8778/13868 [1:50:04<1:04:23,  1.32it/s]Training epoch 1:  63% 8779/13868 [1:50:04<1:03:46,  1.33it/s]Training epoch 1:  63% 8780/13868 [1:50:05<1:03:15,  1.34it/s]Training epoch 1:  63% 8781/13868 [1:50:06<1:03:13,  1.34it/s]Training epoch 1:  63% 8782/13868 [1:50:07<1:02:59,  1.35it/s]Training epoch 1:  63% 8783/13868 [1:50:07<1:03:14,  1.34it/s]Training epoch 1:  63% 8784/13868 [1:50:08<1:03:05,  1.34it/s]Training epoch 1:  63% 8785/13868 [1:50:09<1:04:30,  1.31it/s]Training epoch 1:  63% 8786/13868 [1:50:10<1:04:19,  1.32it/s]Training epoch 1:  63% 8787/13868 [1:50:10<1:04:04,  1.32it/s]Training epoch 1:  63% 8788/13868 [1:50:11<1:03:52,  1.33it/s]Training epoch 1:  63% 8789/13868 [1:50:12<1:02:45,  1.35it/s]Training epoch 1:  63% 8790/13868 [1:50:13<1:03:02,  1.34it/s]Training epoch 1:  63% 8791/13868 [1:50:13<1:04:02,  1.32it/s]Training epoch 1:  63% 8792/13868 [1:50:14<1:03:42,  1.33it/s]Training epoch 1:  63% 8793/13868 [1:50:15<1:03:56,  1.32it/s]Training epoch 1:  63% 8794/13868 [1:50:16<1:04:09,  1.32it/s]Training epoch 1:  63% 8795/13868 [1:50:16<1:04:11,  1.32it/s]Training epoch 1:  63% 8796/13868 [1:50:17<1:04:49,  1.30it/s]Training epoch 1:  63% 8797/13868 [1:50:18<1:04:47,  1.30it/s]Training epoch 1:  63% 8798/13868 [1:50:19<1:04:32,  1.31it/s]Training epoch 1:  63% 8799/13868 [1:50:20<1:05:21,  1.29it/s]Training epoch 1:  63% 8800/13868 [1:50:20<1:08:48,  1.23it/s]Training epoch 1:  63% 8801/13868 [1:50:21<1:07:43,  1.25it/s]Training epoch 1:  63% 8802/13868 [1:50:22<1:06:27,  1.27it/s]Training epoch 1:  63% 8803/13868 [1:50:23<1:06:48,  1.26it/s]Training epoch 1:  63% 8804/13868 [1:50:24<1:06:04,  1.28it/s]Training epoch 1:  63% 8805/13868 [1:50:24<1:06:36,  1.27it/s]Training epoch 1:  63% 8806/13868 [1:50:25<1:06:26,  1.27it/s]Training epoch 1:  64% 8807/13868 [1:50:26<1:04:52,  1.30it/s]Training epoch 1:  64% 8808/13868 [1:50:27<1:04:58,  1.30it/s]Training epoch 1:  64% 8809/13868 [1:50:27<1:04:22,  1.31it/s]Training epoch 1:  64% 8810/13868 [1:50:28<1:04:40,  1.30it/s]Training epoch 1:  64% 8811/13868 [1:50:29<1:04:25,  1.31it/s]Training epoch 1:  64% 8812/13868 [1:50:30<1:04:13,  1.31it/s]Training epoch 1:  64% 8813/13868 [1:50:30<1:04:02,  1.32it/s]Training epoch 1:  64% 8814/13868 [1:50:31<1:04:33,  1.30it/s]Training epoch 1:  64% 8815/13868 [1:50:32<1:05:38,  1.28it/s]Training epoch 1:  64% 8816/13868 [1:50:33<1:05:14,  1.29it/s]Training epoch 1:  64% 8817/13868 [1:50:34<1:04:27,  1.31it/s]Training epoch 1:  64% 8818/13868 [1:50:34<1:05:07,  1.29it/s]Training epoch 1:  64% 8819/13868 [1:50:35<1:04:52,  1.30it/s]Training epoch 1:  64% 8820/13868 [1:50:36<1:04:36,  1.30it/s]Training epoch 1:  64% 8821/13868 [1:50:37<1:04:32,  1.30it/s]Training epoch 1:  64% 8822/13868 [1:50:37<1:04:30,  1.30it/s]Training epoch 1:  64% 8823/13868 [1:50:38<1:04:27,  1.30it/s]Training epoch 1:  64% 8824/13868 [1:50:39<1:04:13,  1.31it/s]Training epoch 1:  64% 8825/13868 [1:50:40<1:03:59,  1.31it/s]Training epoch 1:  64% 8826/13868 [1:50:40<1:03:47,  1.32it/s]Training epoch 1:  64% 8827/13868 [1:50:41<1:04:22,  1.31it/s]Training epoch 1:  64% 8828/13868 [1:50:42<1:03:51,  1.32it/s]Training epoch 1:  64% 8829/13868 [1:50:43<1:04:12,  1.31it/s]Training epoch 1:  64% 8830/13868 [1:50:43<1:04:16,  1.31it/s]Training epoch 1:  64% 8831/13868 [1:50:44<1:03:38,  1.32it/s]Training epoch 1:  64% 8832/13868 [1:50:45<1:04:19,  1.30it/s]Training epoch 1:  64% 8833/13868 [1:50:46<1:04:29,  1.30it/s]Training epoch 1:  64% 8834/13868 [1:50:47<1:04:23,  1.30it/s]Training epoch 1:  64% 8835/13868 [1:50:47<1:03:51,  1.31it/s]Training epoch 1:  64% 8836/13868 [1:50:48<1:04:29,  1.30it/s]Training epoch 1:  64% 8837/13868 [1:50:49<1:03:37,  1.32it/s]Training epoch 1:  64% 8838/13868 [1:50:50<1:03:48,  1.31it/s]Training epoch 1:  64% 8839/13868 [1:50:50<1:04:24,  1.30it/s]Training epoch 1:  64% 8840/13868 [1:50:51<1:03:20,  1.32it/s]Training epoch 1:  64% 8841/13868 [1:50:52<1:03:38,  1.32it/s]Training epoch 1:  64% 8842/13868 [1:50:53<1:03:55,  1.31it/s]Training epoch 1:  64% 8843/13868 [1:50:53<1:02:49,  1.33it/s]Training epoch 1:  64% 8844/13868 [1:50:54<1:03:20,  1.32it/s]Training epoch 1:  64% 8845/13868 [1:50:55<1:03:21,  1.32it/s]Training epoch 1:  64% 8846/13868 [1:50:56<1:03:50,  1.31it/s]Training epoch 1:  64% 8847/13868 [1:50:56<1:04:14,  1.30it/s]Training epoch 1:  64% 8848/13868 [1:50:57<1:03:48,  1.31it/s]Training epoch 1:  64% 8849/13868 [1:50:58<1:03:55,  1.31it/s]Training epoch 1:  64% 8850/13868 [1:50:59<1:03:58,  1.31it/s]Training epoch 1:  64% 8851/13868 [1:50:59<1:03:46,  1.31it/s]Training epoch 1:  64% 8852/13868 [1:51:00<1:03:37,  1.31it/s]Training epoch 1:  64% 8853/13868 [1:51:01<1:03:05,  1.32it/s]Training epoch 1:  64% 8854/13868 [1:51:02<1:02:55,  1.33it/s]Training epoch 1:  64% 8855/13868 [1:51:02<1:02:50,  1.33it/s]Training epoch 1:  64% 8856/13868 [1:51:03<1:03:23,  1.32it/s]Training epoch 1:  64% 8857/13868 [1:51:04<1:03:53,  1.31it/s]Training epoch 1:  64% 8858/13868 [1:51:05<1:04:09,  1.30it/s]Training epoch 1:  64% 8859/13868 [1:51:06<1:02:44,  1.33it/s]Training epoch 1:  64% 8860/13868 [1:51:06<1:02:21,  1.34it/s]Training epoch 1:  64% 8861/13868 [1:51:07<1:02:26,  1.34it/s]Training epoch 1:  64% 8862/13868 [1:51:08<1:03:20,  1.32it/s]Training epoch 1:  64% 8863/13868 [1:51:09<1:03:56,  1.30it/s]Training epoch 1:  64% 8864/13868 [1:51:09<1:03:33,  1.31it/s]Training epoch 1:  64% 8865/13868 [1:51:10<1:04:36,  1.29it/s]Training epoch 1:  64% 8866/13868 [1:51:11<1:04:41,  1.29it/s]Training epoch 1:  64% 8867/13868 [1:51:12<1:02:59,  1.32it/s]Training epoch 1:  64% 8868/13868 [1:51:12<1:02:31,  1.33it/s]Training epoch 1:  64% 8869/13868 [1:51:13<1:03:04,  1.32it/s]Training epoch 1:  64% 8870/13868 [1:51:14<1:03:38,  1.31it/s]Training epoch 1:  64% 8871/13868 [1:51:15<1:03:56,  1.30it/s]Training epoch 1:  64% 8872/13868 [1:51:15<1:03:02,  1.32it/s]Training epoch 1:  64% 8873/13868 [1:51:16<1:02:48,  1.33it/s]Training epoch 1:  64% 8874/13868 [1:51:17<1:02:59,  1.32it/s]Training epoch 1:  64% 8875/13868 [1:51:18<1:02:43,  1.33it/s]Training epoch 1:  64% 8876/13868 [1:51:18<1:02:44,  1.33it/s]Training epoch 1:  64% 8877/13868 [1:51:19<1:02:40,  1.33it/s]Training epoch 1:  64% 8878/13868 [1:51:20<1:03:06,  1.32it/s]Training epoch 1:  64% 8879/13868 [1:51:21<1:02:42,  1.33it/s]Training epoch 1:  64% 8880/13868 [1:51:21<1:02:52,  1.32it/s]Training epoch 1:  64% 8881/13868 [1:51:22<1:02:53,  1.32it/s]Training epoch 1:  64% 8882/13868 [1:51:23<1:01:56,  1.34it/s]Training epoch 1:  64% 8883/13868 [1:51:24<1:02:31,  1.33it/s]Training epoch 1:  64% 8884/13868 [1:51:24<1:02:39,  1.33it/s]Training epoch 1:  64% 8885/13868 [1:51:25<1:03:01,  1.32it/s]Training epoch 1:  64% 8886/13868 [1:51:26<1:03:35,  1.31it/s]Training epoch 1:  64% 8887/13868 [1:51:27<1:04:07,  1.29it/s]Training epoch 1:  64% 8888/13868 [1:51:28<1:02:53,  1.32it/s]Training epoch 1:  64% 8889/13868 [1:51:28<1:02:36,  1.33it/s]Training epoch 1:  64% 8890/13868 [1:51:29<1:02:01,  1.34it/s]Training epoch 1:  64% 8891/13868 [1:51:30<1:01:31,  1.35it/s]Training epoch 1:  64% 8892/13868 [1:51:31<1:02:18,  1.33it/s]Training epoch 1:  64% 8893/13868 [1:51:31<1:02:22,  1.33it/s]Training epoch 1:  64% 8894/13868 [1:51:32<1:02:51,  1.32it/s]Training epoch 1:  64% 8895/13868 [1:51:33<1:02:16,  1.33it/s]Training epoch 1:  64% 8896/13868 [1:51:34<1:03:51,  1.30it/s]Training epoch 1:  64% 8897/13868 [1:51:34<1:04:22,  1.29it/s]Training epoch 1:  64% 8898/13868 [1:51:35<1:03:23,  1.31it/s]Training epoch 1:  64% 8899/13868 [1:51:36<1:03:33,  1.30it/s]Training epoch 1:  64% 8900/13868 [1:51:37<1:08:51,  1.20it/s]Training epoch 1:  64% 8901/13868 [1:51:38<1:06:31,  1.24it/s]Training epoch 1:  64% 8902/13868 [1:51:38<1:05:23,  1.27it/s]Training epoch 1:  64% 8903/13868 [1:51:39<1:04:56,  1.27it/s]Training epoch 1:  64% 8904/13868 [1:51:40<1:04:26,  1.28it/s]Training epoch 1:  64% 8905/13868 [1:51:41<1:04:33,  1.28it/s]Training epoch 1:  64% 8906/13868 [1:51:41<1:04:24,  1.28it/s]Training epoch 1:  64% 8907/13868 [1:51:42<1:03:32,  1.30it/s]Training epoch 1:  64% 8908/13868 [1:51:43<1:03:18,  1.31it/s]Training epoch 1:  64% 8909/13868 [1:51:44<1:03:22,  1.30it/s]Training epoch 1:  64% 8910/13868 [1:51:45<1:03:20,  1.30it/s]Training epoch 1:  64% 8911/13868 [1:51:45<1:03:19,  1.30it/s]Training epoch 1:  64% 8912/13868 [1:51:46<1:02:54,  1.31it/s]Training epoch 1:  64% 8913/13868 [1:51:47<1:02:51,  1.31it/s]Training epoch 1:  64% 8914/13868 [1:51:48<1:03:29,  1.30it/s]Training epoch 1:  64% 8915/13868 [1:51:48<1:03:06,  1.31it/s]Training epoch 1:  64% 8916/13868 [1:51:49<1:03:07,  1.31it/s]Training epoch 1:  64% 8917/13868 [1:51:50<1:04:43,  1.27it/s]Training epoch 1:  64% 8918/13868 [1:51:51<1:05:36,  1.26it/s]Training epoch 1:  64% 8919/13868 [1:51:52<1:05:05,  1.27it/s]Training epoch 1:  64% 8920/13868 [1:51:52<1:04:38,  1.28it/s]Training epoch 1:  64% 8921/13868 [1:51:53<1:04:24,  1.28it/s]Training epoch 1:  64% 8922/13868 [1:51:54<1:03:47,  1.29it/s]Training epoch 1:  64% 8923/13868 [1:51:55<1:03:54,  1.29it/s]Training epoch 1:  64% 8924/13868 [1:51:55<1:03:09,  1.30it/s]Training epoch 1:  64% 8925/13868 [1:51:56<1:02:07,  1.33it/s]Training epoch 1:  64% 8926/13868 [1:51:57<1:03:14,  1.30it/s]Training epoch 1:  64% 8927/13868 [1:51:58<1:03:16,  1.30it/s]Training epoch 1:  64% 8928/13868 [1:51:58<1:02:42,  1.31it/s]Training epoch 1:  64% 8929/13868 [1:51:59<1:03:33,  1.30it/s]Training epoch 1:  64% 8930/13868 [1:52:00<1:03:00,  1.31it/s]Training epoch 1:  64% 8931/13868 [1:52:01<1:02:49,  1.31it/s]Training epoch 1:  64% 8932/13868 [1:52:01<1:01:51,  1.33it/s]Training epoch 1:  64% 8933/13868 [1:52:02<1:02:03,  1.33it/s]Training epoch 1:  64% 8934/13868 [1:52:03<1:02:43,  1.31it/s]Training epoch 1:  64% 8935/13868 [1:52:04<1:03:01,  1.30it/s]Training epoch 1:  64% 8936/13868 [1:52:05<1:02:59,  1.31it/s]Training epoch 1:  64% 8937/13868 [1:52:05<1:03:16,  1.30it/s]Training epoch 1:  64% 8938/13868 [1:52:06<1:02:09,  1.32it/s]Training epoch 1:  64% 8939/13868 [1:52:07<1:02:33,  1.31it/s]Training epoch 1:  64% 8940/13868 [1:52:08<1:01:36,  1.33it/s]Training epoch 1:  64% 8941/13868 [1:52:08<1:02:25,  1.32it/s]Training epoch 1:  64% 8942/13868 [1:52:09<1:02:31,  1.31it/s]Training epoch 1:  64% 8943/13868 [1:52:10<1:02:17,  1.32it/s]Training epoch 1:  64% 8944/13868 [1:52:11<1:02:27,  1.31it/s]Training epoch 1:  65% 8945/13868 [1:52:11<1:02:50,  1.31it/s]Training epoch 1:  65% 8946/13868 [1:52:12<1:02:25,  1.31it/s]Training epoch 1:  65% 8947/13868 [1:52:13<1:02:37,  1.31it/s]Training epoch 1:  65% 8948/13868 [1:52:14<1:02:10,  1.32it/s]Training epoch 1:  65% 8949/13868 [1:52:14<1:01:45,  1.33it/s]Training epoch 1:  65% 8950/13868 [1:52:15<1:01:50,  1.33it/s]Training epoch 1:  65% 8951/13868 [1:52:16<1:02:25,  1.31it/s]Training epoch 1:  65% 8952/13868 [1:52:17<1:02:03,  1.32it/s]Training epoch 1:  65% 8953/13868 [1:52:17<1:02:22,  1.31it/s]Training epoch 1:  65% 8954/13868 [1:52:18<1:02:15,  1.32it/s]Training epoch 1:  65% 8955/13868 [1:52:19<1:01:54,  1.32it/s]Training epoch 1:  65% 8956/13868 [1:52:20<1:02:12,  1.32it/s]Training epoch 1:  65% 8957/13868 [1:52:20<1:01:52,  1.32it/s]Training epoch 1:  65% 8958/13868 [1:52:21<1:02:20,  1.31it/s]Training epoch 1:  65% 8959/13868 [1:52:22<1:03:04,  1.30it/s]Training epoch 1:  65% 8960/13868 [1:52:23<1:02:52,  1.30it/s]Training epoch 1:  65% 8961/13868 [1:52:24<1:02:41,  1.30it/s]Training epoch 1:  65% 8962/13868 [1:52:24<1:03:15,  1.29it/s]Training epoch 1:  65% 8963/13868 [1:52:25<1:03:20,  1.29it/s]Training epoch 1:  65% 8964/13868 [1:52:26<1:02:45,  1.30it/s]Training epoch 1:  65% 8965/13868 [1:52:27<1:02:45,  1.30it/s]Training epoch 1:  65% 8966/13868 [1:52:27<1:03:10,  1.29it/s]Training epoch 1:  65% 8967/13868 [1:52:28<1:02:27,  1.31it/s]Training epoch 1:  65% 8968/13868 [1:52:29<1:01:35,  1.33it/s]Training epoch 1:  65% 8969/13868 [1:52:30<1:01:15,  1.33it/s]Training epoch 1:  65% 8970/13868 [1:52:30<1:01:25,  1.33it/s]Training epoch 1:  65% 8971/13868 [1:52:31<1:01:38,  1.32it/s]Training epoch 1:  65% 8972/13868 [1:52:32<1:02:06,  1.31it/s]Training epoch 1:  65% 8973/13868 [1:52:33<1:01:43,  1.32it/s]Training epoch 1:  65% 8974/13868 [1:52:33<1:00:46,  1.34it/s]Training epoch 1:  65% 8975/13868 [1:52:34<1:01:07,  1.33it/s]Training epoch 1:  65% 8976/13868 [1:52:35<1:01:06,  1.33it/s]Training epoch 1:  65% 8977/13868 [1:52:36<1:01:07,  1.33it/s]Training epoch 1:  65% 8978/13868 [1:52:36<1:01:40,  1.32it/s]Training epoch 1:  65% 8979/13868 [1:52:37<1:01:46,  1.32it/s]Training epoch 1:  65% 8980/13868 [1:52:38<1:02:32,  1.30it/s]Training epoch 1:  65% 8981/13868 [1:52:39<1:02:10,  1.31it/s]Training epoch 1:  65% 8982/13868 [1:52:39<1:02:08,  1.31it/s]Training epoch 1:  65% 8983/13868 [1:52:40<1:01:08,  1.33it/s]Training epoch 1:  65% 8984/13868 [1:52:41<1:01:18,  1.33it/s]Training epoch 1:  65% 8985/13868 [1:52:42<1:01:01,  1.33it/s]Training epoch 1:  65% 8986/13868 [1:52:42<1:02:20,  1.31it/s]Training epoch 1:  65% 8987/13868 [1:52:43<1:01:24,  1.32it/s]Training epoch 1:  65% 8988/13868 [1:52:44<1:01:43,  1.32it/s]Training epoch 1:  65% 8989/13868 [1:52:45<1:02:26,  1.30it/s]Training epoch 1:  65% 8990/13868 [1:52:46<1:02:16,  1.31it/s]Training epoch 1:  65% 8991/13868 [1:52:46<1:01:59,  1.31it/s]Training epoch 1:  65% 8992/13868 [1:52:47<1:01:34,  1.32it/s]Training epoch 1:  65% 8993/13868 [1:52:48<1:01:26,  1.32it/s]Training epoch 1:  65% 8994/13868 [1:52:49<1:00:57,  1.33it/s]Training epoch 1:  65% 8995/13868 [1:52:49<1:02:08,  1.31it/s]Training epoch 1:  65% 8996/13868 [1:52:50<1:03:03,  1.29it/s]Training epoch 1:  65% 8997/13868 [1:52:51<1:02:45,  1.29it/s]Training epoch 1:  65% 8998/13868 [1:52:52<1:02:31,  1.30it/s]Training epoch 1:  65% 8999/13868 [1:52:52<1:01:45,  1.31it/s]Training epoch 1:  65% 9000/13868 [1:52:53<1:05:15,  1.24it/s]Training epoch 1:  65% 9001/13868 [1:52:54<1:04:05,  1.27it/s]Training epoch 1:  65% 9002/13868 [1:52:55<1:03:48,  1.27it/s]Training epoch 1:  65% 9003/13868 [1:52:56<1:02:29,  1.30it/s]Training epoch 1:  65% 9004/13868 [1:52:56<1:02:21,  1.30it/s]Training epoch 1:  65% 9005/13868 [1:52:57<1:01:54,  1.31it/s]Training epoch 1:  65% 9006/13868 [1:52:58<1:01:03,  1.33it/s]Training epoch 1:  65% 9007/13868 [1:52:59<1:01:22,  1.32it/s]Training epoch 1:  65% 9008/13868 [1:52:59<1:01:01,  1.33it/s]Training epoch 1:  65% 9009/13868 [1:53:00<1:01:16,  1.32it/s]Training epoch 1:  65% 9010/13868 [1:53:01<1:01:10,  1.32it/s]Training epoch 1:  65% 9011/13868 [1:53:02<1:01:58,  1.31it/s]Training epoch 1:  65% 9012/13868 [1:53:02<1:01:29,  1.32it/s]Training epoch 1:  65% 9013/13868 [1:53:03<1:02:21,  1.30it/s]Training epoch 1:  65% 9014/13868 [1:53:04<1:03:09,  1.28it/s]Training epoch 1:  65% 9015/13868 [1:53:05<1:02:37,  1.29it/s]Training epoch 1:  65% 9016/13868 [1:53:05<1:01:38,  1.31it/s]Training epoch 1:  65% 9017/13868 [1:53:06<1:01:53,  1.31it/s]Training epoch 1:  65% 9018/13868 [1:53:07<1:00:26,  1.34it/s]Training epoch 1:  65% 9019/13868 [1:53:08<1:00:46,  1.33it/s]Training epoch 1:  65% 9020/13868 [1:53:08<1:01:24,  1.32it/s]Training epoch 1:  65% 9021/13868 [1:53:09<1:00:47,  1.33it/s]Training epoch 1:  65% 9022/13868 [1:53:10<1:00:33,  1.33it/s]Training epoch 1:  65% 9023/13868 [1:53:11<1:00:55,  1.33it/s]Training epoch 1:  65% 9024/13868 [1:53:12<1:01:35,  1.31it/s]Training epoch 1:  65% 9025/13868 [1:53:12<1:01:20,  1.32it/s]Training epoch 1:  65% 9026/13868 [1:53:13<1:02:00,  1.30it/s]Training epoch 1:  65% 9027/13868 [1:53:14<1:01:50,  1.30it/s]Training epoch 1:  65% 9028/13868 [1:53:15<1:02:14,  1.30it/s]Training epoch 1:  65% 9029/13868 [1:53:15<1:01:52,  1.30it/s]Training epoch 1:  65% 9030/13868 [1:53:16<1:00:25,  1.33it/s]Training epoch 1:  65% 9031/13868 [1:53:17<1:00:11,  1.34it/s]Training epoch 1:  65% 9032/13868 [1:53:18<1:01:18,  1.31it/s]Training epoch 1:  65% 9033/13868 [1:53:18<1:01:09,  1.32it/s]Training epoch 1:  65% 9034/13868 [1:53:19<1:00:17,  1.34it/s]Training epoch 1:  65% 9035/13868 [1:53:20<1:00:54,  1.32it/s]Training epoch 1:  65% 9036/13868 [1:53:21<1:01:52,  1.30it/s]Training epoch 1:  65% 9037/13868 [1:53:21<1:00:44,  1.33it/s]Training epoch 1:  65% 9038/13868 [1:53:22<1:01:12,  1.32it/s]Training epoch 1:  65% 9039/13868 [1:53:23<1:01:48,  1.30it/s]Training epoch 1:  65% 9040/13868 [1:53:24<1:01:06,  1.32it/s]Training epoch 1:  65% 9041/13868 [1:53:24<1:00:24,  1.33it/s]Training epoch 1:  65% 9042/13868 [1:53:25<1:00:10,  1.34it/s]Training epoch 1:  65% 9043/13868 [1:53:26<1:00:01,  1.34it/s]Training epoch 1:  65% 9044/13868 [1:53:27<1:00:23,  1.33it/s]Training epoch 1:  65% 9045/13868 [1:53:27<1:00:20,  1.33it/s]Training epoch 1:  65% 9046/13868 [1:53:28<59:54,  1.34it/s]  Training epoch 1:  65% 9047/13868 [1:53:29<1:00:23,  1.33it/s]Training epoch 1:  65% 9048/13868 [1:53:30<1:00:48,  1.32it/s]Training epoch 1:  65% 9049/13868 [1:53:30<1:01:01,  1.32it/s]Training epoch 1:  65% 9050/13868 [1:53:31<1:00:16,  1.33it/s]Training epoch 1:  65% 9051/13868 [1:53:32<1:00:39,  1.32it/s]Training epoch 1:  65% 9052/13868 [1:53:33<1:00:25,  1.33it/s]Training epoch 1:  65% 9053/13868 [1:53:33<1:00:33,  1.33it/s]Training epoch 1:  65% 9054/13868 [1:53:34<1:00:39,  1.32it/s]Training epoch 1:  65% 9055/13868 [1:53:35<59:57,  1.34it/s]  Training epoch 1:  65% 9056/13868 [1:53:36<1:00:34,  1.32it/s]Training epoch 1:  65% 9057/13868 [1:53:36<1:00:39,  1.32it/s]Training epoch 1:  65% 9058/13868 [1:53:37<1:00:28,  1.33it/s]Training epoch 1:  65% 9059/13868 [1:53:38<1:00:25,  1.33it/s]Training epoch 1:  65% 9060/13868 [1:53:39<1:01:35,  1.30it/s]Training epoch 1:  65% 9061/13868 [1:53:40<1:01:15,  1.31it/s]Training epoch 1:  65% 9062/13868 [1:53:40<1:00:35,  1.32it/s]Training epoch 1:  65% 9063/13868 [1:53:41<1:01:04,  1.31it/s]Training epoch 1:  65% 9064/13868 [1:53:42<1:02:10,  1.29it/s]Training epoch 1:  65% 9065/13868 [1:53:43<1:02:14,  1.29it/s]Training epoch 1:  65% 9066/13868 [1:53:43<1:01:58,  1.29it/s]Training epoch 1:  65% 9067/13868 [1:53:44<1:01:34,  1.30it/s]Training epoch 1:  65% 9068/13868 [1:53:45<1:00:44,  1.32it/s]Training epoch 1:  65% 9069/13868 [1:53:46<1:00:50,  1.31it/s]Training epoch 1:  65% 9070/13868 [1:53:46<1:00:09,  1.33it/s]Training epoch 1:  65% 9071/13868 [1:53:47<1:00:45,  1.32it/s]Training epoch 1:  65% 9072/13868 [1:53:48<1:01:07,  1.31it/s]Training epoch 1:  65% 9073/13868 [1:53:49<1:00:34,  1.32it/s]Training epoch 1:  65% 9074/13868 [1:53:49<1:00:09,  1.33it/s]Training epoch 1:  65% 9075/13868 [1:53:50<1:01:07,  1.31it/s]Training epoch 1:  65% 9076/13868 [1:53:51<1:00:32,  1.32it/s]Training epoch 1:  65% 9077/13868 [1:53:52<1:01:18,  1.30it/s]Training epoch 1:  65% 9078/13868 [1:53:53<1:01:38,  1.30it/s]Training epoch 1:  65% 9079/13868 [1:53:53<1:00:51,  1.31it/s]Training epoch 1:  65% 9080/13868 [1:53:54<1:00:31,  1.32it/s]Training epoch 1:  65% 9081/13868 [1:53:55<1:00:22,  1.32it/s]Training epoch 1:  65% 9082/13868 [1:53:56<1:00:13,  1.32it/s]Training epoch 1:  65% 9083/13868 [1:53:56<1:00:07,  1.33it/s]Training epoch 1:  66% 9084/13868 [1:53:57<59:52,  1.33it/s]  Training epoch 1:  66% 9085/13868 [1:53:58<59:37,  1.34it/s]Training epoch 1:  66% 9086/13868 [1:53:59<59:49,  1.33it/s]Training epoch 1:  66% 9087/13868 [1:53:59<59:41,  1.33it/s]Training epoch 1:  66% 9088/13868 [1:54:00<58:59,  1.35it/s]Training epoch 1:  66% 9089/13868 [1:54:01<59:58,  1.33it/s]Training epoch 1:  66% 9090/13868 [1:54:02<1:00:22,  1.32it/s]Training epoch 1:  66% 9091/13868 [1:54:02<59:51,  1.33it/s]  Training epoch 1:  66% 9092/13868 [1:54:03<1:00:34,  1.31it/s]Training epoch 1:  66% 9093/13868 [1:54:04<1:00:28,  1.32it/s]Training epoch 1:  66% 9094/13868 [1:54:05<1:00:44,  1.31it/s]Training epoch 1:  66% 9095/13868 [1:54:05<1:00:44,  1.31it/s]Training epoch 1:  66% 9096/13868 [1:54:06<1:00:04,  1.32it/s]Training epoch 1:  66% 9097/13868 [1:54:07<59:28,  1.34it/s]  Training epoch 1:  66% 9098/13868 [1:54:08<59:52,  1.33it/s]Training epoch 1:  66% 9099/13868 [1:54:08<1:00:09,  1.32it/s]Training epoch 1:  66% 9100/13868 [1:54:09<1:03:39,  1.25it/s]Training epoch 1:  66% 9101/13868 [1:54:10<1:03:41,  1.25it/s]Training epoch 1:  66% 9102/13868 [1:54:11<1:02:52,  1.26it/s]Training epoch 1:  66% 9103/13868 [1:54:12<1:02:13,  1.28it/s]Training epoch 1:  66% 9104/13868 [1:54:12<1:01:36,  1.29it/s]Training epoch 1:  66% 9105/13868 [1:54:13<1:00:51,  1.30it/s]Training epoch 1:  66% 9106/13868 [1:54:14<1:00:13,  1.32it/s]Training epoch 1:  66% 9107/13868 [1:54:15<59:58,  1.32it/s]  Training epoch 1:  66% 9108/13868 [1:54:15<59:28,  1.33it/s]Training epoch 1:  66% 9109/13868 [1:54:16<59:30,  1.33it/s]Training epoch 1:  66% 9110/13868 [1:54:17<59:27,  1.33it/s]Training epoch 1:  66% 9111/13868 [1:54:18<59:07,  1.34it/s]Training epoch 1:  66% 9112/13868 [1:54:18<59:45,  1.33it/s]Training epoch 1:  66% 9113/13868 [1:54:19<59:58,  1.32it/s]Training epoch 1:  66% 9114/13868 [1:54:20<1:00:58,  1.30it/s]Training epoch 1:  66% 9115/13868 [1:54:21<1:00:48,  1.30it/s]Training epoch 1:  66% 9116/13868 [1:54:21<1:00:28,  1.31it/s]Training epoch 1:  66% 9117/13868 [1:54:22<1:01:29,  1.29it/s]Training epoch 1:  66% 9118/13868 [1:54:23<1:00:22,  1.31it/s]Training epoch 1:  66% 9119/13868 [1:54:24<59:37,  1.33it/s]  Training epoch 1:  66% 9120/13868 [1:54:24<1:00:49,  1.30it/s]Training epoch 1:  66% 9121/13868 [1:54:25<1:00:09,  1.32it/s]Training epoch 1:  66% 9122/13868 [1:54:26<1:00:44,  1.30it/s]Training epoch 1:  66% 9123/13868 [1:54:27<1:00:38,  1.30it/s]Training epoch 1:  66% 9124/13868 [1:54:28<1:00:53,  1.30it/s]Training epoch 1:  66% 9125/13868 [1:54:28<1:01:38,  1.28it/s]Training epoch 1:  66% 9126/13868 [1:54:29<1:01:28,  1.29it/s]Training epoch 1:  66% 9127/13868 [1:54:30<1:00:17,  1.31it/s]Training epoch 1:  66% 9128/13868 [1:54:31<1:00:38,  1.30it/s]Training epoch 1:  66% 9129/13868 [1:54:31<59:55,  1.32it/s]  Training epoch 1:  66% 9130/13868 [1:54:32<1:00:13,  1.31it/s]Training epoch 1:  66% 9131/13868 [1:54:33<59:47,  1.32it/s]  Training epoch 1:  66% 9132/13868 [1:54:34<1:00:44,  1.30it/s]Training epoch 1:  66% 9133/13868 [1:54:34<1:00:13,  1.31it/s]Training epoch 1:  66% 9134/13868 [1:54:35<1:00:37,  1.30it/s]Training epoch 1:  66% 9135/13868 [1:54:36<59:36,  1.32it/s]  Training epoch 1:  66% 9136/13868 [1:54:37<59:36,  1.32it/s]Training epoch 1:  66% 9137/13868 [1:54:37<1:00:05,  1.31it/s]Training epoch 1:  66% 9138/13868 [1:54:38<1:00:09,  1.31it/s]Training epoch 1:  66% 9139/13868 [1:54:39<1:00:04,  1.31it/s]Training epoch 1:  66% 9140/13868 [1:54:40<1:00:14,  1.31it/s]Training epoch 1:  66% 9141/13868 [1:54:41<1:00:07,  1.31it/s]Training epoch 1:  66% 9142/13868 [1:54:41<59:40,  1.32it/s]  Training epoch 1:  66% 9143/13868 [1:54:42<59:48,  1.32it/s]Training epoch 1:  66% 9144/13868 [1:54:43<1:00:12,  1.31it/s]Training epoch 1:  66% 9145/13868 [1:54:44<59:50,  1.32it/s]  Training epoch 1:  66% 9146/13868 [1:54:44<1:00:20,  1.30it/s]Training epoch 1:  66% 9147/13868 [1:54:45<59:47,  1.32it/s]  Training epoch 1:  66% 9148/13868 [1:54:46<1:00:12,  1.31it/s]Training epoch 1:  66% 9149/13868 [1:54:47<1:00:18,  1.30it/s]Training epoch 1:  66% 9150/13868 [1:54:47<1:00:12,  1.31it/s]Training epoch 1:  66% 9151/13868 [1:54:48<59:50,  1.31it/s]  Training epoch 1:  66% 9152/13868 [1:54:49<59:43,  1.32it/s]Training epoch 1:  66% 9153/13868 [1:54:50<59:24,  1.32it/s]Training epoch 1:  66% 9154/13868 [1:54:50<59:37,  1.32it/s]Training epoch 1:  66% 9155/13868 [1:54:51<59:29,  1.32it/s]Training epoch 1:  66% 9156/13868 [1:54:52<59:49,  1.31it/s]Training epoch 1:  66% 9157/13868 [1:54:53<59:59,  1.31it/s]Training epoch 1:  66% 9158/13868 [1:54:53<59:01,  1.33it/s]Training epoch 1:  66% 9159/13868 [1:54:54<58:35,  1.34it/s]Training epoch 1:  66% 9160/13868 [1:54:55<59:39,  1.32it/s]Training epoch 1:  66% 9161/13868 [1:54:56<59:51,  1.31it/s]Training epoch 1:  66% 9162/13868 [1:54:56<59:46,  1.31it/s]Training epoch 1:  66% 9163/13868 [1:54:57<59:52,  1.31it/s]Training epoch 1:  66% 9164/13868 [1:54:58<1:00:00,  1.31it/s]Training epoch 1:  66% 9165/13868 [1:54:59<1:00:28,  1.30it/s]Training epoch 1:  66% 9166/13868 [1:55:00<1:00:01,  1.31it/s]Training epoch 1:  66% 9167/13868 [1:55:00<59:57,  1.31it/s]  Training epoch 1:  66% 9168/13868 [1:55:01<59:41,  1.31it/s]Training epoch 1:  66% 9169/13868 [1:55:02<1:00:07,  1.30it/s]Training epoch 1:  66% 9170/13868 [1:55:03<59:02,  1.33it/s]  Training epoch 1:  66% 9171/13868 [1:55:03<59:09,  1.32it/s]Training epoch 1:  66% 9172/13868 [1:55:04<58:56,  1.33it/s]Training epoch 1:  66% 9173/13868 [1:55:05<58:31,  1.34it/s]Training epoch 1:  66% 9174/13868 [1:55:06<58:41,  1.33it/s]Training epoch 1:  66% 9175/13868 [1:55:06<58:57,  1.33it/s]Training epoch 1:  66% 9176/13868 [1:55:07<59:15,  1.32it/s]Training epoch 1:  66% 9177/13868 [1:55:08<58:39,  1.33it/s]Training epoch 1:  66% 9178/13868 [1:55:09<59:14,  1.32it/s]Training epoch 1:  66% 9179/13868 [1:55:09<59:40,  1.31it/s]Training epoch 1:  66% 9180/13868 [1:55:10<59:53,  1.30it/s]Training epoch 1:  66% 9181/13868 [1:55:11<59:10,  1.32it/s]Training epoch 1:  66% 9182/13868 [1:55:12<59:01,  1.32it/s]Training epoch 1:  66% 9183/13868 [1:55:12<59:14,  1.32it/s]Training epoch 1:  66% 9184/13868 [1:55:13<59:11,  1.32it/s]Training epoch 1:  66% 9185/13868 [1:55:14<59:05,  1.32it/s]Training epoch 1:  66% 9186/13868 [1:55:15<59:08,  1.32it/s]Training epoch 1:  66% 9187/13868 [1:55:15<59:37,  1.31it/s]Training epoch 1:  66% 9188/13868 [1:55:16<59:52,  1.30it/s]Training epoch 1:  66% 9189/13868 [1:55:17<1:00:09,  1.30it/s]Training epoch 1:  66% 9190/13868 [1:55:18<59:49,  1.30it/s]  Training epoch 1:  66% 9191/13868 [1:55:19<59:41,  1.31it/s]Training epoch 1:  66% 9192/13868 [1:55:19<1:00:11,  1.29it/s]Training epoch 1:  66% 9193/13868 [1:55:20<59:51,  1.30it/s]  Training epoch 1:  66% 9194/13868 [1:55:21<1:00:17,  1.29it/s]Training epoch 1:  66% 9195/13868 [1:55:22<1:00:42,  1.28it/s]Training epoch 1:  66% 9196/13868 [1:55:22<1:00:51,  1.28it/s]Training epoch 1:  66% 9197/13868 [1:55:23<59:57,  1.30it/s]  Training epoch 1:  66% 9198/13868 [1:55:24<59:49,  1.30it/s]Training epoch 1:  66% 9199/13868 [1:55:25<1:00:16,  1.29it/s]Training epoch 1:  66% 9200/13868 [1:55:26<1:02:14,  1.25it/s]Training epoch 1:  66% 9201/13868 [1:55:26<1:01:00,  1.27it/s]Training epoch 1:  66% 9202/13868 [1:55:27<1:01:01,  1.27it/s]Training epoch 1:  66% 9203/13868 [1:55:28<1:00:18,  1.29it/s]Training epoch 1:  66% 9204/13868 [1:55:29<1:00:07,  1.29it/s]Training epoch 1:  66% 9205/13868 [1:55:29<59:06,  1.31it/s]  Training epoch 1:  66% 9206/13868 [1:55:30<58:32,  1.33it/s]Training epoch 1:  66% 9207/13868 [1:55:31<58:27,  1.33it/s]Training epoch 1:  66% 9208/13868 [1:55:32<59:38,  1.30it/s]Training epoch 1:  66% 9209/13868 [1:55:32<59:47,  1.30it/s]Training epoch 1:  66% 9210/13868 [1:55:33<59:58,  1.29it/s]Training epoch 1:  66% 9211/13868 [1:55:34<58:57,  1.32it/s]Training epoch 1:  66% 9212/13868 [1:55:35<58:51,  1.32it/s]Training epoch 1:  66% 9213/13868 [1:55:36<59:01,  1.31it/s]Training epoch 1:  66% 9214/13868 [1:55:36<58:56,  1.32it/s]Training epoch 1:  66% 9215/13868 [1:55:37<59:02,  1.31it/s]Training epoch 1:  66% 9216/13868 [1:55:38<57:37,  1.35it/s]Training epoch 1:  66% 9217/13868 [1:55:38<57:59,  1.34it/s]Training epoch 1:  66% 9218/13868 [1:55:39<58:52,  1.32it/s]Training epoch 1:  66% 9219/13868 [1:55:40<59:21,  1.31it/s]Training epoch 1:  66% 9220/13868 [1:55:41<58:59,  1.31it/s]Training epoch 1:  66% 9221/13868 [1:55:42<58:13,  1.33it/s]Training epoch 1:  66% 9222/13868 [1:55:42<58:35,  1.32it/s]Training epoch 1:  67% 9223/13868 [1:55:43<58:53,  1.31it/s]Training epoch 1:  67% 9224/13868 [1:55:44<58:23,  1.33it/s]Training epoch 1:  67% 9225/13868 [1:55:45<58:36,  1.32it/s]Training epoch 1:  67% 9226/13868 [1:55:45<58:25,  1.32it/s]Training epoch 1:  67% 9227/13868 [1:55:46<58:30,  1.32it/s]Training epoch 1:  67% 9228/13868 [1:55:47<58:10,  1.33it/s]Training epoch 1:  67% 9229/13868 [1:55:48<59:16,  1.30it/s]Training epoch 1:  67% 9230/13868 [1:55:48<58:54,  1.31it/s]Training epoch 1:  67% 9231/13868 [1:55:49<58:58,  1.31it/s]Training epoch 1:  67% 9232/13868 [1:55:50<58:42,  1.32it/s]Training epoch 1:  67% 9233/13868 [1:55:51<58:17,  1.33it/s]Training epoch 1:  67% 9234/13868 [1:55:51<59:12,  1.30it/s]Training epoch 1:  67% 9235/13868 [1:55:52<59:46,  1.29it/s]Training epoch 1:  67% 9236/13868 [1:55:53<59:20,  1.30it/s]Training epoch 1:  67% 9237/13868 [1:55:54<59:14,  1.30it/s]Training epoch 1:  67% 9238/13868 [1:55:55<59:47,  1.29it/s]Training epoch 1:  67% 9239/13868 [1:55:55<59:17,  1.30it/s]Training epoch 1:  67% 9240/13868 [1:55:56<59:50,  1.29it/s]Training epoch 1:  67% 9241/13868 [1:55:57<59:27,  1.30it/s]Training epoch 1:  67% 9242/13868 [1:55:58<59:15,  1.30it/s]Training epoch 1:  67% 9243/13868 [1:55:58<59:21,  1.30it/s]Training epoch 1:  67% 9244/13868 [1:55:59<58:53,  1.31it/s]Training epoch 1:  67% 9245/13868 [1:56:00<59:18,  1.30it/s]Training epoch 1:  67% 9246/13868 [1:56:01<59:27,  1.30it/s]Training epoch 1:  67% 9247/13868 [1:56:01<59:11,  1.30it/s]Training epoch 1:  67% 9248/13868 [1:56:02<58:46,  1.31it/s]Training epoch 1:  67% 9249/13868 [1:56:03<57:27,  1.34it/s]Training epoch 1:  67% 9250/13868 [1:56:04<57:18,  1.34it/s]Training epoch 1:  67% 9251/13868 [1:56:04<57:24,  1.34it/s]Training epoch 1:  67% 9252/13868 [1:56:05<57:45,  1.33it/s]Training epoch 1:  67% 9253/13868 [1:56:06<57:26,  1.34it/s]Training epoch 1:  67% 9254/13868 [1:56:07<56:42,  1.36it/s]Training epoch 1:  67% 9255/13868 [1:56:07<57:22,  1.34it/s]Training epoch 1:  67% 9256/13868 [1:56:08<57:29,  1.34it/s]Training epoch 1:  67% 9257/13868 [1:56:09<57:42,  1.33it/s]Training epoch 1:  67% 9258/13868 [1:56:10<58:01,  1.32it/s]Training epoch 1:  67% 9259/13868 [1:56:10<58:41,  1.31it/s]Training epoch 1:  67% 9260/13868 [1:56:11<58:14,  1.32it/s]Training epoch 1:  67% 9261/13868 [1:56:12<58:37,  1.31it/s]Training epoch 1:  67% 9262/13868 [1:56:13<58:49,  1.31it/s]Training epoch 1:  67% 9263/13868 [1:56:14<58:55,  1.30it/s]Training epoch 1:  67% 9264/13868 [1:56:14<58:33,  1.31it/s]Training epoch 1:  67% 9265/13868 [1:56:15<59:13,  1.30it/s]Training epoch 1:  67% 9266/13868 [1:56:16<59:31,  1.29it/s]Training epoch 1:  67% 9267/13868 [1:56:17<58:51,  1.30it/s]Training epoch 1:  67% 9268/13868 [1:56:17<58:52,  1.30it/s]Training epoch 1:  67% 9269/13868 [1:56:18<58:41,  1.31it/s]Training epoch 1:  67% 9270/13868 [1:56:19<58:21,  1.31it/s]Training epoch 1:  67% 9271/13868 [1:56:20<58:29,  1.31it/s]Training epoch 1:  67% 9272/13868 [1:56:20<57:48,  1.32it/s]Training epoch 1:  67% 9273/13868 [1:56:21<57:29,  1.33it/s]Training epoch 1:  67% 9274/13868 [1:56:22<57:05,  1.34it/s]Training epoch 1:  67% 9275/13868 [1:56:23<57:39,  1.33it/s]Training epoch 1:  67% 9276/13868 [1:56:23<57:33,  1.33it/s]Training epoch 1:  67% 9277/13868 [1:56:24<57:48,  1.32it/s]Training epoch 1:  67% 9278/13868 [1:56:25<58:21,  1.31it/s]Training epoch 1:  67% 9279/13868 [1:56:26<58:39,  1.30it/s]Training epoch 1:  67% 9280/13868 [1:56:26<58:55,  1.30it/s]Training epoch 1:  67% 9281/13868 [1:56:27<58:22,  1.31it/s]Training epoch 1:  67% 9282/13868 [1:56:28<58:24,  1.31it/s]Training epoch 1:  67% 9283/13868 [1:56:29<58:24,  1.31it/s]Training epoch 1:  67% 9284/13868 [1:56:30<58:34,  1.30it/s]Training epoch 1:  67% 9285/13868 [1:56:30<57:56,  1.32it/s]Training epoch 1:  67% 9286/13868 [1:56:31<58:06,  1.31it/s]Training epoch 1:  67% 9287/13868 [1:56:32<57:49,  1.32it/s]Training epoch 1:  67% 9288/13868 [1:56:33<57:56,  1.32it/s]Training epoch 1:  67% 9289/13868 [1:56:33<58:17,  1.31it/s]Training epoch 1:  67% 9290/13868 [1:56:34<57:46,  1.32it/s]Training epoch 1:  67% 9291/13868 [1:56:35<57:51,  1.32it/s]Training epoch 1:  67% 9292/13868 [1:56:36<58:04,  1.31it/s]Training epoch 1:  67% 9293/13868 [1:56:36<58:16,  1.31it/s]Training epoch 1:  67% 9294/13868 [1:56:37<57:34,  1.32it/s]Training epoch 1:  67% 9295/13868 [1:56:38<58:21,  1.31it/s]Training epoch 1:  67% 9296/13868 [1:56:39<58:14,  1.31it/s]Training epoch 1:  67% 9297/13868 [1:56:39<57:35,  1.32it/s]Training epoch 1:  67% 9298/13868 [1:56:40<57:22,  1.33it/s]Training epoch 1:  67% 9299/13868 [1:56:41<57:51,  1.32it/s]Training epoch 1:  67% 9300/13868 [1:56:42<1:02:18,  1.22it/s]Training epoch 1:  67% 9301/13868 [1:56:43<1:01:03,  1.25it/s]Training epoch 1:  67% 9302/13868 [1:56:43<1:00:06,  1.27it/s]Training epoch 1:  67% 9303/13868 [1:56:44<59:53,  1.27it/s]  Training epoch 1:  67% 9304/13868 [1:56:45<58:49,  1.29it/s]Training epoch 1:  67% 9305/13868 [1:56:46<59:21,  1.28it/s]Training epoch 1:  67% 9306/13868 [1:56:46<58:07,  1.31it/s]Training epoch 1:  67% 9307/13868 [1:56:47<57:20,  1.33it/s]Training epoch 1:  67% 9308/13868 [1:56:48<57:51,  1.31it/s]Training epoch 1:  67% 9309/13868 [1:56:49<57:32,  1.32it/s]Training epoch 1:  67% 9310/13868 [1:56:49<57:28,  1.32it/s]Training epoch 1:  67% 9311/13868 [1:56:50<58:11,  1.31it/s]Training epoch 1:  67% 9312/13868 [1:56:51<57:46,  1.31it/s]Training epoch 1:  67% 9313/13868 [1:56:52<57:00,  1.33it/s]Training epoch 1:  67% 9314/13868 [1:56:52<56:36,  1.34it/s]Training epoch 1:  67% 9315/13868 [1:56:53<57:35,  1.32it/s]Training epoch 1:  67% 9316/13868 [1:56:54<57:21,  1.32it/s]Training epoch 1:  67% 9317/13868 [1:56:55<56:41,  1.34it/s]Training epoch 1:  67% 9318/13868 [1:56:55<56:34,  1.34it/s]Training epoch 1:  67% 9319/13868 [1:56:56<56:09,  1.35it/s]Training epoch 1:  67% 9320/13868 [1:56:57<55:57,  1.35it/s]Training epoch 1:  67% 9321/13868 [1:56:58<56:38,  1.34it/s]Training epoch 1:  67% 9322/13868 [1:56:58<57:21,  1.32it/s]Training epoch 1:  67% 9323/13868 [1:56:59<57:52,  1.31it/s]Training epoch 1:  67% 9324/13868 [1:57:00<56:53,  1.33it/s]Training epoch 1:  67% 9325/13868 [1:57:01<57:20,  1.32it/s]Training epoch 1:  67% 9326/13868 [1:57:02<58:05,  1.30it/s]Training epoch 1:  67% 9327/13868 [1:57:02<57:17,  1.32it/s]Training epoch 1:  67% 9328/13868 [1:57:03<56:55,  1.33it/s]Training epoch 1:  67% 9329/13868 [1:57:04<56:15,  1.34it/s]Training epoch 1:  67% 9330/13868 [1:57:04<56:15,  1.34it/s]Training epoch 1:  67% 9331/13868 [1:57:05<56:43,  1.33it/s]Training epoch 1:  67% 9332/13868 [1:57:06<57:31,  1.31it/s]Training epoch 1:  67% 9333/13868 [1:57:07<57:39,  1.31it/s]Training epoch 1:  67% 9334/13868 [1:57:08<57:38,  1.31it/s]Training epoch 1:  67% 9335/13868 [1:57:08<57:33,  1.31it/s]Training epoch 1:  67% 9336/13868 [1:57:09<57:16,  1.32it/s]Training epoch 1:  67% 9337/13868 [1:57:10<57:15,  1.32it/s]Training epoch 1:  67% 9338/13868 [1:57:11<57:08,  1.32it/s]Training epoch 1:  67% 9339/13868 [1:57:11<56:48,  1.33it/s]Training epoch 1:  67% 9340/13868 [1:57:12<56:17,  1.34it/s]Training epoch 1:  67% 9341/13868 [1:57:13<57:21,  1.32it/s]Training epoch 1:  67% 9342/13868 [1:57:14<56:39,  1.33it/s]Training epoch 1:  67% 9343/13868 [1:57:14<56:39,  1.33it/s]Training epoch 1:  67% 9344/13868 [1:57:15<56:57,  1.32it/s]Training epoch 1:  67% 9345/13868 [1:57:16<56:37,  1.33it/s]Training epoch 1:  67% 9346/13868 [1:57:17<56:33,  1.33it/s]Training epoch 1:  67% 9347/13868 [1:57:17<56:11,  1.34it/s]Training epoch 1:  67% 9348/13868 [1:57:18<56:39,  1.33it/s]Training epoch 1:  67% 9349/13868 [1:57:19<57:37,  1.31it/s]Training epoch 1:  67% 9350/13868 [1:57:20<57:40,  1.31it/s]Training epoch 1:  67% 9351/13868 [1:57:20<57:57,  1.30it/s]Training epoch 1:  67% 9352/13868 [1:57:21<57:48,  1.30it/s]Training epoch 1:  67% 9353/13868 [1:57:22<57:40,  1.30it/s]Training epoch 1:  67% 9354/13868 [1:57:23<57:04,  1.32it/s]Training epoch 1:  67% 9355/13868 [1:57:23<57:03,  1.32it/s]Training epoch 1:  67% 9356/13868 [1:57:24<56:27,  1.33it/s]Training epoch 1:  67% 9357/13868 [1:57:25<57:01,  1.32it/s]Training epoch 1:  67% 9358/13868 [1:57:26<57:23,  1.31it/s]Training epoch 1:  67% 9359/13868 [1:57:26<56:23,  1.33it/s]Training epoch 1:  67% 9360/13868 [1:57:27<56:29,  1.33it/s]Training epoch 1:  68% 9361/13868 [1:57:28<56:54,  1.32it/s]Training epoch 1:  68% 9362/13868 [1:57:29<56:49,  1.32it/s]Training epoch 1:  68% 9363/13868 [1:57:29<56:36,  1.33it/s]Training epoch 1:  68% 9364/13868 [1:57:30<57:04,  1.32it/s]Training epoch 1:  68% 9365/13868 [1:57:31<56:53,  1.32it/s]Training epoch 1:  68% 9366/13868 [1:57:32<56:30,  1.33it/s]Training epoch 1:  68% 9367/13868 [1:57:32<56:19,  1.33it/s]Training epoch 1:  68% 9368/13868 [1:57:33<56:15,  1.33it/s]Training epoch 1:  68% 9369/13868 [1:57:34<55:39,  1.35it/s]Training epoch 1:  68% 9370/13868 [1:57:35<56:35,  1.32it/s]Training epoch 1:  68% 9371/13868 [1:57:36<56:59,  1.32it/s]Training epoch 1:  68% 9372/13868 [1:57:36<57:01,  1.31it/s]Training epoch 1:  68% 9373/13868 [1:57:37<57:11,  1.31it/s]Training epoch 1:  68% 9374/13868 [1:57:38<56:57,  1.32it/s]Training epoch 1:  68% 9375/13868 [1:57:39<56:32,  1.32it/s]Training epoch 1:  68% 9376/13868 [1:57:39<57:04,  1.31it/s]Training epoch 1:  68% 9377/13868 [1:57:40<57:30,  1.30it/s]Training epoch 1:  68% 9378/13868 [1:57:41<56:36,  1.32it/s]Training epoch 1:  68% 9379/13868 [1:57:42<56:55,  1.31it/s]Training epoch 1:  68% 9380/13868 [1:57:42<55:38,  1.34it/s]Training epoch 1:  68% 9381/13868 [1:57:43<55:49,  1.34it/s]Training epoch 1:  68% 9382/13868 [1:57:44<56:39,  1.32it/s]Training epoch 1:  68% 9383/13868 [1:57:45<56:53,  1.31it/s]Training epoch 1:  68% 9384/13868 [1:57:45<57:38,  1.30it/s]Training epoch 1:  68% 9385/13868 [1:57:46<58:07,  1.29it/s]Training epoch 1:  68% 9386/13868 [1:57:47<57:00,  1.31it/s]Training epoch 1:  68% 9387/13868 [1:57:48<57:06,  1.31it/s]Training epoch 1:  68% 9388/13868 [1:57:48<57:07,  1.31it/s]Training epoch 1:  68% 9389/13868 [1:57:49<57:02,  1.31it/s]Training epoch 1:  68% 9390/13868 [1:57:50<56:04,  1.33it/s]Training epoch 1:  68% 9391/13868 [1:57:51<56:12,  1.33it/s]Training epoch 1:  68% 9392/13868 [1:57:51<56:20,  1.32it/s]Training epoch 1:  68% 9393/13868 [1:57:52<56:15,  1.33it/s]Training epoch 1:  68% 9394/13868 [1:57:53<56:59,  1.31it/s]Training epoch 1:  68% 9395/13868 [1:57:54<57:08,  1.30it/s]Training epoch 1:  68% 9396/13868 [1:57:55<56:54,  1.31it/s]Training epoch 1:  68% 9397/13868 [1:57:55<57:01,  1.31it/s]Training epoch 1:  68% 9398/13868 [1:57:56<57:21,  1.30it/s]Training epoch 1:  68% 9399/13868 [1:57:57<56:45,  1.31it/s]Training epoch 1:  68% 9400/13868 [1:57:58<1:00:09,  1.24it/s]Training epoch 1:  68% 9401/13868 [1:57:59<59:36,  1.25it/s]  Training epoch 1:  68% 9402/13868 [1:57:59<59:38,  1.25it/s]Training epoch 1:  68% 9403/13868 [1:58:00<58:19,  1.28it/s]Training epoch 1:  68% 9404/13868 [1:58:01<58:03,  1.28it/s]Training epoch 1:  68% 9405/13868 [1:58:02<57:14,  1.30it/s]Training epoch 1:  68% 9406/13868 [1:58:02<56:21,  1.32it/s]Training epoch 1:  68% 9407/13868 [1:58:03<56:31,  1.32it/s]Training epoch 1:  68% 9408/13868 [1:58:04<55:45,  1.33it/s]Training epoch 1:  68% 9409/13868 [1:58:05<55:38,  1.34it/s]Training epoch 1:  68% 9410/13868 [1:58:05<56:09,  1.32it/s]Training epoch 1:  68% 9411/13868 [1:58:06<56:23,  1.32it/s]Training epoch 1:  68% 9412/13868 [1:58:07<56:50,  1.31it/s]Training epoch 1:  68% 9413/13868 [1:58:08<57:51,  1.28it/s]Training epoch 1:  68% 9414/13868 [1:58:08<57:14,  1.30it/s]Training epoch 1:  68% 9415/13868 [1:58:09<56:27,  1.31it/s]Training epoch 1:  68% 9416/13868 [1:58:10<56:14,  1.32it/s]Training epoch 1:  68% 9417/13868 [1:58:11<56:33,  1.31it/s]Training epoch 1:  68% 9418/13868 [1:58:11<55:42,  1.33it/s]Training epoch 1:  68% 9419/13868 [1:58:12<56:01,  1.32it/s]Training epoch 1:  68% 9420/13868 [1:58:13<55:45,  1.33it/s]Training epoch 1:  68% 9421/13868 [1:58:14<54:59,  1.35it/s]Training epoch 1:  68% 9422/13868 [1:58:14<54:44,  1.35it/s]Training epoch 1:  68% 9423/13868 [1:58:15<55:23,  1.34it/s]Training epoch 1:  68% 9424/13868 [1:58:16<55:17,  1.34it/s]Training epoch 1:  68% 9425/13868 [1:58:17<56:19,  1.31it/s]Training epoch 1:  68% 9426/13868 [1:58:17<56:35,  1.31it/s]Training epoch 1:  68% 9427/13868 [1:58:18<56:07,  1.32it/s]Training epoch 1:  68% 9428/13868 [1:58:19<56:06,  1.32it/s]Training epoch 1:  68% 9429/13868 [1:58:20<56:03,  1.32it/s]Training epoch 1:  68% 9430/13868 [1:58:20<55:32,  1.33it/s]Training epoch 1:  68% 9431/13868 [1:58:21<55:10,  1.34it/s]Training epoch 1:  68% 9432/13868 [1:58:22<55:52,  1.32it/s]Training epoch 1:  68% 9433/13868 [1:58:23<56:10,  1.32it/s]Training epoch 1:  68% 9434/13868 [1:58:24<56:19,  1.31it/s]Training epoch 1:  68% 9435/13868 [1:58:24<55:46,  1.32it/s]Training epoch 1:  68% 9436/13868 [1:58:25<55:29,  1.33it/s]Training epoch 1:  68% 9437/13868 [1:58:26<55:16,  1.34it/s]Training epoch 1:  68% 9438/13868 [1:58:26<54:39,  1.35it/s]Training epoch 1:  68% 9439/13868 [1:58:27<54:49,  1.35it/s]Training epoch 1:  68% 9440/13868 [1:58:28<54:10,  1.36it/s]Training epoch 1:  68% 9441/13868 [1:58:29<54:50,  1.35it/s]Training epoch 1:  68% 9442/13868 [1:58:29<55:05,  1.34it/s]Training epoch 1:  68% 9443/13868 [1:58:30<55:36,  1.33it/s]Training epoch 1:  68% 9444/13868 [1:58:31<56:31,  1.30it/s]Training epoch 1:  68% 9445/13868 [1:58:32<56:39,  1.30it/s]Training epoch 1:  68% 9446/13868 [1:58:33<56:17,  1.31it/s]Training epoch 1:  68% 9447/13868 [1:58:33<56:06,  1.31it/s]Training epoch 1:  68% 9448/13868 [1:58:34<55:53,  1.32it/s]Training epoch 1:  68% 9449/13868 [1:58:35<55:42,  1.32it/s]Training epoch 1:  68% 9450/13868 [1:58:36<55:42,  1.32it/s]Training epoch 1:  68% 9451/13868 [1:58:36<55:38,  1.32it/s]Training epoch 1:  68% 9452/13868 [1:58:37<55:21,  1.33it/s]Training epoch 1:  68% 9453/13868 [1:58:38<55:08,  1.33it/s]Training epoch 1:  68% 9454/13868 [1:58:39<54:54,  1.34it/s]Training epoch 1:  68% 9455/13868 [1:58:39<54:31,  1.35it/s]Training epoch 1:  68% 9456/13868 [1:58:40<55:11,  1.33it/s]Training epoch 1:  68% 9457/13868 [1:58:41<55:04,  1.33it/s]Training epoch 1:  68% 9458/13868 [1:58:42<55:20,  1.33it/s]Training epoch 1:  68% 9459/13868 [1:58:42<55:42,  1.32it/s]Training epoch 1:  68% 9460/13868 [1:58:43<55:30,  1.32it/s]Training epoch 1:  68% 9461/13868 [1:58:44<55:14,  1.33it/s]Training epoch 1:  68% 9462/13868 [1:58:45<55:37,  1.32it/s]Training epoch 1:  68% 9463/13868 [1:58:45<56:08,  1.31it/s]Training epoch 1:  68% 9464/13868 [1:58:46<55:33,  1.32it/s]Training epoch 1:  68% 9465/13868 [1:58:47<55:35,  1.32it/s]Training epoch 1:  68% 9466/13868 [1:58:48<54:56,  1.34it/s]Training epoch 1:  68% 9467/13868 [1:58:48<54:57,  1.33it/s]Training epoch 1:  68% 9468/13868 [1:58:49<55:22,  1.32it/s]Training epoch 1:  68% 9469/13868 [1:58:50<55:13,  1.33it/s]Training epoch 1:  68% 9470/13868 [1:58:51<55:26,  1.32it/s]Training epoch 1:  68% 9471/13868 [1:58:51<55:33,  1.32it/s]Training epoch 1:  68% 9472/13868 [1:58:52<55:36,  1.32it/s]Training epoch 1:  68% 9473/13868 [1:58:53<55:36,  1.32it/s]Training epoch 1:  68% 9474/13868 [1:58:54<56:08,  1.30it/s]Training epoch 1:  68% 9475/13868 [1:58:54<55:43,  1.31it/s]Training epoch 1:  68% 9476/13868 [1:58:55<55:26,  1.32it/s]Training epoch 1:  68% 9477/13868 [1:58:56<56:12,  1.30it/s]Training epoch 1:  68% 9478/13868 [1:58:57<55:56,  1.31it/s]Training epoch 1:  68% 9479/13868 [1:58:57<55:41,  1.31it/s]Training epoch 1:  68% 9480/13868 [1:58:58<55:57,  1.31it/s]Training epoch 1:  68% 9481/13868 [1:58:59<55:57,  1.31it/s]Training epoch 1:  68% 9482/13868 [1:59:00<55:18,  1.32it/s]Training epoch 1:  68% 9483/13868 [1:59:01<55:40,  1.31it/s]Training epoch 1:  68% 9484/13868 [1:59:01<55:02,  1.33it/s]Training epoch 1:  68% 9485/13868 [1:59:02<54:54,  1.33it/s]Training epoch 1:  68% 9486/13868 [1:59:03<54:47,  1.33it/s]Training epoch 1:  68% 9487/13868 [1:59:03<54:42,  1.33it/s]Training epoch 1:  68% 9488/13868 [1:59:04<55:07,  1.32it/s]Training epoch 1:  68% 9489/13868 [1:59:05<55:05,  1.32it/s]Training epoch 1:  68% 9490/13868 [1:59:06<55:08,  1.32it/s]Training epoch 1:  68% 9491/13868 [1:59:07<54:58,  1.33it/s]Training epoch 1:  68% 9492/13868 [1:59:07<54:45,  1.33it/s]Training epoch 1:  68% 9493/13868 [1:59:08<54:33,  1.34it/s]Training epoch 1:  68% 9494/13868 [1:59:09<54:30,  1.34it/s]Training epoch 1:  68% 9495/13868 [1:59:10<54:39,  1.33it/s]Training epoch 1:  68% 9496/13868 [1:59:10<54:18,  1.34it/s]Training epoch 1:  68% 9497/13868 [1:59:11<54:13,  1.34it/s]Training epoch 1:  68% 9498/13868 [1:59:12<54:43,  1.33it/s]Training epoch 1:  68% 9499/13868 [1:59:13<55:23,  1.31it/s]Training epoch 1:  69% 9500/13868 [1:59:13<59:28,  1.22it/s]Training epoch 1:  69% 9501/13868 [1:59:14<59:00,  1.23it/s]Training epoch 1:  69% 9502/13868 [1:59:15<57:26,  1.27it/s]Training epoch 1:  69% 9503/13868 [1:59:16<56:30,  1.29it/s]Training epoch 1:  69% 9504/13868 [1:59:17<56:06,  1.30it/s]Training epoch 1:  69% 9505/13868 [1:59:17<55:14,  1.32it/s]Training epoch 1:  69% 9506/13868 [1:59:18<55:11,  1.32it/s]Training epoch 1:  69% 9507/13868 [1:59:19<55:28,  1.31it/s]Training epoch 1:  69% 9508/13868 [1:59:20<55:38,  1.31it/s]Training epoch 1:  69% 9509/13868 [1:59:20<55:00,  1.32it/s]Training epoch 1:  69% 9510/13868 [1:59:21<54:51,  1.32it/s]Training epoch 1:  69% 9511/13868 [1:59:22<54:22,  1.34it/s]Training epoch 1:  69% 9512/13868 [1:59:23<54:41,  1.33it/s]Training epoch 1:  69% 9513/13868 [1:59:23<54:08,  1.34it/s]Training epoch 1:  69% 9514/13868 [1:59:24<53:33,  1.35it/s]Training epoch 1:  69% 9515/13868 [1:59:25<53:58,  1.34it/s]Training epoch 1:  69% 9516/13868 [1:59:26<55:03,  1.32it/s]Training epoch 1:  69% 9517/13868 [1:59:26<55:07,  1.32it/s]Training epoch 1:  69% 9518/13868 [1:59:27<54:58,  1.32it/s]Training epoch 1:  69% 9519/13868 [1:59:28<55:14,  1.31it/s]Training epoch 1:  69% 9520/13868 [1:59:29<54:16,  1.34it/s]Training epoch 1:  69% 9521/13868 [1:59:29<54:29,  1.33it/s]Training epoch 1:  69% 9522/13868 [1:59:30<54:40,  1.32it/s]Training epoch 1:  69% 9523/13868 [1:59:31<54:57,  1.32it/s]Training epoch 1:  69% 9524/13868 [1:59:32<54:50,  1.32it/s]Training epoch 1:  69% 9525/13868 [1:59:32<55:30,  1.30it/s]Training epoch 1:  69% 9526/13868 [1:59:33<54:44,  1.32it/s]Training epoch 1:  69% 9527/13868 [1:59:34<54:44,  1.32it/s]Training epoch 1:  69% 9528/13868 [1:59:35<54:24,  1.33it/s]Training epoch 1:  69% 9529/13868 [1:59:35<54:47,  1.32it/s]Training epoch 1:  69% 9530/13868 [1:59:36<54:04,  1.34it/s]Training epoch 1:  69% 9531/13868 [1:59:37<55:12,  1.31it/s]Training epoch 1:  69% 9532/13868 [1:59:38<55:41,  1.30it/s]Training epoch 1:  69% 9533/13868 [1:59:38<54:22,  1.33it/s]Training epoch 1:  69% 9534/13868 [1:59:39<54:37,  1.32it/s]Training epoch 1:  69% 9535/13868 [1:59:40<54:39,  1.32it/s]Training epoch 1:  69% 9536/13868 [1:59:41<54:46,  1.32it/s]Training epoch 1:  69% 9537/13868 [1:59:41<55:03,  1.31it/s]Training epoch 1:  69% 9538/13868 [1:59:42<55:00,  1.31it/s]Training epoch 1:  69% 9539/13868 [1:59:43<54:20,  1.33it/s]Training epoch 1:  69% 9540/13868 [1:59:44<54:32,  1.32it/s]Training epoch 1:  69% 9541/13868 [1:59:44<54:50,  1.31it/s]Training epoch 1:  69% 9542/13868 [1:59:45<53:40,  1.34it/s]Training epoch 1:  69% 9543/13868 [1:59:46<53:43,  1.34it/s]Training epoch 1:  69% 9544/13868 [1:59:47<54:27,  1.32it/s]Training epoch 1:  69% 9545/13868 [1:59:47<54:29,  1.32it/s]Training epoch 1:  69% 9546/13868 [1:59:48<54:46,  1.32it/s]Training epoch 1:  69% 9547/13868 [1:59:49<55:09,  1.31it/s]Training epoch 1:  69% 9548/13868 [1:59:50<54:48,  1.31it/s]Training epoch 1:  69% 9549/13868 [1:59:51<54:31,  1.32it/s]Training epoch 1:  69% 9550/13868 [1:59:51<55:00,  1.31it/s]Training epoch 1:  69% 9551/13868 [1:59:52<54:50,  1.31it/s]Training epoch 1:  69% 9552/13868 [1:59:53<54:59,  1.31it/s]Training epoch 1:  69% 9553/13868 [1:59:54<55:11,  1.30it/s]Training epoch 1:  69% 9554/13868 [1:59:54<55:22,  1.30it/s]Training epoch 1:  69% 9555/13868 [1:59:55<55:10,  1.30it/s]Training epoch 1:  69% 9556/13868 [1:59:56<54:58,  1.31it/s]Training epoch 1:  69% 9557/13868 [1:59:57<54:28,  1.32it/s]Training epoch 1:  69% 9558/13868 [1:59:57<54:36,  1.32it/s]Training epoch 1:  69% 9559/13868 [1:59:58<55:02,  1.30it/s]Training epoch 1:  69% 9560/13868 [1:59:59<54:23,  1.32it/s]Training epoch 1:  69% 9561/13868 [2:00:00<53:39,  1.34it/s]Training epoch 1:  69% 9562/13868 [2:00:00<53:42,  1.34it/s]Training epoch 1:  69% 9563/13868 [2:00:01<53:15,  1.35it/s]Training epoch 1:  69% 9564/13868 [2:00:02<54:14,  1.32it/s]Training epoch 1:  69% 9565/13868 [2:00:03<55:02,  1.30it/s]Training epoch 1:  69% 9566/13868 [2:00:03<55:11,  1.30it/s]Training epoch 1:  69% 9567/13868 [2:00:04<54:33,  1.31it/s]Training epoch 1:  69% 9568/13868 [2:00:05<55:11,  1.30it/s]Training epoch 1:  69% 9569/13868 [2:00:06<54:23,  1.32it/s]Training epoch 1:  69% 9570/13868 [2:00:07<54:21,  1.32it/s]Training epoch 1:  69% 9571/13868 [2:00:07<54:12,  1.32it/s]Training epoch 1:  69% 9572/13868 [2:00:08<53:56,  1.33it/s]Training epoch 1:  69% 9573/13868 [2:00:09<54:20,  1.32it/s]Training epoch 1:  69% 9574/13868 [2:00:10<54:54,  1.30it/s]Training epoch 1:  69% 9575/13868 [2:00:10<54:26,  1.31it/s]Training epoch 1:  69% 9576/13868 [2:00:11<54:01,  1.32it/s]Training epoch 1:  69% 9577/13868 [2:00:12<54:38,  1.31it/s]Training epoch 1:  69% 9578/13868 [2:00:13<55:22,  1.29it/s]Training epoch 1:  69% 9579/13868 [2:00:13<54:34,  1.31it/s]Training epoch 1:  69% 9580/13868 [2:00:14<55:01,  1.30it/s]Training epoch 1:  69% 9581/13868 [2:00:15<54:58,  1.30it/s]Training epoch 1:  69% 9582/13868 [2:00:16<54:36,  1.31it/s]Training epoch 1:  69% 9583/13868 [2:00:16<54:36,  1.31it/s]Training epoch 1:  69% 9584/13868 [2:00:17<54:53,  1.30it/s]Training epoch 1:  69% 9585/13868 [2:00:18<54:15,  1.32it/s]Training epoch 1:  69% 9586/13868 [2:00:19<54:49,  1.30it/s]Training epoch 1:  69% 9587/13868 [2:00:20<54:57,  1.30it/s]Training epoch 1:  69% 9588/13868 [2:00:20<55:35,  1.28it/s]Training epoch 1:  69% 9589/13868 [2:00:21<55:36,  1.28it/s]Training epoch 1:  69% 9590/13868 [2:00:22<55:02,  1.30it/s]Training epoch 1:  69% 9591/13868 [2:00:23<54:43,  1.30it/s]Training epoch 1:  69% 9592/13868 [2:00:23<54:44,  1.30it/s]Training epoch 1:  69% 9593/13868 [2:00:24<54:43,  1.30it/s]Training epoch 1:  69% 9594/13868 [2:00:25<55:23,  1.29it/s]Training epoch 1:  69% 9595/13868 [2:00:26<55:54,  1.27it/s]Training epoch 1:  69% 9596/13868 [2:00:27<55:55,  1.27it/s]Training epoch 1:  69% 9597/13868 [2:00:27<55:16,  1.29it/s]Training epoch 1:  69% 9598/13868 [2:00:28<54:56,  1.30it/s]Training epoch 1:  69% 9599/13868 [2:00:29<54:26,  1.31it/s]Training epoch 1:  69% 9600/13868 [2:00:30<58:07,  1.22it/s]Training epoch 1:  69% 9601/13868 [2:00:31<57:00,  1.25it/s]Training epoch 1:  69% 9602/13868 [2:00:31<55:31,  1.28it/s]Training epoch 1:  69% 9603/13868 [2:00:32<54:45,  1.30it/s]Training epoch 1:  69% 9604/13868 [2:00:33<54:36,  1.30it/s]Training epoch 1:  69% 9605/13868 [2:00:34<54:59,  1.29it/s]Training epoch 1:  69% 9606/13868 [2:00:34<55:05,  1.29it/s]Training epoch 1:  69% 9607/13868 [2:00:35<54:27,  1.30it/s]Training epoch 1:  69% 9608/13868 [2:00:36<53:42,  1.32it/s]Training epoch 1:  69% 9609/13868 [2:00:37<53:47,  1.32it/s]Training epoch 1:  69% 9610/13868 [2:00:37<53:41,  1.32it/s]Training epoch 1:  69% 9611/13868 [2:00:38<53:16,  1.33it/s]Training epoch 1:  69% 9612/13868 [2:00:39<52:43,  1.35it/s]Training epoch 1:  69% 9613/13868 [2:00:40<53:26,  1.33it/s]Training epoch 1:  69% 9614/13868 [2:00:40<53:34,  1.32it/s]Training epoch 1:  69% 9615/13868 [2:00:41<53:32,  1.32it/s]Training epoch 1:  69% 9616/13868 [2:00:42<54:10,  1.31it/s]Training epoch 1:  69% 9617/13868 [2:00:43<53:38,  1.32it/s]Training epoch 1:  69% 9618/13868 [2:00:43<53:36,  1.32it/s]Training epoch 1:  69% 9619/13868 [2:00:44<52:48,  1.34it/s]Training epoch 1:  69% 9620/13868 [2:00:45<52:56,  1.34it/s]Training epoch 1:  69% 9621/13868 [2:00:46<53:17,  1.33it/s]Training epoch 1:  69% 9622/13868 [2:00:46<53:49,  1.31it/s]Training epoch 1:  69% 9623/13868 [2:00:47<53:42,  1.32it/s]Training epoch 1:  69% 9624/13868 [2:00:48<54:22,  1.30it/s]Training epoch 1:  69% 9625/13868 [2:00:49<53:55,  1.31it/s]Training epoch 1:  69% 9626/13868 [2:00:49<54:01,  1.31it/s]Training epoch 1:  69% 9627/13868 [2:00:50<54:12,  1.30it/s]Training epoch 1:  69% 9628/13868 [2:00:51<54:38,  1.29it/s]Training epoch 1:  69% 9629/13868 [2:00:52<54:43,  1.29it/s]Training epoch 1:  69% 9630/13868 [2:00:53<54:28,  1.30it/s]Training epoch 1:  69% 9631/13868 [2:00:53<54:25,  1.30it/s]Training epoch 1:  69% 9632/13868 [2:00:54<54:10,  1.30it/s]Training epoch 1:  69% 9633/13868 [2:00:55<53:07,  1.33it/s]Training epoch 1:  69% 9634/13868 [2:00:56<53:25,  1.32it/s]Training epoch 1:  69% 9635/13868 [2:00:56<53:20,  1.32it/s]Training epoch 1:  69% 9636/13868 [2:00:57<54:02,  1.31it/s]Training epoch 1:  69% 9637/13868 [2:00:58<54:20,  1.30it/s]Training epoch 1:  69% 9638/13868 [2:00:59<53:24,  1.32it/s]Training epoch 1:  70% 9639/13868 [2:00:59<53:20,  1.32it/s]Training epoch 1:  70% 9640/13868 [2:01:00<53:53,  1.31it/s]Training epoch 1:  70% 9641/13868 [2:01:01<54:06,  1.30it/s]Training epoch 1:  70% 9642/13868 [2:01:02<53:48,  1.31it/s]Training epoch 1:  70% 9643/13868 [2:01:02<53:26,  1.32it/s]Training epoch 1:  70% 9644/13868 [2:01:03<54:14,  1.30it/s]Training epoch 1:  70% 9645/13868 [2:01:04<53:51,  1.31it/s]Training epoch 1:  70% 9646/13868 [2:01:05<53:51,  1.31it/s]Training epoch 1:  70% 9647/13868 [2:01:05<53:19,  1.32it/s]Training epoch 1:  70% 9648/13868 [2:01:06<53:31,  1.31it/s]Training epoch 1:  70% 9649/13868 [2:01:07<53:38,  1.31it/s]Training epoch 1:  70% 9650/13868 [2:01:08<53:08,  1.32it/s]Training epoch 1:  70% 9651/13868 [2:01:09<53:55,  1.30it/s]Training epoch 1:  70% 9652/13868 [2:01:09<54:04,  1.30it/s]Training epoch 1:  70% 9653/13868 [2:01:10<53:53,  1.30it/s]Training epoch 1:  70% 9654/13868 [2:01:11<54:18,  1.29it/s]Training epoch 1:  70% 9655/13868 [2:01:12<53:41,  1.31it/s]Training epoch 1:  70% 9656/13868 [2:01:12<54:20,  1.29it/s]Training epoch 1:  70% 9657/13868 [2:01:13<54:02,  1.30it/s]Training epoch 1:  70% 9658/13868 [2:01:14<54:16,  1.29it/s]Training epoch 1:  70% 9659/13868 [2:01:15<54:10,  1.29it/s]Training epoch 1:  70% 9660/13868 [2:01:15<53:19,  1.32it/s]Training epoch 1:  70% 9661/13868 [2:01:16<53:18,  1.32it/s]Training epoch 1:  70% 9662/13868 [2:01:17<53:28,  1.31it/s]Training epoch 1:  70% 9663/13868 [2:01:18<53:29,  1.31it/s]Training epoch 1:  70% 9664/13868 [2:01:19<53:23,  1.31it/s]Training epoch 1:  70% 9665/13868 [2:01:19<52:54,  1.32it/s]Training epoch 1:  70% 9666/13868 [2:01:20<53:17,  1.31it/s]Training epoch 1:  70% 9667/13868 [2:01:21<53:21,  1.31it/s]Training epoch 1:  70% 9668/13868 [2:01:22<53:39,  1.30it/s]Training epoch 1:  70% 9669/13868 [2:01:22<52:27,  1.33it/s]Training epoch 1:  70% 9670/13868 [2:01:23<52:27,  1.33it/s]Training epoch 1:  70% 9671/13868 [2:01:24<51:56,  1.35it/s]Training epoch 1:  70% 9672/13868 [2:01:25<52:09,  1.34it/s]Training epoch 1:  70% 9673/13868 [2:01:25<51:56,  1.35it/s]Training epoch 1:  70% 9674/13868 [2:01:26<53:00,  1.32it/s]Training epoch 1:  70% 9675/13868 [2:01:27<52:15,  1.34it/s]Training epoch 1:  70% 9676/13868 [2:01:28<52:37,  1.33it/s]Training epoch 1:  70% 9677/13868 [2:01:28<52:06,  1.34it/s]Training epoch 1:  70% 9678/13868 [2:01:29<52:21,  1.33it/s]Training epoch 1:  70% 9679/13868 [2:01:30<52:37,  1.33it/s]Training epoch 1:  70% 9680/13868 [2:01:31<52:27,  1.33it/s]Training epoch 1:  70% 9681/13868 [2:01:31<51:54,  1.34it/s]Training epoch 1:  70% 9682/13868 [2:01:32<52:19,  1.33it/s]Training epoch 1:  70% 9683/13868 [2:01:33<52:58,  1.32it/s]Training epoch 1:  70% 9684/13868 [2:01:34<53:11,  1.31it/s]Training epoch 1:  70% 9685/13868 [2:01:34<53:47,  1.30it/s]Training epoch 1:  70% 9686/13868 [2:01:35<53:53,  1.29it/s]Training epoch 1:  70% 9687/13868 [2:01:36<53:22,  1.31it/s]Training epoch 1:  70% 9688/13868 [2:01:37<53:26,  1.30it/s]Training epoch 1:  70% 9689/13868 [2:01:37<53:48,  1.29it/s]Training epoch 1:  70% 9690/13868 [2:01:38<53:14,  1.31it/s]Training epoch 1:  70% 9691/13868 [2:01:39<52:56,  1.31it/s]Training epoch 1:  70% 9692/13868 [2:01:40<53:35,  1.30it/s]Training epoch 1:  70% 9693/13868 [2:01:40<53:10,  1.31it/s]Training epoch 1:  70% 9694/13868 [2:01:41<53:08,  1.31it/s]Training epoch 1:  70% 9695/13868 [2:01:42<53:20,  1.30it/s]Training epoch 1:  70% 9696/13868 [2:01:43<53:48,  1.29it/s]Training epoch 1:  70% 9697/13868 [2:01:44<54:07,  1.28it/s]Training epoch 1:  70% 9698/13868 [2:01:44<53:52,  1.29it/s]Training epoch 1:  70% 9699/13868 [2:01:45<53:01,  1.31it/s]Training epoch 1:  70% 9700/13868 [2:01:46<55:55,  1.24it/s]Training epoch 1:  70% 9701/13868 [2:01:47<54:47,  1.27it/s]Training epoch 1:  70% 9702/13868 [2:01:48<54:07,  1.28it/s]Training epoch 1:  70% 9703/13868 [2:01:48<53:33,  1.30it/s]Training epoch 1:  70% 9704/13868 [2:01:49<53:20,  1.30it/s]Training epoch 1:  70% 9705/13868 [2:01:50<52:20,  1.33it/s]Training epoch 1:  70% 9706/13868 [2:01:51<52:20,  1.33it/s]Training epoch 1:  70% 9707/13868 [2:01:51<52:28,  1.32it/s]Training epoch 1:  70% 9708/13868 [2:01:52<53:50,  1.29it/s]Training epoch 1:  70% 9709/13868 [2:01:53<53:57,  1.28it/s]Training epoch 1:  70% 9710/13868 [2:01:54<53:49,  1.29it/s]Training epoch 1:  70% 9711/13868 [2:01:54<53:21,  1.30it/s]Training epoch 1:  70% 9712/13868 [2:01:55<52:59,  1.31it/s]Training epoch 1:  70% 9713/13868 [2:01:56<53:05,  1.30it/s]Training epoch 1:  70% 9714/13868 [2:01:57<53:08,  1.30it/s]Training epoch 1:  70% 9715/13868 [2:01:57<53:03,  1.30it/s]Training epoch 1:  70% 9716/13868 [2:01:58<52:05,  1.33it/s]Training epoch 1:  70% 9717/13868 [2:01:59<52:24,  1.32it/s]Training epoch 1:  70% 9718/13868 [2:02:00<52:43,  1.31it/s]Training epoch 1:  70% 9719/13868 [2:02:00<52:34,  1.32it/s]Training epoch 1:  70% 9720/13868 [2:02:01<52:57,  1.31it/s]Training epoch 1:  70% 9721/13868 [2:02:02<53:13,  1.30it/s]Training epoch 1:  70% 9722/13868 [2:02:03<53:07,  1.30it/s]Training epoch 1:  70% 9723/13868 [2:02:04<53:03,  1.30it/s]Training epoch 1:  70% 9724/13868 [2:02:04<53:15,  1.30it/s]Training epoch 1:  70% 9725/13868 [2:02:05<53:03,  1.30it/s]Training epoch 1:  70% 9726/13868 [2:02:06<52:08,  1.32it/s]Training epoch 1:  70% 9727/13868 [2:02:07<52:18,  1.32it/s]Training epoch 1:  70% 9728/13868 [2:02:07<52:33,  1.31it/s]Training epoch 1:  70% 9729/13868 [2:02:08<52:10,  1.32it/s]Training epoch 1:  70% 9730/13868 [2:02:09<52:41,  1.31it/s]Training epoch 1:  70% 9731/13868 [2:02:10<52:45,  1.31it/s]Training epoch 1:  70% 9732/13868 [2:02:10<52:35,  1.31it/s]Training epoch 1:  70% 9733/13868 [2:02:11<51:57,  1.33it/s]Training epoch 1:  70% 9734/13868 [2:02:12<52:09,  1.32it/s]Training epoch 1:  70% 9735/13868 [2:02:13<51:41,  1.33it/s]Training epoch 1:  70% 9736/13868 [2:02:13<51:58,  1.33it/s]Training epoch 1:  70% 9737/13868 [2:02:14<52:11,  1.32it/s]Training epoch 1:  70% 9738/13868 [2:02:15<51:38,  1.33it/s]Training epoch 1:  70% 9739/13868 [2:02:16<52:12,  1.32it/s]Training epoch 1:  70% 9740/13868 [2:02:16<52:43,  1.30it/s]Training epoch 1:  70% 9741/13868 [2:02:17<53:10,  1.29it/s]Training epoch 1:  70% 9742/13868 [2:02:18<52:21,  1.31it/s]Training epoch 1:  70% 9743/13868 [2:02:19<52:57,  1.30it/s]Training epoch 1:  70% 9744/13868 [2:02:20<52:36,  1.31it/s]Training epoch 1:  70% 9745/13868 [2:02:20<52:07,  1.32it/s]Training epoch 1:  70% 9746/13868 [2:02:21<52:00,  1.32it/s]Training epoch 1:  70% 9747/13868 [2:02:22<52:14,  1.31it/s]Training epoch 1:  70% 9748/13868 [2:02:23<52:58,  1.30it/s]Training epoch 1:  70% 9749/13868 [2:02:23<52:08,  1.32it/s]Training epoch 1:  70% 9750/13868 [2:02:24<52:22,  1.31it/s]Training epoch 1:  70% 9751/13868 [2:02:25<52:00,  1.32it/s]Training epoch 1:  70% 9752/13868 [2:02:26<52:33,  1.31it/s]Training epoch 1:  70% 9753/13868 [2:02:26<52:04,  1.32it/s]Training epoch 1:  70% 9754/13868 [2:02:27<52:01,  1.32it/s]Training epoch 1:  70% 9755/13868 [2:02:28<51:58,  1.32it/s]Training epoch 1:  70% 9756/13868 [2:02:29<52:05,  1.32it/s]Training epoch 1:  70% 9757/13868 [2:02:29<52:42,  1.30it/s]Training epoch 1:  70% 9758/13868 [2:02:30<52:23,  1.31it/s]Training epoch 1:  70% 9759/13868 [2:02:31<52:00,  1.32it/s]Training epoch 1:  70% 9760/13868 [2:02:32<51:46,  1.32it/s]Training epoch 1:  70% 9761/13868 [2:02:32<52:14,  1.31it/s]Training epoch 1:  70% 9762/13868 [2:02:33<52:33,  1.30it/s]Training epoch 1:  70% 9763/13868 [2:02:34<52:48,  1.30it/s]Training epoch 1:  70% 9764/13868 [2:02:35<52:38,  1.30it/s]Training epoch 1:  70% 9765/13868 [2:02:36<52:14,  1.31it/s]Training epoch 1:  70% 9766/13868 [2:02:36<52:52,  1.29it/s]Training epoch 1:  70% 9767/13868 [2:02:37<52:23,  1.30it/s]Training epoch 1:  70% 9768/13868 [2:02:38<52:41,  1.30it/s]Training epoch 1:  70% 9769/13868 [2:02:39<52:30,  1.30it/s]Training epoch 1:  70% 9770/13868 [2:02:39<52:04,  1.31it/s]Training epoch 1:  70% 9771/13868 [2:02:40<51:18,  1.33it/s]Training epoch 1:  70% 9772/13868 [2:02:41<52:00,  1.31it/s]Training epoch 1:  70% 9773/13868 [2:02:42<51:44,  1.32it/s]Training epoch 1:  70% 9774/13868 [2:02:42<51:24,  1.33it/s]Training epoch 1:  70% 9775/13868 [2:02:43<51:29,  1.32it/s]Training epoch 1:  70% 9776/13868 [2:02:44<52:12,  1.31it/s]Training epoch 1:  71% 9777/13868 [2:02:45<51:45,  1.32it/s]Training epoch 1:  71% 9778/13868 [2:02:45<51:48,  1.32it/s]Training epoch 1:  71% 9779/13868 [2:02:46<52:08,  1.31it/s]Training epoch 1:  71% 9780/13868 [2:02:47<51:32,  1.32it/s]Training epoch 1:  71% 9781/13868 [2:02:48<51:31,  1.32it/s]Training epoch 1:  71% 9782/13868 [2:02:48<51:25,  1.32it/s]Training epoch 1:  71% 9783/13868 [2:02:49<51:33,  1.32it/s]Training epoch 1:  71% 9784/13868 [2:02:50<52:10,  1.30it/s]Training epoch 1:  71% 9785/13868 [2:02:51<52:04,  1.31it/s]Training epoch 1:  71% 9786/13868 [2:02:52<52:07,  1.31it/s]Training epoch 1:  71% 9787/13868 [2:02:52<51:16,  1.33it/s]Training epoch 1:  71% 9788/13868 [2:02:53<51:08,  1.33it/s]Training epoch 1:  71% 9789/13868 [2:02:54<51:30,  1.32it/s]Training epoch 1:  71% 9790/13868 [2:02:55<51:54,  1.31it/s]Training epoch 1:  71% 9791/13868 [2:02:55<52:23,  1.30it/s]Training epoch 1:  71% 9792/13868 [2:02:56<51:49,  1.31it/s]Training epoch 1:  71% 9793/13868 [2:02:57<51:49,  1.31it/s]Training epoch 1:  71% 9794/13868 [2:02:58<51:51,  1.31it/s]Training epoch 1:  71% 9795/13868 [2:02:58<51:47,  1.31it/s]Training epoch 1:  71% 9796/13868 [2:02:59<52:10,  1.30it/s]Training epoch 1:  71% 9797/13868 [2:03:00<51:16,  1.32it/s]Training epoch 1:  71% 9798/13868 [2:03:01<51:10,  1.33it/s]Training epoch 1:  71% 9799/13868 [2:03:01<51:18,  1.32it/s]Training epoch 1:  71% 9800/13868 [2:03:02<54:16,  1.25it/s]Training epoch 1:  71% 9801/13868 [2:03:03<53:32,  1.27it/s]Training epoch 1:  71% 9802/13868 [2:03:04<52:55,  1.28it/s]Training epoch 1:  71% 9803/13868 [2:03:05<52:18,  1.30it/s]Training epoch 1:  71% 9804/13868 [2:03:05<51:51,  1.31it/s]Training epoch 1:  71% 9805/13868 [2:03:06<51:49,  1.31it/s]Training epoch 1:  71% 9806/13868 [2:03:07<51:31,  1.31it/s]Training epoch 1:  71% 9807/13868 [2:03:08<50:44,  1.33it/s]Training epoch 1:  71% 9808/13868 [2:03:08<51:03,  1.33it/s]Training epoch 1:  71% 9809/13868 [2:03:09<51:22,  1.32it/s]Training epoch 1:  71% 9810/13868 [2:03:10<51:34,  1.31it/s]Training epoch 1:  71% 9811/13868 [2:03:11<51:10,  1.32it/s]Training epoch 1:  71% 9812/13868 [2:03:11<50:48,  1.33it/s]Training epoch 1:  71% 9813/13868 [2:03:12<51:01,  1.32it/s]Training epoch 1:  71% 9814/13868 [2:03:13<51:20,  1.32it/s]Training epoch 1:  71% 9815/13868 [2:03:14<51:56,  1.30it/s]Training epoch 1:  71% 9816/13868 [2:03:14<52:13,  1.29it/s]Training epoch 1:  71% 9817/13868 [2:03:15<52:15,  1.29it/s]Training epoch 1:  71% 9818/13868 [2:03:16<51:54,  1.30it/s]Training epoch 1:  71% 9819/13868 [2:03:17<51:50,  1.30it/s]Training epoch 1:  71% 9820/13868 [2:03:18<52:06,  1.29it/s]Training epoch 1:  71% 9821/13868 [2:03:18<52:09,  1.29it/s]Training epoch 1:  71% 9822/13868 [2:03:19<52:33,  1.28it/s]Training epoch 1:  71% 9823/13868 [2:03:20<51:15,  1.32it/s]Training epoch 1:  71% 9824/13868 [2:03:21<51:28,  1.31it/s]Training epoch 1:  71% 9825/13868 [2:03:21<50:26,  1.34it/s]Training epoch 1:  71% 9826/13868 [2:03:22<49:53,  1.35it/s]Training epoch 1:  71% 9827/13868 [2:03:23<49:51,  1.35it/s]Training epoch 1:  71% 9828/13868 [2:03:24<50:19,  1.34it/s]Training epoch 1:  71% 9829/13868 [2:03:24<51:06,  1.32it/s]Training epoch 1:  71% 9830/13868 [2:03:25<51:31,  1.31it/s]Training epoch 1:  71% 9831/13868 [2:03:26<51:06,  1.32it/s]Training epoch 1:  71% 9832/13868 [2:03:27<50:57,  1.32it/s]Training epoch 1:  71% 9833/13868 [2:03:27<51:11,  1.31it/s]Training epoch 1:  71% 9834/13868 [2:03:28<50:38,  1.33it/s]Training epoch 1:  71% 9835/13868 [2:03:29<50:59,  1.32it/s]Training epoch 1:  71% 9836/13868 [2:03:30<51:26,  1.31it/s]Training epoch 1:  71% 9837/13868 [2:03:30<51:36,  1.30it/s]Training epoch 1:  71% 9838/13868 [2:03:31<51:39,  1.30it/s]Training epoch 1:  71% 9839/13868 [2:03:32<51:40,  1.30it/s]Training epoch 1:  71% 9840/13868 [2:03:33<51:20,  1.31it/s]Training epoch 1:  71% 9841/13868 [2:03:34<51:18,  1.31it/s]Training epoch 1:  71% 9842/13868 [2:03:34<51:31,  1.30it/s]Training epoch 1:  71% 9843/13868 [2:03:35<52:33,  1.28it/s]Training epoch 1:  71% 9844/13868 [2:03:36<52:42,  1.27it/s]Training epoch 1:  71% 9845/13868 [2:03:37<52:19,  1.28it/s]Training epoch 1:  71% 9846/13868 [2:03:37<52:28,  1.28it/s]Training epoch 1:  71% 9847/13868 [2:03:38<51:33,  1.30it/s]Training epoch 1:  71% 9848/13868 [2:03:39<51:16,  1.31it/s]Training epoch 1:  71% 9849/13868 [2:03:40<51:25,  1.30it/s]Training epoch 1:  71% 9850/13868 [2:03:40<51:22,  1.30it/s]Training epoch 1:  71% 9851/13868 [2:03:41<51:21,  1.30it/s]Training epoch 1:  71% 9852/13868 [2:03:42<51:22,  1.30it/s]Training epoch 1:  71% 9853/13868 [2:03:43<51:20,  1.30it/s]Training epoch 1:  71% 9854/13868 [2:03:44<50:56,  1.31it/s]Training epoch 1:  71% 9855/13868 [2:03:44<50:51,  1.32it/s]Training epoch 1:  71% 9856/13868 [2:03:45<50:45,  1.32it/s]Training epoch 1:  71% 9857/13868 [2:03:46<49:52,  1.34it/s]Training epoch 1:  71% 9858/13868 [2:03:47<50:17,  1.33it/s]Training epoch 1:  71% 9859/13868 [2:03:47<49:58,  1.34it/s]Training epoch 1:  71% 9860/13868 [2:03:48<50:36,  1.32it/s]Training epoch 1:  71% 9861/13868 [2:03:49<50:58,  1.31it/s]Training epoch 1:  71% 9862/13868 [2:03:50<50:48,  1.31it/s]Training epoch 1:  71% 9863/13868 [2:03:50<50:54,  1.31it/s]Training epoch 1:  71% 9864/13868 [2:03:51<51:23,  1.30it/s]Training epoch 1:  71% 9865/13868 [2:03:52<51:01,  1.31it/s]Training epoch 1:  71% 9866/13868 [2:03:53<51:53,  1.29it/s]Training epoch 1:  71% 9867/13868 [2:03:53<51:19,  1.30it/s]Training epoch 1:  71% 9868/13868 [2:03:54<51:07,  1.30it/s]Training epoch 1:  71% 9869/13868 [2:03:55<50:59,  1.31it/s]Training epoch 1:  71% 9870/13868 [2:03:56<50:55,  1.31it/s]Training epoch 1:  71% 9871/13868 [2:03:57<51:07,  1.30it/s]Training epoch 1:  71% 9872/13868 [2:03:57<51:42,  1.29it/s]Training epoch 1:  71% 9873/13868 [2:03:58<51:36,  1.29it/s]Training epoch 1:  71% 9874/13868 [2:03:59<50:24,  1.32it/s]Training epoch 1:  71% 9875/13868 [2:04:00<50:35,  1.32it/s]Training epoch 1:  71% 9876/13868 [2:04:00<50:52,  1.31it/s]Training epoch 1:  71% 9877/13868 [2:04:01<51:12,  1.30it/s]Training epoch 1:  71% 9878/13868 [2:04:02<51:05,  1.30it/s]Training epoch 1:  71% 9879/13868 [2:04:03<51:00,  1.30it/s]Training epoch 1:  71% 9880/13868 [2:04:03<50:03,  1.33it/s]Training epoch 1:  71% 9881/13868 [2:04:04<50:07,  1.33it/s]Training epoch 1:  71% 9882/13868 [2:04:05<50:37,  1.31it/s]Training epoch 1:  71% 9883/13868 [2:04:06<50:04,  1.33it/s]Training epoch 1:  71% 9884/13868 [2:04:06<50:30,  1.31it/s]Training epoch 1:  71% 9885/13868 [2:04:07<51:19,  1.29it/s]Training epoch 1:  71% 9886/13868 [2:04:08<50:03,  1.33it/s]Training epoch 1:  71% 9887/13868 [2:04:09<50:47,  1.31it/s]Training epoch 1:  71% 9888/13868 [2:04:09<49:49,  1.33it/s]Training epoch 1:  71% 9889/13868 [2:04:10<49:17,  1.35it/s]Training epoch 1:  71% 9890/13868 [2:04:11<49:38,  1.34it/s]Training epoch 1:  71% 9891/13868 [2:04:12<49:34,  1.34it/s]Training epoch 1:  71% 9892/13868 [2:04:12<49:06,  1.35it/s]Training epoch 1:  71% 9893/13868 [2:04:13<50:15,  1.32it/s]Training epoch 1:  71% 9894/13868 [2:04:14<50:16,  1.32it/s]Training epoch 1:  71% 9895/13868 [2:04:15<50:19,  1.32it/s]Training epoch 1:  71% 9896/13868 [2:04:15<50:10,  1.32it/s]Training epoch 1:  71% 9897/13868 [2:04:16<50:52,  1.30it/s]Training epoch 1:  71% 9898/13868 [2:04:17<50:12,  1.32it/s]Training epoch 1:  71% 9899/13868 [2:04:18<50:24,  1.31it/s]Training epoch 1:  71% 9900/13868 [2:04:19<53:01,  1.25it/s]Training epoch 1:  71% 9901/13868 [2:04:19<51:20,  1.29it/s]Training epoch 1:  71% 9902/13868 [2:04:20<50:54,  1.30it/s]Training epoch 1:  71% 9903/13868 [2:04:21<50:14,  1.32it/s]Training epoch 1:  71% 9904/13868 [2:04:22<49:52,  1.32it/s]Training epoch 1:  71% 9905/13868 [2:04:22<50:00,  1.32it/s]Training epoch 1:  71% 9906/13868 [2:04:23<50:01,  1.32it/s]Training epoch 1:  71% 9907/13868 [2:04:24<49:19,  1.34it/s]Training epoch 1:  71% 9908/13868 [2:04:25<49:43,  1.33it/s]Training epoch 1:  71% 9909/13868 [2:04:25<49:51,  1.32it/s]Training epoch 1:  71% 9910/13868 [2:04:26<49:42,  1.33it/s]Training epoch 1:  71% 9911/13868 [2:04:27<49:38,  1.33it/s]Training epoch 1:  71% 9912/13868 [2:04:28<49:37,  1.33it/s]Training epoch 1:  71% 9913/13868 [2:04:28<49:39,  1.33it/s]Training epoch 1:  71% 9914/13868 [2:04:29<49:56,  1.32it/s]Training epoch 1:  71% 9915/13868 [2:04:30<49:27,  1.33it/s]Training epoch 1:  72% 9916/13868 [2:04:31<50:03,  1.32it/s]Training epoch 1:  72% 9917/13868 [2:04:31<50:02,  1.32it/s]Training epoch 1:  72% 9918/13868 [2:04:32<50:55,  1.29it/s]Training epoch 1:  72% 9919/13868 [2:04:33<50:59,  1.29it/s]Training epoch 1:  72% 9920/13868 [2:04:34<50:32,  1.30it/s]Training epoch 1:  72% 9921/13868 [2:04:35<50:11,  1.31it/s]Training epoch 1:  72% 9922/13868 [2:04:35<50:37,  1.30it/s]Training epoch 1:  72% 9923/13868 [2:04:36<50:02,  1.31it/s]Training epoch 1:  72% 9924/13868 [2:04:37<49:54,  1.32it/s]Training epoch 1:  72% 9925/13868 [2:04:38<50:04,  1.31it/s]Training epoch 1:  72% 9926/13868 [2:04:38<49:48,  1.32it/s]Training epoch 1:  72% 9927/13868 [2:04:39<50:21,  1.30it/s]Training epoch 1:  72% 9928/13868 [2:04:40<50:28,  1.30it/s]Training epoch 1:  72% 9929/13868 [2:04:41<49:43,  1.32it/s]Training epoch 1:  72% 9930/13868 [2:04:41<49:26,  1.33it/s]Training epoch 1:  72% 9931/13868 [2:04:42<48:55,  1.34it/s]Training epoch 1:  72% 9932/13868 [2:04:43<49:25,  1.33it/s]Training epoch 1:  72% 9933/13868 [2:04:44<48:34,  1.35it/s]Training epoch 1:  72% 9934/13868 [2:04:44<48:36,  1.35it/s]Training epoch 1:  72% 9935/13868 [2:04:45<48:22,  1.35it/s]Training epoch 1:  72% 9936/13868 [2:04:46<48:49,  1.34it/s]Training epoch 1:  72% 9937/13868 [2:04:47<49:24,  1.33it/s]Training epoch 1:  72% 9938/13868 [2:04:47<49:32,  1.32it/s]Training epoch 1:  72% 9939/13868 [2:04:48<49:23,  1.33it/s]Training epoch 1:  72% 9940/13868 [2:04:49<49:57,  1.31it/s]Training epoch 1:  72% 9941/13868 [2:04:50<49:03,  1.33it/s]Training epoch 1:  72% 9942/13868 [2:04:50<49:11,  1.33it/s]Training epoch 1:  72% 9943/13868 [2:04:51<48:39,  1.34it/s]Training epoch 1:  72% 9944/13868 [2:04:52<49:01,  1.33it/s]Training epoch 1:  72% 9945/13868 [2:04:53<49:08,  1.33it/s]Training epoch 1:  72% 9946/13868 [2:04:53<48:59,  1.33it/s]Training epoch 1:  72% 9947/13868 [2:04:54<48:51,  1.34it/s]Training epoch 1:  72% 9948/13868 [2:04:55<49:27,  1.32it/s]Training epoch 1:  72% 9949/13868 [2:04:56<49:32,  1.32it/s]Training epoch 1:  72% 9950/13868 [2:04:56<49:55,  1.31it/s]Training epoch 1:  72% 9951/13868 [2:04:57<49:40,  1.31it/s]Training epoch 1:  72% 9952/13868 [2:04:58<50:12,  1.30it/s]Training epoch 1:  72% 9953/13868 [2:04:59<49:19,  1.32it/s]Training epoch 1:  72% 9954/13868 [2:04:59<49:28,  1.32it/s]Training epoch 1:  72% 9955/13868 [2:05:00<48:33,  1.34it/s]Training epoch 1:  72% 9956/13868 [2:05:01<48:08,  1.35it/s]Training epoch 1:  72% 9957/13868 [2:05:02<48:02,  1.36it/s]Training epoch 1:  72% 9958/13868 [2:05:02<48:16,  1.35it/s]Training epoch 1:  72% 9959/13868 [2:05:03<48:11,  1.35it/s]Training epoch 1:  72% 9960/13868 [2:05:04<48:52,  1.33it/s]Training epoch 1:  72% 9961/13868 [2:05:05<48:34,  1.34it/s]Training epoch 1:  72% 9962/13868 [2:05:05<48:34,  1.34it/s]Training epoch 1:  72% 9963/13868 [2:05:06<48:38,  1.34it/s]Training epoch 1:  72% 9964/13868 [2:05:07<48:57,  1.33it/s]Training epoch 1:  72% 9965/13868 [2:05:08<48:40,  1.34it/s]Training epoch 1:  72% 9966/13868 [2:05:08<49:13,  1.32it/s]Training epoch 1:  72% 9967/13868 [2:05:09<49:11,  1.32it/s]Training epoch 1:  72% 9968/13868 [2:05:10<49:05,  1.32it/s]Training epoch 1:  72% 9969/13868 [2:05:11<49:11,  1.32it/s]Training epoch 1:  72% 9970/13868 [2:05:11<49:45,  1.31it/s]Training epoch 1:  72% 9971/13868 [2:05:12<49:23,  1.32it/s]Training epoch 1:  72% 9972/13868 [2:05:13<49:39,  1.31it/s]Training epoch 1:  72% 9973/13868 [2:05:14<49:30,  1.31it/s]Training epoch 1:  72% 9974/13868 [2:05:14<49:30,  1.31it/s]Training epoch 1:  72% 9975/13868 [2:05:15<48:54,  1.33it/s]Training epoch 1:  72% 9976/13868 [2:05:16<49:19,  1.32it/s]Training epoch 1:  72% 9977/13868 [2:05:17<48:32,  1.34it/s]Training epoch 1:  72% 9978/13868 [2:05:17<48:17,  1.34it/s]Training epoch 1:  72% 9979/13868 [2:05:18<48:49,  1.33it/s]Training epoch 1:  72% 9980/13868 [2:05:19<49:03,  1.32it/s]Training epoch 1:  72% 9981/13868 [2:05:20<48:44,  1.33it/s]Training epoch 1:  72% 9982/13868 [2:05:20<48:33,  1.33it/s]Training epoch 1:  72% 9983/13868 [2:05:21<48:53,  1.32it/s]Training epoch 1:  72% 9984/13868 [2:05:22<49:04,  1.32it/s]Training epoch 1:  72% 9985/13868 [2:05:23<49:27,  1.31it/s]Training epoch 1:  72% 9986/13868 [2:05:24<49:06,  1.32it/s]Training epoch 1:  72% 9987/13868 [2:05:24<48:45,  1.33it/s]Training epoch 1:  72% 9988/13868 [2:05:25<48:52,  1.32it/s]Training epoch 1:  72% 9989/13868 [2:05:26<49:13,  1.31it/s]Training epoch 1:  72% 9990/13868 [2:05:27<49:48,  1.30it/s]Training epoch 1:  72% 9991/13868 [2:05:27<49:12,  1.31it/s]Training epoch 1:  72% 9992/13868 [2:05:28<48:51,  1.32it/s]Training epoch 1:  72% 9993/13868 [2:05:29<48:35,  1.33it/s]Training epoch 1:  72% 9994/13868 [2:05:30<48:31,  1.33it/s]Training epoch 1:  72% 9995/13868 [2:05:30<48:45,  1.32it/s]Training epoch 1:  72% 9996/13868 [2:05:31<48:46,  1.32it/s]Training epoch 1:  72% 9997/13868 [2:05:32<49:04,  1.31it/s]Training epoch 1:  72% 9998/13868 [2:05:33<48:52,  1.32it/s]Training epoch 1:  72% 9999/13868 [2:05:33<48:15,  1.34it/s]Training epoch 1:  72% 10000/13868 [2:05:34<51:40,  1.25it/s]Training epoch 1:  72% 10001/13868 [2:05:35<50:33,  1.27it/s]Training epoch 1:  72% 10002/13868 [2:05:36<50:08,  1.29it/s]Training epoch 1:  72% 10003/13868 [2:05:37<49:43,  1.30it/s]Training epoch 1:  72% 10004/13868 [2:05:37<49:44,  1.29it/s]Training epoch 1:  72% 10005/13868 [2:05:38<49:54,  1.29it/s]Training epoch 1:  72% 10006/13868 [2:05:39<49:30,  1.30it/s]Training epoch 1:  72% 10007/13868 [2:05:40<49:13,  1.31it/s]Training epoch 1:  72% 10008/13868 [2:05:40<49:12,  1.31it/s]Training epoch 1:  72% 10009/13868 [2:05:41<48:51,  1.32it/s]Training epoch 1:  72% 10010/13868 [2:05:42<49:00,  1.31it/s]Training epoch 1:  72% 10011/13868 [2:05:43<49:07,  1.31it/s]Training epoch 1:  72% 10012/13868 [2:05:43<48:55,  1.31it/s]Training epoch 1:  72% 10013/13868 [2:05:44<48:43,  1.32it/s]Training epoch 1:  72% 10014/13868 [2:05:45<48:41,  1.32it/s]Training epoch 1:  72% 10015/13868 [2:05:46<48:33,  1.32it/s]Training epoch 1:  72% 10016/13868 [2:05:46<48:31,  1.32it/s]Training epoch 1:  72% 10017/13868 [2:05:47<48:27,  1.32it/s]Training epoch 1:  72% 10018/13868 [2:05:48<48:24,  1.33it/s]Training epoch 1:  72% 10019/13868 [2:05:49<48:37,  1.32it/s]Training epoch 1:  72% 10020/13868 [2:05:49<49:02,  1.31it/s]Training epoch 1:  72% 10021/13868 [2:05:50<49:14,  1.30it/s]Training epoch 1:  72% 10022/13868 [2:05:51<48:50,  1.31it/s]Training epoch 1:  72% 10023/13868 [2:05:52<48:47,  1.31it/s]Training epoch 1:  72% 10024/13868 [2:05:53<49:00,  1.31it/s]Training epoch 1:  72% 10025/13868 [2:05:53<48:56,  1.31it/s]Training epoch 1:  72% 10026/13868 [2:05:54<48:44,  1.31it/s]Training epoch 1:  72% 10027/13868 [2:05:55<48:48,  1.31it/s]Training epoch 1:  72% 10028/13868 [2:05:56<49:00,  1.31it/s]Training epoch 1:  72% 10029/13868 [2:05:56<49:00,  1.31it/s]Training epoch 1:  72% 10030/13868 [2:05:57<48:51,  1.31it/s]Training epoch 1:  72% 10031/13868 [2:05:58<48:12,  1.33it/s]Training epoch 1:  72% 10032/13868 [2:05:59<48:41,  1.31it/s]Training epoch 1:  72% 10033/13868 [2:05:59<48:41,  1.31it/s]Training epoch 1:  72% 10034/13868 [2:06:00<47:48,  1.34it/s]Training epoch 1:  72% 10035/13868 [2:06:01<47:42,  1.34it/s]Training epoch 1:  72% 10036/13868 [2:06:02<47:50,  1.34it/s]Training epoch 1:  72% 10037/13868 [2:06:02<47:24,  1.35it/s]Training epoch 1:  72% 10038/13868 [2:06:03<47:57,  1.33it/s]Training epoch 1:  72% 10039/13868 [2:06:04<48:25,  1.32it/s]Training epoch 1:  72% 10040/13868 [2:06:05<47:36,  1.34it/s]Training epoch 1:  72% 10041/13868 [2:06:05<47:45,  1.34it/s]Training epoch 1:  72% 10042/13868 [2:06:06<47:34,  1.34it/s]Training epoch 1:  72% 10043/13868 [2:06:07<47:42,  1.34it/s]Training epoch 1:  72% 10044/13868 [2:06:08<47:23,  1.34it/s]Training epoch 1:  72% 10045/13868 [2:06:08<47:51,  1.33it/s]Training epoch 1:  72% 10046/13868 [2:06:09<48:43,  1.31it/s]Training epoch 1:  72% 10047/13868 [2:06:10<48:15,  1.32it/s]Training epoch 1:  72% 10048/13868 [2:06:11<48:22,  1.32it/s]Training epoch 1:  72% 10049/13868 [2:06:11<48:15,  1.32it/s]Training epoch 1:  72% 10050/13868 [2:06:12<48:20,  1.32it/s]Training epoch 1:  72% 10051/13868 [2:06:13<48:13,  1.32it/s]Training epoch 1:  72% 10052/13868 [2:06:14<48:58,  1.30it/s]Training epoch 1:  72% 10053/13868 [2:06:14<48:44,  1.30it/s]Training epoch 1:  72% 10054/13868 [2:06:15<48:51,  1.30it/s]Training epoch 1:  73% 10055/13868 [2:06:16<48:12,  1.32it/s]Training epoch 1:  73% 10056/13868 [2:06:17<48:18,  1.32it/s]Training epoch 1:  73% 10057/13868 [2:06:18<48:11,  1.32it/s]Training epoch 1:  73% 10058/13868 [2:06:18<48:16,  1.32it/s]Training epoch 1:  73% 10059/13868 [2:06:19<48:06,  1.32it/s]Training epoch 1:  73% 10060/13868 [2:06:20<48:46,  1.30it/s]Training epoch 1:  73% 10061/13868 [2:06:21<48:42,  1.30it/s]Training epoch 1:  73% 10062/13868 [2:06:21<48:36,  1.30it/s]Training epoch 1:  73% 10063/13868 [2:06:22<48:47,  1.30it/s]Training epoch 1:  73% 10064/13868 [2:06:23<48:39,  1.30it/s]Training epoch 1:  73% 10065/13868 [2:06:24<48:22,  1.31it/s]Training epoch 1:  73% 10066/13868 [2:06:24<47:18,  1.34it/s]Training epoch 1:  73% 10067/13868 [2:06:25<47:31,  1.33it/s]Training epoch 1:  73% 10068/13868 [2:06:26<48:14,  1.31it/s]Training epoch 1:  73% 10069/13868 [2:06:27<48:30,  1.31it/s]Training epoch 1:  73% 10070/13868 [2:06:27<48:28,  1.31it/s]Training epoch 1:  73% 10071/13868 [2:06:28<48:10,  1.31it/s]Training epoch 1:  73% 10072/13868 [2:06:29<48:02,  1.32it/s]Training epoch 1:  73% 10073/13868 [2:06:30<47:55,  1.32it/s]Training epoch 1:  73% 10074/13868 [2:06:30<47:33,  1.33it/s]Training epoch 1:  73% 10075/13868 [2:06:31<47:42,  1.32it/s]Training epoch 1:  73% 10076/13868 [2:06:32<48:30,  1.30it/s]Training epoch 1:  73% 10077/13868 [2:06:33<48:19,  1.31it/s]Training epoch 1:  73% 10078/13868 [2:06:34<48:32,  1.30it/s]Training epoch 1:  73% 10079/13868 [2:06:34<48:40,  1.30it/s]Training epoch 1:  73% 10080/13868 [2:06:35<48:13,  1.31it/s]Training epoch 1:  73% 10081/13868 [2:06:36<48:00,  1.31it/s]Training epoch 1:  73% 10082/13868 [2:06:37<48:24,  1.30it/s]Training epoch 1:  73% 10083/13868 [2:06:37<48:16,  1.31it/s]Training epoch 1:  73% 10084/13868 [2:06:38<48:16,  1.31it/s]Training epoch 1:  73% 10085/13868 [2:06:39<47:50,  1.32it/s]Training epoch 1:  73% 10086/13868 [2:06:40<47:42,  1.32it/s]Training epoch 1:  73% 10087/13868 [2:06:40<47:44,  1.32it/s]Training epoch 1:  73% 10088/13868 [2:06:41<48:00,  1.31it/s]Training epoch 1:  73% 10089/13868 [2:06:42<48:21,  1.30it/s]Training epoch 1:  73% 10090/13868 [2:06:43<48:30,  1.30it/s]Training epoch 1:  73% 10091/13868 [2:06:43<48:44,  1.29it/s]Training epoch 1:  73% 10092/13868 [2:06:44<48:51,  1.29it/s]Training epoch 1:  73% 10093/13868 [2:06:45<48:18,  1.30it/s]Training epoch 1:  73% 10094/13868 [2:06:46<48:09,  1.31it/s]Training epoch 1:  73% 10095/13868 [2:06:47<47:45,  1.32it/s]Training epoch 1:  73% 10096/13868 [2:06:47<47:49,  1.31it/s]Training epoch 1:  73% 10097/13868 [2:06:48<47:41,  1.32it/s]Training epoch 1:  73% 10098/13868 [2:06:49<47:35,  1.32it/s]Training epoch 1:  73% 10099/13868 [2:06:50<47:58,  1.31it/s]Training epoch 1:  73% 10100/13868 [2:06:50<50:31,  1.24it/s]Training epoch 1:  73% 10101/13868 [2:06:51<50:15,  1.25it/s]Training epoch 1:  73% 10102/13868 [2:06:52<49:43,  1.26it/s]Training epoch 1:  73% 10103/13868 [2:06:53<49:25,  1.27it/s]Training epoch 1:  73% 10104/13868 [2:06:54<48:51,  1.28it/s]Training epoch 1:  73% 10105/13868 [2:06:54<48:26,  1.29it/s]Training epoch 1:  73% 10106/13868 [2:06:55<48:49,  1.28it/s]Training epoch 1:  73% 10107/13868 [2:06:56<48:11,  1.30it/s]Training epoch 1:  73% 10108/13868 [2:06:57<48:18,  1.30it/s]Training epoch 1:  73% 10109/13868 [2:06:57<47:48,  1.31it/s]Training epoch 1:  73% 10110/13868 [2:06:58<47:54,  1.31it/s]Training epoch 1:  73% 10111/13868 [2:06:59<47:58,  1.31it/s]Training epoch 1:  73% 10112/13868 [2:07:00<47:20,  1.32it/s]Training epoch 1:  73% 10113/13868 [2:07:00<46:59,  1.33it/s]Training epoch 1:  73% 10114/13868 [2:07:01<46:30,  1.35it/s]Training epoch 1:  73% 10115/13868 [2:07:02<46:51,  1.33it/s]Training epoch 1:  73% 10116/13868 [2:07:03<46:42,  1.34it/s]Training epoch 1:  73% 10117/13868 [2:07:03<47:02,  1.33it/s]Training epoch 1:  73% 10118/13868 [2:07:04<47:27,  1.32it/s]Training epoch 1:  73% 10119/13868 [2:07:05<47:09,  1.33it/s]Training epoch 1:  73% 10120/13868 [2:07:06<47:50,  1.31it/s]Training epoch 1:  73% 10121/13868 [2:07:06<47:55,  1.30it/s]Training epoch 1:  73% 10122/13868 [2:07:07<47:27,  1.32it/s]Training epoch 1:  73% 10123/13868 [2:07:08<47:17,  1.32it/s]Training epoch 1:  73% 10124/13868 [2:07:09<47:08,  1.32it/s]Training epoch 1:  73% 10125/13868 [2:07:09<46:44,  1.33it/s]Training epoch 1:  73% 10126/13868 [2:07:10<46:48,  1.33it/s]Training epoch 1:  73% 10127/13868 [2:07:11<47:10,  1.32it/s]Training epoch 1:  73% 10128/13868 [2:07:12<47:04,  1.32it/s]Training epoch 1:  73% 10129/13868 [2:07:12<47:15,  1.32it/s]Training epoch 1:  73% 10130/13868 [2:07:13<47:10,  1.32it/s]Training epoch 1:  73% 10131/13868 [2:07:14<46:50,  1.33it/s]Training epoch 1:  73% 10132/13868 [2:07:15<46:59,  1.33it/s]Training epoch 1:  73% 10133/13868 [2:07:16<47:35,  1.31it/s]Training epoch 1:  73% 10134/13868 [2:07:16<47:12,  1.32it/s]Training epoch 1:  73% 10135/13868 [2:07:17<47:26,  1.31it/s]Training epoch 1:  73% 10136/13868 [2:07:18<47:11,  1.32it/s]Training epoch 1:  73% 10137/13868 [2:07:19<47:14,  1.32it/s]Training epoch 1:  73% 10138/13868 [2:07:19<47:30,  1.31it/s]Training epoch 1:  73% 10139/13868 [2:07:20<48:14,  1.29it/s]Training epoch 1:  73% 10140/13868 [2:07:21<47:21,  1.31it/s]Training epoch 1:  73% 10141/13868 [2:07:22<47:01,  1.32it/s]Training epoch 1:  73% 10142/13868 [2:07:22<47:05,  1.32it/s]Training epoch 1:  73% 10143/13868 [2:07:23<46:57,  1.32it/s]Training epoch 1:  73% 10144/13868 [2:07:24<46:47,  1.33it/s]Training epoch 1:  73% 10145/13868 [2:07:25<46:57,  1.32it/s]Training epoch 1:  73% 10146/13868 [2:07:25<46:38,  1.33it/s]Training epoch 1:  73% 10147/13868 [2:07:26<46:22,  1.34it/s]Training epoch 1:  73% 10148/13868 [2:07:27<47:05,  1.32it/s]Training epoch 1:  73% 10149/13868 [2:07:28<47:09,  1.31it/s]Training epoch 1:  73% 10150/13868 [2:07:28<47:23,  1.31it/s]Training epoch 1:  73% 10151/13868 [2:07:29<47:20,  1.31it/s]Training epoch 1:  73% 10152/13868 [2:07:30<47:44,  1.30it/s]Training epoch 1:  73% 10153/13868 [2:07:31<47:32,  1.30it/s]Training epoch 1:  73% 10154/13868 [2:07:31<47:01,  1.32it/s]Training epoch 1:  73% 10155/13868 [2:07:32<47:00,  1.32it/s]Training epoch 1:  73% 10156/13868 [2:07:33<47:02,  1.32it/s]Training epoch 1:  73% 10157/13868 [2:07:34<47:25,  1.30it/s]Training epoch 1:  73% 10158/13868 [2:07:35<47:16,  1.31it/s]Training epoch 1:  73% 10159/13868 [2:07:35<47:08,  1.31it/s]Training epoch 1:  73% 10160/13868 [2:07:36<47:02,  1.31it/s]Training epoch 1:  73% 10161/13868 [2:07:37<47:06,  1.31it/s]Training epoch 1:  73% 10162/13868 [2:07:38<47:18,  1.31it/s]Training epoch 1:  73% 10163/13868 [2:07:38<46:58,  1.31it/s]Training epoch 1:  73% 10164/13868 [2:07:39<47:08,  1.31it/s]Training epoch 1:  73% 10165/13868 [2:07:40<47:04,  1.31it/s]Training epoch 1:  73% 10166/13868 [2:07:41<46:34,  1.32it/s]Training epoch 1:  73% 10167/13868 [2:07:41<46:49,  1.32it/s]Training epoch 1:  73% 10168/13868 [2:07:42<46:50,  1.32it/s]Training epoch 1:  73% 10169/13868 [2:07:43<47:29,  1.30it/s]Training epoch 1:  73% 10170/13868 [2:07:44<46:54,  1.31it/s]Training epoch 1:  73% 10171/13868 [2:07:44<46:40,  1.32it/s]Training epoch 1:  73% 10172/13868 [2:07:45<47:05,  1.31it/s]Training epoch 1:  73% 10173/13868 [2:07:46<47:02,  1.31it/s]Training epoch 1:  73% 10174/13868 [2:07:47<46:16,  1.33it/s]Training epoch 1:  73% 10175/13868 [2:07:47<46:49,  1.31it/s]Training epoch 1:  73% 10176/13868 [2:07:48<47:24,  1.30it/s]Training epoch 1:  73% 10177/13868 [2:07:49<46:43,  1.32it/s]Training epoch 1:  73% 10178/13868 [2:07:50<47:05,  1.31it/s]Training epoch 1:  73% 10179/13868 [2:07:51<47:02,  1.31it/s]Training epoch 1:  73% 10180/13868 [2:07:51<47:37,  1.29it/s]Training epoch 1:  73% 10181/13868 [2:07:52<47:29,  1.29it/s]Training epoch 1:  73% 10182/13868 [2:07:53<47:20,  1.30it/s]Training epoch 1:  73% 10183/13868 [2:07:54<46:37,  1.32it/s]Training epoch 1:  73% 10184/13868 [2:07:54<46:51,  1.31it/s]Training epoch 1:  73% 10185/13868 [2:07:55<47:15,  1.30it/s]Training epoch 1:  73% 10186/13868 [2:07:56<47:04,  1.30it/s]Training epoch 1:  73% 10187/13868 [2:07:57<46:52,  1.31it/s]Training epoch 1:  73% 10188/13868 [2:07:57<46:44,  1.31it/s]Training epoch 1:  73% 10189/13868 [2:07:58<45:57,  1.33it/s]Training epoch 1:  73% 10190/13868 [2:07:59<46:21,  1.32it/s]Training epoch 1:  73% 10191/13868 [2:08:00<46:27,  1.32it/s]Training epoch 1:  73% 10192/13868 [2:08:00<46:07,  1.33it/s]Training epoch 1:  74% 10193/13868 [2:08:01<45:26,  1.35it/s]Training epoch 1:  74% 10194/13868 [2:08:02<45:17,  1.35it/s]Training epoch 1:  74% 10195/13868 [2:08:03<44:40,  1.37it/s]Training epoch 1:  74% 10196/13868 [2:08:03<44:23,  1.38it/s]Training epoch 1:  74% 10197/13868 [2:08:04<44:52,  1.36it/s]Training epoch 1:  74% 10198/13868 [2:08:05<45:11,  1.35it/s]Training epoch 1:  74% 10199/13868 [2:08:06<46:11,  1.32it/s]Training epoch 1:  74% 10200/13868 [2:08:07<49:15,  1.24it/s]Training epoch 1:  74% 10201/13868 [2:08:07<48:23,  1.26it/s]Training epoch 1:  74% 10202/13868 [2:08:08<47:44,  1.28it/s]Training epoch 1:  74% 10203/13868 [2:08:09<46:43,  1.31it/s]Training epoch 1:  74% 10204/13868 [2:08:10<46:34,  1.31it/s]Training epoch 1:  74% 10205/13868 [2:08:10<46:26,  1.31it/s]Training epoch 1:  74% 10206/13868 [2:08:11<46:38,  1.31it/s]Training epoch 1:  74% 10207/13868 [2:08:12<46:40,  1.31it/s]Training epoch 1:  74% 10208/13868 [2:08:13<46:49,  1.30it/s]Training epoch 1:  74% 10209/13868 [2:08:13<46:19,  1.32it/s]Training epoch 1:  74% 10210/13868 [2:08:14<46:53,  1.30it/s]Training epoch 1:  74% 10211/13868 [2:08:15<46:57,  1.30it/s]Training epoch 1:  74% 10212/13868 [2:08:16<47:18,  1.29it/s]Training epoch 1:  74% 10213/13868 [2:08:16<46:20,  1.31it/s]Training epoch 1:  74% 10214/13868 [2:08:17<46:28,  1.31it/s]Training epoch 1:  74% 10215/13868 [2:08:18<46:59,  1.30it/s]Training epoch 1:  74% 10216/13868 [2:08:19<47:04,  1.29it/s]Training epoch 1:  74% 10217/13868 [2:08:20<47:06,  1.29it/s]Training epoch 1:  74% 10218/13868 [2:08:20<46:48,  1.30it/s]Training epoch 1:  74% 10219/13868 [2:08:21<46:29,  1.31it/s]Training epoch 1:  74% 10220/13868 [2:08:22<46:52,  1.30it/s]Training epoch 1:  74% 10221/13868 [2:08:23<46:45,  1.30it/s]Training epoch 1:  74% 10222/13868 [2:08:23<46:27,  1.31it/s]Training epoch 1:  74% 10223/13868 [2:08:24<46:56,  1.29it/s]Training epoch 1:  74% 10224/13868 [2:08:25<46:56,  1.29it/s]Training epoch 1:  74% 10225/13868 [2:08:26<45:49,  1.33it/s]Training epoch 1:  74% 10226/13868 [2:08:26<46:19,  1.31it/s]Training epoch 1:  74% 10227/13868 [2:08:27<46:38,  1.30it/s]Training epoch 1:  74% 10228/13868 [2:08:28<47:00,  1.29it/s]Training epoch 1:  74% 10229/13868 [2:08:29<46:45,  1.30it/s]Training epoch 1:  74% 10230/13868 [2:08:30<46:52,  1.29it/s]Training epoch 1:  74% 10231/13868 [2:08:30<45:56,  1.32it/s]Training epoch 1:  74% 10232/13868 [2:08:31<45:50,  1.32it/s]Training epoch 1:  74% 10233/13868 [2:08:32<46:01,  1.32it/s]Training epoch 1:  74% 10234/13868 [2:08:33<46:22,  1.31it/s]Training epoch 1:  74% 10235/13868 [2:08:33<45:59,  1.32it/s]Training epoch 1:  74% 10236/13868 [2:08:34<46:39,  1.30it/s]Training epoch 1:  74% 10237/13868 [2:08:35<46:21,  1.31it/s]Training epoch 1:  74% 10238/13868 [2:08:36<46:33,  1.30it/s]Training epoch 1:  74% 10239/13868 [2:08:36<47:11,  1.28it/s]Training epoch 1:  74% 10240/13868 [2:08:37<46:08,  1.31it/s]Training epoch 1:  74% 10241/13868 [2:08:38<45:47,  1.32it/s]Training epoch 1:  74% 10242/13868 [2:08:39<46:03,  1.31it/s]Training epoch 1:  74% 10243/13868 [2:08:39<45:27,  1.33it/s]Training epoch 1:  74% 10244/13868 [2:08:40<45:47,  1.32it/s]Training epoch 1:  74% 10245/13868 [2:08:41<46:14,  1.31it/s]Training epoch 1:  74% 10246/13868 [2:08:42<46:39,  1.29it/s]Training epoch 1:  74% 10247/13868 [2:08:43<46:26,  1.30it/s]Training epoch 1:  74% 10248/13868 [2:08:43<45:45,  1.32it/s]Training epoch 1:  74% 10249/13868 [2:08:44<45:34,  1.32it/s]Training epoch 1:  74% 10250/13868 [2:08:45<45:26,  1.33it/s]Training epoch 1:  74% 10251/13868 [2:08:45<45:22,  1.33it/s]Training epoch 1:  74% 10252/13868 [2:08:46<45:26,  1.33it/s]Training epoch 1:  74% 10253/13868 [2:08:47<45:36,  1.32it/s]Training epoch 1:  74% 10254/13868 [2:08:48<45:58,  1.31it/s]Training epoch 1:  74% 10255/13868 [2:08:49<45:29,  1.32it/s]Training epoch 1:  74% 10256/13868 [2:08:49<46:31,  1.29it/s]Training epoch 1:  74% 10257/13868 [2:08:50<46:56,  1.28it/s]Training epoch 1:  74% 10258/13868 [2:08:51<46:37,  1.29it/s]Training epoch 1:  74% 10259/13868 [2:08:52<46:22,  1.30it/s]Training epoch 1:  74% 10260/13868 [2:08:52<45:55,  1.31it/s]Training epoch 1:  74% 10261/13868 [2:08:53<45:17,  1.33it/s]Training epoch 1:  74% 10262/13868 [2:08:54<45:15,  1.33it/s]Training epoch 1:  74% 10263/13868 [2:08:55<45:10,  1.33it/s]Training epoch 1:  74% 10264/13868 [2:08:55<45:14,  1.33it/s]Training epoch 1:  74% 10265/13868 [2:08:56<45:17,  1.33it/s]Training epoch 1:  74% 10266/13868 [2:08:57<45:29,  1.32it/s]Training epoch 1:  74% 10267/13868 [2:08:58<45:33,  1.32it/s]Training epoch 1:  74% 10268/13868 [2:08:58<45:34,  1.32it/s]Training epoch 1:  74% 10269/13868 [2:08:59<45:45,  1.31it/s]Training epoch 1:  74% 10270/13868 [2:09:00<45:20,  1.32it/s]Training epoch 1:  74% 10271/13868 [2:09:01<46:14,  1.30it/s]Training epoch 1:  74% 10272/13868 [2:09:02<46:17,  1.29it/s]Training epoch 1:  74% 10273/13868 [2:09:02<46:03,  1.30it/s]Training epoch 1:  74% 10274/13868 [2:09:03<45:58,  1.30it/s]Training epoch 1:  74% 10275/13868 [2:09:04<45:28,  1.32it/s]Training epoch 1:  74% 10276/13868 [2:09:05<46:00,  1.30it/s]Training epoch 1:  74% 10277/13868 [2:09:05<46:16,  1.29it/s]Training epoch 1:  74% 10278/13868 [2:09:06<46:03,  1.30it/s]Training epoch 1:  74% 10279/13868 [2:09:07<45:02,  1.33it/s]Training epoch 1:  74% 10280/13868 [2:09:08<44:57,  1.33it/s]Training epoch 1:  74% 10281/13868 [2:09:08<45:05,  1.33it/s]Training epoch 1:  74% 10282/13868 [2:09:09<45:25,  1.32it/s]Training epoch 1:  74% 10283/13868 [2:09:10<45:40,  1.31it/s]Training epoch 1:  74% 10284/13868 [2:09:11<45:47,  1.30it/s]Training epoch 1:  74% 10285/13868 [2:09:11<45:19,  1.32it/s]Training epoch 1:  74% 10286/13868 [2:09:12<45:10,  1.32it/s]Training epoch 1:  74% 10287/13868 [2:09:13<45:27,  1.31it/s]Training epoch 1:  74% 10288/13868 [2:09:14<44:27,  1.34it/s]Training epoch 1:  74% 10289/13868 [2:09:14<44:25,  1.34it/s]Training epoch 1:  74% 10290/13868 [2:09:15<44:19,  1.35it/s]Training epoch 1:  74% 10291/13868 [2:09:16<45:15,  1.32it/s]Training epoch 1:  74% 10292/13868 [2:09:17<45:12,  1.32it/s]Training epoch 1:  74% 10293/13868 [2:09:17<45:27,  1.31it/s]Training epoch 1:  74% 10294/13868 [2:09:18<45:20,  1.31it/s]Training epoch 1:  74% 10295/13868 [2:09:19<45:06,  1.32it/s]Training epoch 1:  74% 10296/13868 [2:09:20<44:30,  1.34it/s]Training epoch 1:  74% 10297/13868 [2:09:20<44:57,  1.32it/s]Training epoch 1:  74% 10298/13868 [2:09:21<45:01,  1.32it/s]Training epoch 1:  74% 10299/13868 [2:09:22<45:06,  1.32it/s]Training epoch 1:  74% 10300/13868 [2:09:23<47:12,  1.26it/s]Training epoch 1:  74% 10301/13868 [2:09:24<46:55,  1.27it/s]Training epoch 1:  74% 10302/13868 [2:09:24<47:15,  1.26it/s]Training epoch 1:  74% 10303/13868 [2:09:25<47:15,  1.26it/s]Training epoch 1:  74% 10304/13868 [2:09:26<46:40,  1.27it/s]Training epoch 1:  74% 10305/13868 [2:09:27<46:19,  1.28it/s]Training epoch 1:  74% 10306/13868 [2:09:28<45:55,  1.29it/s]Training epoch 1:  74% 10307/13868 [2:09:28<45:37,  1.30it/s]Training epoch 1:  74% 10308/13868 [2:09:29<46:12,  1.28it/s]Training epoch 1:  74% 10309/13868 [2:09:30<45:48,  1.29it/s]Training epoch 1:  74% 10310/13868 [2:09:31<45:29,  1.30it/s]Training epoch 1:  74% 10311/13868 [2:09:31<44:46,  1.32it/s]Training epoch 1:  74% 10312/13868 [2:09:32<45:23,  1.31it/s]Training epoch 1:  74% 10313/13868 [2:09:33<44:59,  1.32it/s]Training epoch 1:  74% 10314/13868 [2:09:34<45:00,  1.32it/s]Training epoch 1:  74% 10315/13868 [2:09:34<45:10,  1.31it/s]Training epoch 1:  74% 10316/13868 [2:09:35<45:22,  1.30it/s]Training epoch 1:  74% 10317/13868 [2:09:36<45:01,  1.31it/s]Training epoch 1:  74% 10318/13868 [2:09:37<44:45,  1.32it/s]Training epoch 1:  74% 10319/13868 [2:09:37<44:29,  1.33it/s]Training epoch 1:  74% 10320/13868 [2:09:38<45:03,  1.31it/s]Training epoch 1:  74% 10321/13868 [2:09:39<44:44,  1.32it/s]Training epoch 1:  74% 10322/13868 [2:09:40<45:13,  1.31it/s]Training epoch 1:  74% 10323/13868 [2:09:40<44:56,  1.31it/s]Training epoch 1:  74% 10324/13868 [2:09:41<45:34,  1.30it/s]Training epoch 1:  74% 10325/13868 [2:09:42<44:59,  1.31it/s]Training epoch 1:  74% 10326/13868 [2:09:43<45:01,  1.31it/s]Training epoch 1:  74% 10327/13868 [2:09:44<44:35,  1.32it/s]Training epoch 1:  74% 10328/13868 [2:09:44<44:48,  1.32it/s]Training epoch 1:  74% 10329/13868 [2:09:45<44:49,  1.32it/s]Training epoch 1:  74% 10330/13868 [2:09:46<43:51,  1.34it/s]Training epoch 1:  74% 10331/13868 [2:09:46<43:42,  1.35it/s]Training epoch 1:  75% 10332/13868 [2:09:47<43:42,  1.35it/s]Training epoch 1:  75% 10333/13868 [2:09:48<44:13,  1.33it/s]Training epoch 1:  75% 10334/13868 [2:09:49<44:21,  1.33it/s]Training epoch 1:  75% 10335/13868 [2:09:49<43:54,  1.34it/s]Training epoch 1:  75% 10336/13868 [2:09:50<44:11,  1.33it/s]Training epoch 1:  75% 10337/13868 [2:09:51<43:41,  1.35it/s]Training epoch 1:  75% 10338/13868 [2:09:52<43:48,  1.34it/s]Training epoch 1:  75% 10339/13868 [2:09:52<44:00,  1.34it/s]Training epoch 1:  75% 10340/13868 [2:09:53<43:48,  1.34it/s]Training epoch 1:  75% 10341/13868 [2:09:54<44:50,  1.31it/s]Training epoch 1:  75% 10342/13868 [2:09:55<44:41,  1.32it/s]Training epoch 1:  75% 10343/13868 [2:09:56<44:30,  1.32it/s]Training epoch 1:  75% 10344/13868 [2:09:56<45:02,  1.30it/s]Training epoch 1:  75% 10345/13868 [2:09:57<44:58,  1.31it/s]Training epoch 1:  75% 10346/13868 [2:09:58<44:35,  1.32it/s]Training epoch 1:  75% 10347/13868 [2:09:59<44:20,  1.32it/s]Training epoch 1:  75% 10348/13868 [2:09:59<44:30,  1.32it/s]Training epoch 1:  75% 10349/13868 [2:10:00<43:40,  1.34it/s]Training epoch 1:  75% 10350/13868 [2:10:01<44:17,  1.32it/s]Training epoch 1:  75% 10351/13868 [2:10:02<44:05,  1.33it/s]Training epoch 1:  75% 10352/13868 [2:10:02<44:30,  1.32it/s]Training epoch 1:  75% 10353/13868 [2:10:03<43:58,  1.33it/s]Training epoch 1:  75% 10354/13868 [2:10:04<44:26,  1.32it/s]Training epoch 1:  75% 10355/13868 [2:10:05<44:00,  1.33it/s]Training epoch 1:  75% 10356/13868 [2:10:05<44:47,  1.31it/s]Training epoch 1:  75% 10357/13868 [2:10:06<44:20,  1.32it/s]Training epoch 1:  75% 10358/13868 [2:10:07<44:26,  1.32it/s]Training epoch 1:  75% 10359/13868 [2:10:08<44:28,  1.31it/s]Training epoch 1:  75% 10360/13868 [2:10:08<44:50,  1.30it/s]Training epoch 1:  75% 10361/13868 [2:10:09<44:35,  1.31it/s]Training epoch 1:  75% 10362/13868 [2:10:10<44:36,  1.31it/s]Training epoch 1:  75% 10363/13868 [2:10:11<44:18,  1.32it/s]Training epoch 1:  75% 10364/13868 [2:10:11<44:21,  1.32it/s]Training epoch 1:  75% 10365/13868 [2:10:12<43:21,  1.35it/s]Training epoch 1:  75% 10366/13868 [2:10:13<43:44,  1.33it/s]Training epoch 1:  75% 10367/13868 [2:10:14<44:09,  1.32it/s]Training epoch 1:  75% 10368/13868 [2:10:14<43:45,  1.33it/s]Training epoch 1:  75% 10369/13868 [2:10:15<44:21,  1.31it/s]Training epoch 1:  75% 10370/13868 [2:10:16<43:40,  1.33it/s]Training epoch 1:  75% 10371/13868 [2:10:17<43:49,  1.33it/s]Training epoch 1:  75% 10372/13868 [2:10:17<44:04,  1.32it/s]Training epoch 1:  75% 10373/13868 [2:10:18<43:14,  1.35it/s]Training epoch 1:  75% 10374/13868 [2:10:19<43:44,  1.33it/s]Training epoch 1:  75% 10375/13868 [2:10:20<43:41,  1.33it/s]Training epoch 1:  75% 10376/13868 [2:10:20<43:13,  1.35it/s]Training epoch 1:  75% 10377/13868 [2:10:21<43:42,  1.33it/s]Training epoch 1:  75% 10378/13868 [2:10:22<44:03,  1.32it/s]Training epoch 1:  75% 10379/13868 [2:10:23<43:29,  1.34it/s]Training epoch 1:  75% 10380/13868 [2:10:23<43:49,  1.33it/s]Training epoch 1:  75% 10381/13868 [2:10:24<43:51,  1.33it/s]Training epoch 1:  75% 10382/13868 [2:10:25<43:48,  1.33it/s]Training epoch 1:  75% 10383/13868 [2:10:26<43:39,  1.33it/s]Training epoch 1:  75% 10384/13868 [2:10:26<43:53,  1.32it/s]Training epoch 1:  75% 10385/13868 [2:10:27<43:56,  1.32it/s]Training epoch 1:  75% 10386/13868 [2:10:28<44:03,  1.32it/s]Training epoch 1:  75% 10387/13868 [2:10:29<43:23,  1.34it/s]Training epoch 1:  75% 10388/13868 [2:10:29<43:22,  1.34it/s]Training epoch 1:  75% 10389/13868 [2:10:30<43:35,  1.33it/s]Training epoch 1:  75% 10390/13868 [2:10:31<43:59,  1.32it/s]Training epoch 1:  75% 10391/13868 [2:10:32<43:44,  1.33it/s]Training epoch 1:  75% 10392/13868 [2:10:33<44:08,  1.31it/s]Training epoch 1:  75% 10393/13868 [2:10:33<44:10,  1.31it/s]Training epoch 1:  75% 10394/13868 [2:10:34<43:52,  1.32it/s]Training epoch 1:  75% 10395/13868 [2:10:35<44:15,  1.31it/s]Training epoch 1:  75% 10396/13868 [2:10:36<44:16,  1.31it/s]Training epoch 1:  75% 10397/13868 [2:10:36<43:40,  1.32it/s]Training epoch 1:  75% 10398/13868 [2:10:37<43:15,  1.34it/s]Training epoch 1:  75% 10399/13868 [2:10:38<43:45,  1.32it/s]Training epoch 1:  75% 10400/13868 [2:10:39<45:07,  1.28it/s]Training epoch 1:  75% 10401/13868 [2:10:39<44:43,  1.29it/s]Training epoch 1:  75% 10402/13868 [2:10:40<44:51,  1.29it/s]Training epoch 1:  75% 10403/13868 [2:10:41<44:55,  1.29it/s]Training epoch 1:  75% 10404/13868 [2:10:42<45:03,  1.28it/s]Training epoch 1:  75% 10405/13868 [2:10:43<43:55,  1.31it/s]Training epoch 1:  75% 10406/13868 [2:10:43<44:15,  1.30it/s]Training epoch 1:  75% 10407/13868 [2:10:44<43:29,  1.33it/s]Training epoch 1:  75% 10408/13868 [2:10:45<43:19,  1.33it/s]Training epoch 1:  75% 10409/13868 [2:10:45<43:05,  1.34it/s]Training epoch 1:  75% 10410/13868 [2:10:46<43:34,  1.32it/s]Training epoch 1:  75% 10411/13868 [2:10:47<43:28,  1.33it/s]Training epoch 1:  75% 10412/13868 [2:10:48<43:40,  1.32it/s]Training epoch 1:  75% 10413/13868 [2:10:49<43:34,  1.32it/s]Training epoch 1:  75% 10414/13868 [2:10:49<44:05,  1.31it/s]Training epoch 1:  75% 10415/13868 [2:10:50<43:36,  1.32it/s]Training epoch 1:  75% 10416/13868 [2:10:51<43:49,  1.31it/s]Training epoch 1:  75% 10417/13868 [2:10:52<43:32,  1.32it/s]Training epoch 1:  75% 10418/13868 [2:10:52<43:28,  1.32it/s]Training epoch 1:  75% 10419/13868 [2:10:53<43:31,  1.32it/s]Training epoch 1:  75% 10420/13868 [2:10:54<43:41,  1.32it/s]Training epoch 1:  75% 10421/13868 [2:10:55<43:38,  1.32it/s]Training epoch 1:  75% 10422/13868 [2:10:55<43:41,  1.31it/s]Training epoch 1:  75% 10423/13868 [2:10:56<43:59,  1.31it/s]Training epoch 1:  75% 10424/13868 [2:10:57<43:54,  1.31it/s]Training epoch 1:  75% 10425/13868 [2:10:58<43:54,  1.31it/s]Training epoch 1:  75% 10426/13868 [2:10:58<43:44,  1.31it/s]Training epoch 1:  75% 10427/13868 [2:10:59<43:28,  1.32it/s]Training epoch 1:  75% 10428/13868 [2:11:00<43:25,  1.32it/s]Training epoch 1:  75% 10429/13868 [2:11:01<42:55,  1.34it/s]Training epoch 1:  75% 10430/13868 [2:11:01<43:27,  1.32it/s]Training epoch 1:  75% 10431/13868 [2:11:02<43:00,  1.33it/s]Training epoch 1:  75% 10432/13868 [2:11:03<43:01,  1.33it/s]Training epoch 1:  75% 10433/13868 [2:11:04<43:22,  1.32it/s]Training epoch 1:  75% 10434/13868 [2:11:05<44:04,  1.30it/s]Training epoch 1:  75% 10435/13868 [2:11:05<43:34,  1.31it/s]Training epoch 1:  75% 10436/13868 [2:11:06<43:49,  1.31it/s]Training epoch 1:  75% 10437/13868 [2:11:07<43:03,  1.33it/s]Training epoch 1:  75% 10438/13868 [2:11:08<43:22,  1.32it/s]Training epoch 1:  75% 10439/13868 [2:11:08<43:43,  1.31it/s]Training epoch 1:  75% 10440/13868 [2:11:09<43:37,  1.31it/s]Training epoch 1:  75% 10441/13868 [2:11:10<43:48,  1.30it/s]Training epoch 1:  75% 10442/13868 [2:11:11<43:19,  1.32it/s]Training epoch 1:  75% 10443/13868 [2:11:11<42:44,  1.34it/s]Training epoch 1:  75% 10444/13868 [2:11:12<43:31,  1.31it/s]Training epoch 1:  75% 10445/13868 [2:11:13<43:12,  1.32it/s]Training epoch 1:  75% 10446/13868 [2:11:14<43:19,  1.32it/s]Training epoch 1:  75% 10447/13868 [2:11:14<42:46,  1.33it/s]Training epoch 1:  75% 10448/13868 [2:11:15<42:43,  1.33it/s]Training epoch 1:  75% 10449/13868 [2:11:16<42:28,  1.34it/s]Training epoch 1:  75% 10450/13868 [2:11:17<42:48,  1.33it/s]Training epoch 1:  75% 10451/13868 [2:11:17<42:38,  1.34it/s]Training epoch 1:  75% 10452/13868 [2:11:18<42:55,  1.33it/s]Training epoch 1:  75% 10453/13868 [2:11:19<42:55,  1.33it/s]Training epoch 1:  75% 10454/13868 [2:11:20<42:41,  1.33it/s]Training epoch 1:  75% 10455/13868 [2:11:20<42:13,  1.35it/s]Training epoch 1:  75% 10456/13868 [2:11:21<43:16,  1.31it/s]Training epoch 1:  75% 10457/13868 [2:11:22<43:26,  1.31it/s]Training epoch 1:  75% 10458/13868 [2:11:23<43:35,  1.30it/s]Training epoch 1:  75% 10459/13868 [2:11:23<43:22,  1.31it/s]Training epoch 1:  75% 10460/13868 [2:11:24<43:25,  1.31it/s]Training epoch 1:  75% 10461/13868 [2:11:25<43:13,  1.31it/s]Training epoch 1:  75% 10462/13868 [2:11:26<43:14,  1.31it/s]Training epoch 1:  75% 10463/13868 [2:11:26<43:10,  1.31it/s]Training epoch 1:  75% 10464/13868 [2:11:27<42:15,  1.34it/s]Training epoch 1:  75% 10465/13868 [2:11:28<42:08,  1.35it/s]Training epoch 1:  75% 10466/13868 [2:11:29<42:41,  1.33it/s]Training epoch 1:  75% 10467/13868 [2:11:29<42:25,  1.34it/s]Training epoch 1:  75% 10468/13868 [2:11:30<42:23,  1.34it/s]Training epoch 1:  75% 10469/13868 [2:11:31<42:24,  1.34it/s]Training epoch 1:  75% 10470/13868 [2:11:32<42:32,  1.33it/s]Training epoch 1:  76% 10471/13868 [2:11:32<42:36,  1.33it/s]Training epoch 1:  76% 10472/13868 [2:11:33<42:57,  1.32it/s]Training epoch 1:  76% 10473/13868 [2:11:34<42:53,  1.32it/s]Training epoch 1:  76% 10474/13868 [2:11:35<43:05,  1.31it/s]Training epoch 1:  76% 10475/13868 [2:11:36<43:31,  1.30it/s]Training epoch 1:  76% 10476/13868 [2:11:36<43:12,  1.31it/s]Training epoch 1:  76% 10477/13868 [2:11:37<43:02,  1.31it/s]Training epoch 1:  76% 10478/13868 [2:11:38<43:12,  1.31it/s]Training epoch 1:  76% 10479/13868 [2:11:39<42:10,  1.34it/s]Training epoch 1:  76% 10480/13868 [2:11:39<41:55,  1.35it/s]Training epoch 1:  76% 10481/13868 [2:11:40<42:29,  1.33it/s]Training epoch 1:  76% 10482/13868 [2:11:41<42:43,  1.32it/s]Training epoch 1:  76% 10483/13868 [2:11:42<42:35,  1.32it/s]Training epoch 1:  76% 10484/13868 [2:11:42<42:56,  1.31it/s]Training epoch 1:  76% 10485/13868 [2:11:43<42:39,  1.32it/s]Training epoch 1:  76% 10486/13868 [2:11:44<42:38,  1.32it/s]Training epoch 1:  76% 10487/13868 [2:11:45<42:51,  1.31it/s]Training epoch 1:  76% 10488/13868 [2:11:45<42:59,  1.31it/s]Training epoch 1:  76% 10489/13868 [2:11:46<42:47,  1.32it/s]Training epoch 1:  76% 10490/13868 [2:11:47<43:24,  1.30it/s]Training epoch 1:  76% 10491/13868 [2:11:48<42:30,  1.32it/s]Training epoch 1:  76% 10492/13868 [2:11:48<42:32,  1.32it/s]Training epoch 1:  76% 10493/13868 [2:11:49<42:18,  1.33it/s]Training epoch 1:  76% 10494/13868 [2:11:50<42:16,  1.33it/s]Training epoch 1:  76% 10495/13868 [2:11:51<42:53,  1.31it/s]Training epoch 1:  76% 10496/13868 [2:11:51<42:41,  1.32it/s]Training epoch 1:  76% 10497/13868 [2:11:52<42:11,  1.33it/s]Training epoch 1:  76% 10498/13868 [2:11:53<41:49,  1.34it/s]Training epoch 1:  76% 10499/13868 [2:11:54<42:33,  1.32it/s]Training epoch 1:  76% 10500/13868 [2:11:55<47:52,  1.17it/s]Training epoch 1:  76% 10501/13868 [2:11:55<45:47,  1.23it/s]Training epoch 1:  76% 10502/13868 [2:11:56<44:51,  1.25it/s]Training epoch 1:  76% 10503/13868 [2:11:57<44:09,  1.27it/s]Training epoch 1:  76% 10504/13868 [2:11:58<43:43,  1.28it/s]Training epoch 1:  76% 10505/13868 [2:11:59<43:17,  1.29it/s]Training epoch 1:  76% 10506/13868 [2:11:59<43:13,  1.30it/s]Training epoch 1:  76% 10507/13868 [2:12:00<42:18,  1.32it/s]Training epoch 1:  76% 10508/13868 [2:12:01<41:48,  1.34it/s]Training epoch 1:  76% 10509/13868 [2:12:01<41:40,  1.34it/s]Training epoch 1:  76% 10510/13868 [2:12:02<41:52,  1.34it/s]Training epoch 1:  76% 10511/13868 [2:12:03<41:31,  1.35it/s]Training epoch 1:  76% 10512/13868 [2:12:04<42:15,  1.32it/s]Training epoch 1:  76% 10513/13868 [2:12:04<41:54,  1.33it/s]Training epoch 1:  76% 10514/13868 [2:12:05<42:12,  1.32it/s]Training epoch 1:  76% 10515/13868 [2:12:06<42:01,  1.33it/s]Training epoch 1:  76% 10516/13868 [2:12:07<42:24,  1.32it/s]Training epoch 1:  76% 10517/13868 [2:12:08<42:26,  1.32it/s]Training epoch 1:  76% 10518/13868 [2:12:08<41:50,  1.33it/s]Training epoch 1:  76% 10519/13868 [2:12:09<41:36,  1.34it/s]Training epoch 1:  76% 10520/13868 [2:12:10<41:17,  1.35it/s]Training epoch 1:  76% 10521/13868 [2:12:10<41:24,  1.35it/s]Training epoch 1:  76% 10522/13868 [2:12:11<41:23,  1.35it/s]Training epoch 1:  76% 10523/13868 [2:12:12<41:36,  1.34it/s]Training epoch 1:  76% 10524/13868 [2:12:13<41:23,  1.35it/s]Training epoch 1:  76% 10525/13868 [2:12:13<41:37,  1.34it/s]Training epoch 1:  76% 10526/13868 [2:12:14<42:02,  1.33it/s]Training epoch 1:  76% 10527/13868 [2:12:15<41:19,  1.35it/s]Training epoch 1:  76% 10528/13868 [2:12:16<41:00,  1.36it/s]Training epoch 1:  76% 10529/13868 [2:12:16<40:55,  1.36it/s]Training epoch 1:  76% 10530/13868 [2:12:17<40:40,  1.37it/s]Training epoch 1:  76% 10531/13868 [2:12:18<40:40,  1.37it/s]Training epoch 1:  76% 10532/13868 [2:12:19<41:15,  1.35it/s]Training epoch 1:  76% 10533/13868 [2:12:19<41:09,  1.35it/s]Training epoch 1:  76% 10534/13868 [2:12:20<41:45,  1.33it/s]Training epoch 1:  76% 10535/13868 [2:12:21<41:45,  1.33it/s]Training epoch 1:  76% 10536/13868 [2:12:22<41:24,  1.34it/s]Training epoch 1:  76% 10537/13868 [2:12:22<41:20,  1.34it/s]Training epoch 1:  76% 10538/13868 [2:12:23<42:03,  1.32it/s]Training epoch 1:  76% 10539/13868 [2:12:24<41:18,  1.34it/s]Training epoch 1:  76% 10540/13868 [2:12:25<41:37,  1.33it/s]Training epoch 1:  76% 10541/13868 [2:12:25<41:13,  1.34it/s]Training epoch 1:  76% 10542/13868 [2:12:26<41:27,  1.34it/s]Training epoch 1:  76% 10543/13868 [2:12:27<41:37,  1.33it/s]Training epoch 1:  76% 10544/13868 [2:12:28<41:36,  1.33it/s]Training epoch 1:  76% 10545/13868 [2:12:28<41:29,  1.33it/s]Training epoch 1:  76% 10546/13868 [2:12:29<41:24,  1.34it/s]Training epoch 1:  76% 10547/13868 [2:12:30<40:59,  1.35it/s]Training epoch 1:  76% 10548/13868 [2:12:31<41:30,  1.33it/s]Training epoch 1:  76% 10549/13868 [2:12:31<41:11,  1.34it/s]Training epoch 1:  76% 10550/13868 [2:12:32<41:37,  1.33it/s]Training epoch 1:  76% 10551/13868 [2:12:33<41:59,  1.32it/s]Training epoch 1:  76% 10552/13868 [2:12:34<41:47,  1.32it/s]Training epoch 1:  76% 10553/13868 [2:12:34<41:33,  1.33it/s]Training epoch 1:  76% 10554/13868 [2:12:35<42:08,  1.31it/s]Training epoch 1:  76% 10555/13868 [2:12:36<41:48,  1.32it/s]Training epoch 1:  76% 10556/13868 [2:12:37<41:39,  1.32it/s]Training epoch 1:  76% 10557/13868 [2:12:37<41:49,  1.32it/s]Training epoch 1:  76% 10558/13868 [2:12:38<41:47,  1.32it/s]Training epoch 1:  76% 10559/13868 [2:12:39<41:22,  1.33it/s]Training epoch 1:  76% 10560/13868 [2:12:40<41:36,  1.33it/s]Training epoch 1:  76% 10561/13868 [2:12:40<41:38,  1.32it/s]Training epoch 1:  76% 10562/13868 [2:12:41<41:29,  1.33it/s]Training epoch 1:  76% 10563/13868 [2:12:42<41:29,  1.33it/s]Training epoch 1:  76% 10564/13868 [2:12:43<41:28,  1.33it/s]Training epoch 1:  76% 10565/13868 [2:12:43<41:19,  1.33it/s]Training epoch 1:  76% 10566/13868 [2:12:44<41:34,  1.32it/s]Training epoch 1:  76% 10567/13868 [2:12:45<41:30,  1.33it/s]Training epoch 1:  76% 10568/13868 [2:12:46<41:54,  1.31it/s]Training epoch 1:  76% 10569/13868 [2:12:46<41:22,  1.33it/s]Training epoch 1:  76% 10570/13868 [2:12:47<41:44,  1.32it/s]Training epoch 1:  76% 10571/13868 [2:12:48<41:12,  1.33it/s]Training epoch 1:  76% 10572/13868 [2:12:49<40:35,  1.35it/s]Training epoch 1:  76% 10573/13868 [2:12:49<40:53,  1.34it/s]Training epoch 1:  76% 10574/13868 [2:12:50<41:23,  1.33it/s]Training epoch 1:  76% 10575/13868 [2:12:51<41:22,  1.33it/s]Training epoch 1:  76% 10576/13868 [2:12:52<41:32,  1.32it/s]Training epoch 1:  76% 10577/13868 [2:12:53<42:00,  1.31it/s]Training epoch 1:  76% 10578/13868 [2:12:53<42:27,  1.29it/s]Training epoch 1:  76% 10579/13868 [2:12:54<42:05,  1.30it/s]Training epoch 1:  76% 10580/13868 [2:12:55<41:59,  1.30it/s]Training epoch 1:  76% 10581/13868 [2:12:56<41:50,  1.31it/s]Training epoch 1:  76% 10582/13868 [2:12:56<41:32,  1.32it/s]Training epoch 1:  76% 10583/13868 [2:12:57<41:37,  1.32it/s]Training epoch 1:  76% 10584/13868 [2:12:58<41:52,  1.31it/s]Training epoch 1:  76% 10585/13868 [2:12:59<41:46,  1.31it/s]Training epoch 1:  76% 10586/13868 [2:12:59<41:31,  1.32it/s]Training epoch 1:  76% 10587/13868 [2:13:00<41:06,  1.33it/s]Training epoch 1:  76% 10588/13868 [2:13:01<41:16,  1.32it/s]Training epoch 1:  76% 10589/13868 [2:13:02<40:53,  1.34it/s]Training epoch 1:  76% 10590/13868 [2:13:02<41:15,  1.32it/s]Training epoch 1:  76% 10591/13868 [2:13:03<41:10,  1.33it/s]Training epoch 1:  76% 10592/13868 [2:13:04<40:39,  1.34it/s]Training epoch 1:  76% 10593/13868 [2:13:05<40:24,  1.35it/s]Training epoch 1:  76% 10594/13868 [2:13:05<40:47,  1.34it/s]Training epoch 1:  76% 10595/13868 [2:13:06<41:04,  1.33it/s]Training epoch 1:  76% 10596/13868 [2:13:07<41:10,  1.32it/s]Training epoch 1:  76% 10597/13868 [2:13:08<41:26,  1.32it/s]Training epoch 1:  76% 10598/13868 [2:13:08<41:38,  1.31it/s]Training epoch 1:  76% 10599/13868 [2:13:09<41:17,  1.32it/s]Training epoch 1:  76% 10600/13868 [2:13:10<43:13,  1.26it/s]Training epoch 1:  76% 10601/13868 [2:13:11<42:28,  1.28it/s]Training epoch 1:  76% 10602/13868 [2:13:12<42:19,  1.29it/s]Training epoch 1:  76% 10603/13868 [2:13:12<42:06,  1.29it/s]Training epoch 1:  76% 10604/13868 [2:13:13<42:19,  1.29it/s]Training epoch 1:  76% 10605/13868 [2:13:14<42:19,  1.29it/s]Training epoch 1:  76% 10606/13868 [2:13:15<42:07,  1.29it/s]Training epoch 1:  76% 10607/13868 [2:13:15<41:41,  1.30it/s]Training epoch 1:  76% 10608/13868 [2:13:16<41:58,  1.29it/s]Training epoch 1:  76% 10609/13868 [2:13:17<41:53,  1.30it/s]Training epoch 1:  77% 10610/13868 [2:13:18<41:43,  1.30it/s]Training epoch 1:  77% 10611/13868 [2:13:18<41:44,  1.30it/s]Training epoch 1:  77% 10612/13868 [2:13:19<41:26,  1.31it/s]Training epoch 1:  77% 10613/13868 [2:13:20<41:35,  1.30it/s]Training epoch 1:  77% 10614/13868 [2:13:21<41:05,  1.32it/s]Training epoch 1:  77% 10615/13868 [2:13:21<40:56,  1.32it/s]Training epoch 1:  77% 10616/13868 [2:13:22<41:20,  1.31it/s]Training epoch 1:  77% 10617/13868 [2:13:23<41:17,  1.31it/s]Training epoch 1:  77% 10618/13868 [2:13:24<40:47,  1.33it/s]Training epoch 1:  77% 10619/13868 [2:13:25<40:58,  1.32it/s]Training epoch 1:  77% 10620/13868 [2:13:25<41:10,  1.31it/s]Training epoch 1:  77% 10621/13868 [2:13:26<41:06,  1.32it/s]Training epoch 1:  77% 10622/13868 [2:13:27<41:02,  1.32it/s]Training epoch 1:  77% 10623/13868 [2:13:28<41:29,  1.30it/s]Training epoch 1:  77% 10624/13868 [2:13:28<40:42,  1.33it/s]Training epoch 1:  77% 10625/13868 [2:13:29<40:40,  1.33it/s]Training epoch 1:  77% 10626/13868 [2:13:30<40:31,  1.33it/s]Training epoch 1:  77% 10627/13868 [2:13:31<40:17,  1.34it/s]Training epoch 1:  77% 10628/13868 [2:13:31<40:45,  1.32it/s]Training epoch 1:  77% 10629/13868 [2:13:32<40:26,  1.33it/s]Training epoch 1:  77% 10630/13868 [2:13:33<40:26,  1.33it/s]Training epoch 1:  77% 10631/13868 [2:13:34<40:40,  1.33it/s]Training epoch 1:  77% 10632/13868 [2:13:34<41:22,  1.30it/s]Training epoch 1:  77% 10633/13868 [2:13:35<41:26,  1.30it/s]Training epoch 1:  77% 10634/13868 [2:13:36<41:30,  1.30it/s]Training epoch 1:  77% 10635/13868 [2:13:37<41:04,  1.31it/s]Training epoch 1:  77% 10636/13868 [2:13:37<40:25,  1.33it/s]Training epoch 1:  77% 10637/13868 [2:13:38<40:33,  1.33it/s]Training epoch 1:  77% 10638/13868 [2:13:39<41:04,  1.31it/s]Training epoch 1:  77% 10639/13868 [2:13:40<40:45,  1.32it/s]Training epoch 1:  77% 10640/13868 [2:13:40<40:49,  1.32it/s]Training epoch 1:  77% 10641/13868 [2:13:41<40:50,  1.32it/s]Training epoch 1:  77% 10642/13868 [2:13:42<40:48,  1.32it/s]Training epoch 1:  77% 10643/13868 [2:13:43<41:02,  1.31it/s]Training epoch 1:  77% 10644/13868 [2:13:44<41:08,  1.31it/s]Training epoch 1:  77% 10645/13868 [2:13:44<41:14,  1.30it/s]Training epoch 1:  77% 10646/13868 [2:13:45<41:20,  1.30it/s]Training epoch 1:  77% 10647/13868 [2:13:46<41:09,  1.30it/s]Training epoch 1:  77% 10648/13868 [2:13:47<41:15,  1.30it/s]Training epoch 1:  77% 10649/13868 [2:13:47<40:45,  1.32it/s]Training epoch 1:  77% 10650/13868 [2:13:48<41:07,  1.30it/s]Training epoch 1:  77% 10651/13868 [2:13:49<40:30,  1.32it/s]Training epoch 1:  77% 10652/13868 [2:13:50<40:37,  1.32it/s]Training epoch 1:  77% 10653/13868 [2:13:50<41:06,  1.30it/s]Training epoch 1:  77% 10654/13868 [2:13:51<41:04,  1.30it/s]Training epoch 1:  77% 10655/13868 [2:13:52<40:41,  1.32it/s]Training epoch 1:  77% 10656/13868 [2:13:53<40:24,  1.32it/s]Training epoch 1:  77% 10657/13868 [2:13:53<40:15,  1.33it/s]Training epoch 1:  77% 10658/13868 [2:13:54<40:57,  1.31it/s]Training epoch 1:  77% 10659/13868 [2:13:55<41:04,  1.30it/s]Training epoch 1:  77% 10660/13868 [2:13:56<40:31,  1.32it/s]Training epoch 1:  77% 10661/13868 [2:13:56<40:08,  1.33it/s]Training epoch 1:  77% 10662/13868 [2:13:57<40:10,  1.33it/s]Training epoch 1:  77% 10663/13868 [2:13:58<40:39,  1.31it/s]Training epoch 1:  77% 10664/13868 [2:13:59<40:52,  1.31it/s]Training epoch 1:  77% 10665/13868 [2:14:00<40:46,  1.31it/s]Training epoch 1:  77% 10666/13868 [2:14:00<39:38,  1.35it/s]Training epoch 1:  77% 10667/13868 [2:14:01<40:14,  1.33it/s]Training epoch 1:  77% 10668/13868 [2:14:02<39:40,  1.34it/s]Training epoch 1:  77% 10669/13868 [2:14:02<39:55,  1.34it/s]Training epoch 1:  77% 10670/13868 [2:14:03<39:58,  1.33it/s]Training epoch 1:  77% 10671/13868 [2:14:04<40:06,  1.33it/s]Training epoch 1:  77% 10672/13868 [2:14:05<40:35,  1.31it/s]Training epoch 1:  77% 10673/13868 [2:14:06<40:33,  1.31it/s]Training epoch 1:  77% 10674/13868 [2:14:06<40:47,  1.31it/s]Training epoch 1:  77% 10675/13868 [2:14:07<40:22,  1.32it/s]Training epoch 1:  77% 10676/13868 [2:14:08<39:48,  1.34it/s]Training epoch 1:  77% 10677/13868 [2:14:09<39:47,  1.34it/s]Training epoch 1:  77% 10678/13868 [2:14:09<40:12,  1.32it/s]Training epoch 1:  77% 10679/13868 [2:14:10<40:00,  1.33it/s]Training epoch 1:  77% 10680/13868 [2:14:11<39:58,  1.33it/s]Training epoch 1:  77% 10681/13868 [2:14:12<40:30,  1.31it/s]Training epoch 1:  77% 10682/13868 [2:14:12<40:44,  1.30it/s]Training epoch 1:  77% 10683/13868 [2:14:13<41:26,  1.28it/s]Training epoch 1:  77% 10684/13868 [2:14:14<41:18,  1.28it/s]Training epoch 1:  77% 10685/13868 [2:14:15<41:05,  1.29it/s]Training epoch 1:  77% 10686/13868 [2:14:15<40:46,  1.30it/s]Training epoch 1:  77% 10687/13868 [2:14:16<40:34,  1.31it/s]Training epoch 1:  77% 10688/13868 [2:14:17<40:39,  1.30it/s]Training epoch 1:  77% 10689/13868 [2:14:18<40:16,  1.32it/s]Training epoch 1:  77% 10690/13868 [2:14:18<40:17,  1.31it/s]Training epoch 1:  77% 10691/13868 [2:14:19<39:32,  1.34it/s]Training epoch 1:  77% 10692/13868 [2:14:20<39:18,  1.35it/s]Training epoch 1:  77% 10693/13868 [2:14:21<39:51,  1.33it/s]Training epoch 1:  77% 10694/13868 [2:14:21<39:37,  1.34it/s]Training epoch 1:  77% 10695/13868 [2:14:22<40:05,  1.32it/s]Training epoch 1:  77% 10696/13868 [2:14:23<40:42,  1.30it/s]Training epoch 1:  77% 10697/13868 [2:14:24<39:56,  1.32it/s]Training epoch 1:  77% 10698/13868 [2:14:25<40:02,  1.32it/s]Training epoch 1:  77% 10699/13868 [2:14:25<40:03,  1.32it/s]Training epoch 1:  77% 10700/13868 [2:14:26<42:55,  1.23it/s]Training epoch 1:  77% 10701/13868 [2:14:27<41:50,  1.26it/s]Training epoch 1:  77% 10702/13868 [2:14:28<41:12,  1.28it/s]Training epoch 1:  77% 10703/13868 [2:14:28<40:55,  1.29it/s]Training epoch 1:  77% 10704/13868 [2:14:29<41:00,  1.29it/s]Training epoch 1:  77% 10705/13868 [2:14:30<40:46,  1.29it/s]Training epoch 1:  77% 10706/13868 [2:14:31<40:44,  1.29it/s]Training epoch 1:  77% 10707/13868 [2:14:32<40:43,  1.29it/s]Training epoch 1:  77% 10708/13868 [2:14:32<40:38,  1.30it/s]Training epoch 1:  77% 10709/13868 [2:14:33<40:13,  1.31it/s]Training epoch 1:  77% 10710/13868 [2:14:34<40:14,  1.31it/s]Training epoch 1:  77% 10711/13868 [2:14:35<39:54,  1.32it/s]Training epoch 1:  77% 10712/13868 [2:14:35<39:31,  1.33it/s]Training epoch 1:  77% 10713/13868 [2:14:36<39:36,  1.33it/s]Training epoch 1:  77% 10714/13868 [2:14:37<40:03,  1.31it/s]Training epoch 1:  77% 10715/13868 [2:14:38<39:38,  1.33it/s]Training epoch 1:  77% 10716/13868 [2:14:38<39:48,  1.32it/s]Training epoch 1:  77% 10717/13868 [2:14:39<40:13,  1.31it/s]Training epoch 1:  77% 10718/13868 [2:14:40<40:04,  1.31it/s]Training epoch 1:  77% 10719/13868 [2:14:41<39:55,  1.31it/s]Training epoch 1:  77% 10720/13868 [2:14:41<40:46,  1.29it/s]Training epoch 1:  77% 10721/13868 [2:14:42<40:21,  1.30it/s]Training epoch 1:  77% 10722/13868 [2:14:43<40:31,  1.29it/s]Training epoch 1:  77% 10723/13868 [2:14:44<40:12,  1.30it/s]Training epoch 1:  77% 10724/13868 [2:14:45<40:02,  1.31it/s]Training epoch 1:  77% 10725/13868 [2:14:45<39:39,  1.32it/s]Training epoch 1:  77% 10726/13868 [2:14:46<40:06,  1.31it/s]Training epoch 1:  77% 10727/13868 [2:14:47<40:10,  1.30it/s]Training epoch 1:  77% 10728/13868 [2:14:48<40:01,  1.31it/s]Training epoch 1:  77% 10729/13868 [2:14:48<40:42,  1.29it/s]Training epoch 1:  77% 10730/13868 [2:14:49<40:32,  1.29it/s]Training epoch 1:  77% 10731/13868 [2:14:50<40:44,  1.28it/s]Training epoch 1:  77% 10732/13868 [2:14:51<40:30,  1.29it/s]Training epoch 1:  77% 10733/13868 [2:14:51<40:18,  1.30it/s]Training epoch 1:  77% 10734/13868 [2:14:52<40:15,  1.30it/s]Training epoch 1:  77% 10735/13868 [2:14:53<40:05,  1.30it/s]Training epoch 1:  77% 10736/13868 [2:14:54<40:06,  1.30it/s]Training epoch 1:  77% 10737/13868 [2:14:55<39:58,  1.31it/s]Training epoch 1:  77% 10738/13868 [2:14:55<39:59,  1.30it/s]Training epoch 1:  77% 10739/13868 [2:14:56<39:51,  1.31it/s]Training epoch 1:  77% 10740/13868 [2:14:57<40:03,  1.30it/s]Training epoch 1:  77% 10741/13868 [2:14:58<40:33,  1.29it/s]Training epoch 1:  77% 10742/13868 [2:14:58<40:19,  1.29it/s]Training epoch 1:  77% 10743/13868 [2:14:59<40:20,  1.29it/s]Training epoch 1:  77% 10744/13868 [2:15:00<40:58,  1.27it/s]Training epoch 1:  77% 10745/13868 [2:15:01<40:31,  1.28it/s]Training epoch 1:  77% 10746/13868 [2:15:02<40:15,  1.29it/s]Training epoch 1:  77% 10747/13868 [2:15:02<40:06,  1.30it/s]Training epoch 1:  78% 10748/13868 [2:15:03<40:08,  1.30it/s]Training epoch 1:  78% 10749/13868 [2:15:04<39:53,  1.30it/s]Training epoch 1:  78% 10750/13868 [2:15:05<39:44,  1.31it/s]Training epoch 1:  78% 10751/13868 [2:15:05<39:25,  1.32it/s]Training epoch 1:  78% 10752/13868 [2:15:06<40:27,  1.28it/s]Training epoch 1:  78% 10753/13868 [2:15:07<39:42,  1.31it/s]Training epoch 1:  78% 10754/13868 [2:15:08<39:39,  1.31it/s]Training epoch 1:  78% 10755/13868 [2:15:08<39:52,  1.30it/s]Training epoch 1:  78% 10756/13868 [2:15:09<40:06,  1.29it/s]Training epoch 1:  78% 10757/13868 [2:15:10<39:32,  1.31it/s]Training epoch 1:  78% 10758/13868 [2:15:11<39:55,  1.30it/s]Training epoch 1:  78% 10759/13868 [2:15:11<39:28,  1.31it/s]Training epoch 1:  78% 10760/13868 [2:15:12<38:59,  1.33it/s]Training epoch 1:  78% 10761/13868 [2:15:13<39:20,  1.32it/s]Training epoch 1:  78% 10762/13868 [2:15:14<38:59,  1.33it/s]Training epoch 1:  78% 10763/13868 [2:15:14<38:40,  1.34it/s]Training epoch 1:  78% 10764/13868 [2:15:15<39:09,  1.32it/s]Training epoch 1:  78% 10765/13868 [2:15:16<39:16,  1.32it/s]Training epoch 1:  78% 10766/13868 [2:15:17<39:13,  1.32it/s]Training epoch 1:  78% 10767/13868 [2:15:17<39:06,  1.32it/s]Training epoch 1:  78% 10768/13868 [2:15:18<39:09,  1.32it/s]Training epoch 1:  78% 10769/13868 [2:15:19<39:08,  1.32it/s]Training epoch 1:  78% 10770/13868 [2:15:20<39:01,  1.32it/s]Training epoch 1:  78% 10771/13868 [2:15:21<38:48,  1.33it/s]Training epoch 1:  78% 10772/13868 [2:15:21<39:28,  1.31it/s]Training epoch 1:  78% 10773/13868 [2:15:22<39:27,  1.31it/s]Training epoch 1:  78% 10774/13868 [2:15:23<39:20,  1.31it/s]Training epoch 1:  78% 10775/13868 [2:15:24<39:09,  1.32it/s]Training epoch 1:  78% 10776/13868 [2:15:24<39:35,  1.30it/s]Training epoch 1:  78% 10777/13868 [2:15:25<39:29,  1.30it/s]Training epoch 1:  78% 10778/13868 [2:15:26<39:25,  1.31it/s]Training epoch 1:  78% 10779/13868 [2:15:27<39:10,  1.31it/s]Training epoch 1:  78% 10780/13868 [2:15:27<39:24,  1.31it/s]Training epoch 1:  78% 10781/13868 [2:15:28<39:25,  1.30it/s]Training epoch 1:  78% 10782/13868 [2:15:29<39:25,  1.30it/s]Training epoch 1:  78% 10783/13868 [2:15:30<39:39,  1.30it/s]Training epoch 1:  78% 10784/13868 [2:15:30<39:16,  1.31it/s]Training epoch 1:  78% 10785/13868 [2:15:31<39:08,  1.31it/s]Training epoch 1:  78% 10786/13868 [2:15:32<39:24,  1.30it/s]Training epoch 1:  78% 10787/13868 [2:15:33<39:33,  1.30it/s]Training epoch 1:  78% 10788/13868 [2:15:34<38:51,  1.32it/s]Training epoch 1:  78% 10789/13868 [2:15:34<38:52,  1.32it/s]Training epoch 1:  78% 10790/13868 [2:15:35<38:51,  1.32it/s]Training epoch 1:  78% 10791/13868 [2:15:36<38:34,  1.33it/s]Training epoch 1:  78% 10792/13868 [2:15:37<38:36,  1.33it/s]Training epoch 1:  78% 10793/13868 [2:15:37<39:11,  1.31it/s]Training epoch 1:  78% 10794/13868 [2:15:38<39:04,  1.31it/s]Training epoch 1:  78% 10795/13868 [2:15:39<39:17,  1.30it/s]Training epoch 1:  78% 10796/13868 [2:15:40<39:01,  1.31it/s]Training epoch 1:  78% 10797/13868 [2:15:40<38:44,  1.32it/s]Training epoch 1:  78% 10798/13868 [2:15:41<38:36,  1.33it/s]Training epoch 1:  78% 10799/13868 [2:15:42<38:42,  1.32it/s]Training epoch 1:  78% 10800/13868 [2:15:43<40:48,  1.25it/s]Training epoch 1:  78% 10801/13868 [2:15:44<40:06,  1.27it/s]Training epoch 1:  78% 10802/13868 [2:15:44<39:49,  1.28it/s]Training epoch 1:  78% 10803/13868 [2:15:45<39:01,  1.31it/s]Training epoch 1:  78% 10804/13868 [2:15:46<39:25,  1.30it/s]Training epoch 1:  78% 10805/13868 [2:15:47<39:52,  1.28it/s]Training epoch 1:  78% 10806/13868 [2:15:47<39:50,  1.28it/s]Training epoch 1:  78% 10807/13868 [2:15:48<39:38,  1.29it/s]Training epoch 1:  78% 10808/13868 [2:15:49<39:56,  1.28it/s]Training epoch 1:  78% 10809/13868 [2:15:50<39:35,  1.29it/s]Training epoch 1:  78% 10810/13868 [2:15:50<39:35,  1.29it/s]Training epoch 1:  78% 10811/13868 [2:15:51<39:51,  1.28it/s]Training epoch 1:  78% 10812/13868 [2:15:52<39:38,  1.29it/s]Training epoch 1:  78% 10813/13868 [2:15:53<39:31,  1.29it/s]Training epoch 1:  78% 10814/13868 [2:15:54<39:43,  1.28it/s]Training epoch 1:  78% 10815/13868 [2:15:54<39:03,  1.30it/s]Training epoch 1:  78% 10816/13868 [2:15:55<39:09,  1.30it/s]Training epoch 1:  78% 10817/13868 [2:15:56<39:02,  1.30it/s]Training epoch 1:  78% 10818/13868 [2:15:57<38:14,  1.33it/s]Training epoch 1:  78% 10819/13868 [2:15:57<38:27,  1.32it/s]Training epoch 1:  78% 10820/13868 [2:15:58<38:28,  1.32it/s]Training epoch 1:  78% 10821/13868 [2:15:59<38:40,  1.31it/s]Training epoch 1:  78% 10822/13868 [2:16:00<38:51,  1.31it/s]Training epoch 1:  78% 10823/13868 [2:16:00<39:00,  1.30it/s]Training epoch 1:  78% 10824/13868 [2:16:01<38:52,  1.30it/s]Training epoch 1:  78% 10825/13868 [2:16:02<38:20,  1.32it/s]Training epoch 1:  78% 10826/13868 [2:16:03<38:49,  1.31it/s]Training epoch 1:  78% 10827/13868 [2:16:03<38:35,  1.31it/s]Training epoch 1:  78% 10828/13868 [2:16:04<38:40,  1.31it/s]Training epoch 1:  78% 10829/13868 [2:16:05<39:04,  1.30it/s]Training epoch 1:  78% 10830/13868 [2:16:06<38:45,  1.31it/s]Training epoch 1:  78% 10831/13868 [2:16:07<38:51,  1.30it/s]Training epoch 1:  78% 10832/13868 [2:16:07<39:08,  1.29it/s]Training epoch 1:  78% 10833/13868 [2:16:08<38:51,  1.30it/s]Training epoch 1:  78% 10834/13868 [2:16:09<38:38,  1.31it/s]Training epoch 1:  78% 10835/13868 [2:16:10<38:21,  1.32it/s]Training epoch 1:  78% 10836/13868 [2:16:10<38:20,  1.32it/s]Training epoch 1:  78% 10837/13868 [2:16:11<38:26,  1.31it/s]Training epoch 1:  78% 10838/13868 [2:16:12<38:12,  1.32it/s]Training epoch 1:  78% 10839/13868 [2:16:13<38:08,  1.32it/s]Training epoch 1:  78% 10840/13868 [2:16:13<37:33,  1.34it/s]Training epoch 1:  78% 10841/13868 [2:16:14<37:27,  1.35it/s]Training epoch 1:  78% 10842/13868 [2:16:15<38:05,  1.32it/s]Training epoch 1:  78% 10843/13868 [2:16:16<37:35,  1.34it/s]Training epoch 1:  78% 10844/13868 [2:16:16<37:43,  1.34it/s]Training epoch 1:  78% 10845/13868 [2:16:17<37:09,  1.36it/s]Training epoch 1:  78% 10846/13868 [2:16:18<36:59,  1.36it/s]Training epoch 1:  78% 10847/13868 [2:16:19<37:25,  1.35it/s]Training epoch 1:  78% 10848/13868 [2:16:19<37:10,  1.35it/s]Training epoch 1:  78% 10849/13868 [2:16:20<37:03,  1.36it/s]Training epoch 1:  78% 10850/13868 [2:16:21<36:48,  1.37it/s]Training epoch 1:  78% 10851/13868 [2:16:22<37:12,  1.35it/s]Training epoch 1:  78% 10852/13868 [2:16:22<36:53,  1.36it/s]Training epoch 1:  78% 10853/13868 [2:16:23<36:59,  1.36it/s]Training epoch 1:  78% 10854/13868 [2:16:24<36:50,  1.36it/s]Training epoch 1:  78% 10855/13868 [2:16:24<36:43,  1.37it/s]Training epoch 1:  78% 10856/13868 [2:16:25<36:10,  1.39it/s]Training epoch 1:  78% 10857/13868 [2:16:26<36:53,  1.36it/s]Training epoch 1:  78% 10858/13868 [2:16:27<37:19,  1.34it/s]Training epoch 1:  78% 10859/13868 [2:16:27<38:04,  1.32it/s]Training epoch 1:  78% 10860/13868 [2:16:28<37:53,  1.32it/s]Training epoch 1:  78% 10861/13868 [2:16:29<38:16,  1.31it/s]Training epoch 1:  78% 10862/13868 [2:16:30<38:22,  1.31it/s]Training epoch 1:  78% 10863/13868 [2:16:30<38:09,  1.31it/s]Training epoch 1:  78% 10864/13868 [2:16:31<38:12,  1.31it/s]Training epoch 1:  78% 10865/13868 [2:16:32<38:07,  1.31it/s]Training epoch 1:  78% 10866/13868 [2:16:33<38:12,  1.31it/s]Training epoch 1:  78% 10867/13868 [2:16:34<38:32,  1.30it/s]Training epoch 1:  78% 10868/13868 [2:16:34<38:18,  1.31it/s]Training epoch 1:  78% 10869/13868 [2:16:35<37:56,  1.32it/s]Training epoch 1:  78% 10870/13868 [2:16:36<38:08,  1.31it/s]Training epoch 1:  78% 10871/13868 [2:16:37<38:31,  1.30it/s]Training epoch 1:  78% 10872/13868 [2:16:37<37:54,  1.32it/s]Training epoch 1:  78% 10873/13868 [2:16:38<37:26,  1.33it/s]Training epoch 1:  78% 10874/13868 [2:16:39<37:44,  1.32it/s]Training epoch 1:  78% 10875/13868 [2:16:40<37:54,  1.32it/s]Training epoch 1:  78% 10876/13868 [2:16:40<37:21,  1.33it/s]Training epoch 1:  78% 10877/13868 [2:16:41<37:29,  1.33it/s]Training epoch 1:  78% 10878/13868 [2:16:42<36:50,  1.35it/s]Training epoch 1:  78% 10879/13868 [2:16:43<37:07,  1.34it/s]Training epoch 1:  78% 10880/13868 [2:16:43<37:16,  1.34it/s]Training epoch 1:  78% 10881/13868 [2:16:44<37:36,  1.32it/s]Training epoch 1:  78% 10882/13868 [2:16:45<38:05,  1.31it/s]Training epoch 1:  78% 10883/13868 [2:16:46<38:09,  1.30it/s]Training epoch 1:  78% 10884/13868 [2:16:46<38:28,  1.29it/s]Training epoch 1:  78% 10885/13868 [2:16:47<38:28,  1.29it/s]Training epoch 1:  78% 10886/13868 [2:16:48<38:24,  1.29it/s]Training epoch 1:  79% 10887/13868 [2:16:49<38:40,  1.28it/s]Training epoch 1:  79% 10888/13868 [2:16:50<38:02,  1.31it/s]Training epoch 1:  79% 10889/13868 [2:16:50<37:57,  1.31it/s]Training epoch 1:  79% 10890/13868 [2:16:51<37:30,  1.32it/s]Training epoch 1:  79% 10891/13868 [2:16:52<37:08,  1.34it/s]Training epoch 1:  79% 10892/13868 [2:16:53<37:36,  1.32it/s]Training epoch 1:  79% 10893/13868 [2:16:53<37:07,  1.34it/s]Training epoch 1:  79% 10894/13868 [2:16:54<36:49,  1.35it/s]Training epoch 1:  79% 10895/13868 [2:16:55<36:47,  1.35it/s]Training epoch 1:  79% 10896/13868 [2:16:56<37:03,  1.34it/s]Training epoch 1:  79% 10897/13868 [2:16:56<37:24,  1.32it/s]Training epoch 1:  79% 10898/13868 [2:16:57<37:29,  1.32it/s]Training epoch 1:  79% 10899/13868 [2:16:58<37:02,  1.34it/s]Training epoch 1:  79% 10900/13868 [2:16:59<39:33,  1.25it/s]Training epoch 1:  79% 10901/13868 [2:16:59<38:39,  1.28it/s]Training epoch 1:  79% 10902/13868 [2:17:00<37:59,  1.30it/s]Training epoch 1:  79% 10903/13868 [2:17:01<38:01,  1.30it/s]Training epoch 1:  79% 10904/13868 [2:17:02<37:37,  1.31it/s]Training epoch 1:  79% 10905/13868 [2:17:02<37:11,  1.33it/s]Training epoch 1:  79% 10906/13868 [2:17:03<37:05,  1.33it/s]Training epoch 1:  79% 10907/13868 [2:17:04<37:35,  1.31it/s]Training epoch 1:  79% 10908/13868 [2:17:05<36:43,  1.34it/s]Training epoch 1:  79% 10909/13868 [2:17:05<37:27,  1.32it/s]Training epoch 1:  79% 10910/13868 [2:17:06<37:15,  1.32it/s]Training epoch 1:  79% 10911/13868 [2:17:07<36:56,  1.33it/s]Training epoch 1:  79% 10912/13868 [2:17:08<36:35,  1.35it/s]Training epoch 1:  79% 10913/13868 [2:17:08<36:38,  1.34it/s]Training epoch 1:  79% 10914/13868 [2:17:09<36:22,  1.35it/s]Training epoch 1:  79% 10915/13868 [2:17:10<36:16,  1.36it/s]Training epoch 1:  79% 10916/13868 [2:17:11<36:12,  1.36it/s]Training epoch 1:  79% 10917/13868 [2:17:11<36:35,  1.34it/s]Training epoch 1:  79% 10918/13868 [2:17:12<36:32,  1.35it/s]Training epoch 1:  79% 10919/13868 [2:17:13<36:28,  1.35it/s]Training epoch 1:  79% 10920/13868 [2:17:14<36:22,  1.35it/s]Training epoch 1:  79% 10921/13868 [2:17:14<36:56,  1.33it/s]Training epoch 1:  79% 10922/13868 [2:17:15<36:33,  1.34it/s]Training epoch 1:  79% 10923/13868 [2:17:16<36:43,  1.34it/s]Training epoch 1:  79% 10924/13868 [2:17:17<36:48,  1.33it/s]Training epoch 1:  79% 10925/13868 [2:17:17<37:40,  1.30it/s]Training epoch 1:  79% 10926/13868 [2:17:18<38:00,  1.29it/s]Training epoch 1:  79% 10927/13868 [2:17:19<37:39,  1.30it/s]Training epoch 1:  79% 10928/13868 [2:17:20<37:12,  1.32it/s]Training epoch 1:  79% 10929/13868 [2:17:20<37:30,  1.31it/s]Training epoch 1:  79% 10930/13868 [2:17:21<37:22,  1.31it/s]Training epoch 1:  79% 10931/13868 [2:17:22<37:26,  1.31it/s]Training epoch 1:  79% 10932/13868 [2:17:23<37:13,  1.31it/s]Training epoch 1:  79% 10933/13868 [2:17:23<36:55,  1.32it/s]Training epoch 1:  79% 10934/13868 [2:17:24<36:55,  1.32it/s]Training epoch 1:  79% 10935/13868 [2:17:25<36:41,  1.33it/s]Training epoch 1:  79% 10936/13868 [2:17:26<36:08,  1.35it/s]Training epoch 1:  79% 10937/13868 [2:17:26<36:34,  1.34it/s]Training epoch 1:  79% 10938/13868 [2:17:27<36:28,  1.34it/s]Training epoch 1:  79% 10939/13868 [2:17:28<36:05,  1.35it/s]Training epoch 1:  79% 10940/13868 [2:17:29<36:03,  1.35it/s]Training epoch 1:  79% 10941/13868 [2:17:29<36:36,  1.33it/s]Training epoch 1:  79% 10942/13868 [2:17:30<37:20,  1.31it/s]Training epoch 1:  79% 10943/13868 [2:17:31<37:01,  1.32it/s]Training epoch 1:  79% 10944/13868 [2:17:32<36:48,  1.32it/s]Training epoch 1:  79% 10945/13868 [2:17:32<36:50,  1.32it/s]Training epoch 1:  79% 10946/13868 [2:17:33<37:19,  1.31it/s]Training epoch 1:  79% 10947/13868 [2:17:34<37:37,  1.29it/s]Training epoch 1:  79% 10948/13868 [2:17:35<36:34,  1.33it/s]Training epoch 1:  79% 10949/13868 [2:17:36<36:54,  1.32it/s]Training epoch 1:  79% 10950/13868 [2:17:36<36:53,  1.32it/s]Training epoch 1:  79% 10951/13868 [2:17:37<36:32,  1.33it/s]Training epoch 1:  79% 10952/13868 [2:17:38<36:50,  1.32it/s]Training epoch 1:  79% 10953/13868 [2:17:39<36:53,  1.32it/s]Training epoch 1:  79% 10954/13868 [2:17:39<36:28,  1.33it/s]Training epoch 1:  79% 10955/13868 [2:17:40<36:29,  1.33it/s]Training epoch 1:  79% 10956/13868 [2:17:41<36:40,  1.32it/s]Training epoch 1:  79% 10957/13868 [2:17:42<36:00,  1.35it/s]Training epoch 1:  79% 10958/13868 [2:17:42<35:34,  1.36it/s]Training epoch 1:  79% 10959/13868 [2:17:43<35:36,  1.36it/s]Training epoch 1:  79% 10960/13868 [2:17:44<36:03,  1.34it/s]Training epoch 1:  79% 10961/13868 [2:17:45<36:28,  1.33it/s]Training epoch 1:  79% 10962/13868 [2:17:45<36:29,  1.33it/s]Training epoch 1:  79% 10963/13868 [2:17:46<36:05,  1.34it/s]Training epoch 1:  79% 10964/13868 [2:17:47<36:22,  1.33it/s]Training epoch 1:  79% 10965/13868 [2:17:48<36:04,  1.34it/s]Training epoch 1:  79% 10966/13868 [2:17:48<35:50,  1.35it/s]Training epoch 1:  79% 10967/13868 [2:17:49<35:55,  1.35it/s]Training epoch 1:  79% 10968/13868 [2:17:50<36:24,  1.33it/s]Training epoch 1:  79% 10969/13868 [2:17:51<36:35,  1.32it/s]Training epoch 1:  79% 10970/13868 [2:17:51<36:32,  1.32it/s]Training epoch 1:  79% 10971/13868 [2:17:52<36:23,  1.33it/s]Training epoch 1:  79% 10972/13868 [2:17:53<36:09,  1.33it/s]Training epoch 1:  79% 10973/13868 [2:17:54<36:43,  1.31it/s]Training epoch 1:  79% 10974/13868 [2:17:54<36:28,  1.32it/s]Training epoch 1:  79% 10975/13868 [2:17:55<36:05,  1.34it/s]Training epoch 1:  79% 10976/13868 [2:17:56<36:21,  1.33it/s]Training epoch 1:  79% 10977/13868 [2:17:57<36:12,  1.33it/s]Training epoch 1:  79% 10978/13868 [2:17:57<36:23,  1.32it/s]Training epoch 1:  79% 10979/13868 [2:17:58<36:25,  1.32it/s]Training epoch 1:  79% 10980/13868 [2:17:59<36:30,  1.32it/s]Training epoch 1:  79% 10981/13868 [2:18:00<36:11,  1.33it/s]Training epoch 1:  79% 10982/13868 [2:18:00<36:06,  1.33it/s]Training epoch 1:  79% 10983/13868 [2:18:01<36:15,  1.33it/s]Training epoch 1:  79% 10984/13868 [2:18:02<35:53,  1.34it/s]Training epoch 1:  79% 10985/13868 [2:18:03<36:20,  1.32it/s]Training epoch 1:  79% 10986/13868 [2:18:03<36:46,  1.31it/s]Training epoch 1:  79% 10987/13868 [2:18:04<36:35,  1.31it/s]Training epoch 1:  79% 10988/13868 [2:18:05<35:56,  1.34it/s]Training epoch 1:  79% 10989/13868 [2:18:06<35:47,  1.34it/s]Training epoch 1:  79% 10990/13868 [2:18:06<35:45,  1.34it/s]Training epoch 1:  79% 10991/13868 [2:18:07<35:46,  1.34it/s]Training epoch 1:  79% 10992/13868 [2:18:08<36:00,  1.33it/s]Training epoch 1:  79% 10993/13868 [2:18:09<35:47,  1.34it/s]Training epoch 1:  79% 10994/13868 [2:18:09<36:09,  1.32it/s]Training epoch 1:  79% 10995/13868 [2:18:10<36:11,  1.32it/s]Training epoch 1:  79% 10996/13868 [2:18:11<35:46,  1.34it/s]Training epoch 1:  79% 10997/13868 [2:18:12<35:31,  1.35it/s]Training epoch 1:  79% 10998/13868 [2:18:12<35:30,  1.35it/s]Training epoch 1:  79% 10999/13868 [2:18:13<35:33,  1.35it/s]Training epoch 1:  79% 11000/13868 [2:18:14<37:12,  1.28it/s]Training epoch 1:  79% 11001/13868 [2:18:15<36:50,  1.30it/s]Training epoch 1:  79% 11002/13868 [2:18:15<36:26,  1.31it/s]Training epoch 1:  79% 11003/13868 [2:18:16<36:42,  1.30it/s]Training epoch 1:  79% 11004/13868 [2:18:17<36:15,  1.32it/s]Training epoch 1:  79% 11005/13868 [2:18:18<35:44,  1.34it/s]Training epoch 1:  79% 11006/13868 [2:18:18<35:24,  1.35it/s]Training epoch 1:  79% 11007/13868 [2:18:19<35:37,  1.34it/s]Training epoch 1:  79% 11008/13868 [2:18:20<35:26,  1.34it/s]Training epoch 1:  79% 11009/13868 [2:18:21<35:47,  1.33it/s]Training epoch 1:  79% 11010/13868 [2:18:21<35:39,  1.34it/s]Training epoch 1:  79% 11011/13868 [2:18:22<35:10,  1.35it/s]Training epoch 1:  79% 11012/13868 [2:18:23<35:45,  1.33it/s]Training epoch 1:  79% 11013/13868 [2:18:24<36:18,  1.31it/s]Training epoch 1:  79% 11014/13868 [2:18:24<35:42,  1.33it/s]Training epoch 1:  79% 11015/13868 [2:18:25<36:05,  1.32it/s]Training epoch 1:  79% 11016/13868 [2:18:26<35:43,  1.33it/s]Training epoch 1:  79% 11017/13868 [2:18:27<35:19,  1.35it/s]Training epoch 1:  79% 11018/13868 [2:18:27<35:05,  1.35it/s]Training epoch 1:  79% 11019/13868 [2:18:28<34:55,  1.36it/s]Training epoch 1:  79% 11020/13868 [2:18:29<35:28,  1.34it/s]Training epoch 1:  79% 11021/13868 [2:18:30<35:33,  1.33it/s]Training epoch 1:  79% 11022/13868 [2:18:30<35:41,  1.33it/s]Training epoch 1:  79% 11023/13868 [2:18:31<36:11,  1.31it/s]Training epoch 1:  79% 11024/13868 [2:18:32<36:03,  1.31it/s]Training epoch 1:  79% 11025/13868 [2:18:33<36:04,  1.31it/s]Training epoch 1:  80% 11026/13868 [2:18:33<35:28,  1.34it/s]Training epoch 1:  80% 11027/13868 [2:18:34<35:31,  1.33it/s]Training epoch 1:  80% 11028/13868 [2:18:35<35:25,  1.34it/s]Training epoch 1:  80% 11029/13868 [2:18:36<35:40,  1.33it/s]Training epoch 1:  80% 11030/13868 [2:18:36<35:44,  1.32it/s]Training epoch 1:  80% 11031/13868 [2:18:37<35:47,  1.32it/s]Training epoch 1:  80% 11032/13868 [2:18:38<35:43,  1.32it/s]Training epoch 1:  80% 11033/13868 [2:18:39<35:33,  1.33it/s]Training epoch 1:  80% 11034/13868 [2:18:39<35:37,  1.33it/s]Training epoch 1:  80% 11035/13868 [2:18:40<35:27,  1.33it/s]Training epoch 1:  80% 11036/13868 [2:18:41<35:30,  1.33it/s]Training epoch 1:  80% 11037/13868 [2:18:42<35:46,  1.32it/s]Training epoch 1:  80% 11038/13868 [2:18:42<35:36,  1.32it/s]Training epoch 1:  80% 11039/13868 [2:18:43<35:44,  1.32it/s]Training epoch 1:  80% 11040/13868 [2:18:44<35:30,  1.33it/s]Training epoch 1:  80% 11041/13868 [2:18:45<35:30,  1.33it/s]Training epoch 1:  80% 11042/13868 [2:18:45<35:39,  1.32it/s]Training epoch 1:  80% 11043/13868 [2:18:46<36:04,  1.31it/s]Training epoch 1:  80% 11044/13868 [2:18:47<35:55,  1.31it/s]Training epoch 1:  80% 11045/13868 [2:18:48<35:38,  1.32it/s]Training epoch 1:  80% 11046/13868 [2:18:49<35:31,  1.32it/s]Training epoch 1:  80% 11047/13868 [2:18:49<35:18,  1.33it/s]Training epoch 1:  80% 11048/13868 [2:18:50<34:54,  1.35it/s]Training epoch 1:  80% 11049/13868 [2:18:51<34:57,  1.34it/s]Training epoch 1:  80% 11050/13868 [2:18:51<34:58,  1.34it/s]Training epoch 1:  80% 11051/13868 [2:18:52<34:56,  1.34it/s]Training epoch 1:  80% 11052/13868 [2:18:53<34:28,  1.36it/s]Training epoch 1:  80% 11053/13868 [2:18:54<35:20,  1.33it/s]Training epoch 1:  80% 11054/13868 [2:18:55<35:36,  1.32it/s]Training epoch 1:  80% 11055/13868 [2:18:55<35:39,  1.31it/s]Training epoch 1:  80% 11056/13868 [2:18:56<35:49,  1.31it/s]Training epoch 1:  80% 11057/13868 [2:18:57<35:37,  1.32it/s]Training epoch 1:  80% 11058/13868 [2:18:58<35:30,  1.32it/s]Training epoch 1:  80% 11059/13868 [2:18:58<35:05,  1.33it/s]Training epoch 1:  80% 11060/13868 [2:18:59<34:32,  1.35it/s]Training epoch 1:  80% 11061/13868 [2:19:00<34:46,  1.35it/s]Training epoch 1:  80% 11062/13868 [2:19:01<35:08,  1.33it/s]Training epoch 1:  80% 11063/13868 [2:19:01<35:21,  1.32it/s]Training epoch 1:  80% 11064/13868 [2:19:02<35:27,  1.32it/s]Training epoch 1:  80% 11065/13868 [2:19:03<35:29,  1.32it/s]Training epoch 1:  80% 11066/13868 [2:19:04<35:33,  1.31it/s]Training epoch 1:  80% 11067/13868 [2:19:04<35:29,  1.32it/s]Training epoch 1:  80% 11068/13868 [2:19:05<34:55,  1.34it/s]Training epoch 1:  80% 11069/13868 [2:19:06<34:36,  1.35it/s]Training epoch 1:  80% 11070/13868 [2:19:07<34:28,  1.35it/s]Training epoch 1:  80% 11071/13868 [2:19:07<35:08,  1.33it/s]Training epoch 1:  80% 11072/13868 [2:19:08<35:02,  1.33it/s]Training epoch 1:  80% 11073/13868 [2:19:09<35:14,  1.32it/s]Training epoch 1:  80% 11074/13868 [2:19:10<35:27,  1.31it/s]Training epoch 1:  80% 11075/13868 [2:19:10<35:22,  1.32it/s]Training epoch 1:  80% 11076/13868 [2:19:11<35:42,  1.30it/s]Training epoch 1:  80% 11077/13868 [2:19:12<35:45,  1.30it/s]Training epoch 1:  80% 11078/13868 [2:19:13<35:33,  1.31it/s]Training epoch 1:  80% 11079/13868 [2:19:13<35:36,  1.31it/s]Training epoch 1:  80% 11080/13868 [2:19:14<35:25,  1.31it/s]Training epoch 1:  80% 11081/13868 [2:19:15<35:08,  1.32it/s]Training epoch 1:  80% 11082/13868 [2:19:16<34:46,  1.34it/s]Training epoch 1:  80% 11083/13868 [2:19:16<34:49,  1.33it/s]Training epoch 1:  80% 11084/13868 [2:19:17<34:51,  1.33it/s]Training epoch 1:  80% 11085/13868 [2:19:18<34:57,  1.33it/s]Training epoch 1:  80% 11086/13868 [2:19:19<34:36,  1.34it/s]Training epoch 1:  80% 11087/13868 [2:19:19<34:56,  1.33it/s]Training epoch 1:  80% 11088/13868 [2:19:20<34:17,  1.35it/s]Training epoch 1:  80% 11089/13868 [2:19:21<34:18,  1.35it/s]Training epoch 1:  80% 11090/13868 [2:19:22<34:28,  1.34it/s]Training epoch 1:  80% 11091/13868 [2:19:22<34:48,  1.33it/s]Training epoch 1:  80% 11092/13868 [2:19:23<34:41,  1.33it/s]Training epoch 1:  80% 11093/13868 [2:19:24<34:39,  1.33it/s]Training epoch 1:  80% 11094/13868 [2:19:25<34:45,  1.33it/s]Training epoch 1:  80% 11095/13868 [2:19:25<34:53,  1.32it/s]Training epoch 1:  80% 11096/13868 [2:19:26<34:59,  1.32it/s]Training epoch 1:  80% 11097/13868 [2:19:27<34:29,  1.34it/s]Training epoch 1:  80% 11098/13868 [2:19:28<34:36,  1.33it/s]Training epoch 1:  80% 11099/13868 [2:19:28<34:14,  1.35it/s]Training epoch 1:  80% 11100/13868 [2:19:29<36:27,  1.27it/s]Training epoch 1:  80% 11101/13868 [2:19:30<35:53,  1.28it/s]Training epoch 1:  80% 11102/13868 [2:19:31<35:41,  1.29it/s]Training epoch 1:  80% 11103/13868 [2:19:32<35:15,  1.31it/s]Training epoch 1:  80% 11104/13868 [2:19:32<35:40,  1.29it/s]Training epoch 1:  80% 11105/13868 [2:19:33<35:21,  1.30it/s]Training epoch 1:  80% 11106/13868 [2:19:34<34:50,  1.32it/s]Training epoch 1:  80% 11107/13868 [2:19:35<34:50,  1.32it/s]Training epoch 1:  80% 11108/13868 [2:19:35<34:55,  1.32it/s]Training epoch 1:  80% 11109/13868 [2:19:36<34:45,  1.32it/s]Training epoch 1:  80% 11110/13868 [2:19:37<34:42,  1.32it/s]Training epoch 1:  80% 11111/13868 [2:19:38<34:20,  1.34it/s]Training epoch 1:  80% 11112/13868 [2:19:38<34:51,  1.32it/s]Training epoch 1:  80% 11113/13868 [2:19:39<34:59,  1.31it/s]Training epoch 1:  80% 11114/13868 [2:19:40<35:00,  1.31it/s]Training epoch 1:  80% 11115/13868 [2:19:41<35:00,  1.31it/s]Training epoch 1:  80% 11116/13868 [2:19:41<35:06,  1.31it/s]Training epoch 1:  80% 11117/13868 [2:19:42<34:48,  1.32it/s]Training epoch 1:  80% 11118/13868 [2:19:43<35:08,  1.30it/s]Training epoch 1:  80% 11119/13868 [2:19:44<35:00,  1.31it/s]Training epoch 1:  80% 11120/13868 [2:19:44<34:50,  1.31it/s]Training epoch 1:  80% 11121/13868 [2:19:45<34:57,  1.31it/s]Training epoch 1:  80% 11122/13868 [2:19:46<34:59,  1.31it/s]Training epoch 1:  80% 11123/13868 [2:19:47<35:01,  1.31it/s]Training epoch 1:  80% 11124/13868 [2:19:48<34:41,  1.32it/s]Training epoch 1:  80% 11125/13868 [2:19:48<34:37,  1.32it/s]Training epoch 1:  80% 11126/13868 [2:19:49<34:29,  1.32it/s]Training epoch 1:  80% 11127/13868 [2:19:50<34:32,  1.32it/s]Training epoch 1:  80% 11128/13868 [2:19:51<34:47,  1.31it/s]Training epoch 1:  80% 11129/13868 [2:19:51<34:52,  1.31it/s]Training epoch 1:  80% 11130/13868 [2:19:52<34:45,  1.31it/s]Training epoch 1:  80% 11131/13868 [2:19:53<34:37,  1.32it/s]Training epoch 1:  80% 11132/13868 [2:19:54<35:02,  1.30it/s]Training epoch 1:  80% 11133/13868 [2:19:54<34:41,  1.31it/s]Training epoch 1:  80% 11134/13868 [2:19:55<34:59,  1.30it/s]Training epoch 1:  80% 11135/13868 [2:19:56<34:39,  1.31it/s]Training epoch 1:  80% 11136/13868 [2:19:57<34:28,  1.32it/s]Training epoch 1:  80% 11137/13868 [2:19:57<34:25,  1.32it/s]Training epoch 1:  80% 11138/13868 [2:19:58<34:42,  1.31it/s]Training epoch 1:  80% 11139/13868 [2:19:59<34:39,  1.31it/s]Training epoch 1:  80% 11140/13868 [2:20:00<34:42,  1.31it/s]Training epoch 1:  80% 11141/13868 [2:20:00<33:53,  1.34it/s]Training epoch 1:  80% 11142/13868 [2:20:01<34:40,  1.31it/s]Training epoch 1:  80% 11143/13868 [2:20:02<34:09,  1.33it/s]Training epoch 1:  80% 11144/13868 [2:20:03<34:07,  1.33it/s]Training epoch 1:  80% 11145/13868 [2:20:03<33:45,  1.34it/s]Training epoch 1:  80% 11146/13868 [2:20:04<33:45,  1.34it/s]Training epoch 1:  80% 11147/13868 [2:20:05<33:13,  1.37it/s]Training epoch 1:  80% 11148/13868 [2:20:06<33:18,  1.36it/s]Training epoch 1:  80% 11149/13868 [2:20:06<33:45,  1.34it/s]Training epoch 1:  80% 11150/13868 [2:20:07<33:51,  1.34it/s]Training epoch 1:  80% 11151/13868 [2:20:08<33:57,  1.33it/s]Training epoch 1:  80% 11152/13868 [2:20:09<34:07,  1.33it/s]Training epoch 1:  80% 11153/13868 [2:20:09<33:41,  1.34it/s]Training epoch 1:  80% 11154/13868 [2:20:10<34:03,  1.33it/s]Training epoch 1:  80% 11155/13868 [2:20:11<34:05,  1.33it/s]Training epoch 1:  80% 11156/13868 [2:20:12<33:45,  1.34it/s]Training epoch 1:  80% 11157/13868 [2:20:12<33:24,  1.35it/s]Training epoch 1:  80% 11158/13868 [2:20:13<33:28,  1.35it/s]Training epoch 1:  80% 11159/13868 [2:20:14<33:57,  1.33it/s]Training epoch 1:  80% 11160/13868 [2:20:15<34:02,  1.33it/s]Training epoch 1:  80% 11161/13868 [2:20:15<34:09,  1.32it/s]Training epoch 1:  80% 11162/13868 [2:20:16<34:16,  1.32it/s]Training epoch 1:  80% 11163/13868 [2:20:17<33:57,  1.33it/s]Training epoch 1:  81% 11164/13868 [2:20:18<34:23,  1.31it/s]Training epoch 1:  81% 11165/13868 [2:20:18<34:09,  1.32it/s]Training epoch 1:  81% 11166/13868 [2:20:19<33:48,  1.33it/s]Training epoch 1:  81% 11167/13868 [2:20:20<34:02,  1.32it/s]Training epoch 1:  81% 11168/13868 [2:20:21<34:06,  1.32it/s]Training epoch 1:  81% 11169/13868 [2:20:21<34:22,  1.31it/s]Training epoch 1:  81% 11170/13868 [2:20:22<34:29,  1.30it/s]Training epoch 1:  81% 11171/13868 [2:20:23<34:20,  1.31it/s]Training epoch 1:  81% 11172/13868 [2:20:24<34:13,  1.31it/s]Training epoch 1:  81% 11173/13868 [2:20:25<34:18,  1.31it/s]Training epoch 1:  81% 11174/13868 [2:20:25<34:23,  1.31it/s]Training epoch 1:  81% 11175/13868 [2:20:26<33:52,  1.32it/s]Training epoch 1:  81% 11176/13868 [2:20:27<33:53,  1.32it/s]Training epoch 1:  81% 11177/13868 [2:20:28<33:22,  1.34it/s]Training epoch 1:  81% 11178/13868 [2:20:28<33:23,  1.34it/s]Training epoch 1:  81% 11179/13868 [2:20:29<33:13,  1.35it/s]Training epoch 1:  81% 11180/13868 [2:20:30<33:22,  1.34it/s]Training epoch 1:  81% 11181/13868 [2:20:30<33:26,  1.34it/s]Training epoch 1:  81% 11182/13868 [2:20:31<34:10,  1.31it/s]Training epoch 1:  81% 11183/13868 [2:20:32<33:48,  1.32it/s]Training epoch 1:  81% 11184/13868 [2:20:33<33:40,  1.33it/s]Training epoch 1:  81% 11185/13868 [2:20:34<33:38,  1.33it/s]Training epoch 1:  81% 11186/13868 [2:20:34<34:09,  1.31it/s]Training epoch 1:  81% 11187/13868 [2:20:35<33:27,  1.34it/s]Training epoch 1:  81% 11188/13868 [2:20:36<33:39,  1.33it/s]Training epoch 1:  81% 11189/13868 [2:20:37<33:35,  1.33it/s]Training epoch 1:  81% 11190/13868 [2:20:37<33:19,  1.34it/s]Training epoch 1:  81% 11191/13868 [2:20:38<33:17,  1.34it/s]Training epoch 1:  81% 11192/13868 [2:20:39<33:28,  1.33it/s]Training epoch 1:  81% 11193/13868 [2:20:39<32:55,  1.35it/s]Training epoch 1:  81% 11194/13868 [2:20:40<33:36,  1.33it/s]Training epoch 1:  81% 11195/13868 [2:20:41<33:53,  1.31it/s]Training epoch 1:  81% 11196/13868 [2:20:42<33:52,  1.31it/s]Training epoch 1:  81% 11197/13868 [2:20:43<33:24,  1.33it/s]Training epoch 1:  81% 11198/13868 [2:20:43<33:16,  1.34it/s]Training epoch 1:  81% 11199/13868 [2:20:44<33:01,  1.35it/s]Training epoch 1:  81% 11200/13868 [2:20:45<34:54,  1.27it/s]Training epoch 1:  81% 11201/13868 [2:20:46<34:25,  1.29it/s]Training epoch 1:  81% 11202/13868 [2:20:46<34:09,  1.30it/s]Training epoch 1:  81% 11203/13868 [2:20:47<33:50,  1.31it/s]Training epoch 1:  81% 11204/13868 [2:20:48<33:31,  1.32it/s]Training epoch 1:  81% 11205/13868 [2:20:49<33:28,  1.33it/s]Training epoch 1:  81% 11206/13868 [2:20:49<33:05,  1.34it/s]Training epoch 1:  81% 11207/13868 [2:20:50<33:17,  1.33it/s]Training epoch 1:  81% 11208/13868 [2:20:51<33:09,  1.34it/s]Training epoch 1:  81% 11209/13868 [2:20:52<33:27,  1.32it/s]Training epoch 1:  81% 11210/13868 [2:20:52<33:16,  1.33it/s]Training epoch 1:  81% 11211/13868 [2:20:53<33:11,  1.33it/s]Training epoch 1:  81% 11212/13868 [2:20:54<33:19,  1.33it/s]Training epoch 1:  81% 11213/13868 [2:20:55<33:35,  1.32it/s]Training epoch 1:  81% 11214/13868 [2:20:55<33:09,  1.33it/s]Training epoch 1:  81% 11215/13868 [2:20:56<33:04,  1.34it/s]Training epoch 1:  81% 11216/13868 [2:20:57<33:06,  1.33it/s]Training epoch 1:  81% 11217/13868 [2:20:58<32:51,  1.34it/s]Training epoch 1:  81% 11218/13868 [2:20:58<32:55,  1.34it/s]Training epoch 1:  81% 11219/13868 [2:20:59<33:18,  1.33it/s]Training epoch 1:  81% 11220/13868 [2:21:00<33:09,  1.33it/s]Training epoch 1:  81% 11221/13868 [2:21:01<33:19,  1.32it/s]Training epoch 1:  81% 11222/13868 [2:21:01<33:43,  1.31it/s]Training epoch 1:  81% 11223/13868 [2:21:02<33:20,  1.32it/s]Training epoch 1:  81% 11224/13868 [2:21:03<33:26,  1.32it/s]Training epoch 1:  81% 11225/13868 [2:21:04<33:25,  1.32it/s]Training epoch 1:  81% 11226/13868 [2:21:04<33:22,  1.32it/s]Training epoch 1:  81% 11227/13868 [2:21:05<33:21,  1.32it/s]Training epoch 1:  81% 11228/13868 [2:21:06<33:20,  1.32it/s]Training epoch 1:  81% 11229/13868 [2:21:07<32:49,  1.34it/s]Training epoch 1:  81% 11230/13868 [2:21:07<32:53,  1.34it/s]Training epoch 1:  81% 11231/13868 [2:21:08<32:56,  1.33it/s]Training epoch 1:  81% 11232/13868 [2:21:09<33:06,  1.33it/s]Training epoch 1:  81% 11233/13868 [2:21:10<33:08,  1.33it/s]Training epoch 1:  81% 11234/13868 [2:21:10<33:21,  1.32it/s]Training epoch 1:  81% 11235/13868 [2:21:11<32:53,  1.33it/s]Training epoch 1:  81% 11236/13868 [2:21:12<32:55,  1.33it/s]Training epoch 1:  81% 11237/13868 [2:21:13<32:55,  1.33it/s]Training epoch 1:  81% 11238/13868 [2:21:13<33:05,  1.32it/s]Training epoch 1:  81% 11239/13868 [2:21:14<33:06,  1.32it/s]Training epoch 1:  81% 11240/13868 [2:21:15<33:24,  1.31it/s]Training epoch 1:  81% 11241/13868 [2:21:16<33:10,  1.32it/s]Training epoch 1:  81% 11242/13868 [2:21:17<33:28,  1.31it/s]Training epoch 1:  81% 11243/13868 [2:21:17<33:21,  1.31it/s]Training epoch 1:  81% 11244/13868 [2:21:18<33:20,  1.31it/s]Training epoch 1:  81% 11245/13868 [2:21:19<33:04,  1.32it/s]Training epoch 1:  81% 11246/13868 [2:21:20<33:10,  1.32it/s]Training epoch 1:  81% 11247/13868 [2:21:20<32:53,  1.33it/s]Training epoch 1:  81% 11248/13868 [2:21:21<32:55,  1.33it/s]Training epoch 1:  81% 11249/13868 [2:21:22<33:05,  1.32it/s]Training epoch 1:  81% 11250/13868 [2:21:23<33:02,  1.32it/s]Training epoch 1:  81% 11251/13868 [2:21:23<32:35,  1.34it/s]Training epoch 1:  81% 11252/13868 [2:21:24<32:37,  1.34it/s]Training epoch 1:  81% 11253/13868 [2:21:25<32:45,  1.33it/s]Training epoch 1:  81% 11254/13868 [2:21:26<33:16,  1.31it/s]Training epoch 1:  81% 11255/13868 [2:21:26<32:47,  1.33it/s]Training epoch 1:  81% 11256/13868 [2:21:27<32:49,  1.33it/s]Training epoch 1:  81% 11257/13868 [2:21:28<32:29,  1.34it/s]Training epoch 1:  81% 11258/13868 [2:21:29<33:04,  1.32it/s]Training epoch 1:  81% 11259/13868 [2:21:29<32:17,  1.35it/s]Training epoch 1:  81% 11260/13868 [2:21:30<32:53,  1.32it/s]Training epoch 1:  81% 11261/13868 [2:21:31<32:55,  1.32it/s]Training epoch 1:  81% 11262/13868 [2:21:32<33:11,  1.31it/s]Training epoch 1:  81% 11263/13868 [2:21:32<32:52,  1.32it/s]Training epoch 1:  81% 11264/13868 [2:21:33<33:11,  1.31it/s]Training epoch 1:  81% 11265/13868 [2:21:34<33:19,  1.30it/s]Training epoch 1:  81% 11266/13868 [2:21:35<33:26,  1.30it/s]Training epoch 1:  81% 11267/13868 [2:21:35<33:02,  1.31it/s]Training epoch 1:  81% 11268/13868 [2:21:36<32:48,  1.32it/s]Training epoch 1:  81% 11269/13868 [2:21:37<32:56,  1.31it/s]Training epoch 1:  81% 11270/13868 [2:21:38<32:40,  1.33it/s]Training epoch 1:  81% 11271/13868 [2:21:39<32:53,  1.32it/s]Training epoch 1:  81% 11272/13868 [2:21:39<33:04,  1.31it/s]Training epoch 1:  81% 11273/13868 [2:21:40<32:45,  1.32it/s]Training epoch 1:  81% 11274/13868 [2:21:41<32:31,  1.33it/s]Training epoch 1:  81% 11275/13868 [2:21:42<32:50,  1.32it/s]Training epoch 1:  81% 11276/13868 [2:21:42<32:55,  1.31it/s]Training epoch 1:  81% 11277/13868 [2:21:43<32:37,  1.32it/s]Training epoch 1:  81% 11278/13868 [2:21:44<32:41,  1.32it/s]Training epoch 1:  81% 11279/13868 [2:21:45<32:46,  1.32it/s]Training epoch 1:  81% 11280/13868 [2:21:45<32:53,  1.31it/s]Training epoch 1:  81% 11281/13868 [2:21:46<32:28,  1.33it/s]Training epoch 1:  81% 11282/13868 [2:21:47<32:11,  1.34it/s]Training epoch 1:  81% 11283/13868 [2:21:48<32:20,  1.33it/s]Training epoch 1:  81% 11284/13868 [2:21:48<32:29,  1.33it/s]Training epoch 1:  81% 11285/13868 [2:21:49<31:59,  1.35it/s]Training epoch 1:  81% 11286/13868 [2:21:50<32:16,  1.33it/s]Training epoch 1:  81% 11287/13868 [2:21:51<32:40,  1.32it/s]Training epoch 1:  81% 11288/13868 [2:21:51<33:11,  1.30it/s]Training epoch 1:  81% 11289/13868 [2:21:52<32:56,  1.30it/s]Training epoch 1:  81% 11290/13868 [2:21:53<32:45,  1.31it/s]Training epoch 1:  81% 11291/13868 [2:21:54<32:38,  1.32it/s]Training epoch 1:  81% 11292/13868 [2:21:54<32:37,  1.32it/s]Training epoch 1:  81% 11293/13868 [2:21:55<32:14,  1.33it/s]Training epoch 1:  81% 11294/13868 [2:21:56<32:16,  1.33it/s]Training epoch 1:  81% 11295/13868 [2:21:57<32:13,  1.33it/s]Training epoch 1:  81% 11296/13868 [2:21:57<32:29,  1.32it/s]Training epoch 1:  81% 11297/13868 [2:21:58<32:20,  1.33it/s]Training epoch 1:  81% 11298/13868 [2:21:59<31:59,  1.34it/s]Training epoch 1:  81% 11299/13868 [2:22:00<32:30,  1.32it/s]Training epoch 1:  81% 11300/13868 [2:22:01<34:48,  1.23it/s]Training epoch 1:  81% 11301/13868 [2:22:01<33:47,  1.27it/s]Training epoch 1:  81% 11302/13868 [2:22:02<33:15,  1.29it/s]Training epoch 1:  82% 11303/13868 [2:22:03<32:56,  1.30it/s]Training epoch 1:  82% 11304/13868 [2:22:04<32:45,  1.30it/s]Training epoch 1:  82% 11305/13868 [2:22:04<32:25,  1.32it/s]Training epoch 1:  82% 11306/13868 [2:22:05<32:31,  1.31it/s]Training epoch 1:  82% 11307/13868 [2:22:06<32:01,  1.33it/s]Training epoch 1:  82% 11308/13868 [2:22:07<32:21,  1.32it/s]Training epoch 1:  82% 11309/13868 [2:22:07<32:05,  1.33it/s]Training epoch 1:  82% 11310/13868 [2:22:08<31:53,  1.34it/s]Training epoch 1:  82% 11311/13868 [2:22:09<32:07,  1.33it/s]Training epoch 1:  82% 11312/13868 [2:22:10<31:53,  1.34it/s]Training epoch 1:  82% 11313/13868 [2:22:10<31:39,  1.34it/s]Training epoch 1:  82% 11314/13868 [2:22:11<31:35,  1.35it/s]Training epoch 1:  82% 11315/13868 [2:22:12<31:42,  1.34it/s]Training epoch 1:  82% 11316/13868 [2:22:13<31:53,  1.33it/s]Training epoch 1:  82% 11317/13868 [2:22:13<32:04,  1.33it/s]Training epoch 1:  82% 11318/13868 [2:22:14<32:15,  1.32it/s]Training epoch 1:  82% 11319/13868 [2:22:15<32:33,  1.31it/s]Training epoch 1:  82% 11320/13868 [2:22:16<32:17,  1.32it/s]Training epoch 1:  82% 11321/13868 [2:22:16<31:41,  1.34it/s]Training epoch 1:  82% 11322/13868 [2:22:17<32:01,  1.33it/s]Training epoch 1:  82% 11323/13868 [2:22:18<31:17,  1.36it/s]Training epoch 1:  82% 11324/13868 [2:22:19<31:33,  1.34it/s]Training epoch 1:  82% 11325/13868 [2:22:19<31:59,  1.32it/s]Training epoch 1:  82% 11326/13868 [2:22:20<32:00,  1.32it/s]Training epoch 1:  82% 11327/13868 [2:22:21<31:54,  1.33it/s]Training epoch 1:  82% 11328/13868 [2:22:22<31:43,  1.33it/s]Training epoch 1:  82% 11329/13868 [2:22:22<31:32,  1.34it/s]Training epoch 1:  82% 11330/13868 [2:22:23<31:52,  1.33it/s]Training epoch 1:  82% 11331/13868 [2:22:24<32:05,  1.32it/s]Training epoch 1:  82% 11332/13868 [2:22:25<31:57,  1.32it/s]Training epoch 1:  82% 11333/13868 [2:22:25<31:36,  1.34it/s]Training epoch 1:  82% 11334/13868 [2:22:26<31:42,  1.33it/s]Training epoch 1:  82% 11335/13868 [2:22:27<31:10,  1.35it/s]Training epoch 1:  82% 11336/13868 [2:22:28<31:29,  1.34it/s]Training epoch 1:  82% 11337/13868 [2:22:28<31:40,  1.33it/s]Training epoch 1:  82% 11338/13868 [2:22:29<31:34,  1.34it/s]Training epoch 1:  82% 11339/13868 [2:22:30<31:49,  1.32it/s]Training epoch 1:  82% 11340/13868 [2:22:31<32:03,  1.31it/s]Training epoch 1:  82% 11341/13868 [2:22:31<31:29,  1.34it/s]Training epoch 1:  82% 11342/13868 [2:22:32<31:09,  1.35it/s]Training epoch 1:  82% 11343/13868 [2:22:33<31:02,  1.36it/s]Training epoch 1:  82% 11344/13868 [2:22:34<31:37,  1.33it/s]Training epoch 1:  82% 11345/13868 [2:22:34<31:43,  1.33it/s]Training epoch 1:  82% 11346/13868 [2:22:35<32:23,  1.30it/s]Training epoch 1:  82% 11347/13868 [2:22:36<31:50,  1.32it/s]Training epoch 1:  82% 11348/13868 [2:22:37<31:21,  1.34it/s]Training epoch 1:  82% 11349/13868 [2:22:37<31:20,  1.34it/s]Training epoch 1:  82% 11350/13868 [2:22:38<30:50,  1.36it/s]Training epoch 1:  82% 11351/13868 [2:22:39<30:53,  1.36it/s]Training epoch 1:  82% 11352/13868 [2:22:40<31:30,  1.33it/s]Training epoch 1:  82% 11353/13868 [2:22:40<31:14,  1.34it/s]Training epoch 1:  82% 11354/13868 [2:22:41<31:18,  1.34it/s]Training epoch 1:  82% 11355/13868 [2:22:42<31:20,  1.34it/s]Training epoch 1:  82% 11356/13868 [2:22:43<31:07,  1.35it/s]Training epoch 1:  82% 11357/13868 [2:22:43<31:47,  1.32it/s]Training epoch 1:  82% 11358/13868 [2:22:44<31:41,  1.32it/s]Training epoch 1:  82% 11359/13868 [2:22:45<31:46,  1.32it/s]Training epoch 1:  82% 11360/13868 [2:22:46<31:29,  1.33it/s]Training epoch 1:  82% 11361/13868 [2:22:46<31:25,  1.33it/s]Training epoch 1:  82% 11362/13868 [2:22:47<30:55,  1.35it/s]Training epoch 1:  82% 11363/13868 [2:22:48<30:43,  1.36it/s]Training epoch 1:  82% 11364/13868 [2:22:49<31:00,  1.35it/s]Training epoch 1:  82% 11365/13868 [2:22:49<30:38,  1.36it/s]Training epoch 1:  82% 11366/13868 [2:22:50<30:44,  1.36it/s]Training epoch 1:  82% 11367/13868 [2:22:51<31:04,  1.34it/s]Training epoch 1:  82% 11368/13868 [2:22:52<30:36,  1.36it/s]Training epoch 1:  82% 11369/13868 [2:22:52<30:50,  1.35it/s]Training epoch 1:  82% 11370/13868 [2:22:53<31:02,  1.34it/s]Training epoch 1:  82% 11371/13868 [2:22:54<30:44,  1.35it/s]Training epoch 1:  82% 11372/13868 [2:22:55<31:16,  1.33it/s]Training epoch 1:  82% 11373/13868 [2:22:55<31:16,  1.33it/s]Training epoch 1:  82% 11374/13868 [2:22:56<30:57,  1.34it/s]Training epoch 1:  82% 11375/13868 [2:22:57<31:00,  1.34it/s]Training epoch 1:  82% 11376/13868 [2:22:58<30:56,  1.34it/s]Training epoch 1:  82% 11377/13868 [2:22:58<31:13,  1.33it/s]Training epoch 1:  82% 11378/13868 [2:22:59<31:18,  1.33it/s]Training epoch 1:  82% 11379/13868 [2:23:00<31:15,  1.33it/s]Training epoch 1:  82% 11380/13868 [2:23:01<31:19,  1.32it/s]Training epoch 1:  82% 11381/13868 [2:23:01<31:33,  1.31it/s]Training epoch 1:  82% 11382/13868 [2:23:02<31:34,  1.31it/s]Training epoch 1:  82% 11383/13868 [2:23:03<31:11,  1.33it/s]Training epoch 1:  82% 11384/13868 [2:23:04<31:09,  1.33it/s]Training epoch 1:  82% 11385/13868 [2:23:04<31:17,  1.32it/s]Training epoch 1:  82% 11386/13868 [2:23:05<30:45,  1.34it/s]Training epoch 1:  82% 11387/13868 [2:23:06<31:03,  1.33it/s]Training epoch 1:  82% 11388/13868 [2:23:07<31:21,  1.32it/s]Training epoch 1:  82% 11389/13868 [2:23:07<31:27,  1.31it/s]Training epoch 1:  82% 11390/13868 [2:23:08<31:13,  1.32it/s]Training epoch 1:  82% 11391/13868 [2:23:09<31:06,  1.33it/s]Training epoch 1:  82% 11392/13868 [2:23:10<30:54,  1.33it/s]Training epoch 1:  82% 11393/13868 [2:23:10<31:20,  1.32it/s]Training epoch 1:  82% 11394/13868 [2:23:11<31:42,  1.30it/s]Training epoch 1:  82% 11395/13868 [2:23:12<31:58,  1.29it/s]Training epoch 1:  82% 11396/13868 [2:23:13<31:26,  1.31it/s]Training epoch 1:  82% 11397/13868 [2:23:13<31:31,  1.31it/s]Training epoch 1:  82% 11398/13868 [2:23:14<30:48,  1.34it/s]Training epoch 1:  82% 11399/13868 [2:23:15<30:51,  1.33it/s]Training epoch 1:  82% 11400/13868 [2:23:16<32:29,  1.27it/s]Training epoch 1:  82% 11401/13868 [2:23:17<31:52,  1.29it/s]Training epoch 1:  82% 11402/13868 [2:23:17<30:59,  1.33it/s]Training epoch 1:  82% 11403/13868 [2:23:18<31:17,  1.31it/s]Training epoch 1:  82% 11404/13868 [2:23:19<30:51,  1.33it/s]Training epoch 1:  82% 11405/13868 [2:23:20<30:56,  1.33it/s]Training epoch 1:  82% 11406/13868 [2:23:20<31:21,  1.31it/s]Training epoch 1:  82% 11407/13868 [2:23:21<31:25,  1.31it/s]Training epoch 1:  82% 11408/13868 [2:23:22<31:22,  1.31it/s]Training epoch 1:  82% 11409/13868 [2:23:23<31:10,  1.31it/s]Training epoch 1:  82% 11410/13868 [2:23:23<30:41,  1.33it/s]Training epoch 1:  82% 11411/13868 [2:23:24<30:44,  1.33it/s]Training epoch 1:  82% 11412/13868 [2:23:25<30:31,  1.34it/s]Training epoch 1:  82% 11413/13868 [2:23:26<30:18,  1.35it/s]Training epoch 1:  82% 11414/13868 [2:23:26<30:33,  1.34it/s]Training epoch 1:  82% 11415/13868 [2:23:27<30:56,  1.32it/s]Training epoch 1:  82% 11416/13868 [2:23:28<30:41,  1.33it/s]Training epoch 1:  82% 11417/13868 [2:23:29<30:40,  1.33it/s]Training epoch 1:  82% 11418/13868 [2:23:29<30:49,  1.32it/s]Training epoch 1:  82% 11419/13868 [2:23:30<30:50,  1.32it/s]Training epoch 1:  82% 11420/13868 [2:23:31<30:56,  1.32it/s]Training epoch 1:  82% 11421/13868 [2:23:32<31:06,  1.31it/s]Training epoch 1:  82% 11422/13868 [2:23:32<30:55,  1.32it/s]Training epoch 1:  82% 11423/13868 [2:23:33<30:50,  1.32it/s]Training epoch 1:  82% 11424/13868 [2:23:34<30:36,  1.33it/s]Training epoch 1:  82% 11425/13868 [2:23:35<30:42,  1.33it/s]Training epoch 1:  82% 11426/13868 [2:23:35<30:40,  1.33it/s]Training epoch 1:  82% 11427/13868 [2:23:36<31:02,  1.31it/s]Training epoch 1:  82% 11428/13868 [2:23:37<30:59,  1.31it/s]Training epoch 1:  82% 11429/13868 [2:23:38<30:53,  1.32it/s]Training epoch 1:  82% 11430/13868 [2:23:38<31:30,  1.29it/s]Training epoch 1:  82% 11431/13868 [2:23:39<30:36,  1.33it/s]Training epoch 1:  82% 11432/13868 [2:23:40<30:36,  1.33it/s]Training epoch 1:  82% 11433/13868 [2:23:41<30:15,  1.34it/s]Training epoch 1:  82% 11434/13868 [2:23:41<30:28,  1.33it/s]Training epoch 1:  82% 11435/13868 [2:23:42<30:28,  1.33it/s]Training epoch 1:  82% 11436/13868 [2:23:43<30:26,  1.33it/s]Training epoch 1:  82% 11437/13868 [2:23:44<30:21,  1.33it/s]Training epoch 1:  82% 11438/13868 [2:23:44<30:06,  1.34it/s]Training epoch 1:  82% 11439/13868 [2:23:45<30:05,  1.35it/s]Training epoch 1:  82% 11440/13868 [2:23:46<30:05,  1.34it/s]Training epoch 1:  82% 11441/13868 [2:23:47<30:05,  1.34it/s]Training epoch 1:  83% 11442/13868 [2:23:47<30:30,  1.33it/s]Training epoch 1:  83% 11443/13868 [2:23:48<30:47,  1.31it/s]Training epoch 1:  83% 11444/13868 [2:23:49<30:58,  1.30it/s]Training epoch 1:  83% 11445/13868 [2:23:50<30:42,  1.32it/s]Training epoch 1:  83% 11446/13868 [2:23:51<31:01,  1.30it/s]Training epoch 1:  83% 11447/13868 [2:23:51<30:15,  1.33it/s]Training epoch 1:  83% 11448/13868 [2:23:52<30:23,  1.33it/s]Training epoch 1:  83% 11449/13868 [2:23:53<30:48,  1.31it/s]Training epoch 1:  83% 11450/13868 [2:23:54<30:24,  1.33it/s]Training epoch 1:  83% 11451/13868 [2:23:54<30:11,  1.33it/s]Training epoch 1:  83% 11452/13868 [2:23:55<30:17,  1.33it/s]Training epoch 1:  83% 11453/13868 [2:23:56<29:52,  1.35it/s]Training epoch 1:  83% 11454/13868 [2:23:56<29:35,  1.36it/s]Training epoch 1:  83% 11455/13868 [2:23:57<29:37,  1.36it/s]Training epoch 1:  83% 11456/13868 [2:23:58<29:37,  1.36it/s]Training epoch 1:  83% 11457/13868 [2:23:59<29:34,  1.36it/s]Training epoch 1:  83% 11458/13868 [2:23:59<29:34,  1.36it/s]Training epoch 1:  83% 11459/13868 [2:24:00<29:59,  1.34it/s]Training epoch 1:  83% 11460/13868 [2:24:01<30:08,  1.33it/s]Training epoch 1:  83% 11461/13868 [2:24:02<30:20,  1.32it/s]Training epoch 1:  83% 11462/13868 [2:24:02<30:31,  1.31it/s]Training epoch 1:  83% 11463/13868 [2:24:03<30:22,  1.32it/s]Training epoch 1:  83% 11464/13868 [2:24:04<30:20,  1.32it/s]Training epoch 1:  83% 11465/13868 [2:24:05<30:05,  1.33it/s]Training epoch 1:  83% 11466/13868 [2:24:05<30:04,  1.33it/s]Training epoch 1:  83% 11467/13868 [2:24:06<30:08,  1.33it/s]Training epoch 1:  83% 11468/13868 [2:24:07<30:13,  1.32it/s]Training epoch 1:  83% 11469/13868 [2:24:08<30:16,  1.32it/s]Training epoch 1:  83% 11470/13868 [2:24:08<29:53,  1.34it/s]Training epoch 1:  83% 11471/13868 [2:24:09<29:38,  1.35it/s]Training epoch 1:  83% 11472/13868 [2:24:10<29:47,  1.34it/s]Training epoch 1:  83% 11473/13868 [2:24:11<30:15,  1.32it/s]Training epoch 1:  83% 11474/13868 [2:24:11<29:53,  1.33it/s]Training epoch 1:  83% 11475/13868 [2:24:12<29:39,  1.34it/s]Training epoch 1:  83% 11476/13868 [2:24:13<29:34,  1.35it/s]Training epoch 1:  83% 11477/13868 [2:24:14<29:26,  1.35it/s]Training epoch 1:  83% 11478/13868 [2:24:14<29:20,  1.36it/s]Training epoch 1:  83% 11479/13868 [2:24:15<29:22,  1.36it/s]Training epoch 1:  83% 11480/13868 [2:24:16<29:19,  1.36it/s]Training epoch 1:  83% 11481/13868 [2:24:17<29:51,  1.33it/s]Training epoch 1:  83% 11482/13868 [2:24:17<29:39,  1.34it/s]Training epoch 1:  83% 11483/13868 [2:24:18<29:13,  1.36it/s]Training epoch 1:  83% 11484/13868 [2:24:19<29:13,  1.36it/s]Training epoch 1:  83% 11485/13868 [2:24:20<29:28,  1.35it/s]Training epoch 1:  83% 11486/13868 [2:24:20<29:16,  1.36it/s]Training epoch 1:  83% 11487/13868 [2:24:21<29:31,  1.34it/s]Training epoch 1:  83% 11488/13868 [2:24:22<29:53,  1.33it/s]Training epoch 1:  83% 11489/13868 [2:24:23<29:49,  1.33it/s]Training epoch 1:  83% 11490/13868 [2:24:23<29:38,  1.34it/s]Training epoch 1:  83% 11491/13868 [2:24:24<30:00,  1.32it/s]Training epoch 1:  83% 11492/13868 [2:24:25<29:53,  1.32it/s]Training epoch 1:  83% 11493/13868 [2:24:26<29:46,  1.33it/s]Training epoch 1:  83% 11494/13868 [2:24:26<30:01,  1.32it/s]Training epoch 1:  83% 11495/13868 [2:24:27<29:37,  1.33it/s]Training epoch 1:  83% 11496/13868 [2:24:28<29:32,  1.34it/s]Training epoch 1:  83% 11497/13868 [2:24:29<30:00,  1.32it/s]Training epoch 1:  83% 11498/13868 [2:24:29<29:22,  1.34it/s]Training epoch 1:  83% 11499/13868 [2:24:30<29:30,  1.34it/s]Training epoch 1:  83% 11500/13868 [2:24:31<30:54,  1.28it/s]Training epoch 1:  83% 11501/13868 [2:24:32<30:35,  1.29it/s]Training epoch 1:  83% 11502/13868 [2:24:32<30:06,  1.31it/s]Training epoch 1:  83% 11503/13868 [2:24:33<30:26,  1.30it/s]Training epoch 1:  83% 11504/13868 [2:24:34<30:21,  1.30it/s]Training epoch 1:  83% 11505/13868 [2:24:35<30:14,  1.30it/s]Training epoch 1:  83% 11506/13868 [2:24:36<30:15,  1.30it/s]Training epoch 1:  83% 11507/13868 [2:24:36<29:48,  1.32it/s]Training epoch 1:  83% 11508/13868 [2:24:37<29:48,  1.32it/s]Training epoch 1:  83% 11509/13868 [2:24:38<29:53,  1.32it/s]Training epoch 1:  83% 11510/13868 [2:24:39<29:34,  1.33it/s]Training epoch 1:  83% 11511/13868 [2:24:39<29:16,  1.34it/s]Training epoch 1:  83% 11512/13868 [2:24:40<29:33,  1.33it/s]Training epoch 1:  83% 11513/13868 [2:24:41<29:38,  1.32it/s]Training epoch 1:  83% 11514/13868 [2:24:42<29:15,  1.34it/s]Training epoch 1:  83% 11515/13868 [2:24:42<29:26,  1.33it/s]Training epoch 1:  83% 11516/13868 [2:24:43<29:16,  1.34it/s]Training epoch 1:  83% 11517/13868 [2:24:44<29:09,  1.34it/s]Training epoch 1:  83% 11518/13868 [2:24:45<29:04,  1.35it/s]Training epoch 1:  83% 11519/13868 [2:24:45<28:59,  1.35it/s]Training epoch 1:  83% 11520/13868 [2:24:46<29:07,  1.34it/s]Training epoch 1:  83% 11521/13868 [2:24:47<29:18,  1.33it/s]Training epoch 1:  83% 11522/13868 [2:24:48<29:25,  1.33it/s]Training epoch 1:  83% 11523/13868 [2:24:48<29:45,  1.31it/s]Training epoch 1:  83% 11524/13868 [2:24:49<29:23,  1.33it/s]Training epoch 1:  83% 11525/13868 [2:24:50<28:40,  1.36it/s]Training epoch 1:  83% 11526/13868 [2:24:50<28:59,  1.35it/s]Training epoch 1:  83% 11527/13868 [2:24:51<29:09,  1.34it/s]Training epoch 1:  83% 11528/13868 [2:24:52<29:09,  1.34it/s]Training epoch 1:  83% 11529/13868 [2:24:53<29:50,  1.31it/s]Training epoch 1:  83% 11530/13868 [2:24:54<29:34,  1.32it/s]Training epoch 1:  83% 11531/13868 [2:24:54<29:22,  1.33it/s]Training epoch 1:  83% 11532/13868 [2:24:55<29:37,  1.31it/s]Training epoch 1:  83% 11533/13868 [2:24:56<29:21,  1.33it/s]Training epoch 1:  83% 11534/13868 [2:24:57<28:57,  1.34it/s]Training epoch 1:  83% 11535/13868 [2:24:57<28:55,  1.34it/s]Training epoch 1:  83% 11536/13868 [2:24:58<29:16,  1.33it/s]Training epoch 1:  83% 11537/13868 [2:24:59<28:57,  1.34it/s]Training epoch 1:  83% 11538/13868 [2:24:59<28:48,  1.35it/s]Training epoch 1:  83% 11539/13868 [2:25:00<28:55,  1.34it/s]Training epoch 1:  83% 11540/13868 [2:25:01<28:53,  1.34it/s]Training epoch 1:  83% 11541/13868 [2:25:02<28:53,  1.34it/s]Training epoch 1:  83% 11542/13868 [2:25:02<28:58,  1.34it/s]Training epoch 1:  83% 11543/13868 [2:25:03<29:03,  1.33it/s]Training epoch 1:  83% 11544/13868 [2:25:04<29:03,  1.33it/s]Training epoch 1:  83% 11545/13868 [2:25:05<29:05,  1.33it/s]Training epoch 1:  83% 11546/13868 [2:25:06<29:04,  1.33it/s]Training epoch 1:  83% 11547/13868 [2:25:06<29:14,  1.32it/s]Training epoch 1:  83% 11548/13868 [2:25:07<29:25,  1.31it/s]Training epoch 1:  83% 11549/13868 [2:25:08<29:08,  1.33it/s]Training epoch 1:  83% 11550/13868 [2:25:09<28:47,  1.34it/s]Training epoch 1:  83% 11551/13868 [2:25:09<28:39,  1.35it/s]Training epoch 1:  83% 11552/13868 [2:25:10<28:31,  1.35it/s]Training epoch 1:  83% 11553/13868 [2:25:11<28:46,  1.34it/s]Training epoch 1:  83% 11554/13868 [2:25:11<28:16,  1.36it/s]Training epoch 1:  83% 11555/13868 [2:25:12<28:55,  1.33it/s]Training epoch 1:  83% 11556/13868 [2:25:13<28:39,  1.34it/s]Training epoch 1:  83% 11557/13868 [2:25:14<28:58,  1.33it/s]Training epoch 1:  83% 11558/13868 [2:25:14<28:37,  1.34it/s]Training epoch 1:  83% 11559/13868 [2:25:15<28:38,  1.34it/s]Training epoch 1:  83% 11560/13868 [2:25:16<28:34,  1.35it/s]Training epoch 1:  83% 11561/13868 [2:25:17<28:41,  1.34it/s]Training epoch 1:  83% 11562/13868 [2:25:17<28:12,  1.36it/s]Training epoch 1:  83% 11563/13868 [2:25:18<28:11,  1.36it/s]Training epoch 1:  83% 11564/13868 [2:25:19<28:15,  1.36it/s]Training epoch 1:  83% 11565/13868 [2:25:20<28:00,  1.37it/s]Training epoch 1:  83% 11566/13868 [2:25:20<27:52,  1.38it/s]Training epoch 1:  83% 11567/13868 [2:25:21<28:28,  1.35it/s]Training epoch 1:  83% 11568/13868 [2:25:22<28:24,  1.35it/s]Training epoch 1:  83% 11569/13868 [2:25:23<28:32,  1.34it/s]Training epoch 1:  83% 11570/13868 [2:25:23<28:59,  1.32it/s]Training epoch 1:  83% 11571/13868 [2:25:24<28:43,  1.33it/s]Training epoch 1:  83% 11572/13868 [2:25:25<28:35,  1.34it/s]Training epoch 1:  83% 11573/13868 [2:25:26<28:41,  1.33it/s]Training epoch 1:  83% 11574/13868 [2:25:26<28:43,  1.33it/s]Training epoch 1:  83% 11575/13868 [2:25:27<28:51,  1.32it/s]Training epoch 1:  83% 11576/13868 [2:25:28<28:41,  1.33it/s]Training epoch 1:  83% 11577/13868 [2:25:29<28:42,  1.33it/s]Training epoch 1:  83% 11578/13868 [2:25:29<28:55,  1.32it/s]Training epoch 1:  83% 11579/13868 [2:25:30<28:52,  1.32it/s]Training epoch 1:  84% 11580/13868 [2:25:31<28:21,  1.34it/s]Training epoch 1:  84% 11581/13868 [2:25:32<28:12,  1.35it/s]Training epoch 1:  84% 11582/13868 [2:25:32<28:37,  1.33it/s]Training epoch 1:  84% 11583/13868 [2:25:33<28:37,  1.33it/s]Training epoch 1:  84% 11584/13868 [2:25:34<28:33,  1.33it/s]Training epoch 1:  84% 11585/13868 [2:25:35<28:50,  1.32it/s]Training epoch 1:  84% 11586/13868 [2:25:35<28:29,  1.34it/s]Training epoch 1:  84% 11587/13868 [2:25:36<28:38,  1.33it/s]Training epoch 1:  84% 11588/13868 [2:25:37<28:35,  1.33it/s]Training epoch 1:  84% 11589/13868 [2:25:38<28:27,  1.33it/s]Training epoch 1:  84% 11590/13868 [2:25:38<28:20,  1.34it/s]Training epoch 1:  84% 11591/13868 [2:25:39<28:17,  1.34it/s]Training epoch 1:  84% 11592/13868 [2:25:40<28:38,  1.32it/s]Training epoch 1:  84% 11593/13868 [2:25:41<28:17,  1.34it/s]Training epoch 1:  84% 11594/13868 [2:25:41<28:11,  1.34it/s]Training epoch 1:  84% 11595/13868 [2:25:42<28:08,  1.35it/s]Training epoch 1:  84% 11596/13868 [2:25:43<28:03,  1.35it/s]Training epoch 1:  84% 11597/13868 [2:25:44<28:48,  1.31it/s]Training epoch 1:  84% 11598/13868 [2:25:44<28:13,  1.34it/s]Training epoch 1:  84% 11599/13868 [2:25:45<28:09,  1.34it/s]Training epoch 1:  84% 11600/13868 [2:25:46<29:52,  1.27it/s]Training epoch 1:  84% 11601/13868 [2:25:47<29:09,  1.30it/s]Training epoch 1:  84% 11602/13868 [2:25:47<28:30,  1.32it/s]Training epoch 1:  84% 11603/13868 [2:25:48<28:06,  1.34it/s]Training epoch 1:  84% 11604/13868 [2:25:49<28:24,  1.33it/s]Training epoch 1:  84% 11605/13868 [2:25:50<28:18,  1.33it/s]Training epoch 1:  84% 11606/13868 [2:25:50<28:26,  1.33it/s]Training epoch 1:  84% 11607/13868 [2:25:51<28:14,  1.33it/s]Training epoch 1:  84% 11608/13868 [2:25:52<28:16,  1.33it/s]Training epoch 1:  84% 11609/13868 [2:25:53<28:25,  1.32it/s]Training epoch 1:  84% 11610/13868 [2:25:53<28:41,  1.31it/s]Training epoch 1:  84% 11611/13868 [2:25:54<28:53,  1.30it/s]Training epoch 1:  84% 11612/13868 [2:25:55<28:58,  1.30it/s]Training epoch 1:  84% 11613/13868 [2:25:56<28:21,  1.32it/s]Training epoch 1:  84% 11614/13868 [2:25:56<28:28,  1.32it/s]Training epoch 1:  84% 11615/13868 [2:25:57<28:50,  1.30it/s]Training epoch 1:  84% 11616/13868 [2:25:58<28:36,  1.31it/s]Training epoch 1:  84% 11617/13868 [2:25:59<28:27,  1.32it/s]Training epoch 1:  84% 11618/13868 [2:26:00<28:49,  1.30it/s]Training epoch 1:  84% 11619/13868 [2:26:00<28:44,  1.30it/s]Training epoch 1:  84% 11620/13868 [2:26:01<28:45,  1.30it/s]Training epoch 1:  84% 11621/13868 [2:26:02<28:59,  1.29it/s]Training epoch 1:  84% 11622/13868 [2:26:03<28:36,  1.31it/s]Training epoch 1:  84% 11623/13868 [2:26:03<28:43,  1.30it/s]Training epoch 1:  84% 11624/13868 [2:26:04<29:05,  1.29it/s]Training epoch 1:  84% 11625/13868 [2:26:05<28:50,  1.30it/s]Training epoch 1:  84% 11626/13868 [2:26:06<28:50,  1.30it/s]Training epoch 1:  84% 11627/13868 [2:26:07<28:51,  1.29it/s]Training epoch 1:  84% 11628/13868 [2:26:07<28:46,  1.30it/s]Training epoch 1:  84% 11629/13868 [2:26:08<28:45,  1.30it/s]Training epoch 1:  84% 11630/13868 [2:26:09<29:10,  1.28it/s]Training epoch 1:  84% 11631/13868 [2:26:10<29:00,  1.28it/s]Training epoch 1:  84% 11632/13868 [2:26:10<28:37,  1.30it/s]Training epoch 1:  84% 11633/13868 [2:26:11<28:11,  1.32it/s]Training epoch 1:  84% 11634/13868 [2:26:12<28:29,  1.31it/s]Training epoch 1:  84% 11635/13868 [2:26:13<28:10,  1.32it/s]Training epoch 1:  84% 11636/13868 [2:26:13<28:30,  1.31it/s]Training epoch 1:  84% 11637/13868 [2:26:14<28:00,  1.33it/s]Training epoch 1:  84% 11638/13868 [2:26:15<28:12,  1.32it/s]Training epoch 1:  84% 11639/13868 [2:26:16<27:59,  1.33it/s]Training epoch 1:  84% 11640/13868 [2:26:16<27:44,  1.34it/s]Training epoch 1:  84% 11641/13868 [2:26:17<27:47,  1.34it/s]Training epoch 1:  84% 11642/13868 [2:26:18<27:26,  1.35it/s]Training epoch 1:  84% 11643/13868 [2:26:19<27:11,  1.36it/s]Training epoch 1:  84% 11644/13868 [2:26:19<27:09,  1.36it/s]Training epoch 1:  84% 11645/13868 [2:26:20<27:35,  1.34it/s]Training epoch 1:  84% 11646/13868 [2:26:21<27:19,  1.36it/s]Training epoch 1:  84% 11647/13868 [2:26:22<27:10,  1.36it/s]Training epoch 1:  84% 11648/13868 [2:26:22<27:21,  1.35it/s]Training epoch 1:  84% 11649/13868 [2:26:23<27:15,  1.36it/s]Training epoch 1:  84% 11650/13868 [2:26:24<27:29,  1.34it/s]Training epoch 1:  84% 11651/13868 [2:26:25<27:18,  1.35it/s]Training epoch 1:  84% 11652/13868 [2:26:25<27:38,  1.34it/s]Training epoch 1:  84% 11653/13868 [2:26:26<27:42,  1.33it/s]Training epoch 1:  84% 11654/13868 [2:26:27<28:15,  1.31it/s]Training epoch 1:  84% 11655/13868 [2:26:28<27:37,  1.33it/s]Training epoch 1:  84% 11656/13868 [2:26:28<27:40,  1.33it/s]Training epoch 1:  84% 11657/13868 [2:26:29<27:33,  1.34it/s]Training epoch 1:  84% 11658/13868 [2:26:30<27:42,  1.33it/s]Training epoch 1:  84% 11659/13868 [2:26:31<27:55,  1.32it/s]Training epoch 1:  84% 11660/13868 [2:26:31<28:21,  1.30it/s]Training epoch 1:  84% 11661/13868 [2:26:32<27:53,  1.32it/s]Training epoch 1:  84% 11662/13868 [2:26:33<27:46,  1.32it/s]Training epoch 1:  84% 11663/13868 [2:26:34<28:06,  1.31it/s]Training epoch 1:  84% 11664/13868 [2:26:34<27:39,  1.33it/s]Training epoch 1:  84% 11665/13868 [2:26:35<27:28,  1.34it/s]Training epoch 1:  84% 11666/13868 [2:26:36<27:46,  1.32it/s]Training epoch 1:  84% 11667/13868 [2:26:37<27:31,  1.33it/s]Training epoch 1:  84% 11668/13868 [2:26:37<27:31,  1.33it/s]Training epoch 1:  84% 11669/13868 [2:26:38<27:42,  1.32it/s]Training epoch 1:  84% 11670/13868 [2:26:39<28:07,  1.30it/s]Training epoch 1:  84% 11671/13868 [2:26:40<27:58,  1.31it/s]Training epoch 1:  84% 11672/13868 [2:26:40<28:00,  1.31it/s]Training epoch 1:  84% 11673/13868 [2:26:41<27:45,  1.32it/s]Training epoch 1:  84% 11674/13868 [2:26:42<27:23,  1.34it/s]Training epoch 1:  84% 11675/13868 [2:26:43<27:44,  1.32it/s]Training epoch 1:  84% 11676/13868 [2:26:43<27:46,  1.32it/s]Training epoch 1:  84% 11677/13868 [2:26:44<27:45,  1.32it/s]Training epoch 1:  84% 11678/13868 [2:26:45<27:56,  1.31it/s]Training epoch 1:  84% 11679/13868 [2:26:46<28:02,  1.30it/s]Training epoch 1:  84% 11680/13868 [2:26:47<27:53,  1.31it/s]Training epoch 1:  84% 11681/13868 [2:26:47<28:00,  1.30it/s]Training epoch 1:  84% 11682/13868 [2:26:48<27:46,  1.31it/s]Training epoch 1:  84% 11683/13868 [2:26:49<27:40,  1.32it/s]Training epoch 1:  84% 11684/13868 [2:26:50<27:59,  1.30it/s]Training epoch 1:  84% 11685/13868 [2:26:50<27:57,  1.30it/s]Training epoch 1:  84% 11686/13868 [2:26:51<28:06,  1.29it/s]Training epoch 1:  84% 11687/13868 [2:26:52<27:43,  1.31it/s]Training epoch 1:  84% 11688/13868 [2:26:53<28:09,  1.29it/s]Training epoch 1:  84% 11689/13868 [2:26:53<27:32,  1.32it/s]Training epoch 1:  84% 11690/13868 [2:26:54<27:38,  1.31it/s]Training epoch 1:  84% 11691/13868 [2:26:55<27:47,  1.31it/s]Training epoch 1:  84% 11692/13868 [2:26:56<27:59,  1.30it/s]Training epoch 1:  84% 11693/13868 [2:26:57<27:50,  1.30it/s]Training epoch 1:  84% 11694/13868 [2:26:57<27:47,  1.30it/s]Training epoch 1:  84% 11695/13868 [2:26:58<27:28,  1.32it/s]Training epoch 1:  84% 11696/13868 [2:26:59<27:39,  1.31it/s]Training epoch 1:  84% 11697/13868 [2:27:00<27:28,  1.32it/s]Training epoch 1:  84% 11698/13868 [2:27:00<27:19,  1.32it/s]Training epoch 1:  84% 11699/13868 [2:27:01<27:18,  1.32it/s]Training epoch 1:  84% 11700/13868 [2:27:02<28:32,  1.27it/s]Training epoch 1:  84% 11701/13868 [2:27:03<28:20,  1.27it/s]Training epoch 1:  84% 11702/13868 [2:27:03<28:19,  1.27it/s]Training epoch 1:  84% 11703/13868 [2:27:04<27:40,  1.30it/s]Training epoch 1:  84% 11704/13868 [2:27:05<27:31,  1.31it/s]Training epoch 1:  84% 11705/13868 [2:27:06<27:23,  1.32it/s]Training epoch 1:  84% 11706/13868 [2:27:06<27:15,  1.32it/s]Training epoch 1:  84% 11707/13868 [2:27:07<27:00,  1.33it/s]Training epoch 1:  84% 11708/13868 [2:27:08<27:17,  1.32it/s]Training epoch 1:  84% 11709/13868 [2:27:09<26:50,  1.34it/s]Training epoch 1:  84% 11710/13868 [2:27:09<26:54,  1.34it/s]Training epoch 1:  84% 11711/13868 [2:27:10<26:44,  1.34it/s]Training epoch 1:  84% 11712/13868 [2:27:11<27:13,  1.32it/s]Training epoch 1:  84% 11713/13868 [2:27:12<27:01,  1.33it/s]Training epoch 1:  84% 11714/13868 [2:27:12<26:29,  1.36it/s]Training epoch 1:  84% 11715/13868 [2:27:13<26:44,  1.34it/s]Training epoch 1:  84% 11716/13868 [2:27:14<26:57,  1.33it/s]Training epoch 1:  84% 11717/13868 [2:27:15<26:43,  1.34it/s]Training epoch 1:  84% 11718/13868 [2:27:15<27:07,  1.32it/s]Training epoch 1:  85% 11719/13868 [2:27:16<26:45,  1.34it/s]Training epoch 1:  85% 11720/13868 [2:27:17<26:28,  1.35it/s]Training epoch 1:  85% 11721/13868 [2:27:18<26:32,  1.35it/s]Training epoch 1:  85% 11722/13868 [2:27:18<26:51,  1.33it/s]Training epoch 1:  85% 11723/13868 [2:27:19<26:51,  1.33it/s]Training epoch 1:  85% 11724/13868 [2:27:20<26:44,  1.34it/s]Training epoch 1:  85% 11725/13868 [2:27:21<26:46,  1.33it/s]Training epoch 1:  85% 11726/13868 [2:27:21<26:41,  1.34it/s]Training epoch 1:  85% 11727/13868 [2:27:22<26:54,  1.33it/s]Training epoch 1:  85% 11728/13868 [2:27:23<26:23,  1.35it/s]Training epoch 1:  85% 11729/13868 [2:27:24<26:29,  1.35it/s]Training epoch 1:  85% 11730/13868 [2:27:24<26:50,  1.33it/s]Training epoch 1:  85% 11731/13868 [2:27:25<26:35,  1.34it/s]Training epoch 1:  85% 11732/13868 [2:27:26<26:27,  1.35it/s]Training epoch 1:  85% 11733/13868 [2:27:27<25:55,  1.37it/s]Training epoch 1:  85% 11734/13868 [2:27:27<26:19,  1.35it/s]Training epoch 1:  85% 11735/13868 [2:27:28<26:25,  1.35it/s]Training epoch 1:  85% 11736/13868 [2:27:29<26:20,  1.35it/s]Training epoch 1:  85% 11737/13868 [2:27:30<26:31,  1.34it/s]Training epoch 1:  85% 11738/13868 [2:27:30<26:57,  1.32it/s]Training epoch 1:  85% 11739/13868 [2:27:31<26:43,  1.33it/s]Training epoch 1:  85% 11740/13868 [2:27:32<26:44,  1.33it/s]Training epoch 1:  85% 11741/13868 [2:27:33<27:02,  1.31it/s]Training epoch 1:  85% 11742/13868 [2:27:33<27:30,  1.29it/s]Training epoch 1:  85% 11743/13868 [2:27:34<27:07,  1.31it/s]Training epoch 1:  85% 11744/13868 [2:27:35<26:48,  1.32it/s]Training epoch 1:  85% 11745/13868 [2:27:36<26:39,  1.33it/s]Training epoch 1:  85% 11746/13868 [2:27:36<26:38,  1.33it/s]Training epoch 1:  85% 11747/13868 [2:27:37<27:02,  1.31it/s]Training epoch 1:  85% 11748/13868 [2:27:38<26:58,  1.31it/s]Training epoch 1:  85% 11749/13868 [2:27:39<26:34,  1.33it/s]Training epoch 1:  85% 11750/13868 [2:27:39<26:20,  1.34it/s]Training epoch 1:  85% 11751/13868 [2:27:40<26:17,  1.34it/s]Training epoch 1:  85% 11752/13868 [2:27:41<26:13,  1.34it/s]Training epoch 1:  85% 11753/13868 [2:27:42<26:04,  1.35it/s]Training epoch 1:  85% 11754/13868 [2:27:42<26:24,  1.33it/s]Training epoch 1:  85% 11755/13868 [2:27:43<26:26,  1.33it/s]Training epoch 1:  85% 11756/13868 [2:27:44<26:19,  1.34it/s]Training epoch 1:  85% 11757/13868 [2:27:45<26:30,  1.33it/s]Training epoch 1:  85% 11758/13868 [2:27:45<26:21,  1.33it/s]Training epoch 1:  85% 11759/13868 [2:27:46<26:13,  1.34it/s]Training epoch 1:  85% 11760/13868 [2:27:47<26:37,  1.32it/s]Training epoch 1:  85% 11761/13868 [2:27:48<26:43,  1.31it/s]Training epoch 1:  85% 11762/13868 [2:27:48<26:31,  1.32it/s]Training epoch 1:  85% 11763/13868 [2:27:49<26:35,  1.32it/s]Training epoch 1:  85% 11764/13868 [2:27:50<26:18,  1.33it/s]Training epoch 1:  85% 11765/13868 [2:27:51<26:11,  1.34it/s]Training epoch 1:  85% 11766/13868 [2:27:51<26:11,  1.34it/s]Training epoch 1:  85% 11767/13868 [2:27:52<25:39,  1.36it/s]Training epoch 1:  85% 11768/13868 [2:27:53<25:58,  1.35it/s]Training epoch 1:  85% 11769/13868 [2:27:54<26:14,  1.33it/s]Training epoch 1:  85% 11770/13868 [2:27:54<26:15,  1.33it/s]Training epoch 1:  85% 11771/13868 [2:27:55<26:20,  1.33it/s]Training epoch 1:  85% 11772/13868 [2:27:56<26:41,  1.31it/s]Training epoch 1:  85% 11773/13868 [2:27:57<26:18,  1.33it/s]Training epoch 1:  85% 11774/13868 [2:27:57<26:11,  1.33it/s]Training epoch 1:  85% 11775/13868 [2:27:58<26:28,  1.32it/s]Training epoch 1:  85% 11776/13868 [2:27:59<26:32,  1.31it/s]Training epoch 1:  85% 11777/13868 [2:28:00<26:26,  1.32it/s]Training epoch 1:  85% 11778/13868 [2:28:01<26:45,  1.30it/s]Training epoch 1:  85% 11779/13868 [2:28:01<26:24,  1.32it/s]Training epoch 1:  85% 11780/13868 [2:28:02<26:08,  1.33it/s]Training epoch 1:  85% 11781/13868 [2:28:03<25:53,  1.34it/s]Training epoch 1:  85% 11782/13868 [2:28:03<25:57,  1.34it/s]Training epoch 1:  85% 11783/13868 [2:28:04<25:44,  1.35it/s]Training epoch 1:  85% 11784/13868 [2:28:05<26:00,  1.34it/s]Training epoch 1:  85% 11785/13868 [2:28:06<25:58,  1.34it/s]Training epoch 1:  85% 11786/13868 [2:28:06<25:50,  1.34it/s]Training epoch 1:  85% 11787/13868 [2:28:07<25:59,  1.33it/s]Training epoch 1:  85% 11788/13868 [2:28:08<26:10,  1.32it/s]Training epoch 1:  85% 11789/13868 [2:28:09<26:07,  1.33it/s]Training epoch 1:  85% 11790/13868 [2:28:10<26:18,  1.32it/s]Training epoch 1:  85% 11791/13868 [2:28:10<26:03,  1.33it/s]Training epoch 1:  85% 11792/13868 [2:28:11<26:08,  1.32it/s]Training epoch 1:  85% 11793/13868 [2:28:12<25:45,  1.34it/s]Training epoch 1:  85% 11794/13868 [2:28:12<25:56,  1.33it/s]Training epoch 1:  85% 11795/13868 [2:28:13<25:25,  1.36it/s]Training epoch 1:  85% 11796/13868 [2:28:14<25:17,  1.36it/s]Training epoch 1:  85% 11797/13868 [2:28:15<25:30,  1.35it/s]Training epoch 1:  85% 11798/13868 [2:28:15<25:57,  1.33it/s]Training epoch 1:  85% 11799/13868 [2:28:16<25:54,  1.33it/s]Training epoch 1:  85% 11800/13868 [2:28:17<27:31,  1.25it/s]Training epoch 1:  85% 11801/13868 [2:28:18<26:57,  1.28it/s]Training epoch 1:  85% 11802/13868 [2:28:19<26:42,  1.29it/s]Training epoch 1:  85% 11803/13868 [2:28:19<26:22,  1.30it/s]Training epoch 1:  85% 11804/13868 [2:28:20<25:54,  1.33it/s]Training epoch 1:  85% 11805/13868 [2:28:21<25:56,  1.33it/s]Training epoch 1:  85% 11806/13868 [2:28:22<25:57,  1.32it/s]Training epoch 1:  85% 11807/13868 [2:28:22<25:57,  1.32it/s]Training epoch 1:  85% 11808/13868 [2:28:23<26:14,  1.31it/s]Training epoch 1:  85% 11809/13868 [2:28:24<26:03,  1.32it/s]Training epoch 1:  85% 11810/13868 [2:28:25<26:12,  1.31it/s]Training epoch 1:  85% 11811/13868 [2:28:25<26:12,  1.31it/s]Training epoch 1:  85% 11812/13868 [2:28:26<26:11,  1.31it/s]Training epoch 1:  85% 11813/13868 [2:28:27<25:45,  1.33it/s]Training epoch 1:  85% 11814/13868 [2:28:28<25:47,  1.33it/s]Training epoch 1:  85% 11815/13868 [2:28:28<25:57,  1.32it/s]Training epoch 1:  85% 11816/13868 [2:28:29<25:43,  1.33it/s]Training epoch 1:  85% 11817/13868 [2:28:30<25:56,  1.32it/s]Training epoch 1:  85% 11818/13868 [2:28:31<25:56,  1.32it/s]Training epoch 1:  85% 11819/13868 [2:28:31<25:28,  1.34it/s]Training epoch 1:  85% 11820/13868 [2:28:32<25:57,  1.31it/s]Training epoch 1:  85% 11821/13868 [2:28:33<26:10,  1.30it/s]Training epoch 1:  85% 11822/13868 [2:28:34<26:02,  1.31it/s]Training epoch 1:  85% 11823/13868 [2:28:35<26:09,  1.30it/s]Training epoch 1:  85% 11824/13868 [2:28:35<26:02,  1.31it/s]Training epoch 1:  85% 11825/13868 [2:28:36<25:29,  1.34it/s]Training epoch 1:  85% 11826/13868 [2:28:37<25:23,  1.34it/s]Training epoch 1:  85% 11827/13868 [2:28:38<25:30,  1.33it/s]Training epoch 1:  85% 11828/13868 [2:28:38<25:26,  1.34it/s]Training epoch 1:  85% 11829/13868 [2:28:39<25:18,  1.34it/s]Training epoch 1:  85% 11830/13868 [2:28:40<25:15,  1.34it/s]Training epoch 1:  85% 11831/13868 [2:28:41<25:44,  1.32it/s]Training epoch 1:  85% 11832/13868 [2:28:41<25:47,  1.32it/s]Training epoch 1:  85% 11833/13868 [2:28:42<26:09,  1.30it/s]Training epoch 1:  85% 11834/13868 [2:28:43<25:44,  1.32it/s]Training epoch 1:  85% 11835/13868 [2:28:44<25:47,  1.31it/s]Training epoch 1:  85% 11836/13868 [2:28:44<25:31,  1.33it/s]Training epoch 1:  85% 11837/13868 [2:28:45<25:18,  1.34it/s]Training epoch 1:  85% 11838/13868 [2:28:46<25:25,  1.33it/s]Training epoch 1:  85% 11839/13868 [2:28:47<25:48,  1.31it/s]Training epoch 1:  85% 11840/13868 [2:28:47<25:48,  1.31it/s]Training epoch 1:  85% 11841/13868 [2:28:48<25:47,  1.31it/s]Training epoch 1:  85% 11842/13868 [2:28:49<25:44,  1.31it/s]Training epoch 1:  85% 11843/13868 [2:28:50<25:31,  1.32it/s]Training epoch 1:  85% 11844/13868 [2:28:50<25:38,  1.32it/s]Training epoch 1:  85% 11845/13868 [2:28:51<25:46,  1.31it/s]Training epoch 1:  85% 11846/13868 [2:28:52<25:47,  1.31it/s]Training epoch 1:  85% 11847/13868 [2:28:53<25:50,  1.30it/s]Training epoch 1:  85% 11848/13868 [2:28:53<25:44,  1.31it/s]Training epoch 1:  85% 11849/13868 [2:28:54<25:21,  1.33it/s]Training epoch 1:  85% 11850/13868 [2:28:55<25:01,  1.34it/s]Training epoch 1:  85% 11851/13868 [2:28:56<24:55,  1.35it/s]Training epoch 1:  85% 11852/13868 [2:28:56<25:16,  1.33it/s]Training epoch 1:  85% 11853/13868 [2:28:57<25:20,  1.33it/s]Training epoch 1:  85% 11854/13868 [2:28:58<25:32,  1.31it/s]Training epoch 1:  85% 11855/13868 [2:28:59<25:15,  1.33it/s]Training epoch 1:  85% 11856/13868 [2:28:59<25:11,  1.33it/s]Training epoch 1:  85% 11857/13868 [2:29:00<25:18,  1.32it/s]Training epoch 1:  86% 11858/13868 [2:29:01<25:08,  1.33it/s]Training epoch 1:  86% 11859/13868 [2:29:02<25:13,  1.33it/s]Training epoch 1:  86% 11860/13868 [2:29:02<25:28,  1.31it/s]Training epoch 1:  86% 11861/13868 [2:29:03<24:58,  1.34it/s]Training epoch 1:  86% 11862/13868 [2:29:04<24:25,  1.37it/s]Training epoch 1:  86% 11863/13868 [2:29:05<24:22,  1.37it/s]Training epoch 1:  86% 11864/13868 [2:29:05<24:21,  1.37it/s]Training epoch 1:  86% 11865/13868 [2:29:06<24:26,  1.37it/s]Training epoch 1:  86% 11866/13868 [2:29:07<24:24,  1.37it/s]Training epoch 1:  86% 11867/13868 [2:29:08<24:43,  1.35it/s]Training epoch 1:  86% 11868/13868 [2:29:08<24:48,  1.34it/s]Training epoch 1:  86% 11869/13868 [2:29:09<25:03,  1.33it/s]Training epoch 1:  86% 11870/13868 [2:29:10<25:00,  1.33it/s]Training epoch 1:  86% 11871/13868 [2:29:11<25:17,  1.32it/s]Training epoch 1:  86% 11872/13868 [2:29:11<25:04,  1.33it/s]Training epoch 1:  86% 11873/13868 [2:29:12<25:04,  1.33it/s]Training epoch 1:  86% 11874/13868 [2:29:13<24:48,  1.34it/s]Training epoch 1:  86% 11875/13868 [2:29:14<24:39,  1.35it/s]Training epoch 1:  86% 11876/13868 [2:29:14<24:10,  1.37it/s]Training epoch 1:  86% 11877/13868 [2:29:15<24:29,  1.35it/s]Training epoch 1:  86% 11878/13868 [2:29:16<24:31,  1.35it/s]Training epoch 1:  86% 11879/13868 [2:29:17<24:35,  1.35it/s]Training epoch 1:  86% 11880/13868 [2:29:17<24:45,  1.34it/s]Training epoch 1:  86% 11881/13868 [2:29:18<25:07,  1.32it/s]Training epoch 1:  86% 11882/13868 [2:29:19<25:09,  1.32it/s]Training epoch 1:  86% 11883/13868 [2:29:20<25:14,  1.31it/s]Training epoch 1:  86% 11884/13868 [2:29:20<25:08,  1.32it/s]Training epoch 1:  86% 11885/13868 [2:29:21<24:55,  1.33it/s]Training epoch 1:  86% 11886/13868 [2:29:22<24:53,  1.33it/s]Training epoch 1:  86% 11887/13868 [2:29:23<24:50,  1.33it/s]Training epoch 1:  86% 11888/13868 [2:29:23<24:37,  1.34it/s]Training epoch 1:  86% 11889/13868 [2:29:24<24:53,  1.32it/s]Training epoch 1:  86% 11890/13868 [2:29:25<24:37,  1.34it/s]Training epoch 1:  86% 11891/13868 [2:29:26<24:55,  1.32it/s]Training epoch 1:  86% 11892/13868 [2:29:26<25:06,  1.31it/s]Training epoch 1:  86% 11893/13868 [2:29:27<24:58,  1.32it/s]Training epoch 1:  86% 11894/13868 [2:29:28<24:42,  1.33it/s]Training epoch 1:  86% 11895/13868 [2:29:29<24:54,  1.32it/s]Training epoch 1:  86% 11896/13868 [2:29:29<24:39,  1.33it/s]Training epoch 1:  86% 11897/13868 [2:29:30<24:45,  1.33it/s]Training epoch 1:  86% 11898/13868 [2:29:31<24:38,  1.33it/s]Training epoch 1:  86% 11899/13868 [2:29:32<24:34,  1.34it/s]Training epoch 1:  86% 11900/13868 [2:29:33<25:42,  1.28it/s]Training epoch 1:  86% 11901/13868 [2:29:33<25:23,  1.29it/s]Training epoch 1:  86% 11902/13868 [2:29:34<25:15,  1.30it/s]Training epoch 1:  86% 11903/13868 [2:29:35<24:44,  1.32it/s]Training epoch 1:  86% 11904/13868 [2:29:35<24:26,  1.34it/s]Training epoch 1:  86% 11905/13868 [2:29:36<24:24,  1.34it/s]Training epoch 1:  86% 11906/13868 [2:29:37<24:06,  1.36it/s]Training epoch 1:  86% 11907/13868 [2:29:38<24:44,  1.32it/s]Training epoch 1:  86% 11908/13868 [2:29:38<24:35,  1.33it/s]Training epoch 1:  86% 11909/13868 [2:29:39<24:27,  1.34it/s]Training epoch 1:  86% 11910/13868 [2:29:40<24:30,  1.33it/s]Training epoch 1:  86% 11911/13868 [2:29:41<24:31,  1.33it/s]Training epoch 1:  86% 11912/13868 [2:29:41<24:27,  1.33it/s]Training epoch 1:  86% 11913/13868 [2:29:42<24:30,  1.33it/s]Training epoch 1:  86% 11914/13868 [2:29:43<24:29,  1.33it/s]Training epoch 1:  86% 11915/13868 [2:29:44<24:25,  1.33it/s]Training epoch 1:  86% 11916/13868 [2:29:45<24:36,  1.32it/s]Training epoch 1:  86% 11917/13868 [2:29:45<24:30,  1.33it/s]Training epoch 1:  86% 11918/13868 [2:29:46<24:27,  1.33it/s]Training epoch 1:  86% 11919/13868 [2:29:47<24:26,  1.33it/s]Training epoch 1:  86% 11920/13868 [2:29:47<24:20,  1.33it/s]Training epoch 1:  86% 11921/13868 [2:29:48<24:01,  1.35it/s]Training epoch 1:  86% 11922/13868 [2:29:49<24:09,  1.34it/s]Training epoch 1:  86% 11923/13868 [2:29:50<24:09,  1.34it/s]Training epoch 1:  86% 11924/13868 [2:29:50<24:13,  1.34it/s]Training epoch 1:  86% 11925/13868 [2:29:51<24:01,  1.35it/s]Training epoch 1:  86% 11926/13868 [2:29:52<24:18,  1.33it/s]Training epoch 1:  86% 11927/13868 [2:29:53<24:05,  1.34it/s]Training epoch 1:  86% 11928/13868 [2:29:53<24:08,  1.34it/s]Training epoch 1:  86% 11929/13868 [2:29:54<24:00,  1.35it/s]Training epoch 1:  86% 11930/13868 [2:29:55<23:55,  1.35it/s]Training epoch 1:  86% 11931/13868 [2:29:56<23:49,  1.36it/s]Training epoch 1:  86% 11932/13868 [2:29:56<24:07,  1.34it/s]Training epoch 1:  86% 11933/13868 [2:29:57<24:01,  1.34it/s]Training epoch 1:  86% 11934/13868 [2:29:58<23:50,  1.35it/s]Training epoch 1:  86% 11935/13868 [2:29:59<23:58,  1.34it/s]Training epoch 1:  86% 11936/13868 [2:29:59<23:43,  1.36it/s]Training epoch 1:  86% 11937/13868 [2:30:00<23:41,  1.36it/s]Training epoch 1:  86% 11938/13868 [2:30:01<23:33,  1.37it/s]Training epoch 1:  86% 11939/13868 [2:30:02<23:27,  1.37it/s]Training epoch 1:  86% 11940/13868 [2:30:02<23:29,  1.37it/s]Training epoch 1:  86% 11941/13868 [2:30:03<23:51,  1.35it/s]Training epoch 1:  86% 11942/13868 [2:30:04<24:12,  1.33it/s]Training epoch 1:  86% 11943/13868 [2:30:05<23:44,  1.35it/s]Training epoch 1:  86% 11944/13868 [2:30:05<23:41,  1.35it/s]Training epoch 1:  86% 11945/13868 [2:30:06<23:55,  1.34it/s]Training epoch 1:  86% 11946/13868 [2:30:07<23:49,  1.34it/s]Training epoch 1:  86% 11947/13868 [2:30:08<23:52,  1.34it/s]Training epoch 1:  86% 11948/13868 [2:30:08<23:50,  1.34it/s]Training epoch 1:  86% 11949/13868 [2:30:09<23:59,  1.33it/s]Training epoch 1:  86% 11950/13868 [2:30:10<23:47,  1.34it/s]Training epoch 1:  86% 11951/13868 [2:30:11<23:55,  1.34it/s]Training epoch 1:  86% 11952/13868 [2:30:11<23:38,  1.35it/s]Training epoch 1:  86% 11953/13868 [2:30:12<23:38,  1.35it/s]Training epoch 1:  86% 11954/13868 [2:30:13<23:49,  1.34it/s]Training epoch 1:  86% 11955/13868 [2:30:13<23:50,  1.34it/s]Training epoch 1:  86% 11956/13868 [2:30:14<23:33,  1.35it/s]Training epoch 1:  86% 11957/13868 [2:30:15<23:50,  1.34it/s]Training epoch 1:  86% 11958/13868 [2:30:16<23:49,  1.34it/s]Training epoch 1:  86% 11959/13868 [2:30:17<24:01,  1.32it/s]Training epoch 1:  86% 11960/13868 [2:30:17<24:15,  1.31it/s]Training epoch 1:  86% 11961/13868 [2:30:18<24:22,  1.30it/s]Training epoch 1:  86% 11962/13868 [2:30:19<24:16,  1.31it/s]Training epoch 1:  86% 11963/13868 [2:30:20<24:11,  1.31it/s]Training epoch 1:  86% 11964/13868 [2:30:20<23:52,  1.33it/s]Training epoch 1:  86% 11965/13868 [2:30:21<23:58,  1.32it/s]Training epoch 1:  86% 11966/13868 [2:30:22<23:53,  1.33it/s]Training epoch 1:  86% 11967/13868 [2:30:23<23:49,  1.33it/s]Training epoch 1:  86% 11968/13868 [2:30:23<23:25,  1.35it/s]Training epoch 1:  86% 11969/13868 [2:30:24<23:46,  1.33it/s]Training epoch 1:  86% 11970/13868 [2:30:25<23:40,  1.34it/s]Training epoch 1:  86% 11971/13868 [2:30:26<23:41,  1.33it/s]Training epoch 1:  86% 11972/13868 [2:30:26<23:34,  1.34it/s]Training epoch 1:  86% 11973/13868 [2:30:27<23:40,  1.33it/s]Training epoch 1:  86% 11974/13868 [2:30:28<23:46,  1.33it/s]Training epoch 1:  86% 11975/13868 [2:30:29<23:31,  1.34it/s]Training epoch 1:  86% 11976/13868 [2:30:29<23:04,  1.37it/s]Training epoch 1:  86% 11977/13868 [2:30:30<23:09,  1.36it/s]Training epoch 1:  86% 11978/13868 [2:30:31<23:09,  1.36it/s]Training epoch 1:  86% 11979/13868 [2:30:31<23:06,  1.36it/s]Training epoch 1:  86% 11980/13868 [2:30:32<23:04,  1.36it/s]Training epoch 1:  86% 11981/13868 [2:30:33<23:17,  1.35it/s]Training epoch 1:  86% 11982/13868 [2:30:34<22:54,  1.37it/s]Training epoch 1:  86% 11983/13868 [2:30:34<22:55,  1.37it/s]Training epoch 1:  86% 11984/13868 [2:30:35<23:12,  1.35it/s]Training epoch 1:  86% 11985/13868 [2:30:36<23:26,  1.34it/s]Training epoch 1:  86% 11986/13868 [2:30:37<23:23,  1.34it/s]Training epoch 1:  86% 11987/13868 [2:30:37<23:26,  1.34it/s]Training epoch 1:  86% 11988/13868 [2:30:38<23:26,  1.34it/s]Training epoch 1:  86% 11989/13868 [2:30:39<23:14,  1.35it/s]Training epoch 1:  86% 11990/13868 [2:30:40<23:24,  1.34it/s]Training epoch 1:  86% 11991/13868 [2:30:40<23:22,  1.34it/s]Training epoch 1:  86% 11992/13868 [2:30:41<23:17,  1.34it/s]Training epoch 1:  86% 11993/13868 [2:30:42<23:25,  1.33it/s]Training epoch 1:  86% 11994/13868 [2:30:43<23:09,  1.35it/s]Training epoch 1:  86% 11995/13868 [2:30:43<23:15,  1.34it/s]Training epoch 1:  87% 11996/13868 [2:30:44<23:12,  1.34it/s]Training epoch 1:  87% 11997/13868 [2:30:45<23:23,  1.33it/s]Training epoch 1:  87% 11998/13868 [2:30:46<23:25,  1.33it/s]Training epoch 1:  87% 11999/13868 [2:30:46<23:37,  1.32it/s]Training epoch 1:  87% 12000/13868 [2:30:47<24:21,  1.28it/s]Training epoch 1:  87% 12001/13868 [2:30:48<23:45,  1.31it/s]Training epoch 1:  87% 12002/13868 [2:30:49<23:51,  1.30it/s]Training epoch 1:  87% 12003/13868 [2:30:49<23:30,  1.32it/s]Training epoch 1:  87% 12004/13868 [2:30:50<23:27,  1.32it/s]Training epoch 1:  87% 12005/13868 [2:30:51<23:39,  1.31it/s]Training epoch 1:  87% 12006/13868 [2:30:52<23:56,  1.30it/s]Training epoch 1:  87% 12007/13868 [2:30:53<23:58,  1.29it/s]Training epoch 1:  87% 12008/13868 [2:30:53<23:56,  1.29it/s]Training epoch 1:  87% 12009/13868 [2:30:54<23:34,  1.31it/s]Training epoch 1:  87% 12010/13868 [2:30:55<23:26,  1.32it/s]Training epoch 1:  87% 12011/13868 [2:30:56<23:09,  1.34it/s]Training epoch 1:  87% 12012/13868 [2:30:56<23:17,  1.33it/s]Training epoch 1:  87% 12013/13868 [2:30:57<23:17,  1.33it/s]Training epoch 1:  87% 12014/13868 [2:30:58<23:24,  1.32it/s]Training epoch 1:  87% 12015/13868 [2:30:59<23:20,  1.32it/s]Training epoch 1:  87% 12016/13868 [2:30:59<23:30,  1.31it/s]Training epoch 1:  87% 12017/13868 [2:31:00<23:40,  1.30it/s]Training epoch 1:  87% 12018/13868 [2:31:01<23:37,  1.30it/s]Training epoch 1:  87% 12019/13868 [2:31:02<23:24,  1.32it/s]Training epoch 1:  87% 12020/13868 [2:31:02<23:22,  1.32it/s]Training epoch 1:  87% 12021/13868 [2:31:03<22:57,  1.34it/s]Training epoch 1:  87% 12022/13868 [2:31:04<22:47,  1.35it/s]Training epoch 1:  87% 12023/13868 [2:31:05<22:58,  1.34it/s]Training epoch 1:  87% 12024/13868 [2:31:05<23:13,  1.32it/s]Training epoch 1:  87% 12025/13868 [2:31:06<23:15,  1.32it/s]Training epoch 1:  87% 12026/13868 [2:31:07<23:22,  1.31it/s]Training epoch 1:  87% 12027/13868 [2:31:08<23:13,  1.32it/s]Training epoch 1:  87% 12028/13868 [2:31:08<23:06,  1.33it/s]Training epoch 1:  87% 12029/13868 [2:31:09<23:17,  1.32it/s]Training epoch 1:  87% 12030/13868 [2:31:10<22:58,  1.33it/s]Training epoch 1:  87% 12031/13868 [2:31:11<23:02,  1.33it/s]Training epoch 1:  87% 12032/13868 [2:31:11<23:08,  1.32it/s]Training epoch 1:  87% 12033/13868 [2:31:12<22:54,  1.33it/s]Training epoch 1:  87% 12034/13868 [2:31:13<22:45,  1.34it/s]Training epoch 1:  87% 12035/13868 [2:31:14<22:34,  1.35it/s]Training epoch 1:  87% 12036/13868 [2:31:14<22:31,  1.36it/s]Training epoch 1:  87% 12037/13868 [2:31:15<22:49,  1.34it/s]Training epoch 1:  87% 12038/13868 [2:31:16<22:35,  1.35it/s]Training epoch 1:  87% 12039/13868 [2:31:17<22:38,  1.35it/s]Training epoch 1:  87% 12040/13868 [2:31:17<22:37,  1.35it/s]Training epoch 1:  87% 12041/13868 [2:31:18<22:53,  1.33it/s]Training epoch 1:  87% 12042/13868 [2:31:19<22:41,  1.34it/s]Training epoch 1:  87% 12043/13868 [2:31:20<23:02,  1.32it/s]Training epoch 1:  87% 12044/13868 [2:31:20<22:38,  1.34it/s]Training epoch 1:  87% 12045/13868 [2:31:21<22:59,  1.32it/s]Training epoch 1:  87% 12046/13868 [2:31:22<22:44,  1.34it/s]Training epoch 1:  87% 12047/13868 [2:31:23<22:45,  1.33it/s]Training epoch 1:  87% 12048/13868 [2:31:23<22:40,  1.34it/s]Training epoch 1:  87% 12049/13868 [2:31:24<22:31,  1.35it/s]Training epoch 1:  87% 12050/13868 [2:31:25<22:35,  1.34it/s]Training epoch 1:  87% 12051/13868 [2:31:26<22:30,  1.35it/s]Training epoch 1:  87% 12052/13868 [2:31:26<22:33,  1.34it/s]Training epoch 1:  87% 12053/13868 [2:31:27<22:52,  1.32it/s]Training epoch 1:  87% 12054/13868 [2:31:28<22:38,  1.34it/s]Training epoch 1:  87% 12055/13868 [2:31:29<22:56,  1.32it/s]Training epoch 1:  87% 12056/13868 [2:31:29<22:58,  1.31it/s]Training epoch 1:  87% 12057/13868 [2:31:30<22:54,  1.32it/s]Training epoch 1:  87% 12058/13868 [2:31:31<22:43,  1.33it/s]Training epoch 1:  87% 12059/13868 [2:31:32<22:56,  1.31it/s]Training epoch 1:  87% 12060/13868 [2:31:32<22:46,  1.32it/s]Training epoch 1:  87% 12061/13868 [2:31:33<22:41,  1.33it/s]Training epoch 1:  87% 12062/13868 [2:31:34<22:28,  1.34it/s]Training epoch 1:  87% 12063/13868 [2:31:35<22:37,  1.33it/s]Training epoch 1:  87% 12064/13868 [2:31:35<22:31,  1.33it/s]Training epoch 1:  87% 12065/13868 [2:31:36<22:37,  1.33it/s]Training epoch 1:  87% 12066/13868 [2:31:37<22:41,  1.32it/s]Training epoch 1:  87% 12067/13868 [2:31:38<22:36,  1.33it/s]Training epoch 1:  87% 12068/13868 [2:31:38<22:17,  1.35it/s]Training epoch 1:  87% 12069/13868 [2:31:39<22:29,  1.33it/s]Training epoch 1:  87% 12070/13868 [2:31:40<22:18,  1.34it/s]Training epoch 1:  87% 12071/13868 [2:31:41<22:39,  1.32it/s]Training epoch 1:  87% 12072/13868 [2:31:41<22:38,  1.32it/s]Training epoch 1:  87% 12073/13868 [2:31:42<22:26,  1.33it/s]Training epoch 1:  87% 12074/13868 [2:31:43<22:03,  1.36it/s]Training epoch 1:  87% 12075/13868 [2:31:44<22:11,  1.35it/s]Training epoch 1:  87% 12076/13868 [2:31:44<22:09,  1.35it/s]Training epoch 1:  87% 12077/13868 [2:31:45<22:16,  1.34it/s]Training epoch 1:  87% 12078/13868 [2:31:46<22:07,  1.35it/s]Training epoch 1:  87% 12079/13868 [2:31:47<22:12,  1.34it/s]Training epoch 1:  87% 12080/13868 [2:31:47<22:13,  1.34it/s]Training epoch 1:  87% 12081/13868 [2:31:48<22:13,  1.34it/s]Training epoch 1:  87% 12082/13868 [2:31:49<22:02,  1.35it/s]Training epoch 1:  87% 12083/13868 [2:31:50<22:16,  1.34it/s]Training epoch 1:  87% 12084/13868 [2:31:50<22:11,  1.34it/s]Training epoch 1:  87% 12085/13868 [2:31:51<22:25,  1.33it/s]Training epoch 1:  87% 12086/13868 [2:31:52<22:30,  1.32it/s]Training epoch 1:  87% 12087/13868 [2:31:53<22:42,  1.31it/s]Training epoch 1:  87% 12088/13868 [2:31:53<22:31,  1.32it/s]Training epoch 1:  87% 12089/13868 [2:31:54<22:25,  1.32it/s]Training epoch 1:  87% 12090/13868 [2:31:55<22:27,  1.32it/s]Training epoch 1:  87% 12091/13868 [2:31:56<21:51,  1.36it/s]Training epoch 1:  87% 12092/13868 [2:31:56<21:48,  1.36it/s]Training epoch 1:  87% 12093/13868 [2:31:57<21:41,  1.36it/s]Training epoch 1:  87% 12094/13868 [2:31:58<21:49,  1.35it/s]Training epoch 1:  87% 12095/13868 [2:31:59<21:56,  1.35it/s]Training epoch 1:  87% 12096/13868 [2:31:59<21:49,  1.35it/s]Training epoch 1:  87% 12097/13868 [2:32:00<22:00,  1.34it/s]Training epoch 1:  87% 12098/13868 [2:32:01<22:02,  1.34it/s]Training epoch 1:  87% 12099/13868 [2:32:02<22:03,  1.34it/s]Training epoch 1:  87% 12100/13868 [2:32:02<23:08,  1.27it/s]Training epoch 1:  87% 12101/13868 [2:32:03<22:43,  1.30it/s]Training epoch 1:  87% 12102/13868 [2:32:04<22:30,  1.31it/s]Training epoch 1:  87% 12103/13868 [2:32:05<22:20,  1.32it/s]Training epoch 1:  87% 12104/13868 [2:32:05<22:18,  1.32it/s]Training epoch 1:  87% 12105/13868 [2:32:06<22:24,  1.31it/s]Training epoch 1:  87% 12106/13868 [2:32:07<22:01,  1.33it/s]Training epoch 1:  87% 12107/13868 [2:32:08<21:58,  1.34it/s]Training epoch 1:  87% 12108/13868 [2:32:08<21:54,  1.34it/s]Training epoch 1:  87% 12109/13868 [2:32:09<21:59,  1.33it/s]Training epoch 1:  87% 12110/13868 [2:32:10<21:46,  1.35it/s]Training epoch 1:  87% 12111/13868 [2:32:11<22:04,  1.33it/s]Training epoch 1:  87% 12112/13868 [2:32:11<21:50,  1.34it/s]Training epoch 1:  87% 12113/13868 [2:32:12<22:02,  1.33it/s]Training epoch 1:  87% 12114/13868 [2:32:13<22:09,  1.32it/s]Training epoch 1:  87% 12115/13868 [2:32:14<22:00,  1.33it/s]Training epoch 1:  87% 12116/13868 [2:32:14<21:46,  1.34it/s]Training epoch 1:  87% 12117/13868 [2:32:15<22:04,  1.32it/s]Training epoch 1:  87% 12118/13868 [2:32:16<21:57,  1.33it/s]Training epoch 1:  87% 12119/13868 [2:32:17<21:51,  1.33it/s]Training epoch 1:  87% 12120/13868 [2:32:17<21:54,  1.33it/s]Training epoch 1:  87% 12121/13868 [2:32:18<21:38,  1.35it/s]Training epoch 1:  87% 12122/13868 [2:32:19<21:55,  1.33it/s]Training epoch 1:  87% 12123/13868 [2:32:20<21:53,  1.33it/s]Training epoch 1:  87% 12124/13868 [2:32:20<21:54,  1.33it/s]Training epoch 1:  87% 12125/13868 [2:32:21<21:47,  1.33it/s]Training epoch 1:  87% 12126/13868 [2:32:22<22:04,  1.32it/s]Training epoch 1:  87% 12127/13868 [2:32:23<22:02,  1.32it/s]Training epoch 1:  87% 12128/13868 [2:32:23<22:06,  1.31it/s]Training epoch 1:  87% 12129/13868 [2:32:24<22:14,  1.30it/s]Training epoch 1:  87% 12130/13868 [2:32:25<22:14,  1.30it/s]Training epoch 1:  87% 12131/13868 [2:32:26<22:08,  1.31it/s]Training epoch 1:  87% 12132/13868 [2:32:27<22:03,  1.31it/s]Training epoch 1:  87% 12133/13868 [2:32:27<21:51,  1.32it/s]Training epoch 1:  87% 12134/13868 [2:32:28<21:56,  1.32it/s]Training epoch 1:  88% 12135/13868 [2:32:29<22:18,  1.29it/s]Training epoch 1:  88% 12136/13868 [2:32:30<22:02,  1.31it/s]Training epoch 1:  88% 12137/13868 [2:32:30<22:03,  1.31it/s]Training epoch 1:  88% 12138/13868 [2:32:31<22:00,  1.31it/s]Training epoch 1:  88% 12139/13868 [2:32:32<21:53,  1.32it/s]Training epoch 1:  88% 12140/13868 [2:32:33<21:51,  1.32it/s]Training epoch 1:  88% 12141/13868 [2:32:33<21:57,  1.31it/s]Training epoch 1:  88% 12142/13868 [2:32:34<21:49,  1.32it/s]Training epoch 1:  88% 12143/13868 [2:32:35<21:31,  1.34it/s]Training epoch 1:  88% 12144/13868 [2:32:36<21:46,  1.32it/s]Training epoch 1:  88% 12145/13868 [2:32:36<21:40,  1.33it/s]Training epoch 1:  88% 12146/13868 [2:32:37<21:47,  1.32it/s]Training epoch 1:  88% 12147/13868 [2:32:38<21:51,  1.31it/s]Training epoch 1:  88% 12148/13868 [2:32:39<21:52,  1.31it/s]Training epoch 1:  88% 12149/13868 [2:32:39<21:31,  1.33it/s]Training epoch 1:  88% 12150/13868 [2:32:40<21:41,  1.32it/s]Training epoch 1:  88% 12151/13868 [2:32:41<21:31,  1.33it/s]Training epoch 1:  88% 12152/13868 [2:32:42<21:19,  1.34it/s]Training epoch 1:  88% 12153/13868 [2:32:42<21:17,  1.34it/s]Training epoch 1:  88% 12154/13868 [2:32:43<21:28,  1.33it/s]Training epoch 1:  88% 12155/13868 [2:32:45<26:24,  1.08it/s]Training epoch 1:  88% 12156/13868 [2:32:45<24:57,  1.14it/s]Training epoch 1:  88% 12157/13868 [2:32:46<24:20,  1.17it/s]Training epoch 1:  88% 12158/13868 [2:32:47<22:59,  1.24it/s]Training epoch 1:  88% 12159/13868 [2:32:48<22:39,  1.26it/s]Training epoch 1:  88% 12160/13868 [2:32:48<22:09,  1.29it/s]Training epoch 1:  88% 12161/13868 [2:32:49<21:50,  1.30it/s]Training epoch 1:  88% 12162/13868 [2:32:50<21:38,  1.31it/s]Training epoch 1:  88% 12163/13868 [2:32:51<21:24,  1.33it/s]Training epoch 1:  88% 12164/13868 [2:32:51<21:20,  1.33it/s]Training epoch 1:  88% 12165/13868 [2:32:52<21:13,  1.34it/s]Training epoch 1:  88% 12166/13868 [2:32:53<21:00,  1.35it/s]Training epoch 1:  88% 12167/13868 [2:32:53<21:08,  1.34it/s]Training epoch 1:  88% 12168/13868 [2:32:54<20:58,  1.35it/s]Training epoch 1:  88% 12169/13868 [2:32:55<21:08,  1.34it/s]Training epoch 1:  88% 12170/13868 [2:32:56<21:13,  1.33it/s]Training epoch 1:  88% 12171/13868 [2:32:56<21:02,  1.34it/s]Training epoch 1:  88% 12172/13868 [2:32:57<21:03,  1.34it/s]Training epoch 1:  88% 12173/13868 [2:32:58<21:14,  1.33it/s]Training epoch 1:  88% 12174/13868 [2:32:59<21:23,  1.32it/s]Training epoch 1:  88% 12175/13868 [2:32:59<21:21,  1.32it/s]Training epoch 1:  88% 12176/13868 [2:33:00<21:26,  1.31it/s]Training epoch 1:  88% 12177/13868 [2:33:01<21:30,  1.31it/s]Training epoch 1:  88% 12178/13868 [2:33:02<21:13,  1.33it/s]Training epoch 1:  88% 12179/13868 [2:33:02<21:05,  1.33it/s]Training epoch 1:  88% 12180/13868 [2:33:03<21:11,  1.33it/s]Training epoch 1:  88% 12181/13868 [2:33:04<21:15,  1.32it/s]Training epoch 1:  88% 12182/13868 [2:33:05<21:15,  1.32it/s]Training epoch 1:  88% 12183/13868 [2:33:06<21:21,  1.31it/s]Training epoch 1:  88% 12184/13868 [2:33:06<21:06,  1.33it/s]Training epoch 1:  88% 12185/13868 [2:33:07<21:04,  1.33it/s]Training epoch 1:  88% 12186/13868 [2:33:08<21:03,  1.33it/s]Training epoch 1:  88% 12187/13868 [2:33:09<21:02,  1.33it/s]Training epoch 1:  88% 12188/13868 [2:33:09<21:19,  1.31it/s]Training epoch 1:  88% 12189/13868 [2:33:10<21:29,  1.30it/s]Training epoch 1:  88% 12190/13868 [2:33:11<21:23,  1.31it/s]Training epoch 1:  88% 12191/13868 [2:33:12<21:06,  1.32it/s]Training epoch 1:  88% 12192/13868 [2:33:12<21:07,  1.32it/s]Training epoch 1:  88% 12193/13868 [2:33:13<21:15,  1.31it/s]Training epoch 1:  88% 12194/13868 [2:33:14<21:26,  1.30it/s]Training epoch 1:  88% 12195/13868 [2:33:15<21:11,  1.32it/s]Training epoch 1:  88% 12196/13868 [2:33:15<21:18,  1.31it/s]Training epoch 1:  88% 12197/13868 [2:33:16<21:06,  1.32it/s]Training epoch 1:  88% 12198/13868 [2:33:17<21:05,  1.32it/s]Training epoch 1:  88% 12199/13868 [2:33:18<21:11,  1.31it/s]Training epoch 1:  88% 12200/13868 [2:33:19<22:20,  1.24it/s]Training epoch 1:  88% 12201/13868 [2:33:19<21:57,  1.27it/s]Training epoch 1:  88% 12202/13868 [2:33:20<21:34,  1.29it/s]Training epoch 1:  88% 12203/13868 [2:33:21<21:10,  1.31it/s]Training epoch 1:  88% 12204/13868 [2:33:22<21:04,  1.32it/s]Training epoch 1:  88% 12205/13868 [2:33:22<20:54,  1.33it/s]Training epoch 1:  88% 12206/13868 [2:33:23<20:55,  1.32it/s]Training epoch 1:  88% 12207/13868 [2:33:24<20:56,  1.32it/s]Training epoch 1:  88% 12208/13868 [2:33:25<20:59,  1.32it/s]Training epoch 1:  88% 12209/13868 [2:33:25<20:47,  1.33it/s]Training epoch 1:  88% 12210/13868 [2:33:26<20:44,  1.33it/s]Training epoch 1:  88% 12211/13868 [2:33:27<20:41,  1.34it/s]Training epoch 1:  88% 12212/13868 [2:33:28<20:41,  1.33it/s]Training epoch 1:  88% 12213/13868 [2:33:28<20:41,  1.33it/s]Training epoch 1:  88% 12214/13868 [2:33:29<20:27,  1.35it/s]Training epoch 1:  88% 12215/13868 [2:33:30<20:33,  1.34it/s]Training epoch 1:  88% 12216/13868 [2:33:31<20:29,  1.34it/s]Training epoch 1:  88% 12217/13868 [2:33:31<20:43,  1.33it/s]Training epoch 1:  88% 12218/13868 [2:33:32<20:28,  1.34it/s]Training epoch 1:  88% 12219/13868 [2:33:33<20:37,  1.33it/s]Training epoch 1:  88% 12220/13868 [2:33:34<20:51,  1.32it/s]Training epoch 1:  88% 12221/13868 [2:33:34<20:50,  1.32it/s]Training epoch 1:  88% 12222/13868 [2:33:35<21:04,  1.30it/s]Training epoch 1:  88% 12223/13868 [2:33:36<21:04,  1.30it/s]Training epoch 1:  88% 12224/13868 [2:33:37<20:47,  1.32it/s]Training epoch 1:  88% 12225/13868 [2:33:37<20:54,  1.31it/s]Training epoch 1:  88% 12226/13868 [2:33:38<20:42,  1.32it/s]Training epoch 1:  88% 12227/13868 [2:33:39<20:34,  1.33it/s]Training epoch 1:  88% 12228/13868 [2:33:40<20:38,  1.32it/s]Training epoch 1:  88% 12229/13868 [2:33:40<20:36,  1.33it/s]Training epoch 1:  88% 12230/13868 [2:33:41<20:32,  1.33it/s]Training epoch 1:  88% 12231/13868 [2:33:42<20:34,  1.33it/s]Training epoch 1:  88% 12232/13868 [2:33:43<20:25,  1.33it/s]Training epoch 1:  88% 12233/13868 [2:33:43<20:22,  1.34it/s]Training epoch 1:  88% 12234/13868 [2:33:44<20:16,  1.34it/s]Training epoch 1:  88% 12235/13868 [2:33:45<20:29,  1.33it/s]Training epoch 1:  88% 12236/13868 [2:33:46<20:30,  1.33it/s]Training epoch 1:  88% 12237/13868 [2:33:46<20:16,  1.34it/s]Training epoch 1:  88% 12238/13868 [2:33:47<20:08,  1.35it/s]Training epoch 1:  88% 12239/13868 [2:33:48<20:16,  1.34it/s]Training epoch 1:  88% 12240/13868 [2:33:49<20:24,  1.33it/s]Training epoch 1:  88% 12241/13868 [2:33:49<20:38,  1.31it/s]Training epoch 1:  88% 12242/13868 [2:33:50<20:29,  1.32it/s]Training epoch 1:  88% 12243/13868 [2:33:51<20:30,  1.32it/s]Training epoch 1:  88% 12244/13868 [2:33:52<20:42,  1.31it/s]Training epoch 1:  88% 12245/13868 [2:33:52<20:39,  1.31it/s]Training epoch 1:  88% 12246/13868 [2:33:53<20:31,  1.32it/s]Training epoch 1:  88% 12247/13868 [2:33:54<20:38,  1.31it/s]Training epoch 1:  88% 12248/13868 [2:33:55<20:26,  1.32it/s]Training epoch 1:  88% 12249/13868 [2:33:55<20:11,  1.34it/s]Training epoch 1:  88% 12250/13868 [2:33:56<20:19,  1.33it/s]Training epoch 1:  88% 12251/13868 [2:33:57<20:05,  1.34it/s]Training epoch 1:  88% 12252/13868 [2:33:58<20:14,  1.33it/s]Training epoch 1:  88% 12253/13868 [2:33:59<20:15,  1.33it/s]Training epoch 1:  88% 12254/13868 [2:33:59<20:01,  1.34it/s]Training epoch 1:  88% 12255/13868 [2:34:00<20:12,  1.33it/s]Training epoch 1:  88% 12256/13868 [2:34:01<20:13,  1.33it/s]Training epoch 1:  88% 12257/13868 [2:34:02<20:15,  1.33it/s]Training epoch 1:  88% 12258/13868 [2:34:02<20:31,  1.31it/s]Training epoch 1:  88% 12259/13868 [2:34:03<20:31,  1.31it/s]Training epoch 1:  88% 12260/13868 [2:34:04<20:19,  1.32it/s]Training epoch 1:  88% 12261/13868 [2:34:05<20:27,  1.31it/s]Training epoch 1:  88% 12262/13868 [2:34:05<20:30,  1.31it/s]Training epoch 1:  88% 12263/13868 [2:34:06<20:11,  1.33it/s]Training epoch 1:  88% 12264/13868 [2:34:07<20:17,  1.32it/s]Training epoch 1:  88% 12265/13868 [2:34:08<20:34,  1.30it/s]Training epoch 1:  88% 12266/13868 [2:34:08<20:05,  1.33it/s]Training epoch 1:  88% 12267/13868 [2:34:09<20:11,  1.32it/s]Training epoch 1:  88% 12268/13868 [2:34:10<20:15,  1.32it/s]Training epoch 1:  88% 12269/13868 [2:34:11<20:14,  1.32it/s]Training epoch 1:  88% 12270/13868 [2:34:11<20:07,  1.32it/s]Training epoch 1:  88% 12271/13868 [2:34:12<20:25,  1.30it/s]Training epoch 1:  88% 12272/13868 [2:34:13<20:17,  1.31it/s]Training epoch 1:  88% 12273/13868 [2:34:14<20:21,  1.31it/s]Training epoch 1:  89% 12274/13868 [2:34:15<20:33,  1.29it/s]Training epoch 1:  89% 12275/13868 [2:34:15<20:26,  1.30it/s]Training epoch 1:  89% 12276/13868 [2:34:16<20:09,  1.32it/s]Training epoch 1:  89% 12277/13868 [2:34:17<20:10,  1.31it/s]Training epoch 1:  89% 12278/13868 [2:34:18<19:55,  1.33it/s]Training epoch 1:  89% 12279/13868 [2:34:18<19:51,  1.33it/s]Training epoch 1:  89% 12280/13868 [2:34:19<19:55,  1.33it/s]Training epoch 1:  89% 12281/13868 [2:34:20<19:55,  1.33it/s]Training epoch 1:  89% 12282/13868 [2:34:20<19:38,  1.35it/s]Training epoch 1:  89% 12283/13868 [2:34:21<19:41,  1.34it/s]Training epoch 1:  89% 12284/13868 [2:34:22<19:38,  1.34it/s]Training epoch 1:  89% 12285/13868 [2:34:23<19:50,  1.33it/s]Training epoch 1:  89% 12286/13868 [2:34:24<20:04,  1.31it/s]Training epoch 1:  89% 12287/13868 [2:34:24<19:57,  1.32it/s]Training epoch 1:  89% 12288/13868 [2:34:25<19:52,  1.33it/s]Training epoch 1:  89% 12289/13868 [2:34:26<19:51,  1.33it/s]Training epoch 1:  89% 12290/13868 [2:34:27<20:03,  1.31it/s]Training epoch 1:  89% 12291/13868 [2:34:27<20:01,  1.31it/s]Training epoch 1:  89% 12292/13868 [2:34:28<20:14,  1.30it/s]Training epoch 1:  89% 12293/13868 [2:34:29<20:02,  1.31it/s]Training epoch 1:  89% 12294/13868 [2:34:30<20:03,  1.31it/s]Training epoch 1:  89% 12295/13868 [2:34:30<19:32,  1.34it/s]Training epoch 1:  89% 12296/13868 [2:34:31<19:58,  1.31it/s]Training epoch 1:  89% 12297/13868 [2:34:32<20:15,  1.29it/s]Training epoch 1:  89% 12298/13868 [2:34:33<20:28,  1.28it/s]Training epoch 1:  89% 12299/13868 [2:34:33<20:16,  1.29it/s]Training epoch 1:  89% 12300/13868 [2:34:34<20:48,  1.26it/s]Training epoch 1:  89% 12301/13868 [2:34:35<20:27,  1.28it/s]Training epoch 1:  89% 12302/13868 [2:34:36<20:06,  1.30it/s]Training epoch 1:  89% 12303/13868 [2:34:37<19:54,  1.31it/s]Training epoch 1:  89% 12304/13868 [2:34:37<19:51,  1.31it/s]Training epoch 1:  89% 12305/13868 [2:34:38<19:35,  1.33it/s]Training epoch 1:  89% 12306/13868 [2:34:39<19:45,  1.32it/s]Training epoch 1:  89% 12307/13868 [2:34:40<19:45,  1.32it/s]Training epoch 1:  89% 12308/13868 [2:34:40<19:38,  1.32it/s]Training epoch 1:  89% 12309/13868 [2:34:41<19:42,  1.32it/s]Training epoch 1:  89% 12310/13868 [2:34:42<19:43,  1.32it/s]Training epoch 1:  89% 12311/13868 [2:34:43<19:32,  1.33it/s]Training epoch 1:  89% 12312/13868 [2:34:43<19:40,  1.32it/s]Training epoch 1:  89% 12313/13868 [2:34:44<19:36,  1.32it/s]Training epoch 1:  89% 12314/13868 [2:34:45<19:28,  1.33it/s]Training epoch 1:  89% 12315/13868 [2:34:46<19:22,  1.34it/s]Training epoch 1:  89% 12316/13868 [2:34:46<19:32,  1.32it/s]Training epoch 1:  89% 12317/13868 [2:34:47<19:26,  1.33it/s]Training epoch 1:  89% 12318/13868 [2:34:48<19:24,  1.33it/s]Training epoch 1:  89% 12319/13868 [2:34:49<19:39,  1.31it/s]Training epoch 1:  89% 12320/13868 [2:34:49<19:34,  1.32it/s]Training epoch 1:  89% 12321/13868 [2:34:50<19:41,  1.31it/s]Training epoch 1:  89% 12322/13868 [2:34:51<19:51,  1.30it/s]Training epoch 1:  89% 12323/13868 [2:34:52<19:35,  1.31it/s]Training epoch 1:  89% 12324/13868 [2:34:52<19:25,  1.32it/s]Training epoch 1:  89% 12325/13868 [2:34:53<19:24,  1.33it/s]Training epoch 1:  89% 12326/13868 [2:34:54<19:24,  1.32it/s]Training epoch 1:  89% 12327/13868 [2:34:55<19:11,  1.34it/s]Training epoch 1:  89% 12328/13868 [2:34:55<19:08,  1.34it/s]Training epoch 1:  89% 12329/13868 [2:34:56<19:22,  1.32it/s]Training epoch 1:  89% 12330/13868 [2:34:57<19:15,  1.33it/s]Training epoch 1:  89% 12331/13868 [2:34:58<19:14,  1.33it/s]Training epoch 1:  89% 12332/13868 [2:34:58<19:20,  1.32it/s]Training epoch 1:  89% 12333/13868 [2:34:59<19:10,  1.33it/s]Training epoch 1:  89% 12334/13868 [2:35:00<19:18,  1.32it/s]Training epoch 1:  89% 12335/13868 [2:35:01<19:21,  1.32it/s]Training epoch 1:  89% 12336/13868 [2:35:02<19:22,  1.32it/s]Training epoch 1:  89% 12337/13868 [2:35:02<19:37,  1.30it/s]Training epoch 1:  89% 12338/13868 [2:35:03<19:34,  1.30it/s]Training epoch 1:  89% 12339/13868 [2:35:04<19:18,  1.32it/s]Training epoch 1:  89% 12340/13868 [2:35:05<19:17,  1.32it/s]Training epoch 1:  89% 12341/13868 [2:35:05<19:16,  1.32it/s]Training epoch 1:  89% 12342/13868 [2:35:06<19:06,  1.33it/s]Training epoch 1:  89% 12343/13868 [2:35:07<19:19,  1.32it/s]Training epoch 1:  89% 12344/13868 [2:35:08<19:18,  1.32it/s]Training epoch 1:  89% 12345/13868 [2:35:08<19:10,  1.32it/s]Training epoch 1:  89% 12346/13868 [2:35:09<19:02,  1.33it/s]Training epoch 1:  89% 12347/13868 [2:35:10<19:12,  1.32it/s]Training epoch 1:  89% 12348/13868 [2:35:11<19:06,  1.33it/s]Training epoch 1:  89% 12349/13868 [2:35:11<19:04,  1.33it/s]Training epoch 1:  89% 12350/13868 [2:35:12<19:23,  1.31it/s]Training epoch 1:  89% 12351/13868 [2:35:13<19:09,  1.32it/s]Training epoch 1:  89% 12352/13868 [2:35:14<19:14,  1.31it/s]Training epoch 1:  89% 12353/13868 [2:35:14<19:15,  1.31it/s]Training epoch 1:  89% 12354/13868 [2:35:15<19:02,  1.32it/s]Training epoch 1:  89% 12355/13868 [2:35:16<18:57,  1.33it/s]Training epoch 1:  89% 12356/13868 [2:35:17<18:59,  1.33it/s]Training epoch 1:  89% 12357/13868 [2:35:17<18:46,  1.34it/s]Training epoch 1:  89% 12358/13868 [2:35:18<19:07,  1.32it/s]Training epoch 1:  89% 12359/13868 [2:35:19<19:00,  1.32it/s]Training epoch 1:  89% 12360/13868 [2:35:20<18:59,  1.32it/s]Training epoch 1:  89% 12361/13868 [2:35:20<18:58,  1.32it/s]Training epoch 1:  89% 12362/13868 [2:35:21<18:57,  1.32it/s]Training epoch 1:  89% 12363/13868 [2:35:22<19:00,  1.32it/s]Training epoch 1:  89% 12364/13868 [2:35:23<18:56,  1.32it/s]Training epoch 1:  89% 12365/13868 [2:35:23<19:12,  1.30it/s]Training epoch 1:  89% 12366/13868 [2:35:24<18:53,  1.33it/s]Training epoch 1:  89% 12367/13868 [2:35:25<18:57,  1.32it/s]Training epoch 1:  89% 12368/13868 [2:35:26<18:45,  1.33it/s]Training epoch 1:  89% 12369/13868 [2:35:26<18:52,  1.32it/s]Training epoch 1:  89% 12370/13868 [2:35:27<18:54,  1.32it/s]Training epoch 1:  89% 12371/13868 [2:35:28<19:03,  1.31it/s]Training epoch 1:  89% 12372/13868 [2:35:29<19:03,  1.31it/s]Training epoch 1:  89% 12373/13868 [2:35:30<18:51,  1.32it/s]Training epoch 1:  89% 12374/13868 [2:35:30<18:43,  1.33it/s]Training epoch 1:  89% 12375/13868 [2:35:31<18:44,  1.33it/s]Training epoch 1:  89% 12376/13868 [2:35:32<18:36,  1.34it/s]Training epoch 1:  89% 12377/13868 [2:35:33<18:37,  1.33it/s]Training epoch 1:  89% 12378/13868 [2:35:33<18:15,  1.36it/s]Training epoch 1:  89% 12379/13868 [2:35:34<18:23,  1.35it/s]Training epoch 1:  89% 12380/13868 [2:35:35<18:33,  1.34it/s]Training epoch 1:  89% 12381/13868 [2:35:35<18:15,  1.36it/s]Training epoch 1:  89% 12382/13868 [2:35:36<18:16,  1.35it/s]Training epoch 1:  89% 12383/13868 [2:35:37<18:32,  1.33it/s]Training epoch 1:  89% 12384/13868 [2:35:38<18:33,  1.33it/s]Training epoch 1:  89% 12385/13868 [2:35:38<18:32,  1.33it/s]Training epoch 1:  89% 12386/13868 [2:35:39<18:29,  1.34it/s]Training epoch 1:  89% 12387/13868 [2:35:40<18:33,  1.33it/s]Training epoch 1:  89% 12388/13868 [2:35:41<18:27,  1.34it/s]Training epoch 1:  89% 12389/13868 [2:35:41<18:36,  1.33it/s]Training epoch 1:  89% 12390/13868 [2:35:42<18:34,  1.33it/s]Training epoch 1:  89% 12391/13868 [2:35:43<18:22,  1.34it/s]Training epoch 1:  89% 12392/13868 [2:35:44<18:19,  1.34it/s]Training epoch 1:  89% 12393/13868 [2:35:44<18:18,  1.34it/s]Training epoch 1:  89% 12394/13868 [2:35:45<18:24,  1.34it/s]Training epoch 1:  89% 12395/13868 [2:35:46<18:29,  1.33it/s]Training epoch 1:  89% 12396/13868 [2:35:47<18:21,  1.34it/s]Training epoch 1:  89% 12397/13868 [2:35:47<18:05,  1.36it/s]Training epoch 1:  89% 12398/13868 [2:35:48<18:08,  1.35it/s]Training epoch 1:  89% 12399/13868 [2:35:49<17:53,  1.37it/s]Training epoch 1:  89% 12400/13868 [2:35:50<18:47,  1.30it/s]Training epoch 1:  89% 12401/13868 [2:35:51<18:53,  1.29it/s]Training epoch 1:  89% 12402/13868 [2:35:51<18:50,  1.30it/s]Training epoch 1:  89% 12403/13868 [2:35:52<18:41,  1.31it/s]Training epoch 1:  89% 12404/13868 [2:35:53<18:28,  1.32it/s]Training epoch 1:  89% 12405/13868 [2:35:53<18:13,  1.34it/s]Training epoch 1:  89% 12406/13868 [2:35:54<18:10,  1.34it/s]Training epoch 1:  89% 12407/13868 [2:35:55<18:08,  1.34it/s]Training epoch 1:  89% 12408/13868 [2:35:56<17:57,  1.35it/s]Training epoch 1:  89% 12409/13868 [2:35:56<18:00,  1.35it/s]Training epoch 1:  89% 12410/13868 [2:35:57<17:50,  1.36it/s]Training epoch 1:  89% 12411/13868 [2:35:58<17:50,  1.36it/s]Training epoch 1:  90% 12412/13868 [2:35:59<18:03,  1.34it/s]Training epoch 1:  90% 12413/13868 [2:35:59<18:03,  1.34it/s]Training epoch 1:  90% 12414/13868 [2:36:00<17:59,  1.35it/s]Training epoch 1:  90% 12415/13868 [2:36:01<17:58,  1.35it/s]Training epoch 1:  90% 12416/13868 [2:36:02<18:13,  1.33it/s]Training epoch 1:  90% 12417/13868 [2:36:02<18:04,  1.34it/s]Training epoch 1:  90% 12418/13868 [2:36:03<18:18,  1.32it/s]Training epoch 1:  90% 12419/13868 [2:36:04<18:27,  1.31it/s]Training epoch 1:  90% 12420/13868 [2:36:05<18:19,  1.32it/s]Training epoch 1:  90% 12421/13868 [2:36:05<18:15,  1.32it/s]Training epoch 1:  90% 12422/13868 [2:36:06<18:19,  1.32it/s]Training epoch 1:  90% 12423/13868 [2:36:07<18:12,  1.32it/s]Training epoch 1:  90% 12424/13868 [2:36:08<18:11,  1.32it/s]Training epoch 1:  90% 12425/13868 [2:36:08<17:59,  1.34it/s]Training epoch 1:  90% 12426/13868 [2:36:09<18:03,  1.33it/s]Training epoch 1:  90% 12427/13868 [2:36:10<17:50,  1.35it/s]Training epoch 1:  90% 12428/13868 [2:36:11<17:36,  1.36it/s]Training epoch 1:  90% 12429/13868 [2:36:11<17:38,  1.36it/s]Training epoch 1:  90% 12430/13868 [2:36:12<17:36,  1.36it/s]Training epoch 1:  90% 12431/13868 [2:36:13<17:37,  1.36it/s]Training epoch 1:  90% 12432/13868 [2:36:14<17:40,  1.35it/s]Training epoch 1:  90% 12433/13868 [2:36:14<17:32,  1.36it/s]Training epoch 1:  90% 12434/13868 [2:36:15<17:46,  1.34it/s]Training epoch 1:  90% 12435/13868 [2:36:16<17:43,  1.35it/s]Training epoch 1:  90% 12436/13868 [2:36:17<17:46,  1.34it/s]Training epoch 1:  90% 12437/13868 [2:36:17<17:42,  1.35it/s]Training epoch 1:  90% 12438/13868 [2:36:18<17:44,  1.34it/s]Training epoch 1:  90% 12439/13868 [2:36:19<17:41,  1.35it/s]Training epoch 1:  90% 12440/13868 [2:36:20<17:42,  1.34it/s]Training epoch 1:  90% 12441/13868 [2:36:20<17:47,  1.34it/s]Training epoch 1:  90% 12442/13868 [2:36:21<17:37,  1.35it/s]Training epoch 1:  90% 12443/13868 [2:36:22<17:46,  1.34it/s]Training epoch 1:  90% 12444/13868 [2:36:23<17:40,  1.34it/s]Training epoch 1:  90% 12445/13868 [2:36:23<17:40,  1.34it/s]Training epoch 1:  90% 12446/13868 [2:36:24<17:43,  1.34it/s]Training epoch 1:  90% 12447/13868 [2:36:25<17:41,  1.34it/s]Training epoch 1:  90% 12448/13868 [2:36:26<17:47,  1.33it/s]Training epoch 1:  90% 12449/13868 [2:36:26<17:49,  1.33it/s]Training epoch 1:  90% 12450/13868 [2:36:27<17:51,  1.32it/s]Training epoch 1:  90% 12451/13868 [2:36:28<17:44,  1.33it/s]Training epoch 1:  90% 12452/13868 [2:36:29<17:42,  1.33it/s]Training epoch 1:  90% 12453/13868 [2:36:29<17:40,  1.33it/s]Training epoch 1:  90% 12454/13868 [2:36:30<17:36,  1.34it/s]Training epoch 1:  90% 12455/13868 [2:36:31<17:32,  1.34it/s]Training epoch 1:  90% 12456/13868 [2:36:32<17:38,  1.33it/s]Training epoch 1:  90% 12457/13868 [2:36:32<17:56,  1.31it/s]Training epoch 1:  90% 12458/13868 [2:36:33<17:54,  1.31it/s]Training epoch 1:  90% 12459/13868 [2:36:34<17:54,  1.31it/s]Training epoch 1:  90% 12460/13868 [2:36:35<17:38,  1.33it/s]Training epoch 1:  90% 12461/13868 [2:36:35<17:42,  1.32it/s]Training epoch 1:  90% 12462/13868 [2:36:36<17:25,  1.34it/s]Training epoch 1:  90% 12463/13868 [2:36:37<17:24,  1.35it/s]Training epoch 1:  90% 12464/13868 [2:36:38<17:27,  1.34it/s]Training epoch 1:  90% 12465/13868 [2:36:38<17:29,  1.34it/s]Training epoch 1:  90% 12466/13868 [2:36:39<17:25,  1.34it/s]Training epoch 1:  90% 12467/13868 [2:36:40<17:30,  1.33it/s]Training epoch 1:  90% 12468/13868 [2:36:41<17:34,  1.33it/s]Training epoch 1:  90% 12469/13868 [2:36:41<17:23,  1.34it/s]Training epoch 1:  90% 12470/13868 [2:36:42<17:24,  1.34it/s]Training epoch 1:  90% 12471/13868 [2:36:43<17:28,  1.33it/s]Training epoch 1:  90% 12472/13868 [2:36:44<17:20,  1.34it/s]Training epoch 1:  90% 12473/13868 [2:36:44<17:18,  1.34it/s]Training epoch 1:  90% 12474/13868 [2:36:45<17:05,  1.36it/s]Training epoch 1:  90% 12475/13868 [2:36:46<17:07,  1.36it/s]Training epoch 1:  90% 12476/13868 [2:36:47<17:09,  1.35it/s]Training epoch 1:  90% 12477/13868 [2:36:47<17:09,  1.35it/s]Training epoch 1:  90% 12478/13868 [2:36:48<17:01,  1.36it/s]Training epoch 1:  90% 12479/13868 [2:36:49<16:56,  1.37it/s]Training epoch 1:  90% 12480/13868 [2:36:49<17:01,  1.36it/s]Training epoch 1:  90% 12481/13868 [2:36:50<16:56,  1.36it/s]Training epoch 1:  90% 12482/13868 [2:36:51<17:11,  1.34it/s]Training epoch 1:  90% 12483/13868 [2:36:52<17:18,  1.33it/s]Training epoch 1:  90% 12484/13868 [2:36:52<17:10,  1.34it/s]Training epoch 1:  90% 12485/13868 [2:36:53<17:13,  1.34it/s]Training epoch 1:  90% 12486/13868 [2:36:54<17:09,  1.34it/s]Training epoch 1:  90% 12487/13868 [2:36:55<16:50,  1.37it/s]Training epoch 1:  90% 12488/13868 [2:36:55<16:48,  1.37it/s]Training epoch 1:  90% 12489/13868 [2:36:56<16:41,  1.38it/s]Training epoch 1:  90% 12490/13868 [2:36:57<16:42,  1.37it/s]Training epoch 1:  90% 12491/13868 [2:36:58<16:59,  1.35it/s]Training epoch 1:  90% 12492/13868 [2:36:58<17:07,  1.34it/s]Training epoch 1:  90% 12493/13868 [2:36:59<17:12,  1.33it/s]Training epoch 1:  90% 12494/13868 [2:37:00<17:00,  1.35it/s]Training epoch 1:  90% 12495/13868 [2:37:01<17:04,  1.34it/s]Training epoch 1:  90% 12496/13868 [2:37:01<17:01,  1.34it/s]Training epoch 1:  90% 12497/13868 [2:37:02<17:01,  1.34it/s]Training epoch 1:  90% 12498/13868 [2:37:03<16:56,  1.35it/s]Training epoch 1:  90% 12499/13868 [2:37:03<16:32,  1.38it/s]Training epoch 1:  90% 12500/13868 [2:37:04<17:40,  1.29it/s]Training epoch 1:  90% 12501/13868 [2:37:05<17:27,  1.31it/s]Training epoch 1:  90% 12502/13868 [2:37:06<17:24,  1.31it/s]Training epoch 1:  90% 12503/13868 [2:37:07<17:11,  1.32it/s]Training epoch 1:  90% 12504/13868 [2:37:07<17:09,  1.32it/s]Training epoch 1:  90% 12505/13868 [2:37:08<16:54,  1.34it/s]Training epoch 1:  90% 12506/13868 [2:37:09<16:55,  1.34it/s]Training epoch 1:  90% 12507/13868 [2:37:10<17:03,  1.33it/s]Training epoch 1:  90% 12508/13868 [2:37:10<16:53,  1.34it/s]Training epoch 1:  90% 12509/13868 [2:37:11<16:48,  1.35it/s]Training epoch 1:  90% 12510/13868 [2:37:12<16:45,  1.35it/s]Training epoch 1:  90% 12511/13868 [2:37:13<16:42,  1.35it/s]Training epoch 1:  90% 12512/13868 [2:37:13<16:42,  1.35it/s]Training epoch 1:  90% 12513/13868 [2:37:14<16:38,  1.36it/s]Training epoch 1:  90% 12514/13868 [2:37:15<16:39,  1.35it/s]Training epoch 1:  90% 12515/13868 [2:37:16<16:54,  1.33it/s]Training epoch 1:  90% 12516/13868 [2:37:16<16:57,  1.33it/s]Training epoch 1:  90% 12517/13868 [2:37:17<16:50,  1.34it/s]Training epoch 1:  90% 12518/13868 [2:37:18<16:48,  1.34it/s]Training epoch 1:  90% 12519/13868 [2:37:19<17:01,  1.32it/s]Training epoch 1:  90% 12520/13868 [2:37:19<16:55,  1.33it/s]Training epoch 1:  90% 12521/13868 [2:37:20<16:54,  1.33it/s]Training epoch 1:  90% 12522/13868 [2:37:21<16:50,  1.33it/s]Training epoch 1:  90% 12523/13868 [2:37:22<16:49,  1.33it/s]Training epoch 1:  90% 12524/13868 [2:37:22<16:47,  1.33it/s]Training epoch 1:  90% 12525/13868 [2:37:23<16:48,  1.33it/s]Training epoch 1:  90% 12526/13868 [2:37:24<16:46,  1.33it/s]Training epoch 1:  90% 12527/13868 [2:37:25<16:40,  1.34it/s]Training epoch 1:  90% 12528/13868 [2:37:25<16:28,  1.36it/s]Training epoch 1:  90% 12529/13868 [2:37:26<16:27,  1.36it/s]Training epoch 1:  90% 12530/13868 [2:37:27<16:22,  1.36it/s]Training epoch 1:  90% 12531/13868 [2:37:27<16:29,  1.35it/s]Training epoch 1:  90% 12532/13868 [2:37:28<16:21,  1.36it/s]Training epoch 1:  90% 12533/13868 [2:37:29<16:36,  1.34it/s]Training epoch 1:  90% 12534/13868 [2:37:30<16:47,  1.32it/s]Training epoch 1:  90% 12535/13868 [2:37:30<16:40,  1.33it/s]Training epoch 1:  90% 12536/13868 [2:37:31<16:48,  1.32it/s]Training epoch 1:  90% 12537/13868 [2:37:32<16:47,  1.32it/s]Training epoch 1:  90% 12538/13868 [2:37:33<16:44,  1.32it/s]Training epoch 1:  90% 12539/13868 [2:37:33<16:34,  1.34it/s]Training epoch 1:  90% 12540/13868 [2:37:34<16:37,  1.33it/s]Training epoch 1:  90% 12541/13868 [2:37:35<16:30,  1.34it/s]Training epoch 1:  90% 12542/13868 [2:37:36<16:39,  1.33it/s]Training epoch 1:  90% 12543/13868 [2:37:37<16:41,  1.32it/s]Training epoch 1:  90% 12544/13868 [2:37:37<16:35,  1.33it/s]Training epoch 1:  90% 12545/13868 [2:37:38<16:41,  1.32it/s]Training epoch 1:  90% 12546/13868 [2:37:39<16:26,  1.34it/s]Training epoch 1:  90% 12547/13868 [2:37:39<16:18,  1.35it/s]Training epoch 1:  90% 12548/13868 [2:37:40<16:20,  1.35it/s]Training epoch 1:  90% 12549/13868 [2:37:41<16:23,  1.34it/s]Training epoch 1:  90% 12550/13868 [2:37:42<16:14,  1.35it/s]Training epoch 1:  91% 12551/13868 [2:37:42<16:26,  1.33it/s]Training epoch 1:  91% 12552/13868 [2:37:43<16:35,  1.32it/s]Training epoch 1:  91% 12553/13868 [2:37:44<16:29,  1.33it/s]Training epoch 1:  91% 12554/13868 [2:37:45<16:15,  1.35it/s]Training epoch 1:  91% 12555/13868 [2:37:45<16:17,  1.34it/s]Training epoch 1:  91% 12556/13868 [2:37:46<16:23,  1.33it/s]Training epoch 1:  91% 12557/13868 [2:37:47<16:22,  1.33it/s]Training epoch 1:  91% 12558/13868 [2:37:48<16:14,  1.34it/s]Training epoch 1:  91% 12559/13868 [2:37:48<15:56,  1.37it/s]Training epoch 1:  91% 12560/13868 [2:37:49<16:02,  1.36it/s]Training epoch 1:  91% 12561/13868 [2:37:50<16:07,  1.35it/s]Training epoch 1:  91% 12562/13868 [2:37:51<16:06,  1.35it/s]Training epoch 1:  91% 12563/13868 [2:37:51<15:56,  1.36it/s]Training epoch 1:  91% 12564/13868 [2:37:52<16:01,  1.36it/s]Training epoch 1:  91% 12565/13868 [2:37:53<16:09,  1.34it/s]Training epoch 1:  91% 12566/13868 [2:37:54<16:16,  1.33it/s]Training epoch 1:  91% 12567/13868 [2:37:54<16:12,  1.34it/s]Training epoch 1:  91% 12568/13868 [2:37:55<16:11,  1.34it/s]Training epoch 1:  91% 12569/13868 [2:37:56<16:10,  1.34it/s]Training epoch 1:  91% 12570/13868 [2:37:57<16:13,  1.33it/s]Training epoch 1:  91% 12571/13868 [2:37:57<16:08,  1.34it/s]Training epoch 1:  91% 12572/13868 [2:37:58<16:03,  1.35it/s]Training epoch 1:  91% 12573/13868 [2:37:59<15:59,  1.35it/s]Training epoch 1:  91% 12574/13868 [2:38:00<15:50,  1.36it/s]Training epoch 1:  91% 12575/13868 [2:38:00<15:44,  1.37it/s]Training epoch 1:  91% 12576/13868 [2:38:01<15:53,  1.35it/s]Training epoch 1:  91% 12577/13868 [2:38:02<16:00,  1.34it/s]Training epoch 1:  91% 12578/13868 [2:38:03<15:58,  1.35it/s]Training epoch 1:  91% 12579/13868 [2:38:03<15:49,  1.36it/s]Training epoch 1:  91% 12580/13868 [2:38:04<15:42,  1.37it/s]Training epoch 1:  91% 12581/13868 [2:38:05<15:44,  1.36it/s]Training epoch 1:  91% 12582/13868 [2:38:05<15:50,  1.35it/s]Training epoch 1:  91% 12583/13868 [2:38:06<15:51,  1.35it/s]Training epoch 1:  91% 12584/13868 [2:38:07<15:49,  1.35it/s]Training epoch 1:  91% 12585/13868 [2:38:08<15:51,  1.35it/s]Training epoch 1:  91% 12586/13868 [2:38:08<15:54,  1.34it/s]Training epoch 1:  91% 12587/13868 [2:38:09<15:52,  1.34it/s]Training epoch 1:  91% 12588/13868 [2:38:10<15:53,  1.34it/s]Training epoch 1:  91% 12589/13868 [2:38:11<15:44,  1.35it/s]Training epoch 1:  91% 12590/13868 [2:38:11<15:48,  1.35it/s]Training epoch 1:  91% 12591/13868 [2:38:12<15:55,  1.34it/s]Training epoch 1:  91% 12592/13868 [2:38:13<15:56,  1.33it/s]Training epoch 1:  91% 12593/13868 [2:38:14<15:49,  1.34it/s]Training epoch 1:  91% 12594/13868 [2:38:14<15:45,  1.35it/s]Training epoch 1:  91% 12595/13868 [2:38:15<15:52,  1.34it/s]Training epoch 1:  91% 12596/13868 [2:38:16<15:53,  1.33it/s]Training epoch 1:  91% 12597/13868 [2:38:17<15:48,  1.34it/s]Training epoch 1:  91% 12598/13868 [2:38:17<15:51,  1.34it/s]Training epoch 1:  91% 12599/13868 [2:38:18<15:45,  1.34it/s]Training epoch 1:  91% 12600/13868 [2:38:19<16:39,  1.27it/s]Training epoch 1:  91% 12601/13868 [2:38:20<16:25,  1.29it/s]Training epoch 1:  91% 12602/13868 [2:38:20<16:08,  1.31it/s]Training epoch 1:  91% 12603/13868 [2:38:21<15:57,  1.32it/s]Training epoch 1:  91% 12604/13868 [2:38:22<15:43,  1.34it/s]Training epoch 1:  91% 12605/13868 [2:38:23<15:29,  1.36it/s]Training epoch 1:  91% 12606/13868 [2:38:23<15:32,  1.35it/s]Training epoch 1:  91% 12607/13868 [2:38:24<15:38,  1.34it/s]Training epoch 1:  91% 12608/13868 [2:38:25<15:33,  1.35it/s]Training epoch 1:  91% 12609/13868 [2:38:26<15:27,  1.36it/s]Training epoch 1:  91% 12610/13868 [2:38:26<15:29,  1.35it/s]Training epoch 1:  91% 12611/13868 [2:38:27<15:30,  1.35it/s]Training epoch 1:  91% 12612/13868 [2:38:28<15:37,  1.34it/s]Training epoch 1:  91% 12613/13868 [2:38:29<15:57,  1.31it/s]Training epoch 1:  91% 12614/13868 [2:38:29<15:46,  1.32it/s]Training epoch 1:  91% 12615/13868 [2:38:30<15:46,  1.32it/s]Training epoch 1:  91% 12616/13868 [2:38:31<15:51,  1.32it/s]Training epoch 1:  91% 12617/13868 [2:38:32<15:26,  1.35it/s]Training epoch 1:  91% 12618/13868 [2:38:32<15:35,  1.34it/s]Training epoch 1:  91% 12619/13868 [2:38:33<15:36,  1.33it/s]Training epoch 1:  91% 12620/13868 [2:38:34<15:38,  1.33it/s]Training epoch 1:  91% 12621/13868 [2:38:35<15:38,  1.33it/s]Training epoch 1:  91% 12622/13868 [2:38:35<15:39,  1.33it/s]Training epoch 1:  91% 12623/13868 [2:38:36<15:22,  1.35it/s]Training epoch 1:  91% 12624/13868 [2:38:37<15:23,  1.35it/s]Training epoch 1:  91% 12625/13868 [2:38:38<15:40,  1.32it/s]Training epoch 1:  91% 12626/13868 [2:38:38<15:39,  1.32it/s]Training epoch 1:  91% 12627/13868 [2:38:39<15:31,  1.33it/s]Training epoch 1:  91% 12628/13868 [2:38:40<15:23,  1.34it/s]Training epoch 1:  91% 12629/13868 [2:38:41<15:17,  1.35it/s]Training epoch 1:  91% 12630/13868 [2:38:41<15:26,  1.34it/s]Training epoch 1:  91% 12631/13868 [2:38:42<15:18,  1.35it/s]Training epoch 1:  91% 12632/13868 [2:38:43<15:25,  1.34it/s]Training epoch 1:  91% 12633/13868 [2:38:44<15:21,  1.34it/s]Training epoch 1:  91% 12634/13868 [2:38:44<15:21,  1.34it/s]Training epoch 1:  91% 12635/13868 [2:38:45<15:30,  1.33it/s]Training epoch 1:  91% 12636/13868 [2:38:46<15:33,  1.32it/s]Training epoch 1:  91% 12637/13868 [2:38:47<15:38,  1.31it/s]Training epoch 1:  91% 12638/13868 [2:38:47<15:42,  1.30it/s]Training epoch 1:  91% 12639/13868 [2:38:48<15:34,  1.31it/s]Training epoch 1:  91% 12640/13868 [2:38:49<15:44,  1.30it/s]Training epoch 1:  91% 12641/13868 [2:38:50<15:42,  1.30it/s]Training epoch 1:  91% 12642/13868 [2:38:50<15:29,  1.32it/s]Training epoch 1:  91% 12643/13868 [2:38:51<15:12,  1.34it/s]Training epoch 1:  91% 12644/13868 [2:38:52<15:07,  1.35it/s]Training epoch 1:  91% 12645/13868 [2:38:53<15:17,  1.33it/s]Training epoch 1:  91% 12646/13868 [2:38:53<15:13,  1.34it/s]Training epoch 1:  91% 12647/13868 [2:38:54<14:59,  1.36it/s]Training epoch 1:  91% 12648/13868 [2:38:55<14:59,  1.36it/s]Training epoch 1:  91% 12649/13868 [2:38:56<15:02,  1.35it/s]Training epoch 1:  91% 12650/13868 [2:38:56<15:01,  1.35it/s]Training epoch 1:  91% 12651/13868 [2:38:57<15:01,  1.35it/s]Training epoch 1:  91% 12652/13868 [2:38:58<14:57,  1.35it/s]Training epoch 1:  91% 12653/13868 [2:38:59<14:41,  1.38it/s]Training epoch 1:  91% 12654/13868 [2:38:59<14:49,  1.36it/s]Training epoch 1:  91% 12655/13868 [2:39:00<15:01,  1.35it/s]Training epoch 1:  91% 12656/13868 [2:39:01<15:11,  1.33it/s]Training epoch 1:  91% 12657/13868 [2:39:02<15:04,  1.34it/s]Training epoch 1:  91% 12658/13868 [2:39:02<15:14,  1.32it/s]Training epoch 1:  91% 12659/13868 [2:39:03<15:19,  1.31it/s]Training epoch 1:  91% 12660/13868 [2:39:04<15:13,  1.32it/s]Training epoch 1:  91% 12661/13868 [2:39:05<15:12,  1.32it/s]Training epoch 1:  91% 12662/13868 [2:39:05<15:00,  1.34it/s]Training epoch 1:  91% 12663/13868 [2:39:06<14:55,  1.35it/s]Training epoch 1:  91% 12664/13868 [2:39:07<14:50,  1.35it/s]Training epoch 1:  91% 12665/13868 [2:39:08<14:49,  1.35it/s]Training epoch 1:  91% 12666/13868 [2:39:08<14:56,  1.34it/s]Training epoch 1:  91% 12667/13868 [2:39:09<14:54,  1.34it/s]Training epoch 1:  91% 12668/13868 [2:39:10<14:56,  1.34it/s]Training epoch 1:  91% 12669/13868 [2:39:11<14:52,  1.34it/s]Training epoch 1:  91% 12670/13868 [2:39:11<15:04,  1.32it/s]Training epoch 1:  91% 12671/13868 [2:39:12<15:06,  1.32it/s]Training epoch 1:  91% 12672/13868 [2:39:13<15:06,  1.32it/s]Training epoch 1:  91% 12673/13868 [2:39:14<15:01,  1.33it/s]Training epoch 1:  91% 12674/13868 [2:39:14<15:12,  1.31it/s]Training epoch 1:  91% 12675/13868 [2:39:15<15:11,  1.31it/s]Training epoch 1:  91% 12676/13868 [2:39:16<15:03,  1.32it/s]Training epoch 1:  91% 12677/13868 [2:39:17<15:00,  1.32it/s]Training epoch 1:  91% 12678/13868 [2:39:17<15:03,  1.32it/s]Training epoch 1:  91% 12679/13868 [2:39:18<14:46,  1.34it/s]Training epoch 1:  91% 12680/13868 [2:39:19<14:55,  1.33it/s]Training epoch 1:  91% 12681/13868 [2:39:20<14:57,  1.32it/s]Training epoch 1:  91% 12682/13868 [2:39:20<15:01,  1.32it/s]Training epoch 1:  91% 12683/13868 [2:39:21<14:53,  1.33it/s]Training epoch 1:  91% 12684/13868 [2:39:22<14:45,  1.34it/s]Training epoch 1:  91% 12685/13868 [2:39:23<14:55,  1.32it/s]Training epoch 1:  91% 12686/13868 [2:39:23<14:52,  1.32it/s]Training epoch 1:  91% 12687/13868 [2:39:24<14:50,  1.33it/s]Training epoch 1:  91% 12688/13868 [2:39:25<14:50,  1.32it/s]Training epoch 1:  91% 12689/13868 [2:39:26<14:43,  1.33it/s]Training epoch 1:  92% 12690/13868 [2:39:26<14:43,  1.33it/s]Training epoch 1:  92% 12691/13868 [2:39:27<14:33,  1.35it/s]Training epoch 1:  92% 12692/13868 [2:39:28<14:39,  1.34it/s]Training epoch 1:  92% 12693/13868 [2:39:29<14:33,  1.35it/s]Training epoch 1:  92% 12694/13868 [2:39:29<14:38,  1.34it/s]Training epoch 1:  92% 12695/13868 [2:39:30<14:30,  1.35it/s]Training epoch 1:  92% 12696/13868 [2:39:31<14:44,  1.33it/s]Training epoch 1:  92% 12697/13868 [2:39:32<14:53,  1.31it/s]Training epoch 1:  92% 12698/13868 [2:39:32<14:51,  1.31it/s]Training epoch 1:  92% 12699/13868 [2:39:33<14:58,  1.30it/s]Training epoch 1:  92% 12700/13868 [2:39:34<15:37,  1.25it/s]Training epoch 1:  92% 12701/13868 [2:39:35<15:16,  1.27it/s]Training epoch 1:  92% 12702/13868 [2:39:36<15:10,  1.28it/s]Training epoch 1:  92% 12703/13868 [2:39:36<14:49,  1.31it/s]Training epoch 1:  92% 12704/13868 [2:39:37<14:53,  1.30it/s]Training epoch 1:  92% 12705/13868 [2:39:38<14:45,  1.31it/s]Training epoch 1:  92% 12706/13868 [2:39:39<14:40,  1.32it/s]Training epoch 1:  92% 12707/13868 [2:39:39<14:41,  1.32it/s]Training epoch 1:  92% 12708/13868 [2:39:40<14:36,  1.32it/s]Training epoch 1:  92% 12709/13868 [2:39:41<14:27,  1.34it/s]Training epoch 1:  92% 12710/13868 [2:39:42<14:34,  1.32it/s]Training epoch 1:  92% 12711/13868 [2:39:42<14:18,  1.35it/s]Training epoch 1:  92% 12712/13868 [2:39:43<14:20,  1.34it/s]Training epoch 1:  92% 12713/13868 [2:39:44<14:18,  1.34it/s]Training epoch 1:  92% 12714/13868 [2:39:45<14:21,  1.34it/s]Training epoch 1:  92% 12715/13868 [2:39:45<14:25,  1.33it/s]Training epoch 1:  92% 12716/13868 [2:39:46<14:17,  1.34it/s]Training epoch 1:  92% 12717/13868 [2:39:47<14:29,  1.32it/s]Training epoch 1:  92% 12718/13868 [2:39:48<14:23,  1.33it/s]Training epoch 1:  92% 12719/13868 [2:39:48<14:17,  1.34it/s]Training epoch 1:  92% 12720/13868 [2:39:49<14:11,  1.35it/s]Training epoch 1:  92% 12721/13868 [2:39:50<14:12,  1.35it/s]Training epoch 1:  92% 12722/13868 [2:39:51<14:18,  1.33it/s]Training epoch 1:  92% 12723/13868 [2:39:51<13:54,  1.37it/s]Training epoch 1:  92% 12724/13868 [2:39:52<14:00,  1.36it/s]Training epoch 1:  92% 12725/13868 [2:39:53<14:01,  1.36it/s]Training epoch 1:  92% 12726/13868 [2:39:54<14:04,  1.35it/s]Training epoch 1:  92% 12727/13868 [2:39:54<13:59,  1.36it/s]Training epoch 1:  92% 12728/13868 [2:39:55<14:10,  1.34it/s]Training epoch 1:  92% 12729/13868 [2:39:56<14:03,  1.35it/s]Training epoch 1:  92% 12730/13868 [2:39:57<14:08,  1.34it/s]Training epoch 1:  92% 12731/13868 [2:39:57<14:11,  1.34it/s]Training epoch 1:  92% 12732/13868 [2:39:58<14:20,  1.32it/s]Training epoch 1:  92% 12733/13868 [2:39:59<14:18,  1.32it/s]Training epoch 1:  92% 12734/13868 [2:40:00<14:21,  1.32it/s]Training epoch 1:  92% 12735/13868 [2:40:00<14:29,  1.30it/s]Training epoch 1:  92% 12736/13868 [2:40:01<14:21,  1.31it/s]Training epoch 1:  92% 12737/13868 [2:40:02<14:07,  1.33it/s]Training epoch 1:  92% 12738/13868 [2:40:03<14:00,  1.34it/s]Training epoch 1:  92% 12739/13868 [2:40:03<13:49,  1.36it/s]Training epoch 1:  92% 12740/13868 [2:40:04<14:00,  1.34it/s]Training epoch 1:  92% 12741/13868 [2:40:05<14:08,  1.33it/s]Training epoch 1:  92% 12742/13868 [2:40:06<14:10,  1.32it/s]Training epoch 1:  92% 12743/13868 [2:40:06<14:10,  1.32it/s]Training epoch 1:  92% 12744/13868 [2:40:07<14:14,  1.32it/s]Training epoch 1:  92% 12745/13868 [2:40:08<14:07,  1.32it/s]Training epoch 1:  92% 12746/13868 [2:40:09<14:09,  1.32it/s]Training epoch 1:  92% 12747/13868 [2:40:09<14:01,  1.33it/s]Training epoch 1:  92% 12748/13868 [2:40:10<14:08,  1.32it/s]Training epoch 1:  92% 12749/13868 [2:40:11<14:09,  1.32it/s]Training epoch 1:  92% 12750/13868 [2:40:12<14:07,  1.32it/s]Training epoch 1:  92% 12751/13868 [2:40:12<14:03,  1.33it/s]Training epoch 1:  92% 12752/13868 [2:40:13<14:11,  1.31it/s]Training epoch 1:  92% 12753/13868 [2:40:14<13:48,  1.35it/s]Training epoch 1:  92% 12754/13868 [2:40:15<13:54,  1.34it/s]Training epoch 1:  92% 12755/13868 [2:40:15<13:57,  1.33it/s]Training epoch 1:  92% 12756/13868 [2:40:16<14:02,  1.32it/s]Training epoch 1:  92% 12757/13868 [2:40:17<13:57,  1.33it/s]Training epoch 1:  92% 12758/13868 [2:40:18<13:57,  1.32it/s]Training epoch 1:  92% 12759/13868 [2:40:18<13:58,  1.32it/s]Training epoch 1:  92% 12760/13868 [2:40:19<14:01,  1.32it/s]Training epoch 1:  92% 12761/13868 [2:40:20<13:52,  1.33it/s]Training epoch 1:  92% 12762/13868 [2:40:21<13:52,  1.33it/s]Training epoch 1:  92% 12763/13868 [2:40:21<13:44,  1.34it/s]Training epoch 1:  92% 12764/13868 [2:40:22<13:54,  1.32it/s]Training epoch 1:  92% 12765/13868 [2:40:23<13:41,  1.34it/s]Training epoch 1:  92% 12766/13868 [2:40:24<13:39,  1.35it/s]Training epoch 1:  92% 12767/13868 [2:40:24<13:41,  1.34it/s]Training epoch 1:  92% 12768/13868 [2:40:25<13:35,  1.35it/s]Training epoch 1:  92% 12769/13868 [2:40:26<13:33,  1.35it/s]Training epoch 1:  92% 12770/13868 [2:40:27<13:27,  1.36it/s]Training epoch 1:  92% 12771/13868 [2:40:27<13:22,  1.37it/s]Training epoch 1:  92% 12772/13868 [2:40:28<13:31,  1.35it/s]Training epoch 1:  92% 12773/13868 [2:40:29<13:29,  1.35it/s]Training epoch 1:  92% 12774/13868 [2:40:30<13:33,  1.35it/s]Training epoch 1:  92% 12775/13868 [2:40:30<13:32,  1.35it/s]Training epoch 1:  92% 12776/13868 [2:40:31<13:32,  1.34it/s]Training epoch 1:  92% 12777/13868 [2:40:32<13:16,  1.37it/s]Training epoch 1:  92% 12778/13868 [2:40:32<13:23,  1.36it/s]Training epoch 1:  92% 12779/13868 [2:40:33<13:28,  1.35it/s]Training epoch 1:  92% 12780/13868 [2:40:34<13:36,  1.33it/s]Training epoch 1:  92% 12781/13868 [2:40:35<13:39,  1.33it/s]Training epoch 1:  92% 12782/13868 [2:40:36<13:39,  1.32it/s]Training epoch 1:  92% 12783/13868 [2:40:36<13:20,  1.36it/s]Training epoch 1:  92% 12784/13868 [2:40:37<13:29,  1.34it/s]Training epoch 1:  92% 12785/13868 [2:40:38<13:36,  1.33it/s]Training epoch 1:  92% 12786/13868 [2:40:39<13:39,  1.32it/s]Training epoch 1:  92% 12787/13868 [2:40:39<13:40,  1.32it/s]Training epoch 1:  92% 12788/13868 [2:40:40<13:37,  1.32it/s]Training epoch 1:  92% 12789/13868 [2:40:41<13:39,  1.32it/s]Training epoch 1:  92% 12790/13868 [2:40:42<13:26,  1.34it/s]Training epoch 1:  92% 12791/13868 [2:40:42<13:32,  1.33it/s]Training epoch 1:  92% 12792/13868 [2:40:43<13:21,  1.34it/s]Training epoch 1:  92% 12793/13868 [2:40:44<13:17,  1.35it/s]Training epoch 1:  92% 12794/13868 [2:40:45<13:28,  1.33it/s]Training epoch 1:  92% 12795/13868 [2:40:45<13:30,  1.32it/s]Training epoch 1:  92% 12796/13868 [2:40:46<13:25,  1.33it/s]Training epoch 1:  92% 12797/13868 [2:40:47<13:19,  1.34it/s]Training epoch 1:  92% 12798/13868 [2:40:48<13:18,  1.34it/s]Training epoch 1:  92% 12799/13868 [2:40:48<13:21,  1.33it/s]Training epoch 1:  92% 12800/13868 [2:40:49<14:16,  1.25it/s]Training epoch 1:  92% 12801/13868 [2:40:50<13:59,  1.27it/s]Training epoch 1:  92% 12802/13868 [2:40:51<13:48,  1.29it/s]Training epoch 1:  92% 12803/13868 [2:40:51<13:37,  1.30it/s]Training epoch 1:  92% 12804/13868 [2:40:52<13:15,  1.34it/s]Training epoch 1:  92% 12805/13868 [2:40:53<13:06,  1.35it/s]Training epoch 1:  92% 12806/13868 [2:40:54<13:09,  1.35it/s]Training epoch 1:  92% 12807/13868 [2:40:54<13:02,  1.36it/s]Training epoch 1:  92% 12808/13868 [2:40:55<13:07,  1.35it/s]Training epoch 1:  92% 12809/13868 [2:40:56<13:07,  1.34it/s]Training epoch 1:  92% 12810/13868 [2:40:57<13:05,  1.35it/s]Training epoch 1:  92% 12811/13868 [2:40:57<13:09,  1.34it/s]Training epoch 1:  92% 12812/13868 [2:40:58<13:06,  1.34it/s]Training epoch 1:  92% 12813/13868 [2:40:59<13:01,  1.35it/s]Training epoch 1:  92% 12814/13868 [2:41:00<13:09,  1.34it/s]Training epoch 1:  92% 12815/13868 [2:41:00<13:00,  1.35it/s]Training epoch 1:  92% 12816/13868 [2:41:01<13:03,  1.34it/s]Training epoch 1:  92% 12817/13868 [2:41:02<13:06,  1.34it/s]Training epoch 1:  92% 12818/13868 [2:41:03<13:23,  1.31it/s]Training epoch 1:  92% 12819/13868 [2:41:03<13:24,  1.30it/s]Training epoch 1:  92% 12820/13868 [2:41:04<13:24,  1.30it/s]Training epoch 1:  92% 12821/13868 [2:41:05<13:22,  1.30it/s]Training epoch 1:  92% 12822/13868 [2:41:06<13:06,  1.33it/s]Training epoch 1:  92% 12823/13868 [2:41:06<13:02,  1.34it/s]Training epoch 1:  92% 12824/13868 [2:41:07<13:11,  1.32it/s]Training epoch 1:  92% 12825/13868 [2:41:08<12:59,  1.34it/s]Training epoch 1:  92% 12826/13868 [2:41:09<13:02,  1.33it/s]Training epoch 1:  92% 12827/13868 [2:41:09<13:06,  1.32it/s]Training epoch 1:  93% 12828/13868 [2:41:10<12:56,  1.34it/s]Training epoch 1:  93% 12829/13868 [2:41:11<12:50,  1.35it/s]Training epoch 1:  93% 12830/13868 [2:41:12<12:55,  1.34it/s]Training epoch 1:  93% 12831/13868 [2:41:12<12:54,  1.34it/s]Training epoch 1:  93% 12832/13868 [2:41:13<12:46,  1.35it/s]Training epoch 1:  93% 12833/13868 [2:41:14<12:41,  1.36it/s]Training epoch 1:  93% 12834/13868 [2:41:15<12:44,  1.35it/s]Training epoch 1:  93% 12835/13868 [2:41:15<12:47,  1.35it/s]Training epoch 1:  93% 12836/13868 [2:41:16<12:50,  1.34it/s]Training epoch 1:  93% 12837/13868 [2:41:17<12:54,  1.33it/s]Training epoch 1:  93% 12838/13868 [2:41:18<12:51,  1.33it/s]Training epoch 1:  93% 12839/13868 [2:41:18<12:48,  1.34it/s]Training epoch 1:  93% 12840/13868 [2:41:19<12:40,  1.35it/s]Training epoch 1:  93% 12841/13868 [2:41:20<12:46,  1.34it/s]Training epoch 1:  93% 12842/13868 [2:41:21<12:42,  1.35it/s]Training epoch 1:  93% 12843/13868 [2:41:21<12:33,  1.36it/s]Training epoch 1:  93% 12844/13868 [2:41:22<12:26,  1.37it/s]Training epoch 1:  93% 12845/13868 [2:41:23<12:40,  1.35it/s]Training epoch 1:  93% 12846/13868 [2:41:24<12:52,  1.32it/s]Training epoch 1:  93% 12847/13868 [2:41:24<12:48,  1.33it/s]Training epoch 1:  93% 12848/13868 [2:41:25<12:50,  1.32it/s]Training epoch 1:  93% 12849/13868 [2:41:26<12:50,  1.32it/s]Training epoch 1:  93% 12850/13868 [2:41:27<12:48,  1.33it/s]Training epoch 1:  93% 12851/13868 [2:41:27<12:44,  1.33it/s]Training epoch 1:  93% 12852/13868 [2:41:28<12:55,  1.31it/s]Training epoch 1:  93% 12853/13868 [2:41:29<12:48,  1.32it/s]Training epoch 1:  93% 12854/13868 [2:41:30<12:52,  1.31it/s]Training epoch 1:  93% 12855/13868 [2:41:30<12:48,  1.32it/s]Training epoch 1:  93% 12856/13868 [2:41:31<12:54,  1.31it/s]Training epoch 1:  93% 12857/13868 [2:41:32<12:42,  1.33it/s]Training epoch 1:  93% 12858/13868 [2:41:33<12:47,  1.32it/s]Training epoch 1:  93% 12859/13868 [2:41:33<12:42,  1.32it/s]Training epoch 1:  93% 12860/13868 [2:41:34<12:36,  1.33it/s]Training epoch 1:  93% 12861/13868 [2:41:35<12:32,  1.34it/s]Training epoch 1:  93% 12862/13868 [2:41:36<12:19,  1.36it/s]Training epoch 1:  93% 12863/13868 [2:41:36<12:17,  1.36it/s]Training epoch 1:  93% 12864/13868 [2:41:37<12:18,  1.36it/s]Training epoch 1:  93% 12865/13868 [2:41:38<12:20,  1.36it/s]Training epoch 1:  93% 12866/13868 [2:41:39<12:15,  1.36it/s]Training epoch 1:  93% 12867/13868 [2:41:39<12:20,  1.35it/s]Training epoch 1:  93% 12868/13868 [2:41:40<12:24,  1.34it/s]Training epoch 1:  93% 12869/13868 [2:41:41<12:23,  1.34it/s]Training epoch 1:  93% 12870/13868 [2:41:42<12:33,  1.33it/s]Training epoch 1:  93% 12871/13868 [2:41:42<12:20,  1.35it/s]Training epoch 1:  93% 12872/13868 [2:41:43<12:29,  1.33it/s]Training epoch 1:  93% 12873/13868 [2:41:44<12:25,  1.33it/s]Training epoch 1:  93% 12874/13868 [2:41:45<12:18,  1.35it/s]Training epoch 1:  93% 12875/13868 [2:41:45<12:26,  1.33it/s]Training epoch 1:  93% 12876/13868 [2:41:46<12:21,  1.34it/s]Training epoch 1:  93% 12877/13868 [2:41:47<12:18,  1.34it/s]Training epoch 1:  93% 12878/13868 [2:41:47<12:16,  1.34it/s]Training epoch 1:  93% 12879/13868 [2:41:48<12:12,  1.35it/s]Training epoch 1:  93% 12880/13868 [2:41:49<12:21,  1.33it/s]Training epoch 1:  93% 12881/13868 [2:41:50<12:15,  1.34it/s]Training epoch 1:  93% 12882/13868 [2:41:50<12:06,  1.36it/s]Training epoch 1:  93% 12883/13868 [2:41:51<12:20,  1.33it/s]Training epoch 1:  93% 12884/13868 [2:41:52<12:35,  1.30it/s]Training epoch 1:  93% 12885/13868 [2:41:53<12:17,  1.33it/s]Training epoch 1:  93% 12886/13868 [2:41:53<12:11,  1.34it/s]Training epoch 1:  93% 12887/13868 [2:41:54<12:17,  1.33it/s]Training epoch 1:  93% 12888/13868 [2:41:55<12:18,  1.33it/s]Training epoch 1:  93% 12889/13868 [2:41:56<12:15,  1.33it/s]Training epoch 1:  93% 12890/13868 [2:41:56<12:07,  1.34it/s]Training epoch 1:  93% 12891/13868 [2:41:57<12:05,  1.35it/s]Training epoch 1:  93% 12892/13868 [2:41:58<12:12,  1.33it/s]Training epoch 1:  93% 12893/13868 [2:41:59<12:11,  1.33it/s]Training epoch 1:  93% 12894/13868 [2:42:00<12:17,  1.32it/s]Training epoch 1:  93% 12895/13868 [2:42:00<12:06,  1.34it/s]Training epoch 1:  93% 12896/13868 [2:42:01<12:05,  1.34it/s]Training epoch 1:  93% 12897/13868 [2:42:02<12:12,  1.33it/s]Training epoch 1:  93% 12898/13868 [2:42:03<12:15,  1.32it/s]Training epoch 1:  93% 12899/13868 [2:42:03<12:14,  1.32it/s]Training epoch 1:  93% 12900/13868 [2:42:04<12:54,  1.25it/s]Training epoch 1:  93% 12901/13868 [2:42:05<12:34,  1.28it/s]Training epoch 1:  93% 12902/13868 [2:42:06<12:14,  1.32it/s]Training epoch 1:  93% 12903/13868 [2:42:06<12:08,  1.32it/s]Training epoch 1:  93% 12904/13868 [2:42:07<12:11,  1.32it/s]Training epoch 1:  93% 12905/13868 [2:42:08<12:08,  1.32it/s]Training epoch 1:  93% 12906/13868 [2:42:09<12:12,  1.31it/s]Training epoch 1:  93% 12907/13868 [2:42:09<12:08,  1.32it/s]Training epoch 1:  93% 12908/13868 [2:42:10<12:00,  1.33it/s]Training epoch 1:  93% 12909/13868 [2:42:11<11:58,  1.33it/s]Training epoch 1:  93% 12910/13868 [2:42:12<11:58,  1.33it/s]Training epoch 1:  93% 12911/13868 [2:42:12<12:05,  1.32it/s]Training epoch 1:  93% 12912/13868 [2:42:13<12:01,  1.32it/s]Training epoch 1:  93% 12913/13868 [2:42:14<12:03,  1.32it/s]Training epoch 1:  93% 12914/13868 [2:42:15<12:02,  1.32it/s]Training epoch 1:  93% 12915/13868 [2:42:15<12:03,  1.32it/s]Training epoch 1:  93% 12916/13868 [2:42:16<11:58,  1.33it/s]Training epoch 1:  93% 12917/13868 [2:42:17<11:59,  1.32it/s]Training epoch 1:  93% 12918/13868 [2:42:18<11:53,  1.33it/s]Training epoch 1:  93% 12919/13868 [2:42:18<11:54,  1.33it/s]Training epoch 1:  93% 12920/13868 [2:42:19<11:56,  1.32it/s]Training epoch 1:  93% 12921/13868 [2:42:20<11:55,  1.32it/s]Training epoch 1:  93% 12922/13868 [2:42:21<11:48,  1.34it/s]Training epoch 1:  93% 12923/13868 [2:42:21<11:49,  1.33it/s]Training epoch 1:  93% 12924/13868 [2:42:22<11:57,  1.32it/s]Training epoch 1:  93% 12925/13868 [2:42:23<11:48,  1.33it/s]Training epoch 1:  93% 12926/13868 [2:42:24<11:52,  1.32it/s]Training epoch 1:  93% 12927/13868 [2:42:24<11:44,  1.34it/s]Training epoch 1:  93% 12928/13868 [2:42:25<11:39,  1.34it/s]Training epoch 1:  93% 12929/13868 [2:42:26<11:31,  1.36it/s]Training epoch 1:  93% 12930/13868 [2:42:27<11:35,  1.35it/s]Training epoch 1:  93% 12931/13868 [2:42:27<11:28,  1.36it/s]Training epoch 1:  93% 12932/13868 [2:42:28<11:34,  1.35it/s]Training epoch 1:  93% 12933/13868 [2:42:29<11:35,  1.34it/s]Training epoch 1:  93% 12934/13868 [2:42:30<11:33,  1.35it/s]Training epoch 1:  93% 12935/13868 [2:42:30<11:40,  1.33it/s]Training epoch 1:  93% 12936/13868 [2:42:31<11:46,  1.32it/s]Training epoch 1:  93% 12937/13868 [2:42:32<11:49,  1.31it/s]Training epoch 1:  93% 12938/13868 [2:42:33<11:48,  1.31it/s]Training epoch 1:  93% 12939/13868 [2:42:33<11:47,  1.31it/s]Training epoch 1:  93% 12940/13868 [2:42:34<11:41,  1.32it/s]Training epoch 1:  93% 12941/13868 [2:42:35<11:33,  1.34it/s]Training epoch 1:  93% 12942/13868 [2:42:36<11:31,  1.34it/s]Training epoch 1:  93% 12943/13868 [2:42:36<11:26,  1.35it/s]Training epoch 1:  93% 12944/13868 [2:42:37<11:32,  1.33it/s]Training epoch 1:  93% 12945/13868 [2:42:38<11:40,  1.32it/s]Training epoch 1:  93% 12946/13868 [2:42:39<11:40,  1.32it/s]Training epoch 1:  93% 12947/13868 [2:42:39<11:35,  1.32it/s]Training epoch 1:  93% 12948/13868 [2:42:40<11:32,  1.33it/s]Training epoch 1:  93% 12949/13868 [2:42:41<11:30,  1.33it/s]Training epoch 1:  93% 12950/13868 [2:42:42<11:37,  1.32it/s]Training epoch 1:  93% 12951/13868 [2:42:43<11:37,  1.31it/s]Training epoch 1:  93% 12952/13868 [2:42:43<11:36,  1.31it/s]Training epoch 1:  93% 12953/13868 [2:42:44<11:38,  1.31it/s]Training epoch 1:  93% 12954/13868 [2:42:45<11:41,  1.30it/s]Training epoch 1:  93% 12955/13868 [2:42:46<11:26,  1.33it/s]Training epoch 1:  93% 12956/13868 [2:42:46<11:34,  1.31it/s]Training epoch 1:  93% 12957/13868 [2:42:47<11:28,  1.32it/s]Training epoch 1:  93% 12958/13868 [2:42:48<11:33,  1.31it/s]Training epoch 1:  93% 12959/13868 [2:42:49<11:36,  1.31it/s]Training epoch 1:  93% 12960/13868 [2:42:49<11:32,  1.31it/s]Training epoch 1:  93% 12961/13868 [2:42:50<11:38,  1.30it/s]Training epoch 1:  93% 12962/13868 [2:42:51<11:30,  1.31it/s]Training epoch 1:  93% 12963/13868 [2:42:52<11:32,  1.31it/s]Training epoch 1:  93% 12964/13868 [2:42:52<11:36,  1.30it/s]Training epoch 1:  93% 12965/13868 [2:42:53<11:26,  1.32it/s]Training epoch 1:  93% 12966/13868 [2:42:54<11:34,  1.30it/s]Training epoch 1:  94% 12967/13868 [2:42:55<11:27,  1.31it/s]Training epoch 1:  94% 12968/13868 [2:42:55<11:25,  1.31it/s]Training epoch 1:  94% 12969/13868 [2:42:56<11:19,  1.32it/s]Training epoch 1:  94% 12970/13868 [2:42:57<11:21,  1.32it/s]Training epoch 1:  94% 12971/13868 [2:42:58<11:13,  1.33it/s]Training epoch 1:  94% 12972/13868 [2:42:59<11:17,  1.32it/s]Training epoch 1:  94% 12973/13868 [2:42:59<11:12,  1.33it/s]Training epoch 1:  94% 12974/13868 [2:43:00<11:01,  1.35it/s]Training epoch 1:  94% 12975/13868 [2:43:01<10:58,  1.36it/s]Training epoch 1:  94% 12976/13868 [2:43:01<11:02,  1.35it/s]Training epoch 1:  94% 12977/13868 [2:43:02<10:56,  1.36it/s]Training epoch 1:  94% 12978/13868 [2:43:03<11:15,  1.32it/s]Training epoch 1:  94% 12979/13868 [2:43:04<11:18,  1.31it/s]Training epoch 1:  94% 12980/13868 [2:43:05<11:14,  1.32it/s]Training epoch 1:  94% 12981/13868 [2:43:05<11:04,  1.33it/s]Training epoch 1:  94% 12982/13868 [2:43:06<11:05,  1.33it/s]Training epoch 1:  94% 12983/13868 [2:43:07<10:59,  1.34it/s]Training epoch 1:  94% 12984/13868 [2:43:07<11:02,  1.33it/s]Training epoch 1:  94% 12985/13868 [2:43:08<10:56,  1.34it/s]Training epoch 1:  94% 12986/13868 [2:43:09<10:55,  1.35it/s]Training epoch 1:  94% 12987/13868 [2:43:10<10:53,  1.35it/s]Training epoch 1:  94% 12988/13868 [2:43:10<11:01,  1.33it/s]Training epoch 1:  94% 12989/13868 [2:43:11<10:55,  1.34it/s]Training epoch 1:  94% 12990/13868 [2:43:12<10:53,  1.34it/s]Training epoch 1:  94% 12991/13868 [2:43:13<10:59,  1.33it/s]Training epoch 1:  94% 12992/13868 [2:43:13<11:06,  1.31it/s]Training epoch 1:  94% 12993/13868 [2:43:14<10:58,  1.33it/s]Training epoch 1:  94% 12994/13868 [2:43:15<10:52,  1.34it/s]Training epoch 1:  94% 12995/13868 [2:43:16<10:48,  1.35it/s]Training epoch 1:  94% 12996/13868 [2:43:16<10:48,  1.34it/s]Training epoch 1:  94% 12997/13868 [2:43:17<10:42,  1.36it/s]Training epoch 1:  94% 12998/13868 [2:43:18<10:38,  1.36it/s]Training epoch 1:  94% 12999/13868 [2:43:19<10:36,  1.36it/s]Training epoch 1:  94% 13000/13868 [2:43:19<11:12,  1.29it/s]Training epoch 1:  94% 13001/13868 [2:43:20<10:57,  1.32it/s]Training epoch 1:  94% 13002/13868 [2:43:21<11:04,  1.30it/s]Training epoch 1:  94% 13003/13868 [2:43:22<10:51,  1.33it/s]Training epoch 1:  94% 13004/13868 [2:43:22<10:55,  1.32it/s]Training epoch 1:  94% 13005/13868 [2:43:23<10:50,  1.33it/s]Training epoch 1:  94% 13006/13868 [2:43:24<10:53,  1.32it/s]Training epoch 1:  94% 13007/13868 [2:43:25<10:43,  1.34it/s]Training epoch 1:  94% 13008/13868 [2:43:26<10:53,  1.32it/s]Training epoch 1:  94% 13009/13868 [2:43:26<10:46,  1.33it/s]Training epoch 1:  94% 13010/13868 [2:43:27<10:45,  1.33it/s]Training epoch 1:  94% 13011/13868 [2:43:28<10:45,  1.33it/s]Training epoch 1:  94% 13012/13868 [2:43:29<10:59,  1.30it/s]Training epoch 1:  94% 13013/13868 [2:43:29<10:43,  1.33it/s]Training epoch 1:  94% 13014/13868 [2:43:30<10:40,  1.33it/s]Training epoch 1:  94% 13015/13868 [2:43:31<10:38,  1.33it/s]Training epoch 1:  94% 13016/13868 [2:43:31<10:34,  1.34it/s]Training epoch 1:  94% 13017/13868 [2:43:32<10:42,  1.32it/s]Training epoch 1:  94% 13018/13868 [2:43:33<10:40,  1.33it/s]Training epoch 1:  94% 13019/13868 [2:43:34<10:35,  1.34it/s]Training epoch 1:  94% 13020/13868 [2:43:35<10:41,  1.32it/s]Training epoch 1:  94% 13021/13868 [2:43:35<10:36,  1.33it/s]Training epoch 1:  94% 13022/13868 [2:43:36<10:36,  1.33it/s]Training epoch 1:  94% 13023/13868 [2:43:37<10:33,  1.33it/s]Training epoch 1:  94% 13024/13868 [2:43:38<10:28,  1.34it/s]Training epoch 1:  94% 13025/13868 [2:43:38<10:24,  1.35it/s]Training epoch 1:  94% 13026/13868 [2:43:39<10:32,  1.33it/s]Training epoch 1:  94% 13027/13868 [2:43:40<10:29,  1.34it/s]Training epoch 1:  94% 13028/13868 [2:43:41<10:29,  1.33it/s]Training epoch 1:  94% 13029/13868 [2:43:41<10:33,  1.33it/s]Training epoch 1:  94% 13030/13868 [2:43:42<10:22,  1.35it/s]Training epoch 1:  94% 13031/13868 [2:43:43<10:17,  1.36it/s]Training epoch 1:  94% 13032/13868 [2:43:43<10:26,  1.34it/s]Training epoch 1:  94% 13033/13868 [2:43:44<10:22,  1.34it/s]Training epoch 1:  94% 13034/13868 [2:43:45<10:16,  1.35it/s]Training epoch 1:  94% 13035/13868 [2:43:46<10:24,  1.33it/s]Training epoch 1:  94% 13036/13868 [2:43:47<10:35,  1.31it/s]Training epoch 1:  94% 13037/13868 [2:43:47<10:26,  1.33it/s]Training epoch 1:  94% 13038/13868 [2:43:48<10:26,  1.32it/s]Training epoch 1:  94% 13039/13868 [2:43:49<10:17,  1.34it/s]Training epoch 1:  94% 13040/13868 [2:43:50<10:24,  1.33it/s]Training epoch 1:  94% 13041/13868 [2:43:50<10:26,  1.32it/s]Training epoch 1:  94% 13042/13868 [2:43:51<10:23,  1.33it/s]Training epoch 1:  94% 13043/13868 [2:43:52<10:17,  1.34it/s]Training epoch 1:  94% 13044/13868 [2:43:53<10:16,  1.34it/s]Training epoch 1:  94% 13045/13868 [2:43:53<10:19,  1.33it/s]Training epoch 1:  94% 13046/13868 [2:43:54<10:09,  1.35it/s]Training epoch 1:  94% 13047/13868 [2:43:55<10:11,  1.34it/s]Training epoch 1:  94% 13048/13868 [2:43:55<10:08,  1.35it/s]Training epoch 1:  94% 13049/13868 [2:43:56<10:06,  1.35it/s]Training epoch 1:  94% 13050/13868 [2:43:57<10:13,  1.33it/s]Training epoch 1:  94% 13051/13868 [2:43:58<10:03,  1.35it/s]Training epoch 1:  94% 13052/13868 [2:43:58<10:13,  1.33it/s]Training epoch 1:  94% 13053/13868 [2:43:59<10:09,  1.34it/s]Training epoch 1:  94% 13054/13868 [2:44:00<10:11,  1.33it/s]Training epoch 1:  94% 13055/13868 [2:44:01<10:00,  1.35it/s]Training epoch 1:  94% 13056/13868 [2:44:01<10:05,  1.34it/s]Training epoch 1:  94% 13057/13868 [2:44:02<10:08,  1.33it/s]Training epoch 1:  94% 13058/13868 [2:44:03<10:10,  1.33it/s]Training epoch 1:  94% 13059/13868 [2:44:04<10:07,  1.33it/s]Training epoch 1:  94% 13060/13868 [2:44:04<10:13,  1.32it/s]Training epoch 1:  94% 13061/13868 [2:44:05<10:07,  1.33it/s]Training epoch 1:  94% 13062/13868 [2:44:06<10:08,  1.32it/s]Training epoch 1:  94% 13063/13868 [2:44:07<10:07,  1.33it/s]Training epoch 1:  94% 13064/13868 [2:44:07<10:07,  1.32it/s]Training epoch 1:  94% 13065/13868 [2:44:08<10:05,  1.33it/s]Training epoch 1:  94% 13066/13868 [2:44:09<10:05,  1.33it/s]Training epoch 1:  94% 13067/13868 [2:44:10<09:55,  1.35it/s]Training epoch 1:  94% 13068/13868 [2:44:10<09:49,  1.36it/s]Training epoch 1:  94% 13069/13868 [2:44:11<09:49,  1.36it/s]Training epoch 1:  94% 13070/13868 [2:44:12<09:42,  1.37it/s]Training epoch 1:  94% 13071/13868 [2:44:13<09:40,  1.37it/s]Training epoch 1:  94% 13072/13868 [2:44:13<09:48,  1.35it/s]Training epoch 1:  94% 13073/13868 [2:44:14<09:44,  1.36it/s]Training epoch 1:  94% 13074/13868 [2:44:15<09:47,  1.35it/s]Training epoch 1:  94% 13075/13868 [2:44:16<09:49,  1.34it/s]Training epoch 1:  94% 13076/13868 [2:44:16<09:44,  1.35it/s]Training epoch 1:  94% 13077/13868 [2:44:17<09:41,  1.36it/s]Training epoch 1:  94% 13078/13868 [2:44:18<09:46,  1.35it/s]Training epoch 1:  94% 13079/13868 [2:44:19<09:43,  1.35it/s]Training epoch 1:  94% 13080/13868 [2:44:19<09:43,  1.35it/s]Training epoch 1:  94% 13081/13868 [2:44:20<09:36,  1.36it/s]Training epoch 1:  94% 13082/13868 [2:44:21<09:36,  1.36it/s]Training epoch 1:  94% 13083/13868 [2:44:21<09:39,  1.35it/s]Training epoch 1:  94% 13084/13868 [2:44:22<09:44,  1.34it/s]Training epoch 1:  94% 13085/13868 [2:44:23<09:43,  1.34it/s]Training epoch 1:  94% 13086/13868 [2:44:24<09:53,  1.32it/s]Training epoch 1:  94% 13087/13868 [2:44:25<09:53,  1.32it/s]Training epoch 1:  94% 13088/13868 [2:44:25<09:53,  1.31it/s]Training epoch 1:  94% 13089/13868 [2:44:26<09:41,  1.34it/s]Training epoch 1:  94% 13090/13868 [2:44:27<09:36,  1.35it/s]Training epoch 1:  94% 13091/13868 [2:44:27<09:34,  1.35it/s]Training epoch 1:  94% 13092/13868 [2:44:28<09:27,  1.37it/s]Training epoch 1:  94% 13093/13868 [2:44:29<09:25,  1.37it/s]Training epoch 1:  94% 13094/13868 [2:44:30<09:35,  1.34it/s]Training epoch 1:  94% 13095/13868 [2:44:30<09:33,  1.35it/s]Training epoch 1:  94% 13096/13868 [2:44:31<09:35,  1.34it/s]Training epoch 1:  94% 13097/13868 [2:44:32<09:38,  1.33it/s]Training epoch 1:  94% 13098/13868 [2:44:33<09:37,  1.33it/s]Training epoch 1:  94% 13099/13868 [2:44:33<09:38,  1.33it/s]Training epoch 1:  94% 13100/13868 [2:44:34<10:17,  1.24it/s]Training epoch 1:  94% 13101/13868 [2:44:35<10:08,  1.26it/s]Training epoch 1:  94% 13102/13868 [2:44:36<10:08,  1.26it/s]Training epoch 1:  94% 13103/13868 [2:44:37<09:59,  1.28it/s]Training epoch 1:  94% 13104/13868 [2:44:37<09:50,  1.29it/s]Training epoch 1:  94% 13105/13868 [2:44:38<09:48,  1.30it/s]Training epoch 1:  95% 13106/13868 [2:44:39<09:43,  1.31it/s]Training epoch 1:  95% 13107/13868 [2:44:40<09:40,  1.31it/s]Training epoch 1:  95% 13108/13868 [2:44:40<09:38,  1.31it/s]Training epoch 1:  95% 13109/13868 [2:44:41<09:38,  1.31it/s]Training epoch 1:  95% 13110/13868 [2:44:42<09:39,  1.31it/s]Training epoch 1:  95% 13111/13868 [2:44:43<09:37,  1.31it/s]Training epoch 1:  95% 13112/13868 [2:44:44<09:34,  1.32it/s]Training epoch 1:  95% 13113/13868 [2:44:44<09:27,  1.33it/s]Training epoch 1:  95% 13114/13868 [2:44:45<09:20,  1.34it/s]Training epoch 1:  95% 13115/13868 [2:44:46<09:23,  1.34it/s]Training epoch 1:  95% 13116/13868 [2:44:47<09:19,  1.34it/s]Training epoch 1:  95% 13117/13868 [2:44:47<09:15,  1.35it/s]Training epoch 1:  95% 13118/13868 [2:44:48<10:59,  1.14it/s]Training epoch 1:  95% 13119/13868 [2:44:49<10:32,  1.18it/s]Training epoch 1:  95% 13120/13868 [2:44:50<10:19,  1.21it/s]Training epoch 1:  95% 13121/13868 [2:44:51<09:53,  1.26it/s]Training epoch 1:  95% 13122/13868 [2:44:51<09:49,  1.27it/s]Training epoch 1:  95% 13123/13868 [2:44:52<09:28,  1.31it/s]Training epoch 1:  95% 13124/13868 [2:44:53<09:25,  1.31it/s]Training epoch 1:  95% 13125/13868 [2:44:54<09:16,  1.34it/s]Training epoch 1:  95% 13126/13868 [2:44:54<09:08,  1.35it/s]Training epoch 1:  95% 13127/13868 [2:44:55<09:05,  1.36it/s]Training epoch 1:  95% 13128/13868 [2:44:56<09:04,  1.36it/s]Training epoch 1:  95% 13129/13868 [2:44:57<08:58,  1.37it/s]Training epoch 1:  95% 13130/13868 [2:44:57<08:57,  1.37it/s]Training epoch 1:  95% 13131/13868 [2:44:58<08:56,  1.37it/s]Training epoch 1:  95% 13132/13868 [2:44:59<08:54,  1.38it/s]Training epoch 1:  95% 13133/13868 [2:44:59<08:58,  1.36it/s]Training epoch 1:  95% 13134/13868 [2:45:00<09:01,  1.36it/s]Training epoch 1:  95% 13135/13868 [2:45:01<08:56,  1.37it/s]Training epoch 1:  95% 13136/13868 [2:45:02<09:02,  1.35it/s]Training epoch 1:  95% 13137/13868 [2:45:02<09:03,  1.34it/s]Training epoch 1:  95% 13138/13868 [2:45:03<09:05,  1.34it/s]Training epoch 1:  95% 13139/13868 [2:45:04<09:04,  1.34it/s]Training epoch 1:  95% 13140/13868 [2:45:05<09:05,  1.33it/s]Training epoch 1:  95% 13141/13868 [2:45:05<09:06,  1.33it/s]Training epoch 1:  95% 13142/13868 [2:45:06<08:59,  1.34it/s]Training epoch 1:  95% 13143/13868 [2:45:07<09:04,  1.33it/s]Training epoch 1:  95% 13144/13868 [2:45:08<09:05,  1.33it/s]Training epoch 1:  95% 13145/13868 [2:45:08<09:05,  1.32it/s]Training epoch 1:  95% 13146/13868 [2:45:09<09:07,  1.32it/s]Training epoch 1:  95% 13147/13868 [2:45:10<09:08,  1.31it/s]Training epoch 1:  95% 13148/13868 [2:45:11<09:08,  1.31it/s]Training epoch 1:  95% 13149/13868 [2:45:12<09:06,  1.32it/s]Training epoch 1:  95% 13150/13868 [2:45:12<08:54,  1.34it/s]Training epoch 1:  95% 13151/13868 [2:45:13<08:49,  1.35it/s]Training epoch 1:  95% 13152/13868 [2:45:14<08:52,  1.35it/s]Training epoch 1:  95% 13153/13868 [2:45:14<08:50,  1.35it/s]Training epoch 1:  95% 13154/13868 [2:45:15<08:55,  1.33it/s]Training epoch 1:  95% 13155/13868 [2:45:16<08:53,  1.34it/s]Training epoch 1:  95% 13156/13868 [2:45:17<08:58,  1.32it/s]Training epoch 1:  95% 13157/13868 [2:45:17<08:55,  1.33it/s]Training epoch 1:  95% 13158/13868 [2:45:18<08:57,  1.32it/s]Training epoch 1:  95% 13159/13868 [2:45:19<08:56,  1.32it/s]Training epoch 1:  95% 13160/13868 [2:45:20<09:01,  1.31it/s]Training epoch 1:  95% 13161/13868 [2:45:21<08:53,  1.32it/s]Training epoch 1:  95% 13162/13868 [2:45:21<08:55,  1.32it/s]Training epoch 1:  95% 13163/13868 [2:45:22<08:47,  1.34it/s]Training epoch 1:  95% 13164/13868 [2:45:23<08:48,  1.33it/s]Training epoch 1:  95% 13165/13868 [2:45:23<08:35,  1.36it/s]Training epoch 1:  95% 13166/13868 [2:45:24<08:37,  1.36it/s]Training epoch 1:  95% 13167/13868 [2:45:25<08:48,  1.33it/s]Training epoch 1:  95% 13168/13868 [2:45:26<08:47,  1.33it/s]Training epoch 1:  95% 13169/13868 [2:45:27<08:44,  1.33it/s]Training epoch 1:  95% 13170/13868 [2:45:27<08:38,  1.35it/s]Training epoch 1:  95% 13171/13868 [2:45:28<08:37,  1.35it/s]Training epoch 1:  95% 13172/13868 [2:45:29<08:41,  1.34it/s]Training epoch 1:  95% 13173/13868 [2:45:30<08:43,  1.33it/s]Training epoch 1:  95% 13174/13868 [2:45:30<08:45,  1.32it/s]Training epoch 1:  95% 13175/13868 [2:45:31<08:49,  1.31it/s]Training epoch 1:  95% 13176/13868 [2:45:32<08:55,  1.29it/s]Training epoch 1:  95% 13177/13868 [2:45:33<08:46,  1.31it/s]Training epoch 1:  95% 13178/13868 [2:45:33<08:46,  1.31it/s]Training epoch 1:  95% 13179/13868 [2:45:34<08:41,  1.32it/s]Training epoch 1:  95% 13180/13868 [2:45:35<08:34,  1.34it/s]Training epoch 1:  95% 13181/13868 [2:45:36<08:32,  1.34it/s]Training epoch 1:  95% 13182/13868 [2:45:36<08:34,  1.33it/s]Training epoch 1:  95% 13183/13868 [2:45:37<08:33,  1.33it/s]Training epoch 1:  95% 13184/13868 [2:45:38<08:38,  1.32it/s]Training epoch 1:  95% 13185/13868 [2:45:39<08:40,  1.31it/s]Training epoch 1:  95% 13186/13868 [2:45:39<08:37,  1.32it/s]Training epoch 1:  95% 13187/13868 [2:45:40<08:40,  1.31it/s]Training epoch 1:  95% 13188/13868 [2:45:41<08:37,  1.32it/s]Training epoch 1:  95% 13189/13868 [2:45:42<08:31,  1.33it/s]Training epoch 1:  95% 13190/13868 [2:45:42<08:27,  1.34it/s]Training epoch 1:  95% 13191/13868 [2:45:43<08:29,  1.33it/s]Training epoch 1:  95% 13192/13868 [2:45:44<08:27,  1.33it/s]Training epoch 1:  95% 13193/13868 [2:45:45<08:30,  1.32it/s]Training epoch 1:  95% 13194/13868 [2:45:45<08:27,  1.33it/s]Training epoch 1:  95% 13195/13868 [2:45:46<08:20,  1.34it/s]Training epoch 1:  95% 13196/13868 [2:45:47<08:21,  1.34it/s]Training epoch 1:  95% 13197/13868 [2:45:48<08:23,  1.33it/s]Training epoch 1:  95% 13198/13868 [2:45:48<08:19,  1.34it/s]Training epoch 1:  95% 13199/13868 [2:45:49<08:24,  1.33it/s]Training epoch 1:  95% 13200/13868 [2:45:50<08:48,  1.26it/s]Training epoch 1:  95% 13201/13868 [2:45:51<08:37,  1.29it/s]Training epoch 1:  95% 13202/13868 [2:45:52<08:35,  1.29it/s]Training epoch 1:  95% 13203/13868 [2:45:52<08:31,  1.30it/s]Training epoch 1:  95% 13204/13868 [2:45:53<08:23,  1.32it/s]Training epoch 1:  95% 13205/13868 [2:45:54<08:27,  1.31it/s]Training epoch 1:  95% 13206/13868 [2:45:55<08:25,  1.31it/s]Training epoch 1:  95% 13207/13868 [2:45:55<08:25,  1.31it/s]Training epoch 1:  95% 13208/13868 [2:45:56<08:24,  1.31it/s]Training epoch 1:  95% 13209/13868 [2:45:57<08:23,  1.31it/s]Training epoch 1:  95% 13210/13868 [2:45:58<08:12,  1.34it/s]Training epoch 1:  95% 13211/13868 [2:45:58<08:11,  1.34it/s]Training epoch 1:  95% 13212/13868 [2:45:59<08:11,  1.33it/s]Training epoch 1:  95% 13213/13868 [2:46:00<08:12,  1.33it/s]Training epoch 1:  95% 13214/13868 [2:46:01<08:12,  1.33it/s]Training epoch 1:  95% 13215/13868 [2:46:01<08:13,  1.32it/s]Training epoch 1:  95% 13216/13868 [2:46:02<08:09,  1.33it/s]Training epoch 1:  95% 13217/13868 [2:46:03<08:05,  1.34it/s]Training epoch 1:  95% 13218/13868 [2:46:04<08:10,  1.32it/s]Training epoch 1:  95% 13219/13868 [2:46:04<08:07,  1.33it/s]Training epoch 1:  95% 13220/13868 [2:46:05<08:07,  1.33it/s]Training epoch 1:  95% 13221/13868 [2:46:06<08:04,  1.34it/s]Training epoch 1:  95% 13222/13868 [2:46:07<08:00,  1.35it/s]Training epoch 1:  95% 13223/13868 [2:46:07<07:59,  1.35it/s]Training epoch 1:  95% 13224/13868 [2:46:08<08:05,  1.33it/s]Training epoch 1:  95% 13225/13868 [2:46:09<08:04,  1.33it/s]Training epoch 1:  95% 13226/13868 [2:46:10<08:06,  1.32it/s]Training epoch 1:  95% 13227/13868 [2:46:10<08:09,  1.31it/s]Training epoch 1:  95% 13228/13868 [2:46:11<08:05,  1.32it/s]Training epoch 1:  95% 13229/13868 [2:46:12<08:06,  1.31it/s]Training epoch 1:  95% 13230/13868 [2:46:13<08:05,  1.31it/s]Training epoch 1:  95% 13231/13868 [2:46:13<08:02,  1.32it/s]Training epoch 1:  95% 13232/13868 [2:46:14<08:03,  1.32it/s]Training epoch 1:  95% 13233/13868 [2:46:15<08:05,  1.31it/s]Training epoch 1:  95% 13234/13868 [2:46:16<08:10,  1.29it/s]Training epoch 1:  95% 13235/13868 [2:46:17<08:11,  1.29it/s]Training epoch 1:  95% 13236/13868 [2:46:17<08:05,  1.30it/s]Training epoch 1:  95% 13237/13868 [2:46:18<08:00,  1.31it/s]Training epoch 1:  95% 13238/13868 [2:46:19<07:51,  1.33it/s]Training epoch 1:  95% 13239/13868 [2:46:19<07:49,  1.34it/s]Training epoch 1:  95% 13240/13868 [2:46:20<07:55,  1.32it/s]Training epoch 1:  95% 13241/13868 [2:46:21<07:55,  1.32it/s]Training epoch 1:  95% 13242/13868 [2:46:22<08:00,  1.30it/s]Training epoch 1:  95% 13243/13868 [2:46:23<07:55,  1.32it/s]Training epoch 1:  96% 13244/13868 [2:46:23<07:49,  1.33it/s]Training epoch 1:  96% 13245/13868 [2:46:24<07:45,  1.34it/s]Training epoch 1:  96% 13246/13868 [2:46:25<07:43,  1.34it/s]Training epoch 1:  96% 13247/13868 [2:46:26<07:47,  1.33it/s]Training epoch 1:  96% 13248/13868 [2:46:26<07:49,  1.32it/s]Training epoch 1:  96% 13249/13868 [2:46:27<07:40,  1.35it/s]Training epoch 1:  96% 13250/13868 [2:46:28<07:46,  1.32it/s]Training epoch 1:  96% 13251/13868 [2:46:29<07:48,  1.32it/s]Training epoch 1:  96% 13252/13868 [2:46:29<07:45,  1.32it/s]Training epoch 1:  96% 13253/13868 [2:46:30<07:46,  1.32it/s]Training epoch 1:  96% 13254/13868 [2:46:31<07:51,  1.30it/s]Training epoch 1:  96% 13255/13868 [2:46:32<07:44,  1.32it/s]Training epoch 1:  96% 13256/13868 [2:46:32<07:48,  1.31it/s]Training epoch 1:  96% 13257/13868 [2:46:33<07:50,  1.30it/s]Training epoch 1:  96% 13258/13868 [2:46:34<07:47,  1.30it/s]Training epoch 1:  96% 13259/13868 [2:46:35<07:46,  1.30it/s]Training epoch 1:  96% 13260/13868 [2:46:35<07:51,  1.29it/s]Training epoch 1:  96% 13261/13868 [2:46:36<07:47,  1.30it/s]Training epoch 1:  96% 13262/13868 [2:46:37<07:47,  1.30it/s]Training epoch 1:  96% 13263/13868 [2:46:38<07:40,  1.31it/s]Training epoch 1:  96% 13264/13868 [2:46:39<07:43,  1.30it/s]Training epoch 1:  96% 13265/13868 [2:46:39<07:45,  1.29it/s]Training epoch 1:  96% 13266/13868 [2:46:40<07:40,  1.31it/s]Training epoch 1:  96% 13267/13868 [2:46:41<07:38,  1.31it/s]Training epoch 1:  96% 13268/13868 [2:46:42<07:38,  1.31it/s]Training epoch 1:  96% 13269/13868 [2:46:42<07:38,  1.31it/s]Training epoch 1:  96% 13270/13868 [2:46:43<07:26,  1.34it/s]Training epoch 1:  96% 13271/13868 [2:46:44<07:25,  1.34it/s]Training epoch 1:  96% 13272/13868 [2:46:45<07:20,  1.35it/s]Training epoch 1:  96% 13273/13868 [2:46:45<07:29,  1.32it/s]Training epoch 1:  96% 13274/13868 [2:46:46<07:28,  1.33it/s]Training epoch 1:  96% 13275/13868 [2:46:47<07:23,  1.34it/s]Training epoch 1:  96% 13276/13868 [2:46:48<07:27,  1.32it/s]Training epoch 1:  96% 13277/13868 [2:46:48<07:24,  1.33it/s]Training epoch 1:  96% 13278/13868 [2:46:49<07:24,  1.33it/s]Training epoch 1:  96% 13279/13868 [2:46:50<07:24,  1.33it/s]Training epoch 1:  96% 13280/13868 [2:46:51<07:20,  1.33it/s]Training epoch 1:  96% 13281/13868 [2:46:51<07:23,  1.32it/s]Training epoch 1:  96% 13282/13868 [2:46:52<07:27,  1.31it/s]Training epoch 1:  96% 13283/13868 [2:46:53<07:27,  1.31it/s]Training epoch 1:  96% 13284/13868 [2:46:54<07:26,  1.31it/s]Training epoch 1:  96% 13285/13868 [2:46:54<07:17,  1.33it/s]Training epoch 1:  96% 13286/13868 [2:46:55<07:15,  1.34it/s]Training epoch 1:  96% 13287/13868 [2:46:56<07:16,  1.33it/s]Training epoch 1:  96% 13288/13868 [2:46:57<07:14,  1.33it/s]Training epoch 1:  96% 13289/13868 [2:46:57<07:18,  1.32it/s]Training epoch 1:  96% 13290/13868 [2:46:58<07:21,  1.31it/s]Training epoch 1:  96% 13291/13868 [2:46:59<07:19,  1.31it/s]Training epoch 1:  96% 13292/13868 [2:47:00<07:22,  1.30it/s]Training epoch 1:  96% 13293/13868 [2:47:00<07:23,  1.30it/s]Training epoch 1:  96% 13294/13868 [2:47:01<07:20,  1.30it/s]Training epoch 1:  96% 13295/13868 [2:47:02<07:16,  1.31it/s]Training epoch 1:  96% 13296/13868 [2:47:03<07:19,  1.30it/s]Training epoch 1:  96% 13297/13868 [2:47:04<07:16,  1.31it/s]Training epoch 1:  96% 13298/13868 [2:47:04<07:18,  1.30it/s]Training epoch 1:  96% 13299/13868 [2:47:05<07:16,  1.30it/s]Training epoch 1:  96% 13300/13868 [2:47:06<07:39,  1.24it/s]Training epoch 1:  96% 13301/13868 [2:47:07<07:31,  1.26it/s]Training epoch 1:  96% 13302/13868 [2:47:07<07:19,  1.29it/s]Training epoch 1:  96% 13303/13868 [2:47:08<07:11,  1.31it/s]Training epoch 1:  96% 13304/13868 [2:47:09<07:07,  1.32it/s]Training epoch 1:  96% 13305/13868 [2:47:10<07:03,  1.33it/s]Training epoch 1:  96% 13306/13868 [2:47:10<06:55,  1.35it/s]Training epoch 1:  96% 13307/13868 [2:47:11<06:51,  1.36it/s]Training epoch 1:  96% 13308/13868 [2:47:12<06:47,  1.37it/s]Training epoch 1:  96% 13309/13868 [2:47:13<06:48,  1.37it/s]Training epoch 1:  96% 13310/13868 [2:47:13<06:50,  1.36it/s]Training epoch 1:  96% 13311/13868 [2:47:14<06:54,  1.34it/s]Training epoch 1:  96% 13312/13868 [2:47:15<07:01,  1.32it/s]Training epoch 1:  96% 13313/13868 [2:47:16<06:59,  1.32it/s]Training epoch 1:  96% 13314/13868 [2:47:16<07:04,  1.31it/s]Training epoch 1:  96% 13315/13868 [2:47:17<06:54,  1.33it/s]Training epoch 1:  96% 13316/13868 [2:47:18<06:57,  1.32it/s]Training epoch 1:  96% 13317/13868 [2:47:19<07:01,  1.31it/s]Training epoch 1:  96% 13318/13868 [2:47:19<06:55,  1.32it/s]Training epoch 1:  96% 13319/13868 [2:47:20<06:56,  1.32it/s]Training epoch 1:  96% 13320/13868 [2:47:21<06:58,  1.31it/s]Training epoch 1:  96% 13321/13868 [2:47:22<06:54,  1.32it/s]Training epoch 1:  96% 13322/13868 [2:47:22<06:47,  1.34it/s]Training epoch 1:  96% 13323/13868 [2:47:23<06:47,  1.34it/s]Training epoch 1:  96% 13324/13868 [2:47:24<06:46,  1.34it/s]Training epoch 1:  96% 13325/13868 [2:47:25<06:41,  1.35it/s]Training epoch 1:  96% 13326/13868 [2:47:25<06:38,  1.36it/s]Training epoch 1:  96% 13327/13868 [2:47:26<06:38,  1.36it/s]Training epoch 1:  96% 13328/13868 [2:47:27<06:43,  1.34it/s]Training epoch 1:  96% 13329/13868 [2:47:28<06:54,  1.30it/s]Training epoch 1:  96% 13330/13868 [2:47:28<06:45,  1.33it/s]Training epoch 1:  96% 13331/13868 [2:47:29<06:44,  1.33it/s]Training epoch 1:  96% 13332/13868 [2:47:30<06:44,  1.33it/s]Training epoch 1:  96% 13333/13868 [2:47:31<06:45,  1.32it/s]Training epoch 1:  96% 13334/13868 [2:47:31<06:47,  1.31it/s]Training epoch 1:  96% 13335/13868 [2:47:32<06:46,  1.31it/s]Training epoch 1:  96% 13336/13868 [2:47:33<06:42,  1.32it/s]Training epoch 1:  96% 13337/13868 [2:47:34<06:35,  1.34it/s]Training epoch 1:  96% 13338/13868 [2:47:34<06:40,  1.32it/s]Training epoch 1:  96% 13339/13868 [2:47:35<06:42,  1.32it/s]Training epoch 1:  96% 13340/13868 [2:47:36<06:36,  1.33it/s]Training epoch 1:  96% 13341/13868 [2:47:37<06:40,  1.32it/s]Training epoch 1:  96% 13342/13868 [2:47:38<06:39,  1.32it/s]Training epoch 1:  96% 13343/13868 [2:47:38<06:39,  1.32it/s]Training epoch 1:  96% 13344/13868 [2:47:39<06:37,  1.32it/s]Training epoch 1:  96% 13345/13868 [2:47:40<06:36,  1.32it/s]Training epoch 1:  96% 13346/13868 [2:47:41<06:30,  1.34it/s]Training epoch 1:  96% 13347/13868 [2:47:41<06:25,  1.35it/s]Training epoch 1:  96% 13348/13868 [2:47:42<06:24,  1.35it/s]Training epoch 1:  96% 13349/13868 [2:47:43<06:29,  1.33it/s]Training epoch 1:  96% 13350/13868 [2:47:43<06:24,  1.35it/s]Training epoch 1:  96% 13351/13868 [2:47:44<06:24,  1.35it/s]Training epoch 1:  96% 13352/13868 [2:47:45<06:26,  1.33it/s]Training epoch 1:  96% 13353/13868 [2:47:46<06:23,  1.34it/s]Training epoch 1:  96% 13354/13868 [2:47:47<06:30,  1.32it/s]Training epoch 1:  96% 13355/13868 [2:47:47<06:28,  1.32it/s]Training epoch 1:  96% 13356/13868 [2:47:48<06:26,  1.32it/s]Training epoch 1:  96% 13357/13868 [2:47:49<06:31,  1.30it/s]Training epoch 1:  96% 13358/13868 [2:47:50<06:31,  1.30it/s]Training epoch 1:  96% 13359/13868 [2:47:50<06:26,  1.32it/s]Training epoch 1:  96% 13360/13868 [2:47:51<06:19,  1.34it/s]Training epoch 1:  96% 13361/13868 [2:47:52<06:22,  1.32it/s]Training epoch 1:  96% 13362/13868 [2:47:53<06:26,  1.31it/s]Training epoch 1:  96% 13363/13868 [2:47:53<06:23,  1.32it/s]Training epoch 1:  96% 13364/13868 [2:47:54<06:20,  1.32it/s]Training epoch 1:  96% 13365/13868 [2:47:55<06:16,  1.34it/s]Training epoch 1:  96% 13366/13868 [2:47:56<06:14,  1.34it/s]Training epoch 1:  96% 13367/13868 [2:47:56<06:17,  1.33it/s]Training epoch 1:  96% 13368/13868 [2:47:57<06:12,  1.34it/s]Training epoch 1:  96% 13369/13868 [2:47:58<06:14,  1.33it/s]Training epoch 1:  96% 13370/13868 [2:47:59<06:11,  1.34it/s]Training epoch 1:  96% 13371/13868 [2:47:59<06:11,  1.34it/s]Training epoch 1:  96% 13372/13868 [2:48:00<06:13,  1.33it/s]Training epoch 1:  96% 13373/13868 [2:48:01<06:13,  1.33it/s]Training epoch 1:  96% 13374/13868 [2:48:02<06:11,  1.33it/s]Training epoch 1:  96% 13375/13868 [2:48:02<06:11,  1.33it/s]Training epoch 1:  96% 13376/13868 [2:48:03<06:10,  1.33it/s]Training epoch 1:  96% 13377/13868 [2:48:04<06:03,  1.35it/s]Training epoch 1:  96% 13378/13868 [2:48:05<06:07,  1.33it/s]Training epoch 1:  96% 13379/13868 [2:48:05<06:07,  1.33it/s]Training epoch 1:  96% 13380/13868 [2:48:06<06:08,  1.32it/s]Training epoch 1:  96% 13381/13868 [2:48:07<06:10,  1.31it/s]Training epoch 1:  96% 13382/13868 [2:48:08<06:04,  1.33it/s]Training epoch 1:  97% 13383/13868 [2:48:08<06:02,  1.34it/s]Training epoch 1:  97% 13384/13868 [2:48:09<05:59,  1.35it/s]Training epoch 1:  97% 13385/13868 [2:48:10<06:01,  1.34it/s]Training epoch 1:  97% 13386/13868 [2:48:11<05:58,  1.34it/s]Training epoch 1:  97% 13387/13868 [2:48:11<05:58,  1.34it/s]Training epoch 1:  97% 13388/13868 [2:48:12<05:55,  1.35it/s]Training epoch 1:  97% 13389/13868 [2:48:13<05:56,  1.34it/s]Training epoch 1:  97% 13390/13868 [2:48:14<06:02,  1.32it/s]Training epoch 1:  97% 13391/13868 [2:48:14<05:59,  1.33it/s]Training epoch 1:  97% 13392/13868 [2:48:15<05:59,  1.32it/s]Training epoch 1:  97% 13393/13868 [2:48:16<05:55,  1.33it/s]Training epoch 1:  97% 13394/13868 [2:48:17<05:49,  1.36it/s]Training epoch 1:  97% 13395/13868 [2:48:17<05:48,  1.36it/s]Training epoch 1:  97% 13396/13868 [2:48:18<05:50,  1.35it/s]Training epoch 1:  97% 13397/13868 [2:48:19<05:53,  1.33it/s]Training epoch 1:  97% 13398/13868 [2:48:20<05:53,  1.33it/s]Training epoch 1:  97% 13399/13868 [2:48:20<05:51,  1.33it/s]Training epoch 1:  97% 13400/13868 [2:48:21<06:15,  1.25it/s]Training epoch 1:  97% 13401/13868 [2:48:22<06:04,  1.28it/s]Training epoch 1:  97% 13402/13868 [2:48:23<06:04,  1.28it/s]Training epoch 1:  97% 13403/13868 [2:48:23<05:58,  1.30it/s]Training epoch 1:  97% 13404/13868 [2:48:24<05:51,  1.32it/s]Training epoch 1:  97% 13405/13868 [2:48:25<05:51,  1.32it/s]Training epoch 1:  97% 13406/13868 [2:48:26<05:45,  1.34it/s]Training epoch 1:  97% 13407/13868 [2:48:26<05:45,  1.34it/s]Training epoch 1:  97% 13408/13868 [2:48:27<05:47,  1.32it/s]Training epoch 1:  97% 13409/13868 [2:48:28<05:49,  1.31it/s]Training epoch 1:  97% 13410/13868 [2:48:29<05:42,  1.34it/s]Training epoch 1:  97% 13411/13868 [2:48:29<05:45,  1.32it/s]Training epoch 1:  97% 13412/13868 [2:48:30<05:46,  1.32it/s]Training epoch 1:  97% 13413/13868 [2:48:31<05:48,  1.31it/s]Training epoch 1:  97% 13414/13868 [2:48:32<05:50,  1.30it/s]Training epoch 1:  97% 13415/13868 [2:48:33<05:46,  1.31it/s]Training epoch 1:  97% 13416/13868 [2:48:33<05:43,  1.32it/s]Training epoch 1:  97% 13417/13868 [2:48:34<05:43,  1.31it/s]Training epoch 1:  97% 13418/13868 [2:48:35<05:38,  1.33it/s]Training epoch 1:  97% 13419/13868 [2:48:36<05:36,  1.33it/s]Training epoch 1:  97% 13420/13868 [2:48:36<05:39,  1.32it/s]Training epoch 1:  97% 13421/13868 [2:48:37<05:36,  1.33it/s]Training epoch 1:  97% 13422/13868 [2:48:38<05:33,  1.34it/s]Training epoch 1:  97% 13423/13868 [2:48:39<05:32,  1.34it/s]Training epoch 1:  97% 13424/13868 [2:48:39<05:25,  1.36it/s]Training epoch 1:  97% 13425/13868 [2:48:40<05:27,  1.35it/s]Training epoch 1:  97% 13426/13868 [2:48:41<05:31,  1.33it/s]Training epoch 1:  97% 13427/13868 [2:48:41<05:26,  1.35it/s]Training epoch 1:  97% 13428/13868 [2:48:42<05:31,  1.33it/s]Training epoch 1:  97% 13429/13868 [2:48:43<05:29,  1.33it/s]Training epoch 1:  97% 13430/13868 [2:48:44<05:29,  1.33it/s]Training epoch 1:  97% 13431/13868 [2:48:45<05:36,  1.30it/s]Training epoch 1:  97% 13432/13868 [2:48:45<05:35,  1.30it/s]Training epoch 1:  97% 13433/13868 [2:48:46<05:33,  1.31it/s]Training epoch 1:  97% 13434/13868 [2:48:47<05:31,  1.31it/s]Training epoch 1:  97% 13435/13868 [2:48:48<05:30,  1.31it/s]Training epoch 1:  97% 13436/13868 [2:48:48<05:26,  1.32it/s]Training epoch 1:  97% 13437/13868 [2:48:49<05:32,  1.30it/s]Training epoch 1:  97% 13438/13868 [2:48:50<05:29,  1.31it/s]Training epoch 1:  97% 13439/13868 [2:48:51<05:23,  1.32it/s]Training epoch 1:  97% 13440/13868 [2:48:51<05:22,  1.33it/s]Training epoch 1:  97% 13441/13868 [2:48:52<05:22,  1.32it/s]Training epoch 1:  97% 13442/13868 [2:48:53<05:21,  1.33it/s]Training epoch 1:  97% 13443/13868 [2:48:54<05:20,  1.33it/s]Training epoch 1:  97% 13444/13868 [2:48:54<05:24,  1.30it/s]Training epoch 1:  97% 13445/13868 [2:48:55<05:20,  1.32it/s]Training epoch 1:  97% 13446/13868 [2:48:56<05:18,  1.32it/s]Training epoch 1:  97% 13447/13868 [2:48:57<05:17,  1.32it/s]Training epoch 1:  97% 13448/13868 [2:48:57<05:17,  1.32it/s]Training epoch 1:  97% 13449/13868 [2:48:58<05:15,  1.33it/s]Training epoch 1:  97% 13450/13868 [2:48:59<05:11,  1.34it/s]Training epoch 1:  97% 13451/13868 [2:49:00<05:10,  1.34it/s]Training epoch 1:  97% 13452/13868 [2:49:00<05:11,  1.34it/s]Training epoch 1:  97% 13453/13868 [2:49:01<05:08,  1.34it/s]Training epoch 1:  97% 13454/13868 [2:49:02<05:08,  1.34it/s]Training epoch 1:  97% 13455/13868 [2:49:03<05:09,  1.33it/s]Training epoch 1:  97% 13456/13868 [2:49:03<05:12,  1.32it/s]Training epoch 1:  97% 13457/13868 [2:49:04<05:10,  1.32it/s]Training epoch 1:  97% 13458/13868 [2:49:05<05:11,  1.32it/s]Training epoch 1:  97% 13459/13868 [2:49:06<05:12,  1.31it/s]Training epoch 1:  97% 13460/13868 [2:49:07<05:13,  1.30it/s]Training epoch 1:  97% 13461/13868 [2:49:07<05:10,  1.31it/s]Training epoch 1:  97% 13462/13868 [2:49:08<05:06,  1.32it/s]Training epoch 1:  97% 13463/13868 [2:49:09<05:01,  1.34it/s]Training epoch 1:  97% 13464/13868 [2:49:10<05:03,  1.33it/s]Training epoch 1:  97% 13465/13868 [2:49:10<05:01,  1.34it/s]Training epoch 1:  97% 13466/13868 [2:49:11<05:00,  1.34it/s]Training epoch 1:  97% 13467/13868 [2:49:12<05:02,  1.32it/s]Training epoch 1:  97% 13468/13868 [2:49:13<05:01,  1.33it/s]Training epoch 1:  97% 13469/13868 [2:49:13<05:02,  1.32it/s]Training epoch 1:  97% 13470/13868 [2:49:14<05:06,  1.30it/s]Training epoch 1:  97% 13471/13868 [2:49:15<05:03,  1.31it/s]Training epoch 1:  97% 13472/13868 [2:49:16<05:05,  1.30it/s]Training epoch 1:  97% 13473/13868 [2:49:16<05:01,  1.31it/s]Training epoch 1:  97% 13474/13868 [2:49:17<04:59,  1.31it/s]Training epoch 1:  97% 13475/13868 [2:49:18<05:01,  1.30it/s]Training epoch 1:  97% 13476/13868 [2:49:19<04:54,  1.33it/s]Training epoch 1:  97% 13477/13868 [2:49:19<04:55,  1.32it/s]Training epoch 1:  97% 13478/13868 [2:49:20<04:55,  1.32it/s]Training epoch 1:  97% 13479/13868 [2:49:21<04:54,  1.32it/s]Training epoch 1:  97% 13480/13868 [2:49:22<04:50,  1.33it/s]Training epoch 1:  97% 13481/13868 [2:49:22<04:49,  1.34it/s]Training epoch 1:  97% 13482/13868 [2:49:23<04:48,  1.34it/s]Training epoch 1:  97% 13483/13868 [2:49:24<04:49,  1.33it/s]Training epoch 1:  97% 13484/13868 [2:49:25<04:49,  1.33it/s]Training epoch 1:  97% 13485/13868 [2:49:25<04:49,  1.32it/s]Training epoch 1:  97% 13486/13868 [2:49:26<04:47,  1.33it/s]Training epoch 1:  97% 13487/13868 [2:49:27<04:45,  1.33it/s]Training epoch 1:  97% 13488/13868 [2:49:28<04:43,  1.34it/s]Training epoch 1:  97% 13489/13868 [2:49:28<04:40,  1.35it/s]Training epoch 1:  97% 13490/13868 [2:49:29<04:39,  1.35it/s]Training epoch 1:  97% 13491/13868 [2:49:30<04:38,  1.35it/s]Training epoch 1:  97% 13492/13868 [2:49:31<04:37,  1.35it/s]Training epoch 1:  97% 13493/13868 [2:49:31<04:40,  1.34it/s]Training epoch 1:  97% 13494/13868 [2:49:32<04:36,  1.36it/s]Training epoch 1:  97% 13495/13868 [2:49:33<04:37,  1.34it/s]Training epoch 1:  97% 13496/13868 [2:49:34<04:40,  1.33it/s]Training epoch 1:  97% 13497/13868 [2:49:34<04:43,  1.31it/s]Training epoch 1:  97% 13498/13868 [2:49:35<04:43,  1.30it/s]Training epoch 1:  97% 13499/13868 [2:49:36<04:46,  1.29it/s]Training epoch 1:  97% 13500/13868 [2:49:37<05:08,  1.19it/s]Training epoch 1:  97% 13501/13868 [2:49:38<04:55,  1.24it/s]Training epoch 1:  97% 13502/13868 [2:49:38<04:51,  1.26it/s]Training epoch 1:  97% 13503/13868 [2:49:39<04:41,  1.30it/s]Training epoch 1:  97% 13504/13868 [2:49:40<04:36,  1.32it/s]Training epoch 1:  97% 13505/13868 [2:49:41<04:39,  1.30it/s]Training epoch 1:  97% 13506/13868 [2:49:41<04:33,  1.32it/s]Training epoch 1:  97% 13507/13868 [2:49:42<04:35,  1.31it/s]Training epoch 1:  97% 13508/13868 [2:49:43<04:34,  1.31it/s]Training epoch 1:  97% 13509/13868 [2:49:44<04:33,  1.31it/s]Training epoch 1:  97% 13510/13868 [2:49:44<04:29,  1.33it/s]Training epoch 1:  97% 13511/13868 [2:49:45<04:29,  1.33it/s]Training epoch 1:  97% 13512/13868 [2:49:46<04:23,  1.35it/s]Training epoch 1:  97% 13513/13868 [2:49:47<04:24,  1.34it/s]Training epoch 1:  97% 13514/13868 [2:49:47<04:23,  1.35it/s]Training epoch 1:  97% 13515/13868 [2:49:48<04:21,  1.35it/s]Training epoch 1:  97% 13516/13868 [2:49:49<04:19,  1.35it/s]Training epoch 1:  97% 13517/13868 [2:49:50<04:21,  1.34it/s]Training epoch 1:  97% 13518/13868 [2:49:50<04:21,  1.34it/s]Training epoch 1:  97% 13519/13868 [2:49:51<04:19,  1.34it/s]Training epoch 1:  97% 13520/13868 [2:49:52<04:17,  1.35it/s]Training epoch 1:  97% 13521/13868 [2:49:53<04:19,  1.33it/s]Training epoch 1:  98% 13522/13868 [2:49:53<04:21,  1.32it/s]Training epoch 1:  98% 13523/13868 [2:49:54<04:20,  1.32it/s]Training epoch 1:  98% 13524/13868 [2:49:55<04:17,  1.34it/s]Training epoch 1:  98% 13525/13868 [2:49:56<04:16,  1.34it/s]Training epoch 1:  98% 13526/13868 [2:49:56<04:17,  1.33it/s]Training epoch 1:  98% 13527/13868 [2:49:57<04:15,  1.33it/s]Training epoch 1:  98% 13528/13868 [2:49:58<04:17,  1.32it/s]Training epoch 1:  98% 13529/13868 [2:49:59<04:18,  1.31it/s]Training epoch 1:  98% 13530/13868 [2:49:59<04:18,  1.31it/s]Training epoch 1:  98% 13531/13868 [2:50:00<04:17,  1.31it/s]Training epoch 1:  98% 13532/13868 [2:50:01<04:16,  1.31it/s]Training epoch 1:  98% 13533/13868 [2:50:02<04:14,  1.32it/s]Training epoch 1:  98% 13534/13868 [2:50:03<04:14,  1.31it/s]Training epoch 1:  98% 13535/13868 [2:50:03<04:14,  1.31it/s]Training epoch 1:  98% 13536/13868 [2:50:04<04:10,  1.33it/s]Training epoch 1:  98% 13537/13868 [2:50:05<04:07,  1.34it/s]Training epoch 1:  98% 13538/13868 [2:50:05<04:05,  1.34it/s]Training epoch 1:  98% 13539/13868 [2:50:06<04:06,  1.34it/s]Training epoch 1:  98% 13540/13868 [2:50:07<04:05,  1.33it/s]Training epoch 1:  98% 13541/13868 [2:50:08<04:07,  1.32it/s]Training epoch 1:  98% 13542/13868 [2:50:08<04:03,  1.34it/s]Training epoch 1:  98% 13543/13868 [2:50:09<04:02,  1.34it/s]Training epoch 1:  98% 13544/13868 [2:50:10<04:05,  1.32it/s]Training epoch 1:  98% 13545/13868 [2:50:11<04:04,  1.32it/s]Training epoch 1:  98% 13546/13868 [2:50:11<04:01,  1.34it/s]Training epoch 1:  98% 13547/13868 [2:50:12<03:59,  1.34it/s]Training epoch 1:  98% 13548/13868 [2:50:13<03:59,  1.34it/s]Training epoch 1:  98% 13549/13868 [2:50:14<04:00,  1.33it/s]Training epoch 1:  98% 13550/13868 [2:50:14<03:58,  1.33it/s]Training epoch 1:  98% 13551/13868 [2:50:15<03:55,  1.35it/s]Training epoch 1:  98% 13552/13868 [2:50:16<03:57,  1.33it/s]Training epoch 1:  98% 13553/13868 [2:50:17<03:56,  1.33it/s]Training epoch 1:  98% 13554/13868 [2:50:18<03:57,  1.32it/s]Training epoch 1:  98% 13555/13868 [2:50:18<03:51,  1.35it/s]Training epoch 1:  98% 13556/13868 [2:50:19<03:53,  1.34it/s]Training epoch 1:  98% 13557/13868 [2:50:20<03:51,  1.34it/s]Training epoch 1:  98% 13558/13868 [2:50:20<03:51,  1.34it/s]Training epoch 1:  98% 13559/13868 [2:50:21<03:53,  1.32it/s]Training epoch 1:  98% 13560/13868 [2:50:22<03:51,  1.33it/s]Training epoch 1:  98% 13561/13868 [2:50:23<03:50,  1.33it/s]Training epoch 1:  98% 13562/13868 [2:50:24<03:52,  1.32it/s]Training epoch 1:  98% 13563/13868 [2:50:24<03:50,  1.33it/s]Training epoch 1:  98% 13564/13868 [2:50:25<03:49,  1.32it/s]Training epoch 1:  98% 13565/13868 [2:50:26<03:48,  1.33it/s]Training epoch 1:  98% 13566/13868 [2:50:27<03:50,  1.31it/s]Training epoch 1:  98% 13567/13868 [2:50:27<03:47,  1.32it/s]Training epoch 1:  98% 13568/13868 [2:50:28<03:46,  1.32it/s]Training epoch 1:  98% 13569/13868 [2:50:29<03:41,  1.35it/s]Training epoch 1:  98% 13570/13868 [2:50:30<03:43,  1.33it/s]Training epoch 1:  98% 13571/13868 [2:50:30<03:45,  1.31it/s]Training epoch 1:  98% 13572/13868 [2:50:31<03:44,  1.32it/s]Training epoch 1:  98% 13573/13868 [2:50:32<03:42,  1.33it/s]Training epoch 1:  98% 13574/13868 [2:50:33<03:41,  1.33it/s]Training epoch 1:  98% 13575/13868 [2:50:33<03:37,  1.34it/s]Training epoch 1:  98% 13576/13868 [2:50:34<03:36,  1.35it/s]Training epoch 1:  98% 13577/13868 [2:50:35<03:34,  1.35it/s]Training epoch 1:  98% 13578/13868 [2:50:35<03:34,  1.35it/s]Training epoch 1:  98% 13579/13868 [2:50:36<03:35,  1.34it/s]Training epoch 1:  98% 13580/13868 [2:50:37<03:36,  1.33it/s]Training epoch 1:  98% 13581/13868 [2:50:38<03:36,  1.33it/s]Training epoch 1:  98% 13582/13868 [2:50:39<03:35,  1.32it/s]Training epoch 1:  98% 13583/13868 [2:50:39<03:34,  1.33it/s]Training epoch 1:  98% 13584/13868 [2:50:40<03:33,  1.33it/s]Training epoch 1:  98% 13585/13868 [2:50:41<03:33,  1.32it/s]Training epoch 1:  98% 13586/13868 [2:50:42<03:36,  1.30it/s]Training epoch 1:  98% 13587/13868 [2:50:42<03:35,  1.30it/s]Training epoch 1:  98% 13588/13868 [2:50:43<03:35,  1.30it/s]Training epoch 1:  98% 13589/13868 [2:50:44<03:34,  1.30it/s]Training epoch 1:  98% 13590/13868 [2:50:45<03:35,  1.29it/s]Training epoch 1:  98% 13591/13868 [2:50:45<03:34,  1.29it/s]Training epoch 1:  98% 13592/13868 [2:50:46<03:33,  1.29it/s]Training epoch 1:  98% 13593/13868 [2:50:47<03:32,  1.30it/s]Training epoch 1:  98% 13594/13868 [2:50:48<03:32,  1.29it/s]Training epoch 1:  98% 13595/13868 [2:50:49<03:28,  1.31it/s]Training epoch 1:  98% 13596/13868 [2:50:49<03:28,  1.31it/s]Training epoch 1:  98% 13597/13868 [2:50:50<03:27,  1.31it/s]Training epoch 1:  98% 13598/13868 [2:50:51<03:25,  1.31it/s]Training epoch 1:  98% 13599/13868 [2:50:52<03:23,  1.32it/s]Training epoch 1:  98% 13600/13868 [2:50:52<03:33,  1.26it/s]Training epoch 1:  98% 13601/13868 [2:50:53<03:28,  1.28it/s]Training epoch 1:  98% 13602/13868 [2:50:54<03:23,  1.31it/s]Training epoch 1:  98% 13603/13868 [2:50:55<03:22,  1.31it/s]Training epoch 1:  98% 13604/13868 [2:50:55<03:22,  1.30it/s]Training epoch 1:  98% 13605/13868 [2:50:56<03:21,  1.31it/s]Training epoch 1:  98% 13606/13868 [2:50:57<03:18,  1.32it/s]Training epoch 1:  98% 13607/13868 [2:50:58<03:19,  1.31it/s]Training epoch 1:  98% 13608/13868 [2:50:59<03:20,  1.30it/s]Training epoch 1:  98% 13609/13868 [2:50:59<03:15,  1.32it/s]Training epoch 1:  98% 13610/13868 [2:51:00<03:15,  1.32it/s]Training epoch 1:  98% 13611/13868 [2:51:01<03:13,  1.33it/s]Training epoch 1:  98% 13612/13868 [2:51:01<03:10,  1.35it/s]Training epoch 1:  98% 13613/13868 [2:51:02<03:07,  1.36it/s]Training epoch 1:  98% 13614/13868 [2:51:03<03:11,  1.33it/s]Training epoch 1:  98% 13615/13868 [2:51:04<03:06,  1.35it/s]Training epoch 1:  98% 13616/13868 [2:51:04<03:07,  1.35it/s]Training epoch 1:  98% 13617/13868 [2:51:05<03:05,  1.35it/s]Training epoch 1:  98% 13618/13868 [2:51:06<03:04,  1.35it/s]Training epoch 1:  98% 13619/13868 [2:51:07<03:04,  1.35it/s]Training epoch 1:  98% 13620/13868 [2:51:07<03:02,  1.36it/s]Training epoch 1:  98% 13621/13868 [2:51:08<03:03,  1.35it/s]Training epoch 1:  98% 13622/13868 [2:51:09<03:03,  1.34it/s]Training epoch 1:  98% 13623/13868 [2:51:10<03:04,  1.33it/s]Training epoch 1:  98% 13624/13868 [2:51:10<03:02,  1.34it/s]Training epoch 1:  98% 13625/13868 [2:51:11<02:58,  1.36it/s]Training epoch 1:  98% 13626/13868 [2:51:12<02:56,  1.37it/s]Training epoch 1:  98% 13627/13868 [2:51:13<02:56,  1.37it/s]Training epoch 1:  98% 13628/13868 [2:51:13<02:57,  1.36it/s]Training epoch 1:  98% 13629/13868 [2:51:14<02:59,  1.33it/s]Training epoch 1:  98% 13630/13868 [2:51:15<02:58,  1.33it/s]Training epoch 1:  98% 13631/13868 [2:51:16<02:58,  1.33it/s]Training epoch 1:  98% 13632/13868 [2:51:16<02:57,  1.33it/s]Training epoch 1:  98% 13633/13868 [2:51:17<02:56,  1.33it/s]Training epoch 1:  98% 13634/13868 [2:51:18<02:56,  1.33it/s]Training epoch 1:  98% 13635/13868 [2:51:19<02:55,  1.33it/s]Training epoch 1:  98% 13636/13868 [2:51:19<02:53,  1.34it/s]Training epoch 1:  98% 13637/13868 [2:51:20<02:54,  1.33it/s]Training epoch 1:  98% 13638/13868 [2:51:21<02:52,  1.33it/s]Training epoch 1:  98% 13639/13868 [2:51:22<02:51,  1.33it/s]Training epoch 1:  98% 13640/13868 [2:51:22<02:51,  1.33it/s]Training epoch 1:  98% 13641/13868 [2:51:23<02:50,  1.33it/s]Training epoch 1:  98% 13642/13868 [2:51:24<02:49,  1.33it/s]Training epoch 1:  98% 13643/13868 [2:51:25<02:50,  1.32it/s]Training epoch 1:  98% 13644/13868 [2:51:25<02:49,  1.33it/s]Training epoch 1:  98% 13645/13868 [2:51:26<02:46,  1.34it/s]Training epoch 1:  98% 13646/13868 [2:51:27<02:44,  1.35it/s]Training epoch 1:  98% 13647/13868 [2:51:28<02:47,  1.32it/s]Training epoch 1:  98% 13648/13868 [2:51:28<02:47,  1.32it/s]Training epoch 1:  98% 13649/13868 [2:51:29<02:45,  1.33it/s]Training epoch 1:  98% 13650/13868 [2:51:30<02:44,  1.33it/s]Training epoch 1:  98% 13651/13868 [2:51:31<02:42,  1.33it/s]Training epoch 1:  98% 13652/13868 [2:51:31<02:40,  1.34it/s]Training epoch 1:  98% 13653/13868 [2:51:32<02:40,  1.34it/s]Training epoch 1:  98% 13654/13868 [2:51:33<02:38,  1.35it/s]Training epoch 1:  98% 13655/13868 [2:51:34<02:37,  1.35it/s]Training epoch 1:  98% 13656/13868 [2:51:34<02:37,  1.34it/s]Training epoch 1:  98% 13657/13868 [2:51:35<02:35,  1.35it/s]Training epoch 1:  98% 13658/13868 [2:51:36<02:37,  1.33it/s]Training epoch 1:  98% 13659/13868 [2:51:37<02:37,  1.33it/s]Training epoch 1:  99% 13660/13868 [2:51:37<02:36,  1.33it/s]Training epoch 1:  99% 13661/13868 [2:51:38<02:36,  1.32it/s]Training epoch 1:  99% 13662/13868 [2:51:39<02:35,  1.32it/s]Training epoch 1:  99% 13663/13868 [2:51:40<02:36,  1.31it/s]Training epoch 1:  99% 13664/13868 [2:51:40<02:36,  1.30it/s]Training epoch 1:  99% 13665/13868 [2:51:41<02:36,  1.30it/s]Training epoch 1:  99% 13666/13868 [2:51:42<02:32,  1.33it/s]Training epoch 1:  99% 13667/13868 [2:51:43<02:28,  1.35it/s]Training epoch 1:  99% 13668/13868 [2:51:43<02:31,  1.32it/s]Training epoch 1:  99% 13669/13868 [2:51:44<02:30,  1.32it/s]Training epoch 1:  99% 13670/13868 [2:51:45<02:30,  1.31it/s]Training epoch 1:  99% 13671/13868 [2:51:46<02:29,  1.32it/s]Training epoch 1:  99% 13672/13868 [2:51:46<02:27,  1.33it/s]Training epoch 1:  99% 13673/13868 [2:51:47<02:25,  1.34it/s]Training epoch 1:  99% 13674/13868 [2:51:48<02:25,  1.34it/s]Training epoch 1:  99% 13675/13868 [2:51:49<02:25,  1.33it/s]Training epoch 1:  99% 13676/13868 [2:51:49<02:23,  1.33it/s]Training epoch 1:  99% 13677/13868 [2:51:50<02:23,  1.33it/s]Training epoch 1:  99% 13678/13868 [2:51:51<02:22,  1.33it/s]Training epoch 1:  99% 13679/13868 [2:51:52<02:21,  1.33it/s]Training epoch 1:  99% 13680/13868 [2:51:52<02:21,  1.33it/s]Training epoch 1:  99% 13681/13868 [2:51:53<02:20,  1.33it/s]Training epoch 1:  99% 13682/13868 [2:51:54<02:19,  1.34it/s]Training epoch 1:  99% 13683/13868 [2:51:55<02:17,  1.34it/s]Training epoch 1:  99% 13684/13868 [2:51:55<02:19,  1.32it/s]Training epoch 1:  99% 13685/13868 [2:51:56<02:18,  1.32it/s]Training epoch 1:  99% 13686/13868 [2:51:57<02:17,  1.32it/s]Training epoch 1:  99% 13687/13868 [2:51:58<02:15,  1.33it/s]Training epoch 1:  99% 13688/13868 [2:51:58<02:13,  1.35it/s]Training epoch 1:  99% 13689/13868 [2:51:59<02:13,  1.34it/s]Training epoch 1:  99% 13690/13868 [2:52:00<02:13,  1.34it/s]Training epoch 1:  99% 13691/13868 [2:52:01<02:13,  1.33it/s]Training epoch 1:  99% 13692/13868 [2:52:01<02:12,  1.33it/s]Training epoch 1:  99% 13693/13868 [2:52:02<02:11,  1.33it/s]Training epoch 1:  99% 13694/13868 [2:52:03<02:11,  1.33it/s]Training epoch 1:  99% 13695/13868 [2:52:04<02:11,  1.32it/s]Training epoch 1:  99% 13696/13868 [2:52:04<02:09,  1.33it/s]Training epoch 1:  99% 13697/13868 [2:52:05<02:07,  1.34it/s]Training epoch 1:  99% 13698/13868 [2:52:06<02:07,  1.33it/s]Training epoch 1:  99% 13699/13868 [2:52:07<02:06,  1.33it/s]Training epoch 1:  99% 13700/13868 [2:52:08<02:12,  1.27it/s]Training epoch 1:  99% 13701/13868 [2:52:08<02:10,  1.28it/s]Training epoch 1:  99% 13702/13868 [2:52:09<02:06,  1.31it/s]Training epoch 1:  99% 13703/13868 [2:52:10<02:05,  1.32it/s]Training epoch 1:  99% 13704/13868 [2:52:11<02:05,  1.31it/s]Training epoch 1:  99% 13705/13868 [2:52:11<02:03,  1.32it/s]Training epoch 1:  99% 13706/13868 [2:52:12<02:02,  1.32it/s]Training epoch 1:  99% 13707/13868 [2:52:13<02:03,  1.31it/s]Training epoch 1:  99% 13708/13868 [2:52:14<02:01,  1.32it/s]Training epoch 1:  99% 13709/13868 [2:52:14<02:02,  1.30it/s]Training epoch 1:  99% 13710/13868 [2:52:15<01:59,  1.32it/s]Training epoch 1:  99% 13711/13868 [2:52:16<01:58,  1.33it/s]Training epoch 1:  99% 13712/13868 [2:52:17<01:57,  1.32it/s]Training epoch 1:  99% 13713/13868 [2:52:17<01:58,  1.31it/s]Training epoch 1:  99% 13714/13868 [2:52:18<01:57,  1.31it/s]Training epoch 1:  99% 13715/13868 [2:52:19<01:58,  1.30it/s]Training epoch 1:  99% 13716/13868 [2:52:20<01:57,  1.30it/s]Training epoch 1:  99% 13717/13868 [2:52:20<01:53,  1.33it/s]Training epoch 1:  99% 13718/13868 [2:52:21<01:52,  1.34it/s]Training epoch 1:  99% 13719/13868 [2:52:22<01:51,  1.34it/s]Training epoch 1:  99% 13720/13868 [2:52:23<01:50,  1.34it/s]Training epoch 1:  99% 13721/13868 [2:52:23<01:50,  1.33it/s]Training epoch 1:  99% 13722/13868 [2:52:24<01:49,  1.33it/s]Training epoch 1:  99% 13723/13868 [2:52:25<01:48,  1.34it/s]Training epoch 1:  99% 13724/13868 [2:52:26<01:48,  1.33it/s]Training epoch 1:  99% 13725/13868 [2:52:26<01:47,  1.34it/s]Training epoch 1:  99% 13726/13868 [2:52:27<01:47,  1.32it/s]Training epoch 1:  99% 13727/13868 [2:52:28<01:45,  1.33it/s]Training epoch 1:  99% 13728/13868 [2:52:29<01:45,  1.32it/s]Training epoch 1:  99% 13729/13868 [2:52:29<01:44,  1.33it/s]Training epoch 1:  99% 13730/13868 [2:52:30<01:44,  1.33it/s]Training epoch 1:  99% 13731/13868 [2:52:31<01:42,  1.34it/s]Training epoch 1:  99% 13732/13868 [2:52:32<01:42,  1.33it/s]Training epoch 1:  99% 13733/13868 [2:52:32<01:40,  1.34it/s]Training epoch 1:  99% 13734/13868 [2:52:33<01:41,  1.32it/s]Training epoch 1:  99% 13735/13868 [2:52:34<01:39,  1.33it/s]Training epoch 1:  99% 13736/13868 [2:52:35<01:38,  1.34it/s]Training epoch 1:  99% 13737/13868 [2:52:35<01:36,  1.36it/s]Training epoch 1:  99% 13738/13868 [2:52:36<01:36,  1.35it/s]Training epoch 1:  99% 13739/13868 [2:52:37<01:36,  1.34it/s]Training epoch 1:  99% 13740/13868 [2:52:38<01:35,  1.34it/s]Training epoch 1:  99% 13741/13868 [2:52:38<01:33,  1.35it/s]Training epoch 1:  99% 13742/13868 [2:52:39<01:33,  1.35it/s]Training epoch 1:  99% 13743/13868 [2:52:40<01:33,  1.34it/s]Training epoch 1:  99% 13744/13868 [2:52:41<01:33,  1.33it/s]Training epoch 1:  99% 13745/13868 [2:52:41<01:33,  1.32it/s]Training epoch 1:  99% 13746/13868 [2:52:42<01:33,  1.31it/s]Training epoch 1:  99% 13747/13868 [2:52:43<01:32,  1.31it/s]Training epoch 1:  99% 13748/13868 [2:52:44<01:30,  1.33it/s]Training epoch 1:  99% 13749/13868 [2:52:44<01:29,  1.33it/s]Training epoch 1:  99% 13750/13868 [2:52:45<01:28,  1.33it/s]Training epoch 1:  99% 13751/13868 [2:52:46<01:27,  1.34it/s]Training epoch 1:  99% 13752/13868 [2:52:47<01:25,  1.35it/s]Training epoch 1:  99% 13753/13868 [2:52:47<01:25,  1.34it/s]Training epoch 1:  99% 13754/13868 [2:52:48<01:24,  1.35it/s]Training epoch 1:  99% 13755/13868 [2:52:49<01:24,  1.33it/s]Training epoch 1:  99% 13756/13868 [2:52:50<01:24,  1.32it/s]Training epoch 1:  99% 13757/13868 [2:52:50<01:23,  1.33it/s]Training epoch 1:  99% 13758/13868 [2:52:51<01:23,  1.32it/s]Training epoch 1:  99% 13759/13868 [2:52:52<01:23,  1.30it/s]Training epoch 1:  99% 13760/13868 [2:52:53<01:22,  1.31it/s]Training epoch 1:  99% 13761/13868 [2:52:53<01:21,  1.32it/s]Training epoch 1:  99% 13762/13868 [2:52:54<01:18,  1.35it/s]Training epoch 1:  99% 13763/13868 [2:52:55<01:17,  1.36it/s]Training epoch 1:  99% 13764/13868 [2:52:56<01:17,  1.34it/s]Training epoch 1:  99% 13765/13868 [2:52:56<01:17,  1.33it/s]Training epoch 1:  99% 13766/13868 [2:52:57<01:16,  1.33it/s]Training epoch 1:  99% 13767/13868 [2:52:58<01:16,  1.33it/s]Training epoch 1:  99% 13768/13868 [2:52:59<01:15,  1.32it/s]Training epoch 1:  99% 13769/13868 [2:52:59<01:14,  1.33it/s]Training epoch 1:  99% 13770/13868 [2:53:00<01:14,  1.32it/s]Training epoch 1:  99% 13771/13868 [2:53:01<01:13,  1.32it/s]Training epoch 1:  99% 13772/13868 [2:53:02<01:12,  1.33it/s]Training epoch 1:  99% 13773/13868 [2:53:03<01:11,  1.32it/s]Training epoch 1:  99% 13774/13868 [2:53:03<01:11,  1.31it/s]Training epoch 1:  99% 13775/13868 [2:53:04<01:09,  1.34it/s]Training epoch 1:  99% 13776/13868 [2:53:05<01:08,  1.34it/s]Training epoch 1:  99% 13777/13868 [2:53:06<01:08,  1.33it/s]Training epoch 1:  99% 13778/13868 [2:53:06<01:07,  1.33it/s]Training epoch 1:  99% 13779/13868 [2:53:07<01:06,  1.33it/s]Training epoch 1:  99% 13780/13868 [2:53:08<01:06,  1.32it/s]Training epoch 1:  99% 13781/13868 [2:53:09<01:05,  1.33it/s]Training epoch 1:  99% 13782/13868 [2:53:09<01:05,  1.32it/s]Training epoch 1:  99% 13783/13868 [2:53:10<01:04,  1.32it/s]Training epoch 1:  99% 13784/13868 [2:53:11<01:02,  1.34it/s]Training epoch 1:  99% 13785/13868 [2:53:12<01:02,  1.33it/s]Training epoch 1:  99% 13786/13868 [2:53:12<01:02,  1.32it/s]Training epoch 1:  99% 13787/13868 [2:53:13<01:00,  1.34it/s]Training epoch 1:  99% 13788/13868 [2:53:14<01:00,  1.32it/s]Training epoch 1:  99% 13789/13868 [2:53:15<00:59,  1.32it/s]Training epoch 1:  99% 13790/13868 [2:53:15<00:58,  1.33it/s]Training epoch 1:  99% 13791/13868 [2:53:16<00:56,  1.36it/s]Training epoch 1:  99% 13792/13868 [2:53:17<00:57,  1.33it/s]Training epoch 1:  99% 13793/13868 [2:53:18<00:55,  1.35it/s]Training epoch 1:  99% 13794/13868 [2:53:18<00:55,  1.33it/s]Training epoch 1:  99% 13795/13868 [2:53:19<00:54,  1.34it/s]Training epoch 1:  99% 13796/13868 [2:53:20<00:53,  1.34it/s]Training epoch 1:  99% 13797/13868 [2:53:21<00:54,  1.31it/s]Training epoch 1:  99% 13798/13868 [2:53:21<00:52,  1.32it/s]Training epoch 1: 100% 13799/13868 [2:53:22<00:51,  1.33it/s]Training epoch 1: 100% 13800/13868 [2:53:23<00:54,  1.26it/s]Training epoch 1: 100% 13801/13868 [2:53:24<00:51,  1.30it/s]Training epoch 1: 100% 13802/13868 [2:53:24<00:50,  1.30it/s]Training epoch 1: 100% 13803/13868 [2:53:25<00:49,  1.30it/s]Training epoch 1: 100% 13804/13868 [2:53:26<00:48,  1.31it/s]Training epoch 1: 100% 13805/13868 [2:53:27<00:47,  1.32it/s]Training epoch 1: 100% 13806/13868 [2:53:27<00:46,  1.33it/s]Training epoch 1: 100% 13807/13868 [2:53:28<00:45,  1.33it/s]Training epoch 1: 100% 13808/13868 [2:53:29<00:45,  1.32it/s]Training epoch 1: 100% 13809/13868 [2:53:30<00:43,  1.35it/s]Training epoch 1: 100% 13810/13868 [2:53:30<00:43,  1.34it/s]Training epoch 1: 100% 13811/13868 [2:53:31<00:42,  1.35it/s]Training epoch 1: 100% 13812/13868 [2:53:32<00:41,  1.35it/s]Training epoch 1: 100% 13813/13868 [2:53:33<00:40,  1.35it/s]Training epoch 1: 100% 13814/13868 [2:53:33<00:39,  1.38it/s]Training epoch 1: 100% 13815/13868 [2:53:34<00:38,  1.36it/s]Training epoch 1: 100% 13816/13868 [2:53:35<00:38,  1.35it/s]Training epoch 1: 100% 13817/13868 [2:53:36<00:37,  1.34it/s]Training epoch 1: 100% 13818/13868 [2:53:36<00:37,  1.35it/s]Training epoch 1: 100% 13819/13868 [2:53:37<00:36,  1.34it/s]Training epoch 1: 100% 13820/13868 [2:53:38<00:35,  1.35it/s]Training epoch 1: 100% 13821/13868 [2:53:39<00:35,  1.33it/s]Training epoch 1: 100% 13822/13868 [2:53:39<00:34,  1.35it/s]Training epoch 1: 100% 13823/13868 [2:53:40<00:33,  1.35it/s]Training epoch 1: 100% 13824/13868 [2:53:41<00:32,  1.35it/s]Training epoch 1: 100% 13825/13868 [2:53:41<00:31,  1.35it/s]Training epoch 1: 100% 13826/13868 [2:53:42<00:31,  1.33it/s]Training epoch 1: 100% 13827/13868 [2:53:43<00:30,  1.35it/s]Training epoch 1: 100% 13828/13868 [2:53:44<00:29,  1.35it/s]Training epoch 1: 100% 13829/13868 [2:53:44<00:28,  1.35it/s]Training epoch 1: 100% 13830/13868 [2:53:45<00:28,  1.34it/s]Training epoch 1: 100% 13831/13868 [2:53:46<00:28,  1.32it/s]Training epoch 1: 100% 13832/13868 [2:53:47<00:27,  1.33it/s]Training epoch 1: 100% 13833/13868 [2:53:47<00:26,  1.33it/s]Training epoch 1: 100% 13834/13868 [2:53:48<00:26,  1.30it/s]Training epoch 1: 100% 13835/13868 [2:53:49<00:24,  1.32it/s]Training epoch 1: 100% 13836/13868 [2:53:50<00:24,  1.32it/s]Training epoch 1: 100% 13837/13868 [2:53:51<00:23,  1.33it/s]Training epoch 1: 100% 13838/13868 [2:53:51<00:22,  1.34it/s]Training epoch 1: 100% 13839/13868 [2:53:52<00:21,  1.34it/s]Training epoch 1: 100% 13840/13868 [2:53:53<00:20,  1.34it/s]Training epoch 1: 100% 13841/13868 [2:53:53<00:20,  1.34it/s]Training epoch 1: 100% 13842/13868 [2:53:54<00:19,  1.34it/s]Training epoch 1: 100% 13843/13868 [2:53:55<00:18,  1.33it/s]Training epoch 1: 100% 13844/13868 [2:53:56<00:18,  1.33it/s]Training epoch 1: 100% 13845/13868 [2:53:57<00:17,  1.33it/s]Training epoch 1: 100% 13846/13868 [2:53:57<00:16,  1.32it/s]Training epoch 1: 100% 13847/13868 [2:53:58<00:15,  1.32it/s]Training epoch 1: 100% 13848/13868 [2:53:59<00:14,  1.35it/s]Training epoch 1: 100% 13849/13868 [2:53:59<00:14,  1.34it/s]Training epoch 1: 100% 13850/13868 [2:54:00<00:13,  1.33it/s]Training epoch 1: 100% 13851/13868 [2:54:01<00:12,  1.34it/s]Training epoch 1: 100% 13852/13868 [2:54:02<00:11,  1.36it/s]Training epoch 1: 100% 13853/13868 [2:54:02<00:11,  1.35it/s]Training epoch 1: 100% 13854/13868 [2:54:03<00:10,  1.37it/s]Training epoch 1: 100% 13855/13868 [2:54:04<00:09,  1.37it/s]Training epoch 1: 100% 13856/13868 [2:54:05<00:08,  1.35it/s]Training epoch 1: 100% 13857/13868 [2:54:05<00:08,  1.35it/s]Training epoch 1: 100% 13858/13868 [2:54:06<00:07,  1.33it/s]Training epoch 1: 100% 13859/13868 [2:54:07<00:06,  1.33it/s]Training epoch 1: 100% 13860/13868 [2:54:08<00:05,  1.34it/s]Training epoch 1: 100% 13861/13868 [2:54:08<00:05,  1.32it/s]Training epoch 1: 100% 13862/13868 [2:54:09<00:04,  1.31it/s]Training epoch 1: 100% 13863/13868 [2:54:10<00:03,  1.33it/s]Training epoch 1: 100% 13864/13868 [2:54:11<00:02,  1.34it/s]Training epoch 1: 100% 13865/13868 [2:54:11<00:02,  1.37it/s]Training epoch 1: 100% 13866/13868 [2:54:12<00:01,  1.37it/s]Training epoch 1: 100% 13867/13868 [2:54:13<00:00,  1.36it/s]Training epoch 1: 100% 13868/13868 [2:54:13<00:00,  1.66it/s]Training epoch 1: 100% 13868/13868 [2:54:13<00:00,  1.33it/s]
Evaluating on VQA val set:   0% 0/6699 [00:00<?, ?it/s]Evaluating on VQA val set:   0% 1/6699 [00:01<2:27:55,  1.33s/it]Evaluating on VQA val set:   0% 2/6699 [00:02<1:45:33,  1.06it/s]Evaluating on VQA val set:   0% 3/6699 [00:02<1:34:27,  1.18it/s]Evaluating on VQA val set:   0% 4/6699 [00:03<1:30:56,  1.23it/s]Evaluating on VQA val set:   0% 5/6699 [00:04<1:26:06,  1.30it/s]Evaluating on VQA val set:   0% 6/6699 [00:04<1:23:16,  1.34it/s]Evaluating on VQA val set:   0% 7/6699 [00:05<1:23:23,  1.34it/s]Evaluating on VQA val set:   0% 8/6699 [00:06<1:19:56,  1.40it/s]Evaluating on VQA val set:   0% 9/6699 [00:07<1:19:47,  1.40it/s]Evaluating on VQA val set:   0% 10/6699 [00:07<1:18:01,  1.43it/s]Evaluating on VQA val set:   0% 11/6699 [00:08<1:17:00,  1.45it/s]Evaluating on VQA val set:   0% 12/6699 [00:09<1:17:37,  1.44it/s]Evaluating on VQA val set:   0% 13/6699 [00:09<1:19:21,  1.40it/s]Evaluating on VQA val set:   0% 14/6699 [00:10<1:17:34,  1.44it/s]Evaluating on VQA val set:   0% 15/6699 [00:11<1:16:25,  1.46it/s]Evaluating on VQA val set:   0% 16/6699 [00:11<1:16:47,  1.45it/s]Evaluating on VQA val set:   0% 17/6699 [00:12<1:15:28,  1.48it/s]Evaluating on VQA val set:   0% 18/6699 [00:13<1:16:03,  1.46it/s]Evaluating on VQA val set:   0% 19/6699 [00:13<1:15:33,  1.47it/s]Evaluating on VQA val set:   0% 20/6699 [00:14<1:20:11,  1.39it/s]Evaluating on VQA val set:   0% 21/6699 [00:15<1:21:53,  1.36it/s]Evaluating on VQA val set:   0% 22/6699 [00:16<1:21:06,  1.37it/s]Evaluating on VQA val set:   0% 23/6699 [00:16<1:20:50,  1.38it/s]Evaluating on VQA val set:   0% 24/6699 [00:17<1:20:18,  1.39it/s]Evaluating on VQA val set:   0% 25/6699 [00:18<1:20:16,  1.39it/s]Evaluating on VQA val set:   0% 26/6699 [00:19<1:21:24,  1.37it/s]Evaluating on VQA val set:   0% 27/6699 [00:19<1:22:52,  1.34it/s]Evaluating on VQA val set:   0% 28/6699 [00:20<1:22:02,  1.36it/s]Evaluating on VQA val set:   0% 29/6699 [00:21<1:22:06,  1.35it/s]Evaluating on VQA val set:   0% 30/6699 [00:21<1:20:20,  1.38it/s]Evaluating on VQA val set:   0% 31/6699 [00:22<1:17:56,  1.43it/s]Evaluating on VQA val set:   0% 32/6699 [00:23<1:18:48,  1.41it/s]Evaluating on VQA val set:   0% 33/6699 [00:24<1:20:29,  1.38it/s]Evaluating on VQA val set:   1% 34/6699 [00:24<1:19:36,  1.40it/s]Evaluating on VQA val set:   1% 35/6699 [00:25<1:18:22,  1.42it/s]Evaluating on VQA val set:   1% 36/6699 [00:26<1:16:46,  1.45it/s]Evaluating on VQA val set:   1% 37/6699 [00:26<1:17:29,  1.43it/s]Evaluating on VQA val set:   1% 38/6699 [00:27<1:19:07,  1.40it/s]Evaluating on VQA val set:   1% 39/6699 [00:28<1:19:48,  1.39it/s]Evaluating on VQA val set:   1% 40/6699 [00:29<1:21:44,  1.36it/s]Evaluating on VQA val set:   1% 41/6699 [00:29<1:20:25,  1.38it/s]Evaluating on VQA val set:   1% 42/6699 [00:30<1:18:37,  1.41it/s]Evaluating on VQA val set:   1% 43/6699 [00:31<1:18:37,  1.41it/s]Evaluating on VQA val set:   1% 44/6699 [00:31<1:20:29,  1.38it/s]Evaluating on VQA val set:   1% 45/6699 [00:32<1:21:53,  1.35it/s]Evaluating on VQA val set:   1% 46/6699 [00:33<1:19:44,  1.39it/s]Evaluating on VQA val set:   1% 47/6699 [00:34<1:21:05,  1.37it/s]Evaluating on VQA val set:   1% 48/6699 [00:34<1:19:22,  1.40it/s]Evaluating on VQA val set:   1% 49/6699 [00:35<1:18:42,  1.41it/s]Evaluating on VQA val set:   1% 50/6699 [00:36<1:17:32,  1.43it/s]Evaluating on VQA val set:   1% 51/6699 [00:36<1:18:31,  1.41it/s]Evaluating on VQA val set:   1% 52/6699 [00:37<1:16:34,  1.45it/s]Evaluating on VQA val set:   1% 53/6699 [00:38<1:16:01,  1.46it/s]Evaluating on VQA val set:   1% 54/6699 [00:39<1:18:18,  1.41it/s]Evaluating on VQA val set:   1% 55/6699 [00:39<1:20:22,  1.38it/s]Evaluating on VQA val set:   1% 56/6699 [00:40<1:21:19,  1.36it/s]Evaluating on VQA val set:   1% 57/6699 [00:41<1:20:39,  1.37it/s]Evaluating on VQA val set:   1% 58/6699 [00:41<1:20:23,  1.38it/s]Evaluating on VQA val set:   1% 59/6699 [00:42<1:19:51,  1.39it/s]Evaluating on VQA val set:   1% 60/6699 [00:43<1:17:52,  1.42it/s]Evaluating on VQA val set:   1% 61/6699 [00:44<1:18:37,  1.41it/s]Evaluating on VQA val set:   1% 62/6699 [00:44<1:19:09,  1.40it/s]Evaluating on VQA val set:   1% 63/6699 [00:45<1:19:00,  1.40it/s]Evaluating on VQA val set:   1% 64/6699 [00:46<1:20:15,  1.38it/s]Evaluating on VQA val set:   1% 65/6699 [00:46<1:20:14,  1.38it/s]Evaluating on VQA val set:   1% 66/6699 [00:47<1:23:00,  1.33it/s]Evaluating on VQA val set:   1% 67/6699 [00:48<1:19:01,  1.40it/s]Evaluating on VQA val set:   1% 68/6699 [00:49<1:18:19,  1.41it/s]Evaluating on VQA val set:   1% 69/6699 [00:49<1:20:41,  1.37it/s]Evaluating on VQA val set:   1% 70/6699 [00:50<1:19:30,  1.39it/s]Evaluating on VQA val set:   1% 71/6699 [00:51<1:20:35,  1.37it/s]Evaluating on VQA val set:   1% 72/6699 [00:52<1:21:03,  1.36it/s]Evaluating on VQA val set:   1% 73/6699 [00:52<1:18:37,  1.40it/s]Evaluating on VQA val set:   1% 74/6699 [00:53<1:19:10,  1.39it/s]Evaluating on VQA val set:   1% 75/6699 [00:54<1:19:29,  1.39it/s]Evaluating on VQA val set:   1% 76/6699 [00:54<1:19:41,  1.39it/s]Evaluating on VQA val set:   1% 77/6699 [00:55<1:20:51,  1.36it/s]Evaluating on VQA val set:   1% 78/6699 [00:56<1:20:07,  1.38it/s]Evaluating on VQA val set:   1% 79/6699 [00:57<1:17:20,  1.43it/s]Evaluating on VQA val set:   1% 80/6699 [00:57<1:18:32,  1.40it/s]Evaluating on VQA val set:   1% 81/6699 [00:58<1:18:31,  1.40it/s]Evaluating on VQA val set:   1% 82/6699 [00:59<1:16:09,  1.45it/s]Evaluating on VQA val set:   1% 83/6699 [00:59<1:13:24,  1.50it/s]Evaluating on VQA val set:   1% 84/6699 [01:00<1:15:15,  1.46it/s]Evaluating on VQA val set:   1% 85/6699 [01:01<1:13:26,  1.50it/s]Evaluating on VQA val set:   1% 86/6699 [01:01<1:12:08,  1.53it/s]Evaluating on VQA val set:   1% 87/6699 [01:02<1:10:23,  1.57it/s]Evaluating on VQA val set:   1% 88/6699 [01:02<1:07:05,  1.64it/s]Evaluating on VQA val set:   1% 89/6699 [01:03<1:09:30,  1.58it/s]Evaluating on VQA val set:   1% 90/6699 [01:04<1:13:28,  1.50it/s]Evaluating on VQA val set:   1% 91/6699 [01:05<1:14:24,  1.48it/s]Evaluating on VQA val set:   1% 92/6699 [01:05<1:15:38,  1.46it/s]Evaluating on VQA val set:   1% 93/6699 [01:06<1:16:50,  1.43it/s]Evaluating on VQA val set:   1% 94/6699 [01:07<1:16:08,  1.45it/s]Evaluating on VQA val set:   1% 95/6699 [01:07<1:15:31,  1.46it/s]Evaluating on VQA val set:   1% 96/6699 [01:08<1:16:50,  1.43it/s]Evaluating on VQA val set:   1% 97/6699 [01:09<1:16:35,  1.44it/s]Evaluating on VQA val set:   1% 98/6699 [01:09<1:17:59,  1.41it/s]Evaluating on VQA val set:   1% 99/6699 [01:10<1:19:04,  1.39it/s]Evaluating on VQA val set:   1% 100/6699 [01:11<1:16:48,  1.43it/s]Evaluating on VQA val set:   2% 101/6699 [01:12<1:18:33,  1.40it/s]Evaluating on VQA val set:   2% 102/6699 [01:12<1:18:13,  1.41it/s]Evaluating on VQA val set:   2% 103/6699 [01:13<1:16:19,  1.44it/s]Evaluating on VQA val set:   2% 104/6699 [01:14<1:16:28,  1.44it/s]Evaluating on VQA val set:   2% 105/6699 [01:14<1:17:40,  1.41it/s]Evaluating on VQA val set:   2% 106/6699 [01:15<1:18:07,  1.41it/s]Evaluating on VQA val set:   2% 107/6699 [01:16<1:18:47,  1.39it/s]Evaluating on VQA val set:   2% 108/6699 [01:17<1:17:39,  1.41it/s]Evaluating on VQA val set:   2% 109/6699 [01:17<1:16:38,  1.43it/s]Evaluating on VQA val set:   2% 110/6699 [01:18<1:16:51,  1.43it/s]Evaluating on VQA val set:   2% 111/6699 [01:19<1:16:17,  1.44it/s]Evaluating on VQA val set:   2% 112/6699 [01:19<1:16:25,  1.44it/s]Evaluating on VQA val set:   2% 113/6699 [01:20<1:10:54,  1.55it/s]Evaluating on VQA val set:   2% 114/6699 [01:20<1:10:46,  1.55it/s]Evaluating on VQA val set:   2% 115/6699 [01:21<1:16:30,  1.43it/s]Evaluating on VQA val set:   2% 116/6699 [01:22<1:17:55,  1.41it/s]Evaluating on VQA val set:   2% 117/6699 [01:23<1:21:12,  1.35it/s]Evaluating on VQA val set:   2% 118/6699 [01:23<1:18:56,  1.39it/s]Evaluating on VQA val set:   2% 119/6699 [01:24<1:18:30,  1.40it/s]Evaluating on VQA val set:   2% 120/6699 [01:25<1:18:01,  1.41it/s]Evaluating on VQA val set:   2% 121/6699 [01:26<1:19:35,  1.38it/s]Evaluating on VQA val set:   2% 122/6699 [01:26<1:20:24,  1.36it/s]Evaluating on VQA val set:   2% 123/6699 [01:27<1:18:07,  1.40it/s]Evaluating on VQA val set:   2% 124/6699 [01:28<1:17:36,  1.41it/s]Evaluating on VQA val set:   2% 125/6699 [01:28<1:15:47,  1.45it/s]Evaluating on VQA val set:   2% 126/6699 [01:29<1:16:22,  1.43it/s]Evaluating on VQA val set:   2% 127/6699 [01:30<1:18:54,  1.39it/s]Evaluating on VQA val set:   2% 128/6699 [01:31<1:18:12,  1.40it/s]Evaluating on VQA val set:   2% 129/6699 [01:31<1:17:41,  1.41it/s]Evaluating on VQA val set:   2% 130/6699 [01:32<1:14:26,  1.47it/s]Evaluating on VQA val set:   2% 131/6699 [01:33<1:13:58,  1.48it/s]Evaluating on VQA val set:   2% 132/6699 [01:33<1:14:51,  1.46it/s]Evaluating on VQA val set:   2% 133/6699 [01:34<1:16:35,  1.43it/s]Evaluating on VQA val set:   2% 134/6699 [01:35<1:17:53,  1.40it/s]Evaluating on VQA val set:   2% 135/6699 [01:36<1:19:13,  1.38it/s]Evaluating on VQA val set:   2% 136/6699 [01:36<1:15:22,  1.45it/s]Evaluating on VQA val set:   2% 137/6699 [01:37<1:13:47,  1.48it/s]Evaluating on VQA val set:   2% 138/6699 [01:37<1:15:00,  1.46it/s]Evaluating on VQA val set:   2% 139/6699 [01:38<1:14:11,  1.47it/s]Evaluating on VQA val set:   2% 140/6699 [01:39<1:15:51,  1.44it/s]Evaluating on VQA val set:   2% 141/6699 [01:40<1:19:11,  1.38it/s]Evaluating on VQA val set:   2% 142/6699 [01:40<1:21:32,  1.34it/s]Evaluating on VQA val set:   2% 143/6699 [01:41<1:20:02,  1.37it/s]Evaluating on VQA val set:   2% 144/6699 [01:42<1:19:26,  1.38it/s]Evaluating on VQA val set:   2% 145/6699 [01:43<1:16:44,  1.42it/s]Evaluating on VQA val set:   2% 146/6699 [01:43<1:14:58,  1.46it/s]Evaluating on VQA val set:   2% 147/6699 [01:44<1:16:28,  1.43it/s]Evaluating on VQA val set:   2% 148/6699 [01:45<1:15:02,  1.46it/s]Evaluating on VQA val set:   2% 149/6699 [01:45<1:15:34,  1.44it/s]Evaluating on VQA val set:   2% 150/6699 [01:46<1:15:51,  1.44it/s]Evaluating on VQA val set:   2% 151/6699 [01:47<1:15:20,  1.45it/s]Evaluating on VQA val set:   2% 152/6699 [01:47<1:14:31,  1.46it/s]Evaluating on VQA val set:   2% 153/6699 [01:48<1:16:11,  1.43it/s]Evaluating on VQA val set:   2% 154/6699 [01:49<1:16:13,  1.43it/s]Evaluating on VQA val set:   2% 155/6699 [01:49<1:14:50,  1.46it/s]Evaluating on VQA val set:   2% 156/6699 [01:50<1:15:50,  1.44it/s]Evaluating on VQA val set:   2% 157/6699 [01:51<1:17:43,  1.40it/s]Evaluating on VQA val set:   2% 158/6699 [01:52<1:16:11,  1.43it/s]Evaluating on VQA val set:   2% 159/6699 [01:52<1:16:09,  1.43it/s]Evaluating on VQA val set:   2% 160/6699 [01:53<1:17:31,  1.41it/s]Evaluating on VQA val set:   2% 161/6699 [01:54<1:16:56,  1.42it/s]Evaluating on VQA val set:   2% 162/6699 [01:54<1:16:17,  1.43it/s]Evaluating on VQA val set:   2% 163/6699 [01:55<1:17:31,  1.41it/s]Evaluating on VQA val set:   2% 164/6699 [01:56<1:19:01,  1.38it/s]Evaluating on VQA val set:   2% 165/6699 [01:57<1:18:42,  1.38it/s]Evaluating on VQA val set:   2% 166/6699 [01:57<1:18:07,  1.39it/s]Evaluating on VQA val set:   2% 167/6699 [01:58<1:17:31,  1.40it/s]Evaluating on VQA val set:   3% 168/6699 [01:59<1:15:58,  1.43it/s]Evaluating on VQA val set:   3% 169/6699 [01:59<1:13:59,  1.47it/s]Evaluating on VQA val set:   3% 170/6699 [02:00<1:13:49,  1.47it/s]Evaluating on VQA val set:   3% 171/6699 [02:01<1:12:15,  1.51it/s]Evaluating on VQA val set:   3% 172/6699 [02:01<1:13:26,  1.48it/s]Evaluating on VQA val set:   3% 173/6699 [02:02<1:15:28,  1.44it/s]Evaluating on VQA val set:   3% 174/6699 [02:03<1:15:04,  1.45it/s]Evaluating on VQA val set:   3% 175/6699 [02:03<1:12:34,  1.50it/s]Evaluating on VQA val set:   3% 176/6699 [02:04<1:12:40,  1.50it/s]Evaluating on VQA val set:   3% 177/6699 [02:05<1:14:20,  1.46it/s]Evaluating on VQA val set:   3% 178/6699 [02:05<1:14:46,  1.45it/s]Evaluating on VQA val set:   3% 179/6699 [02:06<1:17:24,  1.40it/s]Evaluating on VQA val set:   3% 180/6699 [02:07<1:17:55,  1.39it/s]Evaluating on VQA val set:   3% 181/6699 [02:08<1:16:18,  1.42it/s]Evaluating on VQA val set:   3% 182/6699 [02:08<1:17:07,  1.41it/s]Evaluating on VQA val set:   3% 183/6699 [02:09<1:15:23,  1.44it/s]Evaluating on VQA val set:   3% 184/6699 [02:10<1:16:00,  1.43it/s]Evaluating on VQA val set:   3% 185/6699 [02:10<1:16:23,  1.42it/s]Evaluating on VQA val set:   3% 186/6699 [02:11<1:14:53,  1.45it/s]Evaluating on VQA val set:   3% 187/6699 [02:12<1:15:10,  1.44it/s]Evaluating on VQA val set:   3% 188/6699 [02:12<1:14:21,  1.46it/s]Evaluating on VQA val set:   3% 189/6699 [02:13<1:12:27,  1.50it/s]Evaluating on VQA val set:   3% 190/6699 [02:14<1:13:24,  1.48it/s]Evaluating on VQA val set:   3% 191/6699 [02:14<1:14:10,  1.46it/s]Evaluating on VQA val set:   3% 192/6699 [02:15<1:14:35,  1.45it/s]Evaluating on VQA val set:   3% 193/6699 [02:16<1:17:37,  1.40it/s]Evaluating on VQA val set:   3% 194/6699 [02:17<1:18:43,  1.38it/s]Evaluating on VQA val set:   3% 195/6699 [02:17<1:13:44,  1.47it/s]Evaluating on VQA val set:   3% 196/6699 [02:18<1:11:41,  1.51it/s]Evaluating on VQA val set:   3% 197/6699 [02:19<1:13:00,  1.48it/s]Evaluating on VQA val set:   3% 198/6699 [02:19<1:15:35,  1.43it/s]Evaluating on VQA val set:   3% 199/6699 [02:20<1:15:29,  1.44it/s]Evaluating on VQA val set:   3% 200/6699 [02:21<1:17:34,  1.40it/s]Evaluating on VQA val set:   3% 201/6699 [02:21<1:17:33,  1.40it/s]Evaluating on VQA val set:   3% 202/6699 [02:22<1:18:13,  1.38it/s]Evaluating on VQA val set:   3% 203/6699 [02:23<1:18:15,  1.38it/s]Evaluating on VQA val set:   3% 204/6699 [02:24<1:19:15,  1.37it/s]Evaluating on VQA val set:   3% 205/6699 [02:24<1:18:51,  1.37it/s]Evaluating on VQA val set:   3% 206/6699 [02:25<1:15:26,  1.43it/s]Evaluating on VQA val set:   3% 207/6699 [02:26<1:17:40,  1.39it/s]Evaluating on VQA val set:   3% 208/6699 [02:27<1:18:35,  1.38it/s]Evaluating on VQA val set:   3% 209/6699 [02:27<1:18:53,  1.37it/s]Evaluating on VQA val set:   3% 210/6699 [02:28<1:18:25,  1.38it/s]Evaluating on VQA val set:   3% 211/6699 [02:29<1:19:24,  1.36it/s]Evaluating on VQA val set:   3% 212/6699 [02:29<1:13:44,  1.47it/s]Evaluating on VQA val set:   3% 213/6699 [02:30<1:14:54,  1.44it/s]Evaluating on VQA val set:   3% 214/6699 [02:31<1:11:39,  1.51it/s]Evaluating on VQA val set:   3% 215/6699 [02:31<1:10:15,  1.54it/s]Evaluating on VQA val set:   3% 216/6699 [02:32<1:09:46,  1.55it/s]Evaluating on VQA val set:   3% 217/6699 [02:33<1:11:34,  1.51it/s]Evaluating on VQA val set:   3% 218/6699 [02:33<1:10:51,  1.52it/s]Evaluating on VQA val set:   3% 219/6699 [02:34<1:11:20,  1.51it/s]Evaluating on VQA val set:   3% 220/6699 [02:35<1:13:51,  1.46it/s]Evaluating on VQA val set:   3% 221/6699 [02:35<1:14:59,  1.44it/s]Evaluating on VQA val set:   3% 222/6699 [02:36<1:13:29,  1.47it/s]Evaluating on VQA val set:   3% 223/6699 [02:37<1:12:47,  1.48it/s]Evaluating on VQA val set:   3% 224/6699 [02:37<1:11:41,  1.51it/s]Evaluating on VQA val set:   3% 225/6699 [02:38<1:08:57,  1.56it/s]Evaluating on VQA val set:   3% 226/6699 [02:39<1:11:37,  1.51it/s]Evaluating on VQA val set:   3% 227/6699 [02:39<1:09:24,  1.55it/s]Evaluating on VQA val set:   3% 228/6699 [02:40<1:10:31,  1.53it/s]Evaluating on VQA val set:   3% 229/6699 [02:41<1:11:12,  1.51it/s]Evaluating on VQA val set:   3% 230/6699 [02:41<1:08:31,  1.57it/s]Evaluating on VQA val set:   3% 231/6699 [02:42<1:07:25,  1.60it/s]Evaluating on VQA val set:   3% 232/6699 [02:43<1:12:01,  1.50it/s]Evaluating on VQA val set:   3% 233/6699 [02:43<1:14:15,  1.45it/s]Evaluating on VQA val set:   3% 234/6699 [02:44<1:14:49,  1.44it/s]Evaluating on VQA val set:   4% 235/6699 [02:45<1:19:02,  1.36it/s]Evaluating on VQA val set:   4% 236/6699 [02:45<1:17:50,  1.38it/s]Evaluating on VQA val set:   4% 237/6699 [02:46<1:16:19,  1.41it/s]Evaluating on VQA val set:   4% 238/6699 [02:47<1:17:53,  1.38it/s]Evaluating on VQA val set:   4% 239/6699 [02:48<1:17:24,  1.39it/s]Evaluating on VQA val set:   4% 240/6699 [02:48<1:16:31,  1.41it/s]Evaluating on VQA val set:   4% 241/6699 [02:49<1:16:31,  1.41it/s]Evaluating on VQA val set:   4% 242/6699 [02:50<1:16:26,  1.41it/s]Evaluating on VQA val set:   4% 243/6699 [02:50<1:15:06,  1.43it/s]Evaluating on VQA val set:   4% 244/6699 [02:51<1:15:02,  1.43it/s]Evaluating on VQA val set:   4% 245/6699 [02:52<1:15:06,  1.43it/s]Evaluating on VQA val set:   4% 246/6699 [02:53<1:16:07,  1.41it/s]Evaluating on VQA val set:   4% 247/6699 [02:53<1:15:31,  1.42it/s]Evaluating on VQA val set:   4% 248/6699 [02:54<1:13:45,  1.46it/s]Evaluating on VQA val set:   4% 249/6699 [02:55<1:15:08,  1.43it/s]Evaluating on VQA val set:   4% 250/6699 [02:55<1:15:57,  1.41it/s]Evaluating on VQA val set:   4% 251/6699 [02:56<1:16:48,  1.40it/s]Evaluating on VQA val set:   4% 252/6699 [02:57<1:16:09,  1.41it/s]Evaluating on VQA val set:   4% 253/6699 [02:57<1:17:21,  1.39it/s]Evaluating on VQA val set:   4% 254/6699 [02:58<1:16:39,  1.40it/s]Evaluating on VQA val set:   4% 255/6699 [02:59<1:16:56,  1.40it/s]Evaluating on VQA val set:   4% 256/6699 [03:00<1:13:50,  1.45it/s]Evaluating on VQA val set:   4% 257/6699 [03:00<1:15:01,  1.43it/s]Evaluating on VQA val set:   4% 258/6699 [03:01<1:13:11,  1.47it/s]Evaluating on VQA val set:   4% 259/6699 [03:02<1:12:35,  1.48it/s]Evaluating on VQA val set:   4% 260/6699 [03:02<1:15:03,  1.43it/s]Evaluating on VQA val set:   4% 261/6699 [03:03<1:16:31,  1.40it/s]Evaluating on VQA val set:   4% 262/6699 [03:04<1:14:38,  1.44it/s]Evaluating on VQA val set:   4% 263/6699 [03:04<1:14:03,  1.45it/s]Evaluating on VQA val set:   4% 264/6699 [03:05<1:12:17,  1.48it/s]Evaluating on VQA val set:   4% 265/6699 [03:06<1:14:17,  1.44it/s]Evaluating on VQA val set:   4% 266/6699 [03:06<1:13:13,  1.46it/s]Evaluating on VQA val set:   4% 267/6699 [03:07<1:10:56,  1.51it/s]Evaluating on VQA val set:   4% 268/6699 [03:08<1:13:22,  1.46it/s]Evaluating on VQA val set:   4% 269/6699 [03:08<1:13:00,  1.47it/s]Evaluating on VQA val set:   4% 270/6699 [03:09<1:13:35,  1.46it/s]Evaluating on VQA val set:   4% 271/6699 [03:10<1:11:52,  1.49it/s]Evaluating on VQA val set:   4% 272/6699 [03:11<1:14:29,  1.44it/s]Evaluating on VQA val set:   4% 273/6699 [03:11<1:15:01,  1.43it/s]Evaluating on VQA val set:   4% 274/6699 [03:12<1:17:52,  1.38it/s]Evaluating on VQA val set:   4% 275/6699 [03:13<1:19:00,  1.36it/s]Evaluating on VQA val set:   4% 276/6699 [03:14<1:18:23,  1.37it/s]Evaluating on VQA val set:   4% 277/6699 [03:14<1:18:15,  1.37it/s]Evaluating on VQA val set:   4% 278/6699 [03:15<1:17:37,  1.38it/s]Evaluating on VQA val set:   4% 279/6699 [03:16<1:18:23,  1.36it/s]Evaluating on VQA val set:   4% 280/6699 [03:16<1:18:32,  1.36it/s]Evaluating on VQA val set:   4% 281/6699 [03:17<1:19:34,  1.34it/s]Evaluating on VQA val set:   4% 282/6699 [03:18<1:17:47,  1.37it/s]Evaluating on VQA val set:   4% 283/6699 [03:19<1:17:28,  1.38it/s]Evaluating on VQA val set:   4% 284/6699 [03:19<1:19:19,  1.35it/s]Evaluating on VQA val set:   4% 285/6699 [03:20<1:16:22,  1.40it/s]Evaluating on VQA val set:   4% 286/6699 [03:21<1:15:36,  1.41it/s]Evaluating on VQA val set:   4% 287/6699 [03:21<1:13:49,  1.45it/s]Evaluating on VQA val set:   4% 288/6699 [03:22<1:15:10,  1.42it/s]Evaluating on VQA val set:   4% 289/6699 [03:23<1:14:40,  1.43it/s]Evaluating on VQA val set:   4% 290/6699 [03:24<1:14:43,  1.43it/s]Evaluating on VQA val set:   4% 291/6699 [03:24<1:14:45,  1.43it/s]Evaluating on VQA val set:   4% 292/6699 [03:25<1:15:54,  1.41it/s]Evaluating on VQA val set:   4% 293/6699 [03:26<1:16:21,  1.40it/s]Evaluating on VQA val set:   4% 294/6699 [03:26<1:16:46,  1.39it/s]Evaluating on VQA val set:   4% 295/6699 [03:27<1:17:07,  1.38it/s]Evaluating on VQA val set:   4% 296/6699 [03:28<1:15:57,  1.40it/s]Evaluating on VQA val set:   4% 297/6699 [03:29<1:15:38,  1.41it/s]Evaluating on VQA val set:   4% 298/6699 [03:29<1:16:24,  1.40it/s]Evaluating on VQA val set:   4% 299/6699 [03:30<1:15:22,  1.42it/s]Evaluating on VQA val set:   4% 300/6699 [03:31<1:13:33,  1.45it/s]Evaluating on VQA val set:   4% 301/6699 [03:31<1:14:24,  1.43it/s]Evaluating on VQA val set:   5% 302/6699 [03:32<1:15:03,  1.42it/s]Evaluating on VQA val set:   5% 303/6699 [03:33<1:15:00,  1.42it/s]Evaluating on VQA val set:   5% 304/6699 [03:34<1:17:15,  1.38it/s]Evaluating on VQA val set:   5% 305/6699 [03:34<1:17:28,  1.38it/s]Evaluating on VQA val set:   5% 306/6699 [03:35<1:18:08,  1.36it/s]Evaluating on VQA val set:   5% 307/6699 [03:36<1:17:54,  1.37it/s]Evaluating on VQA val set:   5% 308/6699 [03:36<1:15:45,  1.41it/s]Evaluating on VQA val set:   5% 309/6699 [03:37<1:16:08,  1.40it/s]Evaluating on VQA val set:   5% 310/6699 [03:38<1:17:43,  1.37it/s]Evaluating on VQA val set:   5% 311/6699 [03:38<1:14:20,  1.43it/s]Evaluating on VQA val set:   5% 312/6699 [03:39<1:14:00,  1.44it/s]Evaluating on VQA val set:   5% 313/6699 [03:40<1:12:10,  1.47it/s]Evaluating on VQA val set:   5% 314/6699 [03:40<1:11:29,  1.49it/s]Evaluating on VQA val set:   5% 315/6699 [03:41<1:12:50,  1.46it/s]Evaluating on VQA val set:   5% 316/6699 [03:42<1:14:50,  1.42it/s]Evaluating on VQA val set:   5% 317/6699 [03:43<1:11:07,  1.50it/s]Evaluating on VQA val set:   5% 318/6699 [03:43<1:11:51,  1.48it/s]Evaluating on VQA val set:   5% 319/6699 [03:44<1:10:05,  1.52it/s]Evaluating on VQA val set:   5% 320/6699 [03:44<1:08:22,  1.56it/s]Evaluating on VQA val set:   5% 321/6699 [03:45<1:10:52,  1.50it/s]Evaluating on VQA val set:   5% 322/6699 [03:46<1:11:50,  1.48it/s]Evaluating on VQA val set:   5% 323/6699 [03:47<1:14:10,  1.43it/s]Evaluating on VQA val set:   5% 324/6699 [03:47<1:14:38,  1.42it/s]Evaluating on VQA val set:   5% 325/6699 [03:48<1:16:16,  1.39it/s]Evaluating on VQA val set:   5% 326/6699 [03:49<1:16:30,  1.39it/s]Evaluating on VQA val set:   5% 327/6699 [03:50<1:16:30,  1.39it/s]Evaluating on VQA val set:   5% 328/6699 [03:50<1:14:34,  1.42it/s]Evaluating on VQA val set:   5% 329/6699 [03:51<1:11:12,  1.49it/s]Evaluating on VQA val set:   5% 330/6699 [03:51<1:09:50,  1.52it/s]Evaluating on VQA val set:   5% 331/6699 [03:52<1:12:57,  1.45it/s]Evaluating on VQA val set:   5% 332/6699 [03:53<1:14:00,  1.43it/s]Evaluating on VQA val set:   5% 333/6699 [03:54<1:14:02,  1.43it/s]Evaluating on VQA val set:   5% 334/6699 [03:54<1:14:50,  1.42it/s]Evaluating on VQA val set:   5% 335/6699 [03:55<1:14:08,  1.43it/s]Evaluating on VQA val set:   5% 336/6699 [03:56<1:12:34,  1.46it/s]Evaluating on VQA val set:   5% 337/6699 [03:56<1:14:26,  1.42it/s]Evaluating on VQA val set:   5% 338/6699 [03:57<1:14:44,  1.42it/s]Evaluating on VQA val set:   5% 339/6699 [03:58<1:15:15,  1.41it/s]Evaluating on VQA val set:   5% 340/6699 [03:59<1:15:53,  1.40it/s]Evaluating on VQA val set:   5% 341/6699 [03:59<1:15:07,  1.41it/s]Evaluating on VQA val set:   5% 342/6699 [04:00<1:16:35,  1.38it/s]Evaluating on VQA val set:   5% 343/6699 [04:01<1:16:48,  1.38it/s]Evaluating on VQA val set:   5% 344/6699 [04:01<1:16:51,  1.38it/s]Evaluating on VQA val set:   5% 345/6699 [04:02<1:15:13,  1.41it/s]Evaluating on VQA val set:   5% 346/6699 [04:03<1:16:47,  1.38it/s]Evaluating on VQA val set:   5% 347/6699 [04:04<1:16:44,  1.38it/s]Evaluating on VQA val set:   5% 348/6699 [04:04<1:15:59,  1.39it/s]Evaluating on VQA val set:   5% 349/6699 [04:05<1:17:18,  1.37it/s]Evaluating on VQA val set:   5% 350/6699 [04:06<1:16:39,  1.38it/s]Evaluating on VQA val set:   5% 351/6699 [04:06<1:15:21,  1.40it/s]Evaluating on VQA val set:   5% 352/6699 [04:07<1:14:33,  1.42it/s]Evaluating on VQA val set:   5% 353/6699 [04:08<1:13:17,  1.44it/s]Evaluating on VQA val set:   5% 354/6699 [04:09<1:14:43,  1.42it/s]Evaluating on VQA val set:   5% 355/6699 [04:09<1:14:01,  1.43it/s]Evaluating on VQA val set:   5% 356/6699 [04:10<1:09:48,  1.51it/s]Evaluating on VQA val set:   5% 357/6699 [04:11<1:10:25,  1.50it/s]Evaluating on VQA val set:   5% 358/6699 [04:11<1:11:54,  1.47it/s]Evaluating on VQA val set:   5% 359/6699 [04:12<1:12:36,  1.46it/s]Evaluating on VQA val set:   5% 360/6699 [04:13<1:11:43,  1.47it/s]Evaluating on VQA val set:   5% 361/6699 [04:13<1:16:39,  1.38it/s]Evaluating on VQA val set:   5% 362/6699 [04:14<1:16:28,  1.38it/s]Evaluating on VQA val set:   5% 363/6699 [04:15<1:15:02,  1.41it/s]Evaluating on VQA val set:   5% 364/6699 [04:15<1:13:43,  1.43it/s]Evaluating on VQA val set:   5% 365/6699 [04:16<1:14:39,  1.41it/s]Evaluating on VQA val set:   5% 366/6699 [04:17<1:15:40,  1.39it/s]Evaluating on VQA val set:   5% 367/6699 [04:18<1:18:53,  1.34it/s]Evaluating on VQA val set:   5% 368/6699 [04:18<1:17:42,  1.36it/s]Evaluating on VQA val set:   6% 369/6699 [04:19<1:17:32,  1.36it/s]Evaluating on VQA val set:   6% 370/6699 [04:20<1:17:10,  1.37it/s]Evaluating on VQA val set:   6% 371/6699 [04:21<1:18:40,  1.34it/s]Evaluating on VQA val set:   6% 372/6699 [04:21<1:17:50,  1.35it/s]Evaluating on VQA val set:   6% 373/6699 [04:22<1:17:04,  1.37it/s]Evaluating on VQA val set:   6% 374/6699 [04:23<1:17:06,  1.37it/s]Evaluating on VQA val set:   6% 375/6699 [04:24<1:16:16,  1.38it/s]Evaluating on VQA val set:   6% 376/6699 [04:24<1:18:05,  1.35it/s]Evaluating on VQA val set:   6% 377/6699 [04:25<1:18:02,  1.35it/s]Evaluating on VQA val set:   6% 378/6699 [04:26<1:18:02,  1.35it/s]Evaluating on VQA val set:   6% 379/6699 [04:27<1:16:27,  1.38it/s]Evaluating on VQA val set:   6% 380/6699 [04:27<1:16:27,  1.38it/s]Evaluating on VQA val set:   6% 381/6699 [04:28<1:13:44,  1.43it/s]Evaluating on VQA val set:   6% 382/6699 [04:29<1:12:00,  1.46it/s]Evaluating on VQA val set:   6% 383/6699 [04:29<1:10:44,  1.49it/s]Evaluating on VQA val set:   6% 384/6699 [04:30<1:11:54,  1.46it/s]Evaluating on VQA val set:   6% 385/6699 [04:31<1:14:08,  1.42it/s]Evaluating on VQA val set:   6% 386/6699 [04:31<1:13:25,  1.43it/s]Evaluating on VQA val set:   6% 387/6699 [04:32<1:15:18,  1.40it/s]Evaluating on VQA val set:   6% 388/6699 [04:33<1:15:02,  1.40it/s]Evaluating on VQA val set:   6% 389/6699 [04:34<1:16:17,  1.38it/s]Evaluating on VQA val set:   6% 390/6699 [04:34<1:18:11,  1.34it/s]Evaluating on VQA val set:   6% 391/6699 [04:35<1:17:01,  1.37it/s]Evaluating on VQA val set:   6% 392/6699 [04:36<1:14:48,  1.41it/s]Evaluating on VQA val set:   6% 393/6699 [04:36<1:15:07,  1.40it/s]Evaluating on VQA val set:   6% 394/6699 [04:37<1:14:38,  1.41it/s]Evaluating on VQA val set:   6% 395/6699 [04:38<1:15:35,  1.39it/s]Evaluating on VQA val set:   6% 396/6699 [04:39<1:14:20,  1.41it/s]Evaluating on VQA val set:   6% 397/6699 [04:39<1:14:33,  1.41it/s]Evaluating on VQA val set:   6% 398/6699 [04:40<1:15:37,  1.39it/s]Evaluating on VQA val set:   6% 399/6699 [04:41<1:15:05,  1.40it/s]Evaluating on VQA val set:   6% 400/6699 [04:41<1:16:16,  1.38it/s]Evaluating on VQA val set:   6% 401/6699 [04:42<1:15:59,  1.38it/s]Evaluating on VQA val set:   6% 402/6699 [04:43<1:15:25,  1.39it/s]Evaluating on VQA val set:   6% 403/6699 [04:44<1:16:07,  1.38it/s]Evaluating on VQA val set:   6% 404/6699 [04:44<1:16:11,  1.38it/s]Evaluating on VQA val set:   6% 405/6699 [04:45<1:15:27,  1.39it/s]Evaluating on VQA val set:   6% 406/6699 [04:46<1:14:39,  1.40it/s]Evaluating on VQA val set:   6% 407/6699 [04:46<1:11:13,  1.47it/s]Evaluating on VQA val set:   6% 408/6699 [04:47<1:12:08,  1.45it/s]Evaluating on VQA val set:   6% 409/6699 [04:48<1:13:19,  1.43it/s]Evaluating on VQA val set:   6% 410/6699 [04:49<1:14:20,  1.41it/s]Evaluating on VQA val set:   6% 411/6699 [04:49<1:14:14,  1.41it/s]Evaluating on VQA val set:   6% 412/6699 [04:50<1:12:08,  1.45it/s]Evaluating on VQA val set:   6% 413/6699 [04:51<1:12:34,  1.44it/s]Evaluating on VQA val set:   6% 414/6699 [04:51<1:14:13,  1.41it/s]Evaluating on VQA val set:   6% 415/6699 [04:52<1:14:45,  1.40it/s]Evaluating on VQA val set:   6% 416/6699 [04:53<1:15:44,  1.38it/s]Evaluating on VQA val set:   6% 417/6699 [04:54<1:17:07,  1.36it/s]Evaluating on VQA val set:   6% 418/6699 [04:54<1:16:47,  1.36it/s]Evaluating on VQA val set:   6% 419/6699 [04:55<1:15:49,  1.38it/s]Evaluating on VQA val set:   6% 420/6699 [04:56<1:15:43,  1.38it/s]Evaluating on VQA val set:   6% 421/6699 [04:56<1:16:01,  1.38it/s]Evaluating on VQA val set:   6% 422/6699 [04:57<1:16:11,  1.37it/s]Evaluating on VQA val set:   6% 423/6699 [04:58<1:10:50,  1.48it/s]Evaluating on VQA val set:   6% 424/6699 [04:58<1:11:30,  1.46it/s]Evaluating on VQA val set:   6% 425/6699 [04:59<1:13:39,  1.42it/s]Evaluating on VQA val set:   6% 426/6699 [05:00<1:15:10,  1.39it/s]Evaluating on VQA val set:   6% 427/6699 [05:01<1:15:54,  1.38it/s]Evaluating on VQA val set:   6% 428/6699 [05:01<1:14:48,  1.40it/s]Evaluating on VQA val set:   6% 429/6699 [05:02<1:13:15,  1.43it/s]Evaluating on VQA val set:   6% 430/6699 [05:03<1:11:55,  1.45it/s]Evaluating on VQA val set:   6% 431/6699 [05:03<1:11:30,  1.46it/s]Evaluating on VQA val set:   6% 432/6699 [05:04<1:11:15,  1.47it/s]Evaluating on VQA val set:   6% 433/6699 [05:05<1:11:42,  1.46it/s]Evaluating on VQA val set:   6% 434/6699 [05:06<1:13:32,  1.42it/s]Evaluating on VQA val set:   6% 435/6699 [05:06<1:14:28,  1.40it/s]Evaluating on VQA val set:   7% 436/6699 [05:07<1:14:54,  1.39it/s]Evaluating on VQA val set:   7% 437/6699 [05:08<1:14:33,  1.40it/s]Evaluating on VQA val set:   7% 438/6699 [05:08<1:14:59,  1.39it/s]Evaluating on VQA val set:   7% 439/6699 [05:09<1:15:39,  1.38it/s]Evaluating on VQA val set:   7% 440/6699 [05:10<1:15:10,  1.39it/s]Evaluating on VQA val set:   7% 441/6699 [05:11<1:14:48,  1.39it/s]Evaluating on VQA val set:   7% 442/6699 [05:11<1:13:32,  1.42it/s]Evaluating on VQA val set:   7% 443/6699 [05:12<1:14:24,  1.40it/s]Evaluating on VQA val set:   7% 444/6699 [05:13<1:16:44,  1.36it/s]Evaluating on VQA val set:   7% 445/6699 [05:14<1:16:49,  1.36it/s]Evaluating on VQA val set:   7% 446/6699 [05:14<1:17:31,  1.34it/s]Evaluating on VQA val set:   7% 447/6699 [05:15<1:14:55,  1.39it/s]Evaluating on VQA val set:   7% 448/6699 [05:16<1:13:55,  1.41it/s]Evaluating on VQA val set:   7% 449/6699 [05:16<1:11:33,  1.46it/s]Evaluating on VQA val set:   7% 450/6699 [05:17<1:12:25,  1.44it/s]Evaluating on VQA val set:   7% 451/6699 [05:18<1:13:41,  1.41it/s]Evaluating on VQA val set:   7% 452/6699 [05:18<1:11:20,  1.46it/s]Evaluating on VQA val set:   7% 453/6699 [05:19<1:10:07,  1.48it/s]Evaluating on VQA val set:   7% 454/6699 [05:20<1:10:30,  1.48it/s]Evaluating on VQA val set:   7% 455/6699 [05:20<1:10:48,  1.47it/s]Evaluating on VQA val set:   7% 456/6699 [05:21<1:10:41,  1.47it/s]Evaluating on VQA val set:   7% 457/6699 [05:22<1:12:52,  1.43it/s]Evaluating on VQA val set:   7% 458/6699 [05:22<1:09:23,  1.50it/s]Evaluating on VQA val set:   7% 459/6699 [05:23<1:11:58,  1.44it/s]Evaluating on VQA val set:   7% 460/6699 [05:24<1:15:05,  1.38it/s]Evaluating on VQA val set:   7% 461/6699 [05:25<1:15:13,  1.38it/s]Evaluating on VQA val set:   7% 462/6699 [05:25<1:15:10,  1.38it/s]Evaluating on VQA val set:   7% 463/6699 [05:26<1:15:33,  1.38it/s]Evaluating on VQA val set:   7% 464/6699 [05:27<1:16:28,  1.36it/s]Evaluating on VQA val set:   7% 465/6699 [05:28<1:15:54,  1.37it/s]Evaluating on VQA val set:   7% 466/6699 [05:28<1:16:20,  1.36it/s]Evaluating on VQA val set:   7% 467/6699 [05:29<1:14:39,  1.39it/s]Evaluating on VQA val set:   7% 468/6699 [05:30<1:15:00,  1.38it/s]Evaluating on VQA val set:   7% 469/6699 [05:30<1:14:00,  1.40it/s]Evaluating on VQA val set:   7% 470/6699 [05:31<1:15:03,  1.38it/s]Evaluating on VQA val set:   7% 471/6699 [05:32<1:13:59,  1.40it/s]Evaluating on VQA val set:   7% 472/6699 [05:33<1:15:22,  1.38it/s]Evaluating on VQA val set:   7% 473/6699 [05:33<1:12:39,  1.43it/s]Evaluating on VQA val set:   7% 474/6699 [05:34<1:14:24,  1.39it/s]Evaluating on VQA val set:   7% 475/6699 [05:35<1:12:54,  1.42it/s]Evaluating on VQA val set:   7% 476/6699 [05:35<1:12:49,  1.42it/s]Evaluating on VQA val set:   7% 477/6699 [05:36<1:12:47,  1.42it/s]Evaluating on VQA val set:   7% 478/6699 [05:37<1:12:11,  1.44it/s]Evaluating on VQA val set:   7% 479/6699 [05:37<1:13:08,  1.42it/s]Evaluating on VQA val set:   7% 480/6699 [05:38<1:13:13,  1.42it/s]Evaluating on VQA val set:   7% 481/6699 [05:39<1:12:50,  1.42it/s]Evaluating on VQA val set:   7% 482/6699 [05:40<1:12:25,  1.43it/s]Evaluating on VQA val set:   7% 483/6699 [05:40<1:12:23,  1.43it/s]Evaluating on VQA val set:   7% 484/6699 [05:41<1:12:18,  1.43it/s]Evaluating on VQA val set:   7% 485/6699 [05:42<1:12:37,  1.43it/s]Evaluating on VQA val set:   7% 486/6699 [05:42<1:14:24,  1.39it/s]Evaluating on VQA val set:   7% 487/6699 [05:43<1:14:03,  1.40it/s]Evaluating on VQA val set:   7% 488/6699 [05:44<1:14:31,  1.39it/s]Evaluating on VQA val set:   7% 489/6699 [05:45<1:14:38,  1.39it/s]Evaluating on VQA val set:   7% 490/6699 [05:45<1:10:33,  1.47it/s]Evaluating on VQA val set:   7% 491/6699 [05:46<1:11:56,  1.44it/s]Evaluating on VQA val set:   7% 492/6699 [05:47<1:10:21,  1.47it/s]Evaluating on VQA val set:   7% 493/6699 [05:47<1:10:18,  1.47it/s]Evaluating on VQA val set:   7% 494/6699 [05:48<1:10:11,  1.47it/s]Evaluating on VQA val set:   7% 495/6699 [05:49<1:11:21,  1.45it/s]Evaluating on VQA val set:   7% 496/6699 [05:49<1:13:25,  1.41it/s]Evaluating on VQA val set:   7% 497/6699 [05:50<1:12:33,  1.42it/s]Evaluating on VQA val set:   7% 498/6699 [05:51<1:13:53,  1.40it/s]Evaluating on VQA val set:   7% 499/6699 [05:52<1:14:20,  1.39it/s]Evaluating on VQA val set:   7% 500/6699 [05:52<1:15:06,  1.38it/s]Evaluating on VQA val set:   7% 501/6699 [05:53<1:12:26,  1.43it/s]Evaluating on VQA val set:   7% 502/6699 [05:54<1:11:18,  1.45it/s]Evaluating on VQA val set:   8% 503/6699 [05:54<1:09:23,  1.49it/s]Evaluating on VQA val set:   8% 504/6699 [05:55<1:12:29,  1.42it/s]Evaluating on VQA val set:   8% 505/6699 [05:56<1:12:35,  1.42it/s]Evaluating on VQA val set:   8% 506/6699 [05:57<1:16:13,  1.35it/s]Evaluating on VQA val set:   8% 507/6699 [05:57<1:15:49,  1.36it/s]Evaluating on VQA val set:   8% 508/6699 [05:58<1:14:55,  1.38it/s]Evaluating on VQA val set:   8% 509/6699 [05:59<1:12:13,  1.43it/s]Evaluating on VQA val set:   8% 510/6699 [05:59<1:09:16,  1.49it/s]Evaluating on VQA val set:   8% 511/6699 [06:00<1:10:04,  1.47it/s]Evaluating on VQA val set:   8% 512/6699 [06:01<1:08:24,  1.51it/s]Evaluating on VQA val set:   8% 513/6699 [06:01<1:10:46,  1.46it/s]Evaluating on VQA val set:   8% 514/6699 [06:02<1:11:24,  1.44it/s]Evaluating on VQA val set:   8% 515/6699 [06:03<1:08:42,  1.50it/s]Evaluating on VQA val set:   8% 516/6699 [06:03<1:11:18,  1.45it/s]Evaluating on VQA val set:   8% 517/6699 [06:04<1:12:00,  1.43it/s]Evaluating on VQA val set:   8% 518/6699 [06:05<1:11:25,  1.44it/s]Evaluating on VQA val set:   8% 519/6699 [06:05<1:13:02,  1.41it/s]Evaluating on VQA val set:   8% 520/6699 [06:06<1:11:22,  1.44it/s]Evaluating on VQA val set:   8% 521/6699 [06:07<1:11:31,  1.44it/s]Evaluating on VQA val set:   8% 522/6699 [06:08<1:12:55,  1.41it/s]Evaluating on VQA val set:   8% 523/6699 [06:08<1:14:20,  1.38it/s]Evaluating on VQA val set:   8% 524/6699 [06:09<1:14:23,  1.38it/s]Evaluating on VQA val set:   8% 525/6699 [06:10<1:14:15,  1.39it/s]Evaluating on VQA val set:   8% 526/6699 [06:10<1:14:10,  1.39it/s]Evaluating on VQA val set:   8% 527/6699 [06:11<1:15:10,  1.37it/s]Evaluating on VQA val set:   8% 528/6699 [06:12<1:11:06,  1.45it/s]Evaluating on VQA val set:   8% 529/6699 [06:13<1:12:02,  1.43it/s]Evaluating on VQA val set:   8% 530/6699 [06:13<1:12:01,  1.43it/s]Evaluating on VQA val set:   8% 531/6699 [06:14<1:10:44,  1.45it/s]Evaluating on VQA val set:   8% 532/6699 [06:15<1:11:45,  1.43it/s]Evaluating on VQA val set:   8% 533/6699 [06:15<1:12:25,  1.42it/s]Evaluating on VQA val set:   8% 534/6699 [06:16<1:12:23,  1.42it/s]Evaluating on VQA val set:   8% 535/6699 [06:17<1:13:48,  1.39it/s]Evaluating on VQA val set:   8% 536/6699 [06:18<1:13:50,  1.39it/s]Evaluating on VQA val set:   8% 537/6699 [06:18<1:14:06,  1.39it/s]Evaluating on VQA val set:   8% 538/6699 [06:19<1:14:06,  1.39it/s]Evaluating on VQA val set:   8% 539/6699 [06:20<1:15:22,  1.36it/s]Evaluating on VQA val set:   8% 540/6699 [06:20<1:14:39,  1.37it/s]Evaluating on VQA val set:   8% 541/6699 [06:21<1:12:31,  1.42it/s]Evaluating on VQA val set:   8% 542/6699 [06:22<1:13:11,  1.40it/s]Evaluating on VQA val set:   8% 543/6699 [06:23<1:14:08,  1.38it/s]Evaluating on VQA val set:   8% 544/6699 [06:23<1:13:08,  1.40it/s]Evaluating on VQA val set:   8% 545/6699 [06:24<1:13:12,  1.40it/s]Evaluating on VQA val set:   8% 546/6699 [06:25<1:11:46,  1.43it/s]Evaluating on VQA val set:   8% 547/6699 [06:25<1:11:21,  1.44it/s]Evaluating on VQA val set:   8% 548/6699 [06:26<1:11:38,  1.43it/s]Evaluating on VQA val set:   8% 549/6699 [06:27<1:11:40,  1.43it/s]Evaluating on VQA val set:   8% 550/6699 [06:27<1:08:28,  1.50it/s]Evaluating on VQA val set:   8% 551/6699 [06:28<1:11:39,  1.43it/s]Evaluating on VQA val set:   8% 552/6699 [06:29<1:12:06,  1.42it/s]Evaluating on VQA val set:   8% 553/6699 [06:30<1:11:58,  1.42it/s]Evaluating on VQA val set:   8% 554/6699 [06:30<1:11:37,  1.43it/s]Evaluating on VQA val set:   8% 555/6699 [06:31<1:11:55,  1.42it/s]Evaluating on VQA val set:   8% 556/6699 [06:32<1:12:36,  1.41it/s]Evaluating on VQA val set:   8% 557/6699 [06:32<1:12:49,  1.41it/s]Evaluating on VQA val set:   8% 558/6699 [06:33<1:11:19,  1.44it/s]Evaluating on VQA val set:   8% 559/6699 [06:34<1:12:32,  1.41it/s]Evaluating on VQA val set:   8% 560/6699 [06:34<1:11:49,  1.42it/s]Evaluating on VQA val set:   8% 561/6699 [06:35<1:12:09,  1.42it/s]Evaluating on VQA val set:   8% 562/6699 [06:36<1:12:27,  1.41it/s]Evaluating on VQA val set:   8% 563/6699 [06:37<1:10:37,  1.45it/s]Evaluating on VQA val set:   8% 564/6699 [06:37<1:10:39,  1.45it/s]Evaluating on VQA val set:   8% 565/6699 [06:38<1:11:12,  1.44it/s]Evaluating on VQA val set:   8% 566/6699 [06:39<1:11:17,  1.43it/s]Evaluating on VQA val set:   8% 567/6699 [06:39<1:12:39,  1.41it/s]Evaluating on VQA val set:   8% 568/6699 [06:40<1:14:35,  1.37it/s]Evaluating on VQA val set:   8% 569/6699 [06:41<1:13:04,  1.40it/s]Evaluating on VQA val set:   9% 570/6699 [06:42<1:15:35,  1.35it/s]Evaluating on VQA val set:   9% 571/6699 [06:42<1:17:44,  1.31it/s]Evaluating on VQA val set:   9% 572/6699 [06:43<1:15:36,  1.35it/s]Evaluating on VQA val set:   9% 573/6699 [06:44<1:13:07,  1.40it/s]Evaluating on VQA val set:   9% 574/6699 [06:45<1:14:43,  1.37it/s]Evaluating on VQA val set:   9% 575/6699 [06:45<1:13:33,  1.39it/s]Evaluating on VQA val set:   9% 576/6699 [06:46<1:14:11,  1.38it/s]Evaluating on VQA val set:   9% 577/6699 [06:47<1:13:27,  1.39it/s]Evaluating on VQA val set:   9% 578/6699 [06:47<1:12:23,  1.41it/s]Evaluating on VQA val set:   9% 579/6699 [06:48<1:13:34,  1.39it/s]Evaluating on VQA val set:   9% 580/6699 [06:49<1:13:21,  1.39it/s]Evaluating on VQA val set:   9% 581/6699 [06:50<1:14:23,  1.37it/s]Evaluating on VQA val set:   9% 582/6699 [06:50<1:14:34,  1.37it/s]Evaluating on VQA val set:   9% 583/6699 [06:51<1:12:01,  1.42it/s]Evaluating on VQA val set:   9% 584/6699 [06:52<1:09:36,  1.46it/s]Evaluating on VQA val set:   9% 585/6699 [06:52<1:12:42,  1.40it/s]Evaluating on VQA val set:   9% 586/6699 [06:53<1:15:07,  1.36it/s]Evaluating on VQA val set:   9% 587/6699 [06:54<1:12:35,  1.40it/s]Evaluating on VQA val set:   9% 588/6699 [06:55<1:11:44,  1.42it/s]Evaluating on VQA val set:   9% 589/6699 [06:55<1:11:45,  1.42it/s]Evaluating on VQA val set:   9% 590/6699 [06:56<1:10:05,  1.45it/s]Evaluating on VQA val set:   9% 591/6699 [06:57<1:10:40,  1.44it/s]Evaluating on VQA val set:   9% 592/6699 [06:57<1:12:02,  1.41it/s]Evaluating on VQA val set:   9% 593/6699 [06:58<1:13:21,  1.39it/s]Evaluating on VQA val set:   9% 594/6699 [06:59<1:13:48,  1.38it/s]Evaluating on VQA val set:   9% 595/6699 [07:00<1:13:50,  1.38it/s]Evaluating on VQA val set:   9% 596/6699 [07:00<1:13:03,  1.39it/s]Evaluating on VQA val set:   9% 597/6699 [07:01<1:11:39,  1.42it/s]Evaluating on VQA val set:   9% 598/6699 [07:02<1:13:10,  1.39it/s]Evaluating on VQA val set:   9% 599/6699 [07:02<1:14:01,  1.37it/s]Evaluating on VQA val set:   9% 600/6699 [07:03<1:13:15,  1.39it/s]Evaluating on VQA val set:   9% 601/6699 [07:04<1:12:36,  1.40it/s]Evaluating on VQA val set:   9% 602/6699 [07:05<1:14:12,  1.37it/s]Evaluating on VQA val set:   9% 603/6699 [07:05<1:15:04,  1.35it/s]Evaluating on VQA val set:   9% 604/6699 [07:06<1:15:43,  1.34it/s]Evaluating on VQA val set:   9% 605/6699 [07:07<1:14:13,  1.37it/s]Evaluating on VQA val set:   9% 606/6699 [07:07<1:11:49,  1.41it/s]Evaluating on VQA val set:   9% 607/6699 [07:08<1:10:57,  1.43it/s]Evaluating on VQA val set:   9% 608/6699 [07:09<1:11:28,  1.42it/s]Evaluating on VQA val set:   9% 609/6699 [07:10<1:10:00,  1.45it/s]Evaluating on VQA val set:   9% 610/6699 [07:10<1:11:49,  1.41it/s]Evaluating on VQA val set:   9% 611/6699 [07:11<1:10:45,  1.43it/s]Evaluating on VQA val set:   9% 612/6699 [07:12<1:11:33,  1.42it/s]Evaluating on VQA val set:   9% 613/6699 [07:12<1:13:19,  1.38it/s]Evaluating on VQA val set:   9% 614/6699 [07:13<1:13:42,  1.38it/s]Evaluating on VQA val set:   9% 615/6699 [07:14<1:12:00,  1.41it/s]Evaluating on VQA val set:   9% 616/6699 [07:15<1:11:29,  1.42it/s]Evaluating on VQA val set:   9% 617/6699 [07:15<1:12:03,  1.41it/s]Evaluating on VQA val set:   9% 618/6699 [07:16<1:12:59,  1.39it/s]Evaluating on VQA val set:   9% 619/6699 [07:17<1:13:11,  1.38it/s]Evaluating on VQA val set:   9% 620/6699 [07:17<1:13:23,  1.38it/s]Evaluating on VQA val set:   9% 621/6699 [07:18<1:14:18,  1.36it/s]Evaluating on VQA val set:   9% 622/6699 [07:19<1:12:30,  1.40it/s]Evaluating on VQA val set:   9% 623/6699 [07:20<1:11:50,  1.41it/s]Evaluating on VQA val set:   9% 624/6699 [07:20<1:12:14,  1.40it/s]Evaluating on VQA val set:   9% 625/6699 [07:21<1:12:18,  1.40it/s]Evaluating on VQA val set:   9% 626/6699 [07:22<1:13:24,  1.38it/s]Evaluating on VQA val set:   9% 627/6699 [07:23<1:13:55,  1.37it/s]Evaluating on VQA val set:   9% 628/6699 [07:23<1:14:28,  1.36it/s]Evaluating on VQA val set:   9% 629/6699 [07:24<1:12:16,  1.40it/s]Evaluating on VQA val set:   9% 630/6699 [07:25<1:09:58,  1.45it/s]Evaluating on VQA val set:   9% 631/6699 [07:25<1:11:20,  1.42it/s]Evaluating on VQA val set:   9% 632/6699 [07:26<1:11:09,  1.42it/s]Evaluating on VQA val set:   9% 633/6699 [07:27<1:11:00,  1.42it/s]Evaluating on VQA val set:   9% 634/6699 [07:27<1:10:35,  1.43it/s]Evaluating on VQA val set:   9% 635/6699 [07:28<1:11:33,  1.41it/s]Evaluating on VQA val set:   9% 636/6699 [07:29<1:11:02,  1.42it/s]Evaluating on VQA val set:  10% 637/6699 [07:30<1:10:19,  1.44it/s]Evaluating on VQA val set:  10% 638/6699 [07:30<1:10:34,  1.43it/s]Evaluating on VQA val set:  10% 639/6699 [07:31<1:10:58,  1.42it/s]Evaluating on VQA val set:  10% 640/6699 [07:32<1:10:47,  1.43it/s]Evaluating on VQA val set:  10% 641/6699 [07:32<1:10:47,  1.43it/s]Evaluating on VQA val set:  10% 642/6699 [07:33<1:08:43,  1.47it/s]Evaluating on VQA val set:  10% 643/6699 [07:34<1:07:40,  1.49it/s]Evaluating on VQA val set:  10% 644/6699 [07:34<1:07:55,  1.49it/s]Evaluating on VQA val set:  10% 645/6699 [07:35<1:06:11,  1.52it/s]Evaluating on VQA val set:  10% 646/6699 [07:36<1:07:15,  1.50it/s]Evaluating on VQA val set:  10% 647/6699 [07:36<1:08:40,  1.47it/s]Evaluating on VQA val set:  10% 648/6699 [07:37<1:06:25,  1.52it/s]Evaluating on VQA val set:  10% 649/6699 [07:38<1:07:03,  1.50it/s]Evaluating on VQA val set:  10% 650/6699 [07:38<1:10:57,  1.42it/s]Evaluating on VQA val set:  10% 651/6699 [07:39<1:10:19,  1.43it/s]Evaluating on VQA val set:  10% 652/6699 [07:40<1:09:50,  1.44it/s]Evaluating on VQA val set:  10% 653/6699 [07:40<1:11:28,  1.41it/s]Evaluating on VQA val set:  10% 654/6699 [07:41<1:12:14,  1.39it/s]Evaluating on VQA val set:  10% 655/6699 [07:42<1:12:52,  1.38it/s]Evaluating on VQA val set:  10% 656/6699 [07:43<1:09:51,  1.44it/s]Evaluating on VQA val set:  10% 657/6699 [07:43<1:11:36,  1.41it/s]Evaluating on VQA val set:  10% 658/6699 [07:44<1:12:04,  1.40it/s]Evaluating on VQA val set:  10% 659/6699 [07:45<1:13:46,  1.36it/s]Evaluating on VQA val set:  10% 660/6699 [07:46<1:13:53,  1.36it/s]Evaluating on VQA val set:  10% 661/6699 [07:46<1:12:54,  1.38it/s]Evaluating on VQA val set:  10% 662/6699 [07:47<1:12:40,  1.38it/s]Evaluating on VQA val set:  10% 663/6699 [07:48<1:13:16,  1.37it/s]Evaluating on VQA val set:  10% 664/6699 [07:48<1:12:11,  1.39it/s]Evaluating on VQA val set:  10% 665/6699 [07:49<1:11:27,  1.41it/s]Evaluating on VQA val set:  10% 666/6699 [07:50<1:11:22,  1.41it/s]Evaluating on VQA val set:  10% 667/6699 [07:51<1:10:23,  1.43it/s]Evaluating on VQA val set:  10% 668/6699 [07:51<1:08:14,  1.47it/s]Evaluating on VQA val set:  10% 669/6699 [07:52<1:07:05,  1.50it/s]Evaluating on VQA val set:  10% 670/6699 [07:53<1:08:51,  1.46it/s]Evaluating on VQA val set:  10% 671/6699 [07:53<1:09:57,  1.44it/s]Evaluating on VQA val set:  10% 672/6699 [07:54<1:10:31,  1.42it/s]Evaluating on VQA val set:  10% 673/6699 [07:55<1:12:47,  1.38it/s]Evaluating on VQA val set:  10% 674/6699 [07:55<1:13:47,  1.36it/s]Evaluating on VQA val set:  10% 675/6699 [07:56<1:13:09,  1.37it/s]Evaluating on VQA val set:  10% 676/6699 [07:57<1:12:48,  1.38it/s]Evaluating on VQA val set:  10% 677/6699 [07:58<1:12:56,  1.38it/s]Evaluating on VQA val set:  10% 678/6699 [07:58<1:12:44,  1.38it/s]Evaluating on VQA val set:  10% 679/6699 [07:59<1:12:21,  1.39it/s]Evaluating on VQA val set:  10% 680/6699 [08:00<1:10:44,  1.42it/s]Evaluating on VQA val set:  10% 681/6699 [08:00<1:11:31,  1.40it/s]Evaluating on VQA val set:  10% 682/6699 [08:01<1:12:01,  1.39it/s]Evaluating on VQA val set:  10% 683/6699 [08:02<1:12:55,  1.37it/s]Evaluating on VQA val set:  10% 684/6699 [08:03<1:10:15,  1.43it/s]Evaluating on VQA val set:  10% 685/6699 [08:03<1:13:27,  1.36it/s]Evaluating on VQA val set:  10% 686/6699 [08:04<1:13:36,  1.36it/s]Evaluating on VQA val set:  10% 687/6699 [08:05<1:12:13,  1.39it/s]Evaluating on VQA val set:  10% 688/6699 [08:06<1:12:14,  1.39it/s]Evaluating on VQA val set:  10% 689/6699 [08:06<1:12:28,  1.38it/s]Evaluating on VQA val set:  10% 690/6699 [08:07<1:12:27,  1.38it/s]Evaluating on VQA val set:  10% 691/6699 [08:08<1:12:56,  1.37it/s]Evaluating on VQA val set:  10% 692/6699 [08:08<1:10:56,  1.41it/s]Evaluating on VQA val set:  10% 693/6699 [08:09<1:11:38,  1.40it/s]Evaluating on VQA val set:  10% 694/6699 [08:10<1:09:30,  1.44it/s]Evaluating on VQA val set:  10% 695/6699 [08:10<1:09:59,  1.43it/s]Evaluating on VQA val set:  10% 696/6699 [08:11<1:06:41,  1.50it/s]Evaluating on VQA val set:  10% 697/6699 [08:12<1:08:39,  1.46it/s]Evaluating on VQA val set:  10% 698/6699 [08:13<1:10:09,  1.43it/s]Evaluating on VQA val set:  10% 699/6699 [08:13<1:10:22,  1.42it/s]Evaluating on VQA val set:  10% 700/6699 [08:14<1:11:57,  1.39it/s]Evaluating on VQA val set:  10% 701/6699 [08:15<1:12:57,  1.37it/s]Evaluating on VQA val set:  10% 702/6699 [08:16<1:13:57,  1.35it/s]Evaluating on VQA val set:  10% 703/6699 [08:16<1:13:15,  1.36it/s]Evaluating on VQA val set:  11% 704/6699 [08:17<1:11:30,  1.40it/s]Evaluating on VQA val set:  11% 705/6699 [08:18<1:11:40,  1.39it/s]Evaluating on VQA val set:  11% 706/6699 [08:18<1:12:03,  1.39it/s]Evaluating on VQA val set:  11% 707/6699 [08:19<1:12:10,  1.38it/s]Evaluating on VQA val set:  11% 708/6699 [08:20<1:13:04,  1.37it/s]Evaluating on VQA val set:  11% 709/6699 [08:21<1:12:10,  1.38it/s]Evaluating on VQA val set:  11% 710/6699 [08:21<1:11:41,  1.39it/s]Evaluating on VQA val set:  11% 711/6699 [08:22<1:12:41,  1.37it/s]Evaluating on VQA val set:  11% 712/6699 [08:23<1:15:16,  1.33it/s]Evaluating on VQA val set:  11% 713/6699 [08:24<1:13:14,  1.36it/s]Evaluating on VQA val set:  11% 714/6699 [08:24<1:13:40,  1.35it/s]Evaluating on VQA val set:  11% 715/6699 [08:25<1:13:16,  1.36it/s]Evaluating on VQA val set:  11% 716/6699 [08:26<1:10:23,  1.42it/s]Evaluating on VQA val set:  11% 717/6699 [08:26<1:08:08,  1.46it/s]Evaluating on VQA val set:  11% 718/6699 [08:27<1:08:25,  1.46it/s]Evaluating on VQA val set:  11% 719/6699 [08:28<1:10:07,  1.42it/s]Evaluating on VQA val set:  11% 720/6699 [08:28<1:08:22,  1.46it/s]Evaluating on VQA val set:  11% 721/6699 [08:29<1:08:36,  1.45it/s]Evaluating on VQA val set:  11% 722/6699 [08:30<1:09:06,  1.44it/s]Evaluating on VQA val set:  11% 723/6699 [08:30<1:09:12,  1.44it/s]Evaluating on VQA val set:  11% 724/6699 [08:31<1:08:50,  1.45it/s]Evaluating on VQA val set:  11% 725/6699 [08:32<1:09:42,  1.43it/s]Evaluating on VQA val set:  11% 726/6699 [08:33<1:11:36,  1.39it/s]Evaluating on VQA val set:  11% 727/6699 [08:33<1:11:33,  1.39it/s]Evaluating on VQA val set:  11% 728/6699 [08:34<1:11:21,  1.39it/s]Evaluating on VQA val set:  11% 729/6699 [08:35<1:10:37,  1.41it/s]Evaluating on VQA val set:  11% 730/6699 [08:35<1:10:04,  1.42it/s]Evaluating on VQA val set:  11% 731/6699 [08:36<1:10:28,  1.41it/s]Evaluating on VQA val set:  11% 732/6699 [08:37<1:11:07,  1.40it/s]Evaluating on VQA val set:  11% 733/6699 [08:38<1:10:41,  1.41it/s]Evaluating on VQA val set:  11% 734/6699 [08:38<1:09:46,  1.42it/s]Evaluating on VQA val set:  11% 735/6699 [08:39<1:10:09,  1.42it/s]Evaluating on VQA val set:  11% 736/6699 [08:40<1:07:58,  1.46it/s]Evaluating on VQA val set:  11% 737/6699 [08:40<1:08:57,  1.44it/s]Evaluating on VQA val set:  11% 738/6699 [08:41<1:09:42,  1.43it/s]Evaluating on VQA val set:  11% 739/6699 [08:42<1:12:00,  1.38it/s]Evaluating on VQA val set:  11% 740/6699 [08:43<1:13:25,  1.35it/s]Evaluating on VQA val set:  11% 741/6699 [08:43<1:14:36,  1.33it/s]Evaluating on VQA val set:  11% 742/6699 [08:44<1:12:25,  1.37it/s]Evaluating on VQA val set:  11% 743/6699 [08:45<1:11:59,  1.38it/s]Evaluating on VQA val set:  11% 744/6699 [08:45<1:10:03,  1.42it/s]Evaluating on VQA val set:  11% 745/6699 [08:46<1:09:49,  1.42it/s]Evaluating on VQA val set:  11% 746/6699 [08:47<1:08:56,  1.44it/s]Evaluating on VQA val set:  11% 747/6699 [08:48<1:09:22,  1.43it/s]Evaluating on VQA val set:  11% 748/6699 [08:48<1:10:44,  1.40it/s]Evaluating on VQA val set:  11% 749/6699 [08:49<1:09:35,  1.42it/s]Evaluating on VQA val set:  11% 750/6699 [08:50<1:09:21,  1.43it/s]Evaluating on VQA val set:  11% 751/6699 [08:50<1:10:29,  1.41it/s]Evaluating on VQA val set:  11% 752/6699 [08:51<1:10:04,  1.41it/s]Evaluating on VQA val set:  11% 753/6699 [08:52<1:08:45,  1.44it/s]Evaluating on VQA val set:  11% 754/6699 [08:52<1:10:21,  1.41it/s]Evaluating on VQA val set:  11% 755/6699 [08:53<1:09:35,  1.42it/s]Evaluating on VQA val set:  11% 756/6699 [08:54<1:09:53,  1.42it/s]Evaluating on VQA val set:  11% 757/6699 [08:55<1:09:43,  1.42it/s]Evaluating on VQA val set:  11% 758/6699 [08:55<1:06:19,  1.49it/s]Evaluating on VQA val set:  11% 759/6699 [08:56<1:07:13,  1.47it/s]Evaluating on VQA val set:  11% 760/6699 [08:57<1:09:33,  1.42it/s]Evaluating on VQA val set:  11% 761/6699 [08:57<1:08:48,  1.44it/s]Evaluating on VQA val set:  11% 762/6699 [08:58<1:08:16,  1.45it/s]Evaluating on VQA val set:  11% 763/6699 [08:59<1:07:34,  1.46it/s]Evaluating on VQA val set:  11% 764/6699 [08:59<1:06:00,  1.50it/s]Evaluating on VQA val set:  11% 765/6699 [09:00<1:07:35,  1.46it/s]Evaluating on VQA val set:  11% 766/6699 [09:01<1:05:36,  1.51it/s]Evaluating on VQA val set:  11% 767/6699 [09:01<1:01:51,  1.60it/s]Evaluating on VQA val set:  11% 768/6699 [09:02<1:04:16,  1.54it/s]Evaluating on VQA val set:  11% 769/6699 [09:03<1:07:23,  1.47it/s]Evaluating on VQA val set:  11% 770/6699 [09:03<1:06:39,  1.48it/s]Evaluating on VQA val set:  12% 771/6699 [09:04<1:07:30,  1.46it/s]Evaluating on VQA val set:  12% 772/6699 [09:05<1:08:56,  1.43it/s]Evaluating on VQA val set:  12% 773/6699 [09:05<1:09:41,  1.42it/s]Evaluating on VQA val set:  12% 774/6699 [09:06<1:06:53,  1.48it/s]Evaluating on VQA val set:  12% 775/6699 [09:07<1:08:45,  1.44it/s]Evaluating on VQA val set:  12% 776/6699 [09:08<1:10:45,  1.40it/s]Evaluating on VQA val set:  12% 777/6699 [09:08<1:11:21,  1.38it/s]Evaluating on VQA val set:  12% 778/6699 [09:09<1:11:15,  1.38it/s]Evaluating on VQA val set:  12% 779/6699 [09:10<1:08:50,  1.43it/s]Evaluating on VQA val set:  12% 780/6699 [09:10<1:07:50,  1.45it/s]Evaluating on VQA val set:  12% 781/6699 [09:11<1:09:34,  1.42it/s]Evaluating on VQA val set:  12% 782/6699 [09:12<1:08:35,  1.44it/s]Evaluating on VQA val set:  12% 783/6699 [09:12<1:10:06,  1.41it/s]Evaluating on VQA val set:  12% 784/6699 [09:13<1:11:50,  1.37it/s]Evaluating on VQA val set:  12% 785/6699 [09:14<1:09:57,  1.41it/s]Evaluating on VQA val set:  12% 786/6699 [09:15<1:10:48,  1.39it/s]Evaluating on VQA val set:  12% 787/6699 [09:15<1:08:15,  1.44it/s]Evaluating on VQA val set:  12% 788/6699 [09:16<1:09:15,  1.42it/s]Evaluating on VQA val set:  12% 789/6699 [09:17<1:09:09,  1.42it/s]Evaluating on VQA val set:  12% 790/6699 [09:18<1:12:35,  1.36it/s]Evaluating on VQA val set:  12% 791/6699 [09:18<1:09:54,  1.41it/s]Evaluating on VQA val set:  12% 792/6699 [09:19<1:09:28,  1.42it/s]Evaluating on VQA val set:  12% 793/6699 [09:20<1:10:15,  1.40it/s]Evaluating on VQA val set:  12% 794/6699 [09:20<1:10:52,  1.39it/s]Evaluating on VQA val set:  12% 795/6699 [09:21<1:12:46,  1.35it/s]Evaluating on VQA val set:  12% 796/6699 [09:22<1:13:53,  1.33it/s]Evaluating on VQA val set:  12% 797/6699 [09:23<1:11:10,  1.38it/s]Evaluating on VQA val set:  12% 798/6699 [09:23<1:13:01,  1.35it/s]Evaluating on VQA val set:  12% 799/6699 [09:24<1:12:37,  1.35it/s]Evaluating on VQA val set:  12% 800/6699 [09:25<1:12:56,  1.35it/s]Evaluating on VQA val set:  12% 801/6699 [09:26<1:12:03,  1.36it/s]Evaluating on VQA val set:  12% 802/6699 [09:26<1:13:04,  1.34it/s]Evaluating on VQA val set:  12% 803/6699 [09:27<1:10:11,  1.40it/s]Evaluating on VQA val set:  12% 804/6699 [09:28<1:09:32,  1.41it/s]Evaluating on VQA val set:  12% 805/6699 [09:28<1:08:29,  1.43it/s]Evaluating on VQA val set:  12% 806/6699 [09:29<1:10:53,  1.39it/s]Evaluating on VQA val set:  12% 807/6699 [09:30<1:09:05,  1.42it/s]Evaluating on VQA val set:  12% 808/6699 [09:31<1:10:27,  1.39it/s]Evaluating on VQA val set:  12% 809/6699 [09:31<1:10:03,  1.40it/s]Evaluating on VQA val set:  12% 810/6699 [09:32<1:10:41,  1.39it/s]Evaluating on VQA val set:  12% 811/6699 [09:33<1:08:56,  1.42it/s]Evaluating on VQA val set:  12% 812/6699 [09:33<1:05:18,  1.50it/s]Evaluating on VQA val set:  12% 813/6699 [09:34<1:06:02,  1.49it/s]Evaluating on VQA val set:  12% 814/6699 [09:35<1:06:40,  1.47it/s]Evaluating on VQA val set:  12% 815/6699 [09:35<1:07:47,  1.45it/s]Evaluating on VQA val set:  12% 816/6699 [09:36<1:09:01,  1.42it/s]Evaluating on VQA val set:  12% 817/6699 [09:37<1:07:49,  1.45it/s]Evaluating on VQA val set:  12% 818/6699 [09:37<1:09:02,  1.42it/s]Evaluating on VQA val set:  12% 819/6699 [09:38<1:08:00,  1.44it/s]Evaluating on VQA val set:  12% 820/6699 [09:39<1:11:26,  1.37it/s]Evaluating on VQA val set:  12% 821/6699 [09:40<1:09:55,  1.40it/s]Evaluating on VQA val set:  12% 822/6699 [09:40<1:11:06,  1.38it/s]Evaluating on VQA val set:  12% 823/6699 [09:41<1:08:16,  1.43it/s]Evaluating on VQA val set:  12% 824/6699 [09:42<1:10:37,  1.39it/s]Evaluating on VQA val set:  12% 825/6699 [09:42<1:10:56,  1.38it/s]Evaluating on VQA val set:  12% 826/6699 [09:43<1:09:26,  1.41it/s]Evaluating on VQA val set:  12% 827/6699 [09:44<1:06:52,  1.46it/s]Evaluating on VQA val set:  12% 828/6699 [09:44<1:07:35,  1.45it/s]Evaluating on VQA val set:  12% 829/6699 [09:45<1:05:12,  1.50it/s]Evaluating on VQA val set:  12% 830/6699 [09:46<1:08:02,  1.44it/s]Evaluating on VQA val set:  12% 831/6699 [09:47<1:08:38,  1.42it/s]Evaluating on VQA val set:  12% 832/6699 [09:47<1:08:05,  1.44it/s]Evaluating on VQA val set:  12% 833/6699 [09:48<1:04:48,  1.51it/s]Evaluating on VQA val set:  12% 834/6699 [09:48<1:03:15,  1.55it/s]Evaluating on VQA val set:  12% 835/6699 [09:49<1:03:25,  1.54it/s]Evaluating on VQA val set:  12% 836/6699 [09:50<1:06:29,  1.47it/s]Evaluating on VQA val set:  12% 837/6699 [09:51<1:06:07,  1.48it/s]Evaluating on VQA val set:  13% 838/6699 [09:51<1:09:03,  1.41it/s]Evaluating on VQA val set:  13% 839/6699 [09:52<1:06:55,  1.46it/s]Evaluating on VQA val set:  13% 840/6699 [09:53<1:05:40,  1.49it/s]Evaluating on VQA val set:  13% 841/6699 [09:53<1:05:49,  1.48it/s]Evaluating on VQA val set:  13% 842/6699 [09:54<1:08:40,  1.42it/s]Evaluating on VQA val set:  13% 843/6699 [09:55<1:05:00,  1.50it/s]Evaluating on VQA val set:  13% 844/6699 [09:55<1:05:49,  1.48it/s]Evaluating on VQA val set:  13% 845/6699 [09:56<1:04:32,  1.51it/s]Evaluating on VQA val set:  13% 846/6699 [09:57<1:08:13,  1.43it/s]Evaluating on VQA val set:  13% 847/6699 [09:57<1:07:51,  1.44it/s]Evaluating on VQA val set:  13% 848/6699 [09:58<1:05:59,  1.48it/s]Evaluating on VQA val set:  13% 849/6699 [09:59<1:06:37,  1.46it/s]Evaluating on VQA val set:  13% 850/6699 [10:00<1:10:33,  1.38it/s]Evaluating on VQA val set:  13% 851/6699 [10:00<1:07:40,  1.44it/s]Evaluating on VQA val set:  13% 852/6699 [10:01<1:12:06,  1.35it/s]Evaluating on VQA val set:  13% 853/6699 [10:02<1:09:03,  1.41it/s]Evaluating on VQA val set:  13% 854/6699 [10:02<1:11:26,  1.36it/s]Evaluating on VQA val set:  13% 855/6699 [10:03<1:09:21,  1.40it/s]Evaluating on VQA val set:  13% 856/6699 [10:04<1:11:07,  1.37it/s]Evaluating on VQA val set:  13% 857/6699 [10:05<1:11:19,  1.37it/s]Evaluating on VQA val set:  13% 858/6699 [10:05<1:11:46,  1.36it/s]Evaluating on VQA val set:  13% 859/6699 [10:06<1:11:47,  1.36it/s]Evaluating on VQA val set:  13% 860/6699 [10:07<1:12:28,  1.34it/s]Evaluating on VQA val set:  13% 861/6699 [10:08<1:11:27,  1.36it/s]Evaluating on VQA val set:  13% 862/6699 [10:08<1:09:48,  1.39it/s]Evaluating on VQA val set:  13% 863/6699 [10:09<1:07:48,  1.43it/s]Evaluating on VQA val set:  13% 864/6699 [10:10<1:08:04,  1.43it/s]Evaluating on VQA val set:  13% 865/6699 [10:10<1:05:53,  1.48it/s]Evaluating on VQA val set:  13% 866/6699 [10:11<1:08:04,  1.43it/s]Evaluating on VQA val set:  13% 867/6699 [10:12<1:07:32,  1.44it/s]Evaluating on VQA val set:  13% 868/6699 [10:13<1:11:18,  1.36it/s]Evaluating on VQA val set:  13% 869/6699 [10:13<1:08:25,  1.42it/s]Evaluating on VQA val set:  13% 870/6699 [10:14<1:10:23,  1.38it/s]Evaluating on VQA val set:  13% 871/6699 [10:15<1:08:42,  1.41it/s]Evaluating on VQA val set:  13% 872/6699 [10:15<1:10:01,  1.39it/s]Evaluating on VQA val set:  13% 873/6699 [10:16<1:09:12,  1.40it/s]Evaluating on VQA val set:  13% 874/6699 [10:17<1:12:42,  1.34it/s]Evaluating on VQA val set:  13% 875/6699 [10:18<1:10:48,  1.37it/s]Evaluating on VQA val set:  13% 876/6699 [10:18<1:11:18,  1.36it/s]Evaluating on VQA val set:  13% 877/6699 [10:19<1:09:22,  1.40it/s]Evaluating on VQA val set:  13% 878/6699 [10:20<1:11:15,  1.36it/s]Evaluating on VQA val set:  13% 879/6699 [10:20<1:10:12,  1.38it/s]Evaluating on VQA val set:  13% 880/6699 [10:21<1:11:21,  1.36it/s]Evaluating on VQA val set:  13% 881/6699 [10:22<1:06:47,  1.45it/s]Evaluating on VQA val set:  13% 882/6699 [10:23<1:08:21,  1.42it/s]Evaluating on VQA val set:  13% 883/6699 [10:23<1:10:14,  1.38it/s]Evaluating on VQA val set:  13% 884/6699 [10:24<1:11:56,  1.35it/s]Evaluating on VQA val set:  13% 885/6699 [10:25<1:11:26,  1.36it/s]Evaluating on VQA val set:  13% 886/6699 [10:26<1:11:37,  1.35it/s]Evaluating on VQA val set:  13% 887/6699 [10:26<1:09:12,  1.40it/s]Evaluating on VQA val set:  13% 888/6699 [10:27<1:10:18,  1.38it/s]Evaluating on VQA val set:  13% 889/6699 [10:28<1:10:14,  1.38it/s]Evaluating on VQA val set:  13% 890/6699 [10:28<1:08:06,  1.42it/s]Evaluating on VQA val set:  13% 891/6699 [10:29<1:06:58,  1.45it/s]Evaluating on VQA val set:  13% 892/6699 [10:30<1:08:22,  1.42it/s]Evaluating on VQA val set:  13% 893/6699 [10:30<1:07:05,  1.44it/s]Evaluating on VQA val set:  13% 894/6699 [10:31<1:10:01,  1.38it/s]Evaluating on VQA val set:  13% 895/6699 [10:32<1:09:51,  1.38it/s]Evaluating on VQA val set:  13% 896/6699 [10:33<1:11:37,  1.35it/s]Evaluating on VQA val set:  13% 897/6699 [10:33<1:09:31,  1.39it/s]Evaluating on VQA val set:  13% 898/6699 [10:34<1:12:34,  1.33it/s]Evaluating on VQA val set:  13% 899/6699 [10:35<1:11:35,  1.35it/s]Evaluating on VQA val set:  13% 900/6699 [10:36<1:12:43,  1.33it/s]Evaluating on VQA val set:  13% 901/6699 [10:36<1:12:15,  1.34it/s]Evaluating on VQA val set:  13% 902/6699 [10:37<1:07:46,  1.43it/s]Evaluating on VQA val set:  13% 903/6699 [10:38<1:06:46,  1.45it/s]Evaluating on VQA val set:  13% 904/6699 [10:38<1:09:43,  1.39it/s]Evaluating on VQA val set:  14% 905/6699 [10:39<1:07:47,  1.42it/s]Evaluating on VQA val set:  14% 906/6699 [10:40<1:10:38,  1.37it/s]Evaluating on VQA val set:  14% 907/6699 [10:41<1:11:36,  1.35it/s]Evaluating on VQA val set:  14% 908/6699 [10:41<1:10:18,  1.37it/s]Evaluating on VQA val set:  14% 909/6699 [10:42<1:07:04,  1.44it/s]Evaluating on VQA val set:  14% 910/6699 [10:43<1:09:43,  1.38it/s]Evaluating on VQA val set:  14% 911/6699 [10:43<1:08:40,  1.40it/s]Evaluating on VQA val set:  14% 912/6699 [10:44<1:09:13,  1.39it/s]Evaluating on VQA val set:  14% 913/6699 [10:45<1:06:29,  1.45it/s]Evaluating on VQA val set:  14% 914/6699 [10:46<1:06:52,  1.44it/s]Evaluating on VQA val set:  14% 915/6699 [10:46<1:07:28,  1.43it/s]Evaluating on VQA val set:  14% 916/6699 [10:47<1:10:22,  1.37it/s]Evaluating on VQA val set:  14% 917/6699 [10:48<1:08:35,  1.40it/s]Evaluating on VQA val set:  14% 918/6699 [10:49<1:11:36,  1.35it/s]Evaluating on VQA val set:  14% 919/6699 [10:49<1:09:39,  1.38it/s]Evaluating on VQA val set:  14% 920/6699 [10:50<1:11:33,  1.35it/s]Evaluating on VQA val set:  14% 921/6699 [10:51<1:10:46,  1.36it/s]Evaluating on VQA val set:  14% 922/6699 [10:51<1:09:46,  1.38it/s]Evaluating on VQA val set:  14% 923/6699 [10:52<1:08:49,  1.40it/s]Evaluating on VQA val set:  14% 924/6699 [10:53<1:09:34,  1.38it/s]Evaluating on VQA val set:  14% 925/6699 [10:54<1:07:40,  1.42it/s]Evaluating on VQA val set:  14% 926/6699 [10:54<1:09:52,  1.38it/s]Evaluating on VQA val set:  14% 927/6699 [10:55<1:10:10,  1.37it/s]Evaluating on VQA val set:  14% 928/6699 [10:56<1:12:26,  1.33it/s]Evaluating on VQA val set:  14% 929/6699 [10:56<1:07:51,  1.42it/s]Evaluating on VQA val set:  14% 930/6699 [10:57<1:08:22,  1.41it/s]Evaluating on VQA val set:  14% 931/6699 [10:58<1:07:43,  1.42it/s]Evaluating on VQA val set:  14% 932/6699 [10:59<1:08:34,  1.40it/s]Evaluating on VQA val set:  14% 933/6699 [10:59<1:06:46,  1.44it/s]Evaluating on VQA val set:  14% 934/6699 [11:00<1:09:59,  1.37it/s]Evaluating on VQA val set:  14% 935/6699 [11:01<1:08:00,  1.41it/s]Evaluating on VQA val set:  14% 936/6699 [11:01<1:09:01,  1.39it/s]Evaluating on VQA val set:  14% 937/6699 [11:02<1:09:02,  1.39it/s]Evaluating on VQA val set:  14% 938/6699 [11:03<1:12:05,  1.33it/s]Evaluating on VQA val set:  14% 939/6699 [11:04<1:10:42,  1.36it/s]Evaluating on VQA val set:  14% 940/6699 [11:05<1:12:29,  1.32it/s]Evaluating on VQA val set:  14% 941/6699 [11:05<1:08:24,  1.40it/s]Evaluating on VQA val set:  14% 942/6699 [11:06<1:07:59,  1.41it/s]Evaluating on VQA val set:  14% 943/6699 [11:06<1:03:35,  1.51it/s]Evaluating on VQA val set:  14% 944/6699 [11:07<1:05:36,  1.46it/s]Evaluating on VQA val set:  14% 945/6699 [11:08<1:05:35,  1.46it/s]Evaluating on VQA val set:  14% 946/6699 [11:08<1:05:47,  1.46it/s]Evaluating on VQA val set:  14% 947/6699 [11:09<1:05:31,  1.46it/s]Evaluating on VQA val set:  14% 948/6699 [11:10<1:10:02,  1.37it/s]Evaluating on VQA val set:  14% 949/6699 [11:11<1:08:22,  1.40it/s]Evaluating on VQA val set:  14% 950/6699 [11:11<1:08:46,  1.39it/s]Evaluating on VQA val set:  14% 951/6699 [11:12<1:08:56,  1.39it/s]Evaluating on VQA val set:  14% 952/6699 [11:13<1:08:13,  1.40it/s]Evaluating on VQA val set:  14% 953/6699 [11:14<1:08:20,  1.40it/s]Evaluating on VQA val set:  14% 954/6699 [11:14<1:11:30,  1.34it/s]Evaluating on VQA val set:  14% 955/6699 [11:15<1:08:11,  1.40it/s]Evaluating on VQA val set:  14% 956/6699 [11:16<1:10:43,  1.35it/s]Evaluating on VQA val set:  14% 957/6699 [11:16<1:08:01,  1.41it/s]Evaluating on VQA val set:  14% 958/6699 [11:17<1:09:07,  1.38it/s]Evaluating on VQA val set:  14% 959/6699 [11:18<1:07:47,  1.41it/s]Evaluating on VQA val set:  14% 960/6699 [11:19<1:08:00,  1.41it/s]Evaluating on VQA val set:  14% 961/6699 [11:19<1:05:26,  1.46it/s]Evaluating on VQA val set:  14% 962/6699 [11:20<1:08:29,  1.40it/s]Evaluating on VQA val set:  14% 963/6699 [11:21<1:09:38,  1.37it/s]Evaluating on VQA val set:  14% 964/6699 [11:22<1:10:18,  1.36it/s]Evaluating on VQA val set:  14% 965/6699 [11:22<1:09:34,  1.37it/s]Evaluating on VQA val set:  14% 966/6699 [11:23<1:09:18,  1.38it/s]Evaluating on VQA val set:  14% 967/6699 [11:24<1:07:23,  1.42it/s]Evaluating on VQA val set:  14% 968/6699 [11:24<1:08:14,  1.40it/s]Evaluating on VQA val set:  14% 969/6699 [11:25<1:07:01,  1.42it/s]Evaluating on VQA val set:  14% 970/6699 [11:26<1:04:54,  1.47it/s]Evaluating on VQA val set:  14% 971/6699 [11:26<1:02:47,  1.52it/s]Evaluating on VQA val set:  15% 972/6699 [11:27<1:04:18,  1.48it/s]Evaluating on VQA val set:  15% 973/6699 [11:28<1:05:41,  1.45it/s]Evaluating on VQA val set:  15% 974/6699 [11:28<1:05:25,  1.46it/s]Evaluating on VQA val set:  15% 975/6699 [11:29<1:06:19,  1.44it/s]Evaluating on VQA val set:  15% 976/6699 [11:30<1:03:35,  1.50it/s]Evaluating on VQA val set:  15% 977/6699 [11:30<1:04:53,  1.47it/s]Evaluating on VQA val set:  15% 978/6699 [11:31<1:05:05,  1.47it/s]Evaluating on VQA val set:  15% 979/6699 [11:32<1:06:28,  1.43it/s]Evaluating on VQA val set:  15% 980/6699 [11:32<1:04:39,  1.47it/s]Evaluating on VQA val set:  15% 981/6699 [11:33<1:06:16,  1.44it/s]Evaluating on VQA val set:  15% 982/6699 [11:34<1:06:48,  1.43it/s]Evaluating on VQA val set:  15% 983/6699 [11:35<1:07:48,  1.40it/s]Evaluating on VQA val set:  15% 984/6699 [11:35<1:08:22,  1.39it/s]Evaluating on VQA val set:  15% 985/6699 [11:36<1:08:20,  1.39it/s]Evaluating on VQA val set:  15% 986/6699 [11:37<1:06:46,  1.43it/s]Evaluating on VQA val set:  15% 987/6699 [11:37<1:06:49,  1.42it/s]Evaluating on VQA val set:  15% 988/6699 [11:38<1:07:44,  1.41it/s]Evaluating on VQA val set:  15% 989/6699 [11:39<1:08:32,  1.39it/s]Evaluating on VQA val set:  15% 990/6699 [11:40<1:07:05,  1.42it/s]Evaluating on VQA val set:  15% 991/6699 [11:40<1:08:11,  1.40it/s]Evaluating on VQA val set:  15% 992/6699 [11:41<1:07:24,  1.41it/s]Evaluating on VQA val set:  15% 993/6699 [11:42<1:08:59,  1.38it/s]Evaluating on VQA val set:  15% 994/6699 [11:42<1:07:38,  1.41it/s]Evaluating on VQA val set:  15% 995/6699 [11:43<1:08:01,  1.40it/s]Evaluating on VQA val set:  15% 996/6699 [11:44<1:08:21,  1.39it/s]Evaluating on VQA val set:  15% 997/6699 [11:45<1:07:26,  1.41it/s]Evaluating on VQA val set:  15% 998/6699 [11:45<1:05:55,  1.44it/s]Evaluating on VQA val set:  15% 999/6699 [11:46<1:06:02,  1.44it/s]Evaluating on VQA val set:  15% 1000/6699 [11:47<1:06:43,  1.42it/s]Evaluating on VQA val set:  15% 1001/6699 [11:47<1:06:45,  1.42it/s]Evaluating on VQA val set:  15% 1002/6699 [11:48<1:08:06,  1.39it/s]Evaluating on VQA val set:  15% 1003/6699 [11:49<1:07:36,  1.40it/s]Evaluating on VQA val set:  15% 1004/6699 [11:49<1:04:51,  1.46it/s]Evaluating on VQA val set:  15% 1005/6699 [11:50<1:03:28,  1.49it/s]Evaluating on VQA val set:  15% 1006/6699 [11:51<1:04:15,  1.48it/s]Evaluating on VQA val set:  15% 1007/6699 [11:51<1:03:59,  1.48it/s]Evaluating on VQA val set:  15% 1008/6699 [11:52<1:05:15,  1.45it/s]Evaluating on VQA val set:  15% 1009/6699 [11:53<1:05:35,  1.45it/s]Evaluating on VQA val set:  15% 1010/6699 [11:54<1:04:02,  1.48it/s]Evaluating on VQA val set:  15% 1011/6699 [11:54<1:06:30,  1.43it/s]Evaluating on VQA val set:  15% 1012/6699 [11:55<1:06:59,  1.41it/s]Evaluating on VQA val set:  15% 1013/6699 [11:56<1:06:22,  1.43it/s]Evaluating on VQA val set:  15% 1014/6699 [11:56<1:05:01,  1.46it/s]Evaluating on VQA val set:  15% 1015/6699 [11:57<1:05:18,  1.45it/s]Evaluating on VQA val set:  15% 1016/6699 [11:58<1:04:38,  1.47it/s]Evaluating on VQA val set:  15% 1017/6699 [11:58<1:06:00,  1.43it/s]Evaluating on VQA val set:  15% 1018/6699 [11:59<1:06:06,  1.43it/s]Evaluating on VQA val set:  15% 1019/6699 [12:00<1:04:31,  1.47it/s]Evaluating on VQA val set:  15% 1020/6699 [12:00<1:05:26,  1.45it/s]Evaluating on VQA val set:  15% 1021/6699 [12:01<1:06:24,  1.42it/s]Evaluating on VQA val set:  15% 1022/6699 [12:02<1:06:15,  1.43it/s]Evaluating on VQA val set:  15% 1023/6699 [12:03<1:05:48,  1.44it/s]Evaluating on VQA val set:  15% 1024/6699 [12:03<1:05:52,  1.44it/s]Evaluating on VQA val set:  15% 1025/6699 [12:04<1:05:08,  1.45it/s]Evaluating on VQA val set:  15% 1026/6699 [12:05<1:06:40,  1.42it/s]Evaluating on VQA val set:  15% 1027/6699 [12:05<1:07:06,  1.41it/s]Evaluating on VQA val set:  15% 1028/6699 [12:06<1:06:09,  1.43it/s]Evaluating on VQA val set:  15% 1029/6699 [12:07<1:06:56,  1.41it/s]Evaluating on VQA val set:  15% 1030/6699 [12:08<1:06:10,  1.43it/s]Evaluating on VQA val set:  15% 1031/6699 [12:08<1:03:18,  1.49it/s]Evaluating on VQA val set:  15% 1032/6699 [12:09<1:02:19,  1.52it/s]Evaluating on VQA val set:  15% 1033/6699 [12:10<1:05:33,  1.44it/s]Evaluating on VQA val set:  15% 1034/6699 [12:10<1:06:13,  1.43it/s]Evaluating on VQA val set:  15% 1035/6699 [12:11<1:05:18,  1.45it/s]Evaluating on VQA val set:  15% 1036/6699 [12:12<1:05:58,  1.43it/s]Evaluating on VQA val set:  15% 1037/6699 [12:12<1:05:49,  1.43it/s]Evaluating on VQA val set:  15% 1038/6699 [12:13<1:07:02,  1.41it/s]Evaluating on VQA val set:  16% 1039/6699 [12:14<1:06:03,  1.43it/s]Evaluating on VQA val set:  16% 1040/6699 [12:14<1:06:09,  1.43it/s]Evaluating on VQA val set:  16% 1041/6699 [12:15<1:04:21,  1.47it/s]Evaluating on VQA val set:  16% 1042/6699 [12:16<1:04:42,  1.46it/s]Evaluating on VQA val set:  16% 1043/6699 [12:16<1:02:58,  1.50it/s]Evaluating on VQA val set:  16% 1044/6699 [12:17<1:02:55,  1.50it/s]Evaluating on VQA val set:  16% 1045/6699 [12:18<1:04:13,  1.47it/s]Evaluating on VQA val set:  16% 1046/6699 [12:19<1:07:13,  1.40it/s]Evaluating on VQA val set:  16% 1047/6699 [12:19<1:06:23,  1.42it/s]Evaluating on VQA val set:  16% 1048/6699 [12:20<1:06:51,  1.41it/s]Evaluating on VQA val set:  16% 1049/6699 [12:21<1:04:52,  1.45it/s]Evaluating on VQA val set:  16% 1050/6699 [12:21<1:05:15,  1.44it/s]Evaluating on VQA val set:  16% 1051/6699 [12:22<1:05:30,  1.44it/s]Evaluating on VQA val set:  16% 1052/6699 [12:23<1:06:05,  1.42it/s]Evaluating on VQA val set:  16% 1053/6699 [12:23<1:06:22,  1.42it/s]Evaluating on VQA val set:  16% 1054/6699 [12:24<1:07:48,  1.39it/s]Evaluating on VQA val set:  16% 1055/6699 [12:25<1:08:04,  1.38it/s]Evaluating on VQA val set:  16% 1056/6699 [12:26<1:07:19,  1.40it/s]Evaluating on VQA val set:  16% 1057/6699 [12:26<1:05:38,  1.43it/s]Evaluating on VQA val set:  16% 1058/6699 [12:27<1:04:53,  1.45it/s]Evaluating on VQA val set:  16% 1059/6699 [12:28<1:06:34,  1.41it/s]Evaluating on VQA val set:  16% 1060/6699 [12:28<1:06:00,  1.42it/s]Evaluating on VQA val set:  16% 1061/6699 [12:29<1:07:38,  1.39it/s]Evaluating on VQA val set:  16% 1062/6699 [12:30<1:08:06,  1.38it/s]Evaluating on VQA val set:  16% 1063/6699 [12:31<1:09:27,  1.35it/s]Evaluating on VQA val set:  16% 1064/6699 [12:31<1:08:18,  1.37it/s]Evaluating on VQA val set:  16% 1065/6699 [12:32<1:08:48,  1.36it/s]Evaluating on VQA val set:  16% 1066/6699 [12:33<1:08:52,  1.36it/s]Evaluating on VQA val set:  16% 1067/6699 [12:33<1:05:56,  1.42it/s]Evaluating on VQA val set:  16% 1068/6699 [12:34<1:04:07,  1.46it/s]Evaluating on VQA val set:  16% 1069/6699 [12:35<1:04:37,  1.45it/s]Evaluating on VQA val set:  16% 1070/6699 [12:36<1:06:09,  1.42it/s]Evaluating on VQA val set:  16% 1071/6699 [12:36<1:06:47,  1.40it/s]Evaluating on VQA val set:  16% 1072/6699 [12:37<1:07:40,  1.39it/s]Evaluating on VQA val set:  16% 1073/6699 [12:38<1:07:04,  1.40it/s]Evaluating on VQA val set:  16% 1074/6699 [12:38<1:06:32,  1.41it/s]Evaluating on VQA val set:  16% 1075/6699 [12:39<1:06:00,  1.42it/s]Evaluating on VQA val set:  16% 1076/6699 [12:40<1:06:40,  1.41it/s]Evaluating on VQA val set:  16% 1077/6699 [12:41<1:07:07,  1.40it/s]Evaluating on VQA val set:  16% 1078/6699 [12:41<1:05:29,  1.43it/s]Evaluating on VQA val set:  16% 1079/6699 [12:42<1:07:15,  1.39it/s]Evaluating on VQA val set:  16% 1080/6699 [12:43<1:04:13,  1.46it/s]Evaluating on VQA val set:  16% 1081/6699 [12:43<1:04:33,  1.45it/s]Evaluating on VQA val set:  16% 1082/6699 [12:44<1:04:09,  1.46it/s]Evaluating on VQA val set:  16% 1083/6699 [12:45<1:05:33,  1.43it/s]Evaluating on VQA val set:  16% 1084/6699 [12:45<1:07:24,  1.39it/s]Evaluating on VQA val set:  16% 1085/6699 [12:46<1:07:34,  1.38it/s]Evaluating on VQA val set:  16% 1086/6699 [12:47<1:06:51,  1.40it/s]Evaluating on VQA val set:  16% 1087/6699 [12:48<1:06:49,  1.40it/s]Evaluating on VQA val set:  16% 1088/6699 [12:48<1:04:46,  1.44it/s]Evaluating on VQA val set:  16% 1089/6699 [12:49<1:04:15,  1.46it/s]Evaluating on VQA val set:  16% 1090/6699 [12:50<1:04:57,  1.44it/s]Evaluating on VQA val set:  16% 1091/6699 [12:50<1:05:32,  1.43it/s]Evaluating on VQA val set:  16% 1092/6699 [12:51<1:02:11,  1.50it/s]Evaluating on VQA val set:  16% 1093/6699 [12:52<1:01:46,  1.51it/s]Evaluating on VQA val set:  16% 1094/6699 [12:52<1:02:19,  1.50it/s]Evaluating on VQA val set:  16% 1095/6699 [12:53<1:04:44,  1.44it/s]Evaluating on VQA val set:  16% 1096/6699 [12:54<1:05:25,  1.43it/s]Evaluating on VQA val set:  16% 1097/6699 [12:54<1:06:18,  1.41it/s]Evaluating on VQA val set:  16% 1098/6699 [12:55<1:09:12,  1.35it/s]Evaluating on VQA val set:  16% 1099/6699 [12:56<1:07:06,  1.39it/s]Evaluating on VQA val set:  16% 1100/6699 [12:57<1:06:10,  1.41it/s]Evaluating on VQA val set:  16% 1101/6699 [12:57<1:05:23,  1.43it/s]Evaluating on VQA val set:  16% 1102/6699 [12:58<1:05:01,  1.43it/s]Evaluating on VQA val set:  16% 1103/6699 [12:59<1:06:39,  1.40it/s]Evaluating on VQA val set:  16% 1104/6699 [12:59<1:06:02,  1.41it/s]Evaluating on VQA val set:  16% 1105/6699 [13:00<1:06:27,  1.40it/s]Evaluating on VQA val set:  17% 1106/6699 [13:01<1:06:13,  1.41it/s]Evaluating on VQA val set:  17% 1107/6699 [13:02<1:06:34,  1.40it/s]Evaluating on VQA val set:  17% 1108/6699 [13:02<1:07:33,  1.38it/s]Evaluating on VQA val set:  17% 1109/6699 [13:03<1:07:57,  1.37it/s]Evaluating on VQA val set:  17% 1110/6699 [13:04<1:07:24,  1.38it/s]Evaluating on VQA val set:  17% 1111/6699 [13:05<1:06:25,  1.40it/s]Evaluating on VQA val set:  17% 1112/6699 [13:05<1:06:39,  1.40it/s]Evaluating on VQA val set:  17% 1113/6699 [13:06<1:06:17,  1.40it/s]Evaluating on VQA val set:  17% 1114/6699 [13:07<1:07:13,  1.38it/s]Evaluating on VQA val set:  17% 1115/6699 [13:07<1:06:49,  1.39it/s]Evaluating on VQA val set:  17% 1116/6699 [13:08<1:07:12,  1.38it/s]Evaluating on VQA val set:  17% 1117/6699 [13:09<1:07:18,  1.38it/s]Evaluating on VQA val set:  17% 1118/6699 [13:10<1:05:43,  1.42it/s]Evaluating on VQA val set:  17% 1119/6699 [13:10<1:06:31,  1.40it/s]Evaluating on VQA val set:  17% 1120/6699 [13:11<1:06:50,  1.39it/s]Evaluating on VQA val set:  17% 1121/6699 [13:12<1:06:18,  1.40it/s]Evaluating on VQA val set:  17% 1122/6699 [13:12<1:05:32,  1.42it/s]Evaluating on VQA val set:  17% 1123/6699 [13:13<1:06:12,  1.40it/s]Evaluating on VQA val set:  17% 1124/6699 [13:14<1:07:14,  1.38it/s]Evaluating on VQA val set:  17% 1125/6699 [13:15<1:07:28,  1.38it/s]Evaluating on VQA val set:  17% 1126/6699 [13:15<1:06:34,  1.40it/s]Evaluating on VQA val set:  17% 1127/6699 [13:16<1:06:57,  1.39it/s]Evaluating on VQA val set:  17% 1128/6699 [13:17<1:07:12,  1.38it/s]Evaluating on VQA val set:  17% 1129/6699 [13:17<1:05:42,  1.41it/s]Evaluating on VQA val set:  17% 1130/6699 [13:18<1:05:52,  1.41it/s]Evaluating on VQA val set:  17% 1131/6699 [13:19<1:05:47,  1.41it/s]Evaluating on VQA val set:  17% 1132/6699 [13:19<1:04:29,  1.44it/s]Evaluating on VQA val set:  17% 1133/6699 [13:20<1:05:01,  1.43it/s]Evaluating on VQA val set:  17% 1134/6699 [13:21<1:05:59,  1.41it/s]Evaluating on VQA val set:  17% 1135/6699 [13:22<1:03:29,  1.46it/s]Evaluating on VQA val set:  17% 1136/6699 [13:22<1:03:48,  1.45it/s]Evaluating on VQA val set:  17% 1137/6699 [13:23<1:03:40,  1.46it/s]Evaluating on VQA val set:  17% 1138/6699 [13:24<1:04:11,  1.44it/s]Evaluating on VQA val set:  17% 1139/6699 [13:24<1:03:33,  1.46it/s]Evaluating on VQA val set:  17% 1140/6699 [13:25<1:03:19,  1.46it/s]Evaluating on VQA val set:  17% 1141/6699 [13:26<1:03:23,  1.46it/s]Evaluating on VQA val set:  17% 1142/6699 [13:26<1:02:35,  1.48it/s]Evaluating on VQA val set:  17% 1143/6699 [13:27<1:03:07,  1.47it/s]Evaluating on VQA val set:  17% 1144/6699 [13:28<1:03:28,  1.46it/s]Evaluating on VQA val set:  17% 1145/6699 [13:28<1:04:28,  1.44it/s]Evaluating on VQA val set:  17% 1146/6699 [13:29<1:04:12,  1.44it/s]Evaluating on VQA val set:  17% 1147/6699 [13:30<1:04:59,  1.42it/s]Evaluating on VQA val set:  17% 1148/6699 [13:31<1:03:59,  1.45it/s]Evaluating on VQA val set:  17% 1149/6699 [13:31<1:02:13,  1.49it/s]Evaluating on VQA val set:  17% 1150/6699 [13:32<59:58,  1.54it/s]  Evaluating on VQA val set:  17% 1151/6699 [13:32<1:01:59,  1.49it/s]Evaluating on VQA val set:  17% 1152/6699 [13:33<1:02:26,  1.48it/s]Evaluating on VQA val set:  17% 1153/6699 [13:34<1:03:57,  1.45it/s]Evaluating on VQA val set:  17% 1154/6699 [13:35<1:03:28,  1.46it/s]Evaluating on VQA val set:  17% 1155/6699 [13:35<1:03:16,  1.46it/s]Evaluating on VQA val set:  17% 1156/6699 [13:36<1:05:46,  1.40it/s]Evaluating on VQA val set:  17% 1157/6699 [13:37<1:04:06,  1.44it/s]Evaluating on VQA val set:  17% 1158/6699 [13:37<1:06:15,  1.39it/s]Evaluating on VQA val set:  17% 1159/6699 [13:38<1:07:54,  1.36it/s]Evaluating on VQA val set:  17% 1160/6699 [13:39<1:05:44,  1.40it/s]Evaluating on VQA val set:  17% 1161/6699 [13:40<1:05:29,  1.41it/s]Evaluating on VQA val set:  17% 1162/6699 [13:40<1:06:14,  1.39it/s]Evaluating on VQA val set:  17% 1163/6699 [13:41<1:04:08,  1.44it/s]Evaluating on VQA val set:  17% 1164/6699 [13:42<1:06:40,  1.38it/s]Evaluating on VQA val set:  17% 1165/6699 [13:42<1:07:12,  1.37it/s]Evaluating on VQA val set:  17% 1166/6699 [13:43<1:05:45,  1.40it/s]Evaluating on VQA val set:  17% 1167/6699 [13:44<1:05:16,  1.41it/s]Evaluating on VQA val set:  17% 1168/6699 [13:45<1:05:39,  1.40it/s]Evaluating on VQA val set:  17% 1169/6699 [13:45<1:04:55,  1.42it/s]Evaluating on VQA val set:  17% 1170/6699 [13:46<1:04:09,  1.44it/s]Evaluating on VQA val set:  17% 1171/6699 [13:47<1:04:09,  1.44it/s]Evaluating on VQA val set:  17% 1172/6699 [13:47<1:04:40,  1.42it/s]Evaluating on VQA val set:  18% 1173/6699 [13:48<1:05:42,  1.40it/s]Evaluating on VQA val set:  18% 1174/6699 [13:49<1:05:08,  1.41it/s]Evaluating on VQA val set:  18% 1175/6699 [13:50<1:05:29,  1.41it/s]Evaluating on VQA val set:  18% 1176/6699 [13:50<1:04:23,  1.43it/s]Evaluating on VQA val set:  18% 1177/6699 [13:51<1:06:31,  1.38it/s]Evaluating on VQA val set:  18% 1178/6699 [13:52<1:04:17,  1.43it/s]Evaluating on VQA val set:  18% 1179/6699 [13:52<1:04:08,  1.43it/s]Evaluating on VQA val set:  18% 1180/6699 [13:53<1:05:14,  1.41it/s]Evaluating on VQA val set:  18% 1181/6699 [13:54<1:06:09,  1.39it/s]Evaluating on VQA val set:  18% 1182/6699 [13:55<1:06:51,  1.38it/s]Evaluating on VQA val set:  18% 1183/6699 [13:55<1:06:52,  1.37it/s]Evaluating on VQA val set:  18% 1184/6699 [13:56<1:08:03,  1.35it/s]Evaluating on VQA val set:  18% 1185/6699 [13:57<1:06:24,  1.38it/s]Evaluating on VQA val set:  18% 1186/6699 [13:57<1:07:56,  1.35it/s]Evaluating on VQA val set:  18% 1187/6699 [13:58<1:06:20,  1.38it/s]Evaluating on VQA val set:  18% 1188/6699 [13:59<1:06:47,  1.38it/s]Evaluating on VQA val set:  18% 1189/6699 [14:00<1:07:07,  1.37it/s]Evaluating on VQA val set:  18% 1190/6699 [14:00<1:06:46,  1.38it/s]Evaluating on VQA val set:  18% 1191/6699 [14:01<1:07:49,  1.35it/s]Evaluating on VQA val set:  18% 1192/6699 [14:02<1:08:13,  1.35it/s]Evaluating on VQA val set:  18% 1193/6699 [14:03<1:07:24,  1.36it/s]Evaluating on VQA val set:  18% 1194/6699 [14:03<1:08:30,  1.34it/s]Evaluating on VQA val set:  18% 1195/6699 [14:04<1:07:11,  1.37it/s]Evaluating on VQA val set:  18% 1196/6699 [14:05<1:06:07,  1.39it/s]Evaluating on VQA val set:  18% 1197/6699 [14:06<1:08:19,  1.34it/s]Evaluating on VQA val set:  18% 1198/6699 [14:06<1:07:43,  1.35it/s]Evaluating on VQA val set:  18% 1199/6699 [14:07<1:07:16,  1.36it/s]Evaluating on VQA val set:  18% 1200/6699 [14:08<1:06:22,  1.38it/s]Evaluating on VQA val set:  18% 1201/6699 [14:08<1:06:27,  1.38it/s]Evaluating on VQA val set:  18% 1202/6699 [14:09<1:06:22,  1.38it/s]Evaluating on VQA val set:  18% 1203/6699 [14:10<1:05:10,  1.41it/s]Evaluating on VQA val set:  18% 1204/6699 [14:10<57:10,  1.60it/s]  Evaluating on VQA val set:  18% 1205/6699 [14:11<58:50,  1.56it/s]Evaluating on VQA val set:  18% 1206/6699 [14:12<1:00:37,  1.51it/s]Evaluating on VQA val set:  18% 1207/6699 [14:12<1:03:15,  1.45it/s]Evaluating on VQA val set:  18% 1208/6699 [14:13<1:00:12,  1.52it/s]Evaluating on VQA val set:  18% 1209/6699 [14:14<1:01:35,  1.49it/s]Evaluating on VQA val set:  18% 1210/6699 [14:14<1:02:23,  1.47it/s]Evaluating on VQA val set:  18% 1211/6699 [14:15<1:02:14,  1.47it/s]Evaluating on VQA val set:  18% 1212/6699 [14:16<1:03:06,  1.45it/s]Evaluating on VQA val set:  18% 1213/6699 [14:17<1:04:34,  1.42it/s]Evaluating on VQA val set:  18% 1214/6699 [14:17<1:03:41,  1.44it/s]Evaluating on VQA val set:  18% 1215/6699 [14:18<1:04:12,  1.42it/s]Evaluating on VQA val set:  18% 1216/6699 [14:19<1:04:47,  1.41it/s]Evaluating on VQA val set:  18% 1217/6699 [14:19<1:04:29,  1.42it/s]Evaluating on VQA val set:  18% 1218/6699 [14:20<1:05:19,  1.40it/s]Evaluating on VQA val set:  18% 1219/6699 [14:21<1:03:45,  1.43it/s]Evaluating on VQA val set:  18% 1220/6699 [14:21<1:02:52,  1.45it/s]Evaluating on VQA val set:  18% 1221/6699 [14:22<1:04:23,  1.42it/s]Evaluating on VQA val set:  18% 1222/6699 [14:23<1:05:31,  1.39it/s]Evaluating on VQA val set:  18% 1223/6699 [14:24<1:06:21,  1.38it/s]Evaluating on VQA val set:  18% 1224/6699 [14:24<1:07:32,  1.35it/s]Evaluating on VQA val set:  18% 1225/6699 [14:25<1:07:15,  1.36it/s]Evaluating on VQA val set:  18% 1226/6699 [14:26<1:05:19,  1.40it/s]Evaluating on VQA val set:  18% 1227/6699 [14:26<1:02:02,  1.47it/s]Evaluating on VQA val set:  18% 1228/6699 [14:27<1:02:17,  1.46it/s]Evaluating on VQA val set:  18% 1229/6699 [14:28<1:01:25,  1.48it/s]Evaluating on VQA val set:  18% 1230/6699 [14:28<1:00:18,  1.51it/s]Evaluating on VQA val set:  18% 1231/6699 [14:29<1:01:30,  1.48it/s]Evaluating on VQA val set:  18% 1232/6699 [14:30<1:02:54,  1.45it/s]Evaluating on VQA val set:  18% 1233/6699 [14:30<1:00:19,  1.51it/s]Evaluating on VQA val set:  18% 1234/6699 [14:31<1:01:09,  1.49it/s]Evaluating on VQA val set:  18% 1235/6699 [14:32<1:01:47,  1.47it/s]Evaluating on VQA val set:  18% 1236/6699 [14:32<1:02:01,  1.47it/s]Evaluating on VQA val set:  18% 1237/6699 [14:33<1:00:27,  1.51it/s]Evaluating on VQA val set:  18% 1238/6699 [14:34<1:01:26,  1.48it/s]Evaluating on VQA val set:  18% 1239/6699 [14:34<1:00:56,  1.49it/s]Evaluating on VQA val set:  19% 1240/6699 [14:35<1:03:04,  1.44it/s]Evaluating on VQA val set:  19% 1241/6699 [14:36<1:05:45,  1.38it/s]Evaluating on VQA val set:  19% 1242/6699 [14:37<1:04:38,  1.41it/s]Evaluating on VQA val set:  19% 1243/6699 [14:37<1:04:45,  1.40it/s]Evaluating on VQA val set:  19% 1244/6699 [14:38<1:03:40,  1.43it/s]Evaluating on VQA val set:  19% 1245/6699 [14:39<1:03:30,  1.43it/s]Evaluating on VQA val set:  19% 1246/6699 [14:39<1:03:45,  1.43it/s]Evaluating on VQA val set:  19% 1247/6699 [14:40<1:04:19,  1.41it/s]Evaluating on VQA val set:  19% 1248/6699 [14:41<1:03:30,  1.43it/s]Evaluating on VQA val set:  19% 1249/6699 [14:42<1:04:18,  1.41it/s]Evaluating on VQA val set:  19% 1250/6699 [14:42<1:05:55,  1.38it/s]Evaluating on VQA val set:  19% 1251/6699 [14:43<1:05:42,  1.38it/s]Evaluating on VQA val set:  19% 1252/6699 [14:44<1:03:41,  1.43it/s]Evaluating on VQA val set:  19% 1253/6699 [14:44<1:02:26,  1.45it/s]Evaluating on VQA val set:  19% 1254/6699 [14:45<1:02:17,  1.46it/s]Evaluating on VQA val set:  19% 1255/6699 [14:46<1:03:40,  1.43it/s]Evaluating on VQA val set:  19% 1256/6699 [14:47<1:03:31,  1.43it/s]Evaluating on VQA val set:  19% 1257/6699 [14:47<1:04:32,  1.41it/s]Evaluating on VQA val set:  19% 1258/6699 [14:48<1:04:48,  1.40it/s]Evaluating on VQA val set:  19% 1259/6699 [14:49<1:03:53,  1.42it/s]Evaluating on VQA val set:  19% 1260/6699 [14:49<1:05:24,  1.39it/s]Evaluating on VQA val set:  19% 1261/6699 [14:50<1:06:31,  1.36it/s]Evaluating on VQA val set:  19% 1262/6699 [14:51<1:04:52,  1.40it/s]Evaluating on VQA val set:  19% 1263/6699 [14:52<1:04:36,  1.40it/s]Evaluating on VQA val set:  19% 1264/6699 [14:52<1:03:18,  1.43it/s]Evaluating on VQA val set:  19% 1265/6699 [14:53<1:03:53,  1.42it/s]Evaluating on VQA val set:  19% 1266/6699 [14:54<1:03:33,  1.42it/s]Evaluating on VQA val set:  19% 1267/6699 [14:54<1:04:04,  1.41it/s]Evaluating on VQA val set:  19% 1268/6699 [14:55<1:02:46,  1.44it/s]Evaluating on VQA val set:  19% 1269/6699 [14:56<1:03:47,  1.42it/s]Evaluating on VQA val set:  19% 1270/6699 [14:56<1:02:35,  1.45it/s]Evaluating on VQA val set:  19% 1271/6699 [14:57<1:02:23,  1.45it/s]Evaluating on VQA val set:  19% 1272/6699 [14:58<1:03:01,  1.44it/s]Evaluating on VQA val set:  19% 1273/6699 [14:59<1:04:24,  1.40it/s]Evaluating on VQA val set:  19% 1274/6699 [14:59<1:04:40,  1.40it/s]Evaluating on VQA val set:  19% 1275/6699 [15:00<1:04:15,  1.41it/s]Evaluating on VQA val set:  19% 1276/6699 [15:01<1:03:59,  1.41it/s]Evaluating on VQA val set:  19% 1277/6699 [15:01<1:05:11,  1.39it/s]Evaluating on VQA val set:  19% 1278/6699 [15:02<1:04:15,  1.41it/s]Evaluating on VQA val set:  19% 1279/6699 [15:03<1:05:28,  1.38it/s]Evaluating on VQA val set:  19% 1280/6699 [15:04<1:06:46,  1.35it/s]Evaluating on VQA val set:  19% 1281/6699 [15:04<1:06:40,  1.35it/s]Evaluating on VQA val set:  19% 1282/6699 [15:05<1:06:23,  1.36it/s]Evaluating on VQA val set:  19% 1283/6699 [15:06<1:05:45,  1.37it/s]Evaluating on VQA val set:  19% 1284/6699 [15:07<1:06:11,  1.36it/s]Evaluating on VQA val set:  19% 1285/6699 [15:07<1:04:35,  1.40it/s]Evaluating on VQA val set:  19% 1286/6699 [15:08<1:03:03,  1.43it/s]Evaluating on VQA val set:  19% 1287/6699 [15:09<1:00:32,  1.49it/s]Evaluating on VQA val set:  19% 1288/6699 [15:09<1:01:04,  1.48it/s]Evaluating on VQA val set:  19% 1289/6699 [15:10<1:01:26,  1.47it/s]Evaluating on VQA val set:  19% 1290/6699 [15:11<1:02:44,  1.44it/s]Evaluating on VQA val set:  19% 1291/6699 [15:11<1:01:59,  1.45it/s]Evaluating on VQA val set:  19% 1292/6699 [15:12<1:00:37,  1.49it/s]Evaluating on VQA val set:  19% 1293/6699 [15:13<1:02:00,  1.45it/s]Evaluating on VQA val set:  19% 1294/6699 [15:13<1:03:39,  1.42it/s]Evaluating on VQA val set:  19% 1295/6699 [15:14<1:03:16,  1.42it/s]Evaluating on VQA val set:  19% 1296/6699 [15:15<1:04:28,  1.40it/s]Evaluating on VQA val set:  19% 1297/6699 [15:15<1:01:09,  1.47it/s]Evaluating on VQA val set:  19% 1298/6699 [15:16<1:01:52,  1.45it/s]Evaluating on VQA val set:  19% 1299/6699 [15:17<1:03:44,  1.41it/s]Evaluating on VQA val set:  19% 1300/6699 [15:18<1:03:22,  1.42it/s]Evaluating on VQA val set:  19% 1301/6699 [15:18<1:03:06,  1.43it/s]Evaluating on VQA val set:  19% 1302/6699 [15:19<1:03:25,  1.42it/s]Evaluating on VQA val set:  19% 1303/6699 [15:20<1:03:35,  1.41it/s]Evaluating on VQA val set:  19% 1304/6699 [15:20<1:03:28,  1.42it/s]Evaluating on VQA val set:  19% 1305/6699 [15:21<1:02:04,  1.45it/s]Evaluating on VQA val set:  19% 1306/6699 [15:22<1:03:46,  1.41it/s]Evaluating on VQA val set:  20% 1307/6699 [15:23<1:05:04,  1.38it/s]Evaluating on VQA val set:  20% 1308/6699 [15:23<1:05:27,  1.37it/s]Evaluating on VQA val set:  20% 1309/6699 [15:24<1:03:06,  1.42it/s]Evaluating on VQA val set:  20% 1310/6699 [15:25<1:03:59,  1.40it/s]Evaluating on VQA val set:  20% 1311/6699 [15:25<1:01:30,  1.46it/s]Evaluating on VQA val set:  20% 1312/6699 [15:26<1:00:50,  1.48it/s]Evaluating on VQA val set:  20% 1313/6699 [15:27<1:01:09,  1.47it/s]Evaluating on VQA val set:  20% 1314/6699 [15:27<1:03:15,  1.42it/s]Evaluating on VQA val set:  20% 1315/6699 [15:28<1:03:50,  1.41it/s]Evaluating on VQA val set:  20% 1316/6699 [15:29<1:04:43,  1.39it/s]Evaluating on VQA val set:  20% 1317/6699 [15:30<1:04:14,  1.40it/s]Evaluating on VQA val set:  20% 1318/6699 [15:30<1:05:26,  1.37it/s]Evaluating on VQA val set:  20% 1319/6699 [15:31<1:05:37,  1.37it/s]Evaluating on VQA val set:  20% 1320/6699 [15:32<1:04:15,  1.40it/s]Evaluating on VQA val set:  20% 1321/6699 [15:33<1:05:35,  1.37it/s]Evaluating on VQA val set:  20% 1322/6699 [15:33<1:05:05,  1.38it/s]Evaluating on VQA val set:  20% 1323/6699 [15:34<1:04:32,  1.39it/s]Evaluating on VQA val set:  20% 1324/6699 [15:35<1:03:50,  1.40it/s]Evaluating on VQA val set:  20% 1325/6699 [15:35<1:02:18,  1.44it/s]Evaluating on VQA val set:  20% 1326/6699 [15:36<1:01:21,  1.46it/s]Evaluating on VQA val set:  20% 1327/6699 [15:37<1:01:08,  1.46it/s]Evaluating on VQA val set:  20% 1328/6699 [15:37<1:02:50,  1.42it/s]Evaluating on VQA val set:  20% 1329/6699 [15:38<1:02:07,  1.44it/s]Evaluating on VQA val set:  20% 1330/6699 [15:39<1:01:06,  1.46it/s]Evaluating on VQA val set:  20% 1331/6699 [15:39<1:00:27,  1.48it/s]Evaluating on VQA val set:  20% 1332/6699 [15:40<1:00:42,  1.47it/s]Evaluating on VQA val set:  20% 1333/6699 [15:41<1:03:28,  1.41it/s]Evaluating on VQA val set:  20% 1334/6699 [15:42<1:03:27,  1.41it/s]Evaluating on VQA val set:  20% 1335/6699 [15:42<1:04:17,  1.39it/s]Evaluating on VQA val set:  20% 1336/6699 [15:43<1:02:02,  1.44it/s]Evaluating on VQA val set:  20% 1337/6699 [15:44<1:02:10,  1.44it/s]Evaluating on VQA val set:  20% 1338/6699 [15:44<1:01:54,  1.44it/s]Evaluating on VQA val set:  20% 1339/6699 [15:45<1:03:00,  1.42it/s]Evaluating on VQA val set:  20% 1340/6699 [15:46<1:02:13,  1.44it/s]Evaluating on VQA val set:  20% 1341/6699 [15:47<1:03:28,  1.41it/s]Evaluating on VQA val set:  20% 1342/6699 [15:47<1:03:56,  1.40it/s]Evaluating on VQA val set:  20% 1343/6699 [15:48<1:03:16,  1.41it/s]Evaluating on VQA val set:  20% 1344/6699 [15:49<1:03:44,  1.40it/s]Evaluating on VQA val set:  20% 1345/6699 [15:49<1:05:42,  1.36it/s]Evaluating on VQA val set:  20% 1346/6699 [15:50<1:05:08,  1.37it/s]Evaluating on VQA val set:  20% 1347/6699 [15:51<1:06:45,  1.34it/s]Evaluating on VQA val set:  20% 1348/6699 [15:52<1:05:29,  1.36it/s]Evaluating on VQA val set:  20% 1349/6699 [15:52<1:05:12,  1.37it/s]Evaluating on VQA val set:  20% 1350/6699 [15:53<1:04:56,  1.37it/s]Evaluating on VQA val set:  20% 1351/6699 [15:54<1:04:43,  1.38it/s]Evaluating on VQA val set:  20% 1352/6699 [15:54<1:03:00,  1.41it/s]Evaluating on VQA val set:  20% 1353/6699 [15:55<1:01:40,  1.44it/s]Evaluating on VQA val set:  20% 1354/6699 [15:56<1:03:07,  1.41it/s]Evaluating on VQA val set:  20% 1355/6699 [15:57<1:02:49,  1.42it/s]Evaluating on VQA val set:  20% 1356/6699 [15:57<1:03:18,  1.41it/s]Evaluating on VQA val set:  20% 1357/6699 [15:58<1:01:57,  1.44it/s]Evaluating on VQA val set:  20% 1358/6699 [15:59<1:02:01,  1.44it/s]Evaluating on VQA val set:  20% 1359/6699 [15:59<1:00:45,  1.46it/s]Evaluating on VQA val set:  20% 1360/6699 [16:00<1:00:24,  1.47it/s]Evaluating on VQA val set:  20% 1361/6699 [16:01<58:47,  1.51it/s]  Evaluating on VQA val set:  20% 1362/6699 [16:01<59:10,  1.50it/s]Evaluating on VQA val set:  20% 1363/6699 [16:02<1:00:29,  1.47it/s]Evaluating on VQA val set:  20% 1364/6699 [16:03<1:01:22,  1.45it/s]Evaluating on VQA val set:  20% 1365/6699 [16:04<1:03:49,  1.39it/s]Evaluating on VQA val set:  20% 1366/6699 [16:04<1:03:35,  1.40it/s]Evaluating on VQA val set:  20% 1367/6699 [16:05<1:04:13,  1.38it/s]Evaluating on VQA val set:  20% 1368/6699 [16:06<1:04:38,  1.37it/s]Evaluating on VQA val set:  20% 1369/6699 [16:06<1:03:06,  1.41it/s]Evaluating on VQA val set:  20% 1370/6699 [16:07<1:02:05,  1.43it/s]Evaluating on VQA val set:  20% 1371/6699 [16:08<1:02:20,  1.42it/s]Evaluating on VQA val set:  20% 1372/6699 [16:08<1:03:06,  1.41it/s]Evaluating on VQA val set:  20% 1373/6699 [16:09<1:03:14,  1.40it/s]Evaluating on VQA val set:  21% 1374/6699 [16:10<1:03:35,  1.40it/s]Evaluating on VQA val set:  21% 1375/6699 [16:11<1:03:17,  1.40it/s]Evaluating on VQA val set:  21% 1376/6699 [16:11<1:03:31,  1.40it/s]Evaluating on VQA val set:  21% 1377/6699 [16:12<1:04:57,  1.37it/s]Evaluating on VQA val set:  21% 1378/6699 [16:13<1:04:23,  1.38it/s]Evaluating on VQA val set:  21% 1379/6699 [16:13<59:47,  1.48it/s]  Evaluating on VQA val set:  21% 1380/6699 [16:14<1:00:41,  1.46it/s]Evaluating on VQA val set:  21% 1381/6699 [16:15<1:03:19,  1.40it/s]Evaluating on VQA val set:  21% 1382/6699 [16:15<1:00:42,  1.46it/s]Evaluating on VQA val set:  21% 1383/6699 [16:16<1:02:23,  1.42it/s]Evaluating on VQA val set:  21% 1384/6699 [16:17<1:03:29,  1.40it/s]Evaluating on VQA val set:  21% 1385/6699 [16:18<1:01:37,  1.44it/s]Evaluating on VQA val set:  21% 1386/6699 [16:18<1:02:48,  1.41it/s]Evaluating on VQA val set:  21% 1387/6699 [16:19<1:04:49,  1.37it/s]Evaluating on VQA val set:  21% 1388/6699 [16:20<1:03:51,  1.39it/s]Evaluating on VQA val set:  21% 1389/6699 [16:21<1:04:17,  1.38it/s]Evaluating on VQA val set:  21% 1390/6699 [16:21<1:05:00,  1.36it/s]Evaluating on VQA val set:  21% 1391/6699 [16:22<1:05:31,  1.35it/s]Evaluating on VQA val set:  21% 1392/6699 [16:23<1:03:46,  1.39it/s]Evaluating on VQA val set:  21% 1393/6699 [16:23<1:01:13,  1.44it/s]Evaluating on VQA val set:  21% 1394/6699 [16:24<1:00:32,  1.46it/s]Evaluating on VQA val set:  21% 1395/6699 [16:25<1:00:58,  1.45it/s]Evaluating on VQA val set:  21% 1396/6699 [16:25<1:00:17,  1.47it/s]Evaluating on VQA val set:  21% 1397/6699 [16:26<58:58,  1.50it/s]  Evaluating on VQA val set:  21% 1398/6699 [16:27<1:00:17,  1.47it/s]Evaluating on VQA val set:  21% 1399/6699 [16:28<1:01:49,  1.43it/s]Evaluating on VQA val set:  21% 1400/6699 [16:28<1:02:26,  1.41it/s]Evaluating on VQA val set:  21% 1401/6699 [16:29<1:02:05,  1.42it/s]Evaluating on VQA val set:  21% 1402/6699 [16:30<1:00:53,  1.45it/s]Evaluating on VQA val set:  21% 1403/6699 [16:30<1:00:21,  1.46it/s]Evaluating on VQA val set:  21% 1404/6699 [16:31<1:02:33,  1.41it/s]Evaluating on VQA val set:  21% 1405/6699 [16:32<1:02:04,  1.42it/s]Evaluating on VQA val set:  21% 1406/6699 [16:32<1:01:40,  1.43it/s]Evaluating on VQA val set:  21% 1407/6699 [16:33<1:03:25,  1.39it/s]Evaluating on VQA val set:  21% 1408/6699 [16:34<1:03:37,  1.39it/s]Evaluating on VQA val set:  21% 1409/6699 [16:35<1:03:56,  1.38it/s]Evaluating on VQA val set:  21% 1410/6699 [16:35<1:02:27,  1.41it/s]Evaluating on VQA val set:  21% 1411/6699 [16:36<1:03:09,  1.40it/s]Evaluating on VQA val set:  21% 1412/6699 [16:37<1:03:40,  1.38it/s]Evaluating on VQA val set:  21% 1413/6699 [16:37<1:02:05,  1.42it/s]Evaluating on VQA val set:  21% 1414/6699 [16:38<1:04:33,  1.36it/s]Evaluating on VQA val set:  21% 1415/6699 [16:39<1:04:01,  1.38it/s]Evaluating on VQA val set:  21% 1416/6699 [16:40<1:05:19,  1.35it/s]Evaluating on VQA val set:  21% 1417/6699 [16:41<1:06:46,  1.32it/s]Evaluating on VQA val set:  21% 1418/6699 [16:41<1:04:22,  1.37it/s]Evaluating on VQA val set:  21% 1419/6699 [16:42<1:02:54,  1.40it/s]Evaluating on VQA val set:  21% 1420/6699 [16:43<1:03:33,  1.38it/s]Evaluating on VQA val set:  21% 1421/6699 [16:43<1:02:46,  1.40it/s]Evaluating on VQA val set:  21% 1422/6699 [16:44<1:02:16,  1.41it/s]Evaluating on VQA val set:  21% 1423/6699 [16:45<1:00:19,  1.46it/s]Evaluating on VQA val set:  21% 1424/6699 [16:45<1:00:07,  1.46it/s]Evaluating on VQA val set:  21% 1425/6699 [16:46<1:01:48,  1.42it/s]Evaluating on VQA val set:  21% 1426/6699 [16:47<1:02:41,  1.40it/s]Evaluating on VQA val set:  21% 1427/6699 [16:47<1:00:30,  1.45it/s]Evaluating on VQA val set:  21% 1428/6699 [16:48<1:01:08,  1.44it/s]Evaluating on VQA val set:  21% 1429/6699 [16:49<1:00:21,  1.46it/s]Evaluating on VQA val set:  21% 1430/6699 [16:50<1:00:33,  1.45it/s]Evaluating on VQA val set:  21% 1431/6699 [16:50<1:01:18,  1.43it/s]Evaluating on VQA val set:  21% 1432/6699 [16:51<59:37,  1.47it/s]  Evaluating on VQA val set:  21% 1433/6699 [16:52<1:02:10,  1.41it/s]Evaluating on VQA val set:  21% 1434/6699 [16:52<1:00:14,  1.46it/s]Evaluating on VQA val set:  21% 1435/6699 [16:53<1:02:47,  1.40it/s]Evaluating on VQA val set:  21% 1436/6699 [16:54<1:00:18,  1.45it/s]Evaluating on VQA val set:  21% 1437/6699 [16:54<1:01:29,  1.43it/s]Evaluating on VQA val set:  21% 1438/6699 [16:55<1:01:52,  1.42it/s]Evaluating on VQA val set:  21% 1439/6699 [16:56<59:37,  1.47it/s]  Evaluating on VQA val set:  21% 1440/6699 [16:57<1:01:32,  1.42it/s]Evaluating on VQA val set:  22% 1441/6699 [16:57<1:00:39,  1.44it/s]Evaluating on VQA val set:  22% 1442/6699 [16:58<59:43,  1.47it/s]  Evaluating on VQA val set:  22% 1443/6699 [16:58<58:33,  1.50it/s]Evaluating on VQA val set:  22% 1444/6699 [16:59<1:00:11,  1.45it/s]Evaluating on VQA val set:  22% 1445/6699 [17:00<1:00:46,  1.44it/s]Evaluating on VQA val set:  22% 1446/6699 [17:01<59:58,  1.46it/s]  Evaluating on VQA val set:  22% 1447/6699 [17:01<1:02:04,  1.41it/s]Evaluating on VQA val set:  22% 1448/6699 [17:02<1:01:00,  1.43it/s]Evaluating on VQA val set:  22% 1449/6699 [17:03<1:01:54,  1.41it/s]Evaluating on VQA val set:  22% 1450/6699 [17:03<1:02:29,  1.40it/s]Evaluating on VQA val set:  22% 1451/6699 [17:04<1:02:51,  1.39it/s]Evaluating on VQA val set:  22% 1452/6699 [17:05<1:00:48,  1.44it/s]Evaluating on VQA val set:  22% 1453/6699 [17:06<1:00:37,  1.44it/s]Evaluating on VQA val set:  22% 1454/6699 [17:06<1:02:40,  1.39it/s]Evaluating on VQA val set:  22% 1455/6699 [17:07<1:03:04,  1.39it/s]Evaluating on VQA val set:  22% 1456/6699 [17:08<1:02:54,  1.39it/s]Evaluating on VQA val set:  22% 1457/6699 [17:08<1:02:35,  1.40it/s]Evaluating on VQA val set:  22% 1458/6699 [17:09<1:00:50,  1.44it/s]Evaluating on VQA val set:  22% 1459/6699 [17:10<59:00,  1.48it/s]  Evaluating on VQA val set:  22% 1460/6699 [17:10<59:56,  1.46it/s]Evaluating on VQA val set:  22% 1461/6699 [17:11<1:00:28,  1.44it/s]Evaluating on VQA val set:  22% 1462/6699 [17:12<1:01:12,  1.43it/s]Evaluating on VQA val set:  22% 1463/6699 [17:13<1:02:14,  1.40it/s]Evaluating on VQA val set:  22% 1464/6699 [17:13<1:03:18,  1.38it/s]Evaluating on VQA val set:  22% 1465/6699 [17:14<1:02:53,  1.39it/s]Evaluating on VQA val set:  22% 1466/6699 [17:15<1:03:45,  1.37it/s]Evaluating on VQA val set:  22% 1467/6699 [17:16<1:04:04,  1.36it/s]Evaluating on VQA val set:  22% 1468/6699 [17:16<1:03:29,  1.37it/s]Evaluating on VQA val set:  22% 1469/6699 [17:17<1:02:32,  1.39it/s]Evaluating on VQA val set:  22% 1470/6699 [17:18<1:02:23,  1.40it/s]Evaluating on VQA val set:  22% 1471/6699 [17:18<1:03:00,  1.38it/s]Evaluating on VQA val set:  22% 1472/6699 [17:19<1:03:05,  1.38it/s]Evaluating on VQA val set:  22% 1473/6699 [17:20<1:02:11,  1.40it/s]Evaluating on VQA val set:  22% 1474/6699 [17:21<1:03:52,  1.36it/s]Evaluating on VQA val set:  22% 1475/6699 [17:21<1:01:12,  1.42it/s]Evaluating on VQA val set:  22% 1476/6699 [17:22<1:01:27,  1.42it/s]Evaluating on VQA val set:  22% 1477/6699 [17:23<1:01:44,  1.41it/s]Evaluating on VQA val set:  22% 1478/6699 [17:23<59:30,  1.46it/s]  Evaluating on VQA val set:  22% 1479/6699 [17:24<59:54,  1.45it/s]Evaluating on VQA val set:  22% 1480/6699 [17:25<56:35,  1.54it/s]Evaluating on VQA val set:  22% 1481/6699 [17:25<54:24,  1.60it/s]Evaluating on VQA val set:  22% 1482/6699 [17:26<56:41,  1.53it/s]Evaluating on VQA val set:  22% 1483/6699 [17:27<58:47,  1.48it/s]Evaluating on VQA val set:  22% 1484/6699 [17:27<1:00:24,  1.44it/s]Evaluating on VQA val set:  22% 1485/6699 [17:28<59:21,  1.46it/s]  Evaluating on VQA val set:  22% 1486/6699 [17:29<59:36,  1.46it/s]Evaluating on VQA val set:  22% 1487/6699 [17:29<1:00:33,  1.43it/s]Evaluating on VQA val set:  22% 1488/6699 [17:30<1:00:00,  1.45it/s]Evaluating on VQA val set:  22% 1489/6699 [17:31<1:00:26,  1.44it/s]Evaluating on VQA val set:  22% 1490/6699 [17:31<57:25,  1.51it/s]  Evaluating on VQA val set:  22% 1491/6699 [17:32<57:46,  1.50it/s]Evaluating on VQA val set:  22% 1492/6699 [17:33<1:00:36,  1.43it/s]Evaluating on VQA val set:  22% 1493/6699 [17:34<1:01:37,  1.41it/s]Evaluating on VQA val set:  22% 1494/6699 [17:34<1:03:03,  1.38it/s]Evaluating on VQA val set:  22% 1495/6699 [17:35<1:02:33,  1.39it/s]Evaluating on VQA val set:  22% 1496/6699 [17:36<1:02:32,  1.39it/s]Evaluating on VQA val set:  22% 1497/6699 [17:36<1:02:29,  1.39it/s]Evaluating on VQA val set:  22% 1498/6699 [17:37<1:02:52,  1.38it/s]Evaluating on VQA val set:  22% 1499/6699 [17:38<1:01:12,  1.42it/s]Evaluating on VQA val set:  22% 1500/6699 [17:39<1:00:10,  1.44it/s]Evaluating on VQA val set:  22% 1501/6699 [17:39<1:01:11,  1.42it/s]Evaluating on VQA val set:  22% 1502/6699 [17:40<1:01:22,  1.41it/s]Evaluating on VQA val set:  22% 1503/6699 [17:41<1:00:22,  1.43it/s]Evaluating on VQA val set:  22% 1504/6699 [17:41<59:34,  1.45it/s]  Evaluating on VQA val set:  22% 1505/6699 [17:42<58:22,  1.48it/s]Evaluating on VQA val set:  22% 1506/6699 [17:43<59:04,  1.46it/s]Evaluating on VQA val set:  22% 1507/6699 [17:43<1:00:25,  1.43it/s]Evaluating on VQA val set:  23% 1508/6699 [17:44<1:00:58,  1.42it/s]Evaluating on VQA val set:  23% 1509/6699 [17:45<1:00:31,  1.43it/s]Evaluating on VQA val set:  23% 1510/6699 [17:45<58:56,  1.47it/s]  Evaluating on VQA val set:  23% 1511/6699 [17:46<57:07,  1.51it/s]Evaluating on VQA val set:  23% 1512/6699 [17:47<57:24,  1.51it/s]Evaluating on VQA val set:  23% 1513/6699 [17:47<58:35,  1.48it/s]Evaluating on VQA val set:  23% 1514/6699 [17:48<59:32,  1.45it/s]Evaluating on VQA val set:  23% 1515/6699 [17:49<59:05,  1.46it/s]Evaluating on VQA val set:  23% 1516/6699 [17:50<59:20,  1.46it/s]Evaluating on VQA val set:  23% 1517/6699 [17:50<1:00:31,  1.43it/s]Evaluating on VQA val set:  23% 1518/6699 [17:51<1:00:46,  1.42it/s]Evaluating on VQA val set:  23% 1519/6699 [17:52<1:01:13,  1.41it/s]Evaluating on VQA val set:  23% 1520/6699 [17:52<1:00:38,  1.42it/s]Evaluating on VQA val set:  23% 1521/6699 [17:53<1:00:13,  1.43it/s]Evaluating on VQA val set:  23% 1522/6699 [17:54<1:01:36,  1.40it/s]Evaluating on VQA val set:  23% 1523/6699 [17:55<1:02:26,  1.38it/s]Evaluating on VQA val set:  23% 1524/6699 [17:55<1:01:39,  1.40it/s]Evaluating on VQA val set:  23% 1525/6699 [17:56<1:01:51,  1.39it/s]Evaluating on VQA val set:  23% 1526/6699 [17:57<59:43,  1.44it/s]  Evaluating on VQA val set:  23% 1527/6699 [17:57<57:37,  1.50it/s]Evaluating on VQA val set:  23% 1528/6699 [17:58<1:00:07,  1.43it/s]Evaluating on VQA val set:  23% 1529/6699 [17:59<1:00:26,  1.43it/s]Evaluating on VQA val set:  23% 1530/6699 [17:59<1:00:50,  1.42it/s]Evaluating on VQA val set:  23% 1531/6699 [18:00<1:02:17,  1.38it/s]Evaluating on VQA val set:  23% 1532/6699 [18:01<1:01:43,  1.40it/s]Evaluating on VQA val set:  23% 1533/6699 [18:02<1:02:12,  1.38it/s]Evaluating on VQA val set:  23% 1534/6699 [18:02<1:01:50,  1.39it/s]Evaluating on VQA val set:  23% 1535/6699 [18:03<1:00:34,  1.42it/s]Evaluating on VQA val set:  23% 1536/6699 [18:04<1:01:26,  1.40it/s]Evaluating on VQA val set:  23% 1537/6699 [18:05<1:03:31,  1.35it/s]Evaluating on VQA val set:  23% 1538/6699 [18:05<1:02:55,  1.37it/s]Evaluating on VQA val set:  23% 1539/6699 [18:06<1:00:10,  1.43it/s]Evaluating on VQA val set:  23% 1540/6699 [18:07<59:28,  1.45it/s]  Evaluating on VQA val set:  23% 1541/6699 [18:07<59:00,  1.46it/s]Evaluating on VQA val set:  23% 1542/6699 [18:08<58:38,  1.47it/s]Evaluating on VQA val set:  23% 1543/6699 [18:09<57:56,  1.48it/s]Evaluating on VQA val set:  23% 1544/6699 [18:09<58:36,  1.47it/s]Evaluating on VQA val set:  23% 1545/6699 [18:10<59:39,  1.44it/s]Evaluating on VQA val set:  23% 1546/6699 [18:11<59:07,  1.45it/s]Evaluating on VQA val set:  23% 1547/6699 [18:11<59:02,  1.45it/s]Evaluating on VQA val set:  23% 1548/6699 [18:12<58:12,  1.47it/s]Evaluating on VQA val set:  23% 1549/6699 [18:13<57:43,  1.49it/s]Evaluating on VQA val set:  23% 1550/6699 [18:13<58:50,  1.46it/s]Evaluating on VQA val set:  23% 1551/6699 [18:14<1:01:29,  1.40it/s]Evaluating on VQA val set:  23% 1552/6699 [18:15<59:36,  1.44it/s]  Evaluating on VQA val set:  23% 1553/6699 [18:16<1:00:09,  1.43it/s]Evaluating on VQA val set:  23% 1554/6699 [18:16<59:16,  1.45it/s]  Evaluating on VQA val set:  23% 1555/6699 [18:17<59:12,  1.45it/s]Evaluating on VQA val set:  23% 1556/6699 [18:18<58:05,  1.48it/s]Evaluating on VQA val set:  23% 1557/6699 [18:18<1:00:39,  1.41it/s]Evaluating on VQA val set:  23% 1558/6699 [18:19<1:01:48,  1.39it/s]Evaluating on VQA val set:  23% 1559/6699 [18:20<57:39,  1.49it/s]  Evaluating on VQA val set:  23% 1560/6699 [18:20<58:20,  1.47it/s]Evaluating on VQA val set:  23% 1561/6699 [18:21<1:00:43,  1.41it/s]Evaluating on VQA val set:  23% 1562/6699 [18:22<1:02:04,  1.38it/s]Evaluating on VQA val set:  23% 1563/6699 [18:23<1:02:16,  1.37it/s]Evaluating on VQA val set:  23% 1564/6699 [18:23<1:02:38,  1.37it/s]Evaluating on VQA val set:  23% 1565/6699 [18:24<1:01:11,  1.40it/s]Evaluating on VQA val set:  23% 1566/6699 [18:25<1:01:55,  1.38it/s]Evaluating on VQA val set:  23% 1567/6699 [18:25<1:01:54,  1.38it/s]Evaluating on VQA val set:  23% 1568/6699 [18:26<1:00:33,  1.41it/s]Evaluating on VQA val set:  23% 1569/6699 [18:27<1:01:28,  1.39it/s]Evaluating on VQA val set:  23% 1570/6699 [18:28<1:01:49,  1.38it/s]Evaluating on VQA val set:  23% 1571/6699 [18:28<1:02:20,  1.37it/s]Evaluating on VQA val set:  23% 1572/6699 [18:29<1:01:26,  1.39it/s]Evaluating on VQA val set:  23% 1573/6699 [18:30<1:02:04,  1.38it/s]Evaluating on VQA val set:  23% 1574/6699 [18:30<1:00:44,  1.41it/s]Evaluating on VQA val set:  24% 1575/6699 [18:31<1:01:17,  1.39it/s]Evaluating on VQA val set:  24% 1576/6699 [18:32<1:01:35,  1.39it/s]Evaluating on VQA val set:  24% 1577/6699 [18:33<1:02:04,  1.38it/s]Evaluating on VQA val set:  24% 1578/6699 [18:33<1:00:38,  1.41it/s]Evaluating on VQA val set:  24% 1579/6699 [18:34<1:01:44,  1.38it/s]Evaluating on VQA val set:  24% 1580/6699 [18:35<1:00:41,  1.41it/s]Evaluating on VQA val set:  24% 1581/6699 [18:35<59:58,  1.42it/s]  Evaluating on VQA val set:  24% 1582/6699 [18:36<1:00:44,  1.40it/s]Evaluating on VQA val set:  24% 1583/6699 [18:37<1:01:21,  1.39it/s]Evaluating on VQA val set:  24% 1584/6699 [18:38<1:02:31,  1.36it/s]Evaluating on VQA val set:  24% 1585/6699 [18:38<1:01:23,  1.39it/s]Evaluating on VQA val set:  24% 1586/6699 [18:39<1:02:19,  1.37it/s]Evaluating on VQA val set:  24% 1587/6699 [18:40<1:03:12,  1.35it/s]Evaluating on VQA val set:  24% 1588/6699 [18:41<1:01:47,  1.38it/s]Evaluating on VQA val set:  24% 1589/6699 [18:41<59:43,  1.43it/s]  Evaluating on VQA val set:  24% 1590/6699 [18:42<1:00:49,  1.40it/s]Evaluating on VQA val set:  24% 1591/6699 [18:43<57:49,  1.47it/s]  Evaluating on VQA val set:  24% 1592/6699 [18:43<58:07,  1.46it/s]Evaluating on VQA val set:  24% 1593/6699 [18:44<58:45,  1.45it/s]Evaluating on VQA val set:  24% 1594/6699 [18:45<59:51,  1.42it/s]Evaluating on VQA val set:  24% 1595/6699 [18:45<59:54,  1.42it/s]Evaluating on VQA val set:  24% 1596/6699 [18:46<58:54,  1.44it/s]Evaluating on VQA val set:  24% 1597/6699 [18:47<1:00:19,  1.41it/s]Evaluating on VQA val set:  24% 1598/6699 [18:48<1:01:13,  1.39it/s]Evaluating on VQA val set:  24% 1599/6699 [18:48<1:01:39,  1.38it/s]Evaluating on VQA val set:  24% 1600/6699 [18:49<1:04:28,  1.32it/s]Evaluating on VQA val set:  24% 1601/6699 [18:50<1:04:08,  1.32it/s]Evaluating on VQA val set:  24% 1602/6699 [18:51<1:02:16,  1.36it/s]Evaluating on VQA val set:  24% 1603/6699 [18:51<1:02:31,  1.36it/s]Evaluating on VQA val set:  24% 1604/6699 [18:52<1:02:51,  1.35it/s]Evaluating on VQA val set:  24% 1605/6699 [18:53<1:01:49,  1.37it/s]Evaluating on VQA val set:  24% 1606/6699 [18:53<1:00:47,  1.40it/s]Evaluating on VQA val set:  24% 1607/6699 [18:54<57:58,  1.46it/s]  Evaluating on VQA val set:  24% 1608/6699 [18:55<59:07,  1.44it/s]Evaluating on VQA val set:  24% 1609/6699 [18:56<59:22,  1.43it/s]Evaluating on VQA val set:  24% 1610/6699 [18:56<57:35,  1.47it/s]Evaluating on VQA val set:  24% 1611/6699 [18:57<57:30,  1.47it/s]Evaluating on VQA val set:  24% 1612/6699 [18:58<59:03,  1.44it/s]Evaluating on VQA val set:  24% 1613/6699 [18:58<56:36,  1.50it/s]Evaluating on VQA val set:  24% 1614/6699 [18:59<58:27,  1.45it/s]Evaluating on VQA val set:  24% 1615/6699 [19:00<59:07,  1.43it/s]Evaluating on VQA val set:  24% 1616/6699 [19:00<59:15,  1.43it/s]Evaluating on VQA val set:  24% 1617/6699 [19:01<1:00:23,  1.40it/s]Evaluating on VQA val set:  24% 1618/6699 [19:02<1:00:18,  1.40it/s]Evaluating on VQA val set:  24% 1619/6699 [19:03<1:00:35,  1.40it/s]Evaluating on VQA val set:  24% 1620/6699 [19:03<1:00:22,  1.40it/s]Evaluating on VQA val set:  24% 1621/6699 [19:04<59:50,  1.41it/s]  Evaluating on VQA val set:  24% 1622/6699 [19:04<56:21,  1.50it/s]Evaluating on VQA val set:  24% 1623/6699 [19:05<56:30,  1.50it/s]Evaluating on VQA val set:  24% 1624/6699 [19:06<57:03,  1.48it/s]Evaluating on VQA val set:  24% 1625/6699 [19:07<59:04,  1.43it/s]Evaluating on VQA val set:  24% 1626/6699 [19:07<1:00:05,  1.41it/s]Evaluating on VQA val set:  24% 1627/6699 [19:08<1:00:17,  1.40it/s]Evaluating on VQA val set:  24% 1628/6699 [19:09<1:00:29,  1.40it/s]Evaluating on VQA val set:  24% 1629/6699 [19:09<59:45,  1.41it/s]  Evaluating on VQA val set:  24% 1630/6699 [19:10<59:33,  1.42it/s]Evaluating on VQA val set:  24% 1631/6699 [19:11<59:05,  1.43it/s]Evaluating on VQA val set:  24% 1632/6699 [19:12<1:00:34,  1.39it/s]Evaluating on VQA val set:  24% 1633/6699 [19:12<1:00:47,  1.39it/s]Evaluating on VQA val set:  24% 1634/6699 [19:13<1:01:01,  1.38it/s]Evaluating on VQA val set:  24% 1635/6699 [19:14<1:01:47,  1.37it/s]Evaluating on VQA val set:  24% 1636/6699 [19:14<1:00:28,  1.40it/s]Evaluating on VQA val set:  24% 1637/6699 [19:15<1:00:17,  1.40it/s]Evaluating on VQA val set:  24% 1638/6699 [19:16<58:56,  1.43it/s]  Evaluating on VQA val set:  24% 1639/6699 [19:17<59:47,  1.41it/s]Evaluating on VQA val set:  24% 1640/6699 [19:17<59:05,  1.43it/s]Evaluating on VQA val set:  24% 1641/6699 [19:18<58:54,  1.43it/s]Evaluating on VQA val set:  25% 1642/6699 [19:19<59:22,  1.42it/s]Evaluating on VQA val set:  25% 1643/6699 [19:19<59:53,  1.41it/s]Evaluating on VQA val set:  25% 1644/6699 [19:20<58:41,  1.44it/s]Evaluating on VQA val set:  25% 1645/6699 [19:21<56:25,  1.49it/s]Evaluating on VQA val set:  25% 1646/6699 [19:21<55:06,  1.53it/s]Evaluating on VQA val set:  25% 1647/6699 [19:22<56:17,  1.50it/s]Evaluating on VQA val set:  25% 1648/6699 [19:23<58:32,  1.44it/s]Evaluating on VQA val set:  25% 1649/6699 [19:23<58:40,  1.43it/s]Evaluating on VQA val set:  25% 1650/6699 [19:24<58:14,  1.45it/s]Evaluating on VQA val set:  25% 1651/6699 [19:25<59:02,  1.43it/s]Evaluating on VQA val set:  25% 1652/6699 [19:26<59:33,  1.41it/s]Evaluating on VQA val set:  25% 1653/6699 [19:26<58:59,  1.43it/s]Evaluating on VQA val set:  25% 1654/6699 [19:27<59:54,  1.40it/s]Evaluating on VQA val set:  25% 1655/6699 [19:28<1:00:25,  1.39it/s]Evaluating on VQA val set:  25% 1656/6699 [19:28<1:00:10,  1.40it/s]Evaluating on VQA val set:  25% 1657/6699 [19:29<1:01:37,  1.36it/s]Evaluating on VQA val set:  25% 1658/6699 [19:30<1:02:00,  1.36it/s]Evaluating on VQA val set:  25% 1659/6699 [19:31<1:00:00,  1.40it/s]Evaluating on VQA val set:  25% 1660/6699 [19:31<59:58,  1.40it/s]  Evaluating on VQA val set:  25% 1661/6699 [19:32<1:00:11,  1.39it/s]Evaluating on VQA val set:  25% 1662/6699 [19:33<59:56,  1.40it/s]  Evaluating on VQA val set:  25% 1663/6699 [19:33<59:47,  1.40it/s]Evaluating on VQA val set:  25% 1664/6699 [19:34<58:51,  1.43it/s]Evaluating on VQA val set:  25% 1665/6699 [19:35<58:36,  1.43it/s]Evaluating on VQA val set:  25% 1666/6699 [19:36<57:15,  1.46it/s]Evaluating on VQA val set:  25% 1667/6699 [19:36<57:03,  1.47it/s]Evaluating on VQA val set:  25% 1668/6699 [19:37<58:46,  1.43it/s]Evaluating on VQA val set:  25% 1669/6699 [19:38<59:21,  1.41it/s]Evaluating on VQA val set:  25% 1670/6699 [19:38<59:38,  1.41it/s]Evaluating on VQA val set:  25% 1671/6699 [19:39<1:02:22,  1.34it/s]Evaluating on VQA val set:  25% 1672/6699 [19:40<1:02:54,  1.33it/s]Evaluating on VQA val set:  25% 1673/6699 [19:41<1:00:09,  1.39it/s]Evaluating on VQA val set:  25% 1674/6699 [19:41<58:54,  1.42it/s]  Evaluating on VQA val set:  25% 1675/6699 [19:42<57:58,  1.44it/s]Evaluating on VQA val set:  25% 1676/6699 [19:43<58:10,  1.44it/s]Evaluating on VQA val set:  25% 1677/6699 [19:43<57:24,  1.46it/s]Evaluating on VQA val set:  25% 1678/6699 [19:44<59:24,  1.41it/s]Evaluating on VQA val set:  25% 1679/6699 [19:45<58:16,  1.44it/s]Evaluating on VQA val set:  25% 1680/6699 [19:45<58:26,  1.43it/s]Evaluating on VQA val set:  25% 1681/6699 [19:46<58:51,  1.42it/s]Evaluating on VQA val set:  25% 1682/6699 [19:47<59:40,  1.40it/s]Evaluating on VQA val set:  25% 1683/6699 [19:48<57:47,  1.45it/s]Evaluating on VQA val set:  25% 1684/6699 [19:48<58:19,  1.43it/s]Evaluating on VQA val set:  25% 1685/6699 [19:49<57:19,  1.46it/s]Evaluating on VQA val set:  25% 1686/6699 [19:50<55:49,  1.50it/s]Evaluating on VQA val set:  25% 1687/6699 [19:50<58:11,  1.44it/s]Evaluating on VQA val set:  25% 1688/6699 [19:51<59:05,  1.41it/s]Evaluating on VQA val set:  25% 1689/6699 [19:52<1:00:05,  1.39it/s]Evaluating on VQA val set:  25% 1690/6699 [19:52<56:50,  1.47it/s]  Evaluating on VQA val set:  25% 1691/6699 [19:53<57:51,  1.44it/s]Evaluating on VQA val set:  25% 1692/6699 [19:54<57:00,  1.46it/s]Evaluating on VQA val set:  25% 1693/6699 [19:54<58:04,  1.44it/s]Evaluating on VQA val set:  25% 1694/6699 [19:55<56:29,  1.48it/s]Evaluating on VQA val set:  25% 1695/6699 [19:56<57:27,  1.45it/s]Evaluating on VQA val set:  25% 1696/6699 [19:57<57:05,  1.46it/s]Evaluating on VQA val set:  25% 1697/6699 [19:57<56:58,  1.46it/s]Evaluating on VQA val set:  25% 1698/6699 [19:58<56:57,  1.46it/s]Evaluating on VQA val set:  25% 1699/6699 [19:58<52:22,  1.59it/s]Evaluating on VQA val set:  25% 1700/6699 [19:59<55:13,  1.51it/s]Evaluating on VQA val set:  25% 1701/6699 [20:00<56:14,  1.48it/s]Evaluating on VQA val set:  25% 1702/6699 [20:01<57:38,  1.44it/s]Evaluating on VQA val set:  25% 1703/6699 [20:01<58:45,  1.42it/s]Evaluating on VQA val set:  25% 1704/6699 [20:02<58:26,  1.42it/s]Evaluating on VQA val set:  25% 1705/6699 [20:03<59:04,  1.41it/s]Evaluating on VQA val set:  25% 1706/6699 [20:03<58:05,  1.43it/s]Evaluating on VQA val set:  25% 1707/6699 [20:04<58:12,  1.43it/s]Evaluating on VQA val set:  25% 1708/6699 [20:05<59:26,  1.40it/s]Evaluating on VQA val set:  26% 1709/6699 [20:06<1:00:29,  1.37it/s]Evaluating on VQA val set:  26% 1710/6699 [20:06<59:15,  1.40it/s]  Evaluating on VQA val set:  26% 1711/6699 [20:07<57:52,  1.44it/s]Evaluating on VQA val set:  26% 1712/6699 [20:08<59:14,  1.40it/s]Evaluating on VQA val set:  26% 1713/6699 [20:08<1:01:07,  1.36it/s]Evaluating on VQA val set:  26% 1714/6699 [20:09<1:02:33,  1.33it/s]Evaluating on VQA val set:  26% 1715/6699 [20:10<1:03:40,  1.30it/s]Evaluating on VQA val set:  26% 1716/6699 [20:11<59:03,  1.41it/s]  Evaluating on VQA val set:  26% 1717/6699 [20:11<59:08,  1.40it/s]Evaluating on VQA val set:  26% 1718/6699 [20:12<58:40,  1.41it/s]Evaluating on VQA val set:  26% 1719/6699 [20:13<56:25,  1.47it/s]Evaluating on VQA val set:  26% 1720/6699 [20:13<57:24,  1.45it/s]Evaluating on VQA val set:  26% 1721/6699 [20:14<58:00,  1.43it/s]Evaluating on VQA val set:  26% 1722/6699 [20:15<58:26,  1.42it/s]Evaluating on VQA val set:  26% 1723/6699 [20:16<1:00:02,  1.38it/s]Evaluating on VQA val set:  26% 1724/6699 [20:16<57:37,  1.44it/s]  Evaluating on VQA val set:  26% 1725/6699 [20:17<57:18,  1.45it/s]Evaluating on VQA val set:  26% 1726/6699 [20:18<58:21,  1.42it/s]Evaluating on VQA val set:  26% 1727/6699 [20:18<1:00:07,  1.38it/s]Evaluating on VQA val set:  26% 1728/6699 [20:19<59:25,  1.39it/s]  Evaluating on VQA val set:  26% 1729/6699 [20:20<59:28,  1.39it/s]Evaluating on VQA val set:  26% 1730/6699 [20:20<58:00,  1.43it/s]Evaluating on VQA val set:  26% 1731/6699 [20:21<58:57,  1.40it/s]Evaluating on VQA val set:  26% 1732/6699 [20:22<59:10,  1.40it/s]Evaluating on VQA val set:  26% 1733/6699 [20:23<57:51,  1.43it/s]Evaluating on VQA val set:  26% 1734/6699 [20:23<57:08,  1.45it/s]Evaluating on VQA val set:  26% 1735/6699 [20:24<57:13,  1.45it/s]Evaluating on VQA val set:  26% 1736/6699 [20:25<56:51,  1.45it/s]Evaluating on VQA val set:  26% 1737/6699 [20:25<58:19,  1.42it/s]Evaluating on VQA val set:  26% 1738/6699 [20:26<59:32,  1.39it/s]Evaluating on VQA val set:  26% 1739/6699 [20:27<59:21,  1.39it/s]Evaluating on VQA val set:  26% 1740/6699 [20:28<1:00:23,  1.37it/s]Evaluating on VQA val set:  26% 1741/6699 [20:28<59:57,  1.38it/s]  Evaluating on VQA val set:  26% 1742/6699 [20:29<57:24,  1.44it/s]Evaluating on VQA val set:  26% 1743/6699 [20:30<58:02,  1.42it/s]Evaluating on VQA val set:  26% 1744/6699 [20:30<59:33,  1.39it/s]Evaluating on VQA val set:  26% 1745/6699 [20:31<1:01:23,  1.34it/s]Evaluating on VQA val set:  26% 1746/6699 [20:32<1:00:11,  1.37it/s]Evaluating on VQA val set:  26% 1747/6699 [20:33<59:52,  1.38it/s]  Evaluating on VQA val set:  26% 1748/6699 [20:33<1:00:45,  1.36it/s]Evaluating on VQA val set:  26% 1749/6699 [20:34<59:35,  1.38it/s]  Evaluating on VQA val set:  26% 1750/6699 [20:35<59:13,  1.39it/s]Evaluating on VQA val set:  26% 1751/6699 [20:36<58:37,  1.41it/s]Evaluating on VQA val set:  26% 1752/6699 [20:36<58:37,  1.41it/s]Evaluating on VQA val set:  26% 1753/6699 [20:37<58:58,  1.40it/s]Evaluating on VQA val set:  26% 1754/6699 [20:38<58:44,  1.40it/s]Evaluating on VQA val set:  26% 1755/6699 [20:38<58:44,  1.40it/s]Evaluating on VQA val set:  26% 1756/6699 [20:39<59:46,  1.38it/s]Evaluating on VQA val set:  26% 1757/6699 [20:40<58:38,  1.40it/s]Evaluating on VQA val set:  26% 1758/6699 [20:41<58:48,  1.40it/s]Evaluating on VQA val set:  26% 1759/6699 [20:41<58:02,  1.42it/s]Evaluating on VQA val set:  26% 1760/6699 [20:42<58:07,  1.42it/s]Evaluating on VQA val set:  26% 1761/6699 [20:43<59:28,  1.38it/s]Evaluating on VQA val set:  26% 1762/6699 [20:43<59:53,  1.37it/s]Evaluating on VQA val set:  26% 1763/6699 [20:44<59:29,  1.38it/s]Evaluating on VQA val set:  26% 1764/6699 [20:45<59:21,  1.39it/s]Evaluating on VQA val set:  26% 1765/6699 [20:46<58:09,  1.41it/s]Evaluating on VQA val set:  26% 1766/6699 [20:46<56:48,  1.45it/s]Evaluating on VQA val set:  26% 1767/6699 [20:47<54:01,  1.52it/s]Evaluating on VQA val set:  26% 1768/6699 [20:47<55:23,  1.48it/s]Evaluating on VQA val set:  26% 1769/6699 [20:48<57:02,  1.44it/s]Evaluating on VQA val set:  26% 1770/6699 [20:49<58:07,  1.41it/s]Evaluating on VQA val set:  26% 1771/6699 [20:50<57:55,  1.42it/s]Evaluating on VQA val set:  26% 1772/6699 [20:50<58:21,  1.41it/s]Evaluating on VQA val set:  26% 1773/6699 [20:51<57:24,  1.43it/s]Evaluating on VQA val set:  26% 1774/6699 [20:52<57:40,  1.42it/s]Evaluating on VQA val set:  26% 1775/6699 [20:52<56:04,  1.46it/s]Evaluating on VQA val set:  27% 1776/6699 [20:53<57:14,  1.43it/s]Evaluating on VQA val set:  27% 1777/6699 [20:54<56:44,  1.45it/s]Evaluating on VQA val set:  27% 1778/6699 [20:55<57:28,  1.43it/s]Evaluating on VQA val set:  27% 1779/6699 [20:55<56:54,  1.44it/s]Evaluating on VQA val set:  27% 1780/6699 [20:56<56:15,  1.46it/s]Evaluating on VQA val set:  27% 1781/6699 [20:56<54:02,  1.52it/s]Evaluating on VQA val set:  27% 1782/6699 [20:57<54:04,  1.52it/s]Evaluating on VQA val set:  27% 1783/6699 [20:58<55:03,  1.49it/s]Evaluating on VQA val set:  27% 1784/6699 [20:58<54:37,  1.50it/s]Evaluating on VQA val set:  27% 1785/6699 [20:59<57:04,  1.44it/s]Evaluating on VQA val set:  27% 1786/6699 [21:00<57:47,  1.42it/s]Evaluating on VQA val set:  27% 1787/6699 [21:01<57:42,  1.42it/s]Evaluating on VQA val set:  27% 1788/6699 [21:01<56:55,  1.44it/s]Evaluating on VQA val set:  27% 1789/6699 [21:02<58:16,  1.40it/s]Evaluating on VQA val set:  27% 1790/6699 [21:03<58:33,  1.40it/s]Evaluating on VQA val set:  27% 1791/6699 [21:04<57:55,  1.41it/s]Evaluating on VQA val set:  27% 1792/6699 [21:04<58:23,  1.40it/s]Evaluating on VQA val set:  27% 1793/6699 [21:05<58:42,  1.39it/s]Evaluating on VQA val set:  27% 1794/6699 [21:06<59:25,  1.38it/s]Evaluating on VQA val set:  27% 1795/6699 [21:06<58:17,  1.40it/s]Evaluating on VQA val set:  27% 1796/6699 [21:07<56:30,  1.45it/s]Evaluating on VQA val set:  27% 1797/6699 [21:08<55:28,  1.47it/s]Evaluating on VQA val set:  27% 1798/6699 [21:08<55:02,  1.48it/s]Evaluating on VQA val set:  27% 1799/6699 [21:09<55:26,  1.47it/s]Evaluating on VQA val set:  27% 1800/6699 [21:10<56:34,  1.44it/s]Evaluating on VQA val set:  27% 1801/6699 [21:10<56:14,  1.45it/s]Evaluating on VQA val set:  27% 1802/6699 [21:11<54:53,  1.49it/s]Evaluating on VQA val set:  27% 1803/6699 [21:12<57:26,  1.42it/s]Evaluating on VQA val set:  27% 1804/6699 [21:13<57:40,  1.41it/s]Evaluating on VQA val set:  27% 1805/6699 [21:13<53:50,  1.51it/s]Evaluating on VQA val set:  27% 1806/6699 [21:14<52:51,  1.54it/s]Evaluating on VQA val set:  27% 1807/6699 [21:14<54:17,  1.50it/s]Evaluating on VQA val set:  27% 1808/6699 [21:15<55:34,  1.47it/s]Evaluating on VQA val set:  27% 1809/6699 [21:16<56:09,  1.45it/s]Evaluating on VQA val set:  27% 1810/6699 [21:17<56:46,  1.44it/s]Evaluating on VQA val set:  27% 1811/6699 [21:17<58:14,  1.40it/s]Evaluating on VQA val set:  27% 1812/6699 [21:18<57:17,  1.42it/s]Evaluating on VQA val set:  27% 1813/6699 [21:19<55:59,  1.45it/s]Evaluating on VQA val set:  27% 1814/6699 [21:19<57:41,  1.41it/s]Evaluating on VQA val set:  27% 1815/6699 [21:20<58:15,  1.40it/s]Evaluating on VQA val set:  27% 1816/6699 [21:21<57:48,  1.41it/s]Evaluating on VQA val set:  27% 1817/6699 [21:22<56:45,  1.43it/s]Evaluating on VQA val set:  27% 1818/6699 [21:22<57:18,  1.42it/s]Evaluating on VQA val set:  27% 1819/6699 [21:23<57:32,  1.41it/s]Evaluating on VQA val set:  27% 1820/6699 [21:24<57:51,  1.41it/s]Evaluating on VQA val set:  27% 1821/6699 [21:24<57:37,  1.41it/s]Evaluating on VQA val set:  27% 1822/6699 [21:25<54:56,  1.48it/s]Evaluating on VQA val set:  27% 1823/6699 [21:26<56:38,  1.43it/s]Evaluating on VQA val set:  27% 1824/6699 [21:26<57:01,  1.42it/s]Evaluating on VQA val set:  27% 1825/6699 [21:27<56:26,  1.44it/s]Evaluating on VQA val set:  27% 1826/6699 [21:28<56:57,  1.43it/s]Evaluating on VQA val set:  27% 1827/6699 [21:29<57:13,  1.42it/s]Evaluating on VQA val set:  27% 1828/6699 [21:29<57:10,  1.42it/s]Evaluating on VQA val set:  27% 1829/6699 [21:30<57:41,  1.41it/s]Evaluating on VQA val set:  27% 1830/6699 [21:31<58:51,  1.38it/s]Evaluating on VQA val set:  27% 1831/6699 [21:31<58:08,  1.40it/s]Evaluating on VQA val set:  27% 1832/6699 [21:32<58:51,  1.38it/s]Evaluating on VQA val set:  27% 1833/6699 [21:33<58:57,  1.38it/s]Evaluating on VQA val set:  27% 1834/6699 [21:34<58:09,  1.39it/s]Evaluating on VQA val set:  27% 1835/6699 [21:34<58:09,  1.39it/s]Evaluating on VQA val set:  27% 1836/6699 [21:35<57:26,  1.41it/s]Evaluating on VQA val set:  27% 1837/6699 [21:36<57:45,  1.40it/s]Evaluating on VQA val set:  27% 1838/6699 [21:36<56:03,  1.45it/s]Evaluating on VQA val set:  27% 1839/6699 [21:37<56:46,  1.43it/s]Evaluating on VQA val set:  27% 1840/6699 [21:38<57:03,  1.42it/s]Evaluating on VQA val set:  27% 1841/6699 [21:39<58:56,  1.37it/s]Evaluating on VQA val set:  27% 1842/6699 [21:39<56:09,  1.44it/s]Evaluating on VQA val set:  28% 1843/6699 [21:40<57:46,  1.40it/s]Evaluating on VQA val set:  28% 1844/6699 [21:41<57:11,  1.41it/s]Evaluating on VQA val set:  28% 1845/6699 [21:41<57:28,  1.41it/s]Evaluating on VQA val set:  28% 1846/6699 [21:42<57:15,  1.41it/s]Evaluating on VQA val set:  28% 1847/6699 [21:43<57:25,  1.41it/s]Evaluating on VQA val set:  28% 1848/6699 [21:44<57:46,  1.40it/s]Evaluating on VQA val set:  28% 1849/6699 [21:44<59:03,  1.37it/s]Evaluating on VQA val set:  28% 1850/6699 [21:45<59:11,  1.37it/s]Evaluating on VQA val set:  28% 1851/6699 [21:46<58:00,  1.39it/s]Evaluating on VQA val set:  28% 1852/6699 [21:46<58:39,  1.38it/s]Evaluating on VQA val set:  28% 1853/6699 [21:47<58:14,  1.39it/s]Evaluating on VQA val set:  28% 1854/6699 [21:48<57:53,  1.39it/s]Evaluating on VQA val set:  28% 1855/6699 [21:49<58:33,  1.38it/s]Evaluating on VQA val set:  28% 1856/6699 [21:49<57:19,  1.41it/s]Evaluating on VQA val set:  28% 1857/6699 [21:50<55:33,  1.45it/s]Evaluating on VQA val set:  28% 1858/6699 [21:51<57:08,  1.41it/s]Evaluating on VQA val set:  28% 1859/6699 [21:51<57:32,  1.40it/s]Evaluating on VQA val set:  28% 1860/6699 [21:52<59:15,  1.36it/s]Evaluating on VQA val set:  28% 1861/6699 [21:53<58:51,  1.37it/s]Evaluating on VQA val set:  28% 1862/6699 [21:54<56:44,  1.42it/s]Evaluating on VQA val set:  28% 1863/6699 [21:54<56:21,  1.43it/s]Evaluating on VQA val set:  28% 1864/6699 [21:55<55:13,  1.46it/s]Evaluating on VQA val set:  28% 1865/6699 [21:56<56:23,  1.43it/s]Evaluating on VQA val set:  28% 1866/6699 [21:56<57:50,  1.39it/s]Evaluating on VQA val set:  28% 1867/6699 [21:57<57:20,  1.40it/s]Evaluating on VQA val set:  28% 1868/6699 [21:58<57:53,  1.39it/s]Evaluating on VQA val set:  28% 1869/6699 [21:59<57:36,  1.40it/s]Evaluating on VQA val set:  28% 1870/6699 [21:59<57:50,  1.39it/s]Evaluating on VQA val set:  28% 1871/6699 [22:00<57:53,  1.39it/s]Evaluating on VQA val set:  28% 1872/6699 [22:01<58:13,  1.38it/s]Evaluating on VQA val set:  28% 1873/6699 [22:01<58:19,  1.38it/s]Evaluating on VQA val set:  28% 1874/6699 [22:02<57:29,  1.40it/s]Evaluating on VQA val set:  28% 1875/6699 [22:03<58:37,  1.37it/s]Evaluating on VQA val set:  28% 1876/6699 [22:04<58:05,  1.38it/s]Evaluating on VQA val set:  28% 1877/6699 [22:04<58:21,  1.38it/s]Evaluating on VQA val set:  28% 1878/6699 [22:05<59:06,  1.36it/s]Evaluating on VQA val set:  28% 1879/6699 [22:06<59:20,  1.35it/s]Evaluating on VQA val set:  28% 1880/6699 [22:07<58:14,  1.38it/s]Evaluating on VQA val set:  28% 1881/6699 [22:07<59:18,  1.35it/s]Evaluating on VQA val set:  28% 1882/6699 [22:08<58:24,  1.37it/s]Evaluating on VQA val set:  28% 1883/6699 [22:09<58:57,  1.36it/s]Evaluating on VQA val set:  28% 1884/6699 [22:09<57:58,  1.38it/s]Evaluating on VQA val set:  28% 1885/6699 [22:10<55:31,  1.45it/s]Evaluating on VQA val set:  28% 1886/6699 [22:11<54:54,  1.46it/s]Evaluating on VQA val set:  28% 1887/6699 [22:12<56:57,  1.41it/s]Evaluating on VQA val set:  28% 1888/6699 [22:12<56:58,  1.41it/s]Evaluating on VQA val set:  28% 1889/6699 [22:13<56:53,  1.41it/s]Evaluating on VQA val set:  28% 1890/6699 [22:14<56:35,  1.42it/s]Evaluating on VQA val set:  28% 1891/6699 [22:14<56:07,  1.43it/s]Evaluating on VQA val set:  28% 1892/6699 [22:15<54:42,  1.46it/s]Evaluating on VQA val set:  28% 1893/6699 [22:16<54:21,  1.47it/s]Evaluating on VQA val set:  28% 1894/6699 [22:16<55:47,  1.44it/s]Evaluating on VQA val set:  28% 1895/6699 [22:17<56:42,  1.41it/s]Evaluating on VQA val set:  28% 1896/6699 [22:18<57:20,  1.40it/s]Evaluating on VQA val set:  28% 1897/6699 [22:19<56:52,  1.41it/s]Evaluating on VQA val set:  28% 1898/6699 [22:19<57:25,  1.39it/s]Evaluating on VQA val set:  28% 1899/6699 [22:20<57:44,  1.39it/s]Evaluating on VQA val set:  28% 1900/6699 [22:21<56:59,  1.40it/s]Evaluating on VQA val set:  28% 1901/6699 [22:21<57:00,  1.40it/s]Evaluating on VQA val set:  28% 1902/6699 [22:22<54:12,  1.47it/s]Evaluating on VQA val set:  28% 1903/6699 [22:23<55:01,  1.45it/s]Evaluating on VQA val set:  28% 1904/6699 [22:23<55:14,  1.45it/s]Evaluating on VQA val set:  28% 1905/6699 [22:24<55:44,  1.43it/s]Evaluating on VQA val set:  28% 1906/6699 [22:25<56:45,  1.41it/s]Evaluating on VQA val set:  28% 1907/6699 [22:26<57:26,  1.39it/s]Evaluating on VQA val set:  28% 1908/6699 [22:26<56:54,  1.40it/s]Evaluating on VQA val set:  28% 1909/6699 [22:27<55:34,  1.44it/s]Evaluating on VQA val set:  29% 1910/6699 [22:28<57:04,  1.40it/s]Evaluating on VQA val set:  29% 1911/6699 [22:28<57:00,  1.40it/s]Evaluating on VQA val set:  29% 1912/6699 [22:29<56:48,  1.40it/s]Evaluating on VQA val set:  29% 1913/6699 [22:30<55:12,  1.44it/s]Evaluating on VQA val set:  29% 1914/6699 [22:30<53:14,  1.50it/s]Evaluating on VQA val set:  29% 1915/6699 [22:31<53:31,  1.49it/s]Evaluating on VQA val set:  29% 1916/6699 [22:32<53:05,  1.50it/s]Evaluating on VQA val set:  29% 1917/6699 [22:32<55:21,  1.44it/s]Evaluating on VQA val set:  29% 1918/6699 [22:33<56:01,  1.42it/s]Evaluating on VQA val set:  29% 1919/6699 [22:34<57:28,  1.39it/s]Evaluating on VQA val set:  29% 1920/6699 [22:35<56:41,  1.40it/s]Evaluating on VQA val set:  29% 1921/6699 [22:35<55:16,  1.44it/s]Evaluating on VQA val set:  29% 1922/6699 [22:36<56:45,  1.40it/s]Evaluating on VQA val set:  29% 1923/6699 [22:37<56:01,  1.42it/s]Evaluating on VQA val set:  29% 1924/6699 [22:37<55:26,  1.44it/s]Evaluating on VQA val set:  29% 1925/6699 [22:38<53:26,  1.49it/s]Evaluating on VQA val set:  29% 1926/6699 [22:39<52:22,  1.52it/s]Evaluating on VQA val set:  29% 1927/6699 [22:39<52:46,  1.51it/s]Evaluating on VQA val set:  29% 1928/6699 [22:40<53:14,  1.49it/s]Evaluating on VQA val set:  29% 1929/6699 [22:41<55:40,  1.43it/s]Evaluating on VQA val set:  29% 1930/6699 [22:41<54:18,  1.46it/s]Evaluating on VQA val set:  29% 1931/6699 [22:42<56:15,  1.41it/s]Evaluating on VQA val set:  29% 1932/6699 [22:43<57:55,  1.37it/s]Evaluating on VQA val set:  29% 1933/6699 [22:44<57:16,  1.39it/s]Evaluating on VQA val set:  29% 1934/6699 [22:44<57:10,  1.39it/s]Evaluating on VQA val set:  29% 1935/6699 [22:45<56:35,  1.40it/s]Evaluating on VQA val set:  29% 1936/6699 [22:46<56:59,  1.39it/s]Evaluating on VQA val set:  29% 1937/6699 [22:46<55:12,  1.44it/s]Evaluating on VQA val set:  29% 1938/6699 [22:47<54:25,  1.46it/s]Evaluating on VQA val set:  29% 1939/6699 [22:48<53:40,  1.48it/s]Evaluating on VQA val set:  29% 1940/6699 [22:48<53:28,  1.48it/s]Evaluating on VQA val set:  29% 1941/6699 [22:49<56:27,  1.40it/s]Evaluating on VQA val set:  29% 1942/6699 [22:50<57:15,  1.38it/s]Evaluating on VQA val set:  29% 1943/6699 [22:51<55:57,  1.42it/s]Evaluating on VQA val set:  29% 1944/6699 [22:51<57:12,  1.39it/s]Evaluating on VQA val set:  29% 1945/6699 [22:52<56:43,  1.40it/s]Evaluating on VQA val set:  29% 1946/6699 [22:53<56:28,  1.40it/s]Evaluating on VQA val set:  29% 1947/6699 [22:54<57:23,  1.38it/s]Evaluating on VQA val set:  29% 1948/6699 [22:54<57:31,  1.38it/s]Evaluating on VQA val set:  29% 1949/6699 [22:55<58:30,  1.35it/s]Evaluating on VQA val set:  29% 1950/6699 [22:56<58:51,  1.34it/s]Evaluating on VQA val set:  29% 1951/6699 [22:57<58:05,  1.36it/s]Evaluating on VQA val set:  29% 1952/6699 [22:57<57:05,  1.39it/s]Evaluating on VQA val set:  29% 1953/6699 [22:58<56:10,  1.41it/s]Evaluating on VQA val set:  29% 1954/6699 [22:59<55:46,  1.42it/s]Evaluating on VQA val set:  29% 1955/6699 [22:59<55:58,  1.41it/s]Evaluating on VQA val set:  29% 1956/6699 [23:00<55:32,  1.42it/s]Evaluating on VQA val set:  29% 1957/6699 [23:01<58:18,  1.36it/s]Evaluating on VQA val set:  29% 1958/6699 [23:02<57:53,  1.37it/s]Evaluating on VQA val set:  29% 1959/6699 [23:02<57:15,  1.38it/s]Evaluating on VQA val set:  29% 1960/6699 [23:03<58:50,  1.34it/s]Evaluating on VQA val set:  29% 1961/6699 [23:04<58:55,  1.34it/s]Evaluating on VQA val set:  29% 1962/6699 [23:05<58:01,  1.36it/s]Evaluating on VQA val set:  29% 1963/6699 [23:05<58:53,  1.34it/s]Evaluating on VQA val set:  29% 1964/6699 [23:06<57:22,  1.38it/s]Evaluating on VQA val set:  29% 1965/6699 [23:07<55:02,  1.43it/s]Evaluating on VQA val set:  29% 1966/6699 [23:07<55:31,  1.42it/s]Evaluating on VQA val set:  29% 1967/6699 [23:08<55:19,  1.43it/s]Evaluating on VQA val set:  29% 1968/6699 [23:09<54:57,  1.43it/s]Evaluating on VQA val set:  29% 1969/6699 [23:09<54:59,  1.43it/s]Evaluating on VQA val set:  29% 1970/6699 [23:10<54:19,  1.45it/s]Evaluating on VQA val set:  29% 1971/6699 [23:11<54:53,  1.44it/s]Evaluating on VQA val set:  29% 1972/6699 [23:12<54:56,  1.43it/s]Evaluating on VQA val set:  29% 1973/6699 [23:12<56:23,  1.40it/s]Evaluating on VQA val set:  29% 1974/6699 [23:13<56:54,  1.38it/s]Evaluating on VQA val set:  29% 1975/6699 [23:14<56:38,  1.39it/s]Evaluating on VQA val set:  29% 1976/6699 [23:14<57:08,  1.38it/s]Evaluating on VQA val set:  30% 1977/6699 [23:15<57:18,  1.37it/s]Evaluating on VQA val set:  30% 1978/6699 [23:16<56:49,  1.38it/s]Evaluating on VQA val set:  30% 1979/6699 [23:17<57:01,  1.38it/s]Evaluating on VQA val set:  30% 1980/6699 [23:17<58:21,  1.35it/s]Evaluating on VQA val set:  30% 1981/6699 [23:18<57:37,  1.36it/s]Evaluating on VQA val set:  30% 1982/6699 [23:19<56:23,  1.39it/s]Evaluating on VQA val set:  30% 1983/6699 [23:20<56:28,  1.39it/s]Evaluating on VQA val set:  30% 1984/6699 [23:20<55:59,  1.40it/s]Evaluating on VQA val set:  30% 1985/6699 [23:21<55:18,  1.42it/s]Evaluating on VQA val set:  30% 1986/6699 [23:22<54:27,  1.44it/s]Evaluating on VQA val set:  30% 1987/6699 [23:22<55:06,  1.42it/s]Evaluating on VQA val set:  30% 1988/6699 [23:23<54:57,  1.43it/s]Evaluating on VQA val set:  30% 1989/6699 [23:24<54:44,  1.43it/s]Evaluating on VQA val set:  30% 1990/6699 [23:24<55:18,  1.42it/s]Evaluating on VQA val set:  30% 1991/6699 [23:25<55:13,  1.42it/s]Evaluating on VQA val set:  30% 1992/6699 [23:26<56:00,  1.40it/s]Evaluating on VQA val set:  30% 1993/6699 [23:27<55:57,  1.40it/s]Evaluating on VQA val set:  30% 1994/6699 [23:27<56:41,  1.38it/s]Evaluating on VQA val set:  30% 1995/6699 [23:28<58:05,  1.35it/s]Evaluating on VQA val set:  30% 1996/6699 [23:29<57:06,  1.37it/s]Evaluating on VQA val set:  30% 1997/6699 [23:29<56:23,  1.39it/s]Evaluating on VQA val set:  30% 1998/6699 [23:30<57:00,  1.37it/s]Evaluating on VQA val set:  30% 1999/6699 [23:31<56:16,  1.39it/s]Evaluating on VQA val set:  30% 2000/6699 [23:32<55:41,  1.41it/s]Evaluating on VQA val set:  30% 2001/6699 [23:32<57:22,  1.36it/s]Evaluating on VQA val set:  30% 2002/6699 [23:33<57:40,  1.36it/s]Evaluating on VQA val set:  30% 2003/6699 [23:34<57:34,  1.36it/s]Evaluating on VQA val set:  30% 2004/6699 [23:35<56:43,  1.38it/s]Evaluating on VQA val set:  30% 2005/6699 [23:35<56:38,  1.38it/s]Evaluating on VQA val set:  30% 2006/6699 [23:36<55:50,  1.40it/s]Evaluating on VQA val set:  30% 2007/6699 [23:37<56:06,  1.39it/s]Evaluating on VQA val set:  30% 2008/6699 [23:37<55:48,  1.40it/s]Evaluating on VQA val set:  30% 2009/6699 [23:38<55:21,  1.41it/s]Evaluating on VQA val set:  30% 2010/6699 [23:39<55:46,  1.40it/s]Evaluating on VQA val set:  30% 2011/6699 [23:39<53:17,  1.47it/s]Evaluating on VQA val set:  30% 2012/6699 [23:40<54:10,  1.44it/s]Evaluating on VQA val set:  30% 2013/6699 [23:41<54:23,  1.44it/s]Evaluating on VQA val set:  30% 2014/6699 [23:42<53:32,  1.46it/s]Evaluating on VQA val set:  30% 2015/6699 [23:42<52:08,  1.50it/s]Evaluating on VQA val set:  30% 2016/6699 [23:43<52:38,  1.48it/s]Evaluating on VQA val set:  30% 2017/6699 [23:43<51:59,  1.50it/s]Evaluating on VQA val set:  30% 2018/6699 [23:44<51:38,  1.51it/s]Evaluating on VQA val set:  30% 2019/6699 [23:45<53:00,  1.47it/s]Evaluating on VQA val set:  30% 2020/6699 [23:46<53:34,  1.46it/s]Evaluating on VQA val set:  30% 2021/6699 [23:46<56:00,  1.39it/s]Evaluating on VQA val set:  30% 2022/6699 [23:47<56:57,  1.37it/s]Evaluating on VQA val set:  30% 2023/6699 [23:48<57:24,  1.36it/s]Evaluating on VQA val set:  30% 2024/6699 [23:49<56:33,  1.38it/s]Evaluating on VQA val set:  30% 2025/6699 [23:49<56:12,  1.39it/s]Evaluating on VQA val set:  30% 2026/6699 [23:50<57:32,  1.35it/s]Evaluating on VQA val set:  30% 2027/6699 [23:51<58:20,  1.33it/s]Evaluating on VQA val set:  30% 2028/6699 [23:52<57:54,  1.34it/s]Evaluating on VQA val set:  30% 2029/6699 [23:52<57:00,  1.37it/s]Evaluating on VQA val set:  30% 2030/6699 [23:53<55:04,  1.41it/s]Evaluating on VQA val set:  30% 2031/6699 [23:54<53:47,  1.45it/s]Evaluating on VQA val set:  30% 2032/6699 [23:54<55:38,  1.40it/s]Evaluating on VQA val set:  30% 2033/6699 [23:55<56:40,  1.37it/s]Evaluating on VQA val set:  30% 2034/6699 [23:56<55:14,  1.41it/s]Evaluating on VQA val set:  30% 2035/6699 [23:56<53:49,  1.44it/s]Evaluating on VQA val set:  30% 2036/6699 [23:57<53:00,  1.47it/s]Evaluating on VQA val set:  30% 2037/6699 [23:58<54:10,  1.43it/s]Evaluating on VQA val set:  30% 2038/6699 [23:59<55:10,  1.41it/s]Evaluating on VQA val set:  30% 2039/6699 [23:59<54:52,  1.42it/s]Evaluating on VQA val set:  30% 2040/6699 [24:00<56:58,  1.36it/s]Evaluating on VQA val set:  30% 2041/6699 [24:01<55:46,  1.39it/s]Evaluating on VQA val set:  30% 2042/6699 [24:01<55:49,  1.39it/s]Evaluating on VQA val set:  30% 2043/6699 [24:02<55:44,  1.39it/s]Evaluating on VQA val set:  31% 2044/6699 [24:03<55:01,  1.41it/s]Evaluating on VQA val set:  31% 2045/6699 [24:04<54:22,  1.43it/s]Evaluating on VQA val set:  31% 2046/6699 [24:04<55:15,  1.40it/s]Evaluating on VQA val set:  31% 2047/6699 [24:05<56:25,  1.37it/s]Evaluating on VQA val set:  31% 2048/6699 [24:06<57:25,  1.35it/s]Evaluating on VQA val set:  31% 2049/6699 [24:07<56:36,  1.37it/s]Evaluating on VQA val set:  31% 2050/6699 [24:07<56:23,  1.37it/s]Evaluating on VQA val set:  31% 2051/6699 [24:08<56:16,  1.38it/s]Evaluating on VQA val set:  31% 2052/6699 [24:09<57:15,  1.35it/s]Evaluating on VQA val set:  31% 2053/6699 [24:09<56:45,  1.36it/s]Evaluating on VQA val set:  31% 2054/6699 [24:10<58:10,  1.33it/s]Evaluating on VQA val set:  31% 2055/6699 [24:11<56:09,  1.38it/s]Evaluating on VQA val set:  31% 2056/6699 [24:12<55:24,  1.40it/s]Evaluating on VQA val set:  31% 2057/6699 [24:12<57:08,  1.35it/s]Evaluating on VQA val set:  31% 2058/6699 [24:13<57:44,  1.34it/s]Evaluating on VQA val set:  31% 2059/6699 [24:14<57:07,  1.35it/s]Evaluating on VQA val set:  31% 2060/6699 [24:15<56:56,  1.36it/s]Evaluating on VQA val set:  31% 2061/6699 [24:15<56:33,  1.37it/s]Evaluating on VQA val set:  31% 2062/6699 [24:16<56:47,  1.36it/s]Evaluating on VQA val set:  31% 2063/6699 [24:17<57:18,  1.35it/s]Evaluating on VQA val set:  31% 2064/6699 [24:18<56:14,  1.37it/s]Evaluating on VQA val set:  31% 2065/6699 [24:18<55:48,  1.38it/s]Evaluating on VQA val set:  31% 2066/6699 [24:19<54:10,  1.43it/s]Evaluating on VQA val set:  31% 2067/6699 [24:20<52:24,  1.47it/s]Evaluating on VQA val set:  31% 2068/6699 [24:20<53:31,  1.44it/s]Evaluating on VQA val set:  31% 2069/6699 [24:21<54:47,  1.41it/s]Evaluating on VQA val set:  31% 2070/6699 [24:22<54:44,  1.41it/s]Evaluating on VQA val set:  31% 2071/6699 [24:22<54:49,  1.41it/s]Evaluating on VQA val set:  31% 2072/6699 [24:23<55:21,  1.39it/s]Evaluating on VQA val set:  31% 2073/6699 [24:24<56:03,  1.38it/s]Evaluating on VQA val set:  31% 2074/6699 [24:25<56:23,  1.37it/s]Evaluating on VQA val set:  31% 2075/6699 [24:25<55:33,  1.39it/s]Evaluating on VQA val set:  31% 2076/6699 [24:26<55:55,  1.38it/s]Evaluating on VQA val set:  31% 2077/6699 [24:27<56:16,  1.37it/s]Evaluating on VQA val set:  31% 2078/6699 [24:28<55:14,  1.39it/s]Evaluating on VQA val set:  31% 2079/6699 [24:28<53:01,  1.45it/s]Evaluating on VQA val set:  31% 2080/6699 [24:29<54:35,  1.41it/s]Evaluating on VQA val set:  31% 2081/6699 [24:29<51:08,  1.51it/s]Evaluating on VQA val set:  31% 2082/6699 [24:30<54:27,  1.41it/s]Evaluating on VQA val set:  31% 2083/6699 [24:31<55:09,  1.39it/s]Evaluating on VQA val set:  31% 2084/6699 [24:32<55:34,  1.38it/s]Evaluating on VQA val set:  31% 2085/6699 [24:32<55:00,  1.40it/s]Evaluating on VQA val set:  31% 2086/6699 [24:33<54:43,  1.40it/s]Evaluating on VQA val set:  31% 2087/6699 [24:34<55:48,  1.38it/s]Evaluating on VQA val set:  31% 2088/6699 [24:35<54:30,  1.41it/s]Evaluating on VQA val set:  31% 2089/6699 [24:35<54:35,  1.41it/s]Evaluating on VQA val set:  31% 2090/6699 [24:36<54:40,  1.41it/s]Evaluating on VQA val set:  31% 2091/6699 [24:37<54:14,  1.42it/s]Evaluating on VQA val set:  31% 2092/6699 [24:37<55:21,  1.39it/s]Evaluating on VQA val set:  31% 2093/6699 [24:38<51:54,  1.48it/s]Evaluating on VQA val set:  31% 2094/6699 [24:39<53:15,  1.44it/s]Evaluating on VQA val set:  31% 2095/6699 [24:40<56:34,  1.36it/s]Evaluating on VQA val set:  31% 2096/6699 [24:40<55:48,  1.37it/s]Evaluating on VQA val set:  31% 2097/6699 [24:41<55:51,  1.37it/s]Evaluating on VQA val set:  31% 2098/6699 [24:42<55:17,  1.39it/s]Evaluating on VQA val set:  31% 2099/6699 [24:42<55:32,  1.38it/s]Evaluating on VQA val set:  31% 2100/6699 [24:43<54:30,  1.41it/s]Evaluating on VQA val set:  31% 2101/6699 [24:44<53:31,  1.43it/s]Evaluating on VQA val set:  31% 2102/6699 [24:44<53:01,  1.45it/s]Evaluating on VQA val set:  31% 2103/6699 [24:45<51:51,  1.48it/s]Evaluating on VQA val set:  31% 2104/6699 [24:46<52:39,  1.45it/s]Evaluating on VQA val set:  31% 2105/6699 [24:47<53:00,  1.44it/s]Evaluating on VQA val set:  31% 2106/6699 [24:47<53:34,  1.43it/s]Evaluating on VQA val set:  31% 2107/6699 [24:48<55:10,  1.39it/s]Evaluating on VQA val set:  31% 2108/6699 [24:49<54:50,  1.40it/s]Evaluating on VQA val set:  31% 2109/6699 [24:49<54:47,  1.40it/s]Evaluating on VQA val set:  31% 2110/6699 [24:50<52:02,  1.47it/s]Evaluating on VQA val set:  32% 2111/6699 [24:51<53:42,  1.42it/s]Evaluating on VQA val set:  32% 2112/6699 [24:51<53:01,  1.44it/s]Evaluating on VQA val set:  32% 2113/6699 [24:52<53:26,  1.43it/s]Evaluating on VQA val set:  32% 2114/6699 [24:53<54:08,  1.41it/s]Evaluating on VQA val set:  32% 2115/6699 [24:54<53:39,  1.42it/s]Evaluating on VQA val set:  32% 2116/6699 [24:54<53:44,  1.42it/s]Evaluating on VQA val set:  32% 2117/6699 [24:55<54:12,  1.41it/s]Evaluating on VQA val set:  32% 2118/6699 [24:56<54:45,  1.39it/s]Evaluating on VQA val set:  32% 2119/6699 [24:56<54:11,  1.41it/s]Evaluating on VQA val set:  32% 2120/6699 [24:57<53:56,  1.41it/s]Evaluating on VQA val set:  32% 2121/6699 [24:58<54:09,  1.41it/s]Evaluating on VQA val set:  32% 2122/6699 [24:59<55:08,  1.38it/s]Evaluating on VQA val set:  32% 2123/6699 [24:59<54:17,  1.40it/s]Evaluating on VQA val set:  32% 2124/6699 [25:00<55:39,  1.37it/s]Evaluating on VQA val set:  32% 2125/6699 [25:01<55:55,  1.36it/s]Evaluating on VQA val set:  32% 2126/6699 [25:01<52:45,  1.44it/s]Evaluating on VQA val set:  32% 2127/6699 [25:02<53:09,  1.43it/s]Evaluating on VQA val set:  32% 2128/6699 [25:03<50:40,  1.50it/s]Evaluating on VQA val set:  32% 2129/6699 [25:03<44:53,  1.70it/s]Evaluating on VQA val set:  32% 2130/6699 [25:04<45:38,  1.67it/s]Evaluating on VQA val set:  32% 2131/6699 [25:05<48:55,  1.56it/s]Evaluating on VQA val set:  32% 2132/6699 [25:05<50:43,  1.50it/s]Evaluating on VQA val set:  32% 2133/6699 [25:06<51:59,  1.46it/s]Evaluating on VQA val set:  32% 2134/6699 [25:07<51:47,  1.47it/s]Evaluating on VQA val set:  32% 2135/6699 [25:07<51:47,  1.47it/s]Evaluating on VQA val set:  32% 2136/6699 [25:08<53:24,  1.42it/s]Evaluating on VQA val set:  32% 2137/6699 [25:09<54:06,  1.41it/s]Evaluating on VQA val set:  32% 2138/6699 [25:10<54:28,  1.40it/s]Evaluating on VQA val set:  32% 2139/6699 [25:10<54:39,  1.39it/s]Evaluating on VQA val set:  32% 2140/6699 [25:11<55:04,  1.38it/s]Evaluating on VQA val set:  32% 2141/6699 [25:12<55:38,  1.37it/s]Evaluating on VQA val set:  32% 2142/6699 [25:12<54:41,  1.39it/s]Evaluating on VQA val set:  32% 2143/6699 [25:13<54:30,  1.39it/s]Evaluating on VQA val set:  32% 2144/6699 [25:14<55:30,  1.37it/s]Evaluating on VQA val set:  32% 2145/6699 [25:15<53:52,  1.41it/s]Evaluating on VQA val set:  32% 2146/6699 [25:15<54:10,  1.40it/s]Evaluating on VQA val set:  32% 2147/6699 [25:16<53:02,  1.43it/s]Evaluating on VQA val set:  32% 2148/6699 [25:17<52:21,  1.45it/s]Evaluating on VQA val set:  32% 2149/6699 [25:17<52:01,  1.46it/s]Evaluating on VQA val set:  32% 2150/6699 [25:18<53:01,  1.43it/s]Evaluating on VQA val set:  32% 2151/6699 [25:19<53:25,  1.42it/s]Evaluating on VQA val set:  32% 2152/6699 [25:19<52:40,  1.44it/s]Evaluating on VQA val set:  32% 2153/6699 [25:20<53:02,  1.43it/s]Evaluating on VQA val set:  32% 2154/6699 [25:21<54:01,  1.40it/s]Evaluating on VQA val set:  32% 2155/6699 [25:22<54:05,  1.40it/s]Evaluating on VQA val set:  32% 2156/6699 [25:22<54:51,  1.38it/s]Evaluating on VQA val set:  32% 2157/6699 [25:23<55:20,  1.37it/s]Evaluating on VQA val set:  32% 2158/6699 [25:24<54:41,  1.38it/s]Evaluating on VQA val set:  32% 2159/6699 [25:24<54:10,  1.40it/s]Evaluating on VQA val set:  32% 2160/6699 [25:25<52:11,  1.45it/s]Evaluating on VQA val set:  32% 2161/6699 [25:26<49:12,  1.54it/s]Evaluating on VQA val set:  32% 2162/6699 [25:26<47:23,  1.60it/s]Evaluating on VQA val set:  32% 2163/6699 [25:27<48:59,  1.54it/s]Evaluating on VQA val set:  32% 2164/6699 [25:28<50:55,  1.48it/s]Evaluating on VQA val set:  32% 2165/6699 [25:28<51:58,  1.45it/s]Evaluating on VQA val set:  32% 2166/6699 [25:29<51:51,  1.46it/s]Evaluating on VQA val set:  32% 2167/6699 [25:30<50:17,  1.50it/s]Evaluating on VQA val set:  32% 2168/6699 [25:30<50:44,  1.49it/s]Evaluating on VQA val set:  32% 2169/6699 [25:31<52:42,  1.43it/s]Evaluating on VQA val set:  32% 2170/6699 [25:32<52:46,  1.43it/s]Evaluating on VQA val set:  32% 2171/6699 [25:33<53:23,  1.41it/s]Evaluating on VQA val set:  32% 2172/6699 [25:33<54:06,  1.39it/s]Evaluating on VQA val set:  32% 2173/6699 [25:34<54:05,  1.39it/s]Evaluating on VQA val set:  32% 2174/6699 [25:35<53:50,  1.40it/s]Evaluating on VQA val set:  32% 2175/6699 [25:35<54:32,  1.38it/s]Evaluating on VQA val set:  32% 2176/6699 [25:36<54:06,  1.39it/s]Evaluating on VQA val set:  32% 2177/6699 [25:37<54:01,  1.40it/s]Evaluating on VQA val set:  33% 2178/6699 [25:38<54:25,  1.38it/s]Evaluating on VQA val set:  33% 2179/6699 [25:38<55:00,  1.37it/s]Evaluating on VQA val set:  33% 2180/6699 [25:39<53:57,  1.40it/s]Evaluating on VQA val set:  33% 2181/6699 [25:40<53:15,  1.41it/s]Evaluating on VQA val set:  33% 2182/6699 [25:41<54:14,  1.39it/s]Evaluating on VQA val set:  33% 2183/6699 [25:41<53:17,  1.41it/s]Evaluating on VQA val set:  33% 2184/6699 [25:42<52:55,  1.42it/s]Evaluating on VQA val set:  33% 2185/6699 [25:43<53:36,  1.40it/s]Evaluating on VQA val set:  33% 2186/6699 [25:43<53:27,  1.41it/s]Evaluating on VQA val set:  33% 2187/6699 [25:44<55:43,  1.35it/s]Evaluating on VQA val set:  33% 2188/6699 [25:45<55:22,  1.36it/s]Evaluating on VQA val set:  33% 2189/6699 [25:46<54:55,  1.37it/s]Evaluating on VQA val set:  33% 2190/6699 [25:46<54:52,  1.37it/s]Evaluating on VQA val set:  33% 2191/6699 [25:47<56:19,  1.33it/s]Evaluating on VQA val set:  33% 2192/6699 [25:48<56:27,  1.33it/s]Evaluating on VQA val set:  33% 2193/6699 [25:49<54:51,  1.37it/s]Evaluating on VQA val set:  33% 2194/6699 [25:49<53:15,  1.41it/s]Evaluating on VQA val set:  33% 2195/6699 [25:50<53:05,  1.41it/s]Evaluating on VQA val set:  33% 2196/6699 [25:51<52:36,  1.43it/s]Evaluating on VQA val set:  33% 2197/6699 [25:51<50:21,  1.49it/s]Evaluating on VQA val set:  33% 2198/6699 [25:52<52:00,  1.44it/s]Evaluating on VQA val set:  33% 2199/6699 [25:53<51:19,  1.46it/s]Evaluating on VQA val set:  33% 2200/6699 [25:53<52:55,  1.42it/s]Evaluating on VQA val set:  33% 2201/6699 [25:54<54:58,  1.36it/s]Evaluating on VQA val set:  33% 2202/6699 [25:55<56:31,  1.33it/s]Evaluating on VQA val set:  33% 2203/6699 [25:56<55:01,  1.36it/s]Evaluating on VQA val set:  33% 2204/6699 [25:56<55:33,  1.35it/s]Evaluating on VQA val set:  33% 2205/6699 [25:57<54:21,  1.38it/s]Evaluating on VQA val set:  33% 2206/6699 [25:58<52:41,  1.42it/s]Evaluating on VQA val set:  33% 2207/6699 [25:58<52:29,  1.43it/s]Evaluating on VQA val set:  33% 2208/6699 [25:59<52:38,  1.42it/s]Evaluating on VQA val set:  33% 2209/6699 [26:00<53:18,  1.40it/s]Evaluating on VQA val set:  33% 2210/6699 [26:01<52:35,  1.42it/s]Evaluating on VQA val set:  33% 2211/6699 [26:01<51:27,  1.45it/s]Evaluating on VQA val set:  33% 2212/6699 [26:02<52:36,  1.42it/s]Evaluating on VQA val set:  33% 2213/6699 [26:03<52:04,  1.44it/s]Evaluating on VQA val set:  33% 2214/6699 [26:03<53:04,  1.41it/s]Evaluating on VQA val set:  33% 2215/6699 [26:04<52:51,  1.41it/s]Evaluating on VQA val set:  33% 2216/6699 [26:05<53:30,  1.40it/s]Evaluating on VQA val set:  33% 2217/6699 [26:06<54:18,  1.38it/s]Evaluating on VQA val set:  33% 2218/6699 [26:06<52:04,  1.43it/s]Evaluating on VQA val set:  33% 2219/6699 [26:07<51:00,  1.46it/s]Evaluating on VQA val set:  33% 2220/6699 [26:08<50:59,  1.46it/s]Evaluating on VQA val set:  33% 2221/6699 [26:08<51:15,  1.46it/s]Evaluating on VQA val set:  33% 2222/6699 [26:09<50:34,  1.48it/s]Evaluating on VQA val set:  33% 2223/6699 [26:10<52:08,  1.43it/s]Evaluating on VQA val set:  33% 2224/6699 [26:10<52:42,  1.42it/s]Evaluating on VQA val set:  33% 2225/6699 [26:11<52:30,  1.42it/s]Evaluating on VQA val set:  33% 2226/6699 [26:12<52:05,  1.43it/s]Evaluating on VQA val set:  33% 2227/6699 [26:12<51:15,  1.45it/s]Evaluating on VQA val set:  33% 2228/6699 [26:13<50:31,  1.47it/s]Evaluating on VQA val set:  33% 2229/6699 [26:14<48:41,  1.53it/s]Evaluating on VQA val set:  33% 2230/6699 [26:14<49:12,  1.51it/s]Evaluating on VQA val set:  33% 2231/6699 [26:15<50:47,  1.47it/s]Evaluating on VQA val set:  33% 2232/6699 [26:16<51:04,  1.46it/s]Evaluating on VQA val set:  33% 2233/6699 [26:16<51:38,  1.44it/s]Evaluating on VQA val set:  33% 2234/6699 [26:17<51:22,  1.45it/s]Evaluating on VQA val set:  33% 2235/6699 [26:18<52:03,  1.43it/s]Evaluating on VQA val set:  33% 2236/6699 [26:19<53:01,  1.40it/s]Evaluating on VQA val set:  33% 2237/6699 [26:19<53:00,  1.40it/s]Evaluating on VQA val set:  33% 2238/6699 [26:20<52:31,  1.42it/s]Evaluating on VQA val set:  33% 2239/6699 [26:21<53:29,  1.39it/s]Evaluating on VQA val set:  33% 2240/6699 [26:21<52:48,  1.41it/s]Evaluating on VQA val set:  33% 2241/6699 [26:22<53:10,  1.40it/s]Evaluating on VQA val set:  33% 2242/6699 [26:23<52:52,  1.40it/s]Evaluating on VQA val set:  33% 2243/6699 [26:24<53:38,  1.38it/s]Evaluating on VQA val set:  33% 2244/6699 [26:24<53:27,  1.39it/s]Evaluating on VQA val set:  34% 2245/6699 [26:25<54:33,  1.36it/s]Evaluating on VQA val set:  34% 2246/6699 [26:26<54:49,  1.35it/s]Evaluating on VQA val set:  34% 2247/6699 [26:27<53:43,  1.38it/s]Evaluating on VQA val set:  34% 2248/6699 [26:27<53:12,  1.39it/s]Evaluating on VQA val set:  34% 2249/6699 [26:28<52:37,  1.41it/s]Evaluating on VQA val set:  34% 2250/6699 [26:29<52:30,  1.41it/s]Evaluating on VQA val set:  34% 2251/6699 [26:29<53:12,  1.39it/s]Evaluating on VQA val set:  34% 2252/6699 [26:30<52:48,  1.40it/s]Evaluating on VQA val set:  34% 2253/6699 [26:31<52:05,  1.42it/s]Evaluating on VQA val set:  34% 2254/6699 [26:31<50:58,  1.45it/s]Evaluating on VQA val set:  34% 2255/6699 [26:32<51:46,  1.43it/s]Evaluating on VQA val set:  34% 2256/6699 [26:33<51:46,  1.43it/s]Evaluating on VQA val set:  34% 2257/6699 [26:34<51:23,  1.44it/s]Evaluating on VQA val set:  34% 2258/6699 [26:34<51:24,  1.44it/s]Evaluating on VQA val set:  34% 2259/6699 [26:35<51:51,  1.43it/s]Evaluating on VQA val set:  34% 2260/6699 [26:36<52:15,  1.42it/s]Evaluating on VQA val set:  34% 2261/6699 [26:36<50:11,  1.47it/s]Evaluating on VQA val set:  34% 2262/6699 [26:37<48:35,  1.52it/s]Evaluating on VQA val set:  34% 2263/6699 [26:38<48:26,  1.53it/s]Evaluating on VQA val set:  34% 2264/6699 [26:38<46:19,  1.60it/s]Evaluating on VQA val set:  34% 2265/6699 [26:39<48:14,  1.53it/s]Evaluating on VQA val set:  34% 2266/6699 [26:39<48:05,  1.54it/s]Evaluating on VQA val set:  34% 2267/6699 [26:40<49:38,  1.49it/s]Evaluating on VQA val set:  34% 2268/6699 [26:41<49:48,  1.48it/s]Evaluating on VQA val set:  34% 2269/6699 [26:42<50:11,  1.47it/s]Evaluating on VQA val set:  34% 2270/6699 [26:42<52:06,  1.42it/s]Evaluating on VQA val set:  34% 2271/6699 [26:43<51:02,  1.45it/s]Evaluating on VQA val set:  34% 2272/6699 [26:44<53:02,  1.39it/s]Evaluating on VQA val set:  34% 2273/6699 [26:44<53:09,  1.39it/s]Evaluating on VQA val set:  34% 2274/6699 [26:45<53:09,  1.39it/s]Evaluating on VQA val set:  34% 2275/6699 [26:46<54:29,  1.35it/s]Evaluating on VQA val set:  34% 2276/6699 [26:47<53:19,  1.38it/s]Evaluating on VQA val set:  34% 2277/6699 [26:47<54:54,  1.34it/s]Evaluating on VQA val set:  34% 2278/6699 [26:48<54:40,  1.35it/s]Evaluating on VQA val set:  34% 2279/6699 [26:49<53:56,  1.37it/s]Evaluating on VQA val set:  34% 2280/6699 [26:50<53:07,  1.39it/s]Evaluating on VQA val set:  34% 2281/6699 [26:50<51:59,  1.42it/s]Evaluating on VQA val set:  34% 2282/6699 [26:51<50:29,  1.46it/s]Evaluating on VQA val set:  34% 2283/6699 [26:52<51:43,  1.42it/s]Evaluating on VQA val set:  34% 2284/6699 [26:52<51:22,  1.43it/s]Evaluating on VQA val set:  34% 2285/6699 [26:53<53:01,  1.39it/s]Evaluating on VQA val set:  34% 2286/6699 [26:54<54:24,  1.35it/s]Evaluating on VQA val set:  34% 2287/6699 [26:55<54:10,  1.36it/s]Evaluating on VQA val set:  34% 2288/6699 [26:55<54:47,  1.34it/s]Evaluating on VQA val set:  34% 2289/6699 [26:56<54:25,  1.35it/s]Evaluating on VQA val set:  34% 2290/6699 [26:57<54:48,  1.34it/s]Evaluating on VQA val set:  34% 2291/6699 [26:58<54:34,  1.35it/s]Evaluating on VQA val set:  34% 2292/6699 [26:58<53:50,  1.36it/s]Evaluating on VQA val set:  34% 2293/6699 [26:59<52:44,  1.39it/s]Evaluating on VQA val set:  34% 2294/6699 [27:00<53:15,  1.38it/s]Evaluating on VQA val set:  34% 2295/6699 [27:00<51:18,  1.43it/s]Evaluating on VQA val set:  34% 2296/6699 [27:01<51:32,  1.42it/s]Evaluating on VQA val set:  34% 2297/6699 [27:02<52:03,  1.41it/s]Evaluating on VQA val set:  34% 2298/6699 [27:03<52:20,  1.40it/s]Evaluating on VQA val set:  34% 2299/6699 [27:03<52:21,  1.40it/s]Evaluating on VQA val set:  34% 2300/6699 [27:04<52:49,  1.39it/s]Evaluating on VQA val set:  34% 2301/6699 [27:05<53:07,  1.38it/s]Evaluating on VQA val set:  34% 2302/6699 [27:05<52:32,  1.39it/s]Evaluating on VQA val set:  34% 2303/6699 [27:06<50:21,  1.45it/s]Evaluating on VQA val set:  34% 2304/6699 [27:07<50:00,  1.46it/s]Evaluating on VQA val set:  34% 2305/6699 [27:07<48:03,  1.52it/s]Evaluating on VQA val set:  34% 2306/6699 [27:08<49:46,  1.47it/s]Evaluating on VQA val set:  34% 2307/6699 [27:09<51:05,  1.43it/s]Evaluating on VQA val set:  34% 2308/6699 [27:10<52:28,  1.39it/s]Evaluating on VQA val set:  34% 2309/6699 [27:10<53:20,  1.37it/s]Evaluating on VQA val set:  34% 2310/6699 [27:11<52:50,  1.38it/s]Evaluating on VQA val set:  34% 2311/6699 [27:12<52:12,  1.40it/s]Evaluating on VQA val set:  35% 2312/6699 [27:12<51:27,  1.42it/s]Evaluating on VQA val set:  35% 2313/6699 [27:13<51:14,  1.43it/s]Evaluating on VQA val set:  35% 2314/6699 [27:14<51:00,  1.43it/s]Evaluating on VQA val set:  35% 2315/6699 [27:14<49:34,  1.47it/s]Evaluating on VQA val set:  35% 2316/6699 [27:15<48:41,  1.50it/s]Evaluating on VQA val set:  35% 2317/6699 [27:16<46:16,  1.58it/s]Evaluating on VQA val set:  35% 2318/6699 [27:16<48:18,  1.51it/s]Evaluating on VQA val set:  35% 2319/6699 [27:17<49:41,  1.47it/s]Evaluating on VQA val set:  35% 2320/6699 [27:18<50:17,  1.45it/s]Evaluating on VQA val set:  35% 2321/6699 [27:19<52:31,  1.39it/s]Evaluating on VQA val set:  35% 2322/6699 [27:19<52:31,  1.39it/s]Evaluating on VQA val set:  35% 2323/6699 [27:20<53:44,  1.36it/s]Evaluating on VQA val set:  35% 2324/6699 [27:21<54:25,  1.34it/s]Evaluating on VQA val set:  35% 2325/6699 [27:22<53:10,  1.37it/s]Evaluating on VQA val set:  35% 2326/6699 [27:22<53:22,  1.37it/s]Evaluating on VQA val set:  35% 2327/6699 [27:23<52:28,  1.39it/s]Evaluating on VQA val set:  35% 2328/6699 [27:24<49:32,  1.47it/s]Evaluating on VQA val set:  35% 2329/6699 [27:24<49:32,  1.47it/s]Evaluating on VQA val set:  35% 2330/6699 [27:25<48:55,  1.49it/s]Evaluating on VQA val set:  35% 2331/6699 [27:26<50:24,  1.44it/s]Evaluating on VQA val set:  35% 2332/6699 [27:26<51:36,  1.41it/s]Evaluating on VQA val set:  35% 2333/6699 [27:27<50:40,  1.44it/s]Evaluating on VQA val set:  35% 2334/6699 [27:28<50:09,  1.45it/s]Evaluating on VQA val set:  35% 2335/6699 [27:28<50:09,  1.45it/s]Evaluating on VQA val set:  35% 2336/6699 [27:29<51:12,  1.42it/s]Evaluating on VQA val set:  35% 2337/6699 [27:30<51:29,  1.41it/s]Evaluating on VQA val set:  35% 2338/6699 [27:31<51:02,  1.42it/s]Evaluating on VQA val set:  35% 2339/6699 [27:31<52:20,  1.39it/s]Evaluating on VQA val set:  35% 2340/6699 [27:32<52:33,  1.38it/s]Evaluating on VQA val set:  35% 2341/6699 [27:33<52:10,  1.39it/s]Evaluating on VQA val set:  35% 2342/6699 [27:33<51:57,  1.40it/s]Evaluating on VQA val set:  35% 2343/6699 [27:34<51:56,  1.40it/s]Evaluating on VQA val set:  35% 2344/6699 [27:35<51:40,  1.40it/s]Evaluating on VQA val set:  35% 2345/6699 [27:36<51:02,  1.42it/s]Evaluating on VQA val set:  35% 2346/6699 [27:36<50:38,  1.43it/s]Evaluating on VQA val set:  35% 2347/6699 [27:37<48:52,  1.48it/s]Evaluating on VQA val set:  35% 2348/6699 [27:38<50:23,  1.44it/s]Evaluating on VQA val set:  35% 2349/6699 [27:38<50:25,  1.44it/s]Evaluating on VQA val set:  35% 2350/6699 [27:39<52:58,  1.37it/s]Evaluating on VQA val set:  35% 2351/6699 [27:40<51:10,  1.42it/s]Evaluating on VQA val set:  35% 2352/6699 [27:40<50:13,  1.44it/s]Evaluating on VQA val set:  35% 2353/6699 [27:41<51:01,  1.42it/s]Evaluating on VQA val set:  35% 2354/6699 [27:42<50:46,  1.43it/s]Evaluating on VQA val set:  35% 2355/6699 [27:42<49:58,  1.45it/s]Evaluating on VQA val set:  35% 2356/6699 [27:43<48:37,  1.49it/s]Evaluating on VQA val set:  35% 2357/6699 [27:44<49:55,  1.45it/s]Evaluating on VQA val set:  35% 2358/6699 [27:45<51:34,  1.40it/s]Evaluating on VQA val set:  35% 2359/6699 [27:45<52:14,  1.38it/s]Evaluating on VQA val set:  35% 2360/6699 [27:46<51:55,  1.39it/s]Evaluating on VQA val set:  35% 2361/6699 [27:47<50:22,  1.44it/s]Evaluating on VQA val set:  35% 2362/6699 [27:47<50:48,  1.42it/s]Evaluating on VQA val set:  35% 2363/6699 [27:48<49:53,  1.45it/s]Evaluating on VQA val set:  35% 2364/6699 [27:49<49:13,  1.47it/s]Evaluating on VQA val set:  35% 2365/6699 [27:49<48:54,  1.48it/s]Evaluating on VQA val set:  35% 2366/6699 [27:50<49:19,  1.46it/s]Evaluating on VQA val set:  35% 2367/6699 [27:51<49:13,  1.47it/s]Evaluating on VQA val set:  35% 2368/6699 [27:52<50:04,  1.44it/s]Evaluating on VQA val set:  35% 2369/6699 [27:52<49:34,  1.46it/s]Evaluating on VQA val set:  35% 2370/6699 [27:53<50:47,  1.42it/s]Evaluating on VQA val set:  35% 2371/6699 [27:54<49:52,  1.45it/s]Evaluating on VQA val set:  35% 2372/6699 [27:54<48:39,  1.48it/s]Evaluating on VQA val set:  35% 2373/6699 [27:55<49:13,  1.46it/s]Evaluating on VQA val set:  35% 2374/6699 [27:56<50:49,  1.42it/s]Evaluating on VQA val set:  35% 2375/6699 [27:56<49:57,  1.44it/s]Evaluating on VQA val set:  35% 2376/6699 [27:57<50:48,  1.42it/s]Evaluating on VQA val set:  35% 2377/6699 [27:58<49:35,  1.45it/s]Evaluating on VQA val set:  35% 2378/6699 [27:58<49:47,  1.45it/s]Evaluating on VQA val set:  36% 2379/6699 [27:59<49:56,  1.44it/s]Evaluating on VQA val set:  36% 2380/6699 [28:00<50:22,  1.43it/s]Evaluating on VQA val set:  36% 2381/6699 [28:01<51:17,  1.40it/s]Evaluating on VQA val set:  36% 2382/6699 [28:01<52:12,  1.38it/s]Evaluating on VQA val set:  36% 2383/6699 [28:02<50:10,  1.43it/s]Evaluating on VQA val set:  36% 2384/6699 [28:03<51:51,  1.39it/s]Evaluating on VQA val set:  36% 2385/6699 [28:03<51:02,  1.41it/s]Evaluating on VQA val set:  36% 2386/6699 [28:04<52:03,  1.38it/s]Evaluating on VQA val set:  36% 2387/6699 [28:05<51:41,  1.39it/s]Evaluating on VQA val set:  36% 2388/6699 [28:06<50:51,  1.41it/s]Evaluating on VQA val set:  36% 2389/6699 [28:06<51:06,  1.41it/s]Evaluating on VQA val set:  36% 2390/6699 [28:07<51:39,  1.39it/s]Evaluating on VQA val set:  36% 2391/6699 [28:08<50:53,  1.41it/s]Evaluating on VQA val set:  36% 2392/6699 [28:08<49:32,  1.45it/s]Evaluating on VQA val set:  36% 2393/6699 [28:09<50:51,  1.41it/s]Evaluating on VQA val set:  36% 2394/6699 [28:10<50:20,  1.43it/s]Evaluating on VQA val set:  36% 2395/6699 [28:11<49:49,  1.44it/s]Evaluating on VQA val set:  36% 2396/6699 [28:11<49:38,  1.44it/s]Evaluating on VQA val set:  36% 2397/6699 [28:12<49:22,  1.45it/s]Evaluating on VQA val set:  36% 2398/6699 [28:13<50:18,  1.42it/s]Evaluating on VQA val set:  36% 2399/6699 [28:13<50:29,  1.42it/s]Evaluating on VQA val set:  36% 2400/6699 [28:14<50:56,  1.41it/s]Evaluating on VQA val set:  36% 2401/6699 [28:15<52:03,  1.38it/s]Evaluating on VQA val set:  36% 2402/6699 [28:16<51:50,  1.38it/s]Evaluating on VQA val set:  36% 2403/6699 [28:16<51:59,  1.38it/s]Evaluating on VQA val set:  36% 2404/6699 [28:17<49:38,  1.44it/s]Evaluating on VQA val set:  36% 2405/6699 [28:18<49:29,  1.45it/s]Evaluating on VQA val set:  36% 2406/6699 [28:18<50:04,  1.43it/s]Evaluating on VQA val set:  36% 2407/6699 [28:19<50:16,  1.42it/s]Evaluating on VQA val set:  36% 2408/6699 [28:20<50:32,  1.41it/s]Evaluating on VQA val set:  36% 2409/6699 [28:20<48:27,  1.48it/s]Evaluating on VQA val set:  36% 2410/6699 [28:21<49:48,  1.44it/s]Evaluating on VQA val set:  36% 2411/6699 [28:22<49:46,  1.44it/s]Evaluating on VQA val set:  36% 2412/6699 [28:22<50:21,  1.42it/s]Evaluating on VQA val set:  36% 2413/6699 [28:23<50:49,  1.41it/s]Evaluating on VQA val set:  36% 2414/6699 [28:24<48:59,  1.46it/s]Evaluating on VQA val set:  36% 2415/6699 [28:25<49:28,  1.44it/s]Evaluating on VQA val set:  36% 2416/6699 [28:25<49:29,  1.44it/s]Evaluating on VQA val set:  36% 2417/6699 [28:26<50:01,  1.43it/s]Evaluating on VQA val set:  36% 2418/6699 [28:27<50:53,  1.40it/s]Evaluating on VQA val set:  36% 2419/6699 [28:27<49:58,  1.43it/s]Evaluating on VQA val set:  36% 2420/6699 [28:28<48:39,  1.47it/s]Evaluating on VQA val set:  36% 2421/6699 [28:29<48:53,  1.46it/s]Evaluating on VQA val set:  36% 2422/6699 [28:29<49:32,  1.44it/s]Evaluating on VQA val set:  36% 2423/6699 [28:30<49:25,  1.44it/s]Evaluating on VQA val set:  36% 2424/6699 [28:31<49:58,  1.43it/s]Evaluating on VQA val set:  36% 2425/6699 [28:32<51:01,  1.40it/s]Evaluating on VQA val set:  36% 2426/6699 [28:32<49:53,  1.43it/s]Evaluating on VQA val set:  36% 2427/6699 [28:33<49:22,  1.44it/s]Evaluating on VQA val set:  36% 2428/6699 [28:34<49:51,  1.43it/s]Evaluating on VQA val set:  36% 2429/6699 [28:34<48:56,  1.45it/s]Evaluating on VQA val set:  36% 2430/6699 [28:35<49:25,  1.44it/s]Evaluating on VQA val set:  36% 2431/6699 [28:36<50:04,  1.42it/s]Evaluating on VQA val set:  36% 2432/6699 [28:36<49:53,  1.43it/s]Evaluating on VQA val set:  36% 2433/6699 [28:37<50:48,  1.40it/s]Evaluating on VQA val set:  36% 2434/6699 [28:38<51:11,  1.39it/s]Evaluating on VQA val set:  36% 2435/6699 [28:39<49:50,  1.43it/s]Evaluating on VQA val set:  36% 2436/6699 [28:39<50:18,  1.41it/s]Evaluating on VQA val set:  36% 2437/6699 [28:40<50:55,  1.40it/s]Evaluating on VQA val set:  36% 2438/6699 [28:41<51:34,  1.38it/s]Evaluating on VQA val set:  36% 2439/6699 [28:42<52:32,  1.35it/s]Evaluating on VQA val set:  36% 2440/6699 [28:42<51:39,  1.37it/s]Evaluating on VQA val set:  36% 2441/6699 [28:43<52:06,  1.36it/s]Evaluating on VQA val set:  36% 2442/6699 [28:44<49:27,  1.43it/s]Evaluating on VQA val set:  36% 2443/6699 [28:44<49:22,  1.44it/s]Evaluating on VQA val set:  36% 2444/6699 [28:45<50:09,  1.41it/s]Evaluating on VQA val set:  36% 2445/6699 [28:46<50:22,  1.41it/s]Evaluating on VQA val set:  37% 2446/6699 [28:46<48:00,  1.48it/s]Evaluating on VQA val set:  37% 2447/6699 [28:47<48:35,  1.46it/s]Evaluating on VQA val set:  37% 2448/6699 [28:48<49:45,  1.42it/s]Evaluating on VQA val set:  37% 2449/6699 [28:49<50:39,  1.40it/s]Evaluating on VQA val set:  37% 2450/6699 [28:49<51:08,  1.38it/s]Evaluating on VQA val set:  37% 2451/6699 [28:50<51:12,  1.38it/s]Evaluating on VQA val set:  37% 2452/6699 [28:51<51:57,  1.36it/s]Evaluating on VQA val set:  37% 2453/6699 [28:52<52:31,  1.35it/s]Evaluating on VQA val set:  37% 2454/6699 [28:52<50:59,  1.39it/s]Evaluating on VQA val set:  37% 2455/6699 [28:53<50:18,  1.41it/s]Evaluating on VQA val set:  37% 2456/6699 [28:53<48:09,  1.47it/s]Evaluating on VQA val set:  37% 2457/6699 [28:54<47:19,  1.49it/s]Evaluating on VQA val set:  37% 2458/6699 [28:55<47:52,  1.48it/s]Evaluating on VQA val set:  37% 2459/6699 [28:56<48:33,  1.46it/s]Evaluating on VQA val set:  37% 2460/6699 [28:56<49:06,  1.44it/s]Evaluating on VQA val set:  37% 2461/6699 [28:57<50:10,  1.41it/s]Evaluating on VQA val set:  37% 2462/6699 [28:58<49:00,  1.44it/s]Evaluating on VQA val set:  37% 2463/6699 [28:58<49:36,  1.42it/s]Evaluating on VQA val set:  37% 2464/6699 [28:59<50:21,  1.40it/s]Evaluating on VQA val set:  37% 2465/6699 [29:00<49:42,  1.42it/s]Evaluating on VQA val set:  37% 2466/6699 [29:01<49:50,  1.42it/s]Evaluating on VQA val set:  37% 2467/6699 [29:01<49:46,  1.42it/s]Evaluating on VQA val set:  37% 2468/6699 [29:02<49:44,  1.42it/s]Evaluating on VQA val set:  37% 2469/6699 [29:03<50:09,  1.41it/s]Evaluating on VQA val set:  37% 2470/6699 [29:03<51:10,  1.38it/s]Evaluating on VQA val set:  37% 2471/6699 [29:04<49:15,  1.43it/s]Evaluating on VQA val set:  37% 2472/6699 [29:05<49:07,  1.43it/s]Evaluating on VQA val set:  37% 2473/6699 [29:06<51:15,  1.37it/s]Evaluating on VQA val set:  37% 2474/6699 [29:06<51:22,  1.37it/s]Evaluating on VQA val set:  37% 2475/6699 [29:07<50:02,  1.41it/s]Evaluating on VQA val set:  37% 2476/6699 [29:08<50:25,  1.40it/s]Evaluating on VQA val set:  37% 2477/6699 [29:08<50:50,  1.38it/s]Evaluating on VQA val set:  37% 2478/6699 [29:09<49:45,  1.41it/s]Evaluating on VQA val set:  37% 2479/6699 [29:10<50:46,  1.39it/s]Evaluating on VQA val set:  37% 2480/6699 [29:10<49:33,  1.42it/s]Evaluating on VQA val set:  37% 2481/6699 [29:11<49:15,  1.43it/s]Evaluating on VQA val set:  37% 2482/6699 [29:12<49:37,  1.42it/s]Evaluating on VQA val set:  37% 2483/6699 [29:12<47:00,  1.49it/s]Evaluating on VQA val set:  37% 2484/6699 [29:13<48:09,  1.46it/s]Evaluating on VQA val set:  37% 2485/6699 [29:14<47:32,  1.48it/s]Evaluating on VQA val set:  37% 2486/6699 [29:14<46:09,  1.52it/s]Evaluating on VQA val set:  37% 2487/6699 [29:15<47:48,  1.47it/s]Evaluating on VQA val set:  37% 2488/6699 [29:16<47:48,  1.47it/s]Evaluating on VQA val set:  37% 2489/6699 [29:17<46:39,  1.50it/s]Evaluating on VQA val set:  37% 2490/6699 [29:17<48:18,  1.45it/s]Evaluating on VQA val set:  37% 2491/6699 [29:18<49:28,  1.42it/s]Evaluating on VQA val set:  37% 2492/6699 [29:19<49:37,  1.41it/s]Evaluating on VQA val set:  37% 2493/6699 [29:19<49:11,  1.42it/s]Evaluating on VQA val set:  37% 2494/6699 [29:20<49:43,  1.41it/s]Evaluating on VQA val set:  37% 2495/6699 [29:21<49:31,  1.41it/s]Evaluating on VQA val set:  37% 2496/6699 [29:22<48:44,  1.44it/s]Evaluating on VQA val set:  37% 2497/6699 [29:22<49:57,  1.40it/s]Evaluating on VQA val set:  37% 2498/6699 [29:23<51:30,  1.36it/s]Evaluating on VQA val set:  37% 2499/6699 [29:24<51:39,  1.35it/s]Evaluating on VQA val set:  37% 2500/6699 [29:24<51:02,  1.37it/s]Evaluating on VQA val set:  37% 2501/6699 [29:25<50:17,  1.39it/s]Evaluating on VQA val set:  37% 2502/6699 [29:26<49:27,  1.41it/s]Evaluating on VQA val set:  37% 2503/6699 [29:27<49:59,  1.40it/s]Evaluating on VQA val set:  37% 2504/6699 [29:27<46:44,  1.50it/s]Evaluating on VQA val set:  37% 2505/6699 [29:28<47:33,  1.47it/s]Evaluating on VQA val set:  37% 2506/6699 [29:29<48:27,  1.44it/s]Evaluating on VQA val set:  37% 2507/6699 [29:29<49:19,  1.42it/s]Evaluating on VQA val set:  37% 2508/6699 [29:30<48:23,  1.44it/s]Evaluating on VQA val set:  37% 2509/6699 [29:31<47:59,  1.46it/s]Evaluating on VQA val set:  37% 2510/6699 [29:31<47:11,  1.48it/s]Evaluating on VQA val set:  37% 2511/6699 [29:32<47:06,  1.48it/s]Evaluating on VQA val set:  37% 2512/6699 [29:33<47:26,  1.47it/s]Evaluating on VQA val set:  38% 2513/6699 [29:33<47:33,  1.47it/s]Evaluating on VQA val set:  38% 2514/6699 [29:34<47:48,  1.46it/s]Evaluating on VQA val set:  38% 2515/6699 [29:35<47:38,  1.46it/s]Evaluating on VQA val set:  38% 2516/6699 [29:35<48:40,  1.43it/s]Evaluating on VQA val set:  38% 2517/6699 [29:36<49:19,  1.41it/s]Evaluating on VQA val set:  38% 2518/6699 [29:37<50:13,  1.39it/s]Evaluating on VQA val set:  38% 2519/6699 [29:38<50:16,  1.39it/s]Evaluating on VQA val set:  38% 2520/6699 [29:38<48:25,  1.44it/s]Evaluating on VQA val set:  38% 2521/6699 [29:39<48:25,  1.44it/s]Evaluating on VQA val set:  38% 2522/6699 [29:40<48:46,  1.43it/s]Evaluating on VQA val set:  38% 2523/6699 [29:40<49:41,  1.40it/s]Evaluating on VQA val set:  38% 2524/6699 [29:41<50:27,  1.38it/s]Evaluating on VQA val set:  38% 2525/6699 [29:42<49:13,  1.41it/s]Evaluating on VQA val set:  38% 2526/6699 [29:42<46:59,  1.48it/s]Evaluating on VQA val set:  38% 2527/6699 [29:43<46:35,  1.49it/s]Evaluating on VQA val set:  38% 2528/6699 [29:44<46:58,  1.48it/s]Evaluating on VQA val set:  38% 2529/6699 [29:45<47:52,  1.45it/s]Evaluating on VQA val set:  38% 2530/6699 [29:45<47:03,  1.48it/s]Evaluating on VQA val set:  38% 2531/6699 [29:46<46:05,  1.51it/s]Evaluating on VQA val set:  38% 2532/6699 [29:46<45:45,  1.52it/s]Evaluating on VQA val set:  38% 2533/6699 [29:47<46:31,  1.49it/s]Evaluating on VQA val set:  38% 2534/6699 [29:48<46:34,  1.49it/s]Evaluating on VQA val set:  38% 2535/6699 [29:49<46:42,  1.49it/s]Evaluating on VQA val set:  38% 2536/6699 [29:49<45:53,  1.51it/s]Evaluating on VQA val set:  38% 2537/6699 [29:50<46:13,  1.50it/s]Evaluating on VQA val set:  38% 2538/6699 [29:51<47:34,  1.46it/s]Evaluating on VQA val set:  38% 2539/6699 [29:51<49:06,  1.41it/s]Evaluating on VQA val set:  38% 2540/6699 [29:52<49:23,  1.40it/s]Evaluating on VQA val set:  38% 2541/6699 [29:53<48:36,  1.43it/s]Evaluating on VQA val set:  38% 2542/6699 [29:53<48:55,  1.42it/s]Evaluating on VQA val set:  38% 2543/6699 [29:54<48:36,  1.43it/s]Evaluating on VQA val set:  38% 2544/6699 [29:55<47:56,  1.44it/s]Evaluating on VQA val set:  38% 2545/6699 [29:56<48:17,  1.43it/s]Evaluating on VQA val set:  38% 2546/6699 [29:56<48:39,  1.42it/s]Evaluating on VQA val set:  38% 2547/6699 [29:57<49:22,  1.40it/s]Evaluating on VQA val set:  38% 2548/6699 [29:58<49:26,  1.40it/s]Evaluating on VQA val set:  38% 2549/6699 [29:58<47:57,  1.44it/s]Evaluating on VQA val set:  38% 2550/6699 [29:59<49:01,  1.41it/s]Evaluating on VQA val set:  38% 2551/6699 [30:00<49:31,  1.40it/s]Evaluating on VQA val set:  38% 2552/6699 [30:00<48:54,  1.41it/s]Evaluating on VQA val set:  38% 2553/6699 [30:01<48:33,  1.42it/s]Evaluating on VQA val set:  38% 2554/6699 [30:02<48:51,  1.41it/s]Evaluating on VQA val set:  38% 2555/6699 [30:03<49:09,  1.40it/s]Evaluating on VQA val set:  38% 2556/6699 [30:03<47:22,  1.46it/s]Evaluating on VQA val set:  38% 2557/6699 [30:04<47:55,  1.44it/s]Evaluating on VQA val set:  38% 2558/6699 [30:05<48:57,  1.41it/s]Evaluating on VQA val set:  38% 2559/6699 [30:05<49:46,  1.39it/s]Evaluating on VQA val set:  38% 2560/6699 [30:06<49:15,  1.40it/s]Evaluating on VQA val set:  38% 2561/6699 [30:07<49:48,  1.38it/s]Evaluating on VQA val set:  38% 2562/6699 [30:08<49:56,  1.38it/s]Evaluating on VQA val set:  38% 2563/6699 [30:08<50:16,  1.37it/s]Evaluating on VQA val set:  38% 2564/6699 [30:09<49:42,  1.39it/s]Evaluating on VQA val set:  38% 2565/6699 [30:10<50:35,  1.36it/s]Evaluating on VQA val set:  38% 2566/6699 [30:11<49:48,  1.38it/s]Evaluating on VQA val set:  38% 2567/6699 [30:11<50:08,  1.37it/s]Evaluating on VQA val set:  38% 2568/6699 [30:12<50:35,  1.36it/s]Evaluating on VQA val set:  38% 2569/6699 [30:13<50:08,  1.37it/s]Evaluating on VQA val set:  38% 2570/6699 [30:13<49:57,  1.38it/s]Evaluating on VQA val set:  38% 2571/6699 [30:14<46:17,  1.49it/s]Evaluating on VQA val set:  38% 2572/6699 [30:15<48:40,  1.41it/s]Evaluating on VQA val set:  38% 2573/6699 [30:16<48:43,  1.41it/s]Evaluating on VQA val set:  38% 2574/6699 [30:16<49:09,  1.40it/s]Evaluating on VQA val set:  38% 2575/6699 [30:17<50:25,  1.36it/s]Evaluating on VQA val set:  38% 2576/6699 [30:18<48:04,  1.43it/s]Evaluating on VQA val set:  38% 2577/6699 [30:18<46:07,  1.49it/s]Evaluating on VQA val set:  38% 2578/6699 [30:19<46:28,  1.48it/s]Evaluating on VQA val set:  38% 2579/6699 [30:20<46:42,  1.47it/s]Evaluating on VQA val set:  39% 2580/6699 [30:20<48:13,  1.42it/s]Evaluating on VQA val set:  39% 2581/6699 [30:21<48:34,  1.41it/s]Evaluating on VQA val set:  39% 2582/6699 [30:22<47:07,  1.46it/s]Evaluating on VQA val set:  39% 2583/6699 [30:23<48:48,  1.41it/s]Evaluating on VQA val set:  39% 2584/6699 [30:23<49:04,  1.40it/s]Evaluating on VQA val set:  39% 2585/6699 [30:24<49:26,  1.39it/s]Evaluating on VQA val set:  39% 2586/6699 [30:25<48:12,  1.42it/s]Evaluating on VQA val set:  39% 2587/6699 [30:25<48:15,  1.42it/s]Evaluating on VQA val set:  39% 2588/6699 [30:26<47:58,  1.43it/s]Evaluating on VQA val set:  39% 2589/6699 [30:27<47:58,  1.43it/s]Evaluating on VQA val set:  39% 2590/6699 [30:27<46:57,  1.46it/s]Evaluating on VQA val set:  39% 2591/6699 [30:28<46:27,  1.47it/s]Evaluating on VQA val set:  39% 2592/6699 [30:29<47:38,  1.44it/s]Evaluating on VQA val set:  39% 2593/6699 [30:29<46:58,  1.46it/s]Evaluating on VQA val set:  39% 2594/6699 [30:30<46:13,  1.48it/s]Evaluating on VQA val set:  39% 2595/6699 [30:31<45:37,  1.50it/s]Evaluating on VQA val set:  39% 2596/6699 [30:31<46:25,  1.47it/s]Evaluating on VQA val set:  39% 2597/6699 [30:32<47:32,  1.44it/s]Evaluating on VQA val set:  39% 2598/6699 [30:33<48:19,  1.41it/s]Evaluating on VQA val set:  39% 2599/6699 [30:34<47:46,  1.43it/s]Evaluating on VQA val set:  39% 2600/6699 [30:34<48:05,  1.42it/s]Evaluating on VQA val set:  39% 2601/6699 [30:35<47:22,  1.44it/s]Evaluating on VQA val set:  39% 2602/6699 [30:36<48:39,  1.40it/s]Evaluating on VQA val set:  39% 2603/6699 [30:36<46:19,  1.47it/s]Evaluating on VQA val set:  39% 2604/6699 [30:37<47:39,  1.43it/s]Evaluating on VQA val set:  39% 2605/6699 [30:38<46:25,  1.47it/s]Evaluating on VQA val set:  39% 2606/6699 [30:38<46:55,  1.45it/s]Evaluating on VQA val set:  39% 2607/6699 [30:39<48:26,  1.41it/s]Evaluating on VQA val set:  39% 2608/6699 [30:40<48:30,  1.41it/s]Evaluating on VQA val set:  39% 2609/6699 [30:41<48:34,  1.40it/s]Evaluating on VQA val set:  39% 2610/6699 [30:41<48:22,  1.41it/s]Evaluating on VQA val set:  39% 2611/6699 [30:42<50:20,  1.35it/s]Evaluating on VQA val set:  39% 2612/6699 [30:43<49:03,  1.39it/s]Evaluating on VQA val set:  39% 2613/6699 [30:43<48:02,  1.42it/s]Evaluating on VQA val set:  39% 2614/6699 [30:44<48:12,  1.41it/s]Evaluating on VQA val set:  39% 2615/6699 [30:45<48:49,  1.39it/s]Evaluating on VQA val set:  39% 2616/6699 [30:46<47:37,  1.43it/s]Evaluating on VQA val set:  39% 2617/6699 [30:46<47:19,  1.44it/s]Evaluating on VQA val set:  39% 2618/6699 [30:47<47:11,  1.44it/s]Evaluating on VQA val set:  39% 2619/6699 [30:48<47:28,  1.43it/s]Evaluating on VQA val set:  39% 2620/6699 [30:48<49:18,  1.38it/s]Evaluating on VQA val set:  39% 2621/6699 [30:49<49:43,  1.37it/s]Evaluating on VQA val set:  39% 2622/6699 [30:50<50:01,  1.36it/s]Evaluating on VQA val set:  39% 2623/6699 [30:51<46:50,  1.45it/s]Evaluating on VQA val set:  39% 2624/6699 [30:51<47:55,  1.42it/s]Evaluating on VQA val set:  39% 2625/6699 [30:52<48:51,  1.39it/s]Evaluating on VQA val set:  39% 2626/6699 [30:53<48:33,  1.40it/s]Evaluating on VQA val set:  39% 2627/6699 [30:53<48:13,  1.41it/s]Evaluating on VQA val set:  39% 2628/6699 [30:54<48:45,  1.39it/s]Evaluating on VQA val set:  39% 2629/6699 [30:55<49:26,  1.37it/s]Evaluating on VQA val set:  39% 2630/6699 [30:56<49:33,  1.37it/s]Evaluating on VQA val set:  39% 2631/6699 [30:56<49:10,  1.38it/s]Evaluating on VQA val set:  39% 2632/6699 [30:57<48:17,  1.40it/s]Evaluating on VQA val set:  39% 2633/6699 [30:58<48:17,  1.40it/s]Evaluating on VQA val set:  39% 2634/6699 [30:58<47:51,  1.42it/s]Evaluating on VQA val set:  39% 2635/6699 [30:59<49:21,  1.37it/s]Evaluating on VQA val set:  39% 2636/6699 [31:00<49:11,  1.38it/s]Evaluating on VQA val set:  39% 2637/6699 [31:01<48:35,  1.39it/s]Evaluating on VQA val set:  39% 2638/6699 [31:01<49:56,  1.36it/s]Evaluating on VQA val set:  39% 2639/6699 [31:02<48:17,  1.40it/s]Evaluating on VQA val set:  39% 2640/6699 [31:03<48:34,  1.39it/s]Evaluating on VQA val set:  39% 2641/6699 [31:03<47:11,  1.43it/s]Evaluating on VQA val set:  39% 2642/6699 [31:04<46:51,  1.44it/s]Evaluating on VQA val set:  39% 2643/6699 [31:05<48:50,  1.38it/s]Evaluating on VQA val set:  39% 2644/6699 [31:06<46:55,  1.44it/s]Evaluating on VQA val set:  39% 2645/6699 [31:06<46:59,  1.44it/s]Evaluating on VQA val set:  39% 2646/6699 [31:07<47:19,  1.43it/s]Evaluating on VQA val set:  40% 2647/6699 [31:08<47:07,  1.43it/s]Evaluating on VQA val set:  40% 2648/6699 [31:08<47:30,  1.42it/s]Evaluating on VQA val set:  40% 2649/6699 [31:09<47:30,  1.42it/s]Evaluating on VQA val set:  40% 2650/6699 [31:10<47:35,  1.42it/s]Evaluating on VQA val set:  40% 2651/6699 [31:10<47:00,  1.44it/s]Evaluating on VQA val set:  40% 2652/6699 [31:11<46:14,  1.46it/s]Evaluating on VQA val set:  40% 2653/6699 [31:12<45:20,  1.49it/s]Evaluating on VQA val set:  40% 2654/6699 [31:12<44:10,  1.53it/s]Evaluating on VQA val set:  40% 2655/6699 [31:13<44:55,  1.50it/s]Evaluating on VQA val set:  40% 2656/6699 [31:14<46:54,  1.44it/s]Evaluating on VQA val set:  40% 2657/6699 [31:15<46:00,  1.46it/s]Evaluating on VQA val set:  40% 2658/6699 [31:15<44:34,  1.51it/s]Evaluating on VQA val set:  40% 2659/6699 [31:16<45:21,  1.48it/s]Evaluating on VQA val set:  40% 2660/6699 [31:16<45:22,  1.48it/s]Evaluating on VQA val set:  40% 2661/6699 [31:17<46:38,  1.44it/s]Evaluating on VQA val set:  40% 2662/6699 [31:18<48:05,  1.40it/s]Evaluating on VQA val set:  40% 2663/6699 [31:19<47:51,  1.41it/s]Evaluating on VQA val set:  40% 2664/6699 [31:19<48:47,  1.38it/s]Evaluating on VQA val set:  40% 2665/6699 [31:20<48:19,  1.39it/s]Evaluating on VQA val set:  40% 2666/6699 [31:21<48:02,  1.40it/s]Evaluating on VQA val set:  40% 2667/6699 [31:22<46:48,  1.44it/s]Evaluating on VQA val set:  40% 2668/6699 [31:22<47:08,  1.43it/s]Evaluating on VQA val set:  40% 2669/6699 [31:23<47:13,  1.42it/s]Evaluating on VQA val set:  40% 2670/6699 [31:24<47:47,  1.41it/s]Evaluating on VQA val set:  40% 2671/6699 [31:24<47:38,  1.41it/s]Evaluating on VQA val set:  40% 2672/6699 [31:25<48:09,  1.39it/s]Evaluating on VQA val set:  40% 2673/6699 [31:26<47:43,  1.41it/s]Evaluating on VQA val set:  40% 2674/6699 [31:27<48:08,  1.39it/s]Evaluating on VQA val set:  40% 2675/6699 [31:27<49:20,  1.36it/s]Evaluating on VQA val set:  40% 2676/6699 [31:28<48:29,  1.38it/s]Evaluating on VQA val set:  40% 2677/6699 [31:29<47:52,  1.40it/s]Evaluating on VQA val set:  40% 2678/6699 [31:29<47:22,  1.41it/s]Evaluating on VQA val set:  40% 2679/6699 [31:30<47:16,  1.42it/s]Evaluating on VQA val set:  40% 2680/6699 [31:31<46:31,  1.44it/s]Evaluating on VQA val set:  40% 2681/6699 [31:31<47:01,  1.42it/s]Evaluating on VQA val set:  40% 2682/6699 [31:32<47:15,  1.42it/s]Evaluating on VQA val set:  40% 2683/6699 [31:33<46:06,  1.45it/s]Evaluating on VQA val set:  40% 2684/6699 [31:33<44:47,  1.49it/s]Evaluating on VQA val set:  40% 2685/6699 [31:34<45:24,  1.47it/s]Evaluating on VQA val set:  40% 2686/6699 [31:35<46:48,  1.43it/s]Evaluating on VQA val set:  40% 2687/6699 [31:36<45:33,  1.47it/s]Evaluating on VQA val set:  40% 2688/6699 [31:36<46:37,  1.43it/s]Evaluating on VQA val set:  40% 2689/6699 [31:37<46:48,  1.43it/s]Evaluating on VQA val set:  40% 2690/6699 [31:38<47:16,  1.41it/s]Evaluating on VQA val set:  40% 2691/6699 [31:38<47:04,  1.42it/s]Evaluating on VQA val set:  40% 2692/6699 [31:39<48:17,  1.38it/s]Evaluating on VQA val set:  40% 2693/6699 [31:40<47:18,  1.41it/s]Evaluating on VQA val set:  40% 2694/6699 [31:41<47:43,  1.40it/s]Evaluating on VQA val set:  40% 2695/6699 [31:41<48:19,  1.38it/s]Evaluating on VQA val set:  40% 2696/6699 [31:42<47:13,  1.41it/s]Evaluating on VQA val set:  40% 2697/6699 [31:43<46:36,  1.43it/s]Evaluating on VQA val set:  40% 2698/6699 [31:43<46:43,  1.43it/s]Evaluating on VQA val set:  40% 2699/6699 [31:44<46:10,  1.44it/s]Evaluating on VQA val set:  40% 2700/6699 [31:45<47:29,  1.40it/s]Evaluating on VQA val set:  40% 2701/6699 [31:46<46:41,  1.43it/s]Evaluating on VQA val set:  40% 2702/6699 [31:46<46:23,  1.44it/s]Evaluating on VQA val set:  40% 2703/6699 [31:47<45:32,  1.46it/s]Evaluating on VQA val set:  40% 2704/6699 [31:48<46:12,  1.44it/s]Evaluating on VQA val set:  40% 2705/6699 [31:48<45:28,  1.46it/s]Evaluating on VQA val set:  40% 2706/6699 [31:49<46:04,  1.44it/s]Evaluating on VQA val set:  40% 2707/6699 [31:50<47:29,  1.40it/s]Evaluating on VQA val set:  40% 2708/6699 [31:50<47:43,  1.39it/s]Evaluating on VQA val set:  40% 2709/6699 [31:51<47:49,  1.39it/s]Evaluating on VQA val set:  40% 2710/6699 [31:52<46:53,  1.42it/s]Evaluating on VQA val set:  40% 2711/6699 [31:52<44:03,  1.51it/s]Evaluating on VQA val set:  40% 2712/6699 [31:53<44:13,  1.50it/s]Evaluating on VQA val set:  40% 2713/6699 [31:54<44:47,  1.48it/s]Evaluating on VQA val set:  41% 2714/6699 [31:54<46:03,  1.44it/s]Evaluating on VQA val set:  41% 2715/6699 [31:55<45:38,  1.46it/s]Evaluating on VQA val set:  41% 2716/6699 [31:56<45:25,  1.46it/s]Evaluating on VQA val set:  41% 2717/6699 [31:57<45:34,  1.46it/s]Evaluating on VQA val set:  41% 2718/6699 [31:57<45:44,  1.45it/s]Evaluating on VQA val set:  41% 2719/6699 [31:58<45:15,  1.47it/s]Evaluating on VQA val set:  41% 2720/6699 [31:59<46:25,  1.43it/s]Evaluating on VQA val set:  41% 2721/6699 [31:59<47:45,  1.39it/s]Evaluating on VQA val set:  41% 2722/6699 [32:00<47:10,  1.40it/s]Evaluating on VQA val set:  41% 2723/6699 [32:01<47:03,  1.41it/s]Evaluating on VQA val set:  41% 2724/6699 [32:02<48:00,  1.38it/s]Evaluating on VQA val set:  41% 2725/6699 [32:02<48:11,  1.37it/s]Evaluating on VQA val set:  41% 2726/6699 [32:03<47:09,  1.40it/s]Evaluating on VQA val set:  41% 2727/6699 [32:04<47:20,  1.40it/s]Evaluating on VQA val set:  41% 2728/6699 [32:04<46:56,  1.41it/s]Evaluating on VQA val set:  41% 2729/6699 [32:05<45:14,  1.46it/s]Evaluating on VQA val set:  41% 2730/6699 [32:06<44:12,  1.50it/s]Evaluating on VQA val set:  41% 2731/6699 [32:06<45:23,  1.46it/s]Evaluating on VQA val set:  41% 2732/6699 [32:07<46:28,  1.42it/s]Evaluating on VQA val set:  41% 2733/6699 [32:08<47:30,  1.39it/s]Evaluating on VQA val set:  41% 2734/6699 [32:09<46:10,  1.43it/s]Evaluating on VQA val set:  41% 2735/6699 [32:09<45:45,  1.44it/s]Evaluating on VQA val set:  41% 2736/6699 [32:10<46:18,  1.43it/s]Evaluating on VQA val set:  41% 2737/6699 [32:11<45:27,  1.45it/s]Evaluating on VQA val set:  41% 2738/6699 [32:11<45:32,  1.45it/s]Evaluating on VQA val set:  41% 2739/6699 [32:12<46:23,  1.42it/s]Evaluating on VQA val set:  41% 2740/6699 [32:13<45:45,  1.44it/s]Evaluating on VQA val set:  41% 2741/6699 [32:13<46:25,  1.42it/s]Evaluating on VQA val set:  41% 2742/6699 [32:14<46:58,  1.40it/s]Evaluating on VQA val set:  41% 2743/6699 [32:15<47:24,  1.39it/s]Evaluating on VQA val set:  41% 2744/6699 [32:16<47:19,  1.39it/s]Evaluating on VQA val set:  41% 2745/6699 [32:16<46:29,  1.42it/s]Evaluating on VQA val set:  41% 2746/6699 [32:17<47:33,  1.39it/s]Evaluating on VQA val set:  41% 2747/6699 [32:18<46:56,  1.40it/s]Evaluating on VQA val set:  41% 2748/6699 [32:18<46:28,  1.42it/s]Evaluating on VQA val set:  41% 2749/6699 [32:19<45:50,  1.44it/s]Evaluating on VQA val set:  41% 2750/6699 [32:20<45:40,  1.44it/s]Evaluating on VQA val set:  41% 2751/6699 [32:20<45:37,  1.44it/s]Evaluating on VQA val set:  41% 2752/6699 [32:21<43:38,  1.51it/s]Evaluating on VQA val set:  41% 2753/6699 [32:22<45:07,  1.46it/s]Evaluating on VQA val set:  41% 2754/6699 [32:23<45:44,  1.44it/s]Evaluating on VQA val set:  41% 2755/6699 [32:23<44:45,  1.47it/s]Evaluating on VQA val set:  41% 2756/6699 [32:24<45:38,  1.44it/s]Evaluating on VQA val set:  41% 2757/6699 [32:25<45:57,  1.43it/s]Evaluating on VQA val set:  41% 2758/6699 [32:25<46:57,  1.40it/s]Evaluating on VQA val set:  41% 2759/6699 [32:26<46:27,  1.41it/s]Evaluating on VQA val set:  41% 2760/6699 [32:27<44:46,  1.47it/s]Evaluating on VQA val set:  41% 2761/6699 [32:27<45:51,  1.43it/s]Evaluating on VQA val set:  41% 2762/6699 [32:28<45:28,  1.44it/s]Evaluating on VQA val set:  41% 2763/6699 [32:29<45:09,  1.45it/s]Evaluating on VQA val set:  41% 2764/6699 [32:30<47:02,  1.39it/s]Evaluating on VQA val set:  41% 2765/6699 [32:30<47:54,  1.37it/s]Evaluating on VQA val set:  41% 2766/6699 [32:31<47:09,  1.39it/s]Evaluating on VQA val set:  41% 2767/6699 [32:32<45:29,  1.44it/s]Evaluating on VQA val set:  41% 2768/6699 [32:32<46:17,  1.42it/s]Evaluating on VQA val set:  41% 2769/6699 [32:33<45:48,  1.43it/s]Evaluating on VQA val set:  41% 2770/6699 [32:34<45:40,  1.43it/s]Evaluating on VQA val set:  41% 2771/6699 [32:34<45:22,  1.44it/s]Evaluating on VQA val set:  41% 2772/6699 [32:35<45:06,  1.45it/s]Evaluating on VQA val set:  41% 2773/6699 [32:36<44:35,  1.47it/s]Evaluating on VQA val set:  41% 2774/6699 [32:36<42:17,  1.55it/s]Evaluating on VQA val set:  41% 2775/6699 [32:37<43:52,  1.49it/s]Evaluating on VQA val set:  41% 2776/6699 [32:38<45:22,  1.44it/s]Evaluating on VQA val set:  41% 2777/6699 [32:39<45:51,  1.43it/s]Evaluating on VQA val set:  41% 2778/6699 [32:39<46:06,  1.42it/s]Evaluating on VQA val set:  41% 2779/6699 [32:40<47:22,  1.38it/s]Evaluating on VQA val set:  41% 2780/6699 [32:41<46:30,  1.40it/s]Evaluating on VQA val set:  42% 2781/6699 [32:41<46:27,  1.41it/s]Evaluating on VQA val set:  42% 2782/6699 [32:42<47:11,  1.38it/s]Evaluating on VQA val set:  42% 2783/6699 [32:43<47:24,  1.38it/s]Evaluating on VQA val set:  42% 2784/6699 [32:44<47:52,  1.36it/s]Evaluating on VQA val set:  42% 2785/6699 [32:44<47:10,  1.38it/s]Evaluating on VQA val set:  42% 2786/6699 [32:45<44:54,  1.45it/s]Evaluating on VQA val set:  42% 2787/6699 [32:46<43:17,  1.51it/s]Evaluating on VQA val set:  42% 2788/6699 [32:46<45:15,  1.44it/s]Evaluating on VQA val set:  42% 2789/6699 [32:47<45:09,  1.44it/s]Evaluating on VQA val set:  42% 2790/6699 [32:48<44:46,  1.45it/s]Evaluating on VQA val set:  42% 2791/6699 [32:48<46:51,  1.39it/s]Evaluating on VQA val set:  42% 2792/6699 [32:49<46:13,  1.41it/s]Evaluating on VQA val set:  42% 2793/6699 [32:50<46:44,  1.39it/s]Evaluating on VQA val set:  42% 2794/6699 [32:51<46:20,  1.40it/s]Evaluating on VQA val set:  42% 2795/6699 [32:51<46:27,  1.40it/s]Evaluating on VQA val set:  42% 2796/6699 [32:52<47:11,  1.38it/s]Evaluating on VQA val set:  42% 2797/6699 [32:53<47:59,  1.36it/s]Evaluating on VQA val set:  42% 2798/6699 [32:54<47:41,  1.36it/s]Evaluating on VQA val set:  42% 2799/6699 [32:54<47:36,  1.37it/s]Evaluating on VQA val set:  42% 2800/6699 [32:55<46:58,  1.38it/s]Evaluating on VQA val set:  42% 2801/6699 [32:56<47:01,  1.38it/s]Evaluating on VQA val set:  42% 2802/6699 [32:56<46:14,  1.40it/s]Evaluating on VQA val set:  42% 2803/6699 [32:57<46:21,  1.40it/s]Evaluating on VQA val set:  42% 2804/6699 [32:58<47:19,  1.37it/s]Evaluating on VQA val set:  42% 2805/6699 [32:59<46:34,  1.39it/s]Evaluating on VQA val set:  42% 2806/6699 [32:59<46:44,  1.39it/s]Evaluating on VQA val set:  42% 2807/6699 [33:00<45:55,  1.41it/s]Evaluating on VQA val set:  42% 2808/6699 [33:01<46:07,  1.41it/s]Evaluating on VQA val set:  42% 2809/6699 [33:01<45:49,  1.42it/s]Evaluating on VQA val set:  42% 2810/6699 [33:02<46:34,  1.39it/s]Evaluating on VQA val set:  42% 2811/6699 [33:03<47:17,  1.37it/s]Evaluating on VQA val set:  42% 2812/6699 [33:04<47:21,  1.37it/s]Evaluating on VQA val set:  42% 2813/6699 [33:04<47:38,  1.36it/s]Evaluating on VQA val set:  42% 2814/6699 [33:05<47:43,  1.36it/s]Evaluating on VQA val set:  42% 2815/6699 [33:06<46:59,  1.38it/s]Evaluating on VQA val set:  42% 2816/6699 [33:07<47:36,  1.36it/s]Evaluating on VQA val set:  42% 2817/6699 [33:07<48:24,  1.34it/s]Evaluating on VQA val set:  42% 2818/6699 [33:08<47:02,  1.38it/s]Evaluating on VQA val set:  42% 2819/6699 [33:09<47:28,  1.36it/s]Evaluating on VQA val set:  42% 2820/6699 [33:10<47:24,  1.36it/s]Evaluating on VQA val set:  42% 2821/6699 [33:10<46:19,  1.40it/s]Evaluating on VQA val set:  42% 2822/6699 [33:11<45:20,  1.43it/s]Evaluating on VQA val set:  42% 2823/6699 [33:12<44:17,  1.46it/s]Evaluating on VQA val set:  42% 2824/6699 [33:12<43:19,  1.49it/s]Evaluating on VQA val set:  42% 2825/6699 [33:13<44:14,  1.46it/s]Evaluating on VQA val set:  42% 2826/6699 [33:14<45:07,  1.43it/s]Evaluating on VQA val set:  42% 2827/6699 [33:14<45:42,  1.41it/s]Evaluating on VQA val set:  42% 2828/6699 [33:15<45:35,  1.42it/s]Evaluating on VQA val set:  42% 2829/6699 [33:16<44:29,  1.45it/s]Evaluating on VQA val set:  42% 2830/6699 [33:16<44:44,  1.44it/s]Evaluating on VQA val set:  42% 2831/6699 [33:17<44:41,  1.44it/s]Evaluating on VQA val set:  42% 2832/6699 [33:18<45:21,  1.42it/s]Evaluating on VQA val set:  42% 2833/6699 [33:18<44:54,  1.43it/s]Evaluating on VQA val set:  42% 2834/6699 [33:19<44:24,  1.45it/s]Evaluating on VQA val set:  42% 2835/6699 [33:20<44:45,  1.44it/s]Evaluating on VQA val set:  42% 2836/6699 [33:21<45:00,  1.43it/s]Evaluating on VQA val set:  42% 2837/6699 [33:21<45:24,  1.42it/s]Evaluating on VQA val set:  42% 2838/6699 [33:22<44:41,  1.44it/s]Evaluating on VQA val set:  42% 2839/6699 [33:23<44:50,  1.43it/s]Evaluating on VQA val set:  42% 2840/6699 [33:23<45:56,  1.40it/s]Evaluating on VQA val set:  42% 2841/6699 [33:24<45:55,  1.40it/s]Evaluating on VQA val set:  42% 2842/6699 [33:25<46:54,  1.37it/s]Evaluating on VQA val set:  42% 2843/6699 [33:26<46:36,  1.38it/s]Evaluating on VQA val set:  42% 2844/6699 [33:26<46:06,  1.39it/s]Evaluating on VQA val set:  42% 2845/6699 [33:27<46:37,  1.38it/s]Evaluating on VQA val set:  42% 2846/6699 [33:28<46:39,  1.38it/s]Evaluating on VQA val set:  42% 2847/6699 [33:28<44:10,  1.45it/s]Evaluating on VQA val set:  43% 2848/6699 [33:29<43:47,  1.47it/s]Evaluating on VQA val set:  43% 2849/6699 [33:30<43:08,  1.49it/s]Evaluating on VQA val set:  43% 2850/6699 [33:30<42:53,  1.50it/s]Evaluating on VQA val set:  43% 2851/6699 [33:31<42:49,  1.50it/s]Evaluating on VQA val set:  43% 2852/6699 [33:32<44:28,  1.44it/s]Evaluating on VQA val set:  43% 2853/6699 [33:32<44:41,  1.43it/s]Evaluating on VQA val set:  43% 2854/6699 [33:33<43:21,  1.48it/s]Evaluating on VQA val set:  43% 2855/6699 [33:34<43:42,  1.47it/s]Evaluating on VQA val set:  43% 2856/6699 [33:35<44:08,  1.45it/s]Evaluating on VQA val set:  43% 2857/6699 [33:35<45:42,  1.40it/s]Evaluating on VQA val set:  43% 2858/6699 [33:36<46:07,  1.39it/s]Evaluating on VQA val set:  43% 2859/6699 [33:37<46:43,  1.37it/s]Evaluating on VQA val set:  43% 2860/6699 [33:38<46:55,  1.36it/s]Evaluating on VQA val set:  43% 2861/6699 [33:38<45:30,  1.41it/s]Evaluating on VQA val set:  43% 2862/6699 [33:39<45:35,  1.40it/s]Evaluating on VQA val set:  43% 2863/6699 [33:40<44:50,  1.43it/s]Evaluating on VQA val set:  43% 2864/6699 [33:40<43:53,  1.46it/s]Evaluating on VQA val set:  43% 2865/6699 [33:41<44:55,  1.42it/s]Evaluating on VQA val set:  43% 2866/6699 [33:42<46:24,  1.38it/s]Evaluating on VQA val set:  43% 2867/6699 [33:43<47:20,  1.35it/s]Evaluating on VQA val set:  43% 2868/6699 [33:43<47:11,  1.35it/s]Evaluating on VQA val set:  43% 2869/6699 [33:44<46:58,  1.36it/s]Evaluating on VQA val set:  43% 2870/6699 [33:45<44:47,  1.42it/s]Evaluating on VQA val set:  43% 2871/6699 [33:45<45:52,  1.39it/s]Evaluating on VQA val set:  43% 2872/6699 [33:46<45:25,  1.40it/s]Evaluating on VQA val set:  43% 2873/6699 [33:47<45:06,  1.41it/s]Evaluating on VQA val set:  43% 2874/6699 [33:47<45:25,  1.40it/s]Evaluating on VQA val set:  43% 2875/6699 [33:48<45:21,  1.41it/s]Evaluating on VQA val set:  43% 2876/6699 [33:49<46:26,  1.37it/s]Evaluating on VQA val set:  43% 2877/6699 [33:50<46:20,  1.37it/s]Evaluating on VQA val set:  43% 2878/6699 [33:50<46:29,  1.37it/s]Evaluating on VQA val set:  43% 2879/6699 [33:51<46:25,  1.37it/s]Evaluating on VQA val set:  43% 2880/6699 [33:52<45:26,  1.40it/s]Evaluating on VQA val set:  43% 2881/6699 [33:53<46:15,  1.38it/s]Evaluating on VQA val set:  43% 2882/6699 [33:53<45:22,  1.40it/s]Evaluating on VQA val set:  43% 2883/6699 [33:54<45:26,  1.40it/s]Evaluating on VQA val set:  43% 2884/6699 [33:55<44:53,  1.42it/s]Evaluating on VQA val set:  43% 2885/6699 [33:55<44:28,  1.43it/s]Evaluating on VQA val set:  43% 2886/6699 [33:56<43:48,  1.45it/s]Evaluating on VQA val set:  43% 2887/6699 [33:57<43:14,  1.47it/s]Evaluating on VQA val set:  43% 2888/6699 [33:57<41:56,  1.51it/s]Evaluating on VQA val set:  43% 2889/6699 [33:58<42:05,  1.51it/s]Evaluating on VQA val set:  43% 2890/6699 [33:59<43:41,  1.45it/s]Evaluating on VQA val set:  43% 2891/6699 [33:59<43:51,  1.45it/s]Evaluating on VQA val set:  43% 2892/6699 [34:00<45:07,  1.41it/s]Evaluating on VQA val set:  43% 2893/6699 [34:01<46:29,  1.36it/s]Evaluating on VQA val set:  43% 2894/6699 [34:02<46:26,  1.37it/s]Evaluating on VQA val set:  43% 2895/6699 [34:02<45:22,  1.40it/s]Evaluating on VQA val set:  43% 2896/6699 [34:03<45:23,  1.40it/s]Evaluating on VQA val set:  43% 2897/6699 [34:04<44:27,  1.43it/s]Evaluating on VQA val set:  43% 2898/6699 [34:04<44:01,  1.44it/s]Evaluating on VQA val set:  43% 2899/6699 [34:05<43:15,  1.46it/s]Evaluating on VQA val set:  43% 2900/6699 [34:06<42:35,  1.49it/s]Evaluating on VQA val set:  43% 2901/6699 [34:06<44:16,  1.43it/s]Evaluating on VQA val set:  43% 2902/6699 [34:07<43:59,  1.44it/s]Evaluating on VQA val set:  43% 2903/6699 [34:08<42:49,  1.48it/s]Evaluating on VQA val set:  43% 2904/6699 [34:09<43:44,  1.45it/s]Evaluating on VQA val set:  43% 2905/6699 [34:09<45:04,  1.40it/s]Evaluating on VQA val set:  43% 2906/6699 [34:10<44:34,  1.42it/s]Evaluating on VQA val set:  43% 2907/6699 [34:11<45:09,  1.40it/s]Evaluating on VQA val set:  43% 2908/6699 [34:11<45:44,  1.38it/s]Evaluating on VQA val set:  43% 2909/6699 [34:12<46:14,  1.37it/s]Evaluating on VQA val set:  43% 2910/6699 [34:13<45:11,  1.40it/s]Evaluating on VQA val set:  43% 2911/6699 [34:14<44:03,  1.43it/s]Evaluating on VQA val set:  43% 2912/6699 [34:14<44:06,  1.43it/s]Evaluating on VQA val set:  43% 2913/6699 [34:15<42:47,  1.47it/s]Evaluating on VQA val set:  43% 2914/6699 [34:16<43:09,  1.46it/s]Evaluating on VQA val set:  44% 2915/6699 [34:16<44:05,  1.43it/s]Evaluating on VQA val set:  44% 2916/6699 [34:17<44:37,  1.41it/s]Evaluating on VQA val set:  44% 2917/6699 [34:18<46:00,  1.37it/s]Evaluating on VQA val set:  44% 2918/6699 [34:19<47:09,  1.34it/s]Evaluating on VQA val set:  44% 2919/6699 [34:19<46:18,  1.36it/s]Evaluating on VQA val set:  44% 2920/6699 [34:20<44:57,  1.40it/s]Evaluating on VQA val set:  44% 2921/6699 [34:21<43:43,  1.44it/s]Evaluating on VQA val set:  44% 2922/6699 [34:21<44:12,  1.42it/s]Evaluating on VQA val set:  44% 2923/6699 [34:22<44:33,  1.41it/s]Evaluating on VQA val set:  44% 2924/6699 [34:23<44:20,  1.42it/s]Evaluating on VQA val set:  44% 2925/6699 [34:23<44:19,  1.42it/s]Evaluating on VQA val set:  44% 2926/6699 [34:24<44:51,  1.40it/s]Evaluating on VQA val set:  44% 2927/6699 [34:25<44:13,  1.42it/s]Evaluating on VQA val set:  44% 2928/6699 [34:26<44:56,  1.40it/s]Evaluating on VQA val set:  44% 2929/6699 [34:26<44:46,  1.40it/s]Evaluating on VQA val set:  44% 2930/6699 [34:27<44:41,  1.41it/s]Evaluating on VQA val set:  44% 2931/6699 [34:28<45:25,  1.38it/s]Evaluating on VQA val set:  44% 2932/6699 [34:29<46:21,  1.35it/s]Evaluating on VQA val set:  44% 2933/6699 [34:29<46:07,  1.36it/s]Evaluating on VQA val set:  44% 2934/6699 [34:30<46:10,  1.36it/s]Evaluating on VQA val set:  44% 2935/6699 [34:31<46:21,  1.35it/s]Evaluating on VQA val set:  44% 2936/6699 [34:32<46:07,  1.36it/s]Evaluating on VQA val set:  44% 2937/6699 [34:32<46:43,  1.34it/s]Evaluating on VQA val set:  44% 2938/6699 [34:33<46:03,  1.36it/s]Evaluating on VQA val set:  44% 2939/6699 [34:34<44:41,  1.40it/s]Evaluating on VQA val set:  44% 2940/6699 [34:34<45:32,  1.38it/s]Evaluating on VQA val set:  44% 2941/6699 [34:35<45:04,  1.39it/s]Evaluating on VQA val set:  44% 2942/6699 [34:36<42:44,  1.46it/s]Evaluating on VQA val set:  44% 2943/6699 [34:36<42:42,  1.47it/s]Evaluating on VQA val set:  44% 2944/6699 [34:37<42:02,  1.49it/s]Evaluating on VQA val set:  44% 2945/6699 [34:38<42:03,  1.49it/s]Evaluating on VQA val set:  44% 2946/6699 [34:38<42:54,  1.46it/s]Evaluating on VQA val set:  44% 2947/6699 [34:39<43:53,  1.42it/s]Evaluating on VQA val set:  44% 2948/6699 [34:40<43:32,  1.44it/s]Evaluating on VQA val set:  44% 2949/6699 [34:41<45:47,  1.36it/s]Evaluating on VQA val set:  44% 2950/6699 [34:41<46:33,  1.34it/s]Evaluating on VQA val set:  44% 2951/6699 [34:42<46:41,  1.34it/s]Evaluating on VQA val set:  44% 2952/6699 [34:43<44:24,  1.41it/s]Evaluating on VQA val set:  44% 2953/6699 [34:44<44:59,  1.39it/s]Evaluating on VQA val set:  44% 2954/6699 [34:44<44:56,  1.39it/s]Evaluating on VQA val set:  44% 2955/6699 [34:45<45:03,  1.38it/s]Evaluating on VQA val set:  44% 2956/6699 [34:46<44:22,  1.41it/s]Evaluating on VQA val set:  44% 2957/6699 [34:46<44:41,  1.40it/s]Evaluating on VQA val set:  44% 2958/6699 [34:47<44:45,  1.39it/s]Evaluating on VQA val set:  44% 2959/6699 [34:48<44:31,  1.40it/s]Evaluating on VQA val set:  44% 2960/6699 [34:49<44:07,  1.41it/s]Evaluating on VQA val set:  44% 2961/6699 [34:49<43:07,  1.44it/s]Evaluating on VQA val set:  44% 2962/6699 [34:50<43:17,  1.44it/s]Evaluating on VQA val set:  44% 2963/6699 [34:51<43:48,  1.42it/s]Evaluating on VQA val set:  44% 2964/6699 [34:51<44:03,  1.41it/s]Evaluating on VQA val set:  44% 2965/6699 [34:52<44:01,  1.41it/s]Evaluating on VQA val set:  44% 2966/6699 [34:53<43:28,  1.43it/s]Evaluating on VQA val set:  44% 2967/6699 [34:53<43:46,  1.42it/s]Evaluating on VQA val set:  44% 2968/6699 [34:54<43:09,  1.44it/s]Evaluating on VQA val set:  44% 2969/6699 [34:55<43:05,  1.44it/s]Evaluating on VQA val set:  44% 2970/6699 [34:56<43:39,  1.42it/s]Evaluating on VQA val set:  44% 2971/6699 [34:56<43:52,  1.42it/s]Evaluating on VQA val set:  44% 2972/6699 [34:57<41:48,  1.49it/s]Evaluating on VQA val set:  44% 2973/6699 [34:58<43:06,  1.44it/s]Evaluating on VQA val set:  44% 2974/6699 [34:58<42:05,  1.48it/s]Evaluating on VQA val set:  44% 2975/6699 [34:59<43:21,  1.43it/s]Evaluating on VQA val set:  44% 2976/6699 [35:00<43:56,  1.41it/s]Evaluating on VQA val set:  44% 2977/6699 [35:00<45:17,  1.37it/s]Evaluating on VQA val set:  44% 2978/6699 [35:01<44:58,  1.38it/s]Evaluating on VQA val set:  44% 2979/6699 [35:02<44:21,  1.40it/s]Evaluating on VQA val set:  44% 2980/6699 [35:03<45:48,  1.35it/s]Evaluating on VQA val set:  44% 2981/6699 [35:03<46:07,  1.34it/s]Evaluating on VQA val set:  45% 2982/6699 [35:04<46:33,  1.33it/s]Evaluating on VQA val set:  45% 2983/6699 [35:05<45:34,  1.36it/s]Evaluating on VQA val set:  45% 2984/6699 [35:06<44:02,  1.41it/s]Evaluating on VQA val set:  45% 2985/6699 [35:06<42:56,  1.44it/s]Evaluating on VQA val set:  45% 2986/6699 [35:07<44:02,  1.41it/s]Evaluating on VQA val set:  45% 2987/6699 [35:08<45:02,  1.37it/s]Evaluating on VQA val set:  45% 2988/6699 [35:08<45:19,  1.36it/s]Evaluating on VQA val set:  45% 2989/6699 [35:09<44:33,  1.39it/s]Evaluating on VQA val set:  45% 2990/6699 [35:10<44:44,  1.38it/s]Evaluating on VQA val set:  45% 2991/6699 [35:11<44:06,  1.40it/s]Evaluating on VQA val set:  45% 2992/6699 [35:11<44:20,  1.39it/s]Evaluating on VQA val set:  45% 2993/6699 [35:12<44:22,  1.39it/s]Evaluating on VQA val set:  45% 2994/6699 [35:13<44:47,  1.38it/s]Evaluating on VQA val set:  45% 2995/6699 [35:13<42:56,  1.44it/s]Evaluating on VQA val set:  45% 2996/6699 [35:14<43:28,  1.42it/s]Evaluating on VQA val set:  45% 2997/6699 [35:15<44:00,  1.40it/s]Evaluating on VQA val set:  45% 2998/6699 [35:16<43:30,  1.42it/s]Evaluating on VQA val set:  45% 2999/6699 [35:16<43:23,  1.42it/s]Evaluating on VQA val set:  45% 3000/6699 [35:17<43:41,  1.41it/s]Evaluating on VQA val set:  45% 3001/6699 [35:18<44:06,  1.40it/s]Evaluating on VQA val set:  45% 3002/6699 [35:18<43:23,  1.42it/s]Evaluating on VQA val set:  45% 3003/6699 [35:19<42:22,  1.45it/s]Evaluating on VQA val set:  45% 3004/6699 [35:20<42:34,  1.45it/s]Evaluating on VQA val set:  45% 3005/6699 [35:20<42:26,  1.45it/s]Evaluating on VQA val set:  45% 3006/6699 [35:21<43:50,  1.40it/s]Evaluating on VQA val set:  45% 3007/6699 [35:22<44:24,  1.39it/s]Evaluating on VQA val set:  45% 3008/6699 [35:23<43:13,  1.42it/s]Evaluating on VQA val set:  45% 3009/6699 [35:23<43:52,  1.40it/s]Evaluating on VQA val set:  45% 3010/6699 [35:24<42:53,  1.43it/s]Evaluating on VQA val set:  45% 3011/6699 [35:25<41:30,  1.48it/s]Evaluating on VQA val set:  45% 3012/6699 [35:25<41:13,  1.49it/s]Evaluating on VQA val set:  45% 3013/6699 [35:26<41:18,  1.49it/s]Evaluating on VQA val set:  45% 3014/6699 [35:27<41:29,  1.48it/s]Evaluating on VQA val set:  45% 3015/6699 [35:27<40:55,  1.50it/s]Evaluating on VQA val set:  45% 3016/6699 [35:28<41:03,  1.49it/s]Evaluating on VQA val set:  45% 3017/6699 [35:29<42:33,  1.44it/s]Evaluating on VQA val set:  45% 3018/6699 [35:29<42:53,  1.43it/s]Evaluating on VQA val set:  45% 3019/6699 [35:30<42:15,  1.45it/s]Evaluating on VQA val set:  45% 3020/6699 [35:31<42:31,  1.44it/s]Evaluating on VQA val set:  45% 3021/6699 [35:31<42:32,  1.44it/s]Evaluating on VQA val set:  45% 3022/6699 [35:32<42:53,  1.43it/s]Evaluating on VQA val set:  45% 3023/6699 [35:33<42:20,  1.45it/s]Evaluating on VQA val set:  45% 3024/6699 [35:34<43:27,  1.41it/s]Evaluating on VQA val set:  45% 3025/6699 [35:34<44:21,  1.38it/s]Evaluating on VQA val set:  45% 3026/6699 [35:35<43:28,  1.41it/s]Evaluating on VQA val set:  45% 3027/6699 [35:36<43:14,  1.42it/s]Evaluating on VQA val set:  45% 3028/6699 [35:36<42:39,  1.43it/s]Evaluating on VQA val set:  45% 3029/6699 [35:37<43:06,  1.42it/s]Evaluating on VQA val set:  45% 3030/6699 [35:38<44:28,  1.37it/s]Evaluating on VQA val set:  45% 3031/6699 [35:39<45:41,  1.34it/s]Evaluating on VQA val set:  45% 3032/6699 [35:39<45:45,  1.34it/s]Evaluating on VQA val set:  45% 3033/6699 [35:40<45:58,  1.33it/s]Evaluating on VQA val set:  45% 3034/6699 [35:41<45:56,  1.33it/s]Evaluating on VQA val set:  45% 3035/6699 [35:42<45:14,  1.35it/s]Evaluating on VQA val set:  45% 3036/6699 [35:42<44:01,  1.39it/s]Evaluating on VQA val set:  45% 3037/6699 [35:43<42:16,  1.44it/s]Evaluating on VQA val set:  45% 3038/6699 [35:44<42:03,  1.45it/s]Evaluating on VQA val set:  45% 3039/6699 [35:44<42:44,  1.43it/s]Evaluating on VQA val set:  45% 3040/6699 [35:45<41:39,  1.46it/s]Evaluating on VQA val set:  45% 3041/6699 [35:46<41:46,  1.46it/s]Evaluating on VQA val set:  45% 3042/6699 [35:46<41:37,  1.46it/s]Evaluating on VQA val set:  45% 3043/6699 [35:47<42:28,  1.43it/s]Evaluating on VQA val set:  45% 3044/6699 [35:48<41:09,  1.48it/s]Evaluating on VQA val set:  45% 3045/6699 [35:48<40:28,  1.50it/s]Evaluating on VQA val set:  45% 3046/6699 [35:49<40:11,  1.51it/s]Evaluating on VQA val set:  45% 3047/6699 [35:50<42:07,  1.44it/s]Evaluating on VQA val set:  45% 3048/6699 [35:51<41:54,  1.45it/s]Evaluating on VQA val set:  46% 3049/6699 [35:51<43:28,  1.40it/s]Evaluating on VQA val set:  46% 3050/6699 [35:52<43:37,  1.39it/s]Evaluating on VQA val set:  46% 3051/6699 [35:53<43:03,  1.41it/s]Evaluating on VQA val set:  46% 3052/6699 [35:53<43:21,  1.40it/s]Evaluating on VQA val set:  46% 3053/6699 [35:54<42:46,  1.42it/s]Evaluating on VQA val set:  46% 3054/6699 [35:55<43:27,  1.40it/s]Evaluating on VQA val set:  46% 3055/6699 [35:56<42:57,  1.41it/s]Evaluating on VQA val set:  46% 3056/6699 [35:56<41:16,  1.47it/s]Evaluating on VQA val set:  46% 3057/6699 [35:57<41:20,  1.47it/s]Evaluating on VQA val set:  46% 3058/6699 [35:57<41:03,  1.48it/s]Evaluating on VQA val set:  46% 3059/6699 [35:58<40:40,  1.49it/s]Evaluating on VQA val set:  46% 3060/6699 [35:59<41:12,  1.47it/s]Evaluating on VQA val set:  46% 3061/6699 [36:00<43:04,  1.41it/s]Evaluating on VQA val set:  46% 3062/6699 [36:00<43:52,  1.38it/s]Evaluating on VQA val set:  46% 3063/6699 [36:01<43:17,  1.40it/s]Evaluating on VQA val set:  46% 3064/6699 [36:02<42:55,  1.41it/s]Evaluating on VQA val set:  46% 3065/6699 [36:03<43:18,  1.40it/s]Evaluating on VQA val set:  46% 3066/6699 [36:03<42:32,  1.42it/s]Evaluating on VQA val set:  46% 3067/6699 [36:04<43:41,  1.39it/s]Evaluating on VQA val set:  46% 3068/6699 [36:05<43:14,  1.40it/s]Evaluating on VQA val set:  46% 3069/6699 [36:05<43:10,  1.40it/s]Evaluating on VQA val set:  46% 3070/6699 [36:06<42:34,  1.42it/s]Evaluating on VQA val set:  46% 3071/6699 [36:07<43:22,  1.39it/s]Evaluating on VQA val set:  46% 3072/6699 [36:08<43:42,  1.38it/s]Evaluating on VQA val set:  46% 3073/6699 [36:08<44:18,  1.36it/s]Evaluating on VQA val set:  46% 3074/6699 [36:09<43:08,  1.40it/s]Evaluating on VQA val set:  46% 3075/6699 [36:10<42:43,  1.41it/s]Evaluating on VQA val set:  46% 3076/6699 [36:10<41:42,  1.45it/s]Evaluating on VQA val set:  46% 3077/6699 [36:11<40:45,  1.48it/s]Evaluating on VQA val set:  46% 3078/6699 [36:12<42:08,  1.43it/s]Evaluating on VQA val set:  46% 3079/6699 [36:12<43:52,  1.37it/s]Evaluating on VQA val set:  46% 3080/6699 [36:13<43:32,  1.39it/s]Evaluating on VQA val set:  46% 3081/6699 [36:14<42:49,  1.41it/s]Evaluating on VQA val set:  46% 3082/6699 [36:15<42:25,  1.42it/s]Evaluating on VQA val set:  46% 3083/6699 [36:15<42:32,  1.42it/s]Evaluating on VQA val set:  46% 3084/6699 [36:16<42:21,  1.42it/s]Evaluating on VQA val set:  46% 3085/6699 [36:17<42:54,  1.40it/s]Evaluating on VQA val set:  46% 3086/6699 [36:17<43:56,  1.37it/s]Evaluating on VQA val set:  46% 3087/6699 [36:18<43:37,  1.38it/s]Evaluating on VQA val set:  46% 3088/6699 [36:19<43:07,  1.40it/s]Evaluating on VQA val set:  46% 3089/6699 [36:20<42:25,  1.42it/s]Evaluating on VQA val set:  46% 3090/6699 [36:20<39:55,  1.51it/s]Evaluating on VQA val set:  46% 3091/6699 [36:21<41:37,  1.44it/s]Evaluating on VQA val set:  46% 3092/6699 [36:22<41:40,  1.44it/s]Evaluating on VQA val set:  46% 3093/6699 [36:22<41:12,  1.46it/s]Evaluating on VQA val set:  46% 3094/6699 [36:23<39:57,  1.50it/s]Evaluating on VQA val set:  46% 3095/6699 [36:24<39:47,  1.51it/s]Evaluating on VQA val set:  46% 3096/6699 [36:24<40:18,  1.49it/s]Evaluating on VQA val set:  46% 3097/6699 [36:25<41:13,  1.46it/s]Evaluating on VQA val set:  46% 3098/6699 [36:26<41:02,  1.46it/s]Evaluating on VQA val set:  46% 3099/6699 [36:26<41:38,  1.44it/s]Evaluating on VQA val set:  46% 3100/6699 [36:27<41:55,  1.43it/s]Evaluating on VQA val set:  46% 3101/6699 [36:28<43:10,  1.39it/s]Evaluating on VQA val set:  46% 3102/6699 [36:28<42:10,  1.42it/s]Evaluating on VQA val set:  46% 3103/6699 [36:29<42:38,  1.41it/s]Evaluating on VQA val set:  46% 3104/6699 [36:30<42:19,  1.42it/s]Evaluating on VQA val set:  46% 3105/6699 [36:31<43:21,  1.38it/s]Evaluating on VQA val set:  46% 3106/6699 [36:31<42:59,  1.39it/s]Evaluating on VQA val set:  46% 3107/6699 [36:32<42:09,  1.42it/s]Evaluating on VQA val set:  46% 3108/6699 [36:33<41:24,  1.45it/s]Evaluating on VQA val set:  46% 3109/6699 [36:33<41:19,  1.45it/s]Evaluating on VQA val set:  46% 3110/6699 [36:34<40:11,  1.49it/s]Evaluating on VQA val set:  46% 3111/6699 [36:35<40:01,  1.49it/s]Evaluating on VQA val set:  46% 3112/6699 [36:35<41:03,  1.46it/s]Evaluating on VQA val set:  46% 3113/6699 [36:36<40:15,  1.48it/s]Evaluating on VQA val set:  46% 3114/6699 [36:37<40:24,  1.48it/s]Evaluating on VQA val set:  46% 3115/6699 [36:37<40:36,  1.47it/s]Evaluating on VQA val set:  47% 3116/6699 [36:38<40:11,  1.49it/s]Evaluating on VQA val set:  47% 3117/6699 [36:39<40:31,  1.47it/s]Evaluating on VQA val set:  47% 3118/6699 [36:40<42:26,  1.41it/s]Evaluating on VQA val set:  47% 3119/6699 [36:40<41:53,  1.42it/s]Evaluating on VQA val set:  47% 3120/6699 [36:41<42:42,  1.40it/s]Evaluating on VQA val set:  47% 3121/6699 [36:42<43:42,  1.36it/s]Evaluating on VQA val set:  47% 3122/6699 [36:42<42:32,  1.40it/s]Evaluating on VQA val set:  47% 3123/6699 [36:43<42:05,  1.42it/s]Evaluating on VQA val set:  47% 3124/6699 [36:44<41:38,  1.43it/s]Evaluating on VQA val set:  47% 3125/6699 [36:44<41:37,  1.43it/s]Evaluating on VQA val set:  47% 3126/6699 [36:45<42:15,  1.41it/s]Evaluating on VQA val set:  47% 3127/6699 [36:46<42:45,  1.39it/s]Evaluating on VQA val set:  47% 3128/6699 [36:47<42:09,  1.41it/s]Evaluating on VQA val set:  47% 3129/6699 [36:47<42:36,  1.40it/s]Evaluating on VQA val set:  47% 3130/6699 [36:48<43:22,  1.37it/s]Evaluating on VQA val set:  47% 3131/6699 [36:49<43:14,  1.38it/s]Evaluating on VQA val set:  47% 3132/6699 [36:50<42:43,  1.39it/s]Evaluating on VQA val set:  47% 3133/6699 [36:50<42:42,  1.39it/s]Evaluating on VQA val set:  47% 3134/6699 [36:51<41:56,  1.42it/s]Evaluating on VQA val set:  47% 3135/6699 [36:52<41:03,  1.45it/s]Evaluating on VQA val set:  47% 3136/6699 [36:52<42:04,  1.41it/s]Evaluating on VQA val set:  47% 3137/6699 [36:53<42:17,  1.40it/s]Evaluating on VQA val set:  47% 3138/6699 [36:54<42:55,  1.38it/s]Evaluating on VQA val set:  47% 3139/6699 [36:55<42:36,  1.39it/s]Evaluating on VQA val set:  47% 3140/6699 [36:55<41:34,  1.43it/s]Evaluating on VQA val set:  47% 3141/6699 [36:56<42:22,  1.40it/s]Evaluating on VQA val set:  47% 3142/6699 [36:57<42:11,  1.41it/s]Evaluating on VQA val set:  47% 3143/6699 [36:57<42:13,  1.40it/s]Evaluating on VQA val set:  47% 3144/6699 [36:58<41:01,  1.44it/s]Evaluating on VQA val set:  47% 3145/6699 [36:59<40:33,  1.46it/s]Evaluating on VQA val set:  47% 3146/6699 [36:59<40:11,  1.47it/s]Evaluating on VQA val set:  47% 3147/6699 [37:00<39:47,  1.49it/s]Evaluating on VQA val set:  47% 3148/6699 [37:01<40:39,  1.46it/s]Evaluating on VQA val set:  47% 3149/6699 [37:01<41:50,  1.41it/s]Evaluating on VQA val set:  47% 3150/6699 [37:02<41:49,  1.41it/s]Evaluating on VQA val set:  47% 3151/6699 [37:03<42:21,  1.40it/s]Evaluating on VQA val set:  47% 3152/6699 [37:04<42:37,  1.39it/s]Evaluating on VQA val set:  47% 3153/6699 [37:04<43:12,  1.37it/s]Evaluating on VQA val set:  47% 3154/6699 [37:05<42:35,  1.39it/s]Evaluating on VQA val set:  47% 3155/6699 [37:06<42:22,  1.39it/s]Evaluating on VQA val set:  47% 3156/6699 [37:07<41:43,  1.42it/s]Evaluating on VQA val set:  47% 3157/6699 [37:07<40:56,  1.44it/s]Evaluating on VQA val set:  47% 3158/6699 [37:08<41:05,  1.44it/s]Evaluating on VQA val set:  47% 3159/6699 [37:09<40:08,  1.47it/s]Evaluating on VQA val set:  47% 3160/6699 [37:09<39:01,  1.51it/s]Evaluating on VQA val set:  47% 3161/6699 [37:10<38:42,  1.52it/s]Evaluating on VQA val set:  47% 3162/6699 [37:11<39:56,  1.48it/s]Evaluating on VQA val set:  47% 3163/6699 [37:11<39:18,  1.50it/s]Evaluating on VQA val set:  47% 3164/6699 [37:12<40:11,  1.47it/s]Evaluating on VQA val set:  47% 3165/6699 [37:13<40:54,  1.44it/s]Evaluating on VQA val set:  47% 3166/6699 [37:13<40:39,  1.45it/s]Evaluating on VQA val set:  47% 3167/6699 [37:14<40:27,  1.45it/s]Evaluating on VQA val set:  47% 3168/6699 [37:15<40:47,  1.44it/s]Evaluating on VQA val set:  47% 3169/6699 [37:15<39:40,  1.48it/s]Evaluating on VQA val set:  47% 3170/6699 [37:16<40:55,  1.44it/s]Evaluating on VQA val set:  47% 3171/6699 [37:17<41:46,  1.41it/s]Evaluating on VQA val set:  47% 3172/6699 [37:17<41:29,  1.42it/s]Evaluating on VQA val set:  47% 3173/6699 [37:18<41:14,  1.42it/s]Evaluating on VQA val set:  47% 3174/6699 [37:19<41:56,  1.40it/s]Evaluating on VQA val set:  47% 3175/6699 [37:20<41:13,  1.42it/s]Evaluating on VQA val set:  47% 3176/6699 [37:20<39:59,  1.47it/s]Evaluating on VQA val set:  47% 3177/6699 [37:21<39:33,  1.48it/s]Evaluating on VQA val set:  47% 3178/6699 [37:22<39:50,  1.47it/s]Evaluating on VQA val set:  47% 3179/6699 [37:22<40:28,  1.45it/s]Evaluating on VQA val set:  47% 3180/6699 [37:23<41:16,  1.42it/s]Evaluating on VQA val set:  47% 3181/6699 [37:24<41:53,  1.40it/s]Evaluating on VQA val set:  47% 3182/6699 [37:24<41:20,  1.42it/s]Evaluating on VQA val set:  48% 3183/6699 [37:25<41:15,  1.42it/s]Evaluating on VQA val set:  48% 3184/6699 [37:26<41:44,  1.40it/s]Evaluating on VQA val set:  48% 3185/6699 [37:27<42:18,  1.38it/s]Evaluating on VQA val set:  48% 3186/6699 [37:27<41:42,  1.40it/s]Evaluating on VQA val set:  48% 3187/6699 [37:28<42:19,  1.38it/s]Evaluating on VQA val set:  48% 3188/6699 [37:29<42:48,  1.37it/s]Evaluating on VQA val set:  48% 3189/6699 [37:30<42:59,  1.36it/s]Evaluating on VQA val set:  48% 3190/6699 [37:31<55:53,  1.05it/s]Evaluating on VQA val set:  48% 3191/6699 [37:32<51:26,  1.14it/s]Evaluating on VQA val set:  48% 3192/6699 [37:32<48:38,  1.20it/s]Evaluating on VQA val set:  48% 3193/6699 [37:33<45:33,  1.28it/s]Evaluating on VQA val set:  48% 3194/6699 [37:34<45:02,  1.30it/s]Evaluating on VQA val set:  48% 3195/6699 [37:35<43:43,  1.34it/s]Evaluating on VQA val set:  48% 3196/6699 [37:35<43:54,  1.33it/s]Evaluating on VQA val set:  48% 3197/6699 [37:36<43:52,  1.33it/s]Evaluating on VQA val set:  48% 3198/6699 [37:37<44:12,  1.32it/s]Evaluating on VQA val set:  48% 3199/6699 [37:37<42:24,  1.38it/s]Evaluating on VQA val set:  48% 3200/6699 [37:38<42:58,  1.36it/s]Evaluating on VQA val set:  48% 3201/6699 [37:39<42:03,  1.39it/s]Evaluating on VQA val set:  48% 3202/6699 [37:40<41:23,  1.41it/s]Evaluating on VQA val set:  48% 3203/6699 [37:40<41:24,  1.41it/s]Evaluating on VQA val set:  48% 3204/6699 [37:41<41:08,  1.42it/s]Evaluating on VQA val set:  48% 3205/6699 [37:42<40:07,  1.45it/s]Evaluating on VQA val set:  48% 3206/6699 [37:42<41:29,  1.40it/s]Evaluating on VQA val set:  48% 3207/6699 [37:43<40:39,  1.43it/s]Evaluating on VQA val set:  48% 3208/6699 [37:44<41:59,  1.39it/s]Evaluating on VQA val set:  48% 3209/6699 [37:45<43:04,  1.35it/s]Evaluating on VQA val set:  48% 3210/6699 [37:45<43:04,  1.35it/s]Evaluating on VQA val set:  48% 3211/6699 [37:46<42:13,  1.38it/s]Evaluating on VQA val set:  48% 3212/6699 [37:47<40:04,  1.45it/s]Evaluating on VQA val set:  48% 3213/6699 [37:47<39:54,  1.46it/s]Evaluating on VQA val set:  48% 3214/6699 [37:48<40:08,  1.45it/s]Evaluating on VQA val set:  48% 3215/6699 [37:49<41:06,  1.41it/s]Evaluating on VQA val set:  48% 3216/6699 [37:49<39:19,  1.48it/s]Evaluating on VQA val set:  48% 3217/6699 [37:50<39:14,  1.48it/s]Evaluating on VQA val set:  48% 3218/6699 [37:51<39:23,  1.47it/s]Evaluating on VQA val set:  48% 3219/6699 [37:52<40:30,  1.43it/s]Evaluating on VQA val set:  48% 3220/6699 [37:52<41:31,  1.40it/s]Evaluating on VQA val set:  48% 3221/6699 [37:53<42:05,  1.38it/s]Evaluating on VQA val set:  48% 3222/6699 [37:54<41:19,  1.40it/s]Evaluating on VQA val set:  48% 3223/6699 [37:54<40:46,  1.42it/s]Evaluating on VQA val set:  48% 3224/6699 [37:55<40:35,  1.43it/s]Evaluating on VQA val set:  48% 3225/6699 [37:56<38:34,  1.50it/s]Evaluating on VQA val set:  48% 3226/6699 [37:56<37:53,  1.53it/s]Evaluating on VQA val set:  48% 3227/6699 [37:57<40:13,  1.44it/s]Evaluating on VQA val set:  48% 3228/6699 [37:58<40:58,  1.41it/s]Evaluating on VQA val set:  48% 3229/6699 [37:59<41:52,  1.38it/s]Evaluating on VQA val set:  48% 3230/6699 [37:59<42:17,  1.37it/s]Evaluating on VQA val set:  48% 3231/6699 [38:00<41:38,  1.39it/s]Evaluating on VQA val set:  48% 3232/6699 [38:01<41:14,  1.40it/s]Evaluating on VQA val set:  48% 3233/6699 [38:01<41:14,  1.40it/s]Evaluating on VQA val set:  48% 3234/6699 [38:02<41:03,  1.41it/s]Evaluating on VQA val set:  48% 3235/6699 [38:03<41:14,  1.40it/s]Evaluating on VQA val set:  48% 3236/6699 [38:04<41:07,  1.40it/s]Evaluating on VQA val set:  48% 3237/6699 [38:04<39:18,  1.47it/s]Evaluating on VQA val set:  48% 3238/6699 [38:05<37:03,  1.56it/s]Evaluating on VQA val set:  48% 3239/6699 [38:05<35:45,  1.61it/s]Evaluating on VQA val set:  48% 3240/6699 [38:06<36:24,  1.58it/s]Evaluating on VQA val set:  48% 3241/6699 [38:07<38:28,  1.50it/s]Evaluating on VQA val set:  48% 3242/6699 [38:07<39:21,  1.46it/s]Evaluating on VQA val set:  48% 3243/6699 [38:08<39:03,  1.47it/s]Evaluating on VQA val set:  48% 3244/6699 [38:09<38:18,  1.50it/s]Evaluating on VQA val set:  48% 3245/6699 [38:10<39:54,  1.44it/s]Evaluating on VQA val set:  48% 3246/6699 [38:10<41:04,  1.40it/s]Evaluating on VQA val set:  48% 3247/6699 [38:11<39:59,  1.44it/s]Evaluating on VQA val set:  48% 3248/6699 [38:12<39:59,  1.44it/s]Evaluating on VQA val set:  48% 3249/6699 [38:12<39:19,  1.46it/s]Evaluating on VQA val set:  49% 3250/6699 [38:13<39:28,  1.46it/s]Evaluating on VQA val set:  49% 3251/6699 [38:14<38:50,  1.48it/s]Evaluating on VQA val set:  49% 3252/6699 [38:14<39:02,  1.47it/s]Evaluating on VQA val set:  49% 3253/6699 [38:15<39:16,  1.46it/s]Evaluating on VQA val set:  49% 3254/6699 [38:16<37:53,  1.52it/s]Evaluating on VQA val set:  49% 3255/6699 [38:16<38:29,  1.49it/s]Evaluating on VQA val set:  49% 3256/6699 [38:17<38:46,  1.48it/s]Evaluating on VQA val set:  49% 3257/6699 [38:18<40:29,  1.42it/s]Evaluating on VQA val set:  49% 3258/6699 [38:19<41:11,  1.39it/s]Evaluating on VQA val set:  49% 3259/6699 [38:19<39:43,  1.44it/s]Evaluating on VQA val set:  49% 3260/6699 [38:20<40:46,  1.41it/s]Evaluating on VQA val set:  49% 3261/6699 [38:21<41:16,  1.39it/s]Evaluating on VQA val set:  49% 3262/6699 [38:21<40:58,  1.40it/s]Evaluating on VQA val set:  49% 3263/6699 [38:22<41:28,  1.38it/s]Evaluating on VQA val set:  49% 3264/6699 [38:23<41:44,  1.37it/s]Evaluating on VQA val set:  49% 3265/6699 [38:23<40:30,  1.41it/s]Evaluating on VQA val set:  49% 3266/6699 [38:24<41:20,  1.38it/s]Evaluating on VQA val set:  49% 3267/6699 [38:25<40:19,  1.42it/s]Evaluating on VQA val set:  49% 3268/6699 [38:26<39:35,  1.44it/s]Evaluating on VQA val set:  49% 3269/6699 [38:26<40:36,  1.41it/s]Evaluating on VQA val set:  49% 3270/6699 [38:27<39:59,  1.43it/s]Evaluating on VQA val set:  49% 3271/6699 [38:28<39:28,  1.45it/s]Evaluating on VQA val set:  49% 3272/6699 [38:28<38:15,  1.49it/s]Evaluating on VQA val set:  49% 3273/6699 [38:29<39:06,  1.46it/s]Evaluating on VQA val set:  49% 3274/6699 [38:30<39:35,  1.44it/s]Evaluating on VQA val set:  49% 3275/6699 [38:30<39:43,  1.44it/s]Evaluating on VQA val set:  49% 3276/6699 [38:31<40:30,  1.41it/s]Evaluating on VQA val set:  49% 3277/6699 [38:32<40:42,  1.40it/s]Evaluating on VQA val set:  49% 3278/6699 [38:33<40:40,  1.40it/s]Evaluating on VQA val set:  49% 3279/6699 [38:33<41:23,  1.38it/s]Evaluating on VQA val set:  49% 3280/6699 [38:34<40:41,  1.40it/s]Evaluating on VQA val set:  49% 3281/6699 [38:35<40:55,  1.39it/s]Evaluating on VQA val set:  49% 3282/6699 [38:36<42:03,  1.35it/s]Evaluating on VQA val set:  49% 3283/6699 [38:36<41:45,  1.36it/s]Evaluating on VQA val set:  49% 3284/6699 [38:37<40:41,  1.40it/s]Evaluating on VQA val set:  49% 3285/6699 [38:38<38:53,  1.46it/s]Evaluating on VQA val set:  49% 3286/6699 [38:38<38:37,  1.47it/s]Evaluating on VQA val set:  49% 3287/6699 [38:39<38:34,  1.47it/s]Evaluating on VQA val set:  49% 3288/6699 [38:40<38:35,  1.47it/s]Evaluating on VQA val set:  49% 3289/6699 [38:40<39:02,  1.46it/s]Evaluating on VQA val set:  49% 3290/6699 [38:41<39:49,  1.43it/s]Evaluating on VQA val set:  49% 3291/6699 [38:42<39:51,  1.43it/s]Evaluating on VQA val set:  49% 3292/6699 [38:42<39:39,  1.43it/s]Evaluating on VQA val set:  49% 3293/6699 [38:43<39:09,  1.45it/s]Evaluating on VQA val set:  49% 3294/6699 [38:44<39:36,  1.43it/s]Evaluating on VQA val set:  49% 3295/6699 [38:44<39:08,  1.45it/s]Evaluating on VQA val set:  49% 3296/6699 [38:46<46:50,  1.21it/s]Evaluating on VQA val set:  49% 3297/6699 [38:46<44:36,  1.27it/s]Evaluating on VQA val set:  49% 3298/6699 [38:47<44:04,  1.29it/s]Evaluating on VQA val set:  49% 3299/6699 [38:48<42:54,  1.32it/s]Evaluating on VQA val set:  49% 3300/6699 [38:48<40:19,  1.40it/s]Evaluating on VQA val set:  49% 3301/6699 [38:49<38:30,  1.47it/s]Evaluating on VQA val set:  49% 3302/6699 [38:50<39:47,  1.42it/s]Evaluating on VQA val set:  49% 3303/6699 [38:50<36:08,  1.57it/s]Evaluating on VQA val set:  49% 3304/6699 [38:51<37:42,  1.50it/s]Evaluating on VQA val set:  49% 3305/6699 [38:52<37:49,  1.50it/s]Evaluating on VQA val set:  49% 3306/6699 [38:52<38:55,  1.45it/s]Evaluating on VQA val set:  49% 3307/6699 [38:53<37:55,  1.49it/s]Evaluating on VQA val set:  49% 3308/6699 [38:54<39:02,  1.45it/s]Evaluating on VQA val set:  49% 3309/6699 [38:54<37:32,  1.50it/s]Evaluating on VQA val set:  49% 3310/6699 [38:55<39:18,  1.44it/s]Evaluating on VQA val set:  49% 3311/6699 [38:56<38:26,  1.47it/s]Evaluating on VQA val set:  49% 3312/6699 [38:56<39:17,  1.44it/s]Evaluating on VQA val set:  49% 3313/6699 [38:57<37:42,  1.50it/s]Evaluating on VQA val set:  49% 3314/6699 [38:58<39:39,  1.42it/s]Evaluating on VQA val set:  49% 3315/6699 [38:59<40:11,  1.40it/s]Evaluating on VQA val set:  49% 3316/6699 [38:59<40:55,  1.38it/s]Evaluating on VQA val set:  50% 3317/6699 [39:00<39:41,  1.42it/s]Evaluating on VQA val set:  50% 3318/6699 [39:01<40:24,  1.39it/s]Evaluating on VQA val set:  50% 3319/6699 [39:01<39:52,  1.41it/s]Evaluating on VQA val set:  50% 3320/6699 [39:02<40:40,  1.38it/s]Evaluating on VQA val set:  50% 3321/6699 [39:03<39:59,  1.41it/s]Evaluating on VQA val set:  50% 3322/6699 [39:04<40:02,  1.41it/s]Evaluating on VQA val set:  50% 3323/6699 [39:04<37:52,  1.49it/s]Evaluating on VQA val set:  50% 3324/6699 [39:05<39:46,  1.41it/s]Evaluating on VQA val set:  50% 3325/6699 [39:06<39:45,  1.41it/s]Evaluating on VQA val set:  50% 3326/6699 [39:06<41:23,  1.36it/s]Evaluating on VQA val set:  50% 3327/6699 [39:07<39:06,  1.44it/s]Evaluating on VQA val set:  50% 3328/6699 [39:08<40:12,  1.40it/s]Evaluating on VQA val set:  50% 3329/6699 [39:09<40:07,  1.40it/s]Evaluating on VQA val set:  50% 3330/6699 [39:09<40:19,  1.39it/s]Evaluating on VQA val set:  50% 3331/6699 [39:10<39:22,  1.43it/s]Evaluating on VQA val set:  50% 3332/6699 [39:11<39:35,  1.42it/s]Evaluating on VQA val set:  50% 3333/6699 [39:11<40:04,  1.40it/s]Evaluating on VQA val set:  50% 3334/6699 [39:12<41:03,  1.37it/s]Evaluating on VQA val set:  50% 3335/6699 [39:13<40:50,  1.37it/s]Evaluating on VQA val set:  50% 3336/6699 [39:14<41:42,  1.34it/s]Evaluating on VQA val set:  50% 3337/6699 [39:14<40:13,  1.39it/s]Evaluating on VQA val set:  50% 3338/6699 [39:15<41:57,  1.34it/s]Evaluating on VQA val set:  50% 3339/6699 [39:16<41:21,  1.35it/s]Evaluating on VQA val set:  50% 3340/6699 [39:17<41:23,  1.35it/s]Evaluating on VQA val set:  50% 3341/6699 [39:17<40:10,  1.39it/s]Evaluating on VQA val set:  50% 3342/6699 [39:18<41:19,  1.35it/s]Evaluating on VQA val set:  50% 3343/6699 [39:19<39:37,  1.41it/s]Evaluating on VQA val set:  50% 3344/6699 [39:19<40:10,  1.39it/s]Evaluating on VQA val set:  50% 3345/6699 [39:20<39:24,  1.42it/s]Evaluating on VQA val set:  50% 3346/6699 [39:21<41:31,  1.35it/s]Evaluating on VQA val set:  50% 3347/6699 [39:22<40:35,  1.38it/s]Evaluating on VQA val set:  50% 3348/6699 [39:22<40:27,  1.38it/s]Evaluating on VQA val set:  50% 3349/6699 [39:23<40:28,  1.38it/s]Evaluating on VQA val set:  50% 3350/6699 [39:24<41:44,  1.34it/s]Evaluating on VQA val set:  50% 3351/6699 [39:25<40:12,  1.39it/s]Evaluating on VQA val set:  50% 3352/6699 [39:25<39:40,  1.41it/s]Evaluating on VQA val set:  50% 3353/6699 [39:26<38:38,  1.44it/s]Evaluating on VQA val set:  50% 3354/6699 [39:27<39:29,  1.41it/s]Evaluating on VQA val set:  50% 3355/6699 [39:27<38:09,  1.46it/s]Evaluating on VQA val set:  50% 3356/6699 [39:28<40:48,  1.37it/s]Evaluating on VQA val set:  50% 3357/6699 [39:29<39:54,  1.40it/s]Evaluating on VQA val set:  50% 3358/6699 [39:30<40:48,  1.36it/s]Evaluating on VQA val set:  50% 3359/6699 [39:30<40:01,  1.39it/s]Evaluating on VQA val set:  50% 3360/6699 [39:31<40:29,  1.37it/s]Evaluating on VQA val set:  50% 3361/6699 [39:32<38:14,  1.46it/s]Evaluating on VQA val set:  50% 3362/6699 [39:32<38:11,  1.46it/s]Evaluating on VQA val set:  50% 3363/6699 [39:33<38:07,  1.46it/s]Evaluating on VQA val set:  50% 3364/6699 [39:34<39:35,  1.40it/s]Evaluating on VQA val set:  50% 3365/6699 [39:34<39:02,  1.42it/s]Evaluating on VQA val set:  50% 3366/6699 [39:35<40:14,  1.38it/s]Evaluating on VQA val set:  50% 3367/6699 [39:36<39:02,  1.42it/s]Evaluating on VQA val set:  50% 3368/6699 [39:37<40:28,  1.37it/s]Evaluating on VQA val set:  50% 3369/6699 [39:37<39:15,  1.41it/s]Evaluating on VQA val set:  50% 3370/6699 [39:38<39:45,  1.40it/s]Evaluating on VQA val set:  50% 3371/6699 [39:39<39:12,  1.41it/s]Evaluating on VQA val set:  50% 3372/6699 [39:39<40:06,  1.38it/s]Evaluating on VQA val set:  50% 3373/6699 [39:40<39:37,  1.40it/s]Evaluating on VQA val set:  50% 3374/6699 [39:41<39:54,  1.39it/s]Evaluating on VQA val set:  50% 3375/6699 [39:42<38:20,  1.45it/s]Evaluating on VQA val set:  50% 3376/6699 [39:42<39:10,  1.41it/s]Evaluating on VQA val set:  50% 3377/6699 [39:43<39:24,  1.40it/s]Evaluating on VQA val set:  50% 3378/6699 [39:44<41:19,  1.34it/s]Evaluating on VQA val set:  50% 3379/6699 [39:44<40:06,  1.38it/s]Evaluating on VQA val set:  50% 3380/6699 [39:45<41:11,  1.34it/s]Evaluating on VQA val set:  50% 3381/6699 [39:46<39:29,  1.40it/s]Evaluating on VQA val set:  50% 3382/6699 [39:47<40:27,  1.37it/s]Evaluating on VQA val set:  51% 3383/6699 [39:47<39:40,  1.39it/s]Evaluating on VQA val set:  51% 3384/6699 [39:48<40:22,  1.37it/s]Evaluating on VQA val set:  51% 3385/6699 [39:49<38:52,  1.42it/s]Evaluating on VQA val set:  51% 3386/6699 [39:50<40:41,  1.36it/s]Evaluating on VQA val set:  51% 3387/6699 [39:50<38:45,  1.42it/s]Evaluating on VQA val set:  51% 3388/6699 [39:51<38:47,  1.42it/s]Evaluating on VQA val set:  51% 3389/6699 [39:52<37:31,  1.47it/s]Evaluating on VQA val set:  51% 3390/6699 [39:52<38:48,  1.42it/s]Evaluating on VQA val set:  51% 3391/6699 [39:53<37:40,  1.46it/s]Evaluating on VQA val set:  51% 3392/6699 [39:54<39:40,  1.39it/s]Evaluating on VQA val set:  51% 3393/6699 [39:54<39:26,  1.40it/s]Evaluating on VQA val set:  51% 3394/6699 [39:55<40:18,  1.37it/s]Evaluating on VQA val set:  51% 3395/6699 [39:56<39:59,  1.38it/s]Evaluating on VQA val set:  51% 3396/6699 [39:57<40:29,  1.36it/s]Evaluating on VQA val set:  51% 3397/6699 [39:57<39:52,  1.38it/s]Evaluating on VQA val set:  51% 3398/6699 [39:58<40:15,  1.37it/s]Evaluating on VQA val set:  51% 3399/6699 [39:59<38:19,  1.44it/s]Evaluating on VQA val set:  51% 3400/6699 [39:59<38:58,  1.41it/s]Evaluating on VQA val set:  51% 3401/6699 [40:00<37:54,  1.45it/s]Evaluating on VQA val set:  51% 3402/6699 [40:01<38:59,  1.41it/s]Evaluating on VQA val set:  51% 3403/6699 [40:02<39:37,  1.39it/s]Evaluating on VQA val set:  51% 3404/6699 [40:02<39:02,  1.41it/s]Evaluating on VQA val set:  51% 3405/6699 [40:03<37:38,  1.46it/s]Evaluating on VQA val set:  51% 3406/6699 [40:04<38:52,  1.41it/s]Evaluating on VQA val set:  51% 3407/6699 [40:04<38:53,  1.41it/s]Evaluating on VQA val set:  51% 3408/6699 [40:05<40:40,  1.35it/s]Evaluating on VQA val set:  51% 3409/6699 [40:06<39:54,  1.37it/s]Evaluating on VQA val set:  51% 3410/6699 [40:07<40:45,  1.34it/s]Evaluating on VQA val set:  51% 3411/6699 [40:07<39:41,  1.38it/s]Evaluating on VQA val set:  51% 3412/6699 [40:08<40:15,  1.36it/s]Evaluating on VQA val set:  51% 3413/6699 [40:09<39:20,  1.39it/s]Evaluating on VQA val set:  51% 3414/6699 [40:10<39:30,  1.39it/s]Evaluating on VQA val set:  51% 3415/6699 [40:10<38:17,  1.43it/s]Evaluating on VQA val set:  51% 3416/6699 [40:11<38:55,  1.41it/s]Evaluating on VQA val set:  51% 3417/6699 [40:12<38:51,  1.41it/s]Evaluating on VQA val set:  51% 3418/6699 [40:12<40:21,  1.35it/s]Evaluating on VQA val set:  51% 3419/6699 [40:13<38:39,  1.41it/s]Evaluating on VQA val set:  51% 3420/6699 [40:14<39:25,  1.39it/s]Evaluating on VQA val set:  51% 3421/6699 [40:14<37:17,  1.46it/s]Evaluating on VQA val set:  51% 3422/6699 [40:15<39:05,  1.40it/s]Evaluating on VQA val set:  51% 3423/6699 [40:16<38:59,  1.40it/s]Evaluating on VQA val set:  51% 3424/6699 [40:17<40:12,  1.36it/s]Evaluating on VQA val set:  51% 3425/6699 [40:17<39:11,  1.39it/s]Evaluating on VQA val set:  51% 3426/6699 [40:18<38:37,  1.41it/s]Evaluating on VQA val set:  51% 3427/6699 [40:19<38:14,  1.43it/s]Evaluating on VQA val set:  51% 3428/6699 [40:19<38:14,  1.43it/s]Evaluating on VQA val set:  51% 3429/6699 [40:20<37:12,  1.46it/s]Evaluating on VQA val set:  51% 3430/6699 [40:21<38:23,  1.42it/s]Evaluating on VQA val set:  51% 3431/6699 [40:22<37:32,  1.45it/s]Evaluating on VQA val set:  51% 3432/6699 [40:22<38:48,  1.40it/s]Evaluating on VQA val set:  51% 3433/6699 [40:23<38:43,  1.41it/s]Evaluating on VQA val set:  51% 3434/6699 [40:24<39:15,  1.39it/s]Evaluating on VQA val set:  51% 3435/6699 [40:24<39:23,  1.38it/s]Evaluating on VQA val set:  51% 3436/6699 [40:25<39:57,  1.36it/s]Evaluating on VQA val set:  51% 3437/6699 [40:26<38:36,  1.41it/s]Evaluating on VQA val set:  51% 3438/6699 [40:27<38:02,  1.43it/s]Evaluating on VQA val set:  51% 3439/6699 [40:27<35:46,  1.52it/s]Evaluating on VQA val set:  51% 3440/6699 [40:28<37:37,  1.44it/s]Evaluating on VQA val set:  51% 3441/6699 [40:29<37:31,  1.45it/s]Evaluating on VQA val set:  51% 3442/6699 [40:29<39:07,  1.39it/s]Evaluating on VQA val set:  51% 3443/6699 [40:30<39:08,  1.39it/s]Evaluating on VQA val set:  51% 3444/6699 [40:31<38:37,  1.40it/s]Evaluating on VQA val set:  51% 3445/6699 [40:32<39:04,  1.39it/s]Evaluating on VQA val set:  51% 3446/6699 [40:32<39:06,  1.39it/s]Evaluating on VQA val set:  51% 3447/6699 [40:33<37:54,  1.43it/s]Evaluating on VQA val set:  51% 3448/6699 [40:34<38:28,  1.41it/s]Evaluating on VQA val set:  51% 3449/6699 [40:34<37:21,  1.45it/s]Evaluating on VQA val set:  52% 3450/6699 [40:35<37:37,  1.44it/s]Evaluating on VQA val set:  52% 3451/6699 [40:36<37:53,  1.43it/s]Evaluating on VQA val set:  52% 3452/6699 [40:37<40:00,  1.35it/s]Evaluating on VQA val set:  52% 3453/6699 [40:37<38:47,  1.39it/s]Evaluating on VQA val set:  52% 3454/6699 [40:38<40:38,  1.33it/s]Evaluating on VQA val set:  52% 3455/6699 [40:39<40:03,  1.35it/s]Evaluating on VQA val set:  52% 3456/6699 [40:40<40:57,  1.32it/s]Evaluating on VQA val set:  52% 3457/6699 [40:40<39:49,  1.36it/s]Evaluating on VQA val set:  52% 3458/6699 [40:41<40:57,  1.32it/s]Evaluating on VQA val set:  52% 3459/6699 [40:42<39:57,  1.35it/s]Evaluating on VQA val set:  52% 3460/6699 [40:43<40:43,  1.33it/s]Evaluating on VQA val set:  52% 3461/6699 [40:43<39:04,  1.38it/s]Evaluating on VQA val set:  52% 3462/6699 [40:44<40:29,  1.33it/s]Evaluating on VQA val set:  52% 3463/6699 [40:45<39:18,  1.37it/s]Evaluating on VQA val set:  52% 3464/6699 [40:45<40:03,  1.35it/s]Evaluating on VQA val set:  52% 3465/6699 [40:46<38:58,  1.38it/s]Evaluating on VQA val set:  52% 3466/6699 [40:47<39:59,  1.35it/s]Evaluating on VQA val set:  52% 3467/6699 [40:48<38:55,  1.38it/s]Evaluating on VQA val set:  52% 3468/6699 [40:48<38:11,  1.41it/s]Evaluating on VQA val set:  52% 3469/6699 [40:49<37:56,  1.42it/s]Evaluating on VQA val set:  52% 3470/6699 [40:50<39:32,  1.36it/s]Evaluating on VQA val set:  52% 3471/6699 [40:50<38:06,  1.41it/s]Evaluating on VQA val set:  52% 3472/6699 [40:51<38:58,  1.38it/s]Evaluating on VQA val set:  52% 3473/6699 [40:52<38:44,  1.39it/s]Evaluating on VQA val set:  52% 3474/6699 [40:53<39:45,  1.35it/s]Evaluating on VQA val set:  52% 3475/6699 [40:53<37:43,  1.42it/s]Evaluating on VQA val set:  52% 3476/6699 [40:54<38:47,  1.38it/s]Evaluating on VQA val set:  52% 3477/6699 [40:55<37:38,  1.43it/s]Evaluating on VQA val set:  52% 3478/6699 [40:55<36:34,  1.47it/s]Evaluating on VQA val set:  52% 3479/6699 [40:56<36:29,  1.47it/s]Evaluating on VQA val set:  52% 3480/6699 [40:57<37:30,  1.43it/s]Evaluating on VQA val set:  52% 3481/6699 [40:57<36:54,  1.45it/s]Evaluating on VQA val set:  52% 3482/6699 [40:58<36:40,  1.46it/s]Evaluating on VQA val set:  52% 3483/6699 [40:59<36:57,  1.45it/s]Evaluating on VQA val set:  52% 3484/6699 [41:00<38:41,  1.38it/s]Evaluating on VQA val set:  52% 3485/6699 [41:00<38:34,  1.39it/s]Evaluating on VQA val set:  52% 3486/6699 [41:01<38:49,  1.38it/s]Evaluating on VQA val set:  52% 3487/6699 [41:02<38:48,  1.38it/s]Evaluating on VQA val set:  52% 3488/6699 [41:03<39:46,  1.35it/s]Evaluating on VQA val set:  52% 3489/6699 [41:03<39:04,  1.37it/s]Evaluating on VQA val set:  52% 3490/6699 [41:04<39:35,  1.35it/s]Evaluating on VQA val set:  52% 3491/6699 [41:05<37:25,  1.43it/s]Evaluating on VQA val set:  52% 3492/6699 [41:05<38:09,  1.40it/s]Evaluating on VQA val set:  52% 3493/6699 [41:06<37:43,  1.42it/s]Evaluating on VQA val set:  52% 3494/6699 [41:07<39:04,  1.37it/s]Evaluating on VQA val set:  52% 3495/6699 [41:07<37:16,  1.43it/s]Evaluating on VQA val set:  52% 3496/6699 [41:08<38:04,  1.40it/s]Evaluating on VQA val set:  52% 3497/6699 [41:09<38:22,  1.39it/s]Evaluating on VQA val set:  52% 3498/6699 [41:10<39:14,  1.36it/s]Evaluating on VQA val set:  52% 3499/6699 [41:10<38:23,  1.39it/s]Evaluating on VQA val set:  52% 3500/6699 [41:11<38:04,  1.40it/s]Evaluating on VQA val set:  52% 3501/6699 [41:12<37:19,  1.43it/s]Evaluating on VQA val set:  52% 3502/6699 [41:13<38:36,  1.38it/s]Evaluating on VQA val set:  52% 3503/6699 [41:13<37:38,  1.42it/s]Evaluating on VQA val set:  52% 3504/6699 [41:14<38:50,  1.37it/s]Evaluating on VQA val set:  52% 3505/6699 [41:15<38:37,  1.38it/s]Evaluating on VQA val set:  52% 3506/6699 [41:15<38:33,  1.38it/s]Evaluating on VQA val set:  52% 3507/6699 [41:16<37:41,  1.41it/s]Evaluating on VQA val set:  52% 3508/6699 [41:17<38:10,  1.39it/s]Evaluating on VQA val set:  52% 3509/6699 [41:18<37:49,  1.41it/s]Evaluating on VQA val set:  52% 3510/6699 [41:18<38:52,  1.37it/s]Evaluating on VQA val set:  52% 3511/6699 [41:19<37:55,  1.40it/s]Evaluating on VQA val set:  52% 3512/6699 [41:20<37:24,  1.42it/s]Evaluating on VQA val set:  52% 3513/6699 [41:20<36:55,  1.44it/s]Evaluating on VQA val set:  52% 3514/6699 [41:21<38:12,  1.39it/s]Evaluating on VQA val set:  52% 3515/6699 [41:22<37:37,  1.41it/s]Evaluating on VQA val set:  52% 3516/6699 [41:23<38:02,  1.39it/s]Evaluating on VQA val set:  53% 3517/6699 [41:23<37:54,  1.40it/s]Evaluating on VQA val set:  53% 3518/6699 [41:24<37:58,  1.40it/s]Evaluating on VQA val set:  53% 3519/6699 [41:25<36:08,  1.47it/s]Evaluating on VQA val set:  53% 3520/6699 [41:25<37:49,  1.40it/s]Evaluating on VQA val set:  53% 3521/6699 [41:26<37:40,  1.41it/s]Evaluating on VQA val set:  53% 3522/6699 [41:27<37:01,  1.43it/s]Evaluating on VQA val set:  53% 3523/6699 [41:27<36:59,  1.43it/s]Evaluating on VQA val set:  53% 3524/6699 [41:28<37:44,  1.40it/s]Evaluating on VQA val set:  53% 3525/6699 [41:29<36:22,  1.45it/s]Evaluating on VQA val set:  53% 3526/6699 [41:30<37:09,  1.42it/s]Evaluating on VQA val set:  53% 3527/6699 [41:30<37:16,  1.42it/s]Evaluating on VQA val set:  53% 3528/6699 [41:31<38:24,  1.38it/s]Evaluating on VQA val set:  53% 3529/6699 [41:32<36:46,  1.44it/s]Evaluating on VQA val set:  53% 3530/6699 [41:32<37:51,  1.40it/s]Evaluating on VQA val set:  53% 3531/6699 [41:33<36:23,  1.45it/s]Evaluating on VQA val set:  53% 3532/6699 [41:34<36:29,  1.45it/s]Evaluating on VQA val set:  53% 3533/6699 [41:34<35:48,  1.47it/s]Evaluating on VQA val set:  53% 3534/6699 [41:35<38:02,  1.39it/s]Evaluating on VQA val set:  53% 3535/6699 [41:36<38:11,  1.38it/s]Evaluating on VQA val set:  53% 3536/6699 [41:37<39:33,  1.33it/s]Evaluating on VQA val set:  53% 3537/6699 [41:37<38:31,  1.37it/s]Evaluating on VQA val set:  53% 3538/6699 [41:38<38:29,  1.37it/s]Evaluating on VQA val set:  53% 3539/6699 [41:39<37:15,  1.41it/s]Evaluating on VQA val set:  53% 3540/6699 [41:40<39:02,  1.35it/s]Evaluating on VQA val set:  53% 3541/6699 [41:40<38:15,  1.38it/s]Evaluating on VQA val set:  53% 3542/6699 [41:41<36:56,  1.42it/s]Evaluating on VQA val set:  53% 3543/6699 [41:42<35:29,  1.48it/s]Evaluating on VQA val set:  53% 3544/6699 [41:42<37:49,  1.39it/s]Evaluating on VQA val set:  53% 3545/6699 [41:43<38:29,  1.37it/s]Evaluating on VQA val set:  53% 3546/6699 [41:44<39:17,  1.34it/s]Evaluating on VQA val set:  53% 3547/6699 [41:45<36:40,  1.43it/s]Evaluating on VQA val set:  53% 3548/6699 [41:45<38:04,  1.38it/s]Evaluating on VQA val set:  53% 3549/6699 [41:46<36:54,  1.42it/s]Evaluating on VQA val set:  53% 3550/6699 [41:47<38:24,  1.37it/s]Evaluating on VQA val set:  53% 3551/6699 [41:48<37:42,  1.39it/s]Evaluating on VQA val set:  53% 3552/6699 [41:48<38:36,  1.36it/s]Evaluating on VQA val set:  53% 3553/6699 [41:49<36:46,  1.43it/s]Evaluating on VQA val set:  53% 3554/6699 [41:50<36:57,  1.42it/s]Evaluating on VQA val set:  53% 3555/6699 [41:50<35:29,  1.48it/s]Evaluating on VQA val set:  53% 3556/6699 [41:51<37:02,  1.41it/s]Evaluating on VQA val set:  53% 3557/6699 [41:52<36:06,  1.45it/s]Evaluating on VQA val set:  53% 3558/6699 [41:52<37:28,  1.40it/s]Evaluating on VQA val set:  53% 3559/6699 [41:53<35:42,  1.47it/s]Evaluating on VQA val set:  53% 3560/6699 [41:54<36:14,  1.44it/s]Evaluating on VQA val set:  53% 3561/6699 [41:54<36:00,  1.45it/s]Evaluating on VQA val set:  53% 3562/6699 [41:55<36:45,  1.42it/s]Evaluating on VQA val set:  53% 3563/6699 [41:56<36:34,  1.43it/s]Evaluating on VQA val set:  53% 3564/6699 [41:56<35:34,  1.47it/s]Evaluating on VQA val set:  53% 3565/6699 [41:57<35:16,  1.48it/s]Evaluating on VQA val set:  53% 3566/6699 [41:58<36:05,  1.45it/s]Evaluating on VQA val set:  53% 3567/6699 [41:59<35:30,  1.47it/s]Evaluating on VQA val set:  53% 3568/6699 [41:59<34:33,  1.51it/s]Evaluating on VQA val set:  53% 3569/6699 [42:00<35:23,  1.47it/s]Evaluating on VQA val set:  53% 3570/6699 [42:01<34:34,  1.51it/s]Evaluating on VQA val set:  53% 3571/6699 [42:01<34:23,  1.52it/s]Evaluating on VQA val set:  53% 3572/6699 [42:02<36:06,  1.44it/s]Evaluating on VQA val set:  53% 3573/6699 [42:03<37:11,  1.40it/s]Evaluating on VQA val set:  53% 3574/6699 [42:03<36:46,  1.42it/s]Evaluating on VQA val set:  53% 3575/6699 [42:04<36:41,  1.42it/s]Evaluating on VQA val set:  53% 3576/6699 [42:05<36:16,  1.43it/s]Evaluating on VQA val set:  53% 3577/6699 [42:05<36:51,  1.41it/s]Evaluating on VQA val set:  53% 3578/6699 [42:06<37:29,  1.39it/s]Evaluating on VQA val set:  53% 3579/6699 [42:07<36:20,  1.43it/s]Evaluating on VQA val set:  53% 3580/6699 [42:08<36:54,  1.41it/s]Evaluating on VQA val set:  53% 3581/6699 [42:08<37:28,  1.39it/s]Evaluating on VQA val set:  53% 3582/6699 [42:09<37:37,  1.38it/s]Evaluating on VQA val set:  53% 3583/6699 [42:10<36:54,  1.41it/s]Evaluating on VQA val set:  54% 3584/6699 [42:10<36:51,  1.41it/s]Evaluating on VQA val set:  54% 3585/6699 [42:11<36:14,  1.43it/s]Evaluating on VQA val set:  54% 3586/6699 [42:12<36:14,  1.43it/s]Evaluating on VQA val set:  54% 3587/6699 [42:13<37:15,  1.39it/s]Evaluating on VQA val set:  54% 3588/6699 [42:13<37:05,  1.40it/s]Evaluating on VQA val set:  54% 3589/6699 [42:14<36:38,  1.41it/s]Evaluating on VQA val set:  54% 3590/6699 [42:15<37:10,  1.39it/s]Evaluating on VQA val set:  54% 3591/6699 [42:15<36:18,  1.43it/s]Evaluating on VQA val set:  54% 3592/6699 [42:16<34:41,  1.49it/s]Evaluating on VQA val set:  54% 3593/6699 [42:17<35:42,  1.45it/s]Evaluating on VQA val set:  54% 3594/6699 [42:17<35:06,  1.47it/s]Evaluating on VQA val set:  54% 3595/6699 [42:18<35:40,  1.45it/s]Evaluating on VQA val set:  54% 3596/6699 [42:19<35:41,  1.45it/s]Evaluating on VQA val set:  54% 3597/6699 [42:19<35:11,  1.47it/s]Evaluating on VQA val set:  54% 3598/6699 [42:20<35:34,  1.45it/s]Evaluating on VQA val set:  54% 3599/6699 [42:21<37:26,  1.38it/s]Evaluating on VQA val set:  54% 3600/6699 [42:22<38:44,  1.33it/s]Evaluating on VQA val set:  54% 3601/6699 [42:23<38:01,  1.36it/s]Evaluating on VQA val set:  54% 3602/6699 [42:23<37:52,  1.36it/s]Evaluating on VQA val set:  54% 3603/6699 [42:24<37:32,  1.37it/s]Evaluating on VQA val set:  54% 3604/6699 [42:25<37:41,  1.37it/s]Evaluating on VQA val set:  54% 3605/6699 [42:25<37:28,  1.38it/s]Evaluating on VQA val set:  54% 3606/6699 [42:26<38:00,  1.36it/s]Evaluating on VQA val set:  54% 3607/6699 [42:27<38:00,  1.36it/s]Evaluating on VQA val set:  54% 3608/6699 [42:28<37:21,  1.38it/s]Evaluating on VQA val set:  54% 3609/6699 [42:28<36:53,  1.40it/s]Evaluating on VQA val set:  54% 3610/6699 [42:29<37:00,  1.39it/s]Evaluating on VQA val set:  54% 3611/6699 [42:30<37:05,  1.39it/s]Evaluating on VQA val set:  54% 3612/6699 [42:31<37:39,  1.37it/s]Evaluating on VQA val set:  54% 3613/6699 [42:31<37:32,  1.37it/s]Evaluating on VQA val set:  54% 3614/6699 [42:32<38:18,  1.34it/s]Evaluating on VQA val set:  54% 3615/6699 [42:33<36:31,  1.41it/s]Evaluating on VQA val set:  54% 3616/6699 [42:33<36:28,  1.41it/s]Evaluating on VQA val set:  54% 3617/6699 [42:34<36:54,  1.39it/s]Evaluating on VQA val set:  54% 3618/6699 [42:35<36:42,  1.40it/s]Evaluating on VQA val set:  54% 3619/6699 [42:35<36:20,  1.41it/s]Evaluating on VQA val set:  54% 3620/6699 [42:36<37:04,  1.38it/s]Evaluating on VQA val set:  54% 3621/6699 [42:37<36:47,  1.39it/s]Evaluating on VQA val set:  54% 3622/6699 [42:38<37:08,  1.38it/s]Evaluating on VQA val set:  54% 3623/6699 [42:38<36:44,  1.40it/s]Evaluating on VQA val set:  54% 3624/6699 [42:39<35:43,  1.43it/s]Evaluating on VQA val set:  54% 3625/6699 [42:40<35:48,  1.43it/s]Evaluating on VQA val set:  54% 3626/6699 [42:41<36:52,  1.39it/s]Evaluating on VQA val set:  54% 3627/6699 [42:41<36:36,  1.40it/s]Evaluating on VQA val set:  54% 3628/6699 [42:42<36:53,  1.39it/s]Evaluating on VQA val set:  54% 3629/6699 [42:43<36:22,  1.41it/s]Evaluating on VQA val set:  54% 3630/6699 [42:43<35:38,  1.44it/s]Evaluating on VQA val set:  54% 3631/6699 [42:44<36:34,  1.40it/s]Evaluating on VQA val set:  54% 3632/6699 [42:45<35:48,  1.43it/s]Evaluating on VQA val set:  54% 3633/6699 [42:45<36:05,  1.42it/s]Evaluating on VQA val set:  54% 3634/6699 [42:46<36:06,  1.41it/s]Evaluating on VQA val set:  54% 3635/6699 [42:47<36:12,  1.41it/s]Evaluating on VQA val set:  54% 3636/6699 [42:48<36:33,  1.40it/s]Evaluating on VQA val set:  54% 3637/6699 [42:48<36:40,  1.39it/s]Evaluating on VQA val set:  54% 3638/6699 [42:49<37:49,  1.35it/s]Evaluating on VQA val set:  54% 3639/6699 [42:50<38:09,  1.34it/s]Evaluating on VQA val set:  54% 3640/6699 [42:51<37:34,  1.36it/s]Evaluating on VQA val set:  54% 3641/6699 [42:51<36:40,  1.39it/s]Evaluating on VQA val set:  54% 3642/6699 [42:52<35:00,  1.46it/s]Evaluating on VQA val set:  54% 3643/6699 [42:53<34:41,  1.47it/s]Evaluating on VQA val set:  54% 3644/6699 [42:53<34:47,  1.46it/s]Evaluating on VQA val set:  54% 3645/6699 [42:54<35:53,  1.42it/s]Evaluating on VQA val set:  54% 3646/6699 [42:55<35:34,  1.43it/s]Evaluating on VQA val set:  54% 3647/6699 [42:55<36:46,  1.38it/s]Evaluating on VQA val set:  54% 3648/6699 [42:56<36:48,  1.38it/s]Evaluating on VQA val set:  54% 3649/6699 [42:57<36:49,  1.38it/s]Evaluating on VQA val set:  54% 3650/6699 [42:58<36:08,  1.41it/s]Evaluating on VQA val set:  55% 3651/6699 [42:58<36:11,  1.40it/s]Evaluating on VQA val set:  55% 3652/6699 [42:59<36:19,  1.40it/s]Evaluating on VQA val set:  55% 3653/6699 [43:00<36:20,  1.40it/s]Evaluating on VQA val set:  55% 3654/6699 [43:00<35:47,  1.42it/s]Evaluating on VQA val set:  55% 3655/6699 [43:01<36:34,  1.39it/s]Evaluating on VQA val set:  55% 3656/6699 [43:02<36:33,  1.39it/s]Evaluating on VQA val set:  55% 3657/6699 [43:03<36:16,  1.40it/s]Evaluating on VQA val set:  55% 3658/6699 [43:03<34:53,  1.45it/s]Evaluating on VQA val set:  55% 3659/6699 [43:04<34:46,  1.46it/s]Evaluating on VQA val set:  55% 3660/6699 [43:05<33:28,  1.51it/s]Evaluating on VQA val set:  55% 3661/6699 [43:05<34:25,  1.47it/s]Evaluating on VQA val set:  55% 3662/6699 [43:06<35:47,  1.41it/s]Evaluating on VQA val set:  55% 3663/6699 [43:07<35:43,  1.42it/s]Evaluating on VQA val set:  55% 3664/6699 [43:07<34:55,  1.45it/s]Evaluating on VQA val set:  55% 3665/6699 [43:08<34:15,  1.48it/s]Evaluating on VQA val set:  55% 3666/6699 [43:09<34:39,  1.46it/s]Evaluating on VQA val set:  55% 3667/6699 [43:09<35:14,  1.43it/s]Evaluating on VQA val set:  55% 3668/6699 [43:10<35:20,  1.43it/s]Evaluating on VQA val set:  55% 3669/6699 [43:11<35:17,  1.43it/s]Evaluating on VQA val set:  55% 3670/6699 [43:11<34:31,  1.46it/s]Evaluating on VQA val set:  55% 3671/6699 [43:12<34:43,  1.45it/s]Evaluating on VQA val set:  55% 3672/6699 [43:13<34:49,  1.45it/s]Evaluating on VQA val set:  55% 3673/6699 [43:14<35:20,  1.43it/s]Evaluating on VQA val set:  55% 3674/6699 [43:14<35:56,  1.40it/s]Evaluating on VQA val set:  55% 3675/6699 [43:15<36:36,  1.38it/s]Evaluating on VQA val set:  55% 3676/6699 [43:16<35:00,  1.44it/s]Evaluating on VQA val set:  55% 3677/6699 [43:16<33:41,  1.50it/s]Evaluating on VQA val set:  55% 3678/6699 [43:17<34:24,  1.46it/s]Evaluating on VQA val set:  55% 3679/6699 [43:18<33:48,  1.49it/s]Evaluating on VQA val set:  55% 3680/6699 [43:18<33:53,  1.48it/s]Evaluating on VQA val set:  55% 3681/6699 [43:19<34:26,  1.46it/s]Evaluating on VQA val set:  55% 3682/6699 [43:20<34:49,  1.44it/s]Evaluating on VQA val set:  55% 3683/6699 [43:21<35:46,  1.41it/s]Evaluating on VQA val set:  55% 3684/6699 [43:21<36:01,  1.39it/s]Evaluating on VQA val set:  55% 3685/6699 [43:22<34:49,  1.44it/s]Evaluating on VQA val set:  55% 3686/6699 [43:23<35:14,  1.42it/s]Evaluating on VQA val set:  55% 3687/6699 [43:23<34:40,  1.45it/s]Evaluating on VQA val set:  55% 3688/6699 [43:24<32:44,  1.53it/s]Evaluating on VQA val set:  55% 3689/6699 [43:25<33:41,  1.49it/s]Evaluating on VQA val set:  55% 3690/6699 [43:25<34:00,  1.47it/s]Evaluating on VQA val set:  55% 3691/6699 [43:26<34:43,  1.44it/s]Evaluating on VQA val set:  55% 3692/6699 [43:27<35:21,  1.42it/s]Evaluating on VQA val set:  55% 3693/6699 [43:27<34:32,  1.45it/s]Evaluating on VQA val set:  55% 3694/6699 [43:28<34:56,  1.43it/s]Evaluating on VQA val set:  55% 3695/6699 [43:29<34:19,  1.46it/s]Evaluating on VQA val set:  55% 3696/6699 [43:29<34:40,  1.44it/s]Evaluating on VQA val set:  55% 3697/6699 [43:30<35:15,  1.42it/s]Evaluating on VQA val set:  55% 3698/6699 [43:31<33:35,  1.49it/s]Evaluating on VQA val set:  55% 3699/6699 [43:32<34:32,  1.45it/s]Evaluating on VQA val set:  55% 3700/6699 [43:32<35:07,  1.42it/s]Evaluating on VQA val set:  55% 3701/6699 [43:33<34:17,  1.46it/s]Evaluating on VQA val set:  55% 3702/6699 [43:34<35:38,  1.40it/s]Evaluating on VQA val set:  55% 3703/6699 [43:34<35:47,  1.40it/s]Evaluating on VQA val set:  55% 3704/6699 [43:35<36:18,  1.37it/s]Evaluating on VQA val set:  55% 3705/6699 [43:36<36:38,  1.36it/s]Evaluating on VQA val set:  55% 3706/6699 [43:37<35:06,  1.42it/s]Evaluating on VQA val set:  55% 3707/6699 [43:37<35:13,  1.42it/s]Evaluating on VQA val set:  55% 3708/6699 [43:38<35:04,  1.42it/s]Evaluating on VQA val set:  55% 3709/6699 [43:39<34:24,  1.45it/s]Evaluating on VQA val set:  55% 3710/6699 [43:39<34:38,  1.44it/s]Evaluating on VQA val set:  55% 3711/6699 [43:40<35:48,  1.39it/s]Evaluating on VQA val set:  55% 3712/6699 [43:41<35:05,  1.42it/s]Evaluating on VQA val set:  55% 3713/6699 [43:42<35:23,  1.41it/s]Evaluating on VQA val set:  55% 3714/6699 [43:42<35:37,  1.40it/s]Evaluating on VQA val set:  55% 3715/6699 [43:43<35:09,  1.41it/s]Evaluating on VQA val set:  55% 3716/6699 [43:44<35:54,  1.38it/s]Evaluating on VQA val set:  55% 3717/6699 [43:44<35:28,  1.40it/s]Evaluating on VQA val set:  56% 3718/6699 [43:45<34:44,  1.43it/s]Evaluating on VQA val set:  56% 3719/6699 [43:46<34:31,  1.44it/s]Evaluating on VQA val set:  56% 3720/6699 [43:46<33:13,  1.49it/s]Evaluating on VQA val set:  56% 3721/6699 [43:47<33:46,  1.47it/s]Evaluating on VQA val set:  56% 3722/6699 [43:48<34:51,  1.42it/s]Evaluating on VQA val set:  56% 3723/6699 [43:48<34:18,  1.45it/s]Evaluating on VQA val set:  56% 3724/6699 [43:49<33:39,  1.47it/s]Evaluating on VQA val set:  56% 3725/6699 [43:50<34:13,  1.45it/s]Evaluating on VQA val set:  56% 3726/6699 [43:51<34:28,  1.44it/s]Evaluating on VQA val set:  56% 3727/6699 [43:51<34:33,  1.43it/s]Evaluating on VQA val set:  56% 3728/6699 [43:52<33:46,  1.47it/s]Evaluating on VQA val set:  56% 3729/6699 [43:53<33:56,  1.46it/s]Evaluating on VQA val set:  56% 3730/6699 [43:53<34:47,  1.42it/s]Evaluating on VQA val set:  56% 3731/6699 [43:54<34:42,  1.43it/s]Evaluating on VQA val set:  56% 3732/6699 [43:55<34:35,  1.43it/s]Evaluating on VQA val set:  56% 3733/6699 [43:55<35:24,  1.40it/s]Evaluating on VQA val set:  56% 3734/6699 [43:56<35:10,  1.40it/s]Evaluating on VQA val set:  56% 3735/6699 [43:57<34:53,  1.42it/s]Evaluating on VQA val set:  56% 3736/6699 [43:58<33:54,  1.46it/s]Evaluating on VQA val set:  56% 3737/6699 [43:58<33:45,  1.46it/s]Evaluating on VQA val set:  56% 3738/6699 [43:59<33:20,  1.48it/s]Evaluating on VQA val set:  56% 3739/6699 [43:59<32:47,  1.50it/s]Evaluating on VQA val set:  56% 3740/6699 [44:00<33:14,  1.48it/s]Evaluating on VQA val set:  56% 3741/6699 [44:01<33:38,  1.47it/s]Evaluating on VQA val set:  56% 3742/6699 [44:02<34:19,  1.44it/s]Evaluating on VQA val set:  56% 3743/6699 [44:02<34:28,  1.43it/s]Evaluating on VQA val set:  56% 3744/6699 [44:03<34:15,  1.44it/s]Evaluating on VQA val set:  56% 3745/6699 [44:04<34:14,  1.44it/s]Evaluating on VQA val set:  56% 3746/6699 [44:04<34:17,  1.44it/s]Evaluating on VQA val set:  56% 3747/6699 [44:05<34:45,  1.42it/s]Evaluating on VQA val set:  56% 3748/6699 [44:06<35:08,  1.40it/s]Evaluating on VQA val set:  56% 3749/6699 [44:07<35:46,  1.37it/s]Evaluating on VQA val set:  56% 3750/6699 [44:07<35:08,  1.40it/s]Evaluating on VQA val set:  56% 3751/6699 [44:08<33:50,  1.45it/s]Evaluating on VQA val set:  56% 3752/6699 [44:09<34:36,  1.42it/s]Evaluating on VQA val set:  56% 3753/6699 [44:09<34:58,  1.40it/s]Evaluating on VQA val set:  56% 3754/6699 [44:10<34:36,  1.42it/s]Evaluating on VQA val set:  56% 3755/6699 [44:11<33:29,  1.47it/s]Evaluating on VQA val set:  56% 3756/6699 [44:11<33:08,  1.48it/s]Evaluating on VQA val set:  56% 3757/6699 [44:12<33:16,  1.47it/s]Evaluating on VQA val set:  56% 3758/6699 [44:13<34:16,  1.43it/s]Evaluating on VQA val set:  56% 3759/6699 [44:13<34:00,  1.44it/s]Evaluating on VQA val set:  56% 3760/6699 [44:14<33:48,  1.45it/s]Evaluating on VQA val set:  56% 3761/6699 [44:15<33:40,  1.45it/s]Evaluating on VQA val set:  56% 3762/6699 [44:16<33:44,  1.45it/s]Evaluating on VQA val set:  56% 3763/6699 [44:16<34:14,  1.43it/s]Evaluating on VQA val set:  56% 3764/6699 [44:17<34:36,  1.41it/s]Evaluating on VQA val set:  56% 3765/6699 [44:18<33:59,  1.44it/s]Evaluating on VQA val set:  56% 3766/6699 [44:18<33:48,  1.45it/s]Evaluating on VQA val set:  56% 3767/6699 [44:19<34:21,  1.42it/s]Evaluating on VQA val set:  56% 3768/6699 [44:20<33:25,  1.46it/s]Evaluating on VQA val set:  56% 3769/6699 [44:20<33:47,  1.45it/s]Evaluating on VQA val set:  56% 3770/6699 [44:21<33:38,  1.45it/s]Evaluating on VQA val set:  56% 3771/6699 [44:22<34:11,  1.43it/s]Evaluating on VQA val set:  56% 3772/6699 [44:23<34:26,  1.42it/s]Evaluating on VQA val set:  56% 3773/6699 [44:23<34:16,  1.42it/s]Evaluating on VQA val set:  56% 3774/6699 [44:24<33:01,  1.48it/s]Evaluating on VQA val set:  56% 3775/6699 [44:25<32:26,  1.50it/s]Evaluating on VQA val set:  56% 3776/6699 [44:25<32:27,  1.50it/s]Evaluating on VQA val set:  56% 3777/6699 [44:26<33:17,  1.46it/s]Evaluating on VQA val set:  56% 3778/6699 [44:27<33:06,  1.47it/s]Evaluating on VQA val set:  56% 3779/6699 [44:27<33:43,  1.44it/s]Evaluating on VQA val set:  56% 3780/6699 [44:28<33:33,  1.45it/s]Evaluating on VQA val set:  56% 3781/6699 [44:29<33:47,  1.44it/s]Evaluating on VQA val set:  56% 3782/6699 [44:29<33:56,  1.43it/s]Evaluating on VQA val set:  56% 3783/6699 [44:30<33:21,  1.46it/s]Evaluating on VQA val set:  56% 3784/6699 [44:31<31:29,  1.54it/s]Evaluating on VQA val set:  57% 3785/6699 [44:31<32:31,  1.49it/s]Evaluating on VQA val set:  57% 3786/6699 [44:32<33:15,  1.46it/s]Evaluating on VQA val set:  57% 3787/6699 [44:33<34:01,  1.43it/s]Evaluating on VQA val set:  57% 3788/6699 [44:33<33:48,  1.43it/s]Evaluating on VQA val set:  57% 3789/6699 [44:34<34:01,  1.43it/s]Evaluating on VQA val set:  57% 3790/6699 [44:35<33:48,  1.43it/s]Evaluating on VQA val set:  57% 3791/6699 [44:36<33:56,  1.43it/s]Evaluating on VQA val set:  57% 3792/6699 [44:36<33:42,  1.44it/s]Evaluating on VQA val set:  57% 3793/6699 [44:37<32:58,  1.47it/s]Evaluating on VQA val set:  57% 3794/6699 [44:38<32:54,  1.47it/s]Evaluating on VQA val set:  57% 3795/6699 [44:38<33:23,  1.45it/s]Evaluating on VQA val set:  57% 3796/6699 [44:39<33:28,  1.45it/s]Evaluating on VQA val set:  57% 3797/6699 [44:40<33:56,  1.43it/s]Evaluating on VQA val set:  57% 3798/6699 [44:40<33:16,  1.45it/s]Evaluating on VQA val set:  57% 3799/6699 [44:41<33:10,  1.46it/s]Evaluating on VQA val set:  57% 3800/6699 [44:42<33:34,  1.44it/s]Evaluating on VQA val set:  57% 3801/6699 [44:43<34:37,  1.39it/s]Evaluating on VQA val set:  57% 3802/6699 [44:43<34:28,  1.40it/s]Evaluating on VQA val set:  57% 3803/6699 [44:44<34:59,  1.38it/s]Evaluating on VQA val set:  57% 3804/6699 [44:45<34:39,  1.39it/s]Evaluating on VQA val set:  57% 3805/6699 [44:45<35:22,  1.36it/s]Evaluating on VQA val set:  57% 3806/6699 [44:46<35:29,  1.36it/s]Evaluating on VQA val set:  57% 3807/6699 [44:47<35:08,  1.37it/s]Evaluating on VQA val set:  57% 3808/6699 [44:48<35:25,  1.36it/s]Evaluating on VQA val set:  57% 3809/6699 [44:48<35:38,  1.35it/s]Evaluating on VQA val set:  57% 3810/6699 [44:49<34:18,  1.40it/s]Evaluating on VQA val set:  57% 3811/6699 [44:50<35:16,  1.36it/s]Evaluating on VQA val set:  57% 3812/6699 [44:51<34:37,  1.39it/s]Evaluating on VQA val set:  57% 3813/6699 [44:51<33:47,  1.42it/s]Evaluating on VQA val set:  57% 3814/6699 [44:52<34:13,  1.40it/s]Evaluating on VQA val set:  57% 3815/6699 [44:53<34:16,  1.40it/s]Evaluating on VQA val set:  57% 3816/6699 [44:53<34:29,  1.39it/s]Evaluating on VQA val set:  57% 3817/6699 [44:54<34:12,  1.40it/s]Evaluating on VQA val set:  57% 3818/6699 [44:55<33:53,  1.42it/s]Evaluating on VQA val set:  57% 3819/6699 [44:55<33:21,  1.44it/s]Evaluating on VQA val set:  57% 3820/6699 [44:56<32:54,  1.46it/s]Evaluating on VQA val set:  57% 3821/6699 [44:57<32:33,  1.47it/s]Evaluating on VQA val set:  57% 3822/6699 [44:58<33:14,  1.44it/s]Evaluating on VQA val set:  57% 3823/6699 [44:58<34:26,  1.39it/s]Evaluating on VQA val set:  57% 3824/6699 [44:59<33:38,  1.42it/s]Evaluating on VQA val set:  57% 3825/6699 [45:00<33:05,  1.45it/s]Evaluating on VQA val set:  57% 3826/6699 [45:00<32:43,  1.46it/s]Evaluating on VQA val set:  57% 3827/6699 [45:01<32:35,  1.47it/s]Evaluating on VQA val set:  57% 3828/6699 [45:02<32:49,  1.46it/s]Evaluating on VQA val set:  57% 3829/6699 [45:02<33:26,  1.43it/s]Evaluating on VQA val set:  57% 3830/6699 [45:03<34:13,  1.40it/s]Evaluating on VQA val set:  57% 3831/6699 [45:04<34:13,  1.40it/s]Evaluating on VQA val set:  57% 3832/6699 [45:04<33:11,  1.44it/s]Evaluating on VQA val set:  57% 3833/6699 [45:05<31:40,  1.51it/s]Evaluating on VQA val set:  57% 3834/6699 [45:06<32:24,  1.47it/s]Evaluating on VQA val set:  57% 3835/6699 [45:06<32:28,  1.47it/s]Evaluating on VQA val set:  57% 3836/6699 [45:07<31:53,  1.50it/s]Evaluating on VQA val set:  57% 3837/6699 [45:08<33:05,  1.44it/s]Evaluating on VQA val set:  57% 3838/6699 [45:08<31:10,  1.53it/s]Evaluating on VQA val set:  57% 3839/6699 [45:09<29:56,  1.59it/s]Evaluating on VQA val set:  57% 3840/6699 [45:10<31:35,  1.51it/s]Evaluating on VQA val set:  57% 3841/6699 [45:10<31:44,  1.50it/s]Evaluating on VQA val set:  57% 3842/6699 [45:11<32:07,  1.48it/s]Evaluating on VQA val set:  57% 3843/6699 [45:12<32:16,  1.48it/s]Evaluating on VQA val set:  57% 3844/6699 [45:12<31:50,  1.49it/s]Evaluating on VQA val set:  57% 3845/6699 [45:13<33:05,  1.44it/s]Evaluating on VQA val set:  57% 3846/6699 [45:14<33:20,  1.43it/s]Evaluating on VQA val set:  57% 3847/6699 [45:15<33:14,  1.43it/s]Evaluating on VQA val set:  57% 3848/6699 [45:15<33:34,  1.42it/s]Evaluating on VQA val set:  57% 3849/6699 [45:16<33:12,  1.43it/s]Evaluating on VQA val set:  57% 3850/6699 [45:17<32:36,  1.46it/s]Evaluating on VQA val set:  57% 3851/6699 [45:17<32:58,  1.44it/s]Evaluating on VQA val set:  58% 3852/6699 [45:18<34:08,  1.39it/s]Evaluating on VQA val set:  58% 3853/6699 [45:19<34:29,  1.37it/s]Evaluating on VQA val set:  58% 3854/6699 [45:20<34:02,  1.39it/s]Evaluating on VQA val set:  58% 3855/6699 [45:20<34:04,  1.39it/s]Evaluating on VQA val set:  58% 3856/6699 [45:21<33:33,  1.41it/s]Evaluating on VQA val set:  58% 3857/6699 [45:22<32:46,  1.45it/s]Evaluating on VQA val set:  58% 3858/6699 [45:22<33:25,  1.42it/s]Evaluating on VQA val set:  58% 3859/6699 [45:23<33:04,  1.43it/s]Evaluating on VQA val set:  58% 3860/6699 [45:24<32:53,  1.44it/s]Evaluating on VQA val set:  58% 3861/6699 [45:24<33:06,  1.43it/s]Evaluating on VQA val set:  58% 3862/6699 [45:25<33:10,  1.43it/s]Evaluating on VQA val set:  58% 3863/6699 [45:26<32:57,  1.43it/s]Evaluating on VQA val set:  58% 3864/6699 [45:27<33:15,  1.42it/s]Evaluating on VQA val set:  58% 3865/6699 [45:27<32:26,  1.46it/s]Evaluating on VQA val set:  58% 3866/6699 [45:28<31:22,  1.51it/s]Evaluating on VQA val set:  58% 3867/6699 [45:29<32:22,  1.46it/s]Evaluating on VQA val set:  58% 3868/6699 [45:29<32:39,  1.44it/s]Evaluating on VQA val set:  58% 3869/6699 [45:30<33:40,  1.40it/s]Evaluating on VQA val set:  58% 3870/6699 [45:31<33:22,  1.41it/s]Evaluating on VQA val set:  58% 3871/6699 [45:31<33:09,  1.42it/s]Evaluating on VQA val set:  58% 3872/6699 [45:32<33:50,  1.39it/s]Evaluating on VQA val set:  58% 3873/6699 [45:33<33:04,  1.42it/s]Evaluating on VQA val set:  58% 3874/6699 [45:34<32:53,  1.43it/s]Evaluating on VQA val set:  58% 3875/6699 [45:34<32:24,  1.45it/s]Evaluating on VQA val set:  58% 3876/6699 [45:35<31:36,  1.49it/s]Evaluating on VQA val set:  58% 3877/6699 [45:36<33:09,  1.42it/s]Evaluating on VQA val set:  58% 3878/6699 [45:36<33:33,  1.40it/s]Evaluating on VQA val set:  58% 3879/6699 [45:37<33:25,  1.41it/s]Evaluating on VQA val set:  58% 3880/6699 [45:38<34:12,  1.37it/s]Evaluating on VQA val set:  58% 3881/6699 [45:39<33:50,  1.39it/s]Evaluating on VQA val set:  58% 3882/6699 [45:39<33:43,  1.39it/s]Evaluating on VQA val set:  58% 3883/6699 [45:40<33:17,  1.41it/s]Evaluating on VQA val set:  58% 3884/6699 [45:41<33:07,  1.42it/s]Evaluating on VQA val set:  58% 3885/6699 [45:41<33:06,  1.42it/s]Evaluating on VQA val set:  58% 3886/6699 [45:42<32:44,  1.43it/s]Evaluating on VQA val set:  58% 3887/6699 [45:43<32:39,  1.44it/s]Evaluating on VQA val set:  58% 3888/6699 [45:43<33:15,  1.41it/s]Evaluating on VQA val set:  58% 3889/6699 [45:44<33:01,  1.42it/s]Evaluating on VQA val set:  58% 3890/6699 [45:45<32:55,  1.42it/s]Evaluating on VQA val set:  58% 3891/6699 [45:46<32:13,  1.45it/s]Evaluating on VQA val set:  58% 3892/6699 [45:46<31:37,  1.48it/s]Evaluating on VQA val set:  58% 3893/6699 [45:47<31:38,  1.48it/s]Evaluating on VQA val set:  58% 3894/6699 [45:48<32:12,  1.45it/s]Evaluating on VQA val set:  58% 3895/6699 [45:48<32:54,  1.42it/s]Evaluating on VQA val set:  58% 3896/6699 [45:49<32:33,  1.43it/s]Evaluating on VQA val set:  58% 3897/6699 [45:50<33:18,  1.40it/s]Evaluating on VQA val set:  58% 3898/6699 [45:51<34:05,  1.37it/s]Evaluating on VQA val set:  58% 3899/6699 [45:51<32:33,  1.43it/s]Evaluating on VQA val set:  58% 3900/6699 [45:52<33:17,  1.40it/s]Evaluating on VQA val set:  58% 3901/6699 [45:53<33:33,  1.39it/s]Evaluating on VQA val set:  58% 3902/6699 [45:53<33:25,  1.39it/s]Evaluating on VQA val set:  58% 3903/6699 [45:54<33:11,  1.40it/s]Evaluating on VQA val set:  58% 3904/6699 [45:55<33:06,  1.41it/s]Evaluating on VQA val set:  58% 3905/6699 [45:55<32:45,  1.42it/s]Evaluating on VQA val set:  58% 3906/6699 [45:56<32:48,  1.42it/s]Evaluating on VQA val set:  58% 3907/6699 [45:57<33:30,  1.39it/s]Evaluating on VQA val set:  58% 3908/6699 [45:58<33:02,  1.41it/s]Evaluating on VQA val set:  58% 3909/6699 [45:58<32:54,  1.41it/s]Evaluating on VQA val set:  58% 3910/6699 [45:59<33:46,  1.38it/s]Evaluating on VQA val set:  58% 3911/6699 [46:00<33:04,  1.40it/s]Evaluating on VQA val set:  58% 3912/6699 [46:00<33:17,  1.40it/s]Evaluating on VQA val set:  58% 3913/6699 [46:01<32:19,  1.44it/s]Evaluating on VQA val set:  58% 3914/6699 [46:02<31:00,  1.50it/s]Evaluating on VQA val set:  58% 3915/6699 [46:02<32:07,  1.44it/s]Evaluating on VQA val set:  58% 3916/6699 [46:03<31:42,  1.46it/s]Evaluating on VQA val set:  58% 3917/6699 [46:04<32:24,  1.43it/s]Evaluating on VQA val set:  58% 3918/6699 [46:05<33:14,  1.39it/s]Evaluating on VQA val set:  59% 3919/6699 [46:05<33:11,  1.40it/s]Evaluating on VQA val set:  59% 3920/6699 [46:06<32:26,  1.43it/s]Evaluating on VQA val set:  59% 3921/6699 [46:07<32:30,  1.42it/s]Evaluating on VQA val set:  59% 3922/6699 [46:07<33:01,  1.40it/s]Evaluating on VQA val set:  59% 3923/6699 [46:08<31:46,  1.46it/s]Evaluating on VQA val set:  59% 3924/6699 [46:09<31:47,  1.45it/s]Evaluating on VQA val set:  59% 3925/6699 [46:10<33:23,  1.38it/s]Evaluating on VQA val set:  59% 3926/6699 [46:10<33:56,  1.36it/s]Evaluating on VQA val set:  59% 3927/6699 [46:11<32:56,  1.40it/s]Evaluating on VQA val set:  59% 3928/6699 [46:12<33:18,  1.39it/s]Evaluating on VQA val set:  59% 3929/6699 [46:12<31:59,  1.44it/s]Evaluating on VQA val set:  59% 3930/6699 [46:13<31:40,  1.46it/s]Evaluating on VQA val set:  59% 3931/6699 [46:14<31:46,  1.45it/s]Evaluating on VQA val set:  59% 3932/6699 [46:14<32:07,  1.44it/s]Evaluating on VQA val set:  59% 3933/6699 [46:15<32:26,  1.42it/s]Evaluating on VQA val set:  59% 3934/6699 [46:16<32:22,  1.42it/s]Evaluating on VQA val set:  59% 3935/6699 [46:17<33:05,  1.39it/s]Evaluating on VQA val set:  59% 3936/6699 [46:17<33:45,  1.36it/s]Evaluating on VQA val set:  59% 3937/6699 [46:18<33:51,  1.36it/s]Evaluating on VQA val set:  59% 3938/6699 [46:19<33:25,  1.38it/s]Evaluating on VQA val set:  59% 3939/6699 [46:19<32:03,  1.44it/s]Evaluating on VQA val set:  59% 3940/6699 [46:20<32:27,  1.42it/s]Evaluating on VQA val set:  59% 3941/6699 [46:21<32:20,  1.42it/s]Evaluating on VQA val set:  59% 3942/6699 [46:22<32:22,  1.42it/s]Evaluating on VQA val set:  59% 3943/6699 [46:22<31:42,  1.45it/s]Evaluating on VQA val set:  59% 3944/6699 [46:23<32:36,  1.41it/s]Evaluating on VQA val set:  59% 3945/6699 [46:24<32:47,  1.40it/s]Evaluating on VQA val set:  59% 3946/6699 [46:24<32:51,  1.40it/s]Evaluating on VQA val set:  59% 3947/6699 [46:25<31:54,  1.44it/s]Evaluating on VQA val set:  59% 3948/6699 [46:26<31:52,  1.44it/s]Evaluating on VQA val set:  59% 3949/6699 [46:27<32:25,  1.41it/s]Evaluating on VQA val set:  59% 3950/6699 [46:27<32:27,  1.41it/s]Evaluating on VQA val set:  59% 3951/6699 [46:28<32:29,  1.41it/s]Evaluating on VQA val set:  59% 3952/6699 [46:29<33:18,  1.37it/s]Evaluating on VQA val set:  59% 3953/6699 [46:29<33:14,  1.38it/s]Evaluating on VQA val set:  59% 3954/6699 [46:30<33:09,  1.38it/s]Evaluating on VQA val set:  59% 3955/6699 [46:31<32:55,  1.39it/s]Evaluating on VQA val set:  59% 3956/6699 [46:32<33:00,  1.38it/s]Evaluating on VQA val set:  59% 3957/6699 [46:32<32:26,  1.41it/s]Evaluating on VQA val set:  59% 3958/6699 [46:33<32:22,  1.41it/s]Evaluating on VQA val set:  59% 3959/6699 [46:34<32:01,  1.43it/s]Evaluating on VQA val set:  59% 3960/6699 [46:34<32:15,  1.41it/s]Evaluating on VQA val set:  59% 3961/6699 [46:35<32:21,  1.41it/s]Evaluating on VQA val set:  59% 3962/6699 [46:36<32:19,  1.41it/s]Evaluating on VQA val set:  59% 3963/6699 [46:36<32:20,  1.41it/s]Evaluating on VQA val set:  59% 3964/6699 [46:37<31:09,  1.46it/s]Evaluating on VQA val set:  59% 3965/6699 [46:38<30:14,  1.51it/s]Evaluating on VQA val set:  59% 3966/6699 [46:38<30:47,  1.48it/s]Evaluating on VQA val set:  59% 3967/6699 [46:39<30:34,  1.49it/s]Evaluating on VQA val set:  59% 3968/6699 [46:40<30:54,  1.47it/s]Evaluating on VQA val set:  59% 3969/6699 [46:41<31:14,  1.46it/s]Evaluating on VQA val set:  59% 3970/6699 [46:41<30:39,  1.48it/s]Evaluating on VQA val set:  59% 3971/6699 [46:42<30:56,  1.47it/s]Evaluating on VQA val set:  59% 3972/6699 [46:43<30:42,  1.48it/s]Evaluating on VQA val set:  59% 3973/6699 [46:43<30:38,  1.48it/s]Evaluating on VQA val set:  59% 3974/6699 [46:44<30:16,  1.50it/s]Evaluating on VQA val set:  59% 3975/6699 [46:44<30:05,  1.51it/s]Evaluating on VQA val set:  59% 3976/6699 [46:45<30:37,  1.48it/s]Evaluating on VQA val set:  59% 3977/6699 [46:46<30:32,  1.49it/s]Evaluating on VQA val set:  59% 3978/6699 [46:47<31:03,  1.46it/s]Evaluating on VQA val set:  59% 3979/6699 [46:47<31:10,  1.45it/s]Evaluating on VQA val set:  59% 3980/6699 [46:48<31:39,  1.43it/s]Evaluating on VQA val set:  59% 3981/6699 [46:49<32:07,  1.41it/s]Evaluating on VQA val set:  59% 3982/6699 [46:49<30:48,  1.47it/s]Evaluating on VQA val set:  59% 3983/6699 [46:50<30:26,  1.49it/s]Evaluating on VQA val set:  59% 3984/6699 [46:51<31:21,  1.44it/s]Evaluating on VQA val set:  59% 3985/6699 [46:51<30:53,  1.46it/s]Evaluating on VQA val set:  60% 3986/6699 [46:52<31:07,  1.45it/s]Evaluating on VQA val set:  60% 3987/6699 [46:53<30:01,  1.51it/s]Evaluating on VQA val set:  60% 3988/6699 [46:53<30:35,  1.48it/s]Evaluating on VQA val set:  60% 3989/6699 [46:54<30:59,  1.46it/s]Evaluating on VQA val set:  60% 3990/6699 [46:55<31:37,  1.43it/s]Evaluating on VQA val set:  60% 3991/6699 [46:56<31:15,  1.44it/s]Evaluating on VQA val set:  60% 3992/6699 [46:56<31:05,  1.45it/s]Evaluating on VQA val set:  60% 3993/6699 [46:57<30:50,  1.46it/s]Evaluating on VQA val set:  60% 3994/6699 [46:58<30:54,  1.46it/s]Evaluating on VQA val set:  60% 3995/6699 [46:58<30:48,  1.46it/s]Evaluating on VQA val set:  60% 3996/6699 [46:59<29:49,  1.51it/s]Evaluating on VQA val set:  60% 3997/6699 [47:00<30:26,  1.48it/s]Evaluating on VQA val set:  60% 3998/6699 [47:00<30:13,  1.49it/s]Evaluating on VQA val set:  60% 3999/6699 [47:01<30:52,  1.46it/s]Evaluating on VQA val set:  60% 4000/6699 [47:02<31:40,  1.42it/s]Evaluating on VQA val set:  60% 4001/6699 [47:02<32:21,  1.39it/s]Evaluating on VQA val set:  60% 4002/6699 [47:03<32:10,  1.40it/s]Evaluating on VQA val set:  60% 4003/6699 [47:04<32:33,  1.38it/s]Evaluating on VQA val set:  60% 4004/6699 [47:05<31:46,  1.41it/s]Evaluating on VQA val set:  60% 4005/6699 [47:05<31:52,  1.41it/s]Evaluating on VQA val set:  60% 4006/6699 [47:06<31:03,  1.45it/s]Evaluating on VQA val set:  60% 4007/6699 [47:06<29:06,  1.54it/s]Evaluating on VQA val set:  60% 4008/6699 [47:07<31:01,  1.45it/s]Evaluating on VQA val set:  60% 4009/6699 [47:08<31:19,  1.43it/s]Evaluating on VQA val set:  60% 4010/6699 [47:09<31:43,  1.41it/s]Evaluating on VQA val set:  60% 4011/6699 [47:09<30:49,  1.45it/s]Evaluating on VQA val set:  60% 4012/6699 [47:10<31:09,  1.44it/s]Evaluating on VQA val set:  60% 4013/6699 [47:11<30:57,  1.45it/s]Evaluating on VQA val set:  60% 4014/6699 [47:11<30:20,  1.47it/s]Evaluating on VQA val set:  60% 4015/6699 [47:12<30:13,  1.48it/s]Evaluating on VQA val set:  60% 4016/6699 [47:13<30:18,  1.48it/s]Evaluating on VQA val set:  60% 4017/6699 [47:13<30:15,  1.48it/s]Evaluating on VQA val set:  60% 4018/6699 [47:14<30:15,  1.48it/s]Evaluating on VQA val set:  60% 4019/6699 [47:15<30:39,  1.46it/s]Evaluating on VQA val set:  60% 4020/6699 [47:16<31:08,  1.43it/s]Evaluating on VQA val set:  60% 4021/6699 [47:16<31:46,  1.40it/s]Evaluating on VQA val set:  60% 4022/6699 [47:17<31:04,  1.44it/s]Evaluating on VQA val set:  60% 4023/6699 [47:18<31:01,  1.44it/s]Evaluating on VQA val set:  60% 4024/6699 [47:18<30:23,  1.47it/s]Evaluating on VQA val set:  60% 4025/6699 [47:19<31:10,  1.43it/s]Evaluating on VQA val set:  60% 4026/6699 [47:20<30:39,  1.45it/s]Evaluating on VQA val set:  60% 4027/6699 [47:20<31:31,  1.41it/s]Evaluating on VQA val set:  60% 4028/6699 [47:21<32:34,  1.37it/s]Evaluating on VQA val set:  60% 4029/6699 [47:22<33:10,  1.34it/s]Evaluating on VQA val set:  60% 4030/6699 [47:23<33:08,  1.34it/s]Evaluating on VQA val set:  60% 4031/6699 [47:23<32:39,  1.36it/s]Evaluating on VQA val set:  60% 4032/6699 [47:24<31:41,  1.40it/s]Evaluating on VQA val set:  60% 4033/6699 [47:25<31:01,  1.43it/s]Evaluating on VQA val set:  60% 4034/6699 [47:25<30:56,  1.44it/s]Evaluating on VQA val set:  60% 4035/6699 [47:26<30:44,  1.44it/s]Evaluating on VQA val set:  60% 4036/6699 [47:27<32:08,  1.38it/s]Evaluating on VQA val set:  60% 4037/6699 [47:28<30:48,  1.44it/s]Evaluating on VQA val set:  60% 4038/6699 [47:28<30:54,  1.43it/s]Evaluating on VQA val set:  60% 4039/6699 [47:29<31:18,  1.42it/s]Evaluating on VQA val set:  60% 4040/6699 [47:30<31:24,  1.41it/s]Evaluating on VQA val set:  60% 4041/6699 [47:30<31:20,  1.41it/s]Evaluating on VQA val set:  60% 4042/6699 [47:31<31:53,  1.39it/s]Evaluating on VQA val set:  60% 4043/6699 [47:32<31:47,  1.39it/s]Evaluating on VQA val set:  60% 4044/6699 [47:33<31:02,  1.43it/s]Evaluating on VQA val set:  60% 4045/6699 [47:33<30:42,  1.44it/s]Evaluating on VQA val set:  60% 4046/6699 [47:34<29:39,  1.49it/s]Evaluating on VQA val set:  60% 4047/6699 [47:34<28:57,  1.53it/s]Evaluating on VQA val set:  60% 4048/6699 [47:35<30:19,  1.46it/s]Evaluating on VQA val set:  60% 4049/6699 [47:36<30:38,  1.44it/s]Evaluating on VQA val set:  60% 4050/6699 [47:37<30:38,  1.44it/s]Evaluating on VQA val set:  60% 4051/6699 [47:37<31:18,  1.41it/s]Evaluating on VQA val set:  60% 4052/6699 [47:38<29:53,  1.48it/s]Evaluating on VQA val set:  61% 4053/6699 [47:39<29:52,  1.48it/s]Evaluating on VQA val set:  61% 4054/6699 [47:39<30:25,  1.45it/s]Evaluating on VQA val set:  61% 4055/6699 [47:40<31:09,  1.41it/s]Evaluating on VQA val set:  61% 4056/6699 [47:41<31:29,  1.40it/s]Evaluating on VQA val set:  61% 4057/6699 [47:41<30:11,  1.46it/s]Evaluating on VQA val set:  61% 4058/6699 [47:42<31:16,  1.41it/s]Evaluating on VQA val set:  61% 4059/6699 [47:43<32:24,  1.36it/s]Evaluating on VQA val set:  61% 4060/6699 [47:44<31:22,  1.40it/s]Evaluating on VQA val set:  61% 4061/6699 [47:44<30:56,  1.42it/s]Evaluating on VQA val set:  61% 4062/6699 [47:45<31:05,  1.41it/s]Evaluating on VQA val set:  61% 4063/6699 [47:46<30:55,  1.42it/s]Evaluating on VQA val set:  61% 4064/6699 [47:47<31:08,  1.41it/s]Evaluating on VQA val set:  61% 4065/6699 [47:47<31:08,  1.41it/s]Evaluating on VQA val set:  61% 4066/6699 [47:48<31:30,  1.39it/s]Evaluating on VQA val set:  61% 4067/6699 [47:49<31:38,  1.39it/s]Evaluating on VQA val set:  61% 4068/6699 [47:49<30:59,  1.42it/s]Evaluating on VQA val set:  61% 4069/6699 [47:50<29:00,  1.51it/s]Evaluating on VQA val set:  61% 4070/6699 [47:51<29:33,  1.48it/s]Evaluating on VQA val set:  61% 4071/6699 [47:51<29:37,  1.48it/s]Evaluating on VQA val set:  61% 4072/6699 [47:52<29:06,  1.50it/s]Evaluating on VQA val set:  61% 4073/6699 [47:53<30:08,  1.45it/s]Evaluating on VQA val set:  61% 4074/6699 [47:53<30:05,  1.45it/s]Evaluating on VQA val set:  61% 4075/6699 [47:54<30:08,  1.45it/s]Evaluating on VQA val set:  61% 4076/6699 [47:55<30:27,  1.43it/s]Evaluating on VQA val set:  61% 4077/6699 [47:56<30:41,  1.42it/s]Evaluating on VQA val set:  61% 4078/6699 [47:56<30:43,  1.42it/s]Evaluating on VQA val set:  61% 4079/6699 [47:57<30:32,  1.43it/s]Evaluating on VQA val set:  61% 4080/6699 [47:58<29:55,  1.46it/s]Evaluating on VQA val set:  61% 4081/6699 [47:58<29:45,  1.47it/s]Evaluating on VQA val set:  61% 4082/6699 [47:59<29:35,  1.47it/s]Evaluating on VQA val set:  61% 4083/6699 [48:00<29:46,  1.46it/s]Evaluating on VQA val set:  61% 4084/6699 [48:00<30:22,  1.44it/s]Evaluating on VQA val set:  61% 4085/6699 [48:01<29:28,  1.48it/s]Evaluating on VQA val set:  61% 4086/6699 [48:02<29:38,  1.47it/s]Evaluating on VQA val set:  61% 4087/6699 [48:02<30:45,  1.42it/s]Evaluating on VQA val set:  61% 4088/6699 [48:03<30:54,  1.41it/s]Evaluating on VQA val set:  61% 4089/6699 [48:04<30:09,  1.44it/s]Evaluating on VQA val set:  61% 4090/6699 [48:04<30:26,  1.43it/s]Evaluating on VQA val set:  61% 4091/6699 [48:05<30:30,  1.42it/s]Evaluating on VQA val set:  61% 4092/6699 [48:06<29:40,  1.46it/s]Evaluating on VQA val set:  61% 4093/6699 [48:06<29:02,  1.50it/s]Evaluating on VQA val set:  61% 4094/6699 [48:07<29:55,  1.45it/s]Evaluating on VQA val set:  61% 4095/6699 [48:08<30:30,  1.42it/s]Evaluating on VQA val set:  61% 4096/6699 [48:09<29:47,  1.46it/s]Evaluating on VQA val set:  61% 4097/6699 [48:09<29:21,  1.48it/s]Evaluating on VQA val set:  61% 4098/6699 [48:10<29:32,  1.47it/s]Evaluating on VQA val set:  61% 4099/6699 [48:11<29:03,  1.49it/s]Evaluating on VQA val set:  61% 4100/6699 [48:11<29:52,  1.45it/s]Evaluating on VQA val set:  61% 4101/6699 [48:12<30:37,  1.41it/s]Evaluating on VQA val set:  61% 4102/6699 [48:13<30:05,  1.44it/s]Evaluating on VQA val set:  61% 4103/6699 [48:13<29:55,  1.45it/s]Evaluating on VQA val set:  61% 4104/6699 [48:14<30:41,  1.41it/s]Evaluating on VQA val set:  61% 4105/6699 [48:15<29:50,  1.45it/s]Evaluating on VQA val set:  61% 4106/6699 [48:16<30:05,  1.44it/s]Evaluating on VQA val set:  61% 4107/6699 [48:16<30:25,  1.42it/s]Evaluating on VQA val set:  61% 4108/6699 [48:17<30:03,  1.44it/s]Evaluating on VQA val set:  61% 4109/6699 [48:18<29:49,  1.45it/s]Evaluating on VQA val set:  61% 4110/6699 [48:18<30:28,  1.42it/s]Evaluating on VQA val set:  61% 4111/6699 [48:19<29:48,  1.45it/s]Evaluating on VQA val set:  61% 4112/6699 [48:20<30:05,  1.43it/s]Evaluating on VQA val set:  61% 4113/6699 [48:20<30:07,  1.43it/s]Evaluating on VQA val set:  61% 4114/6699 [48:21<30:23,  1.42it/s]Evaluating on VQA val set:  61% 4115/6699 [48:22<29:31,  1.46it/s]Evaluating on VQA val set:  61% 4116/6699 [48:22<29:58,  1.44it/s]Evaluating on VQA val set:  61% 4117/6699 [48:23<30:34,  1.41it/s]Evaluating on VQA val set:  61% 4118/6699 [48:24<30:34,  1.41it/s]Evaluating on VQA val set:  61% 4119/6699 [48:25<29:34,  1.45it/s]Evaluating on VQA val set:  62% 4120/6699 [48:25<29:20,  1.46it/s]Evaluating on VQA val set:  62% 4121/6699 [48:26<28:56,  1.48it/s]Evaluating on VQA val set:  62% 4122/6699 [48:27<28:30,  1.51it/s]Evaluating on VQA val set:  62% 4123/6699 [48:27<29:39,  1.45it/s]Evaluating on VQA val set:  62% 4124/6699 [48:28<29:14,  1.47it/s]Evaluating on VQA val set:  62% 4125/6699 [48:29<29:14,  1.47it/s]Evaluating on VQA val set:  62% 4126/6699 [48:29<30:15,  1.42it/s]Evaluating on VQA val set:  62% 4127/6699 [48:30<30:16,  1.42it/s]Evaluating on VQA val set:  62% 4128/6699 [48:31<30:49,  1.39it/s]Evaluating on VQA val set:  62% 4129/6699 [48:32<30:00,  1.43it/s]Evaluating on VQA val set:  62% 4130/6699 [48:32<29:16,  1.46it/s]Evaluating on VQA val set:  62% 4131/6699 [48:33<29:42,  1.44it/s]Evaluating on VQA val set:  62% 4132/6699 [48:34<30:07,  1.42it/s]Evaluating on VQA val set:  62% 4133/6699 [48:34<30:17,  1.41it/s]Evaluating on VQA val set:  62% 4134/6699 [48:35<29:57,  1.43it/s]Evaluating on VQA val set:  62% 4135/6699 [48:36<29:37,  1.44it/s]Evaluating on VQA val set:  62% 4136/6699 [48:36<29:04,  1.47it/s]Evaluating on VQA val set:  62% 4137/6699 [48:37<28:44,  1.49it/s]Evaluating on VQA val set:  62% 4138/6699 [48:38<29:05,  1.47it/s]Evaluating on VQA val set:  62% 4139/6699 [48:38<29:09,  1.46it/s]Evaluating on VQA val set:  62% 4140/6699 [48:39<29:42,  1.44it/s]Evaluating on VQA val set:  62% 4141/6699 [48:40<29:45,  1.43it/s]Evaluating on VQA val set:  62% 4142/6699 [48:41<29:55,  1.42it/s]Evaluating on VQA val set:  62% 4143/6699 [48:41<29:28,  1.45it/s]Evaluating on VQA val set:  62% 4144/6699 [48:42<28:45,  1.48it/s]Evaluating on VQA val set:  62% 4145/6699 [48:42<28:20,  1.50it/s]Evaluating on VQA val set:  62% 4146/6699 [48:43<29:08,  1.46it/s]Evaluating on VQA val set:  62% 4147/6699 [48:44<30:21,  1.40it/s]Evaluating on VQA val set:  62% 4148/6699 [48:45<30:51,  1.38it/s]Evaluating on VQA val set:  62% 4149/6699 [48:45<30:46,  1.38it/s]Evaluating on VQA val set:  62% 4150/6699 [48:46<29:41,  1.43it/s]Evaluating on VQA val set:  62% 4151/6699 [48:47<28:47,  1.48it/s]Evaluating on VQA val set:  62% 4152/6699 [48:47<28:52,  1.47it/s]Evaluating on VQA val set:  62% 4153/6699 [48:48<26:34,  1.60it/s]Evaluating on VQA val set:  62% 4154/6699 [48:49<27:30,  1.54it/s]Evaluating on VQA val set:  62% 4155/6699 [48:49<27:47,  1.53it/s]Evaluating on VQA val set:  62% 4156/6699 [48:50<28:52,  1.47it/s]Evaluating on VQA val set:  62% 4157/6699 [48:51<28:17,  1.50it/s]Evaluating on VQA val set:  62% 4158/6699 [48:51<28:46,  1.47it/s]Evaluating on VQA val set:  62% 4159/6699 [48:52<28:06,  1.51it/s]Evaluating on VQA val set:  62% 4160/6699 [48:53<28:54,  1.46it/s]Evaluating on VQA val set:  62% 4161/6699 [48:53<29:05,  1.45it/s]Evaluating on VQA val set:  62% 4162/6699 [48:54<29:32,  1.43it/s]Evaluating on VQA val set:  62% 4163/6699 [48:55<29:55,  1.41it/s]Evaluating on VQA val set:  62% 4164/6699 [48:56<30:17,  1.39it/s]Evaluating on VQA val set:  62% 4165/6699 [48:56<30:28,  1.39it/s]Evaluating on VQA val set:  62% 4166/6699 [48:57<30:00,  1.41it/s]Evaluating on VQA val set:  62% 4167/6699 [48:58<30:18,  1.39it/s]Evaluating on VQA val set:  62% 4168/6699 [48:59<30:32,  1.38it/s]Evaluating on VQA val set:  62% 4169/6699 [48:59<29:17,  1.44it/s]Evaluating on VQA val set:  62% 4170/6699 [49:00<30:01,  1.40it/s]Evaluating on VQA val set:  62% 4171/6699 [49:01<30:25,  1.39it/s]Evaluating on VQA val set:  62% 4172/6699 [49:01<30:23,  1.39it/s]Evaluating on VQA val set:  62% 4173/6699 [49:02<30:39,  1.37it/s]Evaluating on VQA val set:  62% 4174/6699 [49:03<29:57,  1.40it/s]Evaluating on VQA val set:  62% 4175/6699 [49:04<30:40,  1.37it/s]Evaluating on VQA val set:  62% 4176/6699 [49:04<29:40,  1.42it/s]Evaluating on VQA val set:  62% 4177/6699 [49:05<28:56,  1.45it/s]Evaluating on VQA val set:  62% 4178/6699 [49:06<29:14,  1.44it/s]Evaluating on VQA val set:  62% 4179/6699 [49:06<30:01,  1.40it/s]Evaluating on VQA val set:  62% 4180/6699 [49:07<29:04,  1.44it/s]Evaluating on VQA val set:  62% 4181/6699 [49:08<28:21,  1.48it/s]Evaluating on VQA val set:  62% 4182/6699 [49:08<27:46,  1.51it/s]Evaluating on VQA val set:  62% 4183/6699 [49:09<27:33,  1.52it/s]Evaluating on VQA val set:  62% 4184/6699 [49:10<28:21,  1.48it/s]Evaluating on VQA val set:  62% 4185/6699 [49:10<28:58,  1.45it/s]Evaluating on VQA val set:  62% 4186/6699 [49:11<28:05,  1.49it/s]Evaluating on VQA val set:  63% 4187/6699 [49:12<28:39,  1.46it/s]Evaluating on VQA val set:  63% 4188/6699 [49:12<29:19,  1.43it/s]Evaluating on VQA val set:  63% 4189/6699 [49:13<28:33,  1.46it/s]Evaluating on VQA val set:  63% 4190/6699 [49:14<28:16,  1.48it/s]Evaluating on VQA val set:  63% 4191/6699 [49:14<28:43,  1.46it/s]Evaluating on VQA val set:  63% 4192/6699 [49:15<29:17,  1.43it/s]Evaluating on VQA val set:  63% 4193/6699 [49:16<29:22,  1.42it/s]Evaluating on VQA val set:  63% 4194/6699 [49:17<29:31,  1.41it/s]Evaluating on VQA val set:  63% 4195/6699 [49:17<29:56,  1.39it/s]Evaluating on VQA val set:  63% 4196/6699 [49:18<29:36,  1.41it/s]Evaluating on VQA val set:  63% 4197/6699 [49:19<30:26,  1.37it/s]Evaluating on VQA val set:  63% 4198/6699 [49:20<30:30,  1.37it/s]Evaluating on VQA val set:  63% 4199/6699 [49:20<30:04,  1.39it/s]Evaluating on VQA val set:  63% 4200/6699 [49:21<29:23,  1.42it/s]Evaluating on VQA val set:  63% 4201/6699 [49:22<28:56,  1.44it/s]Evaluating on VQA val set:  63% 4202/6699 [49:22<28:34,  1.46it/s]Evaluating on VQA val set:  63% 4203/6699 [49:23<29:07,  1.43it/s]Evaluating on VQA val set:  63% 4204/6699 [49:24<29:22,  1.42it/s]Evaluating on VQA val set:  63% 4205/6699 [49:24<28:35,  1.45it/s]Evaluating on VQA val set:  63% 4206/6699 [49:25<29:09,  1.43it/s]Evaluating on VQA val set:  63% 4207/6699 [49:26<29:13,  1.42it/s]Evaluating on VQA val set:  63% 4208/6699 [49:26<29:38,  1.40it/s]Evaluating on VQA val set:  63% 4209/6699 [49:27<28:50,  1.44it/s]Evaluating on VQA val set:  63% 4210/6699 [49:28<27:49,  1.49it/s]Evaluating on VQA val set:  63% 4211/6699 [49:28<27:37,  1.50it/s]Evaluating on VQA val set:  63% 4212/6699 [49:29<28:27,  1.46it/s]Evaluating on VQA val set:  63% 4213/6699 [49:30<28:03,  1.48it/s]Evaluating on VQA val set:  63% 4214/6699 [49:30<28:02,  1.48it/s]Evaluating on VQA val set:  63% 4215/6699 [49:31<28:28,  1.45it/s]Evaluating on VQA val set:  63% 4216/6699 [49:32<28:56,  1.43it/s]Evaluating on VQA val set:  63% 4217/6699 [49:33<28:51,  1.43it/s]Evaluating on VQA val set:  63% 4218/6699 [49:33<29:34,  1.40it/s]Evaluating on VQA val set:  63% 4219/6699 [49:34<28:49,  1.43it/s]Evaluating on VQA val set:  63% 4220/6699 [49:35<28:02,  1.47it/s]Evaluating on VQA val set:  63% 4221/6699 [49:35<28:34,  1.45it/s]Evaluating on VQA val set:  63% 4222/6699 [49:36<28:25,  1.45it/s]Evaluating on VQA val set:  63% 4223/6699 [49:37<28:21,  1.45it/s]Evaluating on VQA val set:  63% 4224/6699 [49:37<28:18,  1.46it/s]Evaluating on VQA val set:  63% 4225/6699 [49:38<28:47,  1.43it/s]Evaluating on VQA val set:  63% 4226/6699 [49:39<28:50,  1.43it/s]Evaluating on VQA val set:  63% 4227/6699 [49:40<29:35,  1.39it/s]Evaluating on VQA val set:  63% 4228/6699 [49:40<29:06,  1.41it/s]Evaluating on VQA val set:  63% 4229/6699 [49:41<29:38,  1.39it/s]Evaluating on VQA val set:  63% 4230/6699 [49:42<29:33,  1.39it/s]Evaluating on VQA val set:  63% 4231/6699 [49:42<28:48,  1.43it/s]Evaluating on VQA val set:  63% 4232/6699 [49:43<28:50,  1.43it/s]Evaluating on VQA val set:  63% 4233/6699 [49:44<29:02,  1.42it/s]Evaluating on VQA val set:  63% 4234/6699 [49:45<28:42,  1.43it/s]Evaluating on VQA val set:  63% 4235/6699 [49:45<27:20,  1.50it/s]Evaluating on VQA val set:  63% 4236/6699 [49:46<27:10,  1.51it/s]Evaluating on VQA val set:  63% 4237/6699 [49:47<28:40,  1.43it/s]Evaluating on VQA val set:  63% 4238/6699 [49:47<28:55,  1.42it/s]Evaluating on VQA val set:  63% 4239/6699 [49:48<29:10,  1.41it/s]Evaluating on VQA val set:  63% 4240/6699 [49:49<28:40,  1.43it/s]Evaluating on VQA val set:  63% 4241/6699 [49:49<28:55,  1.42it/s]Evaluating on VQA val set:  63% 4242/6699 [49:50<29:08,  1.41it/s]Evaluating on VQA val set:  63% 4243/6699 [49:51<29:05,  1.41it/s]Evaluating on VQA val set:  63% 4244/6699 [49:52<29:06,  1.41it/s]Evaluating on VQA val set:  63% 4245/6699 [49:52<28:16,  1.45it/s]Evaluating on VQA val set:  63% 4246/6699 [49:53<26:32,  1.54it/s]Evaluating on VQA val set:  63% 4247/6699 [49:53<26:40,  1.53it/s]Evaluating on VQA val set:  63% 4248/6699 [49:54<27:47,  1.47it/s]Evaluating on VQA val set:  63% 4249/6699 [49:55<28:57,  1.41it/s]Evaluating on VQA val set:  63% 4250/6699 [49:56<29:45,  1.37it/s]Evaluating on VQA val set:  63% 4251/6699 [49:56<28:47,  1.42it/s]Evaluating on VQA val set:  63% 4252/6699 [49:57<29:18,  1.39it/s]Evaluating on VQA val set:  63% 4253/6699 [49:58<29:14,  1.39it/s]Evaluating on VQA val set:  64% 4254/6699 [49:59<29:25,  1.38it/s]Evaluating on VQA val set:  64% 4255/6699 [49:59<29:20,  1.39it/s]Evaluating on VQA val set:  64% 4256/6699 [50:00<29:28,  1.38it/s]Evaluating on VQA val set:  64% 4257/6699 [50:01<29:21,  1.39it/s]Evaluating on VQA val set:  64% 4258/6699 [50:01<28:38,  1.42it/s]Evaluating on VQA val set:  64% 4259/6699 [50:02<29:06,  1.40it/s]Evaluating on VQA val set:  64% 4260/6699 [50:03<28:49,  1.41it/s]Evaluating on VQA val set:  64% 4261/6699 [50:04<28:48,  1.41it/s]Evaluating on VQA val set:  64% 4262/6699 [50:04<28:20,  1.43it/s]Evaluating on VQA val set:  64% 4263/6699 [50:05<27:52,  1.46it/s]Evaluating on VQA val set:  64% 4264/6699 [50:06<28:31,  1.42it/s]Evaluating on VQA val set:  64% 4265/6699 [50:06<28:47,  1.41it/s]Evaluating on VQA val set:  64% 4266/6699 [50:07<28:29,  1.42it/s]Evaluating on VQA val set:  64% 4267/6699 [50:08<28:51,  1.40it/s]Evaluating on VQA val set:  64% 4268/6699 [50:08<28:52,  1.40it/s]Evaluating on VQA val set:  64% 4269/6699 [50:09<29:19,  1.38it/s]Evaluating on VQA val set:  64% 4270/6699 [50:10<28:52,  1.40it/s]Evaluating on VQA val set:  64% 4271/6699 [50:11<28:03,  1.44it/s]Evaluating on VQA val set:  64% 4272/6699 [50:11<28:44,  1.41it/s]Evaluating on VQA val set:  64% 4273/6699 [50:12<28:44,  1.41it/s]Evaluating on VQA val set:  64% 4274/6699 [50:13<28:06,  1.44it/s]Evaluating on VQA val set:  64% 4275/6699 [50:13<28:15,  1.43it/s]Evaluating on VQA val set:  64% 4276/6699 [50:14<28:54,  1.40it/s]Evaluating on VQA val set:  64% 4277/6699 [50:15<28:54,  1.40it/s]Evaluating on VQA val set:  64% 4278/6699 [50:16<29:00,  1.39it/s]Evaluating on VQA val set:  64% 4279/6699 [50:16<28:46,  1.40it/s]Evaluating on VQA val set:  64% 4280/6699 [50:17<28:53,  1.40it/s]Evaluating on VQA val set:  64% 4281/6699 [50:18<28:22,  1.42it/s]Evaluating on VQA val set:  64% 4282/6699 [50:18<28:45,  1.40it/s]Evaluating on VQA val set:  64% 4283/6699 [50:19<28:41,  1.40it/s]Evaluating on VQA val set:  64% 4284/6699 [50:20<29:04,  1.38it/s]Evaluating on VQA val set:  64% 4285/6699 [50:21<28:26,  1.41it/s]Evaluating on VQA val set:  64% 4286/6699 [50:21<28:28,  1.41it/s]Evaluating on VQA val set:  64% 4287/6699 [50:22<28:39,  1.40it/s]Evaluating on VQA val set:  64% 4288/6699 [50:23<28:31,  1.41it/s]Evaluating on VQA val set:  64% 4289/6699 [50:23<27:48,  1.44it/s]Evaluating on VQA val set:  64% 4290/6699 [50:24<27:09,  1.48it/s]Evaluating on VQA val set:  64% 4291/6699 [50:25<27:51,  1.44it/s]Evaluating on VQA val set:  64% 4292/6699 [50:25<27:50,  1.44it/s]Evaluating on VQA val set:  64% 4293/6699 [50:26<27:19,  1.47it/s]Evaluating on VQA val set:  64% 4294/6699 [50:27<27:35,  1.45it/s]Evaluating on VQA val set:  64% 4295/6699 [50:27<27:50,  1.44it/s]Evaluating on VQA val set:  64% 4296/6699 [50:28<28:20,  1.41it/s]Evaluating on VQA val set:  64% 4297/6699 [50:29<28:36,  1.40it/s]Evaluating on VQA val set:  64% 4298/6699 [50:30<29:14,  1.37it/s]Evaluating on VQA val set:  64% 4299/6699 [50:30<28:46,  1.39it/s]Evaluating on VQA val set:  64% 4300/6699 [50:31<28:42,  1.39it/s]Evaluating on VQA val set:  64% 4301/6699 [50:32<27:52,  1.43it/s]Evaluating on VQA val set:  64% 4302/6699 [50:32<26:59,  1.48it/s]Evaluating on VQA val set:  64% 4303/6699 [50:33<27:18,  1.46it/s]Evaluating on VQA val set:  64% 4304/6699 [50:34<26:47,  1.49it/s]Evaluating on VQA val set:  64% 4305/6699 [50:34<26:47,  1.49it/s]Evaluating on VQA val set:  64% 4306/6699 [50:35<26:24,  1.51it/s]Evaluating on VQA val set:  64% 4307/6699 [50:36<26:51,  1.48it/s]Evaluating on VQA val set:  64% 4308/6699 [50:36<26:31,  1.50it/s]Evaluating on VQA val set:  64% 4309/6699 [50:37<27:25,  1.45it/s]Evaluating on VQA val set:  64% 4310/6699 [50:38<28:23,  1.40it/s]Evaluating on VQA val set:  64% 4311/6699 [50:39<28:53,  1.38it/s]Evaluating on VQA val set:  64% 4312/6699 [50:39<29:12,  1.36it/s]Evaluating on VQA val set:  64% 4313/6699 [50:40<28:14,  1.41it/s]Evaluating on VQA val set:  64% 4314/6699 [50:41<27:38,  1.44it/s]Evaluating on VQA val set:  64% 4315/6699 [50:41<26:29,  1.50it/s]Evaluating on VQA val set:  64% 4316/6699 [50:42<27:30,  1.44it/s]Evaluating on VQA val set:  64% 4317/6699 [50:43<27:30,  1.44it/s]Evaluating on VQA val set:  64% 4318/6699 [50:43<28:05,  1.41it/s]Evaluating on VQA val set:  64% 4319/6699 [50:44<28:51,  1.37it/s]Evaluating on VQA val set:  64% 4320/6699 [50:45<27:02,  1.47it/s]Evaluating on VQA val set:  65% 4321/6699 [50:46<27:42,  1.43it/s]Evaluating on VQA val set:  65% 4322/6699 [50:46<27:39,  1.43it/s]Evaluating on VQA val set:  65% 4323/6699 [50:47<27:37,  1.43it/s]Evaluating on VQA val set:  65% 4324/6699 [50:48<28:03,  1.41it/s]Evaluating on VQA val set:  65% 4325/6699 [50:48<28:03,  1.41it/s]Evaluating on VQA val set:  65% 4326/6699 [50:49<27:43,  1.43it/s]Evaluating on VQA val set:  65% 4327/6699 [50:50<28:21,  1.39it/s]Evaluating on VQA val set:  65% 4328/6699 [50:51<28:36,  1.38it/s]Evaluating on VQA val set:  65% 4329/6699 [50:51<28:39,  1.38it/s]Evaluating on VQA val set:  65% 4330/6699 [50:52<28:56,  1.36it/s]Evaluating on VQA val set:  65% 4331/6699 [50:53<28:01,  1.41it/s]Evaluating on VQA val set:  65% 4332/6699 [50:53<28:25,  1.39it/s]Evaluating on VQA val set:  65% 4333/6699 [50:54<27:57,  1.41it/s]Evaluating on VQA val set:  65% 4334/6699 [50:55<27:39,  1.43it/s]Evaluating on VQA val set:  65% 4335/6699 [50:56<28:03,  1.40it/s]Evaluating on VQA val set:  65% 4336/6699 [50:56<27:59,  1.41it/s]Evaluating on VQA val set:  65% 4337/6699 [50:57<27:24,  1.44it/s]Evaluating on VQA val set:  65% 4338/6699 [50:58<26:46,  1.47it/s]Evaluating on VQA val set:  65% 4339/6699 [50:58<26:18,  1.50it/s]Evaluating on VQA val set:  65% 4340/6699 [50:59<26:53,  1.46it/s]Evaluating on VQA val set:  65% 4341/6699 [51:00<27:25,  1.43it/s]Evaluating on VQA val set:  65% 4342/6699 [51:00<27:04,  1.45it/s]Evaluating on VQA val set:  65% 4343/6699 [51:01<26:12,  1.50it/s]Evaluating on VQA val set:  65% 4344/6699 [51:02<26:35,  1.48it/s]Evaluating on VQA val set:  65% 4345/6699 [51:02<27:25,  1.43it/s]Evaluating on VQA val set:  65% 4346/6699 [51:03<27:30,  1.43it/s]Evaluating on VQA val set:  65% 4347/6699 [51:04<28:14,  1.39it/s]Evaluating on VQA val set:  65% 4348/6699 [51:05<27:39,  1.42it/s]Evaluating on VQA val set:  65% 4349/6699 [51:05<28:02,  1.40it/s]Evaluating on VQA val set:  65% 4350/6699 [51:06<27:19,  1.43it/s]Evaluating on VQA val set:  65% 4351/6699 [51:07<26:52,  1.46it/s]Evaluating on VQA val set:  65% 4352/6699 [51:07<26:00,  1.50it/s]Evaluating on VQA val set:  65% 4353/6699 [51:08<27:00,  1.45it/s]Evaluating on VQA val set:  65% 4354/6699 [51:09<27:29,  1.42it/s]Evaluating on VQA val set:  65% 4355/6699 [51:09<27:14,  1.43it/s]Evaluating on VQA val set:  65% 4356/6699 [51:10<27:32,  1.42it/s]Evaluating on VQA val set:  65% 4357/6699 [51:11<27:31,  1.42it/s]Evaluating on VQA val set:  65% 4358/6699 [51:12<27:13,  1.43it/s]Evaluating on VQA val set:  65% 4359/6699 [51:12<25:34,  1.53it/s]Evaluating on VQA val set:  65% 4360/6699 [51:13<26:03,  1.50it/s]Evaluating on VQA val set:  65% 4361/6699 [51:14<27:15,  1.43it/s]Evaluating on VQA val set:  65% 4362/6699 [51:14<27:56,  1.39it/s]Evaluating on VQA val set:  65% 4363/6699 [51:15<27:12,  1.43it/s]Evaluating on VQA val set:  65% 4364/6699 [51:16<27:01,  1.44it/s]Evaluating on VQA val set:  65% 4365/6699 [51:16<27:38,  1.41it/s]Evaluating on VQA val set:  65% 4366/6699 [51:17<27:45,  1.40it/s]Evaluating on VQA val set:  65% 4367/6699 [51:18<27:32,  1.41it/s]Evaluating on VQA val set:  65% 4368/6699 [51:19<27:45,  1.40it/s]Evaluating on VQA val set:  65% 4369/6699 [51:19<27:02,  1.44it/s]Evaluating on VQA val set:  65% 4370/6699 [51:20<25:45,  1.51it/s]Evaluating on VQA val set:  65% 4371/6699 [51:20<23:11,  1.67it/s]Evaluating on VQA val set:  65% 4372/6699 [51:21<23:23,  1.66it/s]Evaluating on VQA val set:  65% 4373/6699 [51:22<24:39,  1.57it/s]Evaluating on VQA val set:  65% 4374/6699 [51:22<25:33,  1.52it/s]Evaluating on VQA val set:  65% 4375/6699 [51:23<26:37,  1.45it/s]Evaluating on VQA val set:  65% 4376/6699 [51:24<27:05,  1.43it/s]Evaluating on VQA val set:  65% 4377/6699 [51:24<27:38,  1.40it/s]Evaluating on VQA val set:  65% 4378/6699 [51:25<28:05,  1.38it/s]Evaluating on VQA val set:  65% 4379/6699 [51:26<28:21,  1.36it/s]Evaluating on VQA val set:  65% 4380/6699 [51:27<28:23,  1.36it/s]Evaluating on VQA val set:  65% 4381/6699 [51:27<28:19,  1.36it/s]Evaluating on VQA val set:  65% 4382/6699 [51:28<27:36,  1.40it/s]Evaluating on VQA val set:  65% 4383/6699 [51:29<27:00,  1.43it/s]Evaluating on VQA val set:  65% 4384/6699 [51:29<25:59,  1.48it/s]Evaluating on VQA val set:  65% 4385/6699 [51:30<25:05,  1.54it/s]Evaluating on VQA val set:  65% 4386/6699 [51:31<26:06,  1.48it/s]Evaluating on VQA val set:  65% 4387/6699 [51:31<26:59,  1.43it/s]Evaluating on VQA val set:  66% 4388/6699 [51:32<27:20,  1.41it/s]Evaluating on VQA val set:  66% 4389/6699 [51:33<27:05,  1.42it/s]Evaluating on VQA val set:  66% 4390/6699 [51:34<27:12,  1.41it/s]Evaluating on VQA val set:  66% 4391/6699 [51:34<26:28,  1.45it/s]Evaluating on VQA val set:  66% 4392/6699 [51:35<26:55,  1.43it/s]Evaluating on VQA val set:  66% 4393/6699 [51:36<26:26,  1.45it/s]Evaluating on VQA val set:  66% 4394/6699 [51:36<26:22,  1.46it/s]Evaluating on VQA val set:  66% 4395/6699 [51:37<26:23,  1.45it/s]Evaluating on VQA val set:  66% 4396/6699 [51:38<26:31,  1.45it/s]Evaluating on VQA val set:  66% 4397/6699 [51:38<25:56,  1.48it/s]Evaluating on VQA val set:  66% 4398/6699 [51:39<26:23,  1.45it/s]Evaluating on VQA val set:  66% 4399/6699 [51:40<26:22,  1.45it/s]Evaluating on VQA val set:  66% 4400/6699 [51:40<26:19,  1.46it/s]Evaluating on VQA val set:  66% 4401/6699 [51:41<26:13,  1.46it/s]Evaluating on VQA val set:  66% 4402/6699 [51:42<27:25,  1.40it/s]Evaluating on VQA val set:  66% 4403/6699 [51:43<27:00,  1.42it/s]Evaluating on VQA val set:  66% 4404/6699 [51:43<27:05,  1.41it/s]Evaluating on VQA val set:  66% 4405/6699 [51:44<27:17,  1.40it/s]Evaluating on VQA val set:  66% 4406/6699 [51:45<26:39,  1.43it/s]Evaluating on VQA val set:  66% 4407/6699 [51:45<26:30,  1.44it/s]Evaluating on VQA val set:  66% 4408/6699 [51:46<27:02,  1.41it/s]Evaluating on VQA val set:  66% 4409/6699 [51:47<27:27,  1.39it/s]Evaluating on VQA val set:  66% 4410/6699 [51:48<26:26,  1.44it/s]Evaluating on VQA val set:  66% 4411/6699 [51:48<26:25,  1.44it/s]Evaluating on VQA val set:  66% 4412/6699 [51:49<26:35,  1.43it/s]Evaluating on VQA val set:  66% 4413/6699 [51:50<27:30,  1.39it/s]Evaluating on VQA val set:  66% 4414/6699 [51:50<27:09,  1.40it/s]Evaluating on VQA val set:  66% 4415/6699 [51:51<27:20,  1.39it/s]Evaluating on VQA val set:  66% 4416/6699 [51:52<27:06,  1.40it/s]Evaluating on VQA val set:  66% 4417/6699 [51:52<25:27,  1.49it/s]Evaluating on VQA val set:  66% 4418/6699 [51:53<24:04,  1.58it/s]Evaluating on VQA val set:  66% 4419/6699 [51:54<24:32,  1.55it/s]Evaluating on VQA val set:  66% 4420/6699 [51:54<24:45,  1.53it/s]Evaluating on VQA val set:  66% 4421/6699 [51:55<25:04,  1.51it/s]Evaluating on VQA val set:  66% 4422/6699 [51:56<24:28,  1.55it/s]Evaluating on VQA val set:  66% 4423/6699 [51:56<24:58,  1.52it/s]Evaluating on VQA val set:  66% 4424/6699 [51:57<25:13,  1.50it/s]Evaluating on VQA val set:  66% 4425/6699 [51:58<25:53,  1.46it/s]Evaluating on VQA val set:  66% 4426/6699 [51:58<25:49,  1.47it/s]Evaluating on VQA val set:  66% 4427/6699 [51:59<26:03,  1.45it/s]Evaluating on VQA val set:  66% 4428/6699 [52:00<26:18,  1.44it/s]Evaluating on VQA val set:  66% 4429/6699 [52:00<26:14,  1.44it/s]Evaluating on VQA val set:  66% 4430/6699 [52:01<26:30,  1.43it/s]Evaluating on VQA val set:  66% 4431/6699 [52:02<26:15,  1.44it/s]Evaluating on VQA val set:  66% 4432/6699 [52:03<26:39,  1.42it/s]Evaluating on VQA val set:  66% 4433/6699 [52:03<26:48,  1.41it/s]Evaluating on VQA val set:  66% 4434/6699 [52:04<27:06,  1.39it/s]Evaluating on VQA val set:  66% 4435/6699 [52:05<26:29,  1.42it/s]Evaluating on VQA val set:  66% 4436/6699 [52:05<26:50,  1.41it/s]Evaluating on VQA val set:  66% 4437/6699 [52:06<27:39,  1.36it/s]Evaluating on VQA val set:  66% 4438/6699 [52:07<27:11,  1.39it/s]Evaluating on VQA val set:  66% 4439/6699 [52:08<26:46,  1.41it/s]Evaluating on VQA val set:  66% 4440/6699 [52:08<27:01,  1.39it/s]Evaluating on VQA val set:  66% 4441/6699 [52:09<26:22,  1.43it/s]Evaluating on VQA val set:  66% 4442/6699 [52:10<26:11,  1.44it/s]Evaluating on VQA val set:  66% 4443/6699 [52:10<25:56,  1.45it/s]Evaluating on VQA val set:  66% 4444/6699 [52:11<26:05,  1.44it/s]Evaluating on VQA val set:  66% 4445/6699 [52:12<26:16,  1.43it/s]Evaluating on VQA val set:  66% 4446/6699 [52:13<26:35,  1.41it/s]Evaluating on VQA val set:  66% 4447/6699 [52:13<26:53,  1.40it/s]Evaluating on VQA val set:  66% 4448/6699 [52:14<26:49,  1.40it/s]Evaluating on VQA val set:  66% 4449/6699 [52:15<26:17,  1.43it/s]Evaluating on VQA val set:  66% 4450/6699 [52:15<26:22,  1.42it/s]Evaluating on VQA val set:  66% 4451/6699 [52:16<26:18,  1.42it/s]Evaluating on VQA val set:  66% 4452/6699 [52:17<26:34,  1.41it/s]Evaluating on VQA val set:  66% 4453/6699 [52:18<27:03,  1.38it/s]Evaluating on VQA val set:  66% 4454/6699 [52:18<26:54,  1.39it/s]Evaluating on VQA val set:  67% 4455/6699 [52:19<27:01,  1.38it/s]Evaluating on VQA val set:  67% 4456/6699 [52:20<26:53,  1.39it/s]Evaluating on VQA val set:  67% 4457/6699 [52:20<26:22,  1.42it/s]Evaluating on VQA val set:  67% 4458/6699 [52:21<25:59,  1.44it/s]Evaluating on VQA val set:  67% 4459/6699 [52:22<26:19,  1.42it/s]Evaluating on VQA val set:  67% 4460/6699 [52:22<26:33,  1.41it/s]Evaluating on VQA val set:  67% 4461/6699 [52:23<26:12,  1.42it/s]Evaluating on VQA val set:  67% 4462/6699 [52:24<26:01,  1.43it/s]Evaluating on VQA val set:  67% 4463/6699 [52:24<25:25,  1.47it/s]Evaluating on VQA val set:  67% 4464/6699 [52:25<26:08,  1.42it/s]Evaluating on VQA val set:  67% 4465/6699 [52:26<26:15,  1.42it/s]Evaluating on VQA val set:  67% 4466/6699 [52:27<26:32,  1.40it/s]Evaluating on VQA val set:  67% 4467/6699 [52:27<26:47,  1.39it/s]Evaluating on VQA val set:  67% 4468/6699 [52:28<26:28,  1.40it/s]Evaluating on VQA val set:  67% 4469/6699 [52:29<25:59,  1.43it/s]Evaluating on VQA val set:  67% 4470/6699 [52:30<26:41,  1.39it/s]Evaluating on VQA val set:  67% 4471/6699 [52:30<26:59,  1.38it/s]Evaluating on VQA val set:  67% 4472/6699 [52:31<26:57,  1.38it/s]Evaluating on VQA val set:  67% 4473/6699 [52:32<26:51,  1.38it/s]Evaluating on VQA val set:  67% 4474/6699 [52:32<27:23,  1.35it/s]Evaluating on VQA val set:  67% 4475/6699 [52:33<26:36,  1.39it/s]Evaluating on VQA val set:  67% 4476/6699 [52:34<26:23,  1.40it/s]Evaluating on VQA val set:  67% 4477/6699 [52:35<26:26,  1.40it/s]Evaluating on VQA val set:  67% 4478/6699 [52:35<26:19,  1.41it/s]Evaluating on VQA val set:  67% 4479/6699 [52:36<27:12,  1.36it/s]Evaluating on VQA val set:  67% 4480/6699 [52:37<27:09,  1.36it/s]Evaluating on VQA val set:  67% 4481/6699 [52:38<27:09,  1.36it/s]Evaluating on VQA val set:  67% 4482/6699 [52:38<26:29,  1.40it/s]Evaluating on VQA val set:  67% 4483/6699 [52:39<26:11,  1.41it/s]Evaluating on VQA val set:  67% 4484/6699 [52:40<25:51,  1.43it/s]Evaluating on VQA val set:  67% 4485/6699 [52:40<25:29,  1.45it/s]Evaluating on VQA val set:  67% 4486/6699 [52:41<25:23,  1.45it/s]Evaluating on VQA val set:  67% 4487/6699 [52:42<26:32,  1.39it/s]Evaluating on VQA val set:  67% 4488/6699 [52:42<26:17,  1.40it/s]Evaluating on VQA val set:  67% 4489/6699 [52:43<25:52,  1.42it/s]Evaluating on VQA val set:  67% 4490/6699 [52:44<25:59,  1.42it/s]Evaluating on VQA val set:  67% 4491/6699 [52:45<25:57,  1.42it/s]Evaluating on VQA val set:  67% 4492/6699 [52:45<25:44,  1.43it/s]Evaluating on VQA val set:  67% 4493/6699 [52:46<25:21,  1.45it/s]Evaluating on VQA val set:  67% 4494/6699 [52:47<25:09,  1.46it/s]Evaluating on VQA val set:  67% 4495/6699 [52:47<25:10,  1.46it/s]Evaluating on VQA val set:  67% 4496/6699 [52:48<25:20,  1.45it/s]Evaluating on VQA val set:  67% 4497/6699 [52:49<25:35,  1.43it/s]Evaluating on VQA val set:  67% 4498/6699 [52:49<25:42,  1.43it/s]Evaluating on VQA val set:  67% 4499/6699 [52:50<25:45,  1.42it/s]Evaluating on VQA val set:  67% 4500/6699 [52:51<26:06,  1.40it/s]Evaluating on VQA val set:  67% 4501/6699 [52:52<26:13,  1.40it/s]Evaluating on VQA val set:  67% 4502/6699 [52:52<26:14,  1.40it/s]Evaluating on VQA val set:  67% 4503/6699 [52:53<26:00,  1.41it/s]Evaluating on VQA val set:  67% 4504/6699 [52:54<25:52,  1.41it/s]Evaluating on VQA val set:  67% 4505/6699 [52:54<24:33,  1.49it/s]Evaluating on VQA val set:  67% 4506/6699 [52:55<24:00,  1.52it/s]Evaluating on VQA val set:  67% 4507/6699 [52:56<24:14,  1.51it/s]Evaluating on VQA val set:  67% 4508/6699 [52:56<23:14,  1.57it/s]Evaluating on VQA val set:  67% 4509/6699 [52:57<24:44,  1.48it/s]Evaluating on VQA val set:  67% 4510/6699 [52:58<24:24,  1.50it/s]Evaluating on VQA val set:  67% 4511/6699 [52:58<23:41,  1.54it/s]Evaluating on VQA val set:  67% 4512/6699 [52:59<24:32,  1.48it/s]Evaluating on VQA val set:  67% 4513/6699 [53:00<24:22,  1.50it/s]Evaluating on VQA val set:  67% 4514/6699 [53:00<24:08,  1.51it/s]Evaluating on VQA val set:  67% 4515/6699 [53:01<24:25,  1.49it/s]Evaluating on VQA val set:  67% 4516/6699 [53:02<25:10,  1.45it/s]Evaluating on VQA val set:  67% 4517/6699 [53:02<25:04,  1.45it/s]Evaluating on VQA val set:  67% 4518/6699 [53:03<25:05,  1.45it/s]Evaluating on VQA val set:  67% 4519/6699 [53:04<24:26,  1.49it/s]Evaluating on VQA val set:  67% 4520/6699 [53:04<24:37,  1.47it/s]Evaluating on VQA val set:  67% 4521/6699 [53:05<24:33,  1.48it/s]Evaluating on VQA val set:  68% 4522/6699 [53:06<25:14,  1.44it/s]Evaluating on VQA val set:  68% 4523/6699 [53:06<25:30,  1.42it/s]Evaluating on VQA val set:  68% 4524/6699 [53:07<26:12,  1.38it/s]Evaluating on VQA val set:  68% 4525/6699 [53:08<25:55,  1.40it/s]Evaluating on VQA val set:  68% 4526/6699 [53:09<25:51,  1.40it/s]Evaluating on VQA val set:  68% 4527/6699 [53:09<26:04,  1.39it/s]Evaluating on VQA val set:  68% 4528/6699 [53:10<25:46,  1.40it/s]Evaluating on VQA val set:  68% 4529/6699 [53:11<24:47,  1.46it/s]Evaluating on VQA val set:  68% 4530/6699 [53:11<25:17,  1.43it/s]Evaluating on VQA val set:  68% 4531/6699 [53:12<24:36,  1.47it/s]Evaluating on VQA val set:  68% 4532/6699 [53:13<24:45,  1.46it/s]Evaluating on VQA val set:  68% 4533/6699 [53:13<25:19,  1.43it/s]Evaluating on VQA val set:  68% 4534/6699 [53:14<25:04,  1.44it/s]Evaluating on VQA val set:  68% 4535/6699 [53:15<25:01,  1.44it/s]Evaluating on VQA val set:  68% 4536/6699 [53:16<24:53,  1.45it/s]Evaluating on VQA val set:  68% 4537/6699 [53:16<25:12,  1.43it/s]Evaluating on VQA val set:  68% 4538/6699 [53:17<24:37,  1.46it/s]Evaluating on VQA val set:  68% 4539/6699 [53:18<24:56,  1.44it/s]Evaluating on VQA val set:  68% 4540/6699 [53:18<24:40,  1.46it/s]Evaluating on VQA val set:  68% 4541/6699 [53:19<25:00,  1.44it/s]Evaluating on VQA val set:  68% 4542/6699 [53:20<24:42,  1.46it/s]Evaluating on VQA val set:  68% 4543/6699 [53:20<25:06,  1.43it/s]Evaluating on VQA val set:  68% 4544/6699 [53:21<24:26,  1.47it/s]Evaluating on VQA val set:  68% 4545/6699 [53:22<24:04,  1.49it/s]Evaluating on VQA val set:  68% 4546/6699 [53:22<24:38,  1.46it/s]Evaluating on VQA val set:  68% 4547/6699 [53:23<25:33,  1.40it/s]Evaluating on VQA val set:  68% 4548/6699 [53:24<24:35,  1.46it/s]Evaluating on VQA val set:  68% 4549/6699 [53:24<24:31,  1.46it/s]Evaluating on VQA val set:  68% 4550/6699 [53:25<24:58,  1.43it/s]Evaluating on VQA val set:  68% 4551/6699 [53:26<25:06,  1.43it/s]Evaluating on VQA val set:  68% 4552/6699 [53:27<25:35,  1.40it/s]Evaluating on VQA val set:  68% 4553/6699 [53:27<25:14,  1.42it/s]Evaluating on VQA val set:  68% 4554/6699 [53:28<25:39,  1.39it/s]Evaluating on VQA val set:  68% 4555/6699 [53:29<25:18,  1.41it/s]Evaluating on VQA val set:  68% 4556/6699 [53:29<25:03,  1.43it/s]Evaluating on VQA val set:  68% 4557/6699 [53:30<24:51,  1.44it/s]Evaluating on VQA val set:  68% 4558/6699 [53:31<24:28,  1.46it/s]Evaluating on VQA val set:  68% 4559/6699 [53:31<23:37,  1.51it/s]Evaluating on VQA val set:  68% 4560/6699 [53:32<24:11,  1.47it/s]Evaluating on VQA val set:  68% 4561/6699 [53:33<24:40,  1.44it/s]Evaluating on VQA val set:  68% 4562/6699 [53:33<24:06,  1.48it/s]Evaluating on VQA val set:  68% 4563/6699 [53:34<25:06,  1.42it/s]Evaluating on VQA val set:  68% 4564/6699 [53:35<24:26,  1.46it/s]Evaluating on VQA val set:  68% 4565/6699 [53:36<24:53,  1.43it/s]Evaluating on VQA val set:  68% 4566/6699 [53:36<25:32,  1.39it/s]Evaluating on VQA val set:  68% 4567/6699 [53:37<25:15,  1.41it/s]Evaluating on VQA val set:  68% 4568/6699 [53:38<24:28,  1.45it/s]Evaluating on VQA val set:  68% 4569/6699 [53:38<24:17,  1.46it/s]Evaluating on VQA val set:  68% 4570/6699 [53:39<25:00,  1.42it/s]Evaluating on VQA val set:  68% 4571/6699 [53:40<24:35,  1.44it/s]Evaluating on VQA val set:  68% 4572/6699 [53:40<24:03,  1.47it/s]Evaluating on VQA val set:  68% 4573/6699 [53:41<24:38,  1.44it/s]Evaluating on VQA val set:  68% 4574/6699 [53:42<25:15,  1.40it/s]Evaluating on VQA val set:  68% 4575/6699 [53:43<24:56,  1.42it/s]Evaluating on VQA val set:  68% 4576/6699 [53:43<25:06,  1.41it/s]Evaluating on VQA val set:  68% 4577/6699 [53:44<25:05,  1.41it/s]Evaluating on VQA val set:  68% 4578/6699 [53:45<23:28,  1.51it/s]Evaluating on VQA val set:  68% 4579/6699 [53:45<22:31,  1.57it/s]Evaluating on VQA val set:  68% 4580/6699 [53:46<23:25,  1.51it/s]Evaluating on VQA val set:  68% 4581/6699 [53:47<23:22,  1.51it/s]Evaluating on VQA val set:  68% 4582/6699 [53:47<23:01,  1.53it/s]Evaluating on VQA val set:  68% 4583/6699 [53:48<23:35,  1.49it/s]Evaluating on VQA val set:  68% 4584/6699 [53:49<23:48,  1.48it/s]Evaluating on VQA val set:  68% 4585/6699 [53:49<24:28,  1.44it/s]Evaluating on VQA val set:  68% 4586/6699 [53:50<24:31,  1.44it/s]Evaluating on VQA val set:  68% 4587/6699 [53:51<25:01,  1.41it/s]Evaluating on VQA val set:  68% 4588/6699 [53:51<24:29,  1.44it/s]Evaluating on VQA val set:  69% 4589/6699 [53:52<25:08,  1.40it/s]Evaluating on VQA val set:  69% 4590/6699 [53:53<25:15,  1.39it/s]Evaluating on VQA val set:  69% 4591/6699 [53:54<25:14,  1.39it/s]Evaluating on VQA val set:  69% 4592/6699 [53:54<24:56,  1.41it/s]Evaluating on VQA val set:  69% 4593/6699 [53:55<25:20,  1.38it/s]Evaluating on VQA val set:  69% 4594/6699 [53:56<25:02,  1.40it/s]Evaluating on VQA val set:  69% 4595/6699 [53:57<24:59,  1.40it/s]Evaluating on VQA val set:  69% 4596/6699 [53:57<24:46,  1.41it/s]Evaluating on VQA val set:  69% 4597/6699 [53:58<25:04,  1.40it/s]Evaluating on VQA val set:  69% 4598/6699 [53:59<24:39,  1.42it/s]Evaluating on VQA val set:  69% 4599/6699 [53:59<24:43,  1.42it/s]Evaluating on VQA val set:  69% 4600/6699 [54:00<23:42,  1.48it/s]Evaluating on VQA val set:  69% 4601/6699 [54:01<24:23,  1.43it/s]Evaluating on VQA val set:  69% 4602/6699 [54:01<24:29,  1.43it/s]Evaluating on VQA val set:  69% 4603/6699 [54:02<25:20,  1.38it/s]Evaluating on VQA val set:  69% 4604/6699 [54:03<25:24,  1.37it/s]Evaluating on VQA val set:  69% 4605/6699 [54:04<25:48,  1.35it/s]Evaluating on VQA val set:  69% 4606/6699 [54:04<25:22,  1.38it/s]Evaluating on VQA val set:  69% 4607/6699 [54:05<24:35,  1.42it/s]Evaluating on VQA val set:  69% 4608/6699 [54:06<24:38,  1.41it/s]Evaluating on VQA val set:  69% 4609/6699 [54:06<24:56,  1.40it/s]Evaluating on VQA val set:  69% 4610/6699 [54:07<24:28,  1.42it/s]Evaluating on VQA val set:  69% 4611/6699 [54:08<25:02,  1.39it/s]Evaluating on VQA val set:  69% 4612/6699 [54:09<24:46,  1.40it/s]Evaluating on VQA val set:  69% 4613/6699 [54:09<25:01,  1.39it/s]Evaluating on VQA val set:  69% 4614/6699 [54:10<23:07,  1.50it/s]Evaluating on VQA val set:  69% 4615/6699 [54:10<22:02,  1.58it/s]Evaluating on VQA val set:  69% 4616/6699 [54:11<22:03,  1.57it/s]Evaluating on VQA val set:  69% 4617/6699 [54:12<22:40,  1.53it/s]Evaluating on VQA val set:  69% 4618/6699 [54:12<22:59,  1.51it/s]Evaluating on VQA val set:  69% 4619/6699 [54:13<23:11,  1.49it/s]Evaluating on VQA val set:  69% 4620/6699 [54:14<23:48,  1.46it/s]Evaluating on VQA val set:  69% 4621/6699 [54:15<23:35,  1.47it/s]Evaluating on VQA val set:  69% 4622/6699 [54:15<22:40,  1.53it/s]Evaluating on VQA val set:  69% 4623/6699 [54:16<23:13,  1.49it/s]Evaluating on VQA val set:  69% 4624/6699 [54:17<23:29,  1.47it/s]Evaluating on VQA val set:  69% 4625/6699 [54:17<23:31,  1.47it/s]Evaluating on VQA val set:  69% 4626/6699 [54:18<23:23,  1.48it/s]Evaluating on VQA val set:  69% 4627/6699 [54:19<24:19,  1.42it/s]Evaluating on VQA val set:  69% 4628/6699 [54:19<24:42,  1.40it/s]Evaluating on VQA val set:  69% 4629/6699 [54:20<25:10,  1.37it/s]Evaluating on VQA val set:  69% 4630/6699 [54:21<25:08,  1.37it/s]Evaluating on VQA val set:  69% 4631/6699 [54:22<24:31,  1.41it/s]Evaluating on VQA val set:  69% 4632/6699 [54:22<24:39,  1.40it/s]Evaluating on VQA val set:  69% 4633/6699 [54:23<24:40,  1.40it/s]Evaluating on VQA val set:  69% 4634/6699 [54:24<24:46,  1.39it/s]Evaluating on VQA val set:  69% 4635/6699 [54:24<25:01,  1.37it/s]Evaluating on VQA val set:  69% 4636/6699 [54:25<25:11,  1.36it/s]Evaluating on VQA val set:  69% 4637/6699 [54:26<25:01,  1.37it/s]Evaluating on VQA val set:  69% 4638/6699 [54:27<24:38,  1.39it/s]Evaluating on VQA val set:  69% 4639/6699 [54:27<24:29,  1.40it/s]Evaluating on VQA val set:  69% 4640/6699 [54:28<24:12,  1.42it/s]Evaluating on VQA val set:  69% 4641/6699 [54:29<24:00,  1.43it/s]Evaluating on VQA val set:  69% 4642/6699 [54:29<24:34,  1.40it/s]Evaluating on VQA val set:  69% 4643/6699 [54:30<23:52,  1.44it/s]Evaluating on VQA val set:  69% 4644/6699 [54:31<24:26,  1.40it/s]Evaluating on VQA val set:  69% 4645/6699 [54:32<24:40,  1.39it/s]Evaluating on VQA val set:  69% 4646/6699 [54:32<24:02,  1.42it/s]Evaluating on VQA val set:  69% 4647/6699 [54:33<23:46,  1.44it/s]Evaluating on VQA val set:  69% 4648/6699 [54:34<23:09,  1.48it/s]Evaluating on VQA val set:  69% 4649/6699 [54:34<23:34,  1.45it/s]Evaluating on VQA val set:  69% 4650/6699 [54:35<23:31,  1.45it/s]Evaluating on VQA val set:  69% 4651/6699 [54:36<23:15,  1.47it/s]Evaluating on VQA val set:  69% 4652/6699 [54:36<23:21,  1.46it/s]Evaluating on VQA val set:  69% 4653/6699 [54:37<22:54,  1.49it/s]Evaluating on VQA val set:  69% 4654/6699 [54:38<23:24,  1.46it/s]Evaluating on VQA val set:  69% 4655/6699 [54:38<24:04,  1.42it/s]Evaluating on VQA val set:  70% 4656/6699 [54:39<24:22,  1.40it/s]Evaluating on VQA val set:  70% 4657/6699 [54:40<24:28,  1.39it/s]Evaluating on VQA val set:  70% 4658/6699 [54:41<24:05,  1.41it/s]Evaluating on VQA val set:  70% 4659/6699 [54:41<23:25,  1.45it/s]Evaluating on VQA val set:  70% 4660/6699 [54:42<22:48,  1.49it/s]Evaluating on VQA val set:  70% 4661/6699 [54:43<23:28,  1.45it/s]Evaluating on VQA val set:  70% 4662/6699 [54:43<23:17,  1.46it/s]Evaluating on VQA val set:  70% 4663/6699 [54:44<23:49,  1.42it/s]Evaluating on VQA val set:  70% 4664/6699 [54:45<24:10,  1.40it/s]Evaluating on VQA val set:  70% 4665/6699 [54:45<23:39,  1.43it/s]Evaluating on VQA val set:  70% 4666/6699 [54:46<23:23,  1.45it/s]Evaluating on VQA val set:  70% 4667/6699 [54:47<24:00,  1.41it/s]Evaluating on VQA val set:  70% 4668/6699 [54:48<23:59,  1.41it/s]Evaluating on VQA val set:  70% 4669/6699 [54:48<24:24,  1.39it/s]Evaluating on VQA val set:  70% 4670/6699 [54:49<24:17,  1.39it/s]Evaluating on VQA val set:  70% 4671/6699 [54:50<24:09,  1.40it/s]Evaluating on VQA val set:  70% 4672/6699 [54:50<24:25,  1.38it/s]Evaluating on VQA val set:  70% 4673/6699 [54:51<23:41,  1.43it/s]Evaluating on VQA val set:  70% 4674/6699 [54:52<23:27,  1.44it/s]Evaluating on VQA val set:  70% 4675/6699 [54:53<23:44,  1.42it/s]Evaluating on VQA val set:  70% 4676/6699 [54:53<22:54,  1.47it/s]Evaluating on VQA val set:  70% 4677/6699 [54:54<23:36,  1.43it/s]Evaluating on VQA val set:  70% 4678/6699 [54:55<23:20,  1.44it/s]Evaluating on VQA val set:  70% 4679/6699 [54:55<23:54,  1.41it/s]Evaluating on VQA val set:  70% 4680/6699 [54:56<24:23,  1.38it/s]Evaluating on VQA val set:  70% 4681/6699 [54:57<24:13,  1.39it/s]Evaluating on VQA val set:  70% 4682/6699 [54:57<23:38,  1.42it/s]Evaluating on VQA val set:  70% 4683/6699 [54:58<23:30,  1.43it/s]Evaluating on VQA val set:  70% 4684/6699 [54:59<23:52,  1.41it/s]Evaluating on VQA val set:  70% 4685/6699 [55:00<23:45,  1.41it/s]Evaluating on VQA val set:  70% 4686/6699 [55:00<23:54,  1.40it/s]Evaluating on VQA val set:  70% 4687/6699 [55:01<23:10,  1.45it/s]Evaluating on VQA val set:  70% 4688/6699 [55:02<23:19,  1.44it/s]Evaluating on VQA val set:  70% 4689/6699 [55:02<23:42,  1.41it/s]Evaluating on VQA val set:  70% 4690/6699 [55:03<23:47,  1.41it/s]Evaluating on VQA val set:  70% 4691/6699 [55:04<23:29,  1.42it/s]Evaluating on VQA val set:  70% 4692/6699 [55:05<23:29,  1.42it/s]Evaluating on VQA val set:  70% 4693/6699 [55:05<23:46,  1.41it/s]Evaluating on VQA val set:  70% 4694/6699 [55:06<23:55,  1.40it/s]Evaluating on VQA val set:  70% 4695/6699 [55:07<23:39,  1.41it/s]Evaluating on VQA val set:  70% 4696/6699 [55:07<23:27,  1.42it/s]Evaluating on VQA val set:  70% 4697/6699 [55:08<23:47,  1.40it/s]Evaluating on VQA val set:  70% 4698/6699 [55:09<23:54,  1.40it/s]Evaluating on VQA val set:  70% 4699/6699 [55:09<22:56,  1.45it/s]Evaluating on VQA val set:  70% 4700/6699 [55:10<23:33,  1.41it/s]Evaluating on VQA val set:  70% 4701/6699 [55:11<24:06,  1.38it/s]Evaluating on VQA val set:  70% 4702/6699 [55:12<23:31,  1.41it/s]Evaluating on VQA val set:  70% 4703/6699 [55:12<23:10,  1.44it/s]Evaluating on VQA val set:  70% 4704/6699 [55:13<23:33,  1.41it/s]Evaluating on VQA val set:  70% 4705/6699 [55:14<23:57,  1.39it/s]Evaluating on VQA val set:  70% 4706/6699 [55:14<23:58,  1.38it/s]Evaluating on VQA val set:  70% 4707/6699 [55:15<23:41,  1.40it/s]Evaluating on VQA val set:  70% 4708/6699 [55:16<23:59,  1.38it/s]Evaluating on VQA val set:  70% 4709/6699 [55:17<23:27,  1.41it/s]Evaluating on VQA val set:  70% 4710/6699 [55:17<23:24,  1.42it/s]Evaluating on VQA val set:  70% 4711/6699 [55:18<23:21,  1.42it/s]Evaluating on VQA val set:  70% 4712/6699 [55:19<24:01,  1.38it/s]Evaluating on VQA val set:  70% 4713/6699 [55:19<23:45,  1.39it/s]Evaluating on VQA val set:  70% 4714/6699 [55:20<23:51,  1.39it/s]Evaluating on VQA val set:  70% 4715/6699 [55:21<24:00,  1.38it/s]Evaluating on VQA val set:  70% 4716/6699 [55:22<24:21,  1.36it/s]Evaluating on VQA val set:  70% 4717/6699 [55:22<24:04,  1.37it/s]Evaluating on VQA val set:  70% 4718/6699 [55:23<24:22,  1.35it/s]Evaluating on VQA val set:  70% 4719/6699 [55:24<24:26,  1.35it/s]Evaluating on VQA val set:  70% 4720/6699 [55:25<23:44,  1.39it/s]Evaluating on VQA val set:  70% 4721/6699 [55:25<23:12,  1.42it/s]Evaluating on VQA val set:  70% 4722/6699 [55:26<22:48,  1.44it/s]Evaluating on VQA val set:  71% 4723/6699 [55:27<23:02,  1.43it/s]Evaluating on VQA val set:  71% 4724/6699 [55:27<22:53,  1.44it/s]Evaluating on VQA val set:  71% 4725/6699 [55:28<22:33,  1.46it/s]Evaluating on VQA val set:  71% 4726/6699 [55:29<23:07,  1.42it/s]Evaluating on VQA val set:  71% 4727/6699 [55:29<23:09,  1.42it/s]Evaluating on VQA val set:  71% 4728/6699 [55:30<23:00,  1.43it/s]Evaluating on VQA val set:  71% 4729/6699 [55:31<23:21,  1.41it/s]Evaluating on VQA val set:  71% 4730/6699 [55:32<23:20,  1.41it/s]Evaluating on VQA val set:  71% 4731/6699 [55:32<23:23,  1.40it/s]Evaluating on VQA val set:  71% 4732/6699 [55:33<22:40,  1.45it/s]Evaluating on VQA val set:  71% 4733/6699 [55:34<23:01,  1.42it/s]Evaluating on VQA val set:  71% 4734/6699 [55:34<23:14,  1.41it/s]Evaluating on VQA val set:  71% 4735/6699 [55:35<23:45,  1.38it/s]Evaluating on VQA val set:  71% 4736/6699 [55:36<23:27,  1.39it/s]Evaluating on VQA val set:  71% 4737/6699 [55:37<24:09,  1.35it/s]Evaluating on VQA val set:  71% 4738/6699 [55:37<23:20,  1.40it/s]Evaluating on VQA val set:  71% 4739/6699 [55:38<23:12,  1.41it/s]Evaluating on VQA val set:  71% 4740/6699 [55:39<23:06,  1.41it/s]Evaluating on VQA val set:  71% 4741/6699 [55:39<22:35,  1.44it/s]Evaluating on VQA val set:  71% 4742/6699 [55:40<22:32,  1.45it/s]Evaluating on VQA val set:  71% 4743/6699 [55:41<23:13,  1.40it/s]Evaluating on VQA val set:  71% 4744/6699 [55:42<23:29,  1.39it/s]Evaluating on VQA val set:  71% 4745/6699 [55:42<22:56,  1.42it/s]Evaluating on VQA val set:  71% 4746/6699 [55:43<22:40,  1.44it/s]Evaluating on VQA val set:  71% 4747/6699 [55:44<23:00,  1.41it/s]Evaluating on VQA val set:  71% 4748/6699 [55:44<22:30,  1.44it/s]Evaluating on VQA val set:  71% 4749/6699 [55:45<23:07,  1.41it/s]Evaluating on VQA val set:  71% 4750/6699 [55:46<23:04,  1.41it/s]Evaluating on VQA val set:  71% 4751/6699 [55:46<22:13,  1.46it/s]Evaluating on VQA val set:  71% 4752/6699 [55:47<22:26,  1.45it/s]Evaluating on VQA val set:  71% 4753/6699 [55:48<22:22,  1.45it/s]Evaluating on VQA val set:  71% 4754/6699 [55:48<22:06,  1.47it/s]Evaluating on VQA val set:  71% 4755/6699 [55:49<22:24,  1.45it/s]Evaluating on VQA val set:  71% 4756/6699 [55:50<22:02,  1.47it/s]Evaluating on VQA val set:  71% 4757/6699 [55:51<22:28,  1.44it/s]Evaluating on VQA val set:  71% 4758/6699 [55:51<22:15,  1.45it/s]Evaluating on VQA val set:  71% 4759/6699 [55:52<22:41,  1.42it/s]Evaluating on VQA val set:  71% 4760/6699 [55:53<23:09,  1.40it/s]Evaluating on VQA val set:  71% 4761/6699 [55:53<23:24,  1.38it/s]Evaluating on VQA val set:  71% 4762/6699 [55:54<23:05,  1.40it/s]Evaluating on VQA val set:  71% 4763/6699 [55:55<22:43,  1.42it/s]Evaluating on VQA val set:  71% 4764/6699 [55:56<23:14,  1.39it/s]Evaluating on VQA val set:  71% 4765/6699 [55:56<23:14,  1.39it/s]Evaluating on VQA val set:  71% 4766/6699 [55:57<23:29,  1.37it/s]Evaluating on VQA val set:  71% 4767/6699 [55:58<23:35,  1.36it/s]Evaluating on VQA val set:  71% 4768/6699 [55:59<23:42,  1.36it/s]Evaluating on VQA val set:  71% 4769/6699 [55:59<22:59,  1.40it/s]Evaluating on VQA val set:  71% 4770/6699 [56:00<22:46,  1.41it/s]Evaluating on VQA val set:  71% 4771/6699 [56:01<23:09,  1.39it/s]Evaluating on VQA val set:  71% 4772/6699 [56:01<23:19,  1.38it/s]Evaluating on VQA val set:  71% 4773/6699 [56:02<22:44,  1.41it/s]Evaluating on VQA val set:  71% 4774/6699 [56:03<22:23,  1.43it/s]Evaluating on VQA val set:  71% 4775/6699 [56:03<22:28,  1.43it/s]Evaluating on VQA val set:  71% 4776/6699 [56:04<22:49,  1.40it/s]Evaluating on VQA val set:  71% 4777/6699 [56:05<22:12,  1.44it/s]Evaluating on VQA val set:  71% 4778/6699 [56:06<22:38,  1.41it/s]Evaluating on VQA val set:  71% 4779/6699 [56:06<23:07,  1.38it/s]Evaluating on VQA val set:  71% 4780/6699 [56:07<21:59,  1.45it/s]Evaluating on VQA val set:  71% 4781/6699 [56:08<21:46,  1.47it/s]Evaluating on VQA val set:  71% 4782/6699 [56:08<22:03,  1.45it/s]Evaluating on VQA val set:  71% 4783/6699 [56:09<22:15,  1.43it/s]Evaluating on VQA val set:  71% 4784/6699 [56:10<22:19,  1.43it/s]Evaluating on VQA val set:  71% 4785/6699 [56:10<21:42,  1.47it/s]Evaluating on VQA val set:  71% 4786/6699 [56:11<21:53,  1.46it/s]Evaluating on VQA val set:  71% 4787/6699 [56:12<21:49,  1.46it/s]Evaluating on VQA val set:  71% 4788/6699 [56:12<21:40,  1.47it/s]Evaluating on VQA val set:  71% 4789/6699 [56:13<22:11,  1.43it/s]Evaluating on VQA val set:  72% 4790/6699 [56:14<22:12,  1.43it/s]Evaluating on VQA val set:  72% 4791/6699 [56:15<22:34,  1.41it/s]Evaluating on VQA val set:  72% 4792/6699 [56:15<22:24,  1.42it/s]Evaluating on VQA val set:  72% 4793/6699 [56:16<22:50,  1.39it/s]Evaluating on VQA val set:  72% 4794/6699 [56:17<23:01,  1.38it/s]Evaluating on VQA val set:  72% 4795/6699 [56:17<22:18,  1.42it/s]Evaluating on VQA val set:  72% 4796/6699 [56:18<21:59,  1.44it/s]Evaluating on VQA val set:  72% 4797/6699 [56:19<21:47,  1.45it/s]Evaluating on VQA val set:  72% 4798/6699 [56:19<21:43,  1.46it/s]Evaluating on VQA val set:  72% 4799/6699 [56:20<22:05,  1.43it/s]Evaluating on VQA val set:  72% 4800/6699 [56:21<22:12,  1.43it/s]Evaluating on VQA val set:  72% 4801/6699 [56:22<22:39,  1.40it/s]Evaluating on VQA val set:  72% 4802/6699 [56:22<22:49,  1.38it/s]Evaluating on VQA val set:  72% 4803/6699 [56:23<23:05,  1.37it/s]Evaluating on VQA val set:  72% 4804/6699 [56:24<23:04,  1.37it/s]Evaluating on VQA val set:  72% 4805/6699 [56:25<22:49,  1.38it/s]Evaluating on VQA val set:  72% 4806/6699 [56:25<22:52,  1.38it/s]Evaluating on VQA val set:  72% 4807/6699 [56:26<22:54,  1.38it/s]Evaluating on VQA val set:  72% 4808/6699 [56:27<22:31,  1.40it/s]Evaluating on VQA val set:  72% 4809/6699 [56:27<22:23,  1.41it/s]Evaluating on VQA val set:  72% 4810/6699 [56:28<22:36,  1.39it/s]Evaluating on VQA val set:  72% 4811/6699 [56:29<22:05,  1.42it/s]Evaluating on VQA val set:  72% 4812/6699 [56:29<21:19,  1.47it/s]Evaluating on VQA val set:  72% 4813/6699 [56:30<20:52,  1.51it/s]Evaluating on VQA val set:  72% 4814/6699 [56:31<20:14,  1.55it/s]Evaluating on VQA val set:  72% 4815/6699 [56:31<20:38,  1.52it/s]Evaluating on VQA val set:  72% 4816/6699 [56:32<21:07,  1.49it/s]Evaluating on VQA val set:  72% 4817/6699 [56:33<21:31,  1.46it/s]Evaluating on VQA val set:  72% 4818/6699 [56:33<21:04,  1.49it/s]Evaluating on VQA val set:  72% 4819/6699 [56:34<20:52,  1.50it/s]Evaluating on VQA val set:  72% 4820/6699 [56:35<21:12,  1.48it/s]Evaluating on VQA val set:  72% 4821/6699 [56:35<21:39,  1.45it/s]Evaluating on VQA val set:  72% 4822/6699 [56:36<21:06,  1.48it/s]Evaluating on VQA val set:  72% 4823/6699 [56:37<22:00,  1.42it/s]Evaluating on VQA val set:  72% 4824/6699 [56:38<21:49,  1.43it/s]Evaluating on VQA val set:  72% 4825/6699 [56:38<22:15,  1.40it/s]Evaluating on VQA val set:  72% 4826/6699 [56:39<21:37,  1.44it/s]Evaluating on VQA val set:  72% 4827/6699 [56:40<21:35,  1.45it/s]Evaluating on VQA val set:  72% 4828/6699 [56:40<22:06,  1.41it/s]Evaluating on VQA val set:  72% 4829/6699 [56:41<22:15,  1.40it/s]Evaluating on VQA val set:  72% 4830/6699 [56:42<21:51,  1.42it/s]Evaluating on VQA val set:  72% 4831/6699 [56:42<21:42,  1.43it/s]Evaluating on VQA val set:  72% 4832/6699 [56:43<21:09,  1.47it/s]Evaluating on VQA val set:  72% 4833/6699 [56:44<21:08,  1.47it/s]Evaluating on VQA val set:  72% 4834/6699 [56:45<21:22,  1.45it/s]Evaluating on VQA val set:  72% 4835/6699 [56:45<21:18,  1.46it/s]Evaluating on VQA val set:  72% 4836/6699 [56:46<21:56,  1.41it/s]Evaluating on VQA val set:  72% 4837/6699 [56:47<22:15,  1.39it/s]Evaluating on VQA val set:  72% 4838/6699 [56:47<21:53,  1.42it/s]Evaluating on VQA val set:  72% 4839/6699 [56:48<22:28,  1.38it/s]Evaluating on VQA val set:  72% 4840/6699 [56:49<22:08,  1.40it/s]Evaluating on VQA val set:  72% 4841/6699 [56:49<20:49,  1.49it/s]Evaluating on VQA val set:  72% 4842/6699 [56:50<19:35,  1.58it/s]Evaluating on VQA val set:  72% 4843/6699 [56:51<20:25,  1.51it/s]Evaluating on VQA val set:  72% 4844/6699 [56:51<20:28,  1.51it/s]Evaluating on VQA val set:  72% 4845/6699 [56:52<21:04,  1.47it/s]Evaluating on VQA val set:  72% 4846/6699 [56:53<21:24,  1.44it/s]Evaluating on VQA val set:  72% 4847/6699 [56:53<21:18,  1.45it/s]Evaluating on VQA val set:  72% 4848/6699 [56:54<21:27,  1.44it/s]Evaluating on VQA val set:  72% 4849/6699 [56:55<21:06,  1.46it/s]Evaluating on VQA val set:  72% 4850/6699 [56:55<20:40,  1.49it/s]Evaluating on VQA val set:  72% 4851/6699 [56:56<21:08,  1.46it/s]Evaluating on VQA val set:  72% 4852/6699 [56:57<21:10,  1.45it/s]Evaluating on VQA val set:  72% 4853/6699 [56:58<20:57,  1.47it/s]Evaluating on VQA val set:  72% 4854/6699 [56:58<22:00,  1.40it/s]Evaluating on VQA val set:  72% 4855/6699 [56:59<22:09,  1.39it/s]Evaluating on VQA val set:  72% 4856/6699 [57:00<22:12,  1.38it/s]Evaluating on VQA val set:  73% 4857/6699 [57:01<21:57,  1.40it/s]Evaluating on VQA val set:  73% 4858/6699 [57:01<22:18,  1.38it/s]Evaluating on VQA val set:  73% 4859/6699 [57:02<22:05,  1.39it/s]Evaluating on VQA val set:  73% 4860/6699 [57:03<22:26,  1.37it/s]Evaluating on VQA val set:  73% 4861/6699 [57:03<22:21,  1.37it/s]Evaluating on VQA val set:  73% 4862/6699 [57:04<22:00,  1.39it/s]Evaluating on VQA val set:  73% 4863/6699 [57:05<21:22,  1.43it/s]Evaluating on VQA val set:  73% 4864/6699 [57:06<22:05,  1.38it/s]Evaluating on VQA val set:  73% 4865/6699 [57:06<21:49,  1.40it/s]Evaluating on VQA val set:  73% 4866/6699 [57:07<22:04,  1.38it/s]Evaluating on VQA val set:  73% 4867/6699 [57:08<21:30,  1.42it/s]Evaluating on VQA val set:  73% 4868/6699 [57:08<20:53,  1.46it/s]Evaluating on VQA val set:  73% 4869/6699 [57:09<21:08,  1.44it/s]Evaluating on VQA val set:  73% 4870/6699 [57:10<21:16,  1.43it/s]Evaluating on VQA val set:  73% 4871/6699 [57:10<21:15,  1.43it/s]Evaluating on VQA val set:  73% 4872/6699 [57:11<20:50,  1.46it/s]Evaluating on VQA val set:  73% 4873/6699 [57:12<20:57,  1.45it/s]Evaluating on VQA val set:  73% 4874/6699 [57:12<21:04,  1.44it/s]Evaluating on VQA val set:  73% 4875/6699 [57:13<20:33,  1.48it/s]Evaluating on VQA val set:  73% 4876/6699 [57:14<20:56,  1.45it/s]Evaluating on VQA val set:  73% 4877/6699 [57:15<21:37,  1.40it/s]Evaluating on VQA val set:  73% 4878/6699 [57:15<21:41,  1.40it/s]Evaluating on VQA val set:  73% 4879/6699 [57:16<21:25,  1.42it/s]Evaluating on VQA val set:  73% 4880/6699 [57:17<21:06,  1.44it/s]Evaluating on VQA val set:  73% 4881/6699 [57:17<20:52,  1.45it/s]Evaluating on VQA val set:  73% 4882/6699 [57:18<20:32,  1.47it/s]Evaluating on VQA val set:  73% 4883/6699 [57:19<21:05,  1.43it/s]Evaluating on VQA val set:  73% 4884/6699 [57:19<21:09,  1.43it/s]Evaluating on VQA val set:  73% 4885/6699 [57:20<22:03,  1.37it/s]Evaluating on VQA val set:  73% 4886/6699 [57:21<22:01,  1.37it/s]Evaluating on VQA val set:  73% 4887/6699 [57:22<21:52,  1.38it/s]Evaluating on VQA val set:  73% 4888/6699 [57:22<22:08,  1.36it/s]Evaluating on VQA val set:  73% 4889/6699 [57:23<22:05,  1.37it/s]Evaluating on VQA val set:  73% 4890/6699 [57:24<21:55,  1.38it/s]Evaluating on VQA val set:  73% 4891/6699 [57:25<22:26,  1.34it/s]Evaluating on VQA val set:  73% 4892/6699 [57:25<21:33,  1.40it/s]Evaluating on VQA val set:  73% 4893/6699 [57:26<21:04,  1.43it/s]Evaluating on VQA val set:  73% 4894/6699 [57:27<20:56,  1.44it/s]Evaluating on VQA val set:  73% 4895/6699 [57:27<21:02,  1.43it/s]Evaluating on VQA val set:  73% 4896/6699 [57:28<20:38,  1.46it/s]Evaluating on VQA val set:  73% 4897/6699 [57:29<20:27,  1.47it/s]Evaluating on VQA val set:  73% 4898/6699 [57:29<20:22,  1.47it/s]Evaluating on VQA val set:  73% 4899/6699 [57:30<20:24,  1.47it/s]Evaluating on VQA val set:  73% 4900/6699 [57:31<20:17,  1.48it/s]Evaluating on VQA val set:  73% 4901/6699 [57:31<20:26,  1.47it/s]Evaluating on VQA val set:  73% 4902/6699 [57:32<20:21,  1.47it/s]Evaluating on VQA val set:  73% 4903/6699 [57:33<20:59,  1.43it/s]Evaluating on VQA val set:  73% 4904/6699 [57:34<20:29,  1.46it/s]Evaluating on VQA val set:  73% 4905/6699 [57:34<21:15,  1.41it/s]Evaluating on VQA val set:  73% 4906/6699 [57:35<21:34,  1.39it/s]Evaluating on VQA val set:  73% 4907/6699 [57:36<21:46,  1.37it/s]Evaluating on VQA val set:  73% 4908/6699 [57:36<21:29,  1.39it/s]Evaluating on VQA val set:  73% 4909/6699 [57:37<20:55,  1.43it/s]Evaluating on VQA val set:  73% 4910/6699 [57:38<20:39,  1.44it/s]Evaluating on VQA val set:  73% 4911/6699 [57:38<20:28,  1.46it/s]Evaluating on VQA val set:  73% 4912/6699 [57:39<20:53,  1.43it/s]Evaluating on VQA val set:  73% 4913/6699 [57:40<21:01,  1.42it/s]Evaluating on VQA val set:  73% 4914/6699 [57:41<20:59,  1.42it/s]Evaluating on VQA val set:  73% 4915/6699 [57:41<21:02,  1.41it/s]Evaluating on VQA val set:  73% 4916/6699 [57:42<20:16,  1.47it/s]Evaluating on VQA val set:  73% 4917/6699 [57:43<20:53,  1.42it/s]Evaluating on VQA val set:  73% 4918/6699 [57:43<21:18,  1.39it/s]Evaluating on VQA val set:  73% 4919/6699 [57:44<21:51,  1.36it/s]Evaluating on VQA val set:  73% 4920/6699 [57:45<21:37,  1.37it/s]Evaluating on VQA val set:  73% 4921/6699 [57:46<21:11,  1.40it/s]Evaluating on VQA val set:  73% 4922/6699 [57:46<20:44,  1.43it/s]Evaluating on VQA val set:  73% 4923/6699 [57:47<20:39,  1.43it/s]Evaluating on VQA val set:  74% 4924/6699 [57:48<21:08,  1.40it/s]Evaluating on VQA val set:  74% 4925/6699 [57:48<21:05,  1.40it/s]Evaluating on VQA val set:  74% 4926/6699 [57:49<20:58,  1.41it/s]Evaluating on VQA val set:  74% 4927/6699 [57:50<21:44,  1.36it/s]Evaluating on VQA val set:  74% 4928/6699 [57:51<21:34,  1.37it/s]Evaluating on VQA val set:  74% 4929/6699 [57:51<21:14,  1.39it/s]Evaluating on VQA val set:  74% 4930/6699 [57:52<20:58,  1.41it/s]Evaluating on VQA val set:  74% 4931/6699 [57:53<21:22,  1.38it/s]Evaluating on VQA val set:  74% 4932/6699 [57:54<21:21,  1.38it/s]Evaluating on VQA val set:  74% 4933/6699 [57:54<21:46,  1.35it/s]Evaluating on VQA val set:  74% 4934/6699 [57:55<21:27,  1.37it/s]Evaluating on VQA val set:  74% 4935/6699 [57:56<21:45,  1.35it/s]Evaluating on VQA val set:  74% 4936/6699 [57:56<21:15,  1.38it/s]Evaluating on VQA val set:  74% 4937/6699 [57:57<20:50,  1.41it/s]Evaluating on VQA val set:  74% 4938/6699 [57:58<20:32,  1.43it/s]Evaluating on VQA val set:  74% 4939/6699 [57:58<20:03,  1.46it/s]Evaluating on VQA val set:  74% 4940/6699 [57:59<20:46,  1.41it/s]Evaluating on VQA val set:  74% 4941/6699 [58:00<21:14,  1.38it/s]Evaluating on VQA val set:  74% 4942/6699 [58:01<21:06,  1.39it/s]Evaluating on VQA val set:  74% 4943/6699 [58:01<21:19,  1.37it/s]Evaluating on VQA val set:  74% 4944/6699 [58:02<21:14,  1.38it/s]Evaluating on VQA val set:  74% 4945/6699 [58:03<21:04,  1.39it/s]Evaluating on VQA val set:  74% 4946/6699 [58:04<21:09,  1.38it/s]Evaluating on VQA val set:  74% 4947/6699 [58:04<21:26,  1.36it/s]Evaluating on VQA val set:  74% 4948/6699 [58:05<20:37,  1.42it/s]Evaluating on VQA val set:  74% 4949/6699 [58:06<20:41,  1.41it/s]Evaluating on VQA val set:  74% 4950/6699 [58:06<20:04,  1.45it/s]Evaluating on VQA val set:  74% 4951/6699 [58:07<20:28,  1.42it/s]Evaluating on VQA val set:  74% 4952/6699 [58:08<20:35,  1.41it/s]Evaluating on VQA val set:  74% 4953/6699 [58:09<20:37,  1.41it/s]Evaluating on VQA val set:  74% 4954/6699 [58:09<20:20,  1.43it/s]Evaluating on VQA val set:  74% 4955/6699 [58:10<20:43,  1.40it/s]Evaluating on VQA val set:  74% 4956/6699 [58:11<20:27,  1.42it/s]Evaluating on VQA val set:  74% 4957/6699 [58:11<20:13,  1.44it/s]Evaluating on VQA val set:  74% 4958/6699 [58:12<20:09,  1.44it/s]Evaluating on VQA val set:  74% 4959/6699 [58:13<20:39,  1.40it/s]Evaluating on VQA val set:  74% 4960/6699 [58:13<20:02,  1.45it/s]Evaluating on VQA val set:  74% 4961/6699 [58:14<20:24,  1.42it/s]Evaluating on VQA val set:  74% 4962/6699 [58:15<20:35,  1.41it/s]Evaluating on VQA val set:  74% 4963/6699 [58:16<19:56,  1.45it/s]Evaluating on VQA val set:  74% 4964/6699 [58:16<20:35,  1.40it/s]Evaluating on VQA val set:  74% 4965/6699 [58:17<20:36,  1.40it/s]Evaluating on VQA val set:  74% 4966/6699 [58:18<20:52,  1.38it/s]Evaluating on VQA val set:  74% 4967/6699 [58:18<20:59,  1.38it/s]Evaluating on VQA val set:  74% 4968/6699 [58:19<21:11,  1.36it/s]Evaluating on VQA val set:  74% 4969/6699 [58:20<20:52,  1.38it/s]Evaluating on VQA val set:  74% 4970/6699 [58:21<20:57,  1.38it/s]Evaluating on VQA val set:  74% 4971/6699 [58:21<20:15,  1.42it/s]Evaluating on VQA val set:  74% 4972/6699 [58:22<20:19,  1.42it/s]Evaluating on VQA val set:  74% 4973/6699 [58:23<19:39,  1.46it/s]Evaluating on VQA val set:  74% 4974/6699 [58:23<19:53,  1.44it/s]Evaluating on VQA val set:  74% 4975/6699 [58:24<20:10,  1.42it/s]Evaluating on VQA val set:  74% 4976/6699 [58:25<20:16,  1.42it/s]Evaluating on VQA val set:  74% 4977/6699 [58:25<19:11,  1.50it/s]Evaluating on VQA val set:  74% 4978/6699 [58:26<18:40,  1.54it/s]Evaluating on VQA val set:  74% 4979/6699 [58:27<18:09,  1.58it/s]Evaluating on VQA val set:  74% 4980/6699 [58:27<19:20,  1.48it/s]Evaluating on VQA val set:  74% 4981/6699 [58:28<20:11,  1.42it/s]Evaluating on VQA val set:  74% 4982/6699 [58:29<20:03,  1.43it/s]Evaluating on VQA val set:  74% 4983/6699 [58:30<20:44,  1.38it/s]Evaluating on VQA val set:  74% 4984/6699 [58:30<20:02,  1.43it/s]Evaluating on VQA val set:  74% 4985/6699 [58:31<20:09,  1.42it/s]Evaluating on VQA val set:  74% 4986/6699 [58:32<20:11,  1.41it/s]Evaluating on VQA val set:  74% 4987/6699 [58:32<20:02,  1.42it/s]Evaluating on VQA val set:  74% 4988/6699 [58:33<20:07,  1.42it/s]Evaluating on VQA val set:  74% 4989/6699 [58:34<19:42,  1.45it/s]Evaluating on VQA val set:  74% 4990/6699 [58:34<20:01,  1.42it/s]Evaluating on VQA val set:  75% 4991/6699 [58:35<20:28,  1.39it/s]Evaluating on VQA val set:  75% 4992/6699 [58:36<20:41,  1.37it/s]Evaluating on VQA val set:  75% 4993/6699 [58:37<20:01,  1.42it/s]Evaluating on VQA val set:  75% 4994/6699 [58:37<20:09,  1.41it/s]Evaluating on VQA val set:  75% 4995/6699 [58:38<20:20,  1.40it/s]Evaluating on VQA val set:  75% 4996/6699 [58:39<20:02,  1.42it/s]Evaluating on VQA val set:  75% 4997/6699 [58:40<20:06,  1.41it/s]Evaluating on VQA val set:  75% 4998/6699 [58:40<20:05,  1.41it/s]Evaluating on VQA val set:  75% 4999/6699 [58:41<20:06,  1.41it/s]Evaluating on VQA val set:  75% 5000/6699 [58:42<19:49,  1.43it/s]Evaluating on VQA val set:  75% 5001/6699 [58:42<18:37,  1.52it/s]Evaluating on VQA val set:  75% 5002/6699 [58:43<18:41,  1.51it/s]Evaluating on VQA val set:  75% 5003/6699 [58:43<18:43,  1.51it/s]Evaluating on VQA val set:  75% 5004/6699 [58:44<18:59,  1.49it/s]Evaluating on VQA val set:  75% 5005/6699 [58:45<19:29,  1.45it/s]Evaluating on VQA val set:  75% 5006/6699 [58:46<20:05,  1.40it/s]Evaluating on VQA val set:  75% 5007/6699 [58:46<20:17,  1.39it/s]Evaluating on VQA val set:  75% 5008/6699 [58:47<20:19,  1.39it/s]Evaluating on VQA val set:  75% 5009/6699 [58:48<20:12,  1.39it/s]Evaluating on VQA val set:  75% 5010/6699 [58:49<20:14,  1.39it/s]Evaluating on VQA val set:  75% 5011/6699 [58:49<19:55,  1.41it/s]Evaluating on VQA val set:  75% 5012/6699 [58:50<19:45,  1.42it/s]Evaluating on VQA val set:  75% 5013/6699 [58:51<19:46,  1.42it/s]Evaluating on VQA val set:  75% 5014/6699 [58:51<19:27,  1.44it/s]Evaluating on VQA val set:  75% 5015/6699 [58:52<20:07,  1.39it/s]Evaluating on VQA val set:  75% 5016/6699 [58:53<20:25,  1.37it/s]Evaluating on VQA val set:  75% 5017/6699 [58:54<19:59,  1.40it/s]Evaluating on VQA val set:  75% 5018/6699 [58:54<19:52,  1.41it/s]Evaluating on VQA val set:  75% 5019/6699 [58:55<20:07,  1.39it/s]Evaluating on VQA val set:  75% 5020/6699 [58:56<19:55,  1.40it/s]Evaluating on VQA val set:  75% 5021/6699 [58:56<19:44,  1.42it/s]Evaluating on VQA val set:  75% 5022/6699 [58:57<19:34,  1.43it/s]Evaluating on VQA val set:  75% 5023/6699 [58:58<19:47,  1.41it/s]Evaluating on VQA val set:  75% 5024/6699 [58:58<19:15,  1.45it/s]Evaluating on VQA val set:  75% 5025/6699 [58:59<19:28,  1.43it/s]Evaluating on VQA val set:  75% 5026/6699 [59:00<19:53,  1.40it/s]Evaluating on VQA val set:  75% 5027/6699 [59:01<20:17,  1.37it/s]Evaluating on VQA val set:  75% 5028/6699 [59:01<20:30,  1.36it/s]Evaluating on VQA val set:  75% 5029/6699 [59:02<20:27,  1.36it/s]Evaluating on VQA val set:  75% 5030/6699 [59:03<19:51,  1.40it/s]Evaluating on VQA val set:  75% 5031/6699 [59:04<19:54,  1.40it/s]Evaluating on VQA val set:  75% 5032/6699 [59:04<19:05,  1.45it/s]Evaluating on VQA val set:  75% 5033/6699 [59:05<18:32,  1.50it/s]Evaluating on VQA val set:  75% 5034/6699 [59:05<18:46,  1.48it/s]Evaluating on VQA val set:  75% 5035/6699 [59:06<18:36,  1.49it/s]Evaluating on VQA val set:  75% 5036/6699 [59:07<18:17,  1.52it/s]Evaluating on VQA val set:  75% 5037/6699 [59:07<18:28,  1.50it/s]Evaluating on VQA val set:  75% 5038/6699 [59:08<18:45,  1.48it/s]Evaluating on VQA val set:  75% 5039/6699 [59:09<18:49,  1.47it/s]Evaluating on VQA val set:  75% 5040/6699 [59:10<19:41,  1.40it/s]Evaluating on VQA val set:  75% 5041/6699 [59:10<19:12,  1.44it/s]Evaluating on VQA val set:  75% 5042/6699 [59:11<19:33,  1.41it/s]Evaluating on VQA val set:  75% 5043/6699 [59:12<19:25,  1.42it/s]Evaluating on VQA val set:  75% 5044/6699 [59:12<19:56,  1.38it/s]Evaluating on VQA val set:  75% 5045/6699 [59:13<19:15,  1.43it/s]Evaluating on VQA val set:  75% 5046/6699 [59:14<18:46,  1.47it/s]Evaluating on VQA val set:  75% 5047/6699 [59:14<18:56,  1.45it/s]Evaluating on VQA val set:  75% 5048/6699 [59:15<18:31,  1.48it/s]Evaluating on VQA val set:  75% 5049/6699 [59:16<19:15,  1.43it/s]Evaluating on VQA val set:  75% 5050/6699 [59:17<19:37,  1.40it/s]Evaluating on VQA val set:  75% 5051/6699 [59:17<19:37,  1.40it/s]Evaluating on VQA val set:  75% 5052/6699 [59:18<19:26,  1.41it/s]Evaluating on VQA val set:  75% 5053/6699 [59:19<19:25,  1.41it/s]Evaluating on VQA val set:  75% 5054/6699 [59:19<19:18,  1.42it/s]Evaluating on VQA val set:  75% 5055/6699 [59:20<19:15,  1.42it/s]Evaluating on VQA val set:  75% 5056/6699 [59:21<19:40,  1.39it/s]Evaluating on VQA val set:  75% 5057/6699 [59:22<18:56,  1.44it/s]Evaluating on VQA val set:  76% 5058/6699 [59:22<18:37,  1.47it/s]Evaluating on VQA val set:  76% 5059/6699 [59:23<18:37,  1.47it/s]Evaluating on VQA val set:  76% 5060/6699 [59:24<18:58,  1.44it/s]Evaluating on VQA val set:  76% 5061/6699 [59:24<18:11,  1.50it/s]Evaluating on VQA val set:  76% 5062/6699 [59:25<18:25,  1.48it/s]Evaluating on VQA val set:  76% 5063/6699 [59:25<17:30,  1.56it/s]Evaluating on VQA val set:  76% 5064/6699 [59:26<17:37,  1.55it/s]Evaluating on VQA val set:  76% 5065/6699 [59:27<18:00,  1.51it/s]Evaluating on VQA val set:  76% 5066/6699 [59:27<17:48,  1.53it/s]Evaluating on VQA val set:  76% 5067/6699 [59:28<18:13,  1.49it/s]Evaluating on VQA val set:  76% 5068/6699 [59:29<18:50,  1.44it/s]Evaluating on VQA val set:  76% 5069/6699 [59:30<19:02,  1.43it/s]Evaluating on VQA val set:  76% 5070/6699 [59:30<19:04,  1.42it/s]Evaluating on VQA val set:  76% 5071/6699 [59:31<18:39,  1.45it/s]Evaluating on VQA val set:  76% 5072/6699 [59:32<18:44,  1.45it/s]Evaluating on VQA val set:  76% 5073/6699 [59:32<18:14,  1.49it/s]Evaluating on VQA val set:  76% 5074/6699 [59:33<18:14,  1.48it/s]Evaluating on VQA val set:  76% 5075/6699 [59:34<18:11,  1.49it/s]Evaluating on VQA val set:  76% 5076/6699 [59:34<18:59,  1.42it/s]Evaluating on VQA val set:  76% 5077/6699 [59:35<19:19,  1.40it/s]Evaluating on VQA val set:  76% 5078/6699 [59:36<19:50,  1.36it/s]Evaluating on VQA val set:  76% 5079/6699 [59:37<19:31,  1.38it/s]Evaluating on VQA val set:  76% 5080/6699 [59:37<19:28,  1.39it/s]Evaluating on VQA val set:  76% 5081/6699 [59:38<19:28,  1.38it/s]Evaluating on VQA val set:  76% 5082/6699 [59:39<19:17,  1.40it/s]Evaluating on VQA val set:  76% 5083/6699 [59:39<19:20,  1.39it/s]Evaluating on VQA val set:  76% 5084/6699 [59:40<19:06,  1.41it/s]Evaluating on VQA val set:  76% 5085/6699 [59:41<18:59,  1.42it/s]Evaluating on VQA val set:  76% 5086/6699 [59:42<18:52,  1.42it/s]Evaluating on VQA val set:  76% 5087/6699 [59:42<18:32,  1.45it/s]Evaluating on VQA val set:  76% 5088/6699 [59:43<18:17,  1.47it/s]Evaluating on VQA val set:  76% 5089/6699 [59:44<19:01,  1.41it/s]Evaluating on VQA val set:  76% 5090/6699 [59:44<19:13,  1.39it/s]Evaluating on VQA val set:  76% 5091/6699 [59:45<19:14,  1.39it/s]Evaluating on VQA val set:  76% 5092/6699 [59:46<19:01,  1.41it/s]Evaluating on VQA val set:  76% 5093/6699 [59:46<18:14,  1.47it/s]Evaluating on VQA val set:  76% 5094/6699 [59:47<18:07,  1.48it/s]Evaluating on VQA val set:  76% 5095/6699 [59:48<17:50,  1.50it/s]Evaluating on VQA val set:  76% 5096/6699 [59:48<18:16,  1.46it/s]Evaluating on VQA val set:  76% 5097/6699 [59:49<18:55,  1.41it/s]Evaluating on VQA val set:  76% 5098/6699 [59:50<18:19,  1.46it/s]Evaluating on VQA val set:  76% 5099/6699 [59:51<18:32,  1.44it/s]Evaluating on VQA val set:  76% 5100/6699 [59:51<18:47,  1.42it/s]Evaluating on VQA val set:  76% 5101/6699 [59:52<18:20,  1.45it/s]Evaluating on VQA val set:  76% 5102/6699 [59:53<18:17,  1.46it/s]Evaluating on VQA val set:  76% 5103/6699 [59:53<18:21,  1.45it/s]Evaluating on VQA val set:  76% 5104/6699 [59:54<18:09,  1.46it/s]Evaluating on VQA val set:  76% 5105/6699 [59:55<18:08,  1.46it/s]Evaluating on VQA val set:  76% 5106/6699 [59:55<18:22,  1.44it/s]Evaluating on VQA val set:  76% 5107/6699 [59:56<19:06,  1.39it/s]Evaluating on VQA val set:  76% 5108/6699 [59:57<19:02,  1.39it/s]Evaluating on VQA val set:  76% 5109/6699 [59:58<19:15,  1.38it/s]Evaluating on VQA val set:  76% 5110/6699 [59:58<19:03,  1.39it/s]Evaluating on VQA val set:  76% 5111/6699 [59:59<19:12,  1.38it/s]Evaluating on VQA val set:  76% 5112/6699 [1:00:00<18:43,  1.41it/s]Evaluating on VQA val set:  76% 5113/6699 [1:00:00<18:32,  1.43it/s]Evaluating on VQA val set:  76% 5114/6699 [1:00:01<18:39,  1.42it/s]Evaluating on VQA val set:  76% 5115/6699 [1:00:02<18:26,  1.43it/s]Evaluating on VQA val set:  76% 5116/6699 [1:00:03<18:15,  1.45it/s]Evaluating on VQA val set:  76% 5117/6699 [1:00:03<17:38,  1.49it/s]Evaluating on VQA val set:  76% 5118/6699 [1:00:04<18:01,  1.46it/s]Evaluating on VQA val set:  76% 5119/6699 [1:00:05<18:00,  1.46it/s]Evaluating on VQA val set:  76% 5120/6699 [1:00:05<18:27,  1.43it/s]Evaluating on VQA val set:  76% 5121/6699 [1:00:06<18:41,  1.41it/s]Evaluating on VQA val set:  76% 5122/6699 [1:00:07<18:43,  1.40it/s]Evaluating on VQA val set:  76% 5123/6699 [1:00:07<18:14,  1.44it/s]Evaluating on VQA val set:  76% 5124/6699 [1:00:08<18:32,  1.42it/s]Evaluating on VQA val set:  77% 5125/6699 [1:00:09<18:22,  1.43it/s]Evaluating on VQA val set:  77% 5126/6699 [1:00:09<17:55,  1.46it/s]Evaluating on VQA val set:  77% 5127/6699 [1:00:10<17:50,  1.47it/s]Evaluating on VQA val set:  77% 5128/6699 [1:00:11<17:52,  1.47it/s]Evaluating on VQA val set:  77% 5129/6699 [1:00:12<18:08,  1.44it/s]Evaluating on VQA val set:  77% 5130/6699 [1:00:12<17:49,  1.47it/s]Evaluating on VQA val set:  77% 5131/6699 [1:00:13<17:03,  1.53it/s]Evaluating on VQA val set:  77% 5132/6699 [1:00:13<17:28,  1.49it/s]Evaluating on VQA val set:  77% 5133/6699 [1:00:14<17:39,  1.48it/s]Evaluating on VQA val set:  77% 5134/6699 [1:00:15<17:33,  1.49it/s]Evaluating on VQA val set:  77% 5135/6699 [1:00:16<17:52,  1.46it/s]Evaluating on VQA val set:  77% 5136/6699 [1:00:16<17:39,  1.47it/s]Evaluating on VQA val set:  77% 5137/6699 [1:00:17<17:24,  1.50it/s]Evaluating on VQA val set:  77% 5138/6699 [1:00:18<17:46,  1.46it/s]Evaluating on VQA val set:  77% 5139/6699 [1:00:18<17:47,  1.46it/s]Evaluating on VQA val set:  77% 5140/6699 [1:00:19<18:24,  1.41it/s]Evaluating on VQA val set:  77% 5141/6699 [1:00:20<18:17,  1.42it/s]Evaluating on VQA val set:  77% 5142/6699 [1:00:20<18:35,  1.40it/s]Evaluating on VQA val set:  77% 5143/6699 [1:00:21<18:32,  1.40it/s]Evaluating on VQA val set:  77% 5144/6699 [1:00:22<18:33,  1.40it/s]Evaluating on VQA val set:  77% 5145/6699 [1:00:23<18:52,  1.37it/s]Evaluating on VQA val set:  77% 5146/6699 [1:00:23<18:53,  1.37it/s]Evaluating on VQA val set:  77% 5147/6699 [1:00:24<18:16,  1.41it/s]Evaluating on VQA val set:  77% 5148/6699 [1:00:25<18:03,  1.43it/s]Evaluating on VQA val set:  77% 5149/6699 [1:00:25<17:37,  1.47it/s]Evaluating on VQA val set:  77% 5150/6699 [1:00:26<18:13,  1.42it/s]Evaluating on VQA val set:  77% 5151/6699 [1:00:27<18:27,  1.40it/s]Evaluating on VQA val set:  77% 5152/6699 [1:00:28<18:18,  1.41it/s]Evaluating on VQA val set:  77% 5153/6699 [1:00:28<18:28,  1.39it/s]Evaluating on VQA val set:  77% 5154/6699 [1:00:29<18:08,  1.42it/s]Evaluating on VQA val set:  77% 5155/6699 [1:00:30<18:14,  1.41it/s]Evaluating on VQA val set:  77% 5156/6699 [1:00:30<17:43,  1.45it/s]Evaluating on VQA val set:  77% 5157/6699 [1:00:31<17:42,  1.45it/s]Evaluating on VQA val set:  77% 5158/6699 [1:00:32<18:21,  1.40it/s]Evaluating on VQA val set:  77% 5159/6699 [1:00:32<18:16,  1.40it/s]Evaluating on VQA val set:  77% 5160/6699 [1:00:33<18:16,  1.40it/s]Evaluating on VQA val set:  77% 5161/6699 [1:00:34<17:54,  1.43it/s]Evaluating on VQA val set:  77% 5162/6699 [1:00:35<17:59,  1.42it/s]Evaluating on VQA val set:  77% 5163/6699 [1:00:35<17:39,  1.45it/s]Evaluating on VQA val set:  77% 5164/6699 [1:00:36<17:59,  1.42it/s]Evaluating on VQA val set:  77% 5165/6699 [1:00:37<18:16,  1.40it/s]Evaluating on VQA val set:  77% 5166/6699 [1:00:37<18:31,  1.38it/s]Evaluating on VQA val set:  77% 5167/6699 [1:00:38<18:32,  1.38it/s]Evaluating on VQA val set:  77% 5168/6699 [1:00:39<17:57,  1.42it/s]Evaluating on VQA val set:  77% 5169/6699 [1:00:40<18:13,  1.40it/s]Evaluating on VQA val set:  77% 5170/6699 [1:00:40<18:25,  1.38it/s]Evaluating on VQA val set:  77% 5171/6699 [1:00:41<18:36,  1.37it/s]Evaluating on VQA val set:  77% 5172/6699 [1:00:42<18:26,  1.38it/s]Evaluating on VQA val set:  77% 5173/6699 [1:00:43<18:25,  1.38it/s]Evaluating on VQA val set:  77% 5174/6699 [1:00:43<17:26,  1.46it/s]Evaluating on VQA val set:  77% 5175/6699 [1:00:44<17:27,  1.46it/s]Evaluating on VQA val set:  77% 5176/6699 [1:00:45<17:49,  1.42it/s]Evaluating on VQA val set:  77% 5177/6699 [1:00:45<17:39,  1.44it/s]Evaluating on VQA val set:  77% 5178/6699 [1:00:46<17:56,  1.41it/s]Evaluating on VQA val set:  77% 5179/6699 [1:00:47<17:44,  1.43it/s]Evaluating on VQA val set:  77% 5180/6699 [1:00:47<18:01,  1.41it/s]Evaluating on VQA val set:  77% 5181/6699 [1:00:48<17:58,  1.41it/s]Evaluating on VQA val set:  77% 5182/6699 [1:00:49<18:23,  1.37it/s]Evaluating on VQA val set:  77% 5183/6699 [1:00:50<17:50,  1.42it/s]Evaluating on VQA val set:  77% 5184/6699 [1:00:50<17:32,  1.44it/s]Evaluating on VQA val set:  77% 5185/6699 [1:00:51<17:24,  1.45it/s]Evaluating on VQA val set:  77% 5186/6699 [1:00:52<17:25,  1.45it/s]Evaluating on VQA val set:  77% 5187/6699 [1:00:52<17:04,  1.48it/s]Evaluating on VQA val set:  77% 5188/6699 [1:00:53<17:10,  1.47it/s]Evaluating on VQA val set:  77% 5189/6699 [1:00:54<17:33,  1.43it/s]Evaluating on VQA val set:  77% 5190/6699 [1:00:54<18:10,  1.38it/s]Evaluating on VQA val set:  77% 5191/6699 [1:00:55<18:10,  1.38it/s]Evaluating on VQA val set:  78% 5192/6699 [1:00:56<18:15,  1.38it/s]Evaluating on VQA val set:  78% 5193/6699 [1:00:57<18:06,  1.39it/s]Evaluating on VQA val set:  78% 5194/6699 [1:00:57<17:41,  1.42it/s]Evaluating on VQA val set:  78% 5195/6699 [1:00:58<18:02,  1.39it/s]Evaluating on VQA val set:  78% 5196/6699 [1:00:59<17:50,  1.40it/s]Evaluating on VQA val set:  78% 5197/6699 [1:00:59<17:50,  1.40it/s]Evaluating on VQA val set:  78% 5198/6699 [1:01:00<17:07,  1.46it/s]Evaluating on VQA val set:  78% 5199/6699 [1:01:01<17:11,  1.45it/s]Evaluating on VQA val set:  78% 5200/6699 [1:01:01<17:11,  1.45it/s]Evaluating on VQA val set:  78% 5201/6699 [1:01:02<17:07,  1.46it/s]Evaluating on VQA val set:  78% 5202/6699 [1:01:03<17:21,  1.44it/s]Evaluating on VQA val set:  78% 5203/6699 [1:01:04<17:53,  1.39it/s]Evaluating on VQA val set:  78% 5204/6699 [1:01:04<17:42,  1.41it/s]Evaluating on VQA val set:  78% 5205/6699 [1:01:05<17:47,  1.40it/s]Evaluating on VQA val set:  78% 5206/6699 [1:01:06<17:51,  1.39it/s]Evaluating on VQA val set:  78% 5207/6699 [1:01:06<17:28,  1.42it/s]Evaluating on VQA val set:  78% 5208/6699 [1:01:07<17:31,  1.42it/s]Evaluating on VQA val set:  78% 5209/6699 [1:01:08<17:40,  1.40it/s]Evaluating on VQA val set:  78% 5210/6699 [1:01:08<16:48,  1.48it/s]Evaluating on VQA val set:  78% 5211/6699 [1:01:09<16:54,  1.47it/s]Evaluating on VQA val set:  78% 5212/6699 [1:01:10<17:01,  1.46it/s]Evaluating on VQA val set:  78% 5213/6699 [1:01:10<16:44,  1.48it/s]Evaluating on VQA val set:  78% 5214/6699 [1:01:11<16:56,  1.46it/s]Evaluating on VQA val set:  78% 5215/6699 [1:01:12<17:06,  1.45it/s]Evaluating on VQA val set:  78% 5216/6699 [1:01:13<17:16,  1.43it/s]Evaluating on VQA val set:  78% 5217/6699 [1:01:13<17:29,  1.41it/s]Evaluating on VQA val set:  78% 5218/6699 [1:01:14<17:17,  1.43it/s]Evaluating on VQA val set:  78% 5219/6699 [1:01:15<17:43,  1.39it/s]Evaluating on VQA val set:  78% 5220/6699 [1:01:15<17:49,  1.38it/s]Evaluating on VQA val set:  78% 5221/6699 [1:01:16<17:19,  1.42it/s]Evaluating on VQA val set:  78% 5222/6699 [1:01:17<17:21,  1.42it/s]Evaluating on VQA val set:  78% 5223/6699 [1:01:18<17:29,  1.41it/s]Evaluating on VQA val set:  78% 5224/6699 [1:01:18<17:11,  1.43it/s]Evaluating on VQA val set:  78% 5225/6699 [1:01:19<17:15,  1.42it/s]Evaluating on VQA val set:  78% 5226/6699 [1:01:20<17:51,  1.37it/s]Evaluating on VQA val set:  78% 5227/6699 [1:01:20<17:39,  1.39it/s]Evaluating on VQA val set:  78% 5228/6699 [1:01:21<17:39,  1.39it/s]Evaluating on VQA val set:  78% 5229/6699 [1:01:22<17:34,  1.39it/s]Evaluating on VQA val set:  78% 5230/6699 [1:01:23<17:08,  1.43it/s]Evaluating on VQA val set:  78% 5231/6699 [1:01:23<16:21,  1.50it/s]Evaluating on VQA val set:  78% 5232/6699 [1:01:24<16:36,  1.47it/s]Evaluating on VQA val set:  78% 5233/6699 [1:01:24<16:11,  1.51it/s]Evaluating on VQA val set:  78% 5234/6699 [1:01:25<15:50,  1.54it/s]Evaluating on VQA val set:  78% 5235/6699 [1:01:26<16:25,  1.48it/s]Evaluating on VQA val set:  78% 5236/6699 [1:01:27<17:13,  1.42it/s]Evaluating on VQA val set:  78% 5237/6699 [1:01:27<17:37,  1.38it/s]Evaluating on VQA val set:  78% 5238/6699 [1:01:28<17:44,  1.37it/s]Evaluating on VQA val set:  78% 5239/6699 [1:01:29<17:33,  1.39it/s]Evaluating on VQA val set:  78% 5240/6699 [1:01:29<17:14,  1.41it/s]Evaluating on VQA val set:  78% 5241/6699 [1:01:30<17:45,  1.37it/s]Evaluating on VQA val set:  78% 5242/6699 [1:01:31<16:46,  1.45it/s]Evaluating on VQA val set:  78% 5243/6699 [1:01:32<17:03,  1.42it/s]Evaluating on VQA val set:  78% 5244/6699 [1:01:32<16:49,  1.44it/s]Evaluating on VQA val set:  78% 5245/6699 [1:01:33<16:45,  1.45it/s]Evaluating on VQA val set:  78% 5246/6699 [1:01:34<16:42,  1.45it/s]Evaluating on VQA val set:  78% 5247/6699 [1:01:34<17:02,  1.42it/s]Evaluating on VQA val set:  78% 5248/6699 [1:01:35<16:40,  1.45it/s]Evaluating on VQA val set:  78% 5249/6699 [1:01:36<17:09,  1.41it/s]Evaluating on VQA val set:  78% 5250/6699 [1:01:37<17:16,  1.40it/s]Evaluating on VQA val set:  78% 5251/6699 [1:01:37<17:25,  1.38it/s]Evaluating on VQA val set:  78% 5252/6699 [1:01:38<17:13,  1.40it/s]Evaluating on VQA val set:  78% 5253/6699 [1:01:39<16:49,  1.43it/s]Evaluating on VQA val set:  78% 5254/6699 [1:01:39<16:46,  1.44it/s]Evaluating on VQA val set:  78% 5255/6699 [1:01:40<16:01,  1.50it/s]Evaluating on VQA val set:  78% 5256/6699 [1:01:41<15:54,  1.51it/s]Evaluating on VQA val set:  78% 5257/6699 [1:01:41<16:23,  1.47it/s]Evaluating on VQA val set:  78% 5258/6699 [1:01:42<16:32,  1.45it/s]Evaluating on VQA val set:  79% 5259/6699 [1:01:43<16:53,  1.42it/s]Evaluating on VQA val set:  79% 5260/6699 [1:01:43<17:04,  1.40it/s]Evaluating on VQA val set:  79% 5261/6699 [1:01:44<16:48,  1.43it/s]Evaluating on VQA val set:  79% 5262/6699 [1:01:45<16:45,  1.43it/s]Evaluating on VQA val set:  79% 5263/6699 [1:01:46<17:11,  1.39it/s]Evaluating on VQA val set:  79% 5264/6699 [1:01:46<17:13,  1.39it/s]Evaluating on VQA val set:  79% 5265/6699 [1:01:47<17:29,  1.37it/s]Evaluating on VQA val set:  79% 5266/6699 [1:01:48<16:49,  1.42it/s]Evaluating on VQA val set:  79% 5267/6699 [1:01:48<16:48,  1.42it/s]Evaluating on VQA val set:  79% 5268/6699 [1:01:49<16:31,  1.44it/s]Evaluating on VQA val set:  79% 5269/6699 [1:01:50<17:04,  1.40it/s]Evaluating on VQA val set:  79% 5270/6699 [1:01:51<16:54,  1.41it/s]Evaluating on VQA val set:  79% 5271/6699 [1:01:51<17:24,  1.37it/s]Evaluating on VQA val set:  79% 5272/6699 [1:01:52<16:45,  1.42it/s]Evaluating on VQA val set:  79% 5273/6699 [1:01:53<16:35,  1.43it/s]Evaluating on VQA val set:  79% 5274/6699 [1:01:53<15:43,  1.51it/s]Evaluating on VQA val set:  79% 5275/6699 [1:01:54<15:47,  1.50it/s]Evaluating on VQA val set:  79% 5276/6699 [1:01:55<15:38,  1.52it/s]Evaluating on VQA val set:  79% 5277/6699 [1:01:55<15:58,  1.48it/s]Evaluating on VQA val set:  79% 5278/6699 [1:01:56<15:11,  1.56it/s]Evaluating on VQA val set:  79% 5279/6699 [1:01:56<14:23,  1.64it/s]Evaluating on VQA val set:  79% 5280/6699 [1:01:57<14:49,  1.60it/s]Evaluating on VQA val set:  79% 5281/6699 [1:01:58<15:53,  1.49it/s]Evaluating on VQA val set:  79% 5282/6699 [1:01:59<16:09,  1.46it/s]Evaluating on VQA val set:  79% 5283/6699 [1:01:59<16:55,  1.39it/s]Evaluating on VQA val set:  79% 5284/6699 [1:02:00<16:34,  1.42it/s]Evaluating on VQA val set:  79% 5285/6699 [1:02:01<16:29,  1.43it/s]Evaluating on VQA val set:  79% 5286/6699 [1:02:01<16:30,  1.43it/s]Evaluating on VQA val set:  79% 5287/6699 [1:02:02<16:46,  1.40it/s]Evaluating on VQA val set:  79% 5288/6699 [1:02:03<17:00,  1.38it/s]Evaluating on VQA val set:  79% 5289/6699 [1:02:04<17:22,  1.35it/s]Evaluating on VQA val set:  79% 5290/6699 [1:02:04<17:03,  1.38it/s]Evaluating on VQA val set:  79% 5291/6699 [1:02:05<17:08,  1.37it/s]Evaluating on VQA val set:  79% 5292/6699 [1:02:06<16:53,  1.39it/s]Evaluating on VQA val set:  79% 5293/6699 [1:02:06<16:37,  1.41it/s]Evaluating on VQA val set:  79% 5294/6699 [1:02:07<16:08,  1.45it/s]Evaluating on VQA val set:  79% 5295/6699 [1:02:08<16:36,  1.41it/s]Evaluating on VQA val set:  79% 5296/6699 [1:02:09<16:27,  1.42it/s]Evaluating on VQA val set:  79% 5297/6699 [1:02:09<15:58,  1.46it/s]Evaluating on VQA val set:  79% 5298/6699 [1:02:10<16:07,  1.45it/s]Evaluating on VQA val set:  79% 5299/6699 [1:02:11<16:19,  1.43it/s]Evaluating on VQA val set:  79% 5300/6699 [1:02:11<16:56,  1.38it/s]Evaluating on VQA val set:  79% 5301/6699 [1:02:12<16:41,  1.40it/s]Evaluating on VQA val set:  79% 5302/6699 [1:02:13<16:55,  1.38it/s]Evaluating on VQA val set:  79% 5303/6699 [1:02:14<16:49,  1.38it/s]Evaluating on VQA val set:  79% 5304/6699 [1:02:14<16:51,  1.38it/s]Evaluating on VQA val set:  79% 5305/6699 [1:02:15<16:51,  1.38it/s]Evaluating on VQA val set:  79% 5306/6699 [1:02:16<16:20,  1.42it/s]Evaluating on VQA val set:  79% 5307/6699 [1:02:16<16:31,  1.40it/s]Evaluating on VQA val set:  79% 5308/6699 [1:02:17<15:58,  1.45it/s]Evaluating on VQA val set:  79% 5309/6699 [1:02:18<16:24,  1.41it/s]Evaluating on VQA val set:  79% 5310/6699 [1:02:18<16:16,  1.42it/s]Evaluating on VQA val set:  79% 5311/6699 [1:02:19<16:15,  1.42it/s]Evaluating on VQA val set:  79% 5312/6699 [1:02:20<16:02,  1.44it/s]Evaluating on VQA val set:  79% 5313/6699 [1:02:21<16:16,  1.42it/s]Evaluating on VQA val set:  79% 5314/6699 [1:02:21<16:00,  1.44it/s]Evaluating on VQA val set:  79% 5315/6699 [1:02:22<16:05,  1.43it/s]Evaluating on VQA val set:  79% 5316/6699 [1:02:23<16:15,  1.42it/s]Evaluating on VQA val set:  79% 5317/6699 [1:02:23<16:19,  1.41it/s]Evaluating on VQA val set:  79% 5318/6699 [1:02:24<16:37,  1.38it/s]Evaluating on VQA val set:  79% 5319/6699 [1:02:25<16:17,  1.41it/s]Evaluating on VQA val set:  79% 5320/6699 [1:02:26<16:44,  1.37it/s]Evaluating on VQA val set:  79% 5321/6699 [1:02:26<16:31,  1.39it/s]Evaluating on VQA val set:  79% 5322/6699 [1:02:27<16:30,  1.39it/s]Evaluating on VQA val set:  79% 5323/6699 [1:02:28<16:16,  1.41it/s]Evaluating on VQA val set:  79% 5324/6699 [1:02:28<15:35,  1.47it/s]Evaluating on VQA val set:  79% 5325/6699 [1:02:29<15:52,  1.44it/s]Evaluating on VQA val set:  80% 5326/6699 [1:02:30<15:47,  1.45it/s]Evaluating on VQA val set:  80% 5327/6699 [1:02:30<15:45,  1.45it/s]Evaluating on VQA val set:  80% 5328/6699 [1:02:31<16:02,  1.42it/s]Evaluating on VQA val set:  80% 5329/6699 [1:02:32<16:09,  1.41it/s]Evaluating on VQA val set:  80% 5330/6699 [1:02:33<15:56,  1.43it/s]Evaluating on VQA val set:  80% 5331/6699 [1:02:33<15:58,  1.43it/s]Evaluating on VQA val set:  80% 5332/6699 [1:02:34<15:52,  1.44it/s]Evaluating on VQA val set:  80% 5333/6699 [1:02:35<15:58,  1.43it/s]Evaluating on VQA val set:  80% 5334/6699 [1:02:35<15:51,  1.44it/s]Evaluating on VQA val set:  80% 5335/6699 [1:02:36<15:54,  1.43it/s]Evaluating on VQA val set:  80% 5336/6699 [1:02:37<16:15,  1.40it/s]Evaluating on VQA val set:  80% 5337/6699 [1:02:38<16:16,  1.40it/s]Evaluating on VQA val set:  80% 5338/6699 [1:02:38<16:23,  1.38it/s]Evaluating on VQA val set:  80% 5339/6699 [1:02:39<16:26,  1.38it/s]Evaluating on VQA val set:  80% 5340/6699 [1:02:40<16:14,  1.39it/s]Evaluating on VQA val set:  80% 5341/6699 [1:02:40<16:06,  1.41it/s]Evaluating on VQA val set:  80% 5342/6699 [1:02:41<16:04,  1.41it/s]Evaluating on VQA val set:  80% 5343/6699 [1:02:42<15:20,  1.47it/s]Evaluating on VQA val set:  80% 5344/6699 [1:02:42<15:42,  1.44it/s]Evaluating on VQA val set:  80% 5345/6699 [1:02:43<15:48,  1.43it/s]Evaluating on VQA val set:  80% 5346/6699 [1:02:44<16:20,  1.38it/s]Evaluating on VQA val set:  80% 5347/6699 [1:02:45<16:40,  1.35it/s]Evaluating on VQA val set:  80% 5348/6699 [1:02:45<16:28,  1.37it/s]Evaluating on VQA val set:  80% 5349/6699 [1:02:46<16:21,  1.37it/s]Evaluating on VQA val set:  80% 5350/6699 [1:02:47<16:14,  1.38it/s]Evaluating on VQA val set:  80% 5351/6699 [1:02:47<15:23,  1.46it/s]Evaluating on VQA val set:  80% 5352/6699 [1:02:48<15:34,  1.44it/s]Evaluating on VQA val set:  80% 5353/6699 [1:02:49<15:45,  1.42it/s]Evaluating on VQA val set:  80% 5354/6699 [1:02:50<15:39,  1.43it/s]Evaluating on VQA val set:  80% 5355/6699 [1:02:50<15:32,  1.44it/s]Evaluating on VQA val set:  80% 5356/6699 [1:02:51<15:46,  1.42it/s]Evaluating on VQA val set:  80% 5357/6699 [1:02:52<15:35,  1.43it/s]Evaluating on VQA val set:  80% 5358/6699 [1:02:52<15:17,  1.46it/s]Evaluating on VQA val set:  80% 5359/6699 [1:02:53<15:08,  1.47it/s]Evaluating on VQA val set:  80% 5360/6699 [1:02:54<15:22,  1.45it/s]Evaluating on VQA val set:  80% 5361/6699 [1:02:54<15:40,  1.42it/s]Evaluating on VQA val set:  80% 5362/6699 [1:02:55<15:52,  1.40it/s]Evaluating on VQA val set:  80% 5363/6699 [1:02:56<15:44,  1.41it/s]Evaluating on VQA val set:  80% 5364/6699 [1:02:57<15:52,  1.40it/s]Evaluating on VQA val set:  80% 5365/6699 [1:02:57<16:09,  1.38it/s]Evaluating on VQA val set:  80% 5366/6699 [1:02:58<15:40,  1.42it/s]Evaluating on VQA val set:  80% 5367/6699 [1:02:59<15:44,  1.41it/s]Evaluating on VQA val set:  80% 5368/6699 [1:02:59<15:44,  1.41it/s]Evaluating on VQA val set:  80% 5369/6699 [1:03:00<15:25,  1.44it/s]Evaluating on VQA val set:  80% 5370/6699 [1:03:01<15:38,  1.42it/s]Evaluating on VQA val set:  80% 5371/6699 [1:03:02<15:36,  1.42it/s]Evaluating on VQA val set:  80% 5372/6699 [1:03:02<16:11,  1.37it/s]Evaluating on VQA val set:  80% 5373/6699 [1:03:03<15:45,  1.40it/s]Evaluating on VQA val set:  80% 5374/6699 [1:03:04<15:43,  1.41it/s]Evaluating on VQA val set:  80% 5375/6699 [1:03:04<15:45,  1.40it/s]Evaluating on VQA val set:  80% 5376/6699 [1:03:05<15:15,  1.45it/s]Evaluating on VQA val set:  80% 5377/6699 [1:03:06<15:03,  1.46it/s]Evaluating on VQA val set:  80% 5378/6699 [1:03:06<15:21,  1.43it/s]Evaluating on VQA val set:  80% 5379/6699 [1:03:07<15:50,  1.39it/s]Evaluating on VQA val set:  80% 5380/6699 [1:03:08<15:23,  1.43it/s]Evaluating on VQA val set:  80% 5381/6699 [1:03:09<15:19,  1.43it/s]Evaluating on VQA val set:  80% 5382/6699 [1:03:09<15:17,  1.43it/s]Evaluating on VQA val set:  80% 5383/6699 [1:03:10<15:24,  1.42it/s]Evaluating on VQA val set:  80% 5384/6699 [1:03:11<15:25,  1.42it/s]Evaluating on VQA val set:  80% 5385/6699 [1:03:11<15:17,  1.43it/s]Evaluating on VQA val set:  80% 5386/6699 [1:03:12<15:07,  1.45it/s]Evaluating on VQA val set:  80% 5387/6699 [1:03:13<15:28,  1.41it/s]Evaluating on VQA val set:  80% 5388/6699 [1:03:14<15:29,  1.41it/s]Evaluating on VQA val set:  80% 5389/6699 [1:03:14<15:33,  1.40it/s]Evaluating on VQA val set:  80% 5390/6699 [1:03:15<15:38,  1.40it/s]Evaluating on VQA val set:  80% 5391/6699 [1:03:16<15:43,  1.39it/s]Evaluating on VQA val set:  80% 5392/6699 [1:03:16<15:37,  1.39it/s]Evaluating on VQA val set:  81% 5393/6699 [1:03:17<15:56,  1.37it/s]Evaluating on VQA val set:  81% 5394/6699 [1:03:18<16:00,  1.36it/s]Evaluating on VQA val set:  81% 5395/6699 [1:03:19<15:34,  1.40it/s]Evaluating on VQA val set:  81% 5396/6699 [1:03:19<15:35,  1.39it/s]Evaluating on VQA val set:  81% 5397/6699 [1:03:20<15:07,  1.43it/s]Evaluating on VQA val set:  81% 5398/6699 [1:03:21<15:10,  1.43it/s]Evaluating on VQA val set:  81% 5399/6699 [1:03:21<15:20,  1.41it/s]Evaluating on VQA val set:  81% 5400/6699 [1:03:22<15:27,  1.40it/s]Evaluating on VQA val set:  81% 5401/6699 [1:03:23<15:24,  1.40it/s]Evaluating on VQA val set:  81% 5402/6699 [1:03:24<15:22,  1.41it/s]Evaluating on VQA val set:  81% 5403/6699 [1:03:24<15:10,  1.42it/s]Evaluating on VQA val set:  81% 5404/6699 [1:03:25<14:14,  1.52it/s]Evaluating on VQA val set:  81% 5405/6699 [1:03:25<14:32,  1.48it/s]Evaluating on VQA val set:  81% 5406/6699 [1:03:26<14:53,  1.45it/s]Evaluating on VQA val set:  81% 5407/6699 [1:03:27<14:55,  1.44it/s]Evaluating on VQA val set:  81% 5408/6699 [1:03:28<14:55,  1.44it/s]Evaluating on VQA val set:  81% 5409/6699 [1:03:28<14:57,  1.44it/s]Evaluating on VQA val set:  81% 5410/6699 [1:03:29<14:55,  1.44it/s]Evaluating on VQA val set:  81% 5411/6699 [1:03:30<15:00,  1.43it/s]Evaluating on VQA val set:  81% 5412/6699 [1:03:30<14:41,  1.46it/s]Evaluating on VQA val set:  81% 5413/6699 [1:03:31<14:50,  1.44it/s]Evaluating on VQA val set:  81% 5414/6699 [1:03:32<15:13,  1.41it/s]Evaluating on VQA val set:  81% 5415/6699 [1:03:33<15:12,  1.41it/s]Evaluating on VQA val set:  81% 5416/6699 [1:03:33<14:20,  1.49it/s]Evaluating on VQA val set:  81% 5417/6699 [1:03:34<14:40,  1.46it/s]Evaluating on VQA val set:  81% 5418/6699 [1:03:35<14:46,  1.44it/s]Evaluating on VQA val set:  81% 5419/6699 [1:03:35<15:20,  1.39it/s]Evaluating on VQA val set:  81% 5420/6699 [1:03:36<15:36,  1.37it/s]Evaluating on VQA val set:  81% 5421/6699 [1:03:37<15:20,  1.39it/s]Evaluating on VQA val set:  81% 5422/6699 [1:03:37<15:04,  1.41it/s]Evaluating on VQA val set:  81% 5423/6699 [1:03:38<14:45,  1.44it/s]Evaluating on VQA val set:  81% 5424/6699 [1:03:39<15:01,  1.41it/s]Evaluating on VQA val set:  81% 5425/6699 [1:03:40<15:12,  1.40it/s]Evaluating on VQA val set:  81% 5426/6699 [1:03:40<15:06,  1.40it/s]Evaluating on VQA val set:  81% 5427/6699 [1:03:41<15:09,  1.40it/s]Evaluating on VQA val set:  81% 5428/6699 [1:03:42<15:06,  1.40it/s]Evaluating on VQA val set:  81% 5429/6699 [1:03:42<14:58,  1.41it/s]Evaluating on VQA val set:  81% 5430/6699 [1:03:43<14:57,  1.41it/s]Evaluating on VQA val set:  81% 5431/6699 [1:03:44<14:51,  1.42it/s]Evaluating on VQA val set:  81% 5432/6699 [1:03:45<15:06,  1.40it/s]Evaluating on VQA val set:  81% 5433/6699 [1:03:45<15:00,  1.41it/s]Evaluating on VQA val set:  81% 5434/6699 [1:03:46<14:59,  1.41it/s]Evaluating on VQA val set:  81% 5435/6699 [1:03:47<15:02,  1.40it/s]Evaluating on VQA val set:  81% 5436/6699 [1:03:47<15:04,  1.40it/s]Evaluating on VQA val set:  81% 5437/6699 [1:03:48<14:59,  1.40it/s]Evaluating on VQA val set:  81% 5438/6699 [1:03:49<15:05,  1.39it/s]Evaluating on VQA val set:  81% 5439/6699 [1:03:50<15:06,  1.39it/s]Evaluating on VQA val set:  81% 5440/6699 [1:03:50<15:12,  1.38it/s]Evaluating on VQA val set:  81% 5441/6699 [1:03:51<15:08,  1.38it/s]Evaluating on VQA val set:  81% 5442/6699 [1:03:52<14:50,  1.41it/s]Evaluating on VQA val set:  81% 5443/6699 [1:03:52<15:09,  1.38it/s]Evaluating on VQA val set:  81% 5444/6699 [1:03:53<15:18,  1.37it/s]Evaluating on VQA val set:  81% 5445/6699 [1:03:54<15:12,  1.37it/s]Evaluating on VQA val set:  81% 5446/6699 [1:03:55<15:26,  1.35it/s]Evaluating on VQA val set:  81% 5447/6699 [1:03:55<15:40,  1.33it/s]Evaluating on VQA val set:  81% 5448/6699 [1:03:56<15:05,  1.38it/s]Evaluating on VQA val set:  81% 5449/6699 [1:03:57<15:09,  1.37it/s]Evaluating on VQA val set:  81% 5450/6699 [1:03:58<15:02,  1.38it/s]Evaluating on VQA val set:  81% 5451/6699 [1:03:58<15:26,  1.35it/s]Evaluating on VQA val set:  81% 5452/6699 [1:03:59<15:12,  1.37it/s]Evaluating on VQA val set:  81% 5453/6699 [1:04:00<14:28,  1.44it/s]Evaluating on VQA val set:  81% 5454/6699 [1:04:00<14:31,  1.43it/s]Evaluating on VQA val set:  81% 5455/6699 [1:04:01<14:00,  1.48it/s]Evaluating on VQA val set:  81% 5456/6699 [1:04:02<13:36,  1.52it/s]Evaluating on VQA val set:  81% 5457/6699 [1:04:02<13:52,  1.49it/s]Evaluating on VQA val set:  81% 5458/6699 [1:04:03<14:11,  1.46it/s]Evaluating on VQA val set:  81% 5459/6699 [1:04:04<14:25,  1.43it/s]Evaluating on VQA val set:  82% 5460/6699 [1:04:05<14:44,  1.40it/s]Evaluating on VQA val set:  82% 5461/6699 [1:04:05<14:54,  1.38it/s]Evaluating on VQA val set:  82% 5462/6699 [1:04:06<13:49,  1.49it/s]Evaluating on VQA val set:  82% 5463/6699 [1:04:06<13:30,  1.52it/s]Evaluating on VQA val set:  82% 5464/6699 [1:04:07<13:59,  1.47it/s]Evaluating on VQA val set:  82% 5465/6699 [1:04:08<14:03,  1.46it/s]Evaluating on VQA val set:  82% 5466/6699 [1:04:09<13:45,  1.49it/s]Evaluating on VQA val set:  82% 5467/6699 [1:04:09<14:08,  1.45it/s]Evaluating on VQA val set:  82% 5468/6699 [1:04:10<13:44,  1.49it/s]Evaluating on VQA val set:  82% 5469/6699 [1:04:11<13:59,  1.46it/s]Evaluating on VQA val set:  82% 5470/6699 [1:04:11<14:06,  1.45it/s]Evaluating on VQA val set:  82% 5471/6699 [1:04:12<14:19,  1.43it/s]Evaluating on VQA val set:  82% 5472/6699 [1:04:13<14:28,  1.41it/s]Evaluating on VQA val set:  82% 5473/6699 [1:04:13<14:23,  1.42it/s]Evaluating on VQA val set:  82% 5474/6699 [1:04:14<14:15,  1.43it/s]Evaluating on VQA val set:  82% 5475/6699 [1:04:15<13:45,  1.48it/s]Evaluating on VQA val set:  82% 5476/6699 [1:04:15<13:58,  1.46it/s]Evaluating on VQA val set:  82% 5477/6699 [1:04:16<13:37,  1.49it/s]Evaluating on VQA val set:  82% 5478/6699 [1:04:17<13:56,  1.46it/s]Evaluating on VQA val set:  82% 5479/6699 [1:04:17<13:45,  1.48it/s]Evaluating on VQA val set:  82% 5480/6699 [1:04:18<13:35,  1.49it/s]Evaluating on VQA val set:  82% 5481/6699 [1:04:19<13:55,  1.46it/s]Evaluating on VQA val set:  82% 5482/6699 [1:04:20<13:59,  1.45it/s]Evaluating on VQA val set:  82% 5483/6699 [1:04:20<14:02,  1.44it/s]Evaluating on VQA val set:  82% 5484/6699 [1:04:21<14:35,  1.39it/s]Evaluating on VQA val set:  82% 5485/6699 [1:04:22<14:15,  1.42it/s]Evaluating on VQA val set:  82% 5486/6699 [1:04:22<14:28,  1.40it/s]Evaluating on VQA val set:  82% 5487/6699 [1:04:23<14:38,  1.38it/s]Evaluating on VQA val set:  82% 5488/6699 [1:04:24<14:24,  1.40it/s]Evaluating on VQA val set:  82% 5489/6699 [1:04:25<14:16,  1.41it/s]Evaluating on VQA val set:  82% 5490/6699 [1:04:25<13:50,  1.46it/s]Evaluating on VQA val set:  82% 5491/6699 [1:04:26<13:51,  1.45it/s]Evaluating on VQA val set:  82% 5492/6699 [1:04:27<14:04,  1.43it/s]Evaluating on VQA val set:  82% 5493/6699 [1:04:27<14:12,  1.41it/s]Evaluating on VQA val set:  82% 5494/6699 [1:04:28<14:15,  1.41it/s]Evaluating on VQA val set:  82% 5495/6699 [1:04:29<13:59,  1.43it/s]Evaluating on VQA val set:  82% 5496/6699 [1:04:29<14:15,  1.41it/s]Evaluating on VQA val set:  82% 5497/6699 [1:04:30<14:15,  1.40it/s]Evaluating on VQA val set:  82% 5498/6699 [1:04:31<13:54,  1.44it/s]Evaluating on VQA val set:  82% 5499/6699 [1:04:32<13:59,  1.43it/s]Evaluating on VQA val set:  82% 5500/6699 [1:04:32<14:15,  1.40it/s]Evaluating on VQA val set:  82% 5501/6699 [1:04:33<14:11,  1.41it/s]Evaluating on VQA val set:  82% 5502/6699 [1:04:34<13:39,  1.46it/s]Evaluating on VQA val set:  82% 5503/6699 [1:04:34<13:44,  1.45it/s]Evaluating on VQA val set:  82% 5504/6699 [1:04:35<13:48,  1.44it/s]Evaluating on VQA val set:  82% 5505/6699 [1:04:36<13:19,  1.49it/s]Evaluating on VQA val set:  82% 5506/6699 [1:04:36<13:20,  1.49it/s]Evaluating on VQA val set:  82% 5507/6699 [1:04:37<13:25,  1.48it/s]Evaluating on VQA val set:  82% 5508/6699 [1:04:38<13:40,  1.45it/s]Evaluating on VQA val set:  82% 5509/6699 [1:04:38<13:46,  1.44it/s]Evaluating on VQA val set:  82% 5510/6699 [1:04:39<14:06,  1.40it/s]Evaluating on VQA val set:  82% 5511/6699 [1:04:40<14:08,  1.40it/s]Evaluating on VQA val set:  82% 5512/6699 [1:04:41<14:16,  1.39it/s]Evaluating on VQA val set:  82% 5513/6699 [1:04:41<13:59,  1.41it/s]Evaluating on VQA val set:  82% 5514/6699 [1:04:42<13:58,  1.41it/s]Evaluating on VQA val set:  82% 5515/6699 [1:04:43<14:23,  1.37it/s]Evaluating on VQA val set:  82% 5516/6699 [1:04:44<14:26,  1.37it/s]Evaluating on VQA val set:  82% 5517/6699 [1:04:44<14:15,  1.38it/s]Evaluating on VQA val set:  82% 5518/6699 [1:04:45<14:02,  1.40it/s]Evaluating on VQA val set:  82% 5519/6699 [1:04:46<13:59,  1.41it/s]Evaluating on VQA val set:  82% 5520/6699 [1:04:46<14:06,  1.39it/s]Evaluating on VQA val set:  82% 5521/6699 [1:04:47<14:23,  1.36it/s]Evaluating on VQA val set:  82% 5522/6699 [1:04:48<14:12,  1.38it/s]Evaluating on VQA val set:  82% 5523/6699 [1:04:48<13:35,  1.44it/s]Evaluating on VQA val set:  82% 5524/6699 [1:04:49<13:40,  1.43it/s]Evaluating on VQA val set:  82% 5525/6699 [1:04:50<13:15,  1.48it/s]Evaluating on VQA val set:  82% 5526/6699 [1:04:51<13:28,  1.45it/s]Evaluating on VQA val set:  83% 5527/6699 [1:04:51<13:51,  1.41it/s]Evaluating on VQA val set:  83% 5528/6699 [1:04:52<13:48,  1.41it/s]Evaluating on VQA val set:  83% 5529/6699 [1:04:53<13:49,  1.41it/s]Evaluating on VQA val set:  83% 5530/6699 [1:04:53<13:46,  1.42it/s]Evaluating on VQA val set:  83% 5531/6699 [1:04:54<13:36,  1.43it/s]Evaluating on VQA val set:  83% 5532/6699 [1:04:55<13:39,  1.42it/s]Evaluating on VQA val set:  83% 5533/6699 [1:04:55<13:41,  1.42it/s]Evaluating on VQA val set:  83% 5534/6699 [1:04:56<13:42,  1.42it/s]Evaluating on VQA val set:  83% 5535/6699 [1:04:57<13:55,  1.39it/s]Evaluating on VQA val set:  83% 5536/6699 [1:04:58<13:53,  1.39it/s]Evaluating on VQA val set:  83% 5537/6699 [1:04:58<14:00,  1.38it/s]Evaluating on VQA val set:  83% 5538/6699 [1:04:59<14:14,  1.36it/s]Evaluating on VQA val set:  83% 5539/6699 [1:05:00<14:01,  1.38it/s]Evaluating on VQA val set:  83% 5540/6699 [1:05:01<13:54,  1.39it/s]Evaluating on VQA val set:  83% 5541/6699 [1:05:01<13:25,  1.44it/s]Evaluating on VQA val set:  83% 5542/6699 [1:05:02<13:31,  1.42it/s]Evaluating on VQA val set:  83% 5543/6699 [1:05:03<13:17,  1.45it/s]Evaluating on VQA val set:  83% 5544/6699 [1:05:03<13:15,  1.45it/s]Evaluating on VQA val set:  83% 5545/6699 [1:05:04<12:58,  1.48it/s]Evaluating on VQA val set:  83% 5546/6699 [1:05:05<13:17,  1.45it/s]Evaluating on VQA val set:  83% 5547/6699 [1:05:05<13:14,  1.45it/s]Evaluating on VQA val set:  83% 5548/6699 [1:05:06<13:29,  1.42it/s]Evaluating on VQA val set:  83% 5549/6699 [1:05:07<13:32,  1.42it/s]Evaluating on VQA val set:  83% 5550/6699 [1:05:07<13:32,  1.41it/s]Evaluating on VQA val set:  83% 5551/6699 [1:05:08<13:28,  1.42it/s]Evaluating on VQA val set:  83% 5552/6699 [1:05:09<13:25,  1.42it/s]Evaluating on VQA val set:  83% 5553/6699 [1:05:10<13:39,  1.40it/s]Evaluating on VQA val set:  83% 5554/6699 [1:05:10<13:39,  1.40it/s]Evaluating on VQA val set:  83% 5555/6699 [1:05:11<13:45,  1.39it/s]Evaluating on VQA val set:  83% 5556/6699 [1:05:12<13:13,  1.44it/s]Evaluating on VQA val set:  83% 5557/6699 [1:05:12<13:42,  1.39it/s]Evaluating on VQA val set:  83% 5558/6699 [1:05:13<13:51,  1.37it/s]Evaluating on VQA val set:  83% 5559/6699 [1:05:14<13:34,  1.40it/s]Evaluating on VQA val set:  83% 5560/6699 [1:05:15<13:24,  1.42it/s]Evaluating on VQA val set:  83% 5561/6699 [1:05:15<13:18,  1.43it/s]Evaluating on VQA val set:  83% 5562/6699 [1:05:16<13:27,  1.41it/s]Evaluating on VQA val set:  83% 5563/6699 [1:05:17<13:15,  1.43it/s]Evaluating on VQA val set:  83% 5564/6699 [1:05:17<12:48,  1.48it/s]Evaluating on VQA val set:  83% 5565/6699 [1:05:18<12:14,  1.54it/s]Evaluating on VQA val set:  83% 5566/6699 [1:05:18<11:45,  1.61it/s]Evaluating on VQA val set:  83% 5567/6699 [1:05:19<11:25,  1.65it/s]Evaluating on VQA val set:  83% 5568/6699 [1:05:20<11:51,  1.59it/s]Evaluating on VQA val set:  83% 5569/6699 [1:05:20<12:15,  1.54it/s]Evaluating on VQA val set:  83% 5570/6699 [1:05:21<12:46,  1.47it/s]Evaluating on VQA val set:  83% 5571/6699 [1:05:22<12:57,  1.45it/s]Evaluating on VQA val set:  83% 5572/6699 [1:05:23<12:55,  1.45it/s]Evaluating on VQA val set:  83% 5573/6699 [1:05:23<12:56,  1.45it/s]Evaluating on VQA val set:  83% 5574/6699 [1:05:24<13:04,  1.43it/s]Evaluating on VQA val set:  83% 5575/6699 [1:05:25<13:10,  1.42it/s]Evaluating on VQA val set:  83% 5576/6699 [1:05:25<13:20,  1.40it/s]Evaluating on VQA val set:  83% 5577/6699 [1:05:26<13:38,  1.37it/s]Evaluating on VQA val set:  83% 5578/6699 [1:05:27<13:51,  1.35it/s]Evaluating on VQA val set:  83% 5579/6699 [1:05:28<13:55,  1.34it/s]Evaluating on VQA val set:  83% 5580/6699 [1:05:28<13:38,  1.37it/s]Evaluating on VQA val set:  83% 5581/6699 [1:05:29<13:56,  1.34it/s]Evaluating on VQA val set:  83% 5582/6699 [1:05:30<13:26,  1.39it/s]Evaluating on VQA val set:  83% 5583/6699 [1:05:31<13:06,  1.42it/s]Evaluating on VQA val set:  83% 5584/6699 [1:05:31<12:47,  1.45it/s]Evaluating on VQA val set:  83% 5585/6699 [1:05:32<12:44,  1.46it/s]Evaluating on VQA val set:  83% 5586/6699 [1:05:33<12:36,  1.47it/s]Evaluating on VQA val set:  83% 5587/6699 [1:05:33<12:58,  1.43it/s]Evaluating on VQA val set:  83% 5588/6699 [1:05:34<13:22,  1.38it/s]Evaluating on VQA val set:  83% 5589/6699 [1:05:35<13:15,  1.40it/s]Evaluating on VQA val set:  83% 5590/6699 [1:05:36<13:29,  1.37it/s]Evaluating on VQA val set:  83% 5591/6699 [1:05:36<13:18,  1.39it/s]Evaluating on VQA val set:  83% 5592/6699 [1:05:37<13:25,  1.37it/s]Evaluating on VQA val set:  83% 5593/6699 [1:05:38<12:56,  1.42it/s]Evaluating on VQA val set:  84% 5594/6699 [1:05:38<12:52,  1.43it/s]Evaluating on VQA val set:  84% 5595/6699 [1:05:39<13:06,  1.40it/s]Evaluating on VQA val set:  84% 5596/6699 [1:05:40<12:48,  1.43it/s]Evaluating on VQA val set:  84% 5597/6699 [1:05:40<12:27,  1.47it/s]Evaluating on VQA val set:  84% 5598/6699 [1:05:41<12:37,  1.45it/s]Evaluating on VQA val set:  84% 5599/6699 [1:05:42<12:21,  1.48it/s]Evaluating on VQA val set:  84% 5600/6699 [1:05:42<12:40,  1.44it/s]Evaluating on VQA val set:  84% 5601/6699 [1:05:43<13:03,  1.40it/s]Evaluating on VQA val set:  84% 5602/6699 [1:05:44<13:08,  1.39it/s]Evaluating on VQA val set:  84% 5603/6699 [1:05:45<13:06,  1.39it/s]Evaluating on VQA val set:  84% 5604/6699 [1:05:45<12:45,  1.43it/s]Evaluating on VQA val set:  84% 5605/6699 [1:05:46<12:56,  1.41it/s]Evaluating on VQA val set:  84% 5606/6699 [1:05:47<12:45,  1.43it/s]Evaluating on VQA val set:  84% 5607/6699 [1:05:47<12:00,  1.52it/s]Evaluating on VQA val set:  84% 5608/6699 [1:05:48<12:27,  1.46it/s]Evaluating on VQA val set:  84% 5609/6699 [1:05:49<12:31,  1.45it/s]Evaluating on VQA val set:  84% 5610/6699 [1:05:49<12:47,  1.42it/s]Evaluating on VQA val set:  84% 5611/6699 [1:05:50<13:06,  1.38it/s]Evaluating on VQA val set:  84% 5612/6699 [1:05:51<12:41,  1.43it/s]Evaluating on VQA val set:  84% 5613/6699 [1:05:52<12:58,  1.39it/s]Evaluating on VQA val set:  84% 5614/6699 [1:05:52<12:31,  1.44it/s]Evaluating on VQA val set:  84% 5615/6699 [1:05:53<12:30,  1.44it/s]Evaluating on VQA val set:  84% 5616/6699 [1:05:54<12:34,  1.43it/s]Evaluating on VQA val set:  84% 5617/6699 [1:05:54<12:27,  1.45it/s]Evaluating on VQA val set:  84% 5618/6699 [1:05:55<12:58,  1.39it/s]Evaluating on VQA val set:  84% 5619/6699 [1:05:56<13:22,  1.35it/s]Evaluating on VQA val set:  84% 5620/6699 [1:05:57<13:22,  1.34it/s]Evaluating on VQA val set:  84% 5621/6699 [1:05:57<13:04,  1.37it/s]Evaluating on VQA val set:  84% 5622/6699 [1:05:58<12:43,  1.41it/s]Evaluating on VQA val set:  84% 5623/6699 [1:05:59<12:50,  1.40it/s]Evaluating on VQA val set:  84% 5624/6699 [1:05:59<12:21,  1.45it/s]Evaluating on VQA val set:  84% 5625/6699 [1:06:00<12:18,  1.45it/s]Evaluating on VQA val set:  84% 5626/6699 [1:06:01<11:57,  1.50it/s]Evaluating on VQA val set:  84% 5627/6699 [1:06:01<11:51,  1.51it/s]Evaluating on VQA val set:  84% 5628/6699 [1:06:02<12:36,  1.42it/s]Evaluating on VQA val set:  84% 5629/6699 [1:06:03<12:31,  1.42it/s]Evaluating on VQA val set:  84% 5630/6699 [1:06:04<12:55,  1.38it/s]Evaluating on VQA val set:  84% 5631/6699 [1:06:04<12:54,  1.38it/s]Evaluating on VQA val set:  84% 5632/6699 [1:06:05<12:57,  1.37it/s]Evaluating on VQA val set:  84% 5633/6699 [1:06:06<12:58,  1.37it/s]Evaluating on VQA val set:  84% 5634/6699 [1:06:07<12:52,  1.38it/s]Evaluating on VQA val set:  84% 5635/6699 [1:06:07<12:37,  1.40it/s]Evaluating on VQA val set:  84% 5636/6699 [1:06:08<12:24,  1.43it/s]Evaluating on VQA val set:  84% 5637/6699 [1:06:09<12:11,  1.45it/s]Evaluating on VQA val set:  84% 5638/6699 [1:06:09<12:33,  1.41it/s]Evaluating on VQA val set:  84% 5639/6699 [1:06:10<12:38,  1.40it/s]Evaluating on VQA val set:  84% 5640/6699 [1:06:11<12:36,  1.40it/s]Evaluating on VQA val set:  84% 5641/6699 [1:06:11<12:26,  1.42it/s]Evaluating on VQA val set:  84% 5642/6699 [1:06:12<12:13,  1.44it/s]Evaluating on VQA val set:  84% 5643/6699 [1:06:13<12:13,  1.44it/s]Evaluating on VQA val set:  84% 5644/6699 [1:06:13<12:11,  1.44it/s]Evaluating on VQA val set:  84% 5645/6699 [1:06:14<12:20,  1.42it/s]Evaluating on VQA val set:  84% 5646/6699 [1:06:15<12:11,  1.44it/s]Evaluating on VQA val set:  84% 5647/6699 [1:06:16<12:20,  1.42it/s]Evaluating on VQA val set:  84% 5648/6699 [1:06:16<12:16,  1.43it/s]Evaluating on VQA val set:  84% 5649/6699 [1:06:17<12:21,  1.42it/s]Evaluating on VQA val set:  84% 5650/6699 [1:06:18<12:23,  1.41it/s]Evaluating on VQA val set:  84% 5651/6699 [1:06:18<12:31,  1.39it/s]Evaluating on VQA val set:  84% 5652/6699 [1:06:19<12:35,  1.39it/s]Evaluating on VQA val set:  84% 5653/6699 [1:06:20<12:17,  1.42it/s]Evaluating on VQA val set:  84% 5654/6699 [1:06:20<11:51,  1.47it/s]Evaluating on VQA val set:  84% 5655/6699 [1:06:21<12:25,  1.40it/s]Evaluating on VQA val set:  84% 5656/6699 [1:06:22<12:13,  1.42it/s]Evaluating on VQA val set:  84% 5657/6699 [1:06:23<11:51,  1.47it/s]Evaluating on VQA val set:  84% 5658/6699 [1:06:23<12:18,  1.41it/s]Evaluating on VQA val set:  84% 5659/6699 [1:06:24<12:12,  1.42it/s]Evaluating on VQA val set:  84% 5660/6699 [1:06:25<12:26,  1.39it/s]Evaluating on VQA val set:  85% 5661/6699 [1:06:26<12:26,  1.39it/s]Evaluating on VQA val set:  85% 5662/6699 [1:06:26<12:43,  1.36it/s]Evaluating on VQA val set:  85% 5663/6699 [1:06:27<12:45,  1.35it/s]Evaluating on VQA val set:  85% 5664/6699 [1:06:28<12:16,  1.40it/s]Evaluating on VQA val set:  85% 5665/6699 [1:06:28<12:03,  1.43it/s]Evaluating on VQA val set:  85% 5666/6699 [1:06:29<12:09,  1.42it/s]Evaluating on VQA val set:  85% 5667/6699 [1:06:30<12:09,  1.41it/s]Evaluating on VQA val set:  85% 5668/6699 [1:06:30<11:58,  1.44it/s]Evaluating on VQA val set:  85% 5669/6699 [1:06:31<12:18,  1.39it/s]Evaluating on VQA val set:  85% 5670/6699 [1:06:32<12:20,  1.39it/s]Evaluating on VQA val set:  85% 5671/6699 [1:06:33<12:05,  1.42it/s]Evaluating on VQA val set:  85% 5672/6699 [1:06:33<11:58,  1.43it/s]Evaluating on VQA val set:  85% 5673/6699 [1:06:34<11:52,  1.44it/s]Evaluating on VQA val set:  85% 5674/6699 [1:06:35<11:51,  1.44it/s]Evaluating on VQA val set:  85% 5675/6699 [1:06:35<11:57,  1.43it/s]Evaluating on VQA val set:  85% 5676/6699 [1:06:36<12:09,  1.40it/s]Evaluating on VQA val set:  85% 5677/6699 [1:06:37<12:21,  1.38it/s]Evaluating on VQA val set:  85% 5678/6699 [1:06:38<12:12,  1.39it/s]Evaluating on VQA val set:  85% 5679/6699 [1:06:38<11:10,  1.52it/s]Evaluating on VQA val set:  85% 5680/6699 [1:06:39<11:27,  1.48it/s]Evaluating on VQA val set:  85% 5681/6699 [1:06:40<11:31,  1.47it/s]Evaluating on VQA val set:  85% 5682/6699 [1:06:40<11:38,  1.46it/s]Evaluating on VQA val set:  85% 5683/6699 [1:06:41<11:29,  1.47it/s]Evaluating on VQA val set:  85% 5684/6699 [1:06:42<11:40,  1.45it/s]Evaluating on VQA val set:  85% 5685/6699 [1:06:42<12:02,  1.40it/s]Evaluating on VQA val set:  85% 5686/6699 [1:06:43<12:11,  1.38it/s]Evaluating on VQA val set:  85% 5687/6699 [1:06:44<12:16,  1.37it/s]Evaluating on VQA val set:  85% 5688/6699 [1:06:45<12:12,  1.38it/s]Evaluating on VQA val set:  85% 5689/6699 [1:06:45<11:53,  1.41it/s]Evaluating on VQA val set:  85% 5690/6699 [1:06:46<11:53,  1.41it/s]Evaluating on VQA val set:  85% 5691/6699 [1:06:47<11:53,  1.41it/s]Evaluating on VQA val set:  85% 5692/6699 [1:06:47<11:47,  1.42it/s]Evaluating on VQA val set:  85% 5693/6699 [1:06:48<11:55,  1.41it/s]Evaluating on VQA val set:  85% 5694/6699 [1:06:49<11:54,  1.41it/s]Evaluating on VQA val set:  85% 5695/6699 [1:06:49<11:35,  1.44it/s]Evaluating on VQA val set:  85% 5696/6699 [1:06:50<11:43,  1.43it/s]Evaluating on VQA val set:  85% 5697/6699 [1:06:51<11:33,  1.45it/s]Evaluating on VQA val set:  85% 5698/6699 [1:06:52<11:39,  1.43it/s]Evaluating on VQA val set:  85% 5699/6699 [1:06:52<11:41,  1.42it/s]Evaluating on VQA val set:  85% 5700/6699 [1:06:53<11:57,  1.39it/s]Evaluating on VQA val set:  85% 5701/6699 [1:06:54<11:59,  1.39it/s]Evaluating on VQA val set:  85% 5702/6699 [1:06:54<12:02,  1.38it/s]Evaluating on VQA val set:  85% 5703/6699 [1:06:55<12:05,  1.37it/s]Evaluating on VQA val set:  85% 5704/6699 [1:06:56<12:02,  1.38it/s]Evaluating on VQA val set:  85% 5705/6699 [1:06:57<11:56,  1.39it/s]Evaluating on VQA val set:  85% 5706/6699 [1:06:57<12:05,  1.37it/s]Evaluating on VQA val set:  85% 5707/6699 [1:06:58<11:44,  1.41it/s]Evaluating on VQA val set:  85% 5708/6699 [1:06:59<11:49,  1.40it/s]Evaluating on VQA val set:  85% 5709/6699 [1:06:59<11:38,  1.42it/s]Evaluating on VQA val set:  85% 5710/6699 [1:07:00<11:33,  1.43it/s]Evaluating on VQA val set:  85% 5711/6699 [1:07:01<11:50,  1.39it/s]Evaluating on VQA val set:  85% 5712/6699 [1:07:02<11:51,  1.39it/s]Evaluating on VQA val set:  85% 5713/6699 [1:07:02<11:51,  1.39it/s]Evaluating on VQA val set:  85% 5714/6699 [1:07:03<11:28,  1.43it/s]Evaluating on VQA val set:  85% 5715/6699 [1:07:04<11:26,  1.43it/s]Evaluating on VQA val set:  85% 5716/6699 [1:07:04<11:05,  1.48it/s]Evaluating on VQA val set:  85% 5717/6699 [1:07:05<10:27,  1.56it/s]Evaluating on VQA val set:  85% 5718/6699 [1:07:06<10:33,  1.55it/s]Evaluating on VQA val set:  85% 5719/6699 [1:07:06<10:39,  1.53it/s]Evaluating on VQA val set:  85% 5720/6699 [1:07:07<10:54,  1.50it/s]Evaluating on VQA val set:  85% 5721/6699 [1:07:08<11:14,  1.45it/s]Evaluating on VQA val set:  85% 5722/6699 [1:07:08<11:38,  1.40it/s]Evaluating on VQA val set:  85% 5723/6699 [1:07:09<11:33,  1.41it/s]Evaluating on VQA val set:  85% 5724/6699 [1:07:10<11:13,  1.45it/s]Evaluating on VQA val set:  85% 5725/6699 [1:07:11<11:23,  1.42it/s]Evaluating on VQA val set:  85% 5726/6699 [1:07:11<11:11,  1.45it/s]Evaluating on VQA val set:  85% 5727/6699 [1:07:12<11:12,  1.45it/s]Evaluating on VQA val set:  86% 5728/6699 [1:07:13<11:34,  1.40it/s]Evaluating on VQA val set:  86% 5729/6699 [1:07:13<11:28,  1.41it/s]Evaluating on VQA val set:  86% 5730/6699 [1:07:14<11:28,  1.41it/s]Evaluating on VQA val set:  86% 5731/6699 [1:07:15<11:31,  1.40it/s]Evaluating on VQA val set:  86% 5732/6699 [1:07:15<11:21,  1.42it/s]Evaluating on VQA val set:  86% 5733/6699 [1:07:16<11:19,  1.42it/s]Evaluating on VQA val set:  86% 5734/6699 [1:07:17<11:07,  1.44it/s]Evaluating on VQA val set:  86% 5735/6699 [1:07:18<11:18,  1.42it/s]Evaluating on VQA val set:  86% 5736/6699 [1:07:18<11:04,  1.45it/s]Evaluating on VQA val set:  86% 5737/6699 [1:07:19<11:14,  1.43it/s]Evaluating on VQA val set:  86% 5738/6699 [1:07:20<11:03,  1.45it/s]Evaluating on VQA val set:  86% 5739/6699 [1:07:20<11:13,  1.42it/s]Evaluating on VQA val set:  86% 5740/6699 [1:07:21<11:13,  1.42it/s]Evaluating on VQA val set:  86% 5741/6699 [1:07:22<11:18,  1.41it/s]Evaluating on VQA val set:  86% 5742/6699 [1:07:22<10:58,  1.45it/s]Evaluating on VQA val set:  86% 5743/6699 [1:07:23<11:07,  1.43it/s]Evaluating on VQA val set:  86% 5744/6699 [1:07:24<11:16,  1.41it/s]Evaluating on VQA val set:  86% 5745/6699 [1:07:25<11:27,  1.39it/s]Evaluating on VQA val set:  86% 5746/6699 [1:07:25<11:41,  1.36it/s]Evaluating on VQA val set:  86% 5747/6699 [1:07:26<11:44,  1.35it/s]Evaluating on VQA val set:  86% 5748/6699 [1:07:27<11:50,  1.34it/s]Evaluating on VQA val set:  86% 5749/6699 [1:07:28<11:18,  1.40it/s]Evaluating on VQA val set:  86% 5750/6699 [1:07:28<11:26,  1.38it/s]Evaluating on VQA val set:  86% 5751/6699 [1:07:29<11:28,  1.38it/s]Evaluating on VQA val set:  86% 5752/6699 [1:07:30<11:21,  1.39it/s]Evaluating on VQA val set:  86% 5753/6699 [1:07:30<11:14,  1.40it/s]Evaluating on VQA val set:  86% 5754/6699 [1:07:31<10:58,  1.43it/s]Evaluating on VQA val set:  86% 5755/6699 [1:07:32<10:50,  1.45it/s]Evaluating on VQA val set:  86% 5756/6699 [1:07:32<10:27,  1.50it/s]Evaluating on VQA val set:  86% 5757/6699 [1:07:33<09:58,  1.57it/s]Evaluating on VQA val set:  86% 5758/6699 [1:07:34<10:02,  1.56it/s]Evaluating on VQA val set:  86% 5759/6699 [1:07:34<10:38,  1.47it/s]Evaluating on VQA val set:  86% 5760/6699 [1:07:35<10:48,  1.45it/s]Evaluating on VQA val set:  86% 5761/6699 [1:07:36<10:58,  1.42it/s]Evaluating on VQA val set:  86% 5762/6699 [1:07:36<10:40,  1.46it/s]Evaluating on VQA val set:  86% 5763/6699 [1:07:37<10:38,  1.47it/s]Evaluating on VQA val set:  86% 5764/6699 [1:07:38<10:35,  1.47it/s]Evaluating on VQA val set:  86% 5765/6699 [1:07:38<10:39,  1.46it/s]Evaluating on VQA val set:  86% 5766/6699 [1:07:39<10:26,  1.49it/s]Evaluating on VQA val set:  86% 5767/6699 [1:07:40<10:40,  1.45it/s]Evaluating on VQA val set:  86% 5768/6699 [1:07:41<10:40,  1.45it/s]Evaluating on VQA val set:  86% 5769/6699 [1:07:41<10:35,  1.46it/s]Evaluating on VQA val set:  86% 5770/6699 [1:07:42<11:00,  1.41it/s]Evaluating on VQA val set:  86% 5771/6699 [1:07:43<11:10,  1.38it/s]Evaluating on VQA val set:  86% 5772/6699 [1:07:43<10:58,  1.41it/s]Evaluating on VQA val set:  86% 5773/6699 [1:07:44<11:06,  1.39it/s]Evaluating on VQA val set:  86% 5774/6699 [1:07:45<11:14,  1.37it/s]Evaluating on VQA val set:  86% 5775/6699 [1:07:46<11:04,  1.39it/s]Evaluating on VQA val set:  86% 5776/6699 [1:07:46<10:51,  1.42it/s]Evaluating on VQA val set:  86% 5777/6699 [1:07:47<10:38,  1.44it/s]Evaluating on VQA val set:  86% 5778/6699 [1:07:48<10:35,  1.45it/s]Evaluating on VQA val set:  86% 5779/6699 [1:07:48<10:35,  1.45it/s]Evaluating on VQA val set:  86% 5780/6699 [1:07:49<10:31,  1.46it/s]Evaluating on VQA val set:  86% 5781/6699 [1:07:50<10:19,  1.48it/s]Evaluating on VQA val set:  86% 5782/6699 [1:07:50<10:27,  1.46it/s]Evaluating on VQA val set:  86% 5783/6699 [1:07:51<10:25,  1.46it/s]Evaluating on VQA val set:  86% 5784/6699 [1:07:52<10:38,  1.43it/s]Evaluating on VQA val set:  86% 5785/6699 [1:07:52<10:46,  1.41it/s]Evaluating on VQA val set:  86% 5786/6699 [1:07:53<10:33,  1.44it/s]Evaluating on VQA val set:  86% 5787/6699 [1:07:54<10:55,  1.39it/s]Evaluating on VQA val set:  86% 5788/6699 [1:07:55<10:53,  1.39it/s]Evaluating on VQA val set:  86% 5789/6699 [1:07:55<10:53,  1.39it/s]Evaluating on VQA val set:  86% 5790/6699 [1:07:56<10:33,  1.43it/s]Evaluating on VQA val set:  86% 5791/6699 [1:07:57<10:20,  1.46it/s]Evaluating on VQA val set:  86% 5792/6699 [1:07:57<10:24,  1.45it/s]Evaluating on VQA val set:  86% 5793/6699 [1:07:58<10:29,  1.44it/s]Evaluating on VQA val set:  86% 5794/6699 [1:07:59<10:27,  1.44it/s]Evaluating on VQA val set:  87% 5795/6699 [1:07:59<10:21,  1.45it/s]Evaluating on VQA val set:  87% 5796/6699 [1:08:00<09:54,  1.52it/s]Evaluating on VQA val set:  87% 5797/6699 [1:08:01<09:59,  1.50it/s]Evaluating on VQA val set:  87% 5798/6699 [1:08:01<09:52,  1.52it/s]Evaluating on VQA val set:  87% 5799/6699 [1:08:02<09:58,  1.50it/s]Evaluating on VQA val set:  87% 5800/6699 [1:08:03<09:50,  1.52it/s]Evaluating on VQA val set:  87% 5801/6699 [1:08:03<10:02,  1.49it/s]Evaluating on VQA val set:  87% 5802/6699 [1:08:04<09:51,  1.52it/s]Evaluating on VQA val set:  87% 5803/6699 [1:08:05<09:58,  1.50it/s]Evaluating on VQA val set:  87% 5804/6699 [1:08:05<10:13,  1.46it/s]Evaluating on VQA val set:  87% 5805/6699 [1:08:06<10:16,  1.45it/s]Evaluating on VQA val set:  87% 5806/6699 [1:08:07<10:19,  1.44it/s]Evaluating on VQA val set:  87% 5807/6699 [1:08:07<10:03,  1.48it/s]Evaluating on VQA val set:  87% 5808/6699 [1:08:08<10:32,  1.41it/s]Evaluating on VQA val set:  87% 5809/6699 [1:08:09<10:32,  1.41it/s]Evaluating on VQA val set:  87% 5810/6699 [1:08:10<10:40,  1.39it/s]Evaluating on VQA val set:  87% 5811/6699 [1:08:10<10:49,  1.37it/s]Evaluating on VQA val set:  87% 5812/6699 [1:08:11<10:45,  1.38it/s]Evaluating on VQA val set:  87% 5813/6699 [1:08:12<10:38,  1.39it/s]Evaluating on VQA val set:  87% 5814/6699 [1:08:13<10:58,  1.34it/s]Evaluating on VQA val set:  87% 5815/6699 [1:08:13<10:53,  1.35it/s]Evaluating on VQA val set:  87% 5816/6699 [1:08:14<11:07,  1.32it/s]Evaluating on VQA val set:  87% 5817/6699 [1:08:15<10:49,  1.36it/s]Evaluating on VQA val set:  87% 5818/6699 [1:08:16<10:31,  1.39it/s]Evaluating on VQA val set:  87% 5819/6699 [1:08:16<10:35,  1.38it/s]Evaluating on VQA val set:  87% 5820/6699 [1:08:17<10:35,  1.38it/s]Evaluating on VQA val set:  87% 5821/6699 [1:08:18<10:33,  1.39it/s]Evaluating on VQA val set:  87% 5822/6699 [1:08:18<10:37,  1.38it/s]Evaluating on VQA val set:  87% 5823/6699 [1:08:19<10:38,  1.37it/s]Evaluating on VQA val set:  87% 5824/6699 [1:08:20<10:30,  1.39it/s]Evaluating on VQA val set:  87% 5825/6699 [1:08:21<10:21,  1.41it/s]Evaluating on VQA val set:  87% 5826/6699 [1:08:21<10:14,  1.42it/s]Evaluating on VQA val set:  87% 5827/6699 [1:08:22<10:10,  1.43it/s]Evaluating on VQA val set:  87% 5828/6699 [1:08:23<10:15,  1.41it/s]Evaluating on VQA val set:  87% 5829/6699 [1:08:23<10:29,  1.38it/s]Evaluating on VQA val set:  87% 5830/6699 [1:08:24<10:32,  1.37it/s]Evaluating on VQA val set:  87% 5831/6699 [1:08:25<10:25,  1.39it/s]Evaluating on VQA val set:  87% 5832/6699 [1:08:26<10:35,  1.36it/s]Evaluating on VQA val set:  87% 5833/6699 [1:08:26<10:16,  1.40it/s]Evaluating on VQA val set:  87% 5834/6699 [1:08:27<10:03,  1.43it/s]Evaluating on VQA val set:  87% 5835/6699 [1:08:28<10:08,  1.42it/s]Evaluating on VQA val set:  87% 5836/6699 [1:08:28<09:49,  1.46it/s]Evaluating on VQA val set:  87% 5837/6699 [1:08:29<10:04,  1.43it/s]Evaluating on VQA val set:  87% 5838/6699 [1:08:30<10:11,  1.41it/s]Evaluating on VQA val set:  87% 5839/6699 [1:08:30<09:58,  1.44it/s]Evaluating on VQA val set:  87% 5840/6699 [1:08:31<10:19,  1.39it/s]Evaluating on VQA val set:  87% 5841/6699 [1:08:32<10:17,  1.39it/s]Evaluating on VQA val set:  87% 5842/6699 [1:08:33<10:08,  1.41it/s]Evaluating on VQA val set:  87% 5843/6699 [1:08:33<10:02,  1.42it/s]Evaluating on VQA val set:  87% 5844/6699 [1:08:34<09:56,  1.43it/s]Evaluating on VQA val set:  87% 5845/6699 [1:08:35<09:54,  1.44it/s]Evaluating on VQA val set:  87% 5846/6699 [1:08:35<09:20,  1.52it/s]Evaluating on VQA val set:  87% 5847/6699 [1:08:36<09:24,  1.51it/s]Evaluating on VQA val set:  87% 5848/6699 [1:08:37<09:14,  1.53it/s]Evaluating on VQA val set:  87% 5849/6699 [1:08:37<09:35,  1.48it/s]Evaluating on VQA val set:  87% 5850/6699 [1:08:38<09:55,  1.43it/s]Evaluating on VQA val set:  87% 5851/6699 [1:08:39<09:47,  1.44it/s]Evaluating on VQA val set:  87% 5852/6699 [1:08:39<09:51,  1.43it/s]Evaluating on VQA val set:  87% 5853/6699 [1:08:40<09:45,  1.44it/s]Evaluating on VQA val set:  87% 5854/6699 [1:08:41<10:00,  1.41it/s]Evaluating on VQA val set:  87% 5855/6699 [1:08:42<09:46,  1.44it/s]Evaluating on VQA val set:  87% 5856/6699 [1:08:42<09:45,  1.44it/s]Evaluating on VQA val set:  87% 5857/6699 [1:08:43<09:28,  1.48it/s]Evaluating on VQA val set:  87% 5858/6699 [1:08:44<09:32,  1.47it/s]Evaluating on VQA val set:  87% 5859/6699 [1:08:44<09:10,  1.53it/s]Evaluating on VQA val set:  87% 5860/6699 [1:08:45<09:30,  1.47it/s]Evaluating on VQA val set:  87% 5861/6699 [1:08:46<09:38,  1.45it/s]Evaluating on VQA val set:  88% 5862/6699 [1:08:46<09:19,  1.50it/s]Evaluating on VQA val set:  88% 5863/6699 [1:08:47<09:33,  1.46it/s]Evaluating on VQA val set:  88% 5864/6699 [1:08:48<09:32,  1.46it/s]Evaluating on VQA val set:  88% 5865/6699 [1:08:48<09:36,  1.45it/s]Evaluating on VQA val set:  88% 5866/6699 [1:08:49<09:59,  1.39it/s]Evaluating on VQA val set:  88% 5867/6699 [1:08:50<09:47,  1.41it/s]Evaluating on VQA val set:  88% 5868/6699 [1:08:51<09:51,  1.41it/s]Evaluating on VQA val set:  88% 5869/6699 [1:08:51<09:43,  1.42it/s]Evaluating on VQA val set:  88% 5870/6699 [1:08:52<09:39,  1.43it/s]Evaluating on VQA val set:  88% 5871/6699 [1:08:53<09:57,  1.39it/s]Evaluating on VQA val set:  88% 5872/6699 [1:08:53<09:49,  1.40it/s]Evaluating on VQA val set:  88% 5873/6699 [1:08:54<09:51,  1.40it/s]Evaluating on VQA val set:  88% 5874/6699 [1:08:55<10:06,  1.36it/s]Evaluating on VQA val set:  88% 5875/6699 [1:08:56<09:49,  1.40it/s]Evaluating on VQA val set:  88% 5876/6699 [1:08:56<09:38,  1.42it/s]Evaluating on VQA val set:  88% 5877/6699 [1:08:57<09:47,  1.40it/s]Evaluating on VQA val set:  88% 5878/6699 [1:08:58<09:47,  1.40it/s]Evaluating on VQA val set:  88% 5879/6699 [1:08:58<09:28,  1.44it/s]Evaluating on VQA val set:  88% 5880/6699 [1:08:59<09:22,  1.46it/s]Evaluating on VQA val set:  88% 5881/6699 [1:09:00<09:40,  1.41it/s]Evaluating on VQA val set:  88% 5882/6699 [1:09:01<09:50,  1.38it/s]Evaluating on VQA val set:  88% 5883/6699 [1:09:01<09:28,  1.44it/s]Evaluating on VQA val set:  88% 5884/6699 [1:09:02<09:46,  1.39it/s]Evaluating on VQA val set:  88% 5885/6699 [1:09:03<09:17,  1.46it/s]Evaluating on VQA val set:  88% 5886/6699 [1:09:03<09:29,  1.43it/s]Evaluating on VQA val set:  88% 5887/6699 [1:09:04<09:30,  1.42it/s]Evaluating on VQA val set:  88% 5888/6699 [1:09:05<09:20,  1.45it/s]Evaluating on VQA val set:  88% 5889/6699 [1:09:05<09:24,  1.44it/s]Evaluating on VQA val set:  88% 5890/6699 [1:09:06<09:41,  1.39it/s]Evaluating on VQA val set:  88% 5891/6699 [1:09:07<09:42,  1.39it/s]Evaluating on VQA val set:  88% 5892/6699 [1:09:08<09:39,  1.39it/s]Evaluating on VQA val set:  88% 5893/6699 [1:09:08<09:52,  1.36it/s]Evaluating on VQA val set:  88% 5894/6699 [1:09:09<09:54,  1.35it/s]Evaluating on VQA val set:  88% 5895/6699 [1:09:10<09:30,  1.41it/s]Evaluating on VQA val set:  88% 5896/6699 [1:09:10<09:30,  1.41it/s]Evaluating on VQA val set:  88% 5897/6699 [1:09:11<09:05,  1.47it/s]Evaluating on VQA val set:  88% 5898/6699 [1:09:12<09:07,  1.46it/s]Evaluating on VQA val set:  88% 5899/6699 [1:09:12<08:50,  1.51it/s]Evaluating on VQA val set:  88% 5900/6699 [1:09:13<09:20,  1.43it/s]Evaluating on VQA val set:  88% 5901/6699 [1:09:14<09:39,  1.38it/s]Evaluating on VQA val set:  88% 5902/6699 [1:09:15<09:20,  1.42it/s]Evaluating on VQA val set:  88% 5903/6699 [1:09:15<09:03,  1.46it/s]Evaluating on VQA val set:  88% 5904/6699 [1:09:16<09:09,  1.45it/s]Evaluating on VQA val set:  88% 5905/6699 [1:09:17<09:12,  1.44it/s]Evaluating on VQA val set:  88% 5906/6699 [1:09:17<09:15,  1.43it/s]Evaluating on VQA val set:  88% 5907/6699 [1:09:18<08:55,  1.48it/s]Evaluating on VQA val set:  88% 5908/6699 [1:09:19<09:12,  1.43it/s]Evaluating on VQA val set:  88% 5909/6699 [1:09:19<09:12,  1.43it/s]Evaluating on VQA val set:  88% 5910/6699 [1:09:20<09:17,  1.41it/s]Evaluating on VQA val set:  88% 5911/6699 [1:09:21<09:11,  1.43it/s]Evaluating on VQA val set:  88% 5912/6699 [1:09:21<09:05,  1.44it/s]Evaluating on VQA val set:  88% 5913/6699 [1:09:22<08:56,  1.47it/s]Evaluating on VQA val set:  88% 5914/6699 [1:09:23<09:19,  1.40it/s]Evaluating on VQA val set:  88% 5915/6699 [1:09:24<09:05,  1.44it/s]Evaluating on VQA val set:  88% 5916/6699 [1:09:24<08:58,  1.46it/s]Evaluating on VQA val set:  88% 5917/6699 [1:09:25<09:01,  1.44it/s]Evaluating on VQA val set:  88% 5918/6699 [1:09:26<09:14,  1.41it/s]Evaluating on VQA val set:  88% 5919/6699 [1:09:26<09:12,  1.41it/s]Evaluating on VQA val set:  88% 5920/6699 [1:09:27<09:19,  1.39it/s]Evaluating on VQA val set:  88% 5921/6699 [1:09:28<09:26,  1.37it/s]Evaluating on VQA val set:  88% 5922/6699 [1:09:29<08:53,  1.46it/s]Evaluating on VQA val set:  88% 5923/6699 [1:09:29<09:01,  1.43it/s]Evaluating on VQA val set:  88% 5924/6699 [1:09:30<08:54,  1.45it/s]Evaluating on VQA val set:  88% 5925/6699 [1:09:31<09:00,  1.43it/s]Evaluating on VQA val set:  88% 5926/6699 [1:09:31<09:02,  1.43it/s]Evaluating on VQA val set:  88% 5927/6699 [1:09:32<09:11,  1.40it/s]Evaluating on VQA val set:  88% 5928/6699 [1:09:33<09:18,  1.38it/s]Evaluating on VQA val set:  89% 5929/6699 [1:09:33<09:08,  1.40it/s]Evaluating on VQA val set:  89% 5930/6699 [1:09:34<09:11,  1.39it/s]Evaluating on VQA val set:  89% 5931/6699 [1:09:35<09:19,  1.37it/s]Evaluating on VQA val set:  89% 5932/6699 [1:09:36<09:25,  1.36it/s]Evaluating on VQA val set:  89% 5933/6699 [1:09:36<09:24,  1.36it/s]Evaluating on VQA val set:  89% 5934/6699 [1:09:37<09:18,  1.37it/s]Evaluating on VQA val set:  89% 5935/6699 [1:09:38<08:56,  1.42it/s]Evaluating on VQA val set:  89% 5936/6699 [1:09:39<08:53,  1.43it/s]Evaluating on VQA val set:  89% 5937/6699 [1:09:39<08:43,  1.45it/s]Evaluating on VQA val set:  89% 5938/6699 [1:09:40<09:03,  1.40it/s]Evaluating on VQA val set:  89% 5939/6699 [1:09:41<09:01,  1.40it/s]Evaluating on VQA val set:  89% 5940/6699 [1:09:41<09:00,  1.41it/s]Evaluating on VQA val set:  89% 5941/6699 [1:09:42<08:52,  1.42it/s]Evaluating on VQA val set:  89% 5942/6699 [1:09:43<08:53,  1.42it/s]Evaluating on VQA val set:  89% 5943/6699 [1:09:43<08:43,  1.45it/s]Evaluating on VQA val set:  89% 5944/6699 [1:09:44<08:46,  1.43it/s]Evaluating on VQA val set:  89% 5945/6699 [1:09:45<09:02,  1.39it/s]Evaluating on VQA val set:  89% 5946/6699 [1:09:46<09:04,  1.38it/s]Evaluating on VQA val set:  89% 5947/6699 [1:09:46<09:16,  1.35it/s]Evaluating on VQA val set:  89% 5948/6699 [1:09:47<09:02,  1.39it/s]Evaluating on VQA val set:  89% 5949/6699 [1:09:48<08:48,  1.42it/s]Evaluating on VQA val set:  89% 5950/6699 [1:09:48<08:45,  1.43it/s]Evaluating on VQA val set:  89% 5951/6699 [1:09:49<08:51,  1.41it/s]Evaluating on VQA val set:  89% 5952/6699 [1:09:50<08:37,  1.44it/s]Evaluating on VQA val set:  89% 5953/6699 [1:09:51<08:37,  1.44it/s]Evaluating on VQA val set:  89% 5954/6699 [1:09:51<08:33,  1.45it/s]Evaluating on VQA val set:  89% 5955/6699 [1:09:52<08:27,  1.47it/s]Evaluating on VQA val set:  89% 5956/6699 [1:09:53<08:25,  1.47it/s]Evaluating on VQA val set:  89% 5957/6699 [1:09:53<08:26,  1.47it/s]Evaluating on VQA val set:  89% 5958/6699 [1:09:54<08:38,  1.43it/s]Evaluating on VQA val set:  89% 5959/6699 [1:09:55<08:37,  1.43it/s]Evaluating on VQA val set:  89% 5960/6699 [1:09:55<08:46,  1.40it/s]Evaluating on VQA val set:  89% 5961/6699 [1:09:56<08:41,  1.42it/s]Evaluating on VQA val set:  89% 5962/6699 [1:09:57<08:52,  1.38it/s]Evaluating on VQA val set:  89% 5963/6699 [1:09:58<08:47,  1.40it/s]Evaluating on VQA val set:  89% 5964/6699 [1:09:58<08:52,  1.38it/s]Evaluating on VQA val set:  89% 5965/6699 [1:09:59<08:48,  1.39it/s]Evaluating on VQA val set:  89% 5966/6699 [1:10:00<08:38,  1.41it/s]Evaluating on VQA val set:  89% 5967/6699 [1:10:00<08:42,  1.40it/s]Evaluating on VQA val set:  89% 5968/6699 [1:10:01<08:33,  1.42it/s]Evaluating on VQA val set:  89% 5969/6699 [1:10:02<08:52,  1.37it/s]Evaluating on VQA val set:  89% 5970/6699 [1:10:03<08:45,  1.39it/s]Evaluating on VQA val set:  89% 5971/6699 [1:10:03<08:34,  1.42it/s]Evaluating on VQA val set:  89% 5972/6699 [1:10:04<08:13,  1.47it/s]Evaluating on VQA val set:  89% 5973/6699 [1:10:05<08:15,  1.47it/s]Evaluating on VQA val set:  89% 5974/6699 [1:10:05<08:30,  1.42it/s]Evaluating on VQA val set:  89% 5975/6699 [1:10:06<08:32,  1.41it/s]Evaluating on VQA val set:  89% 5976/6699 [1:10:07<08:33,  1.41it/s]Evaluating on VQA val set:  89% 5977/6699 [1:10:08<08:42,  1.38it/s]Evaluating on VQA val set:  89% 5978/6699 [1:10:08<08:36,  1.40it/s]Evaluating on VQA val set:  89% 5979/6699 [1:10:09<08:27,  1.42it/s]Evaluating on VQA val set:  89% 5980/6699 [1:10:10<08:35,  1.39it/s]Evaluating on VQA val set:  89% 5981/6699 [1:10:10<08:47,  1.36it/s]Evaluating on VQA val set:  89% 5982/6699 [1:10:11<08:46,  1.36it/s]Evaluating on VQA val set:  89% 5983/6699 [1:10:12<08:48,  1.36it/s]Evaluating on VQA val set:  89% 5984/6699 [1:10:13<08:28,  1.41it/s]Evaluating on VQA val set:  89% 5985/6699 [1:10:13<08:47,  1.35it/s]Evaluating on VQA val set:  89% 5986/6699 [1:10:14<08:44,  1.36it/s]Evaluating on VQA val set:  89% 5987/6699 [1:10:15<08:42,  1.36it/s]Evaluating on VQA val set:  89% 5988/6699 [1:10:16<08:35,  1.38it/s]Evaluating on VQA val set:  89% 5989/6699 [1:10:16<08:26,  1.40it/s]Evaluating on VQA val set:  89% 5990/6699 [1:10:17<08:24,  1.40it/s]Evaluating on VQA val set:  89% 5991/6699 [1:10:18<08:32,  1.38it/s]Evaluating on VQA val set:  89% 5992/6699 [1:10:18<08:43,  1.35it/s]Evaluating on VQA val set:  89% 5993/6699 [1:10:19<08:58,  1.31it/s]Evaluating on VQA val set:  89% 5994/6699 [1:10:20<08:45,  1.34it/s]Evaluating on VQA val set:  89% 5995/6699 [1:10:21<08:39,  1.36it/s]Evaluating on VQA val set:  90% 5996/6699 [1:10:21<08:32,  1.37it/s]Evaluating on VQA val set:  90% 5997/6699 [1:10:22<08:28,  1.38it/s]Evaluating on VQA val set:  90% 5998/6699 [1:10:23<08:27,  1.38it/s]Evaluating on VQA val set:  90% 5999/6699 [1:10:24<08:36,  1.35it/s]Evaluating on VQA val set:  90% 6000/6699 [1:10:24<08:23,  1.39it/s]Evaluating on VQA val set:  90% 6001/6699 [1:10:25<08:31,  1.37it/s]Evaluating on VQA val set:  90% 6002/6699 [1:10:26<08:31,  1.36it/s]Evaluating on VQA val set:  90% 6003/6699 [1:10:27<08:28,  1.37it/s]Evaluating on VQA val set:  90% 6004/6699 [1:10:27<08:09,  1.42it/s]Evaluating on VQA val set:  90% 6005/6699 [1:10:28<08:20,  1.39it/s]Evaluating on VQA val set:  90% 6006/6699 [1:10:28<07:49,  1.47it/s]Evaluating on VQA val set:  90% 6007/6699 [1:10:29<07:51,  1.47it/s]Evaluating on VQA val set:  90% 6008/6699 [1:10:30<07:53,  1.46it/s]Evaluating on VQA val set:  90% 6009/6699 [1:10:31<08:04,  1.43it/s]Evaluating on VQA val set:  90% 6010/6699 [1:10:31<08:04,  1.42it/s]Evaluating on VQA val set:  90% 6011/6699 [1:10:32<08:03,  1.42it/s]Evaluating on VQA val set:  90% 6012/6699 [1:10:33<08:11,  1.40it/s]Evaluating on VQA val set:  90% 6013/6699 [1:10:34<08:17,  1.38it/s]Evaluating on VQA val set:  90% 6014/6699 [1:10:34<08:25,  1.36it/s]Evaluating on VQA val set:  90% 6015/6699 [1:10:35<08:20,  1.37it/s]Evaluating on VQA val set:  90% 6016/6699 [1:10:36<07:40,  1.48it/s]Evaluating on VQA val set:  90% 6017/6699 [1:10:36<06:46,  1.68it/s]Evaluating on VQA val set:  90% 6018/6699 [1:10:36<06:10,  1.84it/s]Evaluating on VQA val set:  90% 6019/6699 [1:10:37<06:36,  1.71it/s]Evaluating on VQA val set:  90% 6020/6699 [1:10:38<06:50,  1.65it/s]Evaluating on VQA val set:  90% 6021/6699 [1:10:38<07:17,  1.55it/s]Evaluating on VQA val set:  90% 6022/6699 [1:10:39<07:37,  1.48it/s]Evaluating on VQA val set:  90% 6023/6699 [1:10:40<07:42,  1.46it/s]Evaluating on VQA val set:  90% 6024/6699 [1:10:41<07:51,  1.43it/s]Evaluating on VQA val set:  90% 6025/6699 [1:10:41<08:01,  1.40it/s]Evaluating on VQA val set:  90% 6026/6699 [1:10:42<07:57,  1.41it/s]Evaluating on VQA val set:  90% 6027/6699 [1:10:43<07:46,  1.44it/s]Evaluating on VQA val set:  90% 6028/6699 [1:10:43<07:35,  1.47it/s]Evaluating on VQA val set:  90% 6029/6699 [1:10:44<07:34,  1.47it/s]Evaluating on VQA val set:  90% 6030/6699 [1:10:45<07:47,  1.43it/s]Evaluating on VQA val set:  90% 6031/6699 [1:10:46<07:48,  1.42it/s]Evaluating on VQA val set:  90% 6032/6699 [1:10:46<07:50,  1.42it/s]Evaluating on VQA val set:  90% 6033/6699 [1:10:47<07:59,  1.39it/s]Evaluating on VQA val set:  90% 6034/6699 [1:10:48<07:58,  1.39it/s]Evaluating on VQA val set:  90% 6035/6699 [1:10:48<07:53,  1.40it/s]Evaluating on VQA val set:  90% 6036/6699 [1:10:49<07:27,  1.48it/s]Evaluating on VQA val set:  90% 6037/6699 [1:10:50<07:32,  1.46it/s]Evaluating on VQA val set:  90% 6038/6699 [1:10:50<07:40,  1.43it/s]Evaluating on VQA val set:  90% 6039/6699 [1:10:51<07:34,  1.45it/s]Evaluating on VQA val set:  90% 6040/6699 [1:10:52<07:34,  1.45it/s]Evaluating on VQA val set:  90% 6041/6699 [1:10:52<07:32,  1.45it/s]Evaluating on VQA val set:  90% 6042/6699 [1:10:53<07:30,  1.46it/s]Evaluating on VQA val set:  90% 6043/6699 [1:10:54<07:32,  1.45it/s]Evaluating on VQA val set:  90% 6044/6699 [1:10:55<07:41,  1.42it/s]Evaluating on VQA val set:  90% 6045/6699 [1:10:55<07:24,  1.47it/s]Evaluating on VQA val set:  90% 6046/6699 [1:10:56<07:44,  1.41it/s]Evaluating on VQA val set:  90% 6047/6699 [1:10:57<07:52,  1.38it/s]Evaluating on VQA val set:  90% 6048/6699 [1:10:57<07:54,  1.37it/s]Evaluating on VQA val set:  90% 6049/6699 [1:10:58<07:48,  1.39it/s]Evaluating on VQA val set:  90% 6050/6699 [1:10:59<07:29,  1.44it/s]Evaluating on VQA val set:  90% 6051/6699 [1:10:59<07:28,  1.45it/s]Evaluating on VQA val set:  90% 6052/6699 [1:11:00<07:32,  1.43it/s]Evaluating on VQA val set:  90% 6053/6699 [1:11:01<07:36,  1.42it/s]Evaluating on VQA val set:  90% 6054/6699 [1:11:02<07:26,  1.44it/s]Evaluating on VQA val set:  90% 6055/6699 [1:11:02<07:01,  1.53it/s]Evaluating on VQA val set:  90% 6056/6699 [1:11:03<07:14,  1.48it/s]Evaluating on VQA val set:  90% 6057/6699 [1:11:04<07:21,  1.45it/s]Evaluating on VQA val set:  90% 6058/6699 [1:11:04<07:16,  1.47it/s]Evaluating on VQA val set:  90% 6059/6699 [1:11:05<07:02,  1.51it/s]Evaluating on VQA val set:  90% 6060/6699 [1:11:06<07:12,  1.48it/s]Evaluating on VQA val set:  90% 6061/6699 [1:11:06<07:20,  1.45it/s]Evaluating on VQA val set:  90% 6062/6699 [1:11:07<07:27,  1.42it/s]Evaluating on VQA val set:  91% 6063/6699 [1:11:08<07:30,  1.41it/s]Evaluating on VQA val set:  91% 6064/6699 [1:11:08<07:31,  1.41it/s]Evaluating on VQA val set:  91% 6065/6699 [1:11:09<07:25,  1.42it/s]Evaluating on VQA val set:  91% 6066/6699 [1:11:10<07:27,  1.41it/s]Evaluating on VQA val set:  91% 6067/6699 [1:11:11<07:24,  1.42it/s]Evaluating on VQA val set:  91% 6068/6699 [1:11:11<07:25,  1.42it/s]Evaluating on VQA val set:  91% 6069/6699 [1:11:12<07:20,  1.43it/s]Evaluating on VQA val set:  91% 6070/6699 [1:11:13<07:28,  1.40it/s]Evaluating on VQA val set:  91% 6071/6699 [1:11:13<07:26,  1.41it/s]Evaluating on VQA val set:  91% 6072/6699 [1:11:14<07:34,  1.38it/s]Evaluating on VQA val set:  91% 6073/6699 [1:11:15<07:32,  1.38it/s]Evaluating on VQA val set:  91% 6074/6699 [1:11:16<07:38,  1.36it/s]Evaluating on VQA val set:  91% 6075/6699 [1:11:16<07:51,  1.32it/s]Evaluating on VQA val set:  91% 6076/6699 [1:11:17<07:47,  1.33it/s]Evaluating on VQA val set:  91% 6077/6699 [1:11:18<07:44,  1.34it/s]Evaluating on VQA val set:  91% 6078/6699 [1:11:19<07:38,  1.36it/s]Evaluating on VQA val set:  91% 6079/6699 [1:11:19<07:30,  1.38it/s]Evaluating on VQA val set:  91% 6080/6699 [1:11:20<07:15,  1.42it/s]Evaluating on VQA val set:  91% 6081/6699 [1:11:21<07:05,  1.45it/s]Evaluating on VQA val set:  91% 6082/6699 [1:11:21<07:17,  1.41it/s]Evaluating on VQA val set:  91% 6083/6699 [1:11:22<07:27,  1.38it/s]Evaluating on VQA val set:  91% 6084/6699 [1:11:23<07:37,  1.35it/s]Evaluating on VQA val set:  91% 6085/6699 [1:11:24<07:37,  1.34it/s]Evaluating on VQA val set:  91% 6086/6699 [1:11:24<07:37,  1.34it/s]Evaluating on VQA val set:  91% 6087/6699 [1:11:25<07:34,  1.35it/s]Evaluating on VQA val set:  91% 6088/6699 [1:11:26<07:30,  1.35it/s]Evaluating on VQA val set:  91% 6089/6699 [1:11:27<07:19,  1.39it/s]Evaluating on VQA val set:  91% 6090/6699 [1:11:27<07:15,  1.40it/s]Evaluating on VQA val set:  91% 6091/6699 [1:11:28<07:16,  1.39it/s]Evaluating on VQA val set:  91% 6092/6699 [1:11:29<07:10,  1.41it/s]Evaluating on VQA val set:  91% 6093/6699 [1:11:29<07:13,  1.40it/s]Evaluating on VQA val set:  91% 6094/6699 [1:11:30<07:04,  1.42it/s]Evaluating on VQA val set:  91% 6095/6699 [1:11:31<06:52,  1.46it/s]Evaluating on VQA val set:  91% 6096/6699 [1:11:31<06:59,  1.44it/s]Evaluating on VQA val set:  91% 6097/6699 [1:11:32<07:00,  1.43it/s]Evaluating on VQA val set:  91% 6098/6699 [1:11:33<06:56,  1.44it/s]Evaluating on VQA val set:  91% 6099/6699 [1:11:34<06:51,  1.46it/s]Evaluating on VQA val set:  91% 6100/6699 [1:11:34<06:51,  1.45it/s]Evaluating on VQA val set:  91% 6101/6699 [1:11:35<06:43,  1.48it/s]Evaluating on VQA val set:  91% 6102/6699 [1:11:36<06:50,  1.45it/s]Evaluating on VQA val set:  91% 6103/6699 [1:11:36<06:55,  1.43it/s]Evaluating on VQA val set:  91% 6104/6699 [1:11:37<06:55,  1.43it/s]Evaluating on VQA val set:  91% 6105/6699 [1:11:38<06:41,  1.48it/s]Evaluating on VQA val set:  91% 6106/6699 [1:11:38<06:35,  1.50it/s]Evaluating on VQA val set:  91% 6107/6699 [1:11:39<06:46,  1.46it/s]Evaluating on VQA val set:  91% 6108/6699 [1:11:40<06:52,  1.43it/s]Evaluating on VQA val set:  91% 6109/6699 [1:11:40<06:38,  1.48it/s]Evaluating on VQA val set:  91% 6110/6699 [1:11:41<06:43,  1.46it/s]Evaluating on VQA val set:  91% 6111/6699 [1:11:42<06:41,  1.46it/s]Evaluating on VQA val set:  91% 6112/6699 [1:11:43<06:55,  1.41it/s]Evaluating on VQA val set:  91% 6113/6699 [1:11:43<06:46,  1.44it/s]Evaluating on VQA val set:  91% 6114/6699 [1:11:44<06:50,  1.43it/s]Evaluating on VQA val set:  91% 6115/6699 [1:11:45<06:53,  1.41it/s]Evaluating on VQA val set:  91% 6116/6699 [1:11:45<06:53,  1.41it/s]Evaluating on VQA val set:  91% 6117/6699 [1:11:46<07:03,  1.37it/s]Evaluating on VQA val set:  91% 6118/6699 [1:11:47<07:11,  1.35it/s]Evaluating on VQA val set:  91% 6119/6699 [1:11:48<07:15,  1.33it/s]Evaluating on VQA val set:  91% 6120/6699 [1:11:48<06:59,  1.38it/s]Evaluating on VQA val set:  91% 6121/6699 [1:11:49<06:50,  1.41it/s]Evaluating on VQA val set:  91% 6122/6699 [1:11:50<06:35,  1.46it/s]Evaluating on VQA val set:  91% 6123/6699 [1:11:50<06:30,  1.47it/s]Evaluating on VQA val set:  91% 6124/6699 [1:11:51<06:39,  1.44it/s]Evaluating on VQA val set:  91% 6125/6699 [1:11:52<06:45,  1.42it/s]Evaluating on VQA val set:  91% 6126/6699 [1:11:52<06:35,  1.45it/s]Evaluating on VQA val set:  91% 6127/6699 [1:11:53<06:30,  1.47it/s]Evaluating on VQA val set:  91% 6128/6699 [1:11:54<06:30,  1.46it/s]Evaluating on VQA val set:  91% 6129/6699 [1:11:54<06:31,  1.46it/s]Evaluating on VQA val set:  92% 6130/6699 [1:11:55<06:33,  1.44it/s]Evaluating on VQA val set:  92% 6131/6699 [1:11:56<06:22,  1.48it/s]Evaluating on VQA val set:  92% 6132/6699 [1:11:56<06:27,  1.46it/s]Evaluating on VQA val set:  92% 6133/6699 [1:11:57<06:30,  1.45it/s]Evaluating on VQA val set:  92% 6134/6699 [1:11:58<06:33,  1.43it/s]Evaluating on VQA val set:  92% 6135/6699 [1:11:59<06:33,  1.43it/s]Evaluating on VQA val set:  92% 6136/6699 [1:11:59<06:25,  1.46it/s]Evaluating on VQA val set:  92% 6137/6699 [1:12:00<06:24,  1.46it/s]Evaluating on VQA val set:  92% 6138/6699 [1:12:01<06:16,  1.49it/s]Evaluating on VQA val set:  92% 6139/6699 [1:12:01<06:14,  1.49it/s]Evaluating on VQA val set:  92% 6140/6699 [1:12:02<06:19,  1.47it/s]Evaluating on VQA val set:  92% 6141/6699 [1:12:03<06:24,  1.45it/s]Evaluating on VQA val set:  92% 6142/6699 [1:12:03<06:27,  1.44it/s]Evaluating on VQA val set:  92% 6143/6699 [1:12:04<06:23,  1.45it/s]Evaluating on VQA val set:  92% 6144/6699 [1:12:05<06:19,  1.46it/s]Evaluating on VQA val set:  92% 6145/6699 [1:12:05<06:31,  1.41it/s]Evaluating on VQA val set:  92% 6146/6699 [1:12:06<06:28,  1.42it/s]Evaluating on VQA val set:  92% 6147/6699 [1:12:07<06:27,  1.42it/s]Evaluating on VQA val set:  92% 6148/6699 [1:12:08<06:27,  1.42it/s]Evaluating on VQA val set:  92% 6149/6699 [1:12:08<06:27,  1.42it/s]Evaluating on VQA val set:  92% 6150/6699 [1:12:09<06:18,  1.45it/s]Evaluating on VQA val set:  92% 6151/6699 [1:12:10<06:34,  1.39it/s]Evaluating on VQA val set:  92% 6152/6699 [1:12:10<06:25,  1.42it/s]Evaluating on VQA val set:  92% 6153/6699 [1:12:11<06:20,  1.44it/s]Evaluating on VQA val set:  92% 6154/6699 [1:12:12<06:25,  1.41it/s]Evaluating on VQA val set:  92% 6155/6699 [1:12:13<06:26,  1.41it/s]Evaluating on VQA val set:  92% 6156/6699 [1:12:13<06:32,  1.38it/s]Evaluating on VQA val set:  92% 6157/6699 [1:12:14<06:31,  1.39it/s]Evaluating on VQA val set:  92% 6158/6699 [1:12:15<06:25,  1.40it/s]Evaluating on VQA val set:  92% 6159/6699 [1:12:15<06:31,  1.38it/s]Evaluating on VQA val set:  92% 6160/6699 [1:12:16<06:33,  1.37it/s]Evaluating on VQA val set:  92% 6161/6699 [1:12:17<06:37,  1.35it/s]Evaluating on VQA val set:  92% 6162/6699 [1:12:18<06:26,  1.39it/s]Evaluating on VQA val set:  92% 6163/6699 [1:12:18<06:20,  1.41it/s]Evaluating on VQA val set:  92% 6164/6699 [1:12:19<06:15,  1.42it/s]Evaluating on VQA val set:  92% 6165/6699 [1:12:20<06:17,  1.41it/s]Evaluating on VQA val set:  92% 6166/6699 [1:12:20<06:24,  1.39it/s]Evaluating on VQA val set:  92% 6167/6699 [1:12:21<06:14,  1.42it/s]Evaluating on VQA val set:  92% 6168/6699 [1:12:22<06:23,  1.39it/s]Evaluating on VQA val set:  92% 6169/6699 [1:12:23<06:26,  1.37it/s]Evaluating on VQA val set:  92% 6170/6699 [1:12:23<06:19,  1.39it/s]Evaluating on VQA val set:  92% 6171/6699 [1:12:24<06:23,  1.38it/s]Evaluating on VQA val set:  92% 6172/6699 [1:12:25<06:31,  1.35it/s]Evaluating on VQA val set:  92% 6173/6699 [1:12:26<06:26,  1.36it/s]Evaluating on VQA val set:  92% 6174/6699 [1:12:26<06:21,  1.38it/s]Evaluating on VQA val set:  92% 6175/6699 [1:12:27<06:25,  1.36it/s]Evaluating on VQA val set:  92% 6176/6699 [1:12:28<06:19,  1.38it/s]Evaluating on VQA val set:  92% 6177/6699 [1:12:29<06:29,  1.34it/s]Evaluating on VQA val set:  92% 6178/6699 [1:12:29<06:19,  1.37it/s]Evaluating on VQA val set:  92% 6179/6699 [1:12:30<06:13,  1.39it/s]Evaluating on VQA val set:  92% 6180/6699 [1:12:31<06:12,  1.39it/s]Evaluating on VQA val set:  92% 6181/6699 [1:12:31<06:05,  1.42it/s]Evaluating on VQA val set:  92% 6182/6699 [1:12:32<05:43,  1.51it/s]Evaluating on VQA val set:  92% 6183/6699 [1:12:33<05:43,  1.50it/s]Evaluating on VQA val set:  92% 6184/6699 [1:12:33<05:44,  1.49it/s]Evaluating on VQA val set:  92% 6185/6699 [1:12:34<05:56,  1.44it/s]Evaluating on VQA val set:  92% 6186/6699 [1:12:35<05:52,  1.46it/s]Evaluating on VQA val set:  92% 6187/6699 [1:12:35<05:59,  1.42it/s]Evaluating on VQA val set:  92% 6188/6699 [1:12:36<06:07,  1.39it/s]Evaluating on VQA val set:  92% 6189/6699 [1:12:37<05:58,  1.42it/s]Evaluating on VQA val set:  92% 6190/6699 [1:12:38<06:00,  1.41it/s]Evaluating on VQA val set:  92% 6191/6699 [1:12:38<05:51,  1.45it/s]Evaluating on VQA val set:  92% 6192/6699 [1:12:39<05:52,  1.44it/s]Evaluating on VQA val set:  92% 6193/6699 [1:12:40<05:55,  1.42it/s]Evaluating on VQA val set:  92% 6194/6699 [1:12:40<05:56,  1.42it/s]Evaluating on VQA val set:  92% 6195/6699 [1:12:41<05:57,  1.41it/s]Evaluating on VQA val set:  92% 6196/6699 [1:12:42<06:01,  1.39it/s]Evaluating on VQA val set:  93% 6197/6699 [1:12:43<06:03,  1.38it/s]Evaluating on VQA val set:  93% 6198/6699 [1:12:43<06:05,  1.37it/s]Evaluating on VQA val set:  93% 6199/6699 [1:12:44<06:04,  1.37it/s]Evaluating on VQA val set:  93% 6200/6699 [1:12:45<05:48,  1.43it/s]Evaluating on VQA val set:  93% 6201/6699 [1:12:45<05:49,  1.43it/s]Evaluating on VQA val set:  93% 6202/6699 [1:12:46<05:27,  1.52it/s]Evaluating on VQA val set:  93% 6203/6699 [1:12:47<05:31,  1.50it/s]Evaluating on VQA val set:  93% 6204/6699 [1:12:47<05:39,  1.46it/s]Evaluating on VQA val set:  93% 6205/6699 [1:12:48<05:25,  1.52it/s]Evaluating on VQA val set:  93% 6206/6699 [1:12:49<05:29,  1.49it/s]Evaluating on VQA val set:  93% 6207/6699 [1:12:49<05:27,  1.50it/s]Evaluating on VQA val set:  93% 6208/6699 [1:12:50<05:38,  1.45it/s]Evaluating on VQA val set:  93% 6209/6699 [1:12:51<05:44,  1.42it/s]Evaluating on VQA val set:  93% 6210/6699 [1:12:51<05:46,  1.41it/s]Evaluating on VQA val set:  93% 6211/6699 [1:12:52<05:35,  1.46it/s]Evaluating on VQA val set:  93% 6212/6699 [1:12:53<05:41,  1.43it/s]Evaluating on VQA val set:  93% 6213/6699 [1:12:54<05:42,  1.42it/s]Evaluating on VQA val set:  93% 6214/6699 [1:12:54<05:46,  1.40it/s]Evaluating on VQA val set:  93% 6215/6699 [1:12:55<05:32,  1.45it/s]Evaluating on VQA val set:  93% 6216/6699 [1:12:56<05:33,  1.45it/s]Evaluating on VQA val set:  93% 6217/6699 [1:12:56<05:36,  1.43it/s]Evaluating on VQA val set:  93% 6218/6699 [1:12:57<05:18,  1.51it/s]Evaluating on VQA val set:  93% 6219/6699 [1:12:58<05:26,  1.47it/s]Evaluating on VQA val set:  93% 6220/6699 [1:12:58<05:33,  1.44it/s]Evaluating on VQA val set:  93% 6221/6699 [1:12:59<05:40,  1.40it/s]Evaluating on VQA val set:  93% 6222/6699 [1:13:00<05:41,  1.40it/s]Evaluating on VQA val set:  93% 6223/6699 [1:13:01<05:38,  1.41it/s]Evaluating on VQA val set:  93% 6224/6699 [1:13:01<05:42,  1.39it/s]Evaluating on VQA val set:  93% 6225/6699 [1:13:02<05:46,  1.37it/s]Evaluating on VQA val set:  93% 6226/6699 [1:13:03<05:39,  1.39it/s]Evaluating on VQA val set:  93% 6227/6699 [1:13:03<05:43,  1.37it/s]Evaluating on VQA val set:  93% 6228/6699 [1:13:04<05:43,  1.37it/s]Evaluating on VQA val set:  93% 6229/6699 [1:13:05<05:41,  1.38it/s]Evaluating on VQA val set:  93% 6230/6699 [1:13:06<05:35,  1.40it/s]Evaluating on VQA val set:  93% 6231/6699 [1:13:06<05:22,  1.45it/s]Evaluating on VQA val set:  93% 6232/6699 [1:13:07<05:23,  1.44it/s]Evaluating on VQA val set:  93% 6233/6699 [1:13:08<05:33,  1.40it/s]Evaluating on VQA val set:  93% 6234/6699 [1:13:08<05:26,  1.43it/s]Evaluating on VQA val set:  93% 6235/6699 [1:13:09<05:33,  1.39it/s]Evaluating on VQA val set:  93% 6236/6699 [1:13:10<05:24,  1.43it/s]Evaluating on VQA val set:  93% 6237/6699 [1:13:11<05:28,  1.41it/s]Evaluating on VQA val set:  93% 6238/6699 [1:13:11<05:16,  1.46it/s]Evaluating on VQA val set:  93% 6239/6699 [1:13:12<05:13,  1.47it/s]Evaluating on VQA val set:  93% 6240/6699 [1:13:12<05:08,  1.49it/s]Evaluating on VQA val set:  93% 6241/6699 [1:13:13<05:17,  1.44it/s]Evaluating on VQA val set:  93% 6242/6699 [1:13:14<05:24,  1.41it/s]Evaluating on VQA val set:  93% 6243/6699 [1:13:15<05:32,  1.37it/s]Evaluating on VQA val set:  93% 6244/6699 [1:13:15<05:29,  1.38it/s]Evaluating on VQA val set:  93% 6245/6699 [1:13:16<05:27,  1.39it/s]Evaluating on VQA val set:  93% 6246/6699 [1:13:17<05:19,  1.42it/s]Evaluating on VQA val set:  93% 6247/6699 [1:13:17<05:02,  1.49it/s]Evaluating on VQA val set:  93% 6248/6699 [1:13:18<05:03,  1.49it/s]Evaluating on VQA val set:  93% 6249/6699 [1:13:19<05:14,  1.43it/s]Evaluating on VQA val set:  93% 6250/6699 [1:13:20<05:15,  1.42it/s]Evaluating on VQA val set:  93% 6251/6699 [1:13:20<05:20,  1.40it/s]Evaluating on VQA val set:  93% 6252/6699 [1:13:21<05:27,  1.37it/s]Evaluating on VQA val set:  93% 6253/6699 [1:13:22<05:28,  1.36it/s]Evaluating on VQA val set:  93% 6254/6699 [1:13:23<05:23,  1.38it/s]Evaluating on VQA val set:  93% 6255/6699 [1:13:23<05:20,  1.38it/s]Evaluating on VQA val set:  93% 6256/6699 [1:13:24<05:13,  1.41it/s]Evaluating on VQA val set:  93% 6257/6699 [1:13:25<05:06,  1.44it/s]Evaluating on VQA val set:  93% 6258/6699 [1:13:25<05:03,  1.45it/s]Evaluating on VQA val set:  93% 6259/6699 [1:13:26<05:13,  1.41it/s]Evaluating on VQA val set:  93% 6260/6699 [1:13:27<05:06,  1.43it/s]Evaluating on VQA val set:  93% 6261/6699 [1:13:27<05:04,  1.44it/s]Evaluating on VQA val set:  93% 6262/6699 [1:13:28<05:01,  1.45it/s]Evaluating on VQA val set:  93% 6263/6699 [1:13:29<04:56,  1.47it/s]Evaluating on VQA val set:  94% 6264/6699 [1:13:29<05:06,  1.42it/s]Evaluating on VQA val set:  94% 6265/6699 [1:13:30<05:02,  1.44it/s]Evaluating on VQA val set:  94% 6266/6699 [1:13:31<05:01,  1.43it/s]Evaluating on VQA val set:  94% 6267/6699 [1:13:31<04:55,  1.46it/s]Evaluating on VQA val set:  94% 6268/6699 [1:13:32<04:52,  1.48it/s]Evaluating on VQA val set:  94% 6269/6699 [1:13:33<04:55,  1.45it/s]Evaluating on VQA val set:  94% 6270/6699 [1:13:34<04:56,  1.45it/s]Evaluating on VQA val set:  94% 6271/6699 [1:13:34<05:01,  1.42it/s]Evaluating on VQA val set:  94% 6272/6699 [1:13:35<05:01,  1.42it/s]Evaluating on VQA val set:  94% 6273/6699 [1:13:36<05:02,  1.41it/s]Evaluating on VQA val set:  94% 6274/6699 [1:13:36<05:03,  1.40it/s]Evaluating on VQA val set:  94% 6275/6699 [1:13:37<04:53,  1.44it/s]Evaluating on VQA val set:  94% 6276/6699 [1:13:38<04:59,  1.41it/s]Evaluating on VQA val set:  94% 6277/6699 [1:13:39<04:57,  1.42it/s]Evaluating on VQA val set:  94% 6278/6699 [1:13:39<04:56,  1.42it/s]Evaluating on VQA val set:  94% 6279/6699 [1:13:40<04:47,  1.46it/s]Evaluating on VQA val set:  94% 6280/6699 [1:13:41<04:48,  1.45it/s]Evaluating on VQA val set:  94% 6281/6699 [1:13:41<04:50,  1.44it/s]Evaluating on VQA val set:  94% 6282/6699 [1:13:42<04:49,  1.44it/s]Evaluating on VQA val set:  94% 6283/6699 [1:13:43<04:46,  1.45it/s]Evaluating on VQA val set:  94% 6284/6699 [1:13:43<04:47,  1.44it/s]Evaluating on VQA val set:  94% 6285/6699 [1:13:44<04:41,  1.47it/s]Evaluating on VQA val set:  94% 6286/6699 [1:13:45<04:44,  1.45it/s]Evaluating on VQA val set:  94% 6287/6699 [1:13:45<04:44,  1.45it/s]Evaluating on VQA val set:  94% 6288/6699 [1:13:46<04:38,  1.48it/s]Evaluating on VQA val set:  94% 6289/6699 [1:13:47<04:45,  1.44it/s]Evaluating on VQA val set:  94% 6290/6699 [1:13:47<04:44,  1.44it/s]Evaluating on VQA val set:  94% 6291/6699 [1:13:48<04:35,  1.48it/s]Evaluating on VQA val set:  94% 6292/6699 [1:13:49<04:21,  1.56it/s]Evaluating on VQA val set:  94% 6293/6699 [1:13:49<04:16,  1.59it/s]Evaluating on VQA val set:  94% 6294/6699 [1:13:50<04:27,  1.52it/s]Evaluating on VQA val set:  94% 6295/6699 [1:13:51<04:25,  1.52it/s]Evaluating on VQA val set:  94% 6296/6699 [1:13:51<04:30,  1.49it/s]Evaluating on VQA val set:  94% 6297/6699 [1:13:52<04:39,  1.44it/s]Evaluating on VQA val set:  94% 6298/6699 [1:13:53<04:37,  1.44it/s]Evaluating on VQA val set:  94% 6299/6699 [1:13:53<04:21,  1.53it/s]Evaluating on VQA val set:  94% 6300/6699 [1:13:54<04:11,  1.59it/s]Evaluating on VQA val set:  94% 6301/6699 [1:13:55<04:03,  1.64it/s]Evaluating on VQA val set:  94% 6302/6699 [1:13:55<04:00,  1.65it/s]Evaluating on VQA val set:  94% 6303/6699 [1:13:56<04:10,  1.58it/s]Evaluating on VQA val set:  94% 6304/6699 [1:13:57<04:20,  1.51it/s]Evaluating on VQA val set:  94% 6305/6699 [1:13:57<04:20,  1.51it/s]Evaluating on VQA val set:  94% 6306/6699 [1:13:58<04:23,  1.49it/s]Evaluating on VQA val set:  94% 6307/6699 [1:13:59<04:32,  1.44it/s]Evaluating on VQA val set:  94% 6308/6699 [1:13:59<04:34,  1.42it/s]Evaluating on VQA val set:  94% 6309/6699 [1:14:00<04:36,  1.41it/s]Evaluating on VQA val set:  94% 6310/6699 [1:14:01<04:37,  1.40it/s]Evaluating on VQA val set:  94% 6311/6699 [1:14:01<04:33,  1.42it/s]Evaluating on VQA val set:  94% 6312/6699 [1:14:02<04:26,  1.45it/s]Evaluating on VQA val set:  94% 6313/6699 [1:14:03<04:24,  1.46it/s]Evaluating on VQA val set:  94% 6314/6699 [1:14:04<04:27,  1.44it/s]Evaluating on VQA val set:  94% 6315/6699 [1:14:04<04:28,  1.43it/s]Evaluating on VQA val set:  94% 6316/6699 [1:14:05<04:23,  1.45it/s]Evaluating on VQA val set:  94% 6317/6699 [1:14:06<04:23,  1.45it/s]Evaluating on VQA val set:  94% 6318/6699 [1:14:06<04:24,  1.44it/s]Evaluating on VQA val set:  94% 6319/6699 [1:14:07<04:25,  1.43it/s]Evaluating on VQA val set:  94% 6320/6699 [1:14:08<04:28,  1.41it/s]Evaluating on VQA val set:  94% 6321/6699 [1:14:08<04:27,  1.41it/s]Evaluating on VQA val set:  94% 6322/6699 [1:14:09<04:25,  1.42it/s]Evaluating on VQA val set:  94% 6323/6699 [1:14:10<04:20,  1.44it/s]Evaluating on VQA val set:  94% 6324/6699 [1:14:10<04:17,  1.46it/s]Evaluating on VQA val set:  94% 6325/6699 [1:14:11<04:19,  1.44it/s]Evaluating on VQA val set:  94% 6326/6699 [1:14:12<04:18,  1.44it/s]Evaluating on VQA val set:  94% 6327/6699 [1:14:13<04:22,  1.42it/s]Evaluating on VQA val set:  94% 6328/6699 [1:14:13<04:23,  1.41it/s]Evaluating on VQA val set:  94% 6329/6699 [1:14:14<04:23,  1.40it/s]Evaluating on VQA val set:  94% 6330/6699 [1:14:15<04:17,  1.43it/s]Evaluating on VQA val set:  95% 6331/6699 [1:14:15<04:18,  1.42it/s]Evaluating on VQA val set:  95% 6332/6699 [1:14:16<04:20,  1.41it/s]Evaluating on VQA val set:  95% 6333/6699 [1:14:17<04:16,  1.42it/s]Evaluating on VQA val set:  95% 6334/6699 [1:14:18<04:24,  1.38it/s]Evaluating on VQA val set:  95% 6335/6699 [1:14:18<04:15,  1.43it/s]Evaluating on VQA val set:  95% 6336/6699 [1:14:19<03:56,  1.53it/s]Evaluating on VQA val set:  95% 6337/6699 [1:14:20<04:07,  1.46it/s]Evaluating on VQA val set:  95% 6338/6699 [1:14:20<04:12,  1.43it/s]Evaluating on VQA val set:  95% 6339/6699 [1:14:21<04:17,  1.40it/s]Evaluating on VQA val set:  95% 6340/6699 [1:14:22<04:23,  1.36it/s]Evaluating on VQA val set:  95% 6341/6699 [1:14:23<04:19,  1.38it/s]Evaluating on VQA val set:  95% 6342/6699 [1:14:23<04:18,  1.38it/s]Evaluating on VQA val set:  95% 6343/6699 [1:14:24<04:09,  1.43it/s]Evaluating on VQA val set:  95% 6344/6699 [1:14:25<04:08,  1.43it/s]Evaluating on VQA val set:  95% 6345/6699 [1:14:25<04:08,  1.43it/s]Evaluating on VQA val set:  95% 6346/6699 [1:14:26<04:04,  1.44it/s]Evaluating on VQA val set:  95% 6347/6699 [1:14:27<04:06,  1.43it/s]Evaluating on VQA val set:  95% 6348/6699 [1:14:27<04:05,  1.43it/s]Evaluating on VQA val set:  95% 6349/6699 [1:14:28<04:07,  1.41it/s]Evaluating on VQA val set:  95% 6350/6699 [1:14:29<04:05,  1.42it/s]Evaluating on VQA val set:  95% 6351/6699 [1:14:29<03:54,  1.48it/s]Evaluating on VQA val set:  95% 6352/6699 [1:14:30<03:51,  1.50it/s]Evaluating on VQA val set:  95% 6353/6699 [1:14:31<03:46,  1.52it/s]Evaluating on VQA val set:  95% 6354/6699 [1:14:31<03:50,  1.50it/s]Evaluating on VQA val set:  95% 6355/6699 [1:14:32<03:53,  1.47it/s]Evaluating on VQA val set:  95% 6356/6699 [1:14:33<03:53,  1.47it/s]Evaluating on VQA val set:  95% 6357/6699 [1:14:33<03:50,  1.48it/s]Evaluating on VQA val set:  95% 6358/6699 [1:14:34<03:54,  1.45it/s]Evaluating on VQA val set:  95% 6359/6699 [1:14:35<03:58,  1.43it/s]Evaluating on VQA val set:  95% 6360/6699 [1:14:36<03:59,  1.42it/s]Evaluating on VQA val set:  95% 6361/6699 [1:14:36<03:57,  1.42it/s]Evaluating on VQA val set:  95% 6362/6699 [1:14:37<03:57,  1.42it/s]Evaluating on VQA val set:  95% 6363/6699 [1:14:38<03:59,  1.40it/s]Evaluating on VQA val set:  95% 6364/6699 [1:14:38<03:58,  1.40it/s]Evaluating on VQA val set:  95% 6365/6699 [1:14:39<03:53,  1.43it/s]Evaluating on VQA val set:  95% 6366/6699 [1:14:40<03:55,  1.42it/s]Evaluating on VQA val set:  95% 6367/6699 [1:14:41<03:56,  1.40it/s]Evaluating on VQA val set:  95% 6368/6699 [1:14:41<03:50,  1.43it/s]Evaluating on VQA val set:  95% 6369/6699 [1:14:42<03:52,  1.42it/s]Evaluating on VQA val set:  95% 6370/6699 [1:14:43<03:51,  1.42it/s]Evaluating on VQA val set:  95% 6371/6699 [1:14:43<03:50,  1.42it/s]Evaluating on VQA val set:  95% 6372/6699 [1:14:44<03:53,  1.40it/s]Evaluating on VQA val set:  95% 6373/6699 [1:14:45<03:56,  1.38it/s]Evaluating on VQA val set:  95% 6374/6699 [1:14:46<03:58,  1.36it/s]Evaluating on VQA val set:  95% 6375/6699 [1:14:46<03:54,  1.38it/s]Evaluating on VQA val set:  95% 6376/6699 [1:14:47<03:46,  1.42it/s]Evaluating on VQA val set:  95% 6377/6699 [1:14:48<03:47,  1.41it/s]Evaluating on VQA val set:  95% 6378/6699 [1:14:48<03:53,  1.38it/s]Evaluating on VQA val set:  95% 6379/6699 [1:14:49<03:52,  1.37it/s]Evaluating on VQA val set:  95% 6380/6699 [1:14:50<03:51,  1.38it/s]Evaluating on VQA val set:  95% 6381/6699 [1:14:51<03:48,  1.39it/s]Evaluating on VQA val set:  95% 6382/6699 [1:14:51<03:46,  1.40it/s]Evaluating on VQA val set:  95% 6383/6699 [1:14:52<03:37,  1.45it/s]Evaluating on VQA val set:  95% 6384/6699 [1:14:53<03:41,  1.42it/s]Evaluating on VQA val set:  95% 6385/6699 [1:14:53<03:41,  1.42it/s]Evaluating on VQA val set:  95% 6386/6699 [1:14:54<03:40,  1.42it/s]Evaluating on VQA val set:  95% 6387/6699 [1:14:55<03:40,  1.41it/s]Evaluating on VQA val set:  95% 6388/6699 [1:14:56<03:38,  1.42it/s]Evaluating on VQA val set:  95% 6389/6699 [1:14:56<03:29,  1.48it/s]Evaluating on VQA val set:  95% 6390/6699 [1:14:57<03:33,  1.45it/s]Evaluating on VQA val set:  95% 6391/6699 [1:14:58<03:37,  1.42it/s]Evaluating on VQA val set:  95% 6392/6699 [1:14:58<03:43,  1.38it/s]Evaluating on VQA val set:  95% 6393/6699 [1:14:59<03:44,  1.36it/s]Evaluating on VQA val set:  95% 6394/6699 [1:15:00<03:43,  1.36it/s]Evaluating on VQA val set:  95% 6395/6699 [1:15:01<03:49,  1.32it/s]Evaluating on VQA val set:  95% 6396/6699 [1:15:01<03:51,  1.31it/s]Evaluating on VQA val set:  95% 6397/6699 [1:15:02<03:50,  1.31it/s]Evaluating on VQA val set:  96% 6398/6699 [1:15:03<03:47,  1.32it/s]Evaluating on VQA val set:  96% 6399/6699 [1:15:04<03:45,  1.33it/s]Evaluating on VQA val set:  96% 6400/6699 [1:15:04<03:41,  1.35it/s]Evaluating on VQA val set:  96% 6401/6699 [1:15:05<03:36,  1.37it/s]Evaluating on VQA val set:  96% 6402/6699 [1:15:06<03:37,  1.36it/s]Evaluating on VQA val set:  96% 6403/6699 [1:15:07<03:37,  1.36it/s]Evaluating on VQA val set:  96% 6404/6699 [1:15:07<03:39,  1.34it/s]Evaluating on VQA val set:  96% 6405/6699 [1:15:08<03:28,  1.41it/s]Evaluating on VQA val set:  96% 6406/6699 [1:15:09<03:30,  1.39it/s]Evaluating on VQA val set:  96% 6407/6699 [1:15:09<03:24,  1.42it/s]Evaluating on VQA val set:  96% 6408/6699 [1:15:10<03:26,  1.41it/s]Evaluating on VQA val set:  96% 6409/6699 [1:15:11<03:23,  1.42it/s]Evaluating on VQA val set:  96% 6410/6699 [1:15:12<03:26,  1.40it/s]Evaluating on VQA val set:  96% 6411/6699 [1:15:12<03:22,  1.42it/s]Evaluating on VQA val set:  96% 6412/6699 [1:15:13<03:26,  1.39it/s]Evaluating on VQA val set:  96% 6413/6699 [1:15:14<03:25,  1.39it/s]Evaluating on VQA val set:  96% 6414/6699 [1:15:14<03:19,  1.43it/s]Evaluating on VQA val set:  96% 6415/6699 [1:15:15<03:16,  1.44it/s]Evaluating on VQA val set:  96% 6416/6699 [1:15:16<03:14,  1.45it/s]Evaluating on VQA val set:  96% 6417/6699 [1:15:16<03:12,  1.47it/s]Evaluating on VQA val set:  96% 6418/6699 [1:15:17<03:11,  1.47it/s]Evaluating on VQA val set:  96% 6419/6699 [1:15:18<03:13,  1.45it/s]Evaluating on VQA val set:  96% 6420/6699 [1:15:18<03:11,  1.46it/s]Evaluating on VQA val set:  96% 6421/6699 [1:15:19<03:03,  1.52it/s]Evaluating on VQA val set:  96% 6422/6699 [1:15:20<03:05,  1.49it/s]Evaluating on VQA val set:  96% 6423/6699 [1:15:20<03:13,  1.43it/s]Evaluating on VQA val set:  96% 6424/6699 [1:15:21<03:10,  1.45it/s]Evaluating on VQA val set:  96% 6425/6699 [1:15:22<03:05,  1.48it/s]Evaluating on VQA val set:  96% 6426/6699 [1:15:23<03:09,  1.44it/s]Evaluating on VQA val set:  96% 6427/6699 [1:15:23<03:07,  1.45it/s]Evaluating on VQA val set:  96% 6428/6699 [1:15:24<03:06,  1.45it/s]Evaluating on VQA val set:  96% 6429/6699 [1:15:25<03:07,  1.44it/s]Evaluating on VQA val set:  96% 6430/6699 [1:15:25<03:02,  1.47it/s]Evaluating on VQA val set:  96% 6431/6699 [1:15:26<03:02,  1.47it/s]Evaluating on VQA val set:  96% 6432/6699 [1:15:27<02:57,  1.50it/s]Evaluating on VQA val set:  96% 6433/6699 [1:15:27<02:57,  1.50it/s]Evaluating on VQA val set:  96% 6434/6699 [1:15:28<02:52,  1.54it/s]Evaluating on VQA val set:  96% 6435/6699 [1:15:29<02:53,  1.52it/s]Evaluating on VQA val set:  96% 6436/6699 [1:15:29<02:59,  1.47it/s]Evaluating on VQA val set:  96% 6437/6699 [1:15:30<03:01,  1.44it/s]Evaluating on VQA val set:  96% 6438/6699 [1:15:31<02:57,  1.47it/s]Evaluating on VQA val set:  96% 6439/6699 [1:15:31<02:57,  1.46it/s]Evaluating on VQA val set:  96% 6440/6699 [1:15:32<02:59,  1.44it/s]Evaluating on VQA val set:  96% 6441/6699 [1:15:33<03:01,  1.42it/s]Evaluating on VQA val set:  96% 6442/6699 [1:15:33<03:03,  1.40it/s]Evaluating on VQA val set:  96% 6443/6699 [1:15:34<02:58,  1.43it/s]Evaluating on VQA val set:  96% 6444/6699 [1:15:35<02:59,  1.42it/s]Evaluating on VQA val set:  96% 6445/6699 [1:15:36<02:58,  1.42it/s]Evaluating on VQA val set:  96% 6446/6699 [1:15:36<02:59,  1.41it/s]Evaluating on VQA val set:  96% 6447/6699 [1:15:37<03:00,  1.39it/s]Evaluating on VQA val set:  96% 6448/6699 [1:15:38<02:55,  1.43it/s]Evaluating on VQA val set:  96% 6449/6699 [1:15:38<02:55,  1.42it/s]Evaluating on VQA val set:  96% 6450/6699 [1:15:39<02:49,  1.47it/s]Evaluating on VQA val set:  96% 6451/6699 [1:15:40<02:52,  1.44it/s]Evaluating on VQA val set:  96% 6452/6699 [1:15:40<02:46,  1.48it/s]Evaluating on VQA val set:  96% 6453/6699 [1:15:41<02:47,  1.47it/s]Evaluating on VQA val set:  96% 6454/6699 [1:15:42<02:46,  1.47it/s]Evaluating on VQA val set:  96% 6455/6699 [1:15:42<02:48,  1.45it/s]Evaluating on VQA val set:  96% 6456/6699 [1:15:43<02:47,  1.45it/s]Evaluating on VQA val set:  96% 6457/6699 [1:15:44<02:47,  1.44it/s]Evaluating on VQA val set:  96% 6458/6699 [1:15:45<02:52,  1.40it/s]Evaluating on VQA val set:  96% 6459/6699 [1:15:45<02:46,  1.44it/s]Evaluating on VQA val set:  96% 6460/6699 [1:15:46<02:45,  1.44it/s]Evaluating on VQA val set:  96% 6461/6699 [1:15:47<02:39,  1.49it/s]Evaluating on VQA val set:  96% 6462/6699 [1:15:47<02:38,  1.50it/s]Evaluating on VQA val set:  96% 6463/6699 [1:15:48<02:41,  1.46it/s]Evaluating on VQA val set:  96% 6464/6699 [1:15:49<02:39,  1.48it/s]Evaluating on VQA val set:  97% 6465/6699 [1:15:49<02:45,  1.42it/s]Evaluating on VQA val set:  97% 6466/6699 [1:15:50<02:44,  1.41it/s]Evaluating on VQA val set:  97% 6467/6699 [1:15:51<02:43,  1.41it/s]Evaluating on VQA val set:  97% 6468/6699 [1:15:52<02:41,  1.43it/s]Evaluating on VQA val set:  97% 6469/6699 [1:15:52<02:43,  1.41it/s]Evaluating on VQA val set:  97% 6470/6699 [1:15:53<02:44,  1.39it/s]Evaluating on VQA val set:  97% 6471/6699 [1:15:54<02:39,  1.43it/s]Evaluating on VQA val set:  97% 6472/6699 [1:15:54<02:35,  1.46it/s]Evaluating on VQA val set:  97% 6473/6699 [1:15:55<02:35,  1.45it/s]Evaluating on VQA val set:  97% 6474/6699 [1:15:56<02:41,  1.40it/s]Evaluating on VQA val set:  97% 6475/6699 [1:15:56<02:38,  1.42it/s]Evaluating on VQA val set:  97% 6476/6699 [1:15:57<02:38,  1.40it/s]Evaluating on VQA val set:  97% 6477/6699 [1:15:58<02:32,  1.46it/s]Evaluating on VQA val set:  97% 6478/6699 [1:15:59<02:33,  1.44it/s]Evaluating on VQA val set:  97% 6479/6699 [1:15:59<02:34,  1.42it/s]Evaluating on VQA val set:  97% 6480/6699 [1:16:00<02:35,  1.40it/s]Evaluating on VQA val set:  97% 6481/6699 [1:16:01<02:37,  1.38it/s]Evaluating on VQA val set:  97% 6482/6699 [1:16:01<02:37,  1.38it/s]Evaluating on VQA val set:  97% 6483/6699 [1:16:02<02:36,  1.38it/s]Evaluating on VQA val set:  97% 6484/6699 [1:16:03<02:36,  1.38it/s]Evaluating on VQA val set:  97% 6485/6699 [1:16:04<02:36,  1.37it/s]Evaluating on VQA val set:  97% 6486/6699 [1:16:04<02:34,  1.38it/s]Evaluating on VQA val set:  97% 6487/6699 [1:16:05<02:33,  1.38it/s]Evaluating on VQA val set:  97% 6488/6699 [1:16:06<02:29,  1.41it/s]Evaluating on VQA val set:  97% 6489/6699 [1:16:06<02:30,  1.40it/s]Evaluating on VQA val set:  97% 6490/6699 [1:16:07<02:29,  1.40it/s]Evaluating on VQA val set:  97% 6491/6699 [1:16:08<02:31,  1.37it/s]Evaluating on VQA val set:  97% 6492/6699 [1:16:09<02:30,  1.37it/s]Evaluating on VQA val set:  97% 6493/6699 [1:16:09<02:31,  1.36it/s]Evaluating on VQA val set:  97% 6494/6699 [1:16:10<02:29,  1.37it/s]Evaluating on VQA val set:  97% 6495/6699 [1:16:11<02:27,  1.38it/s]Evaluating on VQA val set:  97% 6496/6699 [1:16:12<02:29,  1.36it/s]Evaluating on VQA val set:  97% 6497/6699 [1:16:12<02:29,  1.35it/s]Evaluating on VQA val set:  97% 6498/6699 [1:16:13<02:26,  1.37it/s]Evaluating on VQA val set:  97% 6499/6699 [1:16:14<02:23,  1.39it/s]Evaluating on VQA val set:  97% 6500/6699 [1:16:14<02:22,  1.40it/s]Evaluating on VQA val set:  97% 6501/6699 [1:16:15<02:19,  1.42it/s]Evaluating on VQA val set:  97% 6502/6699 [1:16:16<02:17,  1.43it/s]Evaluating on VQA val set:  97% 6503/6699 [1:16:17<02:19,  1.41it/s]Evaluating on VQA val set:  97% 6504/6699 [1:16:17<02:20,  1.38it/s]Evaluating on VQA val set:  97% 6505/6699 [1:16:18<02:21,  1.37it/s]Evaluating on VQA val set:  97% 6506/6699 [1:16:19<02:12,  1.46it/s]Evaluating on VQA val set:  97% 6507/6699 [1:16:19<02:11,  1.46it/s]Evaluating on VQA val set:  97% 6508/6699 [1:16:20<02:13,  1.43it/s]Evaluating on VQA val set:  97% 6509/6699 [1:16:21<02:14,  1.42it/s]Evaluating on VQA val set:  97% 6510/6699 [1:16:22<02:13,  1.42it/s]Evaluating on VQA val set:  97% 6511/6699 [1:16:22<02:14,  1.39it/s]Evaluating on VQA val set:  97% 6512/6699 [1:16:23<02:13,  1.40it/s]Evaluating on VQA val set:  97% 6513/6699 [1:16:24<02:13,  1.40it/s]Evaluating on VQA val set:  97% 6514/6699 [1:16:24<02:13,  1.38it/s]Evaluating on VQA val set:  97% 6515/6699 [1:16:25<02:05,  1.46it/s]Evaluating on VQA val set:  97% 6516/6699 [1:16:26<02:01,  1.50it/s]Evaluating on VQA val set:  97% 6517/6699 [1:16:26<02:03,  1.47it/s]Evaluating on VQA val set:  97% 6518/6699 [1:16:27<02:04,  1.46it/s]Evaluating on VQA val set:  97% 6519/6699 [1:16:28<01:59,  1.51it/s]Evaluating on VQA val set:  97% 6520/6699 [1:16:28<01:57,  1.52it/s]Evaluating on VQA val set:  97% 6521/6699 [1:16:29<02:03,  1.44it/s]Evaluating on VQA val set:  97% 6522/6699 [1:16:30<02:07,  1.39it/s]Evaluating on VQA val set:  97% 6523/6699 [1:16:31<02:04,  1.41it/s]Evaluating on VQA val set:  97% 6524/6699 [1:16:31<02:02,  1.43it/s]Evaluating on VQA val set:  97% 6525/6699 [1:16:32<02:00,  1.44it/s]Evaluating on VQA val set:  97% 6526/6699 [1:16:33<02:02,  1.41it/s]Evaluating on VQA val set:  97% 6527/6699 [1:16:33<02:02,  1.40it/s]Evaluating on VQA val set:  97% 6528/6699 [1:16:34<02:03,  1.38it/s]Evaluating on VQA val set:  97% 6529/6699 [1:16:35<01:58,  1.44it/s]Evaluating on VQA val set:  97% 6530/6699 [1:16:35<01:59,  1.41it/s]Evaluating on VQA val set:  97% 6531/6699 [1:16:36<02:01,  1.39it/s]Evaluating on VQA val set:  98% 6532/6699 [1:16:37<02:01,  1.38it/s]Evaluating on VQA val set:  98% 6533/6699 [1:16:38<01:59,  1.39it/s]Evaluating on VQA val set:  98% 6534/6699 [1:16:38<01:56,  1.41it/s]Evaluating on VQA val set:  98% 6535/6699 [1:16:39<01:54,  1.43it/s]Evaluating on VQA val set:  98% 6536/6699 [1:16:40<01:50,  1.48it/s]Evaluating on VQA val set:  98% 6537/6699 [1:16:40<01:46,  1.52it/s]Evaluating on VQA val set:  98% 6538/6699 [1:16:41<01:50,  1.46it/s]Evaluating on VQA val set:  98% 6539/6699 [1:16:42<01:51,  1.44it/s]Evaluating on VQA val set:  98% 6540/6699 [1:16:42<01:50,  1.44it/s]Evaluating on VQA val set:  98% 6541/6699 [1:16:43<01:52,  1.41it/s]Evaluating on VQA val set:  98% 6542/6699 [1:16:44<01:49,  1.43it/s]Evaluating on VQA val set:  98% 6543/6699 [1:16:45<01:50,  1.41it/s]Evaluating on VQA val set:  98% 6544/6699 [1:16:45<01:51,  1.39it/s]Evaluating on VQA val set:  98% 6545/6699 [1:16:46<01:50,  1.40it/s]Evaluating on VQA val set:  98% 6546/6699 [1:16:47<01:48,  1.41it/s]Evaluating on VQA val set:  98% 6547/6699 [1:16:47<01:47,  1.42it/s]Evaluating on VQA val set:  98% 6548/6699 [1:16:48<01:46,  1.42it/s]Evaluating on VQA val set:  98% 6549/6699 [1:16:49<01:46,  1.41it/s]Evaluating on VQA val set:  98% 6550/6699 [1:16:50<01:46,  1.40it/s]Evaluating on VQA val set:  98% 6551/6699 [1:16:50<01:44,  1.41it/s]Evaluating on VQA val set:  98% 6552/6699 [1:16:51<01:44,  1.40it/s]Evaluating on VQA val set:  98% 6553/6699 [1:16:52<01:45,  1.38it/s]Evaluating on VQA val set:  98% 6554/6699 [1:16:52<01:43,  1.40it/s]Evaluating on VQA val set:  98% 6555/6699 [1:16:53<01:42,  1.40it/s]Evaluating on VQA val set:  98% 6556/6699 [1:16:54<01:44,  1.37it/s]Evaluating on VQA val set:  98% 6557/6699 [1:16:55<01:44,  1.36it/s]Evaluating on VQA val set:  98% 6558/6699 [1:16:55<01:42,  1.37it/s]Evaluating on VQA val set:  98% 6559/6699 [1:16:56<01:42,  1.37it/s]Evaluating on VQA val set:  98% 6560/6699 [1:16:57<01:40,  1.38it/s]Evaluating on VQA val set:  98% 6561/6699 [1:16:58<01:41,  1.36it/s]Evaluating on VQA val set:  98% 6562/6699 [1:16:58<01:38,  1.39it/s]Evaluating on VQA val set:  98% 6563/6699 [1:16:59<01:38,  1.38it/s]Evaluating on VQA val set:  98% 6564/6699 [1:17:00<01:38,  1.37it/s]Evaluating on VQA val set:  98% 6565/6699 [1:17:00<01:37,  1.37it/s]Evaluating on VQA val set:  98% 6566/6699 [1:17:01<01:35,  1.39it/s]Evaluating on VQA val set:  98% 6567/6699 [1:17:02<01:36,  1.37it/s]Evaluating on VQA val set:  98% 6568/6699 [1:17:03<01:33,  1.40it/s]Evaluating on VQA val set:  98% 6569/6699 [1:17:03<01:33,  1.39it/s]Evaluating on VQA val set:  98% 6570/6699 [1:17:04<01:32,  1.39it/s]Evaluating on VQA val set:  98% 6571/6699 [1:17:05<01:32,  1.39it/s]Evaluating on VQA val set:  98% 6572/6699 [1:17:05<01:26,  1.46it/s]Evaluating on VQA val set:  98% 6573/6699 [1:17:06<01:24,  1.48it/s]Evaluating on VQA val set:  98% 6574/6699 [1:17:07<01:26,  1.45it/s]Evaluating on VQA val set:  98% 6575/6699 [1:17:07<01:25,  1.44it/s]Evaluating on VQA val set:  98% 6576/6699 [1:17:08<01:24,  1.45it/s]Evaluating on VQA val set:  98% 6577/6699 [1:17:09<01:24,  1.44it/s]Evaluating on VQA val set:  98% 6578/6699 [1:17:10<01:26,  1.41it/s]Evaluating on VQA val set:  98% 6579/6699 [1:17:10<01:25,  1.41it/s]Evaluating on VQA val set:  98% 6580/6699 [1:17:11<01:25,  1.39it/s]Evaluating on VQA val set:  98% 6581/6699 [1:17:12<01:24,  1.40it/s]Evaluating on VQA val set:  98% 6582/6699 [1:17:12<01:23,  1.40it/s]Evaluating on VQA val set:  98% 6583/6699 [1:17:13<01:24,  1.38it/s]Evaluating on VQA val set:  98% 6584/6699 [1:17:14<01:21,  1.41it/s]Evaluating on VQA val set:  98% 6585/6699 [1:17:15<01:19,  1.43it/s]Evaluating on VQA val set:  98% 6586/6699 [1:17:15<01:21,  1.38it/s]Evaluating on VQA val set:  98% 6587/6699 [1:17:16<01:21,  1.37it/s]Evaluating on VQA val set:  98% 6588/6699 [1:17:17<01:19,  1.39it/s]Evaluating on VQA val set:  98% 6589/6699 [1:17:18<01:20,  1.37it/s]Evaluating on VQA val set:  98% 6590/6699 [1:17:18<01:16,  1.42it/s]Evaluating on VQA val set:  98% 6591/6699 [1:17:19<01:16,  1.42it/s]Evaluating on VQA val set:  98% 6592/6699 [1:17:20<01:17,  1.37it/s]Evaluating on VQA val set:  98% 6593/6699 [1:17:20<01:15,  1.40it/s]Evaluating on VQA val set:  98% 6594/6699 [1:17:21<01:15,  1.40it/s]Evaluating on VQA val set:  98% 6595/6699 [1:17:22<01:15,  1.37it/s]Evaluating on VQA val set:  98% 6596/6699 [1:17:23<01:14,  1.39it/s]Evaluating on VQA val set:  98% 6597/6699 [1:17:23<01:11,  1.44it/s]Evaluating on VQA val set:  98% 6598/6699 [1:17:24<01:09,  1.45it/s]Evaluating on VQA val set:  99% 6599/6699 [1:17:24<01:07,  1.48it/s]Evaluating on VQA val set:  99% 6600/6699 [1:17:25<01:07,  1.46it/s]Evaluating on VQA val set:  99% 6601/6699 [1:17:26<01:06,  1.48it/s]Evaluating on VQA val set:  99% 6602/6699 [1:17:27<01:07,  1.44it/s]Evaluating on VQA val set:  99% 6603/6699 [1:17:27<01:05,  1.46it/s]Evaluating on VQA val set:  99% 6604/6699 [1:17:28<01:04,  1.46it/s]Evaluating on VQA val set:  99% 6605/6699 [1:17:29<01:05,  1.44it/s]Evaluating on VQA val set:  99% 6606/6699 [1:17:29<01:05,  1.42it/s]Evaluating on VQA val set:  99% 6607/6699 [1:17:30<01:05,  1.40it/s]Evaluating on VQA val set:  99% 6608/6699 [1:17:31<01:03,  1.44it/s]Evaluating on VQA val set:  99% 6609/6699 [1:17:31<01:03,  1.42it/s]Evaluating on VQA val set:  99% 6610/6699 [1:17:32<01:03,  1.40it/s]Evaluating on VQA val set:  99% 6611/6699 [1:17:33<01:02,  1.42it/s]Evaluating on VQA val set:  99% 6612/6699 [1:17:34<00:59,  1.45it/s]Evaluating on VQA val set:  99% 6613/6699 [1:17:34<00:56,  1.53it/s]Evaluating on VQA val set:  99% 6614/6699 [1:17:35<00:55,  1.54it/s]Evaluating on VQA val set:  99% 6615/6699 [1:17:35<00:54,  1.54it/s]Evaluating on VQA val set:  99% 6616/6699 [1:17:36<00:53,  1.54it/s]Evaluating on VQA val set:  99% 6617/6699 [1:17:37<00:54,  1.51it/s]Evaluating on VQA val set:  99% 6618/6699 [1:17:38<00:56,  1.43it/s]Evaluating on VQA val set:  99% 6619/6699 [1:17:38<00:54,  1.46it/s]Evaluating on VQA val set:  99% 6620/6699 [1:17:39<00:53,  1.48it/s]Evaluating on VQA val set:  99% 6621/6699 [1:17:39<00:51,  1.50it/s]Evaluating on VQA val set:  99% 6622/6699 [1:17:40<00:50,  1.51it/s]Evaluating on VQA val set:  99% 6623/6699 [1:17:41<00:48,  1.55it/s]Evaluating on VQA val set:  99% 6624/6699 [1:17:41<00:49,  1.53it/s]Evaluating on VQA val set:  99% 6625/6699 [1:17:42<00:49,  1.48it/s]Evaluating on VQA val set:  99% 6626/6699 [1:17:43<00:51,  1.43it/s]Evaluating on VQA val set:  99% 6627/6699 [1:17:44<00:50,  1.42it/s]Evaluating on VQA val set:  99% 6628/6699 [1:17:44<00:49,  1.43it/s]Evaluating on VQA val set:  99% 6629/6699 [1:17:45<00:49,  1.43it/s]Evaluating on VQA val set:  99% 6630/6699 [1:17:46<00:48,  1.42it/s]Evaluating on VQA val set:  99% 6631/6699 [1:17:46<00:47,  1.43it/s]Evaluating on VQA val set:  99% 6632/6699 [1:17:47<00:47,  1.41it/s]Evaluating on VQA val set:  99% 6633/6699 [1:17:48<00:47,  1.39it/s]Evaluating on VQA val set:  99% 6634/6699 [1:17:49<00:45,  1.42it/s]Evaluating on VQA val set:  99% 6635/6699 [1:17:49<00:43,  1.46it/s]Evaluating on VQA val set:  99% 6636/6699 [1:17:50<00:44,  1.41it/s]Evaluating on VQA val set:  99% 6637/6699 [1:17:51<00:43,  1.42it/s]Evaluating on VQA val set:  99% 6638/6699 [1:17:51<00:42,  1.45it/s]Evaluating on VQA val set:  99% 6639/6699 [1:17:52<00:40,  1.47it/s]Evaluating on VQA val set:  99% 6640/6699 [1:17:53<00:39,  1.49it/s]Evaluating on VQA val set:  99% 6641/6699 [1:17:53<00:38,  1.51it/s]Evaluating on VQA val set:  99% 6642/6699 [1:17:54<00:38,  1.49it/s]Evaluating on VQA val set:  99% 6643/6699 [1:17:55<00:37,  1.50it/s]Evaluating on VQA val set:  99% 6644/6699 [1:17:55<00:36,  1.51it/s]Evaluating on VQA val set:  99% 6645/6699 [1:17:56<00:34,  1.55it/s]Evaluating on VQA val set:  99% 6646/6699 [1:17:57<00:35,  1.50it/s]Evaluating on VQA val set:  99% 6647/6699 [1:17:57<00:36,  1.42it/s]Evaluating on VQA val set:  99% 6648/6699 [1:17:58<00:35,  1.42it/s]Evaluating on VQA val set:  99% 6649/6699 [1:17:59<00:35,  1.42it/s]Evaluating on VQA val set:  99% 6650/6699 [1:18:00<00:36,  1.35it/s]Evaluating on VQA val set:  99% 6651/6699 [1:18:00<00:34,  1.40it/s]Evaluating on VQA val set:  99% 6652/6699 [1:18:01<00:34,  1.36it/s]Evaluating on VQA val set:  99% 6653/6699 [1:18:02<00:33,  1.36it/s]Evaluating on VQA val set:  99% 6654/6699 [1:18:03<00:33,  1.36it/s]Evaluating on VQA val set:  99% 6655/6699 [1:18:03<00:32,  1.37it/s]Evaluating on VQA val set:  99% 6656/6699 [1:18:04<00:31,  1.35it/s]Evaluating on VQA val set:  99% 6657/6699 [1:18:05<00:29,  1.42it/s]Evaluating on VQA val set:  99% 6658/6699 [1:18:05<00:29,  1.40it/s]Evaluating on VQA val set:  99% 6659/6699 [1:18:06<00:28,  1.41it/s]Evaluating on VQA val set:  99% 6660/6699 [1:18:07<00:28,  1.39it/s]Evaluating on VQA val set:  99% 6661/6699 [1:18:08<00:27,  1.37it/s]Evaluating on VQA val set:  99% 6662/6699 [1:18:08<00:25,  1.44it/s]Evaluating on VQA val set:  99% 6663/6699 [1:18:09<00:24,  1.46it/s]Evaluating on VQA val set:  99% 6664/6699 [1:18:10<00:24,  1.43it/s]Evaluating on VQA val set:  99% 6665/6699 [1:18:10<00:23,  1.44it/s]Evaluating on VQA val set: 100% 6666/6699 [1:18:11<00:22,  1.45it/s]Evaluating on VQA val set: 100% 6667/6699 [1:18:12<00:22,  1.41it/s]Evaluating on VQA val set: 100% 6668/6699 [1:18:12<00:22,  1.40it/s]Evaluating on VQA val set: 100% 6669/6699 [1:18:13<00:21,  1.38it/s]Evaluating on VQA val set: 100% 6670/6699 [1:18:14<00:20,  1.39it/s]Evaluating on VQA val set: 100% 6671/6699 [1:18:15<00:20,  1.37it/s]Evaluating on VQA val set: 100% 6672/6699 [1:18:15<00:19,  1.41it/s]Evaluating on VQA val set: 100% 6673/6699 [1:18:16<00:18,  1.42it/s]Evaluating on VQA val set: 100% 6674/6699 [1:18:17<00:17,  1.41it/s]Evaluating on VQA val set: 100% 6675/6699 [1:18:17<00:17,  1.38it/s]Evaluating on VQA val set: 100% 6676/6699 [1:18:18<00:16,  1.38it/s]Evaluating on VQA val set: 100% 6677/6699 [1:18:19<00:15,  1.40it/s]Evaluating on VQA val set: 100% 6678/6699 [1:18:20<00:15,  1.39it/s]Evaluating on VQA val set: 100% 6679/6699 [1:18:20<00:14,  1.43it/s]Evaluating on VQA val set: 100% 6680/6699 [1:18:21<00:13,  1.41it/s]Evaluating on VQA val set: 100% 6681/6699 [1:18:22<00:12,  1.43it/s]Evaluating on VQA val set: 100% 6682/6699 [1:18:22<00:11,  1.42it/s]Evaluating on VQA val set: 100% 6683/6699 [1:18:23<00:11,  1.45it/s]Evaluating on VQA val set: 100% 6684/6699 [1:18:24<00:10,  1.48it/s]Evaluating on VQA val set: 100% 6685/6699 [1:18:24<00:09,  1.47it/s]Evaluating on VQA val set: 100% 6686/6699 [1:18:25<00:08,  1.46it/s]Evaluating on VQA val set: 100% 6687/6699 [1:18:26<00:08,  1.44it/s]Evaluating on VQA val set: 100% 6688/6699 [1:18:26<00:07,  1.41it/s]Evaluating on VQA val set: 100% 6689/6699 [1:18:27<00:07,  1.39it/s]Evaluating on VQA val set: 100% 6690/6699 [1:18:28<00:06,  1.36it/s]Evaluating on VQA val set: 100% 6691/6699 [1:18:29<00:05,  1.36it/s]Evaluating on VQA val set: 100% 6692/6699 [1:18:29<00:05,  1.36it/s]Evaluating on VQA val set: 100% 6693/6699 [1:18:30<00:04,  1.41it/s]Evaluating on VQA val set: 100% 6694/6699 [1:18:31<00:03,  1.41it/s]Evaluating on VQA val set: 100% 6695/6699 [1:18:32<00:02,  1.43it/s]Evaluating on VQA val set: 100% 6696/6699 [1:18:32<00:02,  1.44it/s]Evaluating on VQA val set: 100% 6697/6699 [1:18:33<00:01,  1.44it/s]Evaluating on VQA val set: 100% 6698/6699 [1:18:34<00:00,  1.43it/s]Evaluating on VQA val set: 100% 6699/6699 [1:18:34<00:00,  1.70it/s]Evaluating on VQA val set: 100% 6699/6699 [1:18:34<00:00,  1.42it/s]
11/15/2022 17:06:15 - INFO - train.train_vqa - Evaluation after epoch 1: 63.35
11/15/2022 17:06:15 - INFO - train.train_vqa - New best evaluation score: 63.35
11/15/2022 17:06:15 - INFO - __main__ - Best VQAv2 evaluation score = 63.35, after epoch 1
11/15/2022 17:06:15 - INFO - __main__ - Saving best model and encoder checkpoint after VQAv2 training
11/15/2022 17:06:17 - INFO - __main__ - Saved checkpoint!
11/15/2022 17:06:17 - INFO - __main__ - Saved continual learning results so far!
11/15/2022 17:06:17 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 17:06:17 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/15/2022 17:06:17 - INFO - __main__ - Training vilt model on task #2: SNLI-VE
11/15/2022 17:06:17 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE train dataloader with batch size of 32
11/15/2022 17:06:19 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE train dataset, with 529527 examples
11/15/2022 17:06:19 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE dev dataloader with batch size of 32
11/15/2022 17:06:19 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE dev dataset, with 17858 examples
Training epoch 1:   0% 0/16548 [00:00<?, ?it/s]/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/nn/functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
11/15/2022 17:06:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:24 - INFO - train.train_snli_ve - loss is tensor(8.2396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 1/16548 [00:05<25:28:43,  5.54s/it]11/15/2022 17:06:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:26 - INFO - train.train_snli_ve - loss is tensor(8.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 2/16548 [00:07<15:16:53,  3.32s/it]11/15/2022 17:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:28 - INFO - train.train_snli_ve - loss is tensor(8.1980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 3/16548 [00:08<11:46:11,  2.56s/it]11/15/2022 17:06:29 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:29 - INFO - train.train_snli_ve - loss is tensor(8.1542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 4/16548 [00:10<10:06:36,  2.20s/it]11/15/2022 17:06:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:31 - INFO - train.train_snli_ve - loss is tensor(8.1993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 5/16548 [00:12<9:14:28,  2.01s/it] 11/15/2022 17:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:33 - INFO - train.train_snli_ve - loss is tensor(8.1995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 6/16548 [00:13<8:40:22,  1.89s/it]11/15/2022 17:06:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:34 - INFO - train.train_snli_ve - loss is tensor(8.1665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 7/16548 [00:15<8:20:42,  1.82s/it]11/15/2022 17:06:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:36 - INFO - train.train_snli_ve - loss is tensor(8.0849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 8/16548 [00:17<8:08:17,  1.77s/it]11/15/2022 17:06:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:38 - INFO - train.train_snli_ve - loss is tensor(8.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 9/16548 [00:18<7:58:33,  1.74s/it]11/15/2022 17:06:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:39 - INFO - train.train_snli_ve - loss is tensor(8.1129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 10/16548 [00:20<7:52:08,  1.71s/it]11/15/2022 17:06:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:41 - INFO - train.train_snli_ve - loss is tensor(8.1355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 11/16548 [00:22<7:50:07,  1.71s/it]11/15/2022 17:06:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:43 - INFO - train.train_snli_ve - loss is tensor(8.0775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 12/16548 [00:23<7:43:58,  1.68s/it]11/15/2022 17:06:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:44 - INFO - train.train_snli_ve - loss is tensor(8.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 13/16548 [00:25<7:45:59,  1.69s/it]11/15/2022 17:06:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:46 - INFO - train.train_snli_ve - loss is tensor(8.1525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 14/16548 [00:27<7:46:13,  1.69s/it]11/15/2022 17:06:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:48 - INFO - train.train_snli_ve - loss is tensor(8.1097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 15/16548 [00:29<7:44:56,  1.69s/it]11/15/2022 17:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:50 - INFO - train.train_snli_ve - loss is tensor(8.0789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 16/16548 [00:30<7:51:56,  1.71s/it]11/15/2022 17:06:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:51 - INFO - train.train_snli_ve - loss is tensor(8.1133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 17/16548 [00:32<7:48:01,  1.70s/it]11/15/2022 17:06:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:53 - INFO - train.train_snli_ve - loss is tensor(8.2135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 18/16548 [00:34<7:51:15,  1.71s/it]11/15/2022 17:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:55 - INFO - train.train_snli_ve - loss is tensor(8.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 19/16548 [00:35<7:47:58,  1.70s/it]11/15/2022 17:06:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:56 - INFO - train.train_snli_ve - loss is tensor(8.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 20/16548 [00:37<7:49:17,  1.70s/it]11/15/2022 17:06:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:06:58 - INFO - train.train_snli_ve - loss is tensor(8.0869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 21/16548 [00:39<7:54:06,  1.72s/it]11/15/2022 17:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:00 - INFO - train.train_snli_ve - loss is tensor(7.9795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 22/16548 [00:40<7:49:31,  1.70s/it]11/15/2022 17:07:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:01 - INFO - train.train_snli_ve - loss is tensor(8.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 23/16548 [00:42<7:48:00,  1.70s/it]11/15/2022 17:07:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:03 - INFO - train.train_snli_ve - loss is tensor(8.1069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 24/16548 [00:44<7:44:50,  1.69s/it]11/15/2022 17:07:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:05 - INFO - train.train_snli_ve - loss is tensor(8.0181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 25/16548 [00:46<7:45:30,  1.69s/it]11/15/2022 17:07:06 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:06 - INFO - train.train_snli_ve - loss is tensor(8.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 26/16548 [00:47<7:44:57,  1.69s/it]11/15/2022 17:07:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:08 - INFO - train.train_snli_ve - loss is tensor(7.9806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 27/16548 [00:49<7:43:08,  1.68s/it]11/15/2022 17:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:10 - INFO - train.train_snli_ve - loss is tensor(7.9238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 28/16548 [00:51<7:47:05,  1.70s/it]11/15/2022 17:07:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:12 - INFO - train.train_snli_ve - loss is tensor(8.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 29/16548 [00:52<7:44:44,  1.69s/it]11/15/2022 17:07:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:13 - INFO - train.train_snli_ve - loss is tensor(8.0084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 30/16548 [00:54<7:46:18,  1.69s/it]11/15/2022 17:07:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:15 - INFO - train.train_snli_ve - loss is tensor(7.9911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 31/16548 [00:56<7:47:42,  1.70s/it]11/15/2022 17:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:17 - INFO - train.train_snli_ve - loss is tensor(7.9919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 32/16548 [00:57<7:45:39,  1.69s/it]11/15/2022 17:07:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:18 - INFO - train.train_snli_ve - loss is tensor(7.9483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 33/16548 [00:59<7:47:50,  1.70s/it]11/15/2022 17:07:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:20 - INFO - train.train_snli_ve - loss is tensor(7.9165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 34/16548 [01:01<7:49:15,  1.70s/it]11/15/2022 17:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:22 - INFO - train.train_snli_ve - loss is tensor(7.9438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 35/16548 [01:03<7:49:21,  1.71s/it]11/15/2022 17:07:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:23 - INFO - train.train_snli_ve - loss is tensor(7.8889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 36/16548 [01:04<7:45:07,  1.69s/it]11/15/2022 17:07:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:25 - INFO - train.train_snli_ve - loss is tensor(7.9159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 37/16548 [01:06<7:44:12,  1.69s/it]11/15/2022 17:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:27 - INFO - train.train_snli_ve - loss is tensor(7.9225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 38/16548 [01:08<7:43:26,  1.68s/it]11/15/2022 17:07:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:28 - INFO - train.train_snli_ve - loss is tensor(7.8971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 39/16548 [01:09<7:42:36,  1.68s/it]11/15/2022 17:07:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:30 - INFO - train.train_snli_ve - loss is tensor(7.8685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 40/16548 [01:11<7:43:47,  1.69s/it]11/15/2022 17:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:32 - INFO - train.train_snli_ve - loss is tensor(7.8663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 41/16548 [01:13<7:44:34,  1.69s/it]11/15/2022 17:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:34 - INFO - train.train_snli_ve - loss is tensor(7.8128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 42/16548 [01:14<7:40:30,  1.67s/it]11/15/2022 17:07:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:35 - INFO - train.train_snli_ve - loss is tensor(7.7976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 43/16548 [01:16<7:39:59,  1.67s/it]11/15/2022 17:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:37 - INFO - train.train_snli_ve - loss is tensor(7.7537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 44/16548 [01:18<7:40:02,  1.67s/it]11/15/2022 17:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:39 - INFO - train.train_snli_ve - loss is tensor(7.7843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 45/16548 [01:19<7:41:26,  1.68s/it]11/15/2022 17:07:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:40 - INFO - train.train_snli_ve - loss is tensor(7.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 46/16548 [01:21<7:40:58,  1.68s/it]11/15/2022 17:07:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:42 - INFO - train.train_snli_ve - loss is tensor(7.8136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 47/16548 [01:23<7:38:42,  1.67s/it]11/15/2022 17:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:44 - INFO - train.train_snli_ve - loss is tensor(7.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 48/16548 [01:24<7:42:33,  1.68s/it]11/15/2022 17:07:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:45 - INFO - train.train_snli_ve - loss is tensor(7.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 49/16548 [01:26<7:41:15,  1.68s/it]11/15/2022 17:07:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:47 - INFO - train.train_snli_ve - loss is tensor(7.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 50/16548 [01:28<7:42:42,  1.68s/it]11/15/2022 17:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:49 - INFO - train.train_snli_ve - loss is tensor(7.6846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 51/16548 [01:29<7:42:09,  1.68s/it]11/15/2022 17:07:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:50 - INFO - train.train_snli_ve - loss is tensor(7.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 52/16548 [01:31<7:42:02,  1.68s/it]11/15/2022 17:07:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:52 - INFO - train.train_snli_ve - loss is tensor(7.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 53/16548 [01:33<7:42:47,  1.68s/it]11/15/2022 17:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:54 - INFO - train.train_snli_ve - loss is tensor(7.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 54/16548 [01:34<7:41:19,  1.68s/it]11/15/2022 17:07:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:55 - INFO - train.train_snli_ve - loss is tensor(7.5686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 55/16548 [01:36<7:37:42,  1.67s/it]11/15/2022 17:07:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:57 - INFO - train.train_snli_ve - loss is tensor(7.5258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 56/16548 [01:38<7:36:37,  1.66s/it]11/15/2022 17:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:07:59 - INFO - train.train_snli_ve - loss is tensor(7.4766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 57/16548 [01:39<7:35:34,  1.66s/it]11/15/2022 17:08:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:00 - INFO - train.train_snli_ve - loss is tensor(7.4442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 58/16548 [01:41<7:35:18,  1.66s/it]11/15/2022 17:08:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:02 - INFO - train.train_snli_ve - loss is tensor(7.5293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 59/16548 [01:43<7:43:47,  1.69s/it]11/15/2022 17:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:04 - INFO - train.train_snli_ve - loss is tensor(7.4073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 60/16548 [01:44<7:42:31,  1.68s/it]11/15/2022 17:08:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:05 - INFO - train.train_snli_ve - loss is tensor(7.4190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 61/16548 [01:46<7:40:05,  1.67s/it]11/15/2022 17:08:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:07 - INFO - train.train_snli_ve - loss is tensor(7.2325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 62/16548 [01:48<7:39:27,  1.67s/it]11/15/2022 17:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:09 - INFO - train.train_snli_ve - loss is tensor(7.3009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 63/16548 [01:49<7:37:05,  1.66s/it]11/15/2022 17:08:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:10 - INFO - train.train_snli_ve - loss is tensor(7.3957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 64/16548 [01:51<7:35:09,  1.66s/it]11/15/2022 17:08:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:12 - INFO - train.train_snli_ve - loss is tensor(7.2265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 65/16548 [01:53<7:37:14,  1.66s/it]11/15/2022 17:08:14 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:14 - INFO - train.train_snli_ve - loss is tensor(7.2118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 66/16548 [01:54<7:42:42,  1.68s/it]11/15/2022 17:08:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:15 - INFO - train.train_snli_ve - loss is tensor(7.2694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 67/16548 [01:56<7:40:21,  1.68s/it]11/15/2022 17:08:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:17 - INFO - train.train_snli_ve - loss is tensor(7.2588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 68/16548 [01:58<7:43:06,  1.69s/it]11/15/2022 17:08:19 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:19 - INFO - train.train_snli_ve - loss is tensor(7.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 69/16548 [02:00<7:48:34,  1.71s/it]11/15/2022 17:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:21 - INFO - train.train_snli_ve - loss is tensor(7.1115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 70/16548 [02:01<7:48:27,  1.71s/it]11/15/2022 17:08:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:22 - INFO - train.train_snli_ve - loss is tensor(6.9988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 71/16548 [02:03<7:47:28,  1.70s/it]11/15/2022 17:08:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:24 - INFO - train.train_snli_ve - loss is tensor(7.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 72/16548 [02:05<7:45:31,  1.70s/it]11/15/2022 17:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:26 - INFO - train.train_snli_ve - loss is tensor(7.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 73/16548 [02:06<7:45:13,  1.69s/it]11/15/2022 17:08:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:27 - INFO - train.train_snli_ve - loss is tensor(7.0245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 74/16548 [02:08<7:42:40,  1.69s/it]11/15/2022 17:08:29 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:29 - INFO - train.train_snli_ve - loss is tensor(6.9604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 75/16548 [02:10<7:42:04,  1.68s/it]11/15/2022 17:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:31 - INFO - train.train_snli_ve - loss is tensor(6.8606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 76/16548 [02:11<7:45:12,  1.69s/it]11/15/2022 17:08:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:32 - INFO - train.train_snli_ve - loss is tensor(6.8720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 77/16548 [02:13<7:45:51,  1.70s/it]11/15/2022 17:08:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:34 - INFO - train.train_snli_ve - loss is tensor(6.8231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 78/16548 [02:15<7:47:09,  1.70s/it]11/15/2022 17:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:36 - INFO - train.train_snli_ve - loss is tensor(6.8023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 79/16548 [02:17<7:49:03,  1.71s/it]11/15/2022 17:08:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:37 - INFO - train.train_snli_ve - loss is tensor(6.8596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 80/16548 [02:18<7:45:50,  1.70s/it]11/15/2022 17:08:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:39 - INFO - train.train_snli_ve - loss is tensor(6.8761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 81/16548 [02:20<7:48:05,  1.71s/it]11/15/2022 17:08:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:41 - INFO - train.train_snli_ve - loss is tensor(6.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 82/16548 [02:22<7:52:14,  1.72s/it]11/15/2022 17:08:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:43 - INFO - train.train_snli_ve - loss is tensor(6.6504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 83/16548 [02:23<7:46:41,  1.70s/it]11/15/2022 17:08:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:44 - INFO - train.train_snli_ve - loss is tensor(6.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 84/16548 [02:25<7:44:39,  1.69s/it]11/15/2022 17:08:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:46 - INFO - train.train_snli_ve - loss is tensor(6.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 85/16548 [02:27<7:45:36,  1.70s/it]11/15/2022 17:08:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:48 - INFO - train.train_snli_ve - loss is tensor(6.5589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 86/16548 [02:28<7:45:15,  1.70s/it]11/15/2022 17:08:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:49 - INFO - train.train_snli_ve - loss is tensor(6.4536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 87/16548 [02:30<7:41:28,  1.68s/it]11/15/2022 17:08:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:51 - INFO - train.train_snli_ve - loss is tensor(6.3741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 88/16548 [02:32<7:43:05,  1.69s/it]11/15/2022 17:08:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:53 - INFO - train.train_snli_ve - loss is tensor(6.4727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 89/16548 [02:33<7:42:03,  1.68s/it]11/15/2022 17:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:54 - INFO - train.train_snli_ve - loss is tensor(6.3079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 90/16548 [02:35<7:40:54,  1.68s/it]11/15/2022 17:08:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:56 - INFO - train.train_snli_ve - loss is tensor(6.3509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 91/16548 [02:37<7:42:40,  1.69s/it]11/15/2022 17:08:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:58 - INFO - train.train_snli_ve - loss is tensor(6.3090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 92/16548 [02:38<7:39:15,  1.67s/it]11/15/2022 17:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:08:59 - INFO - train.train_snli_ve - loss is tensor(6.2834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 93/16548 [02:40<7:38:14,  1.67s/it]11/15/2022 17:09:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:01 - INFO - train.train_snli_ve - loss is tensor(6.2230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 94/16548 [02:42<7:37:29,  1.67s/it]11/15/2022 17:09:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:03 - INFO - train.train_snli_ve - loss is tensor(6.3136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 95/16548 [02:43<7:39:15,  1.67s/it]11/15/2022 17:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:04 - INFO - train.train_snli_ve - loss is tensor(6.1527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 96/16548 [02:45<7:40:13,  1.68s/it]11/15/2022 17:09:06 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:06 - INFO - train.train_snli_ve - loss is tensor(6.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 97/16548 [02:47<7:38:41,  1.67s/it]11/15/2022 17:09:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:08 - INFO - train.train_snli_ve - loss is tensor(5.9766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 98/16548 [02:48<7:38:49,  1.67s/it]11/15/2022 17:09:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:09 - INFO - train.train_snli_ve - loss is tensor(6.0135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 99/16548 [02:50<7:36:43,  1.67s/it]11/15/2022 17:09:11 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:11 - INFO - train.train_snli_ve - loss is tensor(5.9565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 100/16548 [02:52<7:44:17,  1.69s/it]11/15/2022 17:09:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:13 - INFO - train.train_snli_ve - loss is tensor(5.8965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 101/16548 [02:54<7:43:54,  1.69s/it]11/15/2022 17:09:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:15 - INFO - train.train_snli_ve - loss is tensor(5.8920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 102/16548 [02:55<7:45:40,  1.70s/it]11/15/2022 17:09:16 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:16 - INFO - train.train_snli_ve - loss is tensor(5.8463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 103/16548 [02:57<7:41:48,  1.68s/it]11/15/2022 17:09:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:18 - INFO - train.train_snli_ve - loss is tensor(5.5274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 104/16548 [02:59<7:39:45,  1.68s/it]11/15/2022 17:09:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:20 - INFO - train.train_snli_ve - loss is tensor(5.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 105/16548 [03:00<7:38:30,  1.67s/it]11/15/2022 17:09:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:21 - INFO - train.train_snli_ve - loss is tensor(5.4848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 106/16548 [03:02<7:40:35,  1.68s/it]11/15/2022 17:09:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:23 - INFO - train.train_snli_ve - loss is tensor(5.6749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 107/16548 [03:04<7:45:44,  1.70s/it]11/15/2022 17:09:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:25 - INFO - train.train_snli_ve - loss is tensor(5.6062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 108/16548 [03:05<7:44:17,  1.69s/it]11/15/2022 17:09:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:26 - INFO - train.train_snli_ve - loss is tensor(5.5024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 109/16548 [03:07<7:43:48,  1.69s/it]11/15/2022 17:09:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:28 - INFO - train.train_snli_ve - loss is tensor(5.4956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 110/16548 [03:09<7:39:41,  1.68s/it]11/15/2022 17:09:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:30 - INFO - train.train_snli_ve - loss is tensor(5.2740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 111/16548 [03:10<7:39:17,  1.68s/it]11/15/2022 17:09:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:31 - INFO - train.train_snli_ve - loss is tensor(5.3542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 112/16548 [03:12<7:40:36,  1.68s/it]11/15/2022 17:09:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:33 - INFO - train.train_snli_ve - loss is tensor(5.3788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 113/16548 [03:14<7:37:18,  1.67s/it]11/15/2022 17:09:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:35 - INFO - train.train_snli_ve - loss is tensor(5.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 114/16548 [03:15<7:39:40,  1.68s/it]11/15/2022 17:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:36 - INFO - train.train_snli_ve - loss is tensor(5.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 115/16548 [03:17<7:44:42,  1.70s/it]11/15/2022 17:09:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:38 - INFO - train.train_snli_ve - loss is tensor(5.0464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 116/16548 [03:19<7:46:17,  1.70s/it]11/15/2022 17:09:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:40 - INFO - train.train_snli_ve - loss is tensor(4.9301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 117/16548 [03:21<7:46:12,  1.70s/it]11/15/2022 17:09:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:42 - INFO - train.train_snli_ve - loss is tensor(5.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 118/16548 [03:22<7:49:10,  1.71s/it]11/15/2022 17:09:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:43 - INFO - train.train_snli_ve - loss is tensor(4.9620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 119/16548 [03:24<7:46:22,  1.70s/it]11/15/2022 17:09:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:45 - INFO - train.train_snli_ve - loss is tensor(4.9592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 120/16548 [03:26<7:47:06,  1.71s/it]11/15/2022 17:09:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:47 - INFO - train.train_snli_ve - loss is tensor(4.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 121/16548 [03:27<7:44:12,  1.70s/it]11/15/2022 17:09:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:48 - INFO - train.train_snli_ve - loss is tensor(4.7001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 122/16548 [03:29<7:41:13,  1.68s/it]11/15/2022 17:09:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:50 - INFO - train.train_snli_ve - loss is tensor(4.7727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 123/16548 [03:31<7:40:34,  1.68s/it]11/15/2022 17:09:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:52 - INFO - train.train_snli_ve - loss is tensor(4.7980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 124/16548 [03:32<7:40:08,  1.68s/it]11/15/2022 17:09:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:53 - INFO - train.train_snli_ve - loss is tensor(4.6557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 125/16548 [03:34<7:41:27,  1.69s/it]11/15/2022 17:09:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:55 - INFO - train.train_snli_ve - loss is tensor(4.5496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 126/16548 [03:36<7:39:42,  1.68s/it]11/15/2022 17:09:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:57 - INFO - train.train_snli_ve - loss is tensor(4.4504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 127/16548 [03:37<7:42:55,  1.69s/it]11/15/2022 17:09:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:09:58 - INFO - train.train_snli_ve - loss is tensor(4.5863, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 128/16548 [03:39<7:41:53,  1.69s/it]11/15/2022 17:10:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:00 - INFO - train.train_snli_ve - loss is tensor(4.4401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 129/16548 [03:41<7:42:37,  1.69s/it]11/15/2022 17:10:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:02 - INFO - train.train_snli_ve - loss is tensor(4.3162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 130/16548 [03:43<7:42:35,  1.69s/it]11/15/2022 17:10:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:03 - INFO - train.train_snli_ve - loss is tensor(4.4602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 131/16548 [03:44<7:40:35,  1.68s/it]11/15/2022 17:10:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:05 - INFO - train.train_snli_ve - loss is tensor(4.3667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 132/16548 [03:46<7:42:51,  1.69s/it]11/15/2022 17:10:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:07 - INFO - train.train_snli_ve - loss is tensor(4.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 133/16548 [03:48<7:40:41,  1.68s/it]11/15/2022 17:10:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:09 - INFO - train.train_snli_ve - loss is tensor(4.1779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 134/16548 [03:49<7:39:06,  1.68s/it]11/15/2022 17:10:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:10 - INFO - train.train_snli_ve - loss is tensor(4.1689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 135/16548 [03:51<7:39:42,  1.68s/it]11/15/2022 17:10:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:12 - INFO - train.train_snli_ve - loss is tensor(4.0023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 136/16548 [03:53<7:39:24,  1.68s/it]11/15/2022 17:10:14 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:14 - INFO - train.train_snli_ve - loss is tensor(4.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 137/16548 [03:54<7:40:25,  1.68s/it]11/15/2022 17:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:15 - INFO - train.train_snli_ve - loss is tensor(3.8845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 138/16548 [03:56<7:37:34,  1.67s/it]11/15/2022 17:10:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:17 - INFO - train.train_snli_ve - loss is tensor(3.9359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 139/16548 [03:58<7:37:09,  1.67s/it]11/15/2022 17:10:19 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:19 - INFO - train.train_snli_ve - loss is tensor(4.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 140/16548 [03:59<7:39:05,  1.68s/it]11/15/2022 17:10:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:20 - INFO - train.train_snli_ve - loss is tensor(3.8974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 141/16548 [04:01<7:40:43,  1.68s/it]11/15/2022 17:10:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:22 - INFO - train.train_snli_ve - loss is tensor(3.9027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 142/16548 [04:03<7:43:23,  1.69s/it]11/15/2022 17:10:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:24 - INFO - train.train_snli_ve - loss is tensor(3.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 143/16548 [04:04<7:39:07,  1.68s/it]11/15/2022 17:10:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:25 - INFO - train.train_snli_ve - loss is tensor(3.7990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 144/16548 [04:06<7:33:31,  1.66s/it]11/15/2022 17:10:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:27 - INFO - train.train_snli_ve - loss is tensor(3.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 145/16548 [04:08<7:34:54,  1.66s/it]11/15/2022 17:10:29 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:29 - INFO - train.train_snli_ve - loss is tensor(3.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 146/16548 [04:09<7:37:29,  1.67s/it]11/15/2022 17:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:30 - INFO - train.train_snli_ve - loss is tensor(3.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 147/16548 [04:11<7:36:56,  1.67s/it]11/15/2022 17:10:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:32 - INFO - train.train_snli_ve - loss is tensor(3.4108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 148/16548 [04:13<7:34:22,  1.66s/it]11/15/2022 17:10:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:34 - INFO - train.train_snli_ve - loss is tensor(3.4967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 149/16548 [04:14<7:35:21,  1.67s/it]11/15/2022 17:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:35 - INFO - train.train_snli_ve - loss is tensor(3.3905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 150/16548 [04:16<7:36:18,  1.67s/it]11/15/2022 17:10:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:37 - INFO - train.train_snli_ve - loss is tensor(3.3379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 151/16548 [04:18<7:39:15,  1.68s/it]11/15/2022 17:10:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:39 - INFO - train.train_snli_ve - loss is tensor(3.3426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 152/16548 [04:19<7:41:16,  1.69s/it]11/15/2022 17:10:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:40 - INFO - train.train_snli_ve - loss is tensor(3.0700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 153/16548 [04:21<7:40:08,  1.68s/it]11/15/2022 17:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:42 - INFO - train.train_snli_ve - loss is tensor(3.1549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 154/16548 [04:23<7:39:15,  1.68s/it]11/15/2022 17:10:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:44 - INFO - train.train_snli_ve - loss is tensor(3.1840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 155/16548 [04:25<7:42:22,  1.69s/it]11/15/2022 17:10:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:45 - INFO - train.train_snli_ve - loss is tensor(3.2824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 156/16548 [04:26<7:42:33,  1.69s/it]11/15/2022 17:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:47 - INFO - train.train_snli_ve - loss is tensor(2.9704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 157/16548 [04:28<7:45:38,  1.70s/it]11/15/2022 17:10:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:49 - INFO - train.train_snli_ve - loss is tensor(3.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 158/16548 [04:30<7:43:21,  1.70s/it]11/15/2022 17:10:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:51 - INFO - train.train_snli_ve - loss is tensor(2.9198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 159/16548 [04:31<7:43:58,  1.70s/it]11/15/2022 17:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:52 - INFO - train.train_snli_ve - loss is tensor(2.9155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 160/16548 [04:33<7:46:13,  1.71s/it]11/15/2022 17:10:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:54 - INFO - train.train_snli_ve - loss is tensor(2.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 161/16548 [04:35<7:43:55,  1.70s/it]11/15/2022 17:10:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:56 - INFO - train.train_snli_ve - loss is tensor(2.9805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 162/16548 [04:36<7:42:04,  1.69s/it]11/15/2022 17:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:57 - INFO - train.train_snli_ve - loss is tensor(2.9028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 163/16548 [04:38<7:41:38,  1.69s/it]11/15/2022 17:10:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:10:59 - INFO - train.train_snli_ve - loss is tensor(2.9209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 164/16548 [04:40<7:43:48,  1.70s/it]11/15/2022 17:11:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:01 - INFO - train.train_snli_ve - loss is tensor(2.7462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 165/16548 [04:41<7:41:25,  1.69s/it]11/15/2022 17:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:02 - INFO - train.train_snli_ve - loss is tensor(2.7918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 166/16548 [04:43<7:40:43,  1.69s/it]11/15/2022 17:11:04 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:04 - INFO - train.train_snli_ve - loss is tensor(2.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 167/16548 [04:45<7:39:36,  1.68s/it]11/15/2022 17:11:06 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:06 - INFO - train.train_snli_ve - loss is tensor(2.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 168/16548 [04:47<7:39:34,  1.68s/it]11/15/2022 17:11:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:07 - INFO - train.train_snli_ve - loss is tensor(2.6208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 169/16548 [04:48<7:39:13,  1.68s/it]11/15/2022 17:11:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:09 - INFO - train.train_snli_ve - loss is tensor(2.6370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 170/16548 [04:50<7:42:46,  1.70s/it]11/15/2022 17:11:11 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:11 - INFO - train.train_snli_ve - loss is tensor(2.6018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 171/16548 [04:52<7:43:02,  1.70s/it]11/15/2022 17:11:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:13 - INFO - train.train_snli_ve - loss is tensor(2.4669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 172/16548 [04:53<7:43:32,  1.70s/it]11/15/2022 17:11:14 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:14 - INFO - train.train_snli_ve - loss is tensor(2.4505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 173/16548 [04:55<7:41:24,  1.69s/it]11/15/2022 17:11:16 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:16 - INFO - train.train_snli_ve - loss is tensor(2.3413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 174/16548 [04:57<7:39:21,  1.68s/it]11/15/2022 17:11:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:18 - INFO - train.train_snli_ve - loss is tensor(2.4737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 175/16548 [04:58<7:40:43,  1.69s/it]11/15/2022 17:11:19 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:19 - INFO - train.train_snli_ve - loss is tensor(2.3849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 176/16548 [05:00<7:40:13,  1.69s/it]11/15/2022 17:11:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:21 - INFO - train.train_snli_ve - loss is tensor(2.4250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 177/16548 [05:02<7:39:37,  1.68s/it]11/15/2022 17:11:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:23 - INFO - train.train_snli_ve - loss is tensor(2.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 178/16548 [05:03<7:45:34,  1.71s/it]11/15/2022 17:11:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:24 - INFO - train.train_snli_ve - loss is tensor(2.4427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 179/16548 [05:05<7:43:42,  1.70s/it]11/15/2022 17:11:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:26 - INFO - train.train_snli_ve - loss is tensor(2.2278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 180/16548 [05:07<7:44:46,  1.70s/it]11/15/2022 17:11:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:28 - INFO - train.train_snli_ve - loss is tensor(2.2382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 181/16548 [05:09<7:46:49,  1.71s/it]11/15/2022 17:11:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:30 - INFO - train.train_snli_ve - loss is tensor(2.2257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 182/16548 [05:10<7:42:40,  1.70s/it]11/15/2022 17:11:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:31 - INFO - train.train_snli_ve - loss is tensor(2.1720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 183/16548 [05:12<7:42:31,  1.70s/it]11/15/2022 17:11:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:33 - INFO - train.train_snli_ve - loss is tensor(2.1292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 184/16548 [05:14<7:40:03,  1.69s/it]11/15/2022 17:11:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:35 - INFO - train.train_snli_ve - loss is tensor(2.0288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 185/16548 [05:15<7:38:12,  1.68s/it]11/15/2022 17:11:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:36 - INFO - train.train_snli_ve - loss is tensor(2.0396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 186/16548 [05:17<7:39:07,  1.68s/it]11/15/2022 17:11:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:38 - INFO - train.train_snli_ve - loss is tensor(2.1251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 187/16548 [05:19<7:39:25,  1.68s/it]11/15/2022 17:11:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:40 - INFO - train.train_snli_ve - loss is tensor(1.9205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 188/16548 [05:20<7:41:14,  1.69s/it]11/15/2022 17:11:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:41 - INFO - train.train_snli_ve - loss is tensor(1.8698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 189/16548 [05:22<7:38:29,  1.68s/it]11/15/2022 17:11:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:43 - INFO - train.train_snli_ve - loss is tensor(2.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 190/16548 [05:24<7:38:51,  1.68s/it]11/15/2022 17:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:45 - INFO - train.train_snli_ve - loss is tensor(1.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 191/16548 [05:25<7:38:36,  1.68s/it]11/15/2022 17:11:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:46 - INFO - train.train_snli_ve - loss is tensor(1.9167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 192/16548 [05:27<7:42:01,  1.69s/it]11/15/2022 17:11:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:48 - INFO - train.train_snli_ve - loss is tensor(1.8618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 193/16548 [05:29<7:40:09,  1.69s/it]11/15/2022 17:11:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:50 - INFO - train.train_snli_ve - loss is tensor(1.8550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 194/16548 [05:30<7:38:43,  1.68s/it]11/15/2022 17:11:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:51 - INFO - train.train_snli_ve - loss is tensor(1.9174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 195/16548 [05:32<7:36:54,  1.68s/it]11/15/2022 17:11:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:53 - INFO - train.train_snli_ve - loss is tensor(1.8541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 196/16548 [05:34<7:35:41,  1.67s/it]11/15/2022 17:11:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:55 - INFO - train.train_snli_ve - loss is tensor(1.8216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 197/16548 [05:35<7:34:43,  1.67s/it]11/15/2022 17:11:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:56 - INFO - train.train_snli_ve - loss is tensor(1.8844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 198/16548 [05:37<7:34:59,  1.67s/it]11/15/2022 17:11:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:11:58 - INFO - train.train_snli_ve - loss is tensor(1.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 199/16548 [05:39<7:34:50,  1.67s/it]11/15/2022 17:12:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:00 - INFO - train.train_snli_ve - loss is tensor(1.7484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 200/16548 [05:41<7:40:53,  1.69s/it]11/15/2022 17:12:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:01 - INFO - train.train_snli_ve - loss is tensor(1.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 201/16548 [05:42<7:38:48,  1.68s/it]11/15/2022 17:12:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:03 - INFO - train.train_snli_ve - loss is tensor(1.7372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 202/16548 [05:44<7:40:51,  1.69s/it]11/15/2022 17:12:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:05 - INFO - train.train_snli_ve - loss is tensor(1.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 203/16548 [05:46<7:41:22,  1.69s/it]11/15/2022 17:12:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:07 - INFO - train.train_snli_ve - loss is tensor(1.7392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 204/16548 [05:47<7:36:38,  1.68s/it]11/15/2022 17:12:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:08 - INFO - train.train_snli_ve - loss is tensor(1.7034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 205/16548 [05:49<7:42:20,  1.70s/it]11/15/2022 17:12:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:10 - INFO - train.train_snli_ve - loss is tensor(1.7283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 206/16548 [05:51<7:44:19,  1.70s/it]11/15/2022 17:12:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:12 - INFO - train.train_snli_ve - loss is tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 207/16548 [05:52<7:40:36,  1.69s/it]11/15/2022 17:12:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:13 - INFO - train.train_snli_ve - loss is tensor(1.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 208/16548 [05:54<7:44:05,  1.70s/it]11/15/2022 17:12:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:15 - INFO - train.train_snli_ve - loss is tensor(1.6125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 209/16548 [05:56<7:42:14,  1.70s/it]11/15/2022 17:12:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:17 - INFO - train.train_snli_ve - loss is tensor(1.6776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 210/16548 [05:57<7:40:52,  1.69s/it]11/15/2022 17:12:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:18 - INFO - train.train_snli_ve - loss is tensor(1.4999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 211/16548 [05:59<7:38:33,  1.68s/it]11/15/2022 17:12:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:20 - INFO - train.train_snli_ve - loss is tensor(1.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 212/16548 [06:01<7:35:29,  1.67s/it]11/15/2022 17:12:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:22 - INFO - train.train_snli_ve - loss is tensor(1.7363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 213/16548 [06:02<7:35:01,  1.67s/it]11/15/2022 17:12:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:23 - INFO - train.train_snli_ve - loss is tensor(1.5443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 214/16548 [06:04<7:33:21,  1.67s/it]11/15/2022 17:12:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:25 - INFO - train.train_snli_ve - loss is tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 215/16548 [06:06<7:33:37,  1.67s/it]11/15/2022 17:12:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:27 - INFO - train.train_snli_ve - loss is tensor(1.4718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 216/16548 [06:07<7:29:37,  1.65s/it]11/15/2022 17:12:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:28 - INFO - train.train_snli_ve - loss is tensor(1.4993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 217/16548 [06:09<7:28:28,  1.65s/it]11/15/2022 17:12:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:30 - INFO - train.train_snli_ve - loss is tensor(1.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 218/16548 [06:11<7:31:10,  1.66s/it]11/15/2022 17:12:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:32 - INFO - train.train_snli_ve - loss is tensor(1.4048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 219/16548 [06:12<7:30:00,  1.65s/it]11/15/2022 17:12:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:33 - INFO - train.train_snli_ve - loss is tensor(1.4064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 220/16548 [06:14<7:30:24,  1.66s/it]11/15/2022 17:12:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:35 - INFO - train.train_snli_ve - loss is tensor(1.5632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 221/16548 [06:16<7:31:36,  1.66s/it]11/15/2022 17:12:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:37 - INFO - train.train_snli_ve - loss is tensor(1.4604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 222/16548 [06:17<7:32:51,  1.66s/it]11/15/2022 17:12:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:38 - INFO - train.train_snli_ve - loss is tensor(1.4614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 223/16548 [06:19<7:27:06,  1.64s/it]11/15/2022 17:12:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:40 - INFO - train.train_snli_ve - loss is tensor(1.3692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 224/16548 [06:21<7:25:42,  1.64s/it]11/15/2022 17:12:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:41 - INFO - train.train_snli_ve - loss is tensor(1.3789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 225/16548 [06:22<7:25:54,  1.64s/it]11/15/2022 17:12:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:43 - INFO - train.train_snli_ve - loss is tensor(1.5535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 226/16548 [06:24<7:26:29,  1.64s/it]11/15/2022 17:12:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:45 - INFO - train.train_snli_ve - loss is tensor(1.4079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 227/16548 [06:26<7:33:18,  1.67s/it]11/15/2022 17:12:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:47 - INFO - train.train_snli_ve - loss is tensor(1.4664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 228/16548 [06:27<7:34:15,  1.67s/it]11/15/2022 17:12:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:48 - INFO - train.train_snli_ve - loss is tensor(1.4327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 229/16548 [06:29<7:37:44,  1.68s/it]11/15/2022 17:12:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:50 - INFO - train.train_snli_ve - loss is tensor(1.3013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 230/16548 [06:31<7:37:11,  1.68s/it]11/15/2022 17:12:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:52 - INFO - train.train_snli_ve - loss is tensor(1.3458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 231/16548 [06:32<7:36:03,  1.68s/it]11/15/2022 17:12:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:53 - INFO - train.train_snli_ve - loss is tensor(1.4775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 232/16548 [06:34<7:37:50,  1.68s/it]11/15/2022 17:12:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:55 - INFO - train.train_snli_ve - loss is tensor(1.2956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 233/16548 [06:36<7:35:36,  1.68s/it]11/15/2022 17:12:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:57 - INFO - train.train_snli_ve - loss is tensor(1.2984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 234/16548 [06:37<7:38:03,  1.68s/it]11/15/2022 17:12:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:12:58 - INFO - train.train_snli_ve - loss is tensor(1.4931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 235/16548 [06:39<7:33:00,  1.67s/it]11/15/2022 17:13:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:00 - INFO - train.train_snli_ve - loss is tensor(1.2968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 236/16548 [06:41<7:31:51,  1.66s/it]11/15/2022 17:13:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:02 - INFO - train.train_snli_ve - loss is tensor(1.4080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 237/16548 [06:42<7:35:53,  1.68s/it]11/15/2022 17:13:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:03 - INFO - train.train_snli_ve - loss is tensor(1.4484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 238/16548 [06:44<7:39:05,  1.69s/it]11/15/2022 17:13:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:05 - INFO - train.train_snli_ve - loss is tensor(1.3041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 239/16548 [06:46<7:36:17,  1.68s/it]11/15/2022 17:13:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:07 - INFO - train.train_snli_ve - loss is tensor(1.3476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 240/16548 [06:47<7:30:50,  1.66s/it]11/15/2022 17:13:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:08 - INFO - train.train_snli_ve - loss is tensor(1.3001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 241/16548 [06:49<7:28:11,  1.65s/it]11/15/2022 17:13:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:10 - INFO - train.train_snli_ve - loss is tensor(1.5804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 242/16548 [06:51<7:29:33,  1.65s/it]11/15/2022 17:13:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:12 - INFO - train.train_snli_ve - loss is tensor(1.3712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 243/16548 [06:52<7:24:53,  1.64s/it]11/15/2022 17:13:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:13 - INFO - train.train_snli_ve - loss is tensor(1.3134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 244/16548 [06:54<7:24:43,  1.64s/it]11/15/2022 17:13:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:15 - INFO - train.train_snli_ve - loss is tensor(1.2604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 245/16548 [06:56<7:24:30,  1.64s/it]11/15/2022 17:13:16 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:16 - INFO - train.train_snli_ve - loss is tensor(1.2815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 246/16548 [06:57<7:20:36,  1.62s/it]11/15/2022 17:13:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:18 - INFO - train.train_snli_ve - loss is tensor(1.3284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 247/16548 [06:59<7:19:31,  1.62s/it]11/15/2022 17:13:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:20 - INFO - train.train_snli_ve - loss is tensor(1.1948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1% 248/16548 [07:00<7:19:03,  1.62s/it]11/15/2022 17:13:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:21 - INFO - train.train_snli_ve - loss is tensor(1.2490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 249/16548 [07:02<7:20:00,  1.62s/it]11/15/2022 17:13:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:23 - INFO - train.train_snli_ve - loss is tensor(1.1888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 250/16548 [07:04<7:24:08,  1.64s/it]11/15/2022 17:13:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:24 - INFO - train.train_snli_ve - loss is tensor(1.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 251/16548 [07:05<7:21:43,  1.63s/it]11/15/2022 17:13:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:26 - INFO - train.train_snli_ve - loss is tensor(1.2066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 252/16548 [07:07<7:17:56,  1.61s/it]11/15/2022 17:13:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:28 - INFO - train.train_snli_ve - loss is tensor(1.1137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 253/16548 [07:08<7:18:33,  1.61s/it]11/15/2022 17:13:29 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:29 - INFO - train.train_snli_ve - loss is tensor(1.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 254/16548 [07:10<7:18:42,  1.62s/it]11/15/2022 17:13:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:31 - INFO - train.train_snli_ve - loss is tensor(1.3299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 255/16548 [07:12<7:20:25,  1.62s/it]11/15/2022 17:13:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:33 - INFO - train.train_snli_ve - loss is tensor(1.2336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 256/16548 [07:13<7:23:53,  1.63s/it]11/15/2022 17:13:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:34 - INFO - train.train_snli_ve - loss is tensor(1.3151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 257/16548 [07:15<7:23:17,  1.63s/it]11/15/2022 17:13:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:36 - INFO - train.train_snli_ve - loss is tensor(1.2052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 258/16548 [07:17<7:23:38,  1.63s/it]11/15/2022 17:13:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:38 - INFO - train.train_snli_ve - loss is tensor(1.2212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 259/16548 [07:18<7:24:27,  1.64s/it]11/15/2022 17:13:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:39 - INFO - train.train_snli_ve - loss is tensor(1.2451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 260/16548 [07:20<7:25:28,  1.64s/it]11/15/2022 17:13:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:41 - INFO - train.train_snli_ve - loss is tensor(1.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 261/16548 [07:22<7:26:38,  1.65s/it]11/15/2022 17:13:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:43 - INFO - train.train_snli_ve - loss is tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 262/16548 [07:23<7:30:51,  1.66s/it]11/15/2022 17:13:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:44 - INFO - train.train_snli_ve - loss is tensor(1.1554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 263/16548 [07:25<7:29:44,  1.66s/it]11/15/2022 17:13:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:46 - INFO - train.train_snli_ve - loss is tensor(1.3343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 264/16548 [07:27<7:26:24,  1.64s/it]11/15/2022 17:13:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:47 - INFO - train.train_snli_ve - loss is tensor(1.2230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 265/16548 [07:28<7:30:30,  1.66s/it]11/15/2022 17:13:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:49 - INFO - train.train_snli_ve - loss is tensor(1.1677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 266/16548 [07:30<7:29:05,  1.65s/it]11/15/2022 17:13:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:51 - INFO - train.train_snli_ve - loss is tensor(1.1847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 267/16548 [07:31<7:24:44,  1.64s/it]11/15/2022 17:13:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:52 - INFO - train.train_snli_ve - loss is tensor(1.3196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 268/16548 [07:33<7:22:22,  1.63s/it]11/15/2022 17:13:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:54 - INFO - train.train_snli_ve - loss is tensor(1.2770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 269/16548 [07:35<7:22:42,  1.63s/it]11/15/2022 17:13:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:56 - INFO - train.train_snli_ve - loss is tensor(1.1075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 270/16548 [07:36<7:25:16,  1.64s/it]11/15/2022 17:13:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:57 - INFO - train.train_snli_ve - loss is tensor(1.1783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 271/16548 [07:38<7:25:27,  1.64s/it]11/15/2022 17:13:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:13:59 - INFO - train.train_snli_ve - loss is tensor(1.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 272/16548 [07:40<7:28:15,  1.65s/it]11/15/2022 17:14:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:01 - INFO - train.train_snli_ve - loss is tensor(1.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 273/16548 [07:41<7:27:18,  1.65s/it]11/15/2022 17:14:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:02 - INFO - train.train_snli_ve - loss is tensor(1.2414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 274/16548 [07:43<7:25:08,  1.64s/it]11/15/2022 17:14:04 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:04 - INFO - train.train_snli_ve - loss is tensor(1.2682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 275/16548 [07:45<7:23:39,  1.64s/it]11/15/2022 17:14:06 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:06 - INFO - train.train_snli_ve - loss is tensor(1.0483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 276/16548 [07:46<7:25:22,  1.64s/it]11/15/2022 17:14:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:07 - INFO - train.train_snli_ve - loss is tensor(1.0176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 277/16548 [07:48<7:25:31,  1.64s/it]11/15/2022 17:14:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:09 - INFO - train.train_snli_ve - loss is tensor(1.0046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 278/16548 [07:50<7:24:12,  1.64s/it]11/15/2022 17:14:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:10 - INFO - train.train_snli_ve - loss is tensor(0.9966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 279/16548 [07:51<7:20:36,  1.62s/it]11/15/2022 17:14:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:12 - INFO - train.train_snli_ve - loss is tensor(1.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 280/16548 [07:53<7:20:02,  1.62s/it]11/15/2022 17:14:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.9692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:14 - INFO - train.train_snli_ve - loss is tensor(1.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 281/16548 [07:54<7:22:46,  1.63s/it]11/15/2022 17:14:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:15 - INFO - train.train_snli_ve - loss is tensor(1.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 282/16548 [07:56<7:21:12,  1.63s/it]11/15/2022 17:14:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.7631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:17 - INFO - train.train_snli_ve - loss is tensor(1.2539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 283/16548 [07:58<7:21:14,  1.63s/it]11/15/2022 17:14:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.7879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:19 - INFO - train.train_snli_ve - loss is tensor(0.9527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 284/16548 [07:59<7:23:29,  1.64s/it]11/15/2022 17:14:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.8108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:20 - INFO - train.train_snli_ve - loss is tensor(1.3537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 285/16548 [08:01<7:22:30,  1.63s/it]11/15/2022 17:14:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.6513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:22 - INFO - train.train_snli_ve - loss is tensor(1.1728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 286/16548 [08:03<7:22:31,  1.63s/it]11/15/2022 17:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.2861e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:23 - INFO - train.train_snli_ve - loss is tensor(1.2264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 287/16548 [08:04<7:23:52,  1.64s/it]11/15/2022 17:14:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.1296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:25 - INFO - train.train_snli_ve - loss is tensor(1.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 288/16548 [08:06<7:23:52,  1.64s/it]11/15/2022 17:14:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.2623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:27 - INFO - train.train_snli_ve - loss is tensor(1.1574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 289/16548 [08:08<7:27:16,  1.65s/it]11/15/2022 17:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.1590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:28 - INFO - train.train_snli_ve - loss is tensor(1.1748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 290/16548 [08:09<7:25:12,  1.64s/it]11/15/2022 17:14:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.0417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:30 - INFO - train.train_snli_ve - loss is tensor(1.1740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 291/16548 [08:11<7:25:37,  1.64s/it]11/15/2022 17:14:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.8601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:32 - INFO - train.train_snli_ve - loss is tensor(1.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 292/16548 [08:12<7:20:56,  1.63s/it]11/15/2022 17:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.2862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:33 - INFO - train.train_snli_ve - loss is tensor(0.9886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 293/16548 [08:14<7:23:51,  1.64s/it]11/15/2022 17:14:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.9189e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:35 - INFO - train.train_snli_ve - loss is tensor(1.1724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 294/16548 [08:16<7:21:05,  1.63s/it]11/15/2022 17:14:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.9379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:37 - INFO - train.train_snli_ve - loss is tensor(1.0417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 295/16548 [08:17<7:19:07,  1.62s/it]11/15/2022 17:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.6322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:38 - INFO - train.train_snli_ve - loss is tensor(1.0963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 296/16548 [08:19<7:21:30,  1.63s/it]11/15/2022 17:14:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.6398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:40 - INFO - train.train_snli_ve - loss is tensor(1.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 297/16548 [08:21<7:24:41,  1.64s/it]11/15/2022 17:14:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.4192e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:41 - INFO - train.train_snli_ve - loss is tensor(1.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 298/16548 [08:22<7:21:47,  1.63s/it]11/15/2022 17:14:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.4487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:43 - INFO - train.train_snli_ve - loss is tensor(0.9002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 299/16548 [08:24<7:21:32,  1.63s/it]11/15/2022 17:14:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.1826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:45 - INFO - train.train_snli_ve - loss is tensor(1.2222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 300/16548 [08:25<7:26:52,  1.65s/it]11/15/2022 17:14:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.1096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:46 - INFO - train.train_snli_ve - loss is tensor(1.0370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 301/16548 [08:27<7:25:50,  1.65s/it]11/15/2022 17:14:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.1046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:48 - INFO - train.train_snli_ve - loss is tensor(1.1839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 302/16548 [08:29<7:25:22,  1.64s/it]11/15/2022 17:14:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.9341e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:50 - INFO - train.train_snli_ve - loss is tensor(1.0043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 303/16548 [08:30<7:24:27,  1.64s/it]11/15/2022 17:14:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.8131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:51 - INFO - train.train_snli_ve - loss is tensor(1.1277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 304/16548 [08:32<7:27:05,  1.65s/it]11/15/2022 17:14:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.9077e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:53 - INFO - train.train_snli_ve - loss is tensor(0.9424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 305/16548 [08:34<7:26:28,  1.65s/it]11/15/2022 17:14:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.9301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:55 - INFO - train.train_snli_ve - loss is tensor(1.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 306/16548 [08:35<7:27:26,  1.65s/it]11/15/2022 17:14:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.7799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:56 - INFO - train.train_snli_ve - loss is tensor(0.9443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 307/16548 [08:37<7:26:07,  1.65s/it]11/15/2022 17:14:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.6300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:14:58 - INFO - train.train_snli_ve - loss is tensor(1.0196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 308/16548 [08:39<7:23:11,  1.64s/it]11/15/2022 17:15:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.6348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:00 - INFO - train.train_snli_ve - loss is tensor(1.1683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 309/16548 [08:40<7:21:33,  1.63s/it]11/15/2022 17:15:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.6601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:01 - INFO - train.train_snli_ve - loss is tensor(1.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 310/16548 [08:42<7:18:49,  1.62s/it]11/15/2022 17:15:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.1330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:03 - INFO - train.train_snli_ve - loss is tensor(1.1254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 311/16548 [08:44<7:21:20,  1.63s/it]11/15/2022 17:15:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.3520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:04 - INFO - train.train_snli_ve - loss is tensor(0.9329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 312/16548 [08:45<7:23:00,  1.64s/it]11/15/2022 17:15:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.2072e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:06 - INFO - train.train_snli_ve - loss is tensor(1.1709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 313/16548 [08:47<7:21:05,  1.63s/it]11/15/2022 17:15:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.0014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:08 - INFO - train.train_snli_ve - loss is tensor(1.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 314/16548 [08:48<7:18:36,  1.62s/it]11/15/2022 17:15:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.0371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:09 - INFO - train.train_snli_ve - loss is tensor(0.9030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 315/16548 [08:50<7:18:13,  1.62s/it]11/15/2022 17:15:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.2182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:11 - INFO - train.train_snli_ve - loss is tensor(0.9656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 316/16548 [08:52<7:19:30,  1.62s/it]11/15/2022 17:15:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.9324e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:13 - INFO - train.train_snli_ve - loss is tensor(0.8629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 317/16548 [08:53<7:19:10,  1.62s/it]11/15/2022 17:15:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.8458e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:14 - INFO - train.train_snli_ve - loss is tensor(1.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 318/16548 [08:55<7:18:29,  1.62s/it]11/15/2022 17:15:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.7913e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:16 - INFO - train.train_snli_ve - loss is tensor(1.1032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 319/16548 [08:57<7:20:09,  1.63s/it]11/15/2022 17:15:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.7413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:17 - INFO - train.train_snli_ve - loss is tensor(0.9089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 320/16548 [08:58<7:21:30,  1.63s/it]11/15/2022 17:15:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.8795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:19 - INFO - train.train_snli_ve - loss is tensor(0.9916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 321/16548 [09:00<7:20:35,  1.63s/it]11/15/2022 17:15:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.5628e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:21 - INFO - train.train_snli_ve - loss is tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 322/16548 [09:01<7:24:38,  1.64s/it]11/15/2022 17:15:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.3644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:22 - INFO - train.train_snli_ve - loss is tensor(1.0237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 323/16548 [09:03<7:25:10,  1.65s/it]11/15/2022 17:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.3800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:24 - INFO - train.train_snli_ve - loss is tensor(1.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 324/16548 [09:05<7:23:20,  1.64s/it]11/15/2022 17:15:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.3421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:26 - INFO - train.train_snli_ve - loss is tensor(0.9912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 325/16548 [09:06<7:22:18,  1.64s/it]11/15/2022 17:15:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.1738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:27 - INFO - train.train_snli_ve - loss is tensor(1.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 326/16548 [09:08<7:21:32,  1.63s/it]11/15/2022 17:15:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.1059e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:29 - INFO - train.train_snli_ve - loss is tensor(1.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 327/16548 [09:10<7:25:02,  1.65s/it]11/15/2022 17:15:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.1008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:31 - INFO - train.train_snli_ve - loss is tensor(0.8299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 328/16548 [09:11<7:25:30,  1.65s/it]11/15/2022 17:15:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.0302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:32 - INFO - train.train_snli_ve - loss is tensor(1.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 329/16548 [09:13<7:26:41,  1.65s/it]11/15/2022 17:15:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.9465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:34 - INFO - train.train_snli_ve - loss is tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 330/16548 [09:15<7:27:29,  1.66s/it]11/15/2022 17:15:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.7632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:36 - INFO - train.train_snli_ve - loss is tensor(1.4523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 331/16548 [09:16<7:24:30,  1.64s/it]11/15/2022 17:15:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.7479e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:37 - INFO - train.train_snli_ve - loss is tensor(1.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 332/16548 [09:18<7:22:44,  1.64s/it]11/15/2022 17:15:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.7437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:39 - INFO - train.train_snli_ve - loss is tensor(0.9660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 333/16548 [09:19<7:21:20,  1.63s/it]11/15/2022 17:15:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.7246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:40 - INFO - train.train_snli_ve - loss is tensor(1.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 334/16548 [09:21<7:19:41,  1.63s/it]11/15/2022 17:15:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.5834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:42 - INFO - train.train_snli_ve - loss is tensor(1.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 335/16548 [09:23<7:18:31,  1.62s/it]11/15/2022 17:15:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.5663e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:44 - INFO - train.train_snli_ve - loss is tensor(1.3379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 336/16548 [09:24<7:19:22,  1.63s/it]11/15/2022 17:15:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.5183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:45 - INFO - train.train_snli_ve - loss is tensor(0.9406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 337/16548 [09:26<7:20:41,  1.63s/it]11/15/2022 17:15:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.3605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:47 - INFO - train.train_snli_ve - loss is tensor(0.9781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 338/16548 [09:28<7:20:46,  1.63s/it]11/15/2022 17:15:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.2805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:49 - INFO - train.train_snli_ve - loss is tensor(0.9485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 339/16548 [09:29<7:22:31,  1.64s/it]11/15/2022 17:15:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.2593e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:50 - INFO - train.train_snli_ve - loss is tensor(0.8563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 340/16548 [09:31<7:19:23,  1.63s/it]11/15/2022 17:15:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.1938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:52 - INFO - train.train_snli_ve - loss is tensor(0.9965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 341/16548 [09:33<7:25:44,  1.65s/it]11/15/2022 17:15:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.0746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:53 - INFO - train.train_snli_ve - loss is tensor(0.9763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 342/16548 [09:34<7:24:35,  1.65s/it]11/15/2022 17:15:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.0377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:55 - INFO - train.train_snli_ve - loss is tensor(0.9429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 343/16548 [09:36<7:25:20,  1.65s/it]11/15/2022 17:15:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.0127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:57 - INFO - train.train_snli_ve - loss is tensor(1.0241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 344/16548 [09:37<7:22:08,  1.64s/it]11/15/2022 17:15:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.0341e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:15:58 - INFO - train.train_snli_ve - loss is tensor(0.9367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 345/16548 [09:39<7:21:10,  1.63s/it]11/15/2022 17:16:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.8330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:00 - INFO - train.train_snli_ve - loss is tensor(1.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 346/16548 [09:41<7:19:49,  1.63s/it]11/15/2022 17:16:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.8196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:02 - INFO - train.train_snli_ve - loss is tensor(0.9536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 347/16548 [09:42<7:19:47,  1.63s/it]11/15/2022 17:16:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.7151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:03 - INFO - train.train_snli_ve - loss is tensor(1.0566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 348/16548 [09:44<7:19:47,  1.63s/it]11/15/2022 17:16:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.8042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:05 - INFO - train.train_snli_ve - loss is tensor(0.9786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 349/16548 [09:46<7:17:54,  1.62s/it]11/15/2022 17:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.6758e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:06 - INFO - train.train_snli_ve - loss is tensor(1.1330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 350/16548 [09:47<7:16:42,  1.62s/it]11/15/2022 17:16:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.6177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:08 - INFO - train.train_snli_ve - loss is tensor(0.9272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 351/16548 [09:49<7:16:17,  1.62s/it]11/15/2022 17:16:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.6045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:10 - INFO - train.train_snli_ve - loss is tensor(0.8993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 352/16548 [09:50<7:18:54,  1.63s/it]11/15/2022 17:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.5308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:11 - INFO - train.train_snli_ve - loss is tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 353/16548 [09:52<7:22:28,  1.64s/it]11/15/2022 17:16:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.4761e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:13 - INFO - train.train_snli_ve - loss is tensor(1.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 354/16548 [09:54<7:22:15,  1.64s/it]11/15/2022 17:16:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.4456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:15 - INFO - train.train_snli_ve - loss is tensor(0.9072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 355/16548 [09:55<7:19:37,  1.63s/it]11/15/2022 17:16:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.3968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:16 - INFO - train.train_snli_ve - loss is tensor(0.9438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 356/16548 [09:57<7:21:45,  1.64s/it]11/15/2022 17:16:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.3108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:18 - INFO - train.train_snli_ve - loss is tensor(0.9200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 357/16548 [09:59<7:21:18,  1.64s/it]11/15/2022 17:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.2827e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:20 - INFO - train.train_snli_ve - loss is tensor(0.9962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 358/16548 [10:00<7:20:52,  1.63s/it]11/15/2022 17:16:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.3161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:21 - INFO - train.train_snli_ve - loss is tensor(1.2451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 359/16548 [10:02<7:20:04,  1.63s/it]11/15/2022 17:16:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.1935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:23 - INFO - train.train_snli_ve - loss is tensor(1.1365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 360/16548 [10:04<7:18:12,  1.62s/it]11/15/2022 17:16:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.1282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:24 - INFO - train.train_snli_ve - loss is tensor(0.8850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 361/16548 [10:05<7:20:12,  1.63s/it]11/15/2022 17:16:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.1162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:26 - INFO - train.train_snli_ve - loss is tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 362/16548 [10:07<7:20:12,  1.63s/it]11/15/2022 17:16:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.1015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:28 - INFO - train.train_snli_ve - loss is tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 363/16548 [10:08<7:22:26,  1.64s/it]11/15/2022 17:16:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.0539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:29 - INFO - train.train_snli_ve - loss is tensor(1.0330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 364/16548 [10:10<7:22:50,  1.64s/it]11/15/2022 17:16:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.9630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:31 - INFO - train.train_snli_ve - loss is tensor(1.0757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 365/16548 [10:12<7:24:26,  1.65s/it]11/15/2022 17:16:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.9478e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:33 - INFO - train.train_snli_ve - loss is tensor(0.8923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 366/16548 [10:13<7:22:12,  1.64s/it]11/15/2022 17:16:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.9164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:34 - INFO - train.train_snli_ve - loss is tensor(0.9979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 367/16548 [10:15<7:21:40,  1.64s/it]11/15/2022 17:16:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.8572e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:36 - INFO - train.train_snli_ve - loss is tensor(0.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 368/16548 [10:17<7:23:05,  1.64s/it]11/15/2022 17:16:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.8067e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:38 - INFO - train.train_snli_ve - loss is tensor(1.0057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 369/16548 [10:18<7:20:41,  1.63s/it]11/15/2022 17:16:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.7680e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:39 - INFO - train.train_snli_ve - loss is tensor(1.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 370/16548 [10:20<7:22:39,  1.64s/it]11/15/2022 17:16:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.7632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:41 - INFO - train.train_snli_ve - loss is tensor(0.9229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 371/16548 [10:22<7:24:07,  1.65s/it]11/15/2022 17:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.7308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:42 - INFO - train.train_snli_ve - loss is tensor(1.0509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 372/16548 [10:23<7:19:32,  1.63s/it]11/15/2022 17:16:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.6410e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:44 - INFO - train.train_snli_ve - loss is tensor(0.9005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 373/16548 [10:25<7:16:59,  1.62s/it]11/15/2022 17:16:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.6622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:46 - INFO - train.train_snli_ve - loss is tensor(0.9143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 374/16548 [10:26<7:19:35,  1.63s/it]11/15/2022 17:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.5878e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:47 - INFO - train.train_snli_ve - loss is tensor(1.0589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 375/16548 [10:28<7:20:05,  1.63s/it]11/15/2022 17:16:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.5471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:49 - INFO - train.train_snli_ve - loss is tensor(0.9089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 376/16548 [10:30<7:28:13,  1.66s/it]11/15/2022 17:16:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.5222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:51 - INFO - train.train_snli_ve - loss is tensor(1.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 377/16548 [10:31<7:25:53,  1.65s/it]11/15/2022 17:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.5202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:52 - INFO - train.train_snli_ve - loss is tensor(1.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 378/16548 [10:33<7:24:39,  1.65s/it]11/15/2022 17:16:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.4608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:54 - INFO - train.train_snli_ve - loss is tensor(0.9493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 379/16548 [10:35<7:23:04,  1.64s/it]11/15/2022 17:16:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.4633e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:56 - INFO - train.train_snli_ve - loss is tensor(1.0139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 380/16548 [10:36<7:21:38,  1.64s/it]11/15/2022 17:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.3723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:57 - INFO - train.train_snli_ve - loss is tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 381/16548 [10:38<7:19:55,  1.63s/it]11/15/2022 17:16:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:16:59 - INFO - train.train_snli_ve - loss is tensor(1.0498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 382/16548 [10:40<7:17:52,  1.63s/it]11/15/2022 17:17:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.3102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:01 - INFO - train.train_snli_ve - loss is tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 383/16548 [10:41<7:21:10,  1.64s/it]11/15/2022 17:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.2933e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:02 - INFO - train.train_snli_ve - loss is tensor(0.9857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 384/16548 [10:43<7:19:22,  1.63s/it]11/15/2022 17:17:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.2741e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:04 - INFO - train.train_snli_ve - loss is tensor(0.8325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 385/16548 [10:45<7:19:47,  1.63s/it]11/15/2022 17:17:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.2218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:05 - INFO - train.train_snli_ve - loss is tensor(0.9294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 386/16548 [10:46<7:21:55,  1.64s/it]11/15/2022 17:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.1860e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:07 - INFO - train.train_snli_ve - loss is tensor(0.9520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 387/16548 [10:48<7:22:16,  1.64s/it]11/15/2022 17:17:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.1736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:09 - INFO - train.train_snli_ve - loss is tensor(1.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 388/16548 [10:49<7:24:39,  1.65s/it]11/15/2022 17:17:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.1524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:10 - INFO - train.train_snli_ve - loss is tensor(1.0704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 389/16548 [10:51<7:25:52,  1.66s/it]11/15/2022 17:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.0977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:12 - INFO - train.train_snli_ve - loss is tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 390/16548 [10:53<7:25:43,  1.66s/it]11/15/2022 17:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.0957e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:14 - INFO - train.train_snli_ve - loss is tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 391/16548 [10:54<7:23:02,  1.65s/it]11/15/2022 17:17:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.0939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:15 - INFO - train.train_snli_ve - loss is tensor(0.9428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 392/16548 [10:56<7:24:41,  1.65s/it]11/15/2022 17:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.0437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:17 - INFO - train.train_snli_ve - loss is tensor(0.8336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 393/16548 [10:58<7:23:56,  1.65s/it]11/15/2022 17:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.0094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:19 - INFO - train.train_snli_ve - loss is tensor(0.9761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 394/16548 [10:59<7:21:03,  1.64s/it]11/15/2022 17:17:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.9525e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:20 - INFO - train.train_snli_ve - loss is tensor(0.9350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 395/16548 [11:01<7:19:10,  1.63s/it]11/15/2022 17:17:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.9565e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:22 - INFO - train.train_snli_ve - loss is tensor(1.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 396/16548 [11:03<7:18:46,  1.63s/it]11/15/2022 17:17:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.9078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:23 - INFO - train.train_snli_ve - loss is tensor(1.1282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 397/16548 [11:04<7:19:08,  1.63s/it]11/15/2022 17:17:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.8877e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:25 - INFO - train.train_snli_ve - loss is tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 398/16548 [11:06<7:12:24,  1.61s/it]11/15/2022 17:17:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.8494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:27 - INFO - train.train_snli_ve - loss is tensor(0.8798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 399/16548 [11:07<7:16:28,  1.62s/it]11/15/2022 17:17:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:28 - INFO - train.train_snli_ve - loss is tensor(1.0964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 400/16548 [11:09<7:21:16,  1.64s/it]11/15/2022 17:17:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:30 - INFO - train.train_snli_ve - loss is tensor(1.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 401/16548 [11:11<7:21:27,  1.64s/it]11/15/2022 17:17:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8270e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:32 - INFO - train.train_snli_ve - loss is tensor(1.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 402/16548 [11:12<7:27:47,  1.66s/it]11/15/2022 17:17:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:33 - INFO - train.train_snli_ve - loss is tensor(0.8957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 403/16548 [11:14<7:28:51,  1.67s/it]11/15/2022 17:17:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.7666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:35 - INFO - train.train_snli_ve - loss is tensor(0.8877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 404/16548 [11:16<7:30:58,  1.68s/it]11/15/2022 17:17:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7458e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:37 - INFO - train.train_snli_ve - loss is tensor(1.0160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 405/16548 [11:18<7:29:47,  1.67s/it]11/15/2022 17:17:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.7287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:38 - INFO - train.train_snli_ve - loss is tensor(1.1107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 406/16548 [11:19<7:27:59,  1.67s/it]11/15/2022 17:17:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.6820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:40 - INFO - train.train_snli_ve - loss is tensor(0.9092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 407/16548 [11:21<7:27:38,  1.66s/it]11/15/2022 17:17:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.6557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:42 - INFO - train.train_snli_ve - loss is tensor(1.1757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 408/16548 [11:23<7:29:29,  1.67s/it]11/15/2022 17:17:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:43 - INFO - train.train_snli_ve - loss is tensor(1.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 409/16548 [11:24<7:26:07,  1.66s/it]11/15/2022 17:17:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.6531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:45 - INFO - train.train_snli_ve - loss is tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 410/16548 [11:26<7:22:13,  1.64s/it]11/15/2022 17:17:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:47 - INFO - train.train_snli_ve - loss is tensor(0.9170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 411/16548 [11:27<7:20:44,  1.64s/it]11/15/2022 17:17:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:48 - INFO - train.train_snli_ve - loss is tensor(0.8681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 412/16548 [11:29<7:20:35,  1.64s/it]11/15/2022 17:17:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:50 - INFO - train.train_snli_ve - loss is tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2% 413/16548 [11:31<7:22:19,  1.64s/it]11/15/2022 17:17:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.5743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:52 - INFO - train.train_snli_ve - loss is tensor(0.9021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 414/16548 [11:32<7:23:37,  1.65s/it]11/15/2022 17:17:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.5402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:53 - INFO - train.train_snli_ve - loss is tensor(0.9942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 415/16548 [11:34<7:20:23,  1.64s/it]11/15/2022 17:17:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.5165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:55 - INFO - train.train_snli_ve - loss is tensor(0.8871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 416/16548 [11:36<7:19:31,  1.63s/it]11/15/2022 17:17:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.5916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:56 - INFO - train.train_snli_ve - loss is tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 417/16548 [11:37<7:14:54,  1.62s/it]11/15/2022 17:17:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:17:58 - INFO - train.train_snli_ve - loss is tensor(1.0705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 418/16548 [11:39<7:17:43,  1.63s/it]11/15/2022 17:18:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:00 - INFO - train.train_snli_ve - loss is tensor(1.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 419/16548 [11:40<7:16:06,  1.62s/it]11/15/2022 17:18:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:01 - INFO - train.train_snli_ve - loss is tensor(0.8699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 420/16548 [11:42<7:17:39,  1.63s/it]11/15/2022 17:18:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:03 - INFO - train.train_snli_ve - loss is tensor(0.9347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 421/16548 [11:44<7:18:56,  1.63s/it]11/15/2022 17:18:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.4198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:05 - INFO - train.train_snli_ve - loss is tensor(0.9552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 422/16548 [11:45<7:20:48,  1.64s/it]11/15/2022 17:18:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:06 - INFO - train.train_snli_ve - loss is tensor(0.9331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 423/16548 [11:47<7:19:04,  1.63s/it]11/15/2022 17:18:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:08 - INFO - train.train_snli_ve - loss is tensor(1.1572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 424/16548 [11:49<7:18:04,  1.63s/it]11/15/2022 17:18:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.3683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:10 - INFO - train.train_snli_ve - loss is tensor(0.8219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 425/16548 [11:50<7:21:44,  1.64s/it]11/15/2022 17:18:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:11 - INFO - train.train_snli_ve - loss is tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 426/16548 [11:52<7:22:27,  1.65s/it]11/15/2022 17:18:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:13 - INFO - train.train_snli_ve - loss is tensor(0.8011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 427/16548 [11:54<7:19:12,  1.63s/it]11/15/2022 17:18:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:14 - INFO - train.train_snli_ve - loss is tensor(0.9580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 428/16548 [11:55<7:17:12,  1.63s/it]11/15/2022 17:18:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:16 - INFO - train.train_snli_ve - loss is tensor(0.9493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 429/16548 [11:57<7:19:40,  1.64s/it]11/15/2022 17:18:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:18 - INFO - train.train_snli_ve - loss is tensor(0.9083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 430/16548 [11:58<7:20:42,  1.64s/it]11/15/2022 17:18:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:19 - INFO - train.train_snli_ve - loss is tensor(0.8687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 431/16548 [12:00<7:18:24,  1.63s/it]11/15/2022 17:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2213e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:21 - INFO - train.train_snli_ve - loss is tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 432/16548 [12:02<7:16:10,  1.62s/it]11/15/2022 17:18:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:23 - INFO - train.train_snli_ve - loss is tensor(0.9077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 433/16548 [12:03<7:18:27,  1.63s/it]11/15/2022 17:18:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:24 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 434/16548 [12:05<7:14:43,  1.62s/it]11/15/2022 17:18:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:26 - INFO - train.train_snli_ve - loss is tensor(0.7931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 435/16548 [12:07<7:17:01,  1.63s/it]11/15/2022 17:18:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.1624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:27 - INFO - train.train_snli_ve - loss is tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 436/16548 [12:08<7:22:04,  1.65s/it]11/15/2022 17:18:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.1363e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:29 - INFO - train.train_snli_ve - loss is tensor(0.9701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 437/16548 [12:10<7:20:56,  1.64s/it]11/15/2022 17:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:31 - INFO - train.train_snli_ve - loss is tensor(0.9310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 438/16548 [12:12<7:20:00,  1.64s/it]11/15/2022 17:18:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:32 - INFO - train.train_snli_ve - loss is tensor(0.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 439/16548 [12:13<7:19:23,  1.64s/it]11/15/2022 17:18:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:34 - INFO - train.train_snli_ve - loss is tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 440/16548 [12:15<7:16:18,  1.63s/it]11/15/2022 17:18:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:36 - INFO - train.train_snli_ve - loss is tensor(0.8480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 441/16548 [12:16<7:15:52,  1.62s/it]11/15/2022 17:18:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:37 - INFO - train.train_snli_ve - loss is tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 442/16548 [12:18<7:15:16,  1.62s/it]11/15/2022 17:18:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:39 - INFO - train.train_snli_ve - loss is tensor(1.0041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 443/16548 [12:20<7:15:03,  1.62s/it]11/15/2022 17:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:40 - INFO - train.train_snli_ve - loss is tensor(1.1000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 444/16548 [12:21<7:13:52,  1.62s/it]11/15/2022 17:18:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0276e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:42 - INFO - train.train_snli_ve - loss is tensor(0.9275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 445/16548 [12:23<7:15:54,  1.62s/it]11/15/2022 17:18:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.9903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:44 - INFO - train.train_snli_ve - loss is tensor(1.1776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 446/16548 [12:25<7:19:38,  1.64s/it]11/15/2022 17:18:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:45 - INFO - train.train_snli_ve - loss is tensor(1.0793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 447/16548 [12:26<7:25:18,  1.66s/it]11/15/2022 17:18:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.9578e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:47 - INFO - train.train_snli_ve - loss is tensor(1.0090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 448/16548 [12:28<7:21:59,  1.65s/it]11/15/2022 17:18:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:49 - INFO - train.train_snli_ve - loss is tensor(1.2068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 449/16548 [12:30<7:23:39,  1.65s/it]11/15/2022 17:18:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:50 - INFO - train.train_snli_ve - loss is tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 450/16548 [12:31<7:27:07,  1.67s/it]11/15/2022 17:18:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9189e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:52 - INFO - train.train_snli_ve - loss is tensor(0.9213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 451/16548 [12:33<7:23:05,  1.65s/it]11/15/2022 17:18:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.9289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:54 - INFO - train.train_snli_ve - loss is tensor(0.9023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 452/16548 [12:34<7:23:05,  1.65s/it]11/15/2022 17:18:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:55 - INFO - train.train_snli_ve - loss is tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 453/16548 [12:36<7:24:53,  1.66s/it]11/15/2022 17:18:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8957e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:57 - INFO - train.train_snli_ve - loss is tensor(0.8538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 454/16548 [12:38<7:25:16,  1.66s/it]11/15/2022 17:18:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:18:59 - INFO - train.train_snli_ve - loss is tensor(0.9239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 455/16548 [12:39<7:25:32,  1.66s/it]11/15/2022 17:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:00 - INFO - train.train_snli_ve - loss is tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 456/16548 [12:41<7:22:32,  1.65s/it]11/15/2022 17:19:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.8574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:02 - INFO - train.train_snli_ve - loss is tensor(0.8607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 457/16548 [12:43<7:21:11,  1.65s/it]11/15/2022 17:19:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:04 - INFO - train.train_snli_ve - loss is tensor(1.1040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 458/16548 [12:44<7:24:07,  1.66s/it]11/15/2022 17:19:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.8264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:05 - INFO - train.train_snli_ve - loss is tensor(1.2368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 459/16548 [12:46<7:22:40,  1.65s/it]11/15/2022 17:19:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:07 - INFO - train.train_snli_ve - loss is tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 460/16548 [12:48<7:20:55,  1.64s/it]11/15/2022 17:19:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:09 - INFO - train.train_snli_ve - loss is tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 461/16548 [12:49<7:26:35,  1.67s/it]11/15/2022 17:19:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.8098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:10 - INFO - train.train_snli_ve - loss is tensor(0.9315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 462/16548 [12:51<7:21:56,  1.65s/it]11/15/2022 17:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8007e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:12 - INFO - train.train_snli_ve - loss is tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 463/16548 [12:53<7:18:36,  1.64s/it]11/15/2022 17:19:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:14 - INFO - train.train_snli_ve - loss is tensor(0.7301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 464/16548 [12:54<7:21:39,  1.65s/it]11/15/2022 17:19:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:15 - INFO - train.train_snli_ve - loss is tensor(1.0056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 465/16548 [12:56<7:22:09,  1.65s/it]11/15/2022 17:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:17 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 466/16548 [12:58<7:24:29,  1.66s/it]11/15/2022 17:19:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:18 - INFO - train.train_snli_ve - loss is tensor(0.8987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 467/16548 [12:59<7:20:16,  1.64s/it]11/15/2022 17:19:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7383e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:20 - INFO - train.train_snli_ve - loss is tensor(0.9708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 468/16548 [13:01<7:16:56,  1.63s/it]11/15/2022 17:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:22 - INFO - train.train_snli_ve - loss is tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 469/16548 [13:02<7:16:10,  1.63s/it]11/15/2022 17:19:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:23 - INFO - train.train_snli_ve - loss is tensor(0.8539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 470/16548 [13:04<7:20:20,  1.64s/it]11/15/2022 17:19:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6984e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:25 - INFO - train.train_snli_ve - loss is tensor(0.9429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 471/16548 [13:06<7:21:41,  1.65s/it]11/15/2022 17:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:27 - INFO - train.train_snli_ve - loss is tensor(0.8969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 472/16548 [13:07<7:19:30,  1.64s/it]11/15/2022 17:19:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:28 - INFO - train.train_snli_ve - loss is tensor(0.9425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 473/16548 [13:09<7:16:47,  1.63s/it]11/15/2022 17:19:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.6545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:30 - INFO - train.train_snli_ve - loss is tensor(0.9014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 474/16548 [13:11<7:16:24,  1.63s/it]11/15/2022 17:19:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:32 - INFO - train.train_snli_ve - loss is tensor(0.9662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 475/16548 [13:12<7:14:50,  1.62s/it]11/15/2022 17:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:33 - INFO - train.train_snli_ve - loss is tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 476/16548 [13:14<7:18:50,  1.64s/it]11/15/2022 17:19:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:35 - INFO - train.train_snli_ve - loss is tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 477/16548 [13:16<7:17:52,  1.63s/it]11/15/2022 17:19:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:36 - INFO - train.train_snli_ve - loss is tensor(1.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 478/16548 [13:17<7:20:35,  1.64s/it]11/15/2022 17:19:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:38 - INFO - train.train_snli_ve - loss is tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 479/16548 [13:19<7:22:24,  1.65s/it]11/15/2022 17:19:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:40 - INFO - train.train_snli_ve - loss is tensor(0.9970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 480/16548 [13:21<7:26:17,  1.67s/it]11/15/2022 17:19:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:42 - INFO - train.train_snli_ve - loss is tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 481/16548 [13:22<7:26:26,  1.67s/it]11/15/2022 17:19:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:43 - INFO - train.train_snli_ve - loss is tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 482/16548 [13:24<7:28:26,  1.67s/it]11/15/2022 17:19:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:45 - INFO - train.train_snli_ve - loss is tensor(0.8299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 483/16548 [13:26<7:24:57,  1.66s/it]11/15/2022 17:19:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.5398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:46 - INFO - train.train_snli_ve - loss is tensor(0.8032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 484/16548 [13:27<7:22:21,  1.65s/it]11/15/2022 17:19:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:48 - INFO - train.train_snli_ve - loss is tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 485/16548 [13:29<7:21:11,  1.65s/it]11/15/2022 17:19:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:50 - INFO - train.train_snli_ve - loss is tensor(0.9833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 486/16548 [13:30<7:17:46,  1.64s/it]11/15/2022 17:19:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:51 - INFO - train.train_snli_ve - loss is tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 487/16548 [13:32<7:18:33,  1.64s/it]11/15/2022 17:19:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:53 - INFO - train.train_snli_ve - loss is tensor(0.9113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 488/16548 [13:34<7:18:17,  1.64s/it]11/15/2022 17:19:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.4732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:55 - INFO - train.train_snli_ve - loss is tensor(0.8037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 489/16548 [13:35<7:21:09,  1.65s/it]11/15/2022 17:19:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:56 - INFO - train.train_snli_ve - loss is tensor(0.9294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 490/16548 [13:37<7:22:30,  1.65s/it]11/15/2022 17:19:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:19:58 - INFO - train.train_snli_ve - loss is tensor(0.8094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 491/16548 [13:39<7:20:47,  1.65s/it]11/15/2022 17:20:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:00 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 492/16548 [13:40<7:18:44,  1.64s/it]11/15/2022 17:20:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:01 - INFO - train.train_snli_ve - loss is tensor(0.8099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 493/16548 [13:42<7:17:25,  1.63s/it]11/15/2022 17:20:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:03 - INFO - train.train_snli_ve - loss is tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 494/16548 [13:44<7:15:23,  1.63s/it]11/15/2022 17:20:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:04 - INFO - train.train_snli_ve - loss is tensor(0.8900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 495/16548 [13:45<7:18:07,  1.64s/it]11/15/2022 17:20:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.4028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:06 - INFO - train.train_snli_ve - loss is tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 496/16548 [13:47<7:18:53,  1.64s/it]11/15/2022 17:20:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3861e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:08 - INFO - train.train_snli_ve - loss is tensor(1.1530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 497/16548 [13:49<7:18:52,  1.64s/it]11/15/2022 17:20:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3763e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:09 - INFO - train.train_snli_ve - loss is tensor(1.0116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 498/16548 [13:50<7:17:30,  1.64s/it]11/15/2022 17:20:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:11 - INFO - train.train_snli_ve - loss is tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 499/16548 [13:52<7:18:59,  1.64s/it]11/15/2022 17:20:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:13 - INFO - train.train_snli_ve - loss is tensor(1.0021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 500/16548 [13:54<7:26:39,  1.67s/it]11/15/2022 17:20:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.3473e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:14 - INFO - train.train_snli_ve - loss is tensor(0.7843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 501/16548 [13:55<7:26:00,  1.67s/it]11/15/2022 17:20:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:16 - INFO - train.train_snli_ve - loss is tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 502/16548 [13:57<7:21:34,  1.65s/it]11/15/2022 17:20:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.3154e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:18 - INFO - train.train_snli_ve - loss is tensor(0.8114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 503/16548 [13:58<7:21:12,  1.65s/it]11/15/2022 17:20:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:19 - INFO - train.train_snli_ve - loss is tensor(1.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 504/16548 [14:00<7:18:59,  1.64s/it]11/15/2022 17:20:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.3128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:21 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 505/16548 [14:02<7:21:55,  1.65s/it]11/15/2022 17:20:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2942e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:23 - INFO - train.train_snli_ve - loss is tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 506/16548 [14:03<7:19:39,  1.64s/it]11/15/2022 17:20:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2707e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:24 - INFO - train.train_snli_ve - loss is tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 507/16548 [14:05<7:17:45,  1.64s/it]11/15/2022 17:20:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2798e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:26 - INFO - train.train_snli_ve - loss is tensor(0.8988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 508/16548 [14:07<7:17:45,  1.64s/it]11/15/2022 17:20:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:27 - INFO - train.train_snli_ve - loss is tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 509/16548 [14:08<7:14:40,  1.63s/it]11/15/2022 17:20:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:29 - INFO - train.train_snli_ve - loss is tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 510/16548 [14:10<7:18:21,  1.64s/it]11/15/2022 17:20:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:31 - INFO - train.train_snli_ve - loss is tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 511/16548 [14:12<7:20:32,  1.65s/it]11/15/2022 17:20:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:32 - INFO - train.train_snli_ve - loss is tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 512/16548 [14:13<7:17:27,  1.64s/it]11/15/2022 17:20:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.2265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:34 - INFO - train.train_snli_ve - loss is tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 513/16548 [14:15<7:18:58,  1.64s/it]11/15/2022 17:20:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2097e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:36 - INFO - train.train_snli_ve - loss is tensor(0.8274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 514/16548 [14:16<7:17:50,  1.64s/it]11/15/2022 17:20:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:37 - INFO - train.train_snli_ve - loss is tensor(0.8587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 515/16548 [14:18<7:19:04,  1.64s/it]11/15/2022 17:20:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:39 - INFO - train.train_snli_ve - loss is tensor(1.0420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 516/16548 [14:20<7:19:37,  1.65s/it]11/15/2022 17:20:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.1685e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:41 - INFO - train.train_snli_ve - loss is tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 517/16548 [14:21<7:18:29,  1.64s/it]11/15/2022 17:20:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.1520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:42 - INFO - train.train_snli_ve - loss is tensor(0.9066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 518/16548 [14:23<7:16:03,  1.63s/it]11/15/2022 17:20:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:44 - INFO - train.train_snli_ve - loss is tensor(0.8805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 519/16548 [14:25<7:17:40,  1.64s/it]11/15/2022 17:20:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:46 - INFO - train.train_snli_ve - loss is tensor(0.9127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 520/16548 [14:26<7:18:12,  1.64s/it]11/15/2022 17:20:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:47 - INFO - train.train_snli_ve - loss is tensor(0.9128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 521/16548 [14:28<7:17:44,  1.64s/it]11/15/2022 17:20:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:49 - INFO - train.train_snli_ve - loss is tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 522/16548 [14:30<7:18:35,  1.64s/it]11/15/2022 17:20:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:50 - INFO - train.train_snli_ve - loss is tensor(0.9160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 523/16548 [14:31<7:17:38,  1.64s/it]11/15/2022 17:20:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:52 - INFO - train.train_snli_ve - loss is tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 524/16548 [14:33<7:15:12,  1.63s/it]11/15/2022 17:20:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:54 - INFO - train.train_snli_ve - loss is tensor(0.8346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 525/16548 [14:34<7:13:34,  1.62s/it]11/15/2022 17:20:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:55 - INFO - train.train_snli_ve - loss is tensor(0.8072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 526/16548 [14:36<7:15:44,  1.63s/it]11/15/2022 17:20:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:57 - INFO - train.train_snli_ve - loss is tensor(1.1540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 527/16548 [14:38<7:17:48,  1.64s/it]11/15/2022 17:20:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:20:59 - INFO - train.train_snli_ve - loss is tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 528/16548 [14:39<7:15:34,  1.63s/it]11/15/2022 17:21:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0435e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:00 - INFO - train.train_snli_ve - loss is tensor(0.9966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 529/16548 [14:41<7:16:33,  1.64s/it]11/15/2022 17:21:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:02 - INFO - train.train_snli_ve - loss is tensor(0.8310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 530/16548 [14:43<7:18:55,  1.64s/it]11/15/2022 17:21:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:04 - INFO - train.train_snli_ve - loss is tensor(0.8833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 531/16548 [14:44<7:17:21,  1.64s/it]11/15/2022 17:21:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:05 - INFO - train.train_snli_ve - loss is tensor(0.8194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 532/16548 [14:46<7:17:07,  1.64s/it]11/15/2022 17:21:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.9415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:07 - INFO - train.train_snli_ve - loss is tensor(0.9822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 533/16548 [14:48<7:16:25,  1.64s/it]11/15/2022 17:21:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.9893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:09 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 534/16548 [14:49<7:22:17,  1.66s/it]11/15/2022 17:21:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.8825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:10 - INFO - train.train_snli_ve - loss is tensor(0.8235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 535/16548 [14:51<7:22:59,  1.66s/it]11/15/2022 17:21:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.6958e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:12 - INFO - train.train_snli_ve - loss is tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 536/16548 [14:53<7:21:15,  1.65s/it]11/15/2022 17:21:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.6105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:14 - INFO - train.train_snli_ve - loss is tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 537/16548 [14:54<7:22:09,  1.66s/it]11/15/2022 17:21:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.2519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:15 - INFO - train.train_snli_ve - loss is tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 538/16548 [14:56<7:19:41,  1.65s/it]11/15/2022 17:21:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.2941e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:17 - INFO - train.train_snli_ve - loss is tensor(1.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 539/16548 [14:58<7:18:35,  1.64s/it]11/15/2022 17:21:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.0255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:18 - INFO - train.train_snli_ve - loss is tensor(1.0723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 540/16548 [14:59<7:15:31,  1.63s/it]11/15/2022 17:21:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.9999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:20 - INFO - train.train_snli_ve - loss is tensor(0.9610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 541/16548 [15:01<7:16:21,  1.64s/it]11/15/2022 17:21:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.8790e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:22 - INFO - train.train_snli_ve - loss is tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 542/16548 [15:02<7:13:36,  1.63s/it]11/15/2022 17:21:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.8365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:23 - INFO - train.train_snli_ve - loss is tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 543/16548 [15:04<7:18:37,  1.64s/it]11/15/2022 17:21:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.8561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:25 - INFO - train.train_snli_ve - loss is tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 544/16548 [15:06<7:17:13,  1.64s/it]11/15/2022 17:21:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.6753e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:27 - INFO - train.train_snli_ve - loss is tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 545/16548 [15:07<7:19:09,  1.65s/it]11/15/2022 17:21:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.5536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:28 - INFO - train.train_snli_ve - loss is tensor(0.9706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 546/16548 [15:09<7:24:54,  1.67s/it]11/15/2022 17:21:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.4031e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:30 - INFO - train.train_snli_ve - loss is tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 547/16548 [15:11<7:25:44,  1.67s/it]11/15/2022 17:21:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.4765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:32 - INFO - train.train_snli_ve - loss is tensor(0.9803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 548/16548 [15:12<7:20:48,  1.65s/it]11/15/2022 17:21:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.2572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:33 - INFO - train.train_snli_ve - loss is tensor(0.9598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 549/16548 [15:14<7:17:06,  1.64s/it]11/15/2022 17:21:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.4036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:35 - INFO - train.train_snli_ve - loss is tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 550/16548 [15:16<7:18:22,  1.64s/it]11/15/2022 17:21:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.0684e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:36 - INFO - train.train_snli_ve - loss is tensor(0.9058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 551/16548 [15:17<7:16:52,  1.64s/it]11/15/2022 17:21:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.1127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:38 - INFO - train.train_snli_ve - loss is tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 552/16548 [15:19<7:15:34,  1.63s/it]11/15/2022 17:21:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.8279e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:40 - INFO - train.train_snli_ve - loss is tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 553/16548 [15:20<7:13:23,  1.63s/it]11/15/2022 17:21:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.6256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:41 - INFO - train.train_snli_ve - loss is tensor(0.9169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 554/16548 [15:22<7:12:07,  1.62s/it]11/15/2022 17:21:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:43 - INFO - train.train_snli_ve - loss is tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 555/16548 [15:24<7:11:00,  1.62s/it]11/15/2022 17:21:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.4478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:45 - INFO - train.train_snli_ve - loss is tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 556/16548 [15:25<7:11:50,  1.62s/it]11/15/2022 17:21:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.2891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:46 - INFO - train.train_snli_ve - loss is tensor(1.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 557/16548 [15:27<7:11:26,  1.62s/it]11/15/2022 17:21:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.2110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:48 - INFO - train.train_snli_ve - loss is tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 558/16548 [15:29<7:13:39,  1.63s/it]11/15/2022 17:21:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.4524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:49 - INFO - train.train_snli_ve - loss is tensor(0.8812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 559/16548 [15:30<7:14:07,  1.63s/it]11/15/2022 17:21:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.1734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:51 - INFO - train.train_snli_ve - loss is tensor(0.8530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 560/16548 [15:32<7:12:54,  1.62s/it]11/15/2022 17:21:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.1048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:53 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 561/16548 [15:33<7:14:09,  1.63s/it]11/15/2022 17:21:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.1049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:54 - INFO - train.train_snli_ve - loss is tensor(1.0087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 562/16548 [15:35<7:15:21,  1.63s/it]11/15/2022 17:21:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.8075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:56 - INFO - train.train_snli_ve - loss is tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 563/16548 [15:37<7:16:07,  1.64s/it]11/15/2022 17:21:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.5961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:58 - INFO - train.train_snli_ve - loss is tensor(0.9890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 564/16548 [15:38<7:14:47,  1.63s/it]11/15/2022 17:21:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.8290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:21:59 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 565/16548 [15:40<7:16:28,  1.64s/it]11/15/2022 17:22:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.5772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:01 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 566/16548 [15:42<7:13:32,  1.63s/it]11/15/2022 17:22:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.6708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:03 - INFO - train.train_snli_ve - loss is tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 567/16548 [15:43<7:19:43,  1.65s/it]11/15/2022 17:22:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.4721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:04 - INFO - train.train_snli_ve - loss is tensor(0.8682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 568/16548 [15:45<7:20:51,  1.66s/it]11/15/2022 17:22:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.4272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:06 - INFO - train.train_snli_ve - loss is tensor(0.9771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 569/16548 [15:47<7:15:50,  1.64s/it]11/15/2022 17:22:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.4237e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:07 - INFO - train.train_snli_ve - loss is tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 570/16548 [15:48<7:15:00,  1.63s/it]11/15/2022 17:22:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.2531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:09 - INFO - train.train_snli_ve - loss is tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 571/16548 [15:50<7:15:20,  1.63s/it]11/15/2022 17:22:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.0308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:11 - INFO - train.train_snli_ve - loss is tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 572/16548 [15:51<7:13:58,  1.63s/it]11/15/2022 17:22:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.9361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:12 - INFO - train.train_snli_ve - loss is tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 573/16548 [15:53<7:15:07,  1.63s/it]11/15/2022 17:22:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.7335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:14 - INFO - train.train_snli_ve - loss is tensor(0.9032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 574/16548 [15:55<7:17:54,  1.64s/it]11/15/2022 17:22:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.7785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:16 - INFO - train.train_snli_ve - loss is tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 575/16548 [15:56<7:15:40,  1.64s/it]11/15/2022 17:22:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.5569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:17 - INFO - train.train_snli_ve - loss is tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 576/16548 [15:58<7:16:53,  1.64s/it]11/15/2022 17:22:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.7228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:19 - INFO - train.train_snli_ve - loss is tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 577/16548 [16:00<7:15:29,  1.64s/it]11/15/2022 17:22:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.2796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:21 - INFO - train.train_snli_ve - loss is tensor(1.0310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 578/16548 [16:01<7:16:11,  1.64s/it]11/15/2022 17:22:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.5930e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:22 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3% 579/16548 [16:03<7:17:57,  1.65s/it]11/15/2022 17:22:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.4909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:24 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 580/16548 [16:05<7:17:54,  1.65s/it]11/15/2022 17:22:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.1500e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:25 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 581/16548 [16:06<7:14:22,  1.63s/it]11/15/2022 17:22:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.1913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:27 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 582/16548 [16:08<7:15:12,  1.64s/it]11/15/2022 17:22:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.9140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:29 - INFO - train.train_snli_ve - loss is tensor(0.8815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 583/16548 [16:10<7:20:40,  1.66s/it]11/15/2022 17:22:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.9221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:30 - INFO - train.train_snli_ve - loss is tensor(1.0974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 584/16548 [16:11<7:20:48,  1.66s/it]11/15/2022 17:22:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.8880e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:32 - INFO - train.train_snli_ve - loss is tensor(0.9086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 585/16548 [16:13<7:20:16,  1.65s/it]11/15/2022 17:22:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.9971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:34 - INFO - train.train_snli_ve - loss is tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 586/16548 [16:15<7:17:35,  1.64s/it]11/15/2022 17:22:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.8499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:35 - INFO - train.train_snli_ve - loss is tensor(0.9354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 587/16548 [16:16<7:13:56,  1.63s/it]11/15/2022 17:22:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.9145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:37 - INFO - train.train_snli_ve - loss is tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 588/16548 [16:18<7:12:40,  1.63s/it]11/15/2022 17:22:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.7791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:39 - INFO - train.train_snli_ve - loss is tensor(0.9792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 589/16548 [16:19<7:11:23,  1.62s/it]11/15/2022 17:22:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.6837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:40 - INFO - train.train_snli_ve - loss is tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 590/16548 [16:21<7:11:30,  1.62s/it]11/15/2022 17:22:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.7470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:42 - INFO - train.train_snli_ve - loss is tensor(1.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 591/16548 [16:23<7:18:29,  1.65s/it]11/15/2022 17:22:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.7489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:44 - INFO - train.train_snli_ve - loss is tensor(0.8184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 592/16548 [16:24<7:16:42,  1.64s/it]11/15/2022 17:22:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.3191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:45 - INFO - train.train_snli_ve - loss is tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 593/16548 [16:26<7:19:17,  1.65s/it]11/15/2022 17:22:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.2653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:47 - INFO - train.train_snli_ve - loss is tensor(0.7678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 594/16548 [16:28<7:16:53,  1.64s/it]11/15/2022 17:22:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.2502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:49 - INFO - train.train_snli_ve - loss is tensor(0.9560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 595/16548 [16:29<7:18:52,  1.65s/it]11/15/2022 17:22:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.2523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:50 - INFO - train.train_snli_ve - loss is tensor(0.9099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 596/16548 [16:31<7:21:06,  1.66s/it]11/15/2022 17:22:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.6194e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:52 - INFO - train.train_snli_ve - loss is tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 597/16548 [16:33<7:17:08,  1.64s/it]11/15/2022 17:22:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.4767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:53 - INFO - train.train_snli_ve - loss is tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 598/16548 [16:34<7:16:59,  1.64s/it]11/15/2022 17:22:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.6261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:55 - INFO - train.train_snli_ve - loss is tensor(0.9800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 599/16548 [16:36<7:16:49,  1.64s/it]11/15/2022 17:22:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.9016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:57 - INFO - train.train_snli_ve - loss is tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 600/16548 [16:38<7:25:01,  1.67s/it]11/15/2022 17:22:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.8873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:22:58 - INFO - train.train_snli_ve - loss is tensor(0.7920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 601/16548 [16:39<7:22:55,  1.67s/it]11/15/2022 17:23:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.8380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:00 - INFO - train.train_snli_ve - loss is tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 602/16548 [16:41<7:18:15,  1.65s/it]11/15/2022 17:23:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.7339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:02 - INFO - train.train_snli_ve - loss is tensor(0.9015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 603/16548 [16:43<7:20:01,  1.66s/it]11/15/2022 17:23:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.5395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:03 - INFO - train.train_snli_ve - loss is tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 604/16548 [16:44<7:18:19,  1.65s/it]11/15/2022 17:23:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.5175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:05 - INFO - train.train_snli_ve - loss is tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 605/16548 [16:46<7:19:09,  1.65s/it]11/15/2022 17:23:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.4368e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:07 - INFO - train.train_snli_ve - loss is tensor(0.9221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 606/16548 [16:47<7:17:09,  1.65s/it]11/15/2022 17:23:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.6073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:08 - INFO - train.train_snli_ve - loss is tensor(0.9213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 607/16548 [16:49<7:18:42,  1.65s/it]11/15/2022 17:23:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.3762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:10 - INFO - train.train_snli_ve - loss is tensor(1.0404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 608/16548 [16:51<7:15:14,  1.64s/it]11/15/2022 17:23:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:12 - INFO - train.train_snli_ve - loss is tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 609/16548 [16:52<7:18:59,  1.65s/it]11/15/2022 17:23:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.7259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:13 - INFO - train.train_snli_ve - loss is tensor(0.8748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 610/16548 [16:54<7:17:32,  1.65s/it]11/15/2022 17:23:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.7070e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:15 - INFO - train.train_snli_ve - loss is tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 611/16548 [16:56<7:14:52,  1.64s/it]11/15/2022 17:23:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.7553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:17 - INFO - train.train_snli_ve - loss is tensor(0.9640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 612/16548 [16:57<7:16:17,  1.64s/it]11/15/2022 17:23:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.3393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:18 - INFO - train.train_snli_ve - loss is tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 613/16548 [16:59<7:16:59,  1.65s/it]11/15/2022 17:23:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.9922e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:20 - INFO - train.train_snli_ve - loss is tensor(0.9490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 614/16548 [17:01<7:13:50,  1.63s/it]11/15/2022 17:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.2594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:21 - INFO - train.train_snli_ve - loss is tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 615/16548 [17:02<7:14:31,  1.64s/it]11/15/2022 17:23:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.3310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:23 - INFO - train.train_snli_ve - loss is tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 616/16548 [17:04<7:17:24,  1.65s/it]11/15/2022 17:23:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.6685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:25 - INFO - train.train_snli_ve - loss is tensor(1.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 617/16548 [17:06<7:17:20,  1.65s/it]11/15/2022 17:23:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.1461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:26 - INFO - train.train_snli_ve - loss is tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 618/16548 [17:07<7:18:06,  1.65s/it]11/15/2022 17:23:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.1271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:28 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 619/16548 [17:09<7:19:09,  1.65s/it]11/15/2022 17:23:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.9087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:30 - INFO - train.train_snli_ve - loss is tensor(1.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 620/16548 [17:10<7:17:50,  1.65s/it]11/15/2022 17:23:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.0225e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:31 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 621/16548 [17:12<7:14:49,  1.64s/it]11/15/2022 17:23:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.8512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:33 - INFO - train.train_snli_ve - loss is tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 622/16548 [17:14<7:15:52,  1.64s/it]11/15/2022 17:23:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.8765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:35 - INFO - train.train_snli_ve - loss is tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 623/16548 [17:15<7:14:38,  1.64s/it]11/15/2022 17:23:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.9437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:36 - INFO - train.train_snli_ve - loss is tensor(0.8875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 624/16548 [17:17<7:19:20,  1.66s/it]11/15/2022 17:23:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9487e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:38 - INFO - train.train_snli_ve - loss is tensor(0.9352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 625/16548 [17:19<7:16:49,  1.65s/it]11/15/2022 17:23:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.7521e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:40 - INFO - train.train_snli_ve - loss is tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 626/16548 [17:20<7:14:50,  1.64s/it]11/15/2022 17:23:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.9239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:41 - INFO - train.train_snli_ve - loss is tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 627/16548 [17:22<7:17:24,  1.65s/it]11/15/2022 17:23:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:43 - INFO - train.train_snli_ve - loss is tensor(0.8231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 628/16548 [17:24<7:14:36,  1.64s/it]11/15/2022 17:23:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.6414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:44 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 629/16548 [17:25<7:10:58,  1.62s/it]11/15/2022 17:23:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.6928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:46 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 630/16548 [17:27<7:11:32,  1.63s/it]11/15/2022 17:23:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:48 - INFO - train.train_snli_ve - loss is tensor(0.9030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 631/16548 [17:28<7:13:33,  1.63s/it]11/15/2022 17:23:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:49 - INFO - train.train_snli_ve - loss is tensor(1.0872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 632/16548 [17:30<7:10:58,  1.62s/it]11/15/2022 17:23:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.3488e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:51 - INFO - train.train_snli_ve - loss is tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 633/16548 [17:32<7:12:37,  1.63s/it]11/15/2022 17:23:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.0912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:53 - INFO - train.train_snli_ve - loss is tensor(0.7415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 634/16548 [17:33<7:10:09,  1.62s/it]11/15/2022 17:23:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0646e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:54 - INFO - train.train_snli_ve - loss is tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 635/16548 [17:35<7:11:52,  1.63s/it]11/15/2022 17:23:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.3980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:56 - INFO - train.train_snli_ve - loss is tensor(1.1407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 636/16548 [17:37<7:14:37,  1.64s/it]11/15/2022 17:23:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:57 - INFO - train.train_snli_ve - loss is tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 637/16548 [17:38<7:11:56,  1.63s/it]11/15/2022 17:23:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.7378e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:23:59 - INFO - train.train_snli_ve - loss is tensor(0.8510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 638/16548 [17:40<7:11:41,  1.63s/it]11/15/2022 17:24:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.1276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:01 - INFO - train.train_snli_ve - loss is tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 639/16548 [17:41<7:11:50,  1.63s/it]11/15/2022 17:24:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.5417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:02 - INFO - train.train_snli_ve - loss is tensor(0.9875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 640/16548 [17:43<7:12:33,  1.63s/it]11/15/2022 17:24:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3921e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:04 - INFO - train.train_snli_ve - loss is tensor(0.8773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 641/16548 [17:45<7:11:39,  1.63s/it]11/15/2022 17:24:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.8327e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:06 - INFO - train.train_snli_ve - loss is tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 642/16548 [17:46<7:11:16,  1.63s/it]11/15/2022 17:24:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.4554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:07 - INFO - train.train_snli_ve - loss is tensor(0.9601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 643/16548 [17:48<7:15:41,  1.64s/it]11/15/2022 17:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4419e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:09 - INFO - train.train_snli_ve - loss is tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 644/16548 [17:50<7:18:15,  1.65s/it]11/15/2022 17:24:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.7321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:11 - INFO - train.train_snli_ve - loss is tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 645/16548 [17:51<7:17:19,  1.65s/it]11/15/2022 17:24:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:12 - INFO - train.train_snli_ve - loss is tensor(0.7886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 646/16548 [17:53<7:18:20,  1.65s/it]11/15/2022 17:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:14 - INFO - train.train_snli_ve - loss is tensor(0.8144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 647/16548 [17:55<7:20:36,  1.66s/it]11/15/2022 17:24:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.9382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:16 - INFO - train.train_snli_ve - loss is tensor(0.9809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 648/16548 [17:56<7:15:58,  1.65s/it]11/15/2022 17:24:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.1283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:17 - INFO - train.train_snli_ve - loss is tensor(0.9684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 649/16548 [17:58<7:14:09,  1.64s/it]11/15/2022 17:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4945e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:19 - INFO - train.train_snli_ve - loss is tensor(0.8980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 650/16548 [18:00<7:13:41,  1.64s/it]11/15/2022 17:24:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:20 - INFO - train.train_snli_ve - loss is tensor(0.8840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 651/16548 [18:01<7:14:44,  1.64s/it]11/15/2022 17:24:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3043e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:22 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 652/16548 [18:03<7:14:03,  1.64s/it]11/15/2022 17:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1991e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:24 - INFO - train.train_snli_ve - loss is tensor(0.9343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 653/16548 [18:04<7:13:24,  1.64s/it]11/15/2022 17:24:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:25 - INFO - train.train_snli_ve - loss is tensor(1.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 654/16548 [18:06<7:12:34,  1.63s/it]11/15/2022 17:24:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:27 - INFO - train.train_snli_ve - loss is tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 655/16548 [18:08<7:12:39,  1.63s/it]11/15/2022 17:24:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9755e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:29 - INFO - train.train_snli_ve - loss is tensor(0.9539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 656/16548 [18:09<7:14:58,  1.64s/it]11/15/2022 17:24:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:30 - INFO - train.train_snli_ve - loss is tensor(0.8872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 657/16548 [18:11<7:15:23,  1.64s/it]11/15/2022 17:24:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:32 - INFO - train.train_snli_ve - loss is tensor(0.8800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 658/16548 [18:13<7:12:52,  1.63s/it]11/15/2022 17:24:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:34 - INFO - train.train_snli_ve - loss is tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 659/16548 [18:14<7:09:18,  1.62s/it]11/15/2022 17:24:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:35 - INFO - train.train_snli_ve - loss is tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 660/16548 [18:16<7:09:02,  1.62s/it]11/15/2022 17:24:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:37 - INFO - train.train_snli_ve - loss is tensor(0.9471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 661/16548 [18:18<7:09:09,  1.62s/it]11/15/2022 17:24:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:38 - INFO - train.train_snli_ve - loss is tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 662/16548 [18:19<7:09:56,  1.62s/it]11/15/2022 17:24:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:40 - INFO - train.train_snli_ve - loss is tensor(0.9104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 663/16548 [18:21<7:11:48,  1.63s/it]11/15/2022 17:24:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:42 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 664/16548 [18:22<7:10:29,  1.63s/it]11/15/2022 17:24:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1384e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:43 - INFO - train.train_snli_ve - loss is tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 665/16548 [18:24<7:13:08,  1.64s/it]11/15/2022 17:24:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:45 - INFO - train.train_snli_ve - loss is tensor(0.8621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 666/16548 [18:26<7:11:48,  1.63s/it]11/15/2022 17:24:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3548e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:47 - INFO - train.train_snli_ve - loss is tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 667/16548 [18:27<7:12:02,  1.63s/it]11/15/2022 17:24:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7082e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:48 - INFO - train.train_snli_ve - loss is tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 668/16548 [18:29<7:17:31,  1.65s/it]11/15/2022 17:24:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4641e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:50 - INFO - train.train_snli_ve - loss is tensor(0.9020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 669/16548 [18:31<7:18:09,  1.66s/it]11/15/2022 17:24:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:52 - INFO - train.train_snli_ve - loss is tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 670/16548 [18:32<7:15:34,  1.65s/it]11/15/2022 17:24:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:53 - INFO - train.train_snli_ve - loss is tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 671/16548 [18:34<7:15:00,  1.64s/it]11/15/2022 17:24:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:55 - INFO - train.train_snli_ve - loss is tensor(0.7228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 672/16548 [18:36<7:15:43,  1.65s/it]11/15/2022 17:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.1300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:56 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 673/16548 [18:37<7:12:24,  1.63s/it]11/15/2022 17:24:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:24:58 - INFO - train.train_snli_ve - loss is tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 674/16548 [18:39<7:12:36,  1.64s/it]11/15/2022 17:25:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:00 - INFO - train.train_snli_ve - loss is tensor(0.8512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 675/16548 [18:40<7:11:09,  1.63s/it]11/15/2022 17:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:01 - INFO - train.train_snli_ve - loss is tensor(0.9649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 676/16548 [18:42<7:12:26,  1.63s/it]11/15/2022 17:25:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.7904e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:03 - INFO - train.train_snli_ve - loss is tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 677/16548 [18:44<7:10:24,  1.63s/it]11/15/2022 17:25:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.5055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:05 - INFO - train.train_snli_ve - loss is tensor(0.9069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 678/16548 [18:45<7:11:57,  1.63s/it]11/15/2022 17:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.8816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:06 - INFO - train.train_snli_ve - loss is tensor(0.7967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 679/16548 [18:47<7:13:07,  1.64s/it]11/15/2022 17:25:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.8500e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:08 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 680/16548 [18:49<7:16:19,  1.65s/it]11/15/2022 17:25:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.3462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:10 - INFO - train.train_snli_ve - loss is tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 681/16548 [18:50<7:13:43,  1.64s/it]11/15/2022 17:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:11 - INFO - train.train_snli_ve - loss is tensor(0.9840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 682/16548 [18:52<7:14:16,  1.64s/it]11/15/2022 17:25:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:13 - INFO - train.train_snli_ve - loss is tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 683/16548 [18:54<7:14:21,  1.64s/it]11/15/2022 17:25:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.7309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:14 - INFO - train.train_snli_ve - loss is tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 684/16548 [18:55<7:11:23,  1.63s/it]11/15/2022 17:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2329e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:16 - INFO - train.train_snli_ve - loss is tensor(0.8456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 685/16548 [18:57<7:10:47,  1.63s/it]11/15/2022 17:25:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8563e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:18 - INFO - train.train_snli_ve - loss is tensor(1.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 686/16548 [18:58<7:08:13,  1.62s/it]11/15/2022 17:25:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.3699e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:19 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 687/16548 [19:00<7:07:58,  1.62s/it]11/15/2022 17:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:21 - INFO - train.train_snli_ve - loss is tensor(0.9941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 688/16548 [19:02<7:12:11,  1.64s/it]11/15/2022 17:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.6192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:23 - INFO - train.train_snli_ve - loss is tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 689/16548 [19:03<7:15:10,  1.65s/it]11/15/2022 17:25:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:24 - INFO - train.train_snli_ve - loss is tensor(0.7450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 690/16548 [19:05<7:17:17,  1.65s/it]11/15/2022 17:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:26 - INFO - train.train_snli_ve - loss is tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 691/16548 [19:07<7:19:19,  1.66s/it]11/15/2022 17:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4710e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:28 - INFO - train.train_snli_ve - loss is tensor(0.8744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 692/16548 [19:08<7:18:37,  1.66s/it]11/15/2022 17:25:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:29 - INFO - train.train_snli_ve - loss is tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 693/16548 [19:10<7:15:38,  1.65s/it]11/15/2022 17:25:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:31 - INFO - train.train_snli_ve - loss is tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 694/16548 [19:12<7:18:34,  1.66s/it]11/15/2022 17:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:33 - INFO - train.train_snli_ve - loss is tensor(0.9170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 695/16548 [19:13<7:14:55,  1.65s/it]11/15/2022 17:25:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:34 - INFO - train.train_snli_ve - loss is tensor(0.9463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 696/16548 [19:15<7:18:03,  1.66s/it]11/15/2022 17:25:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:36 - INFO - train.train_snli_ve - loss is tensor(0.9127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 697/16548 [19:17<7:20:04,  1.67s/it]11/15/2022 17:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:38 - INFO - train.train_snli_ve - loss is tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 698/16548 [19:18<7:20:38,  1.67s/it]11/15/2022 17:25:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:39 - INFO - train.train_snli_ve - loss is tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 699/16548 [19:20<7:18:04,  1.66s/it]11/15/2022 17:25:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9419e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:41 - INFO - train.train_snli_ve - loss is tensor(0.8022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 700/16548 [19:22<7:27:25,  1.69s/it]11/15/2022 17:25:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:43 - INFO - train.train_snli_ve - loss is tensor(0.8517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 701/16548 [19:23<7:25:02,  1.69s/it]11/15/2022 17:25:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0242e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:44 - INFO - train.train_snli_ve - loss is tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 702/16548 [19:25<7:18:56,  1.66s/it]11/15/2022 17:25:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1992e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:46 - INFO - train.train_snli_ve - loss is tensor(0.7751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 703/16548 [19:27<7:15:13,  1.65s/it]11/15/2022 17:25:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:48 - INFO - train.train_snli_ve - loss is tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 704/16548 [19:28<7:12:36,  1.64s/it]11/15/2022 17:25:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.5514e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:49 - INFO - train.train_snli_ve - loss is tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 705/16548 [19:30<7:09:46,  1.63s/it]11/15/2022 17:25:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.2903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:51 - INFO - train.train_snli_ve - loss is tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 706/16548 [19:31<7:08:48,  1.62s/it]11/15/2022 17:25:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:52 - INFO - train.train_snli_ve - loss is tensor(0.8962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 707/16548 [19:33<7:10:37,  1.63s/it]11/15/2022 17:25:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:54 - INFO - train.train_snli_ve - loss is tensor(1.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 708/16548 [19:35<7:12:11,  1.64s/it]11/15/2022 17:25:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:56 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 709/16548 [19:36<7:12:00,  1.64s/it]11/15/2022 17:25:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:57 - INFO - train.train_snli_ve - loss is tensor(0.9911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 710/16548 [19:38<7:12:26,  1.64s/it]11/15/2022 17:25:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:25:59 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 711/16548 [19:40<7:17:07,  1.66s/it]11/15/2022 17:26:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:01 - INFO - train.train_snli_ve - loss is tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 712/16548 [19:41<7:14:05,  1.64s/it]11/15/2022 17:26:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.6022e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:02 - INFO - train.train_snli_ve - loss is tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 713/16548 [19:43<7:12:24,  1.64s/it]11/15/2022 17:26:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.1295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:04 - INFO - train.train_snli_ve - loss is tensor(0.8598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 714/16548 [19:45<7:11:48,  1.64s/it]11/15/2022 17:26:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:06 - INFO - train.train_snli_ve - loss is tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 715/16548 [19:46<7:10:34,  1.63s/it]11/15/2022 17:26:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9564e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:07 - INFO - train.train_snli_ve - loss is tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 716/16548 [19:48<7:10:35,  1.63s/it]11/15/2022 17:26:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9948e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:09 - INFO - train.train_snli_ve - loss is tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 717/16548 [19:50<7:12:14,  1.64s/it]11/15/2022 17:26:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0360e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:10 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 718/16548 [19:51<7:09:26,  1.63s/it]11/15/2022 17:26:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8973e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:12 - INFO - train.train_snli_ve - loss is tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 719/16548 [19:53<7:09:50,  1.63s/it]11/15/2022 17:26:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1322e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:14 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 720/16548 [19:54<7:10:45,  1.63s/it]11/15/2022 17:26:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6329e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:15 - INFO - train.train_snli_ve - loss is tensor(0.8024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 721/16548 [19:56<7:11:00,  1.63s/it]11/15/2022 17:26:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3002e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:17 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 722/16548 [19:58<7:08:31,  1.62s/it]11/15/2022 17:26:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.9957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:19 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 723/16548 [19:59<7:07:02,  1.62s/it]11/15/2022 17:26:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:20 - INFO - train.train_snli_ve - loss is tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 724/16548 [20:01<7:09:11,  1.63s/it]11/15/2022 17:26:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:22 - INFO - train.train_snli_ve - loss is tensor(0.9852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 725/16548 [20:03<7:10:40,  1.63s/it]11/15/2022 17:26:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.3113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:23 - INFO - train.train_snli_ve - loss is tensor(1.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 726/16548 [20:04<7:10:20,  1.63s/it]11/15/2022 17:26:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.1783e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:25 - INFO - train.train_snli_ve - loss is tensor(0.7916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 727/16548 [20:06<7:10:28,  1.63s/it]11/15/2022 17:26:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.0389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:27 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 728/16548 [20:07<7:11:25,  1.64s/it]11/15/2022 17:26:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.9074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:28 - INFO - train.train_snli_ve - loss is tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 729/16548 [20:09<7:08:37,  1.63s/it]11/15/2022 17:26:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:30 - INFO - train.train_snli_ve - loss is tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 730/16548 [20:11<7:09:33,  1.63s/it]11/15/2022 17:26:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.6569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:32 - INFO - train.train_snli_ve - loss is tensor(0.8539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 731/16548 [20:12<7:08:40,  1.63s/it]11/15/2022 17:26:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.5475e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:33 - INFO - train.train_snli_ve - loss is tensor(0.9055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 732/16548 [20:14<7:10:22,  1.63s/it]11/15/2022 17:26:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.1215e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:35 - INFO - train.train_snli_ve - loss is tensor(1.1088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 733/16548 [20:16<7:15:39,  1.65s/it]11/15/2022 17:26:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:37 - INFO - train.train_snli_ve - loss is tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 734/16548 [20:17<7:11:27,  1.64s/it]11/15/2022 17:26:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:38 - INFO - train.train_snli_ve - loss is tensor(0.8843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 735/16548 [20:19<7:10:30,  1.63s/it]11/15/2022 17:26:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.6072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:40 - INFO - train.train_snli_ve - loss is tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 736/16548 [20:21<7:10:06,  1.63s/it]11/15/2022 17:26:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.8280e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:41 - INFO - train.train_snli_ve - loss is tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 737/16548 [20:22<7:08:37,  1.63s/it]11/15/2022 17:26:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:43 - INFO - train.train_snli_ve - loss is tensor(1.0230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 738/16548 [20:24<7:12:30,  1.64s/it]11/15/2022 17:26:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:45 - INFO - train.train_snli_ve - loss is tensor(1.0557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 739/16548 [20:25<7:11:45,  1.64s/it]11/15/2022 17:26:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6965e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:46 - INFO - train.train_snli_ve - loss is tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 740/16548 [20:27<7:13:22,  1.64s/it]11/15/2022 17:26:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.9434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:48 - INFO - train.train_snli_ve - loss is tensor(1.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 741/16548 [20:29<7:10:38,  1.63s/it]11/15/2022 17:26:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.2535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:50 - INFO - train.train_snli_ve - loss is tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 742/16548 [20:30<7:08:39,  1.63s/it]11/15/2022 17:26:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:51 - INFO - train.train_snli_ve - loss is tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 743/16548 [20:32<7:09:25,  1.63s/it]11/15/2022 17:26:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.3450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:53 - INFO - train.train_snli_ve - loss is tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4% 744/16548 [20:34<7:09:24,  1.63s/it]11/15/2022 17:26:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:54 - INFO - train.train_snli_ve - loss is tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 745/16548 [20:35<7:10:45,  1.64s/it]11/15/2022 17:26:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.7306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:56 - INFO - train.train_snli_ve - loss is tensor(0.8501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 746/16548 [20:37<7:16:41,  1.66s/it]11/15/2022 17:26:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:58 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 747/16548 [20:39<7:11:26,  1.64s/it]11/15/2022 17:26:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1002e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:26:59 - INFO - train.train_snli_ve - loss is tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 748/16548 [20:40<7:10:25,  1.63s/it]11/15/2022 17:27:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.9670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:01 - INFO - train.train_snli_ve - loss is tensor(0.8587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 749/16548 [20:42<7:09:37,  1.63s/it]11/15/2022 17:27:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:03 - INFO - train.train_snli_ve - loss is tensor(0.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 750/16548 [20:43<7:11:24,  1.64s/it]11/15/2022 17:27:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0182e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:04 - INFO - train.train_snli_ve - loss is tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 751/16548 [20:45<7:14:29,  1.65s/it]11/15/2022 17:27:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:06 - INFO - train.train_snli_ve - loss is tensor(0.7910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 752/16548 [20:47<7:11:54,  1.64s/it]11/15/2022 17:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:08 - INFO - train.train_snli_ve - loss is tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 753/16548 [20:48<7:12:31,  1.64s/it]11/15/2022 17:27:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:09 - INFO - train.train_snli_ve - loss is tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 754/16548 [20:50<7:10:52,  1.64s/it]11/15/2022 17:27:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.0864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:11 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 755/16548 [20:52<7:10:31,  1.64s/it]11/15/2022 17:27:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:13 - INFO - train.train_snli_ve - loss is tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 756/16548 [20:53<7:10:35,  1.64s/it]11/15/2022 17:27:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:14 - INFO - train.train_snli_ve - loss is tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 757/16548 [20:55<7:10:26,  1.64s/it]11/15/2022 17:27:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.5217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:16 - INFO - train.train_snli_ve - loss is tensor(0.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 758/16548 [20:57<7:12:43,  1.64s/it]11/15/2022 17:27:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.1073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:17 - INFO - train.train_snli_ve - loss is tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 759/16548 [20:58<7:11:17,  1.64s/it]11/15/2022 17:27:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:19 - INFO - train.train_snli_ve - loss is tensor(0.9762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 760/16548 [21:00<7:10:12,  1.63s/it]11/15/2022 17:27:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:21 - INFO - train.train_snli_ve - loss is tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 761/16548 [21:01<7:08:44,  1.63s/it]11/15/2022 17:27:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.8791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:22 - INFO - train.train_snli_ve - loss is tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 762/16548 [21:03<7:12:34,  1.64s/it]11/15/2022 17:27:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:24 - INFO - train.train_snli_ve - loss is tensor(0.9960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 763/16548 [21:05<7:11:38,  1.64s/it]11/15/2022 17:27:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:26 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 764/16548 [21:06<7:13:57,  1.65s/it]11/15/2022 17:27:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:27 - INFO - train.train_snli_ve - loss is tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 765/16548 [21:08<7:13:35,  1.65s/it]11/15/2022 17:27:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:29 - INFO - train.train_snli_ve - loss is tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 766/16548 [21:10<7:13:40,  1.65s/it]11/15/2022 17:27:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:31 - INFO - train.train_snli_ve - loss is tensor(1.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 767/16548 [21:11<7:12:14,  1.64s/it]11/15/2022 17:27:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.1265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:32 - INFO - train.train_snli_ve - loss is tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 768/16548 [21:13<7:12:34,  1.64s/it]11/15/2022 17:27:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:34 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 769/16548 [21:15<7:11:45,  1.64s/it]11/15/2022 17:27:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:36 - INFO - train.train_snli_ve - loss is tensor(1.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 770/16548 [21:16<7:13:47,  1.65s/it]11/15/2022 17:27:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:37 - INFO - train.train_snli_ve - loss is tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 771/16548 [21:18<7:11:10,  1.64s/it]11/15/2022 17:27:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:39 - INFO - train.train_snli_ve - loss is tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 772/16548 [21:20<7:08:57,  1.63s/it]11/15/2022 17:27:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8698e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:40 - INFO - train.train_snli_ve - loss is tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 773/16548 [21:21<7:08:07,  1.63s/it]11/15/2022 17:27:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:42 - INFO - train.train_snli_ve - loss is tensor(0.8621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 774/16548 [21:23<7:11:32,  1.64s/it]11/15/2022 17:27:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.4540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:44 - INFO - train.train_snli_ve - loss is tensor(0.8989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 775/16548 [21:24<7:11:29,  1.64s/it]11/15/2022 17:27:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:45 - INFO - train.train_snli_ve - loss is tensor(0.8427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 776/16548 [21:26<7:11:26,  1.64s/it]11/15/2022 17:27:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.8560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:47 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 777/16548 [21:28<7:08:51,  1.63s/it]11/15/2022 17:27:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.7546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:49 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 778/16548 [21:29<7:10:04,  1.64s/it]11/15/2022 17:27:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:50 - INFO - train.train_snli_ve - loss is tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 779/16548 [21:31<7:07:16,  1.63s/it]11/15/2022 17:27:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:52 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 780/16548 [21:33<7:08:23,  1.63s/it]11/15/2022 17:27:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.2658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:53 - INFO - train.train_snli_ve - loss is tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 781/16548 [21:34<7:06:54,  1.62s/it]11/15/2022 17:27:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:55 - INFO - train.train_snli_ve - loss is tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 782/16548 [21:36<7:09:37,  1.64s/it]11/15/2022 17:27:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:57 - INFO - train.train_snli_ve - loss is tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 783/16548 [21:37<7:06:58,  1.63s/it]11/15/2022 17:27:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.2382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:27:58 - INFO - train.train_snli_ve - loss is tensor(0.9863, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 784/16548 [21:39<7:07:46,  1.63s/it]11/15/2022 17:28:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:00 - INFO - train.train_snli_ve - loss is tensor(0.9586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 785/16548 [21:41<7:09:40,  1.64s/it]11/15/2022 17:28:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:02 - INFO - train.train_snli_ve - loss is tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 786/16548 [21:42<7:09:55,  1.64s/it]11/15/2022 17:28:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.1926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:03 - INFO - train.train_snli_ve - loss is tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 787/16548 [21:44<7:09:00,  1.63s/it]11/15/2022 17:28:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:05 - INFO - train.train_snli_ve - loss is tensor(0.9430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 788/16548 [21:46<7:06:35,  1.62s/it]11/15/2022 17:28:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2655e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:07 - INFO - train.train_snli_ve - loss is tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 789/16548 [21:47<7:06:31,  1.62s/it]11/15/2022 17:28:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:08 - INFO - train.train_snli_ve - loss is tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 790/16548 [21:49<7:08:21,  1.63s/it]11/15/2022 17:28:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.7503e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:10 - INFO - train.train_snli_ve - loss is tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 791/16548 [21:51<7:09:56,  1.64s/it]11/15/2022 17:28:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:11 - INFO - train.train_snli_ve - loss is tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 792/16548 [21:52<7:10:00,  1.64s/it]11/15/2022 17:28:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.6749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:13 - INFO - train.train_snli_ve - loss is tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 793/16548 [21:54<7:09:04,  1.63s/it]11/15/2022 17:28:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:15 - INFO - train.train_snli_ve - loss is tensor(0.9611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 794/16548 [21:55<7:06:06,  1.62s/it]11/15/2022 17:28:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.5314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:16 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 795/16548 [21:57<7:07:58,  1.63s/it]11/15/2022 17:28:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9017e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:18 - INFO - train.train_snli_ve - loss is tensor(0.9388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 796/16548 [21:59<7:11:28,  1.64s/it]11/15/2022 17:28:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.5691e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:20 - INFO - train.train_snli_ve - loss is tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 797/16548 [22:00<7:08:09,  1.63s/it]11/15/2022 17:28:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9991e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:21 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 798/16548 [22:02<7:08:10,  1.63s/it]11/15/2022 17:28:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.1041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:23 - INFO - train.train_snli_ve - loss is tensor(0.8366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 799/16548 [22:04<7:10:24,  1.64s/it]11/15/2022 17:28:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.5220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:24 - INFO - train.train_snli_ve - loss is tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 800/16548 [22:05<7:15:01,  1.66s/it]11/15/2022 17:28:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7831e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:26 - INFO - train.train_snli_ve - loss is tensor(0.8926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 801/16548 [22:07<7:15:08,  1.66s/it]11/15/2022 17:28:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5423e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:28 - INFO - train.train_snli_ve - loss is tensor(0.8346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 802/16548 [22:09<7:18:22,  1.67s/it]11/15/2022 17:28:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.6271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:30 - INFO - train.train_snli_ve - loss is tensor(1.2489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 803/16548 [22:10<7:15:55,  1.66s/it]11/15/2022 17:28:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.6424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:31 - INFO - train.train_snli_ve - loss is tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 804/16548 [22:12<7:18:22,  1.67s/it]11/15/2022 17:28:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.3013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:33 - INFO - train.train_snli_ve - loss is tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 805/16548 [22:14<7:12:48,  1.65s/it]11/15/2022 17:28:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:34 - INFO - train.train_snli_ve - loss is tensor(0.9145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 806/16548 [22:15<7:09:17,  1.64s/it]11/15/2022 17:28:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:36 - INFO - train.train_snli_ve - loss is tensor(0.8172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 807/16548 [22:17<7:10:46,  1.64s/it]11/15/2022 17:28:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.7092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:38 - INFO - train.train_snli_ve - loss is tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 808/16548 [22:19<7:20:49,  1.68s/it]11/15/2022 17:28:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:40 - INFO - train.train_snli_ve - loss is tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 809/16548 [22:20<7:19:58,  1.68s/it]11/15/2022 17:28:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:41 - INFO - train.train_snli_ve - loss is tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 810/16548 [22:22<7:22:53,  1.69s/it]11/15/2022 17:28:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:43 - INFO - train.train_snli_ve - loss is tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 811/16548 [22:24<7:17:04,  1.67s/it]11/15/2022 17:28:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9558e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:45 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 812/16548 [22:25<7:17:39,  1.67s/it]11/15/2022 17:28:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.3372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:46 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 813/16548 [22:27<7:16:32,  1.66s/it]11/15/2022 17:28:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.7987e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:48 - INFO - train.train_snli_ve - loss is tensor(0.8053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 814/16548 [22:29<7:14:36,  1.66s/it]11/15/2022 17:28:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:49 - INFO - train.train_snli_ve - loss is tensor(0.8544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 815/16548 [22:30<7:09:27,  1.64s/it]11/15/2022 17:28:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.2342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:51 - INFO - train.train_snli_ve - loss is tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 816/16548 [22:32<7:09:07,  1.64s/it]11/15/2022 17:28:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.2635e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:53 - INFO - train.train_snli_ve - loss is tensor(0.8207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 817/16548 [22:34<7:11:42,  1.65s/it]11/15/2022 17:28:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:54 - INFO - train.train_snli_ve - loss is tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 818/16548 [22:35<7:06:54,  1.63s/it]11/15/2022 17:28:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:56 - INFO - train.train_snli_ve - loss is tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 819/16548 [22:37<7:06:00,  1.63s/it]11/15/2022 17:28:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:58 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 820/16548 [22:38<7:07:52,  1.63s/it]11/15/2022 17:28:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.3340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:28:59 - INFO - train.train_snli_ve - loss is tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 821/16548 [22:40<7:05:56,  1.63s/it]11/15/2022 17:29:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:01 - INFO - train.train_snli_ve - loss is tensor(0.8427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 822/16548 [22:42<7:08:59,  1.64s/it]11/15/2022 17:29:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.5409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:03 - INFO - train.train_snli_ve - loss is tensor(0.8758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 823/16548 [22:43<7:11:22,  1.65s/it]11/15/2022 17:29:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2754e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:04 - INFO - train.train_snli_ve - loss is tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 824/16548 [22:45<7:09:39,  1.64s/it]11/15/2022 17:29:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.2629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:06 - INFO - train.train_snli_ve - loss is tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 825/16548 [22:47<7:08:05,  1.63s/it]11/15/2022 17:29:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.5885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:07 - INFO - train.train_snli_ve - loss is tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 826/16548 [22:48<7:11:14,  1.65s/it]11/15/2022 17:29:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.9287e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:09 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 827/16548 [22:50<7:13:50,  1.66s/it]11/15/2022 17:29:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.4112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:11 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 828/16548 [22:52<7:11:26,  1.65s/it]11/15/2022 17:29:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:12 - INFO - train.train_snli_ve - loss is tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 829/16548 [22:53<7:13:51,  1.66s/it]11/15/2022 17:29:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5398e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:14 - INFO - train.train_snli_ve - loss is tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 830/16548 [22:55<7:13:56,  1.66s/it]11/15/2022 17:29:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:16 - INFO - train.train_snli_ve - loss is tensor(0.7347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 831/16548 [22:57<7:14:59,  1.66s/it]11/15/2022 17:29:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.7682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:17 - INFO - train.train_snli_ve - loss is tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 832/16548 [22:58<7:13:41,  1.66s/it]11/15/2022 17:29:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.8850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:19 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 833/16548 [23:00<7:10:01,  1.64s/it]11/15/2022 17:29:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:21 - INFO - train.train_snli_ve - loss is tensor(1.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 834/16548 [23:01<7:11:59,  1.65s/it]11/15/2022 17:29:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3945e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:22 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 835/16548 [23:03<7:12:31,  1.65s/it]11/15/2022 17:29:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.7192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:24 - INFO - train.train_snli_ve - loss is tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 836/16548 [23:05<7:11:34,  1.65s/it]11/15/2022 17:29:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:26 - INFO - train.train_snli_ve - loss is tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 837/16548 [23:06<7:10:30,  1.64s/it]11/15/2022 17:29:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:27 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 838/16548 [23:08<7:09:07,  1.64s/it]11/15/2022 17:29:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:29 - INFO - train.train_snli_ve - loss is tensor(0.9247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 839/16548 [23:10<7:08:20,  1.64s/it]11/15/2022 17:29:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.6863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:31 - INFO - train.train_snli_ve - loss is tensor(0.7720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 840/16548 [23:11<7:09:32,  1.64s/it]11/15/2022 17:29:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0664e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:32 - INFO - train.train_snli_ve - loss is tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 841/16548 [23:13<7:10:22,  1.64s/it]11/15/2022 17:29:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.4077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:34 - INFO - train.train_snli_ve - loss is tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 842/16548 [23:15<7:10:16,  1.64s/it]11/15/2022 17:29:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.6148e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:36 - INFO - train.train_snli_ve - loss is tensor(0.9482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 843/16548 [23:16<7:11:25,  1.65s/it]11/15/2022 17:29:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7170e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:37 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 844/16548 [23:18<7:11:27,  1.65s/it]11/15/2022 17:29:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.8625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:39 - INFO - train.train_snli_ve - loss is tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 845/16548 [23:19<7:07:06,  1.63s/it]11/15/2022 17:29:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:40 - INFO - train.train_snli_ve - loss is tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 846/16548 [23:21<7:08:47,  1.64s/it]11/15/2022 17:29:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.5949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:42 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 847/16548 [23:23<7:13:03,  1.65s/it]11/15/2022 17:29:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5692e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:44 - INFO - train.train_snli_ve - loss is tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 848/16548 [23:24<7:12:14,  1.65s/it]11/15/2022 17:29:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:45 - INFO - train.train_snli_ve - loss is tensor(0.8915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 849/16548 [23:26<7:11:42,  1.65s/it]11/15/2022 17:29:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.1828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:47 - INFO - train.train_snli_ve - loss is tensor(1.0781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 850/16548 [23:28<7:12:32,  1.65s/it]11/15/2022 17:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:49 - INFO - train.train_snli_ve - loss is tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 851/16548 [23:29<7:14:10,  1.66s/it]11/15/2022 17:29:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:50 - INFO - train.train_snli_ve - loss is tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 852/16548 [23:31<7:15:19,  1.66s/it]11/15/2022 17:29:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.8817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:52 - INFO - train.train_snli_ve - loss is tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 853/16548 [23:33<7:14:08,  1.66s/it]11/15/2022 17:29:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:54 - INFO - train.train_snli_ve - loss is tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 854/16548 [23:34<7:14:02,  1.66s/it]11/15/2022 17:29:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6264e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:55 - INFO - train.train_snli_ve - loss is tensor(1.1382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 855/16548 [23:36<7:11:56,  1.65s/it]11/15/2022 17:29:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1106e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:57 - INFO - train.train_snli_ve - loss is tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 856/16548 [23:38<7:14:44,  1.66s/it]11/15/2022 17:29:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.8708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:29:59 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 857/16548 [23:39<7:16:33,  1.67s/it]11/15/2022 17:30:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:00 - INFO - train.train_snli_ve - loss is tensor(0.8266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 858/16548 [23:41<7:13:56,  1.66s/it]11/15/2022 17:30:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.6137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:02 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 859/16548 [23:43<7:12:56,  1.66s/it]11/15/2022 17:30:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.6672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:04 - INFO - train.train_snli_ve - loss is tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 860/16548 [23:44<7:08:55,  1.64s/it]11/15/2022 17:30:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.3725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:05 - INFO - train.train_snli_ve - loss is tensor(0.7770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 861/16548 [23:46<7:07:39,  1.64s/it]11/15/2022 17:30:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.3569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:07 - INFO - train.train_snli_ve - loss is tensor(0.8676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 862/16548 [23:48<7:09:01,  1.64s/it]11/15/2022 17:30:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:09 - INFO - train.train_snli_ve - loss is tensor(1.0443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 863/16548 [23:49<7:09:49,  1.64s/it]11/15/2022 17:30:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.2236e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:10 - INFO - train.train_snli_ve - loss is tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 864/16548 [23:51<7:14:20,  1.66s/it]11/15/2022 17:30:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:12 - INFO - train.train_snli_ve - loss is tensor(0.9705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 865/16548 [23:53<7:14:23,  1.66s/it]11/15/2022 17:30:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:14 - INFO - train.train_snli_ve - loss is tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 866/16548 [23:54<7:13:01,  1.66s/it]11/15/2022 17:30:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.1089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:15 - INFO - train.train_snli_ve - loss is tensor(0.8317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 867/16548 [23:56<7:09:01,  1.64s/it]11/15/2022 17:30:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.1189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:17 - INFO - train.train_snli_ve - loss is tensor(0.9028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 868/16548 [23:58<7:09:52,  1.64s/it]11/15/2022 17:30:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.4312e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:18 - INFO - train.train_snli_ve - loss is tensor(0.8229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 869/16548 [23:59<7:10:32,  1.65s/it]11/15/2022 17:30:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:20 - INFO - train.train_snli_ve - loss is tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 870/16548 [24:01<7:08:42,  1.64s/it]11/15/2022 17:30:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.7447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:22 - INFO - train.train_snli_ve - loss is tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 871/16548 [24:03<7:11:21,  1.65s/it]11/15/2022 17:30:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.3371e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:23 - INFO - train.train_snli_ve - loss is tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 872/16548 [24:04<7:11:02,  1.65s/it]11/15/2022 17:30:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.1351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:25 - INFO - train.train_snli_ve - loss is tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 873/16548 [24:06<7:09:57,  1.65s/it]11/15/2022 17:30:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:27 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 874/16548 [24:07<7:07:08,  1.64s/it]11/15/2022 17:30:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.7509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:28 - INFO - train.train_snli_ve - loss is tensor(1.1494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 875/16548 [24:09<7:08:14,  1.64s/it]11/15/2022 17:30:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.9310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:30 - INFO - train.train_snli_ve - loss is tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 876/16548 [24:11<7:08:43,  1.64s/it]11/15/2022 17:30:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:32 - INFO - train.train_snli_ve - loss is tensor(0.8699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 877/16548 [24:12<7:04:52,  1.63s/it]11/15/2022 17:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:33 - INFO - train.train_snli_ve - loss is tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 878/16548 [24:14<7:09:04,  1.64s/it]11/15/2022 17:30:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.6282e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:35 - INFO - train.train_snli_ve - loss is tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 879/16548 [24:16<7:07:07,  1.64s/it]11/15/2022 17:30:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:37 - INFO - train.train_snli_ve - loss is tensor(0.9942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 880/16548 [24:17<7:11:52,  1.65s/it]11/15/2022 17:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.7092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:38 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 881/16548 [24:19<7:09:12,  1.64s/it]11/15/2022 17:30:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.2470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:40 - INFO - train.train_snli_ve - loss is tensor(0.9350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 882/16548 [24:21<7:09:39,  1.65s/it]11/15/2022 17:30:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:41 - INFO - train.train_snli_ve - loss is tensor(0.8197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 883/16548 [24:22<7:11:35,  1.65s/it]11/15/2022 17:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:43 - INFO - train.train_snli_ve - loss is tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 884/16548 [24:24<7:10:43,  1.65s/it]11/15/2022 17:30:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:45 - INFO - train.train_snli_ve - loss is tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 885/16548 [24:25<7:08:59,  1.64s/it]11/15/2022 17:30:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:46 - INFO - train.train_snli_ve - loss is tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 886/16548 [24:27<7:08:33,  1.64s/it]11/15/2022 17:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.0867e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:48 - INFO - train.train_snli_ve - loss is tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 887/16548 [24:29<7:07:24,  1.64s/it]11/15/2022 17:30:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:50 - INFO - train.train_snli_ve - loss is tensor(0.8669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 888/16548 [24:30<7:07:32,  1.64s/it]11/15/2022 17:30:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.9926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:51 - INFO - train.train_snli_ve - loss is tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 889/16548 [24:32<7:09:53,  1.65s/it]11/15/2022 17:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.6109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:53 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 890/16548 [24:34<7:12:38,  1.66s/it]11/15/2022 17:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:55 - INFO - train.train_snli_ve - loss is tensor(0.9848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 891/16548 [24:35<7:13:14,  1.66s/it]11/15/2022 17:30:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:56 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 892/16548 [24:37<7:10:08,  1.65s/it]11/15/2022 17:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:30:58 - INFO - train.train_snli_ve - loss is tensor(0.8666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 893/16548 [24:39<7:11:05,  1.65s/it]11/15/2022 17:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.3366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:00 - INFO - train.train_snli_ve - loss is tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 894/16548 [24:40<7:11:37,  1.65s/it]11/15/2022 17:31:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:01 - INFO - train.train_snli_ve - loss is tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 895/16548 [24:42<7:08:11,  1.64s/it]11/15/2022 17:31:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.0433e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:03 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 896/16548 [24:44<7:07:07,  1.64s/it]11/15/2022 17:31:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:04 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 897/16548 [24:45<7:06:29,  1.64s/it]11/15/2022 17:31:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.6306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:06 - INFO - train.train_snli_ve - loss is tensor(0.8582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 898/16548 [24:47<7:04:38,  1.63s/it]11/15/2022 17:31:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0588e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:08 - INFO - train.train_snli_ve - loss is tensor(1.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 899/16548 [24:48<7:04:56,  1.63s/it]11/15/2022 17:31:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:09 - INFO - train.train_snli_ve - loss is tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 900/16548 [24:50<7:22:18,  1.70s/it]11/15/2022 17:31:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.5959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:11 - INFO - train.train_snli_ve - loss is tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 901/16548 [24:52<7:18:27,  1.68s/it]11/15/2022 17:31:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:13 - INFO - train.train_snli_ve - loss is tensor(1.2338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 902/16548 [24:54<7:16:57,  1.68s/it]11/15/2022 17:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.7421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:15 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 903/16548 [24:55<7:13:16,  1.66s/it]11/15/2022 17:31:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.5673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:16 - INFO - train.train_snli_ve - loss is tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 904/16548 [24:57<7:11:42,  1.66s/it]11/15/2022 17:31:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.7996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:18 - INFO - train.train_snli_ve - loss is tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 905/16548 [24:58<7:07:31,  1.64s/it]11/15/2022 17:31:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:19 - INFO - train.train_snli_ve - loss is tensor(1.0816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 906/16548 [25:00<7:11:23,  1.65s/it]11/15/2022 17:31:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:21 - INFO - train.train_snli_ve - loss is tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 907/16548 [25:02<7:14:19,  1.67s/it]11/15/2022 17:31:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.2061e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:23 - INFO - train.train_snli_ve - loss is tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 908/16548 [25:04<7:12:05,  1.66s/it]11/15/2022 17:31:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:24 - INFO - train.train_snli_ve - loss is tensor(0.9039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 909/16548 [25:05<7:12:10,  1.66s/it]11/15/2022 17:31:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:26 - INFO - train.train_snli_ve - loss is tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5% 910/16548 [25:07<7:12:44,  1.66s/it]11/15/2022 17:31:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.0559e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:28 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 911/16548 [25:09<7:12:57,  1.66s/it]11/15/2022 17:31:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:29 - INFO - train.train_snli_ve - loss is tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 912/16548 [25:10<7:15:07,  1.67s/it]11/15/2022 17:31:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.0852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:31 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 913/16548 [25:12<7:15:21,  1.67s/it]11/15/2022 17:31:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.0413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:33 - INFO - train.train_snli_ve - loss is tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 914/16548 [25:14<7:13:01,  1.66s/it]11/15/2022 17:31:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.7430e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:34 - INFO - train.train_snli_ve - loss is tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 915/16548 [25:15<7:10:09,  1.65s/it]11/15/2022 17:31:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.0315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:36 - INFO - train.train_snli_ve - loss is tensor(0.8859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 916/16548 [25:17<7:08:56,  1.65s/it]11/15/2022 17:31:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.4580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:38 - INFO - train.train_snli_ve - loss is tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 917/16548 [25:18<7:05:19,  1.63s/it]11/15/2022 17:31:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.3057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:39 - INFO - train.train_snli_ve - loss is tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 918/16548 [25:20<7:08:37,  1.65s/it]11/15/2022 17:31:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.2847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:41 - INFO - train.train_snli_ve - loss is tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 919/16548 [25:22<7:08:29,  1.65s/it]11/15/2022 17:31:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.2937e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:43 - INFO - train.train_snli_ve - loss is tensor(0.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 920/16548 [25:23<7:06:27,  1.64s/it]11/15/2022 17:31:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.3214e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:44 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 921/16548 [25:25<7:05:47,  1.63s/it]11/15/2022 17:31:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.7824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:46 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 922/16548 [25:27<7:07:39,  1.64s/it]11/15/2022 17:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.3827e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:47 - INFO - train.train_snli_ve - loss is tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 923/16548 [25:28<7:06:15,  1.64s/it]11/15/2022 17:31:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.8149e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:49 - INFO - train.train_snli_ve - loss is tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 924/16548 [25:30<7:05:26,  1.63s/it]11/15/2022 17:31:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.1074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:51 - INFO - train.train_snli_ve - loss is tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 925/16548 [25:32<7:09:39,  1.65s/it]11/15/2022 17:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.5222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:52 - INFO - train.train_snli_ve - loss is tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 926/16548 [25:33<7:09:40,  1.65s/it]11/15/2022 17:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.5484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:54 - INFO - train.train_snli_ve - loss is tensor(0.4995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 927/16548 [25:35<7:08:57,  1.65s/it]11/15/2022 17:31:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.7162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:56 - INFO - train.train_snli_ve - loss is tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 928/16548 [25:36<7:08:19,  1.65s/it]11/15/2022 17:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.3739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:57 - INFO - train.train_snli_ve - loss is tensor(1.0111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 929/16548 [25:38<7:06:17,  1.64s/it]11/15/2022 17:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.7109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:31:59 - INFO - train.train_snli_ve - loss is tensor(1.1271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 930/16548 [25:40<7:08:17,  1.65s/it]11/15/2022 17:32:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:01 - INFO - train.train_snli_ve - loss is tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 931/16548 [25:41<7:06:20,  1.64s/it]11/15/2022 17:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.6914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:02 - INFO - train.train_snli_ve - loss is tensor(0.8877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 932/16548 [25:43<7:07:36,  1.64s/it]11/15/2022 17:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.7393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:04 - INFO - train.train_snli_ve - loss is tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 933/16548 [25:45<7:07:50,  1.64s/it]11/15/2022 17:32:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:06 - INFO - train.train_snli_ve - loss is tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 934/16548 [25:46<7:08:37,  1.65s/it]11/15/2022 17:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:07 - INFO - train.train_snli_ve - loss is tensor(0.9817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 935/16548 [25:48<7:12:39,  1.66s/it]11/15/2022 17:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.7415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:09 - INFO - train.train_snli_ve - loss is tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 936/16548 [25:50<7:13:18,  1.67s/it]11/15/2022 17:32:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.3227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:11 - INFO - train.train_snli_ve - loss is tensor(0.8559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 937/16548 [25:51<7:13:12,  1.67s/it]11/15/2022 17:32:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3062e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:12 - INFO - train.train_snli_ve - loss is tensor(0.8619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 938/16548 [25:53<7:13:44,  1.67s/it]11/15/2022 17:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:14 - INFO - train.train_snli_ve - loss is tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 939/16548 [25:55<7:12:09,  1.66s/it]11/15/2022 17:32:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:16 - INFO - train.train_snli_ve - loss is tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 940/16548 [25:56<7:11:26,  1.66s/it]11/15/2022 17:32:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3895e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:17 - INFO - train.train_snli_ve - loss is tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 941/16548 [25:58<7:10:38,  1.66s/it]11/15/2022 17:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.6520e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:19 - INFO - train.train_snli_ve - loss is tensor(0.9701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 942/16548 [26:00<7:13:12,  1.67s/it]11/15/2022 17:32:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:21 - INFO - train.train_snli_ve - loss is tensor(0.8983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 943/16548 [26:01<7:10:48,  1.66s/it]11/15/2022 17:32:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:22 - INFO - train.train_snli_ve - loss is tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 944/16548 [26:03<7:06:09,  1.64s/it]11/15/2022 17:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.2417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:24 - INFO - train.train_snli_ve - loss is tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 945/16548 [26:05<7:03:28,  1.63s/it]11/15/2022 17:32:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.8791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:25 - INFO - train.train_snli_ve - loss is tensor(0.9332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 946/16548 [26:06<7:06:06,  1.64s/it]11/15/2022 17:32:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:27 - INFO - train.train_snli_ve - loss is tensor(0.9069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 947/16548 [26:08<7:07:55,  1.65s/it]11/15/2022 17:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.8656e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:29 - INFO - train.train_snli_ve - loss is tensor(0.8506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 948/16548 [26:09<7:07:19,  1.64s/it]11/15/2022 17:32:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:30 - INFO - train.train_snli_ve - loss is tensor(0.7507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 949/16548 [26:11<7:10:02,  1.65s/it]11/15/2022 17:32:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:32 - INFO - train.train_snli_ve - loss is tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 950/16548 [26:13<7:11:37,  1.66s/it]11/15/2022 17:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:34 - INFO - train.train_snli_ve - loss is tensor(0.8332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 951/16548 [26:14<7:10:31,  1.66s/it]11/15/2022 17:32:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.8537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:35 - INFO - train.train_snli_ve - loss is tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 952/16548 [26:16<7:07:41,  1.65s/it]11/15/2022 17:32:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3995e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:37 - INFO - train.train_snli_ve - loss is tensor(0.9045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 953/16548 [26:18<7:09:47,  1.65s/it]11/15/2022 17:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:39 - INFO - train.train_snli_ve - loss is tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 954/16548 [26:19<7:07:01,  1.64s/it]11/15/2022 17:32:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4738e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:40 - INFO - train.train_snli_ve - loss is tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 955/16548 [26:21<7:06:02,  1.64s/it]11/15/2022 17:32:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:42 - INFO - train.train_snli_ve - loss is tensor(0.7760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 956/16548 [26:23<7:03:54,  1.63s/it]11/15/2022 17:32:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:44 - INFO - train.train_snli_ve - loss is tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 957/16548 [26:24<7:09:02,  1.65s/it]11/15/2022 17:32:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1688e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:45 - INFO - train.train_snli_ve - loss is tensor(0.8450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 958/16548 [26:26<7:06:56,  1.64s/it]11/15/2022 17:32:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:47 - INFO - train.train_snli_ve - loss is tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 959/16548 [26:28<7:06:36,  1.64s/it]11/15/2022 17:32:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:49 - INFO - train.train_snli_ve - loss is tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 960/16548 [26:29<7:09:56,  1.65s/it]11/15/2022 17:32:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.2788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:50 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 961/16548 [26:31<7:05:06,  1.64s/it]11/15/2022 17:32:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.6041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:52 - INFO - train.train_snli_ve - loss is tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 962/16548 [26:32<7:03:15,  1.63s/it]11/15/2022 17:32:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.6920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:53 - INFO - train.train_snli_ve - loss is tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 963/16548 [26:34<7:00:53,  1.62s/it]11/15/2022 17:32:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:55 - INFO - train.train_snli_ve - loss is tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 964/16548 [26:36<7:01:58,  1.62s/it]11/15/2022 17:32:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.2889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:57 - INFO - train.train_snli_ve - loss is tensor(0.9295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 965/16548 [26:37<7:03:34,  1.63s/it]11/15/2022 17:32:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.1034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:32:58 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 966/16548 [26:39<7:03:08,  1.63s/it]11/15/2022 17:33:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.9490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:00 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 967/16548 [26:41<7:03:38,  1.63s/it]11/15/2022 17:33:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.3901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:01 - INFO - train.train_snli_ve - loss is tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 968/16548 [26:42<7:00:13,  1.62s/it]11/15/2022 17:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.8564e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:03 - INFO - train.train_snli_ve - loss is tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 969/16548 [26:44<7:00:52,  1.62s/it]11/15/2022 17:33:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.2085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:05 - INFO - train.train_snli_ve - loss is tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 970/16548 [26:45<7:00:04,  1.62s/it]11/15/2022 17:33:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.6242e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:06 - INFO - train.train_snli_ve - loss is tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 971/16548 [26:47<7:03:28,  1.63s/it]11/15/2022 17:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.1417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:08 - INFO - train.train_snli_ve - loss is tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 972/16548 [26:49<7:03:22,  1.63s/it]11/15/2022 17:33:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.5447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:10 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 973/16548 [26:50<7:02:04,  1.63s/it]11/15/2022 17:33:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.1573e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:11 - INFO - train.train_snli_ve - loss is tensor(0.9849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 974/16548 [26:52<7:04:57,  1.64s/it]11/15/2022 17:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.3289e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:13 - INFO - train.train_snli_ve - loss is tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 975/16548 [26:54<7:06:17,  1.64s/it]11/15/2022 17:33:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.6952e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:15 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 976/16548 [26:55<7:07:37,  1.65s/it]11/15/2022 17:33:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7504e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:16 - INFO - train.train_snli_ve - loss is tensor(0.7845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 977/16548 [26:57<7:07:59,  1.65s/it]11/15/2022 17:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.0938e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:18 - INFO - train.train_snli_ve - loss is tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 978/16548 [26:59<7:09:39,  1.66s/it]11/15/2022 17:33:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:20 - INFO - train.train_snli_ve - loss is tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 979/16548 [27:00<7:08:24,  1.65s/it]11/15/2022 17:33:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.6683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:21 - INFO - train.train_snli_ve - loss is tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 980/16548 [27:02<7:08:28,  1.65s/it]11/15/2022 17:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:23 - INFO - train.train_snli_ve - loss is tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 981/16548 [27:04<7:07:33,  1.65s/it]11/15/2022 17:33:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.5554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:24 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 982/16548 [27:05<7:06:38,  1.64s/it]11/15/2022 17:33:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.3895e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:26 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 983/16548 [27:07<7:05:01,  1.64s/it]11/15/2022 17:33:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.8278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:28 - INFO - train.train_snli_ve - loss is tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 984/16548 [27:08<7:04:50,  1.64s/it]11/15/2022 17:33:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.9365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:29 - INFO - train.train_snli_ve - loss is tensor(0.9895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 985/16548 [27:10<7:06:29,  1.64s/it]11/15/2022 17:33:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.2423e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:31 - INFO - train.train_snli_ve - loss is tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 986/16548 [27:12<7:08:26,  1.65s/it]11/15/2022 17:33:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:33 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 987/16548 [27:13<7:04:46,  1.64s/it]11/15/2022 17:33:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:34 - INFO - train.train_snli_ve - loss is tensor(0.8663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 988/16548 [27:15<7:07:56,  1.65s/it]11/15/2022 17:33:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.9363e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:36 - INFO - train.train_snli_ve - loss is tensor(0.8973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 989/16548 [27:17<7:09:25,  1.66s/it]11/15/2022 17:33:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:38 - INFO - train.train_snli_ve - loss is tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 990/16548 [27:18<7:09:48,  1.66s/it]11/15/2022 17:33:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.1828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:39 - INFO - train.train_snli_ve - loss is tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 991/16548 [27:20<7:05:15,  1.64s/it]11/15/2022 17:33:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.0837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:41 - INFO - train.train_snli_ve - loss is tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 992/16548 [27:22<7:07:15,  1.65s/it]11/15/2022 17:33:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.9338e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:43 - INFO - train.train_snli_ve - loss is tensor(0.9340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 993/16548 [27:23<7:05:53,  1.64s/it]11/15/2022 17:33:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.0398e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:44 - INFO - train.train_snli_ve - loss is tensor(1.0069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 994/16548 [27:25<7:04:06,  1.64s/it]11/15/2022 17:33:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.1913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:46 - INFO - train.train_snli_ve - loss is tensor(0.8266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 995/16548 [27:27<7:04:00,  1.64s/it]11/15/2022 17:33:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.2033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:47 - INFO - train.train_snli_ve - loss is tensor(0.8864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 996/16548 [27:28<7:02:27,  1.63s/it]11/15/2022 17:33:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.6425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:49 - INFO - train.train_snli_ve - loss is tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 997/16548 [27:30<7:02:24,  1.63s/it]11/15/2022 17:33:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:51 - INFO - train.train_snli_ve - loss is tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 998/16548 [27:31<7:02:28,  1.63s/it]11/15/2022 17:33:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.0344e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:52 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 999/16548 [27:33<7:04:58,  1.64s/it]11/15/2022 17:33:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.2586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:54 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1000/16548 [27:35<7:13:26,  1.67s/it]11/15/2022 17:33:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.2947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:56 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1001/16548 [27:37<7:12:15,  1.67s/it]11/15/2022 17:33:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.4018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:57 - INFO - train.train_snli_ve - loss is tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1002/16548 [27:38<7:08:10,  1.65s/it]11/15/2022 17:33:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:33:59 - INFO - train.train_snli_ve - loss is tensor(0.7841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1003/16548 [27:40<7:07:42,  1.65s/it]11/15/2022 17:34:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.2804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:01 - INFO - train.train_snli_ve - loss is tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1004/16548 [27:41<7:09:50,  1.66s/it]11/15/2022 17:34:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.4948e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:02 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1005/16548 [27:43<7:06:04,  1.64s/it]11/15/2022 17:34:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.9898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:04 - INFO - train.train_snli_ve - loss is tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1006/16548 [27:45<7:09:21,  1.66s/it]11/15/2022 17:34:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.9896e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:06 - INFO - train.train_snli_ve - loss is tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1007/16548 [27:46<7:05:58,  1.64s/it]11/15/2022 17:34:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.4802e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:07 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1008/16548 [27:48<7:07:58,  1.65s/it]11/15/2022 17:34:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.3594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:09 - INFO - train.train_snli_ve - loss is tensor(0.8225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1009/16548 [27:50<7:04:13,  1.64s/it]11/15/2022 17:34:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.0511e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:11 - INFO - train.train_snli_ve - loss is tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1010/16548 [27:51<7:01:55,  1.63s/it]11/15/2022 17:34:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.0481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:12 - INFO - train.train_snli_ve - loss is tensor(0.9454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1011/16548 [27:53<7:01:30,  1.63s/it]11/15/2022 17:34:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.7198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:14 - INFO - train.train_snli_ve - loss is tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1012/16548 [27:55<7:02:55,  1.63s/it]11/15/2022 17:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.9241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:15 - INFO - train.train_snli_ve - loss is tensor(0.9179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1013/16548 [27:56<7:02:19,  1.63s/it]11/15/2022 17:34:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.6397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:17 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1014/16548 [27:58<7:00:39,  1.62s/it]11/15/2022 17:34:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.0610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:19 - INFO - train.train_snli_ve - loss is tensor(0.8607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1015/16548 [27:59<6:58:44,  1.62s/it]11/15/2022 17:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.0200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:20 - INFO - train.train_snli_ve - loss is tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1016/16548 [28:01<6:59:47,  1.62s/it]11/15/2022 17:34:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.8041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:22 - INFO - train.train_snli_ve - loss is tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1017/16548 [28:03<7:04:53,  1.64s/it]11/15/2022 17:34:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.5775e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:24 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1018/16548 [28:04<7:05:56,  1.65s/it]11/15/2022 17:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.6572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:25 - INFO - train.train_snli_ve - loss is tensor(1.0667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1019/16548 [28:06<7:02:50,  1.63s/it]11/15/2022 17:34:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:27 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1020/16548 [28:08<7:02:01,  1.63s/it]11/15/2022 17:34:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.6943e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:28 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1021/16548 [28:09<7:01:14,  1.63s/it]11/15/2022 17:34:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.3884e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:30 - INFO - train.train_snli_ve - loss is tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1022/16548 [28:11<6:59:47,  1.62s/it]11/15/2022 17:34:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.3675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:32 - INFO - train.train_snli_ve - loss is tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1023/16548 [28:12<7:00:20,  1.62s/it]11/15/2022 17:34:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:33 - INFO - train.train_snli_ve - loss is tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1024/16548 [28:14<6:58:43,  1.62s/it]11/15/2022 17:34:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.5030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:35 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1025/16548 [28:16<7:02:41,  1.63s/it]11/15/2022 17:34:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.5698e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:37 - INFO - train.train_snli_ve - loss is tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1026/16548 [28:17<7:02:49,  1.63s/it]11/15/2022 17:34:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:38 - INFO - train.train_snli_ve - loss is tensor(0.7451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1027/16548 [28:19<7:05:51,  1.65s/it]11/15/2022 17:34:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.6169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:40 - INFO - train.train_snli_ve - loss is tensor(0.9108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1028/16548 [28:21<7:01:38,  1.63s/it]11/15/2022 17:34:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.0181e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:42 - INFO - train.train_snli_ve - loss is tensor(0.9227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1029/16548 [28:22<7:02:48,  1.63s/it]11/15/2022 17:34:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:43 - INFO - train.train_snli_ve - loss is tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1030/16548 [28:24<7:03:40,  1.64s/it]11/15/2022 17:34:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.5044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:45 - INFO - train.train_snli_ve - loss is tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1031/16548 [28:26<7:07:57,  1.65s/it]11/15/2022 17:34:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9587e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:46 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1032/16548 [28:27<7:04:29,  1.64s/it]11/15/2022 17:34:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:48 - INFO - train.train_snli_ve - loss is tensor(0.8338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1033/16548 [28:29<7:02:57,  1.64s/it]11/15/2022 17:34:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.8167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:50 - INFO - train.train_snli_ve - loss is tensor(1.1411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1034/16548 [28:30<7:04:02,  1.64s/it]11/15/2022 17:34:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.6088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:51 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1035/16548 [28:32<7:00:55,  1.63s/it]11/15/2022 17:34:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.6925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:53 - INFO - train.train_snli_ve - loss is tensor(0.7833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1036/16548 [28:34<6:58:43,  1.62s/it]11/15/2022 17:34:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.3026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:55 - INFO - train.train_snli_ve - loss is tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1037/16548 [28:35<7:00:33,  1.63s/it]11/15/2022 17:34:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.6400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:56 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1038/16548 [28:37<7:09:24,  1.66s/it]11/15/2022 17:34:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.8981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:34:58 - INFO - train.train_snli_ve - loss is tensor(1.0209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1039/16548 [28:39<7:03:33,  1.64s/it]11/15/2022 17:35:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.3060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:00 - INFO - train.train_snli_ve - loss is tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1040/16548 [28:40<7:03:03,  1.64s/it]11/15/2022 17:35:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.7196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:01 - INFO - train.train_snli_ve - loss is tensor(0.7827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1041/16548 [28:42<7:03:35,  1.64s/it]11/15/2022 17:35:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.9528e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:03 - INFO - train.train_snli_ve - loss is tensor(0.9264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1042/16548 [28:44<7:06:41,  1.65s/it]11/15/2022 17:35:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.9507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:04 - INFO - train.train_snli_ve - loss is tensor(0.7960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1043/16548 [28:45<7:03:16,  1.64s/it]11/15/2022 17:35:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.6243e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:06 - INFO - train.train_snli_ve - loss is tensor(0.8627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1044/16548 [28:47<7:04:48,  1.64s/it]11/15/2022 17:35:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:08 - INFO - train.train_snli_ve - loss is tensor(0.9734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1045/16548 [28:48<7:01:37,  1.63s/it]11/15/2022 17:35:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:09 - INFO - train.train_snli_ve - loss is tensor(0.9036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1046/16548 [28:50<7:01:54,  1.63s/it]11/15/2022 17:35:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.8004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:11 - INFO - train.train_snli_ve - loss is tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1047/16548 [28:52<7:05:03,  1.65s/it]11/15/2022 17:35:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.6525e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:13 - INFO - train.train_snli_ve - loss is tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1048/16548 [28:53<7:05:33,  1.65s/it]11/15/2022 17:35:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.9707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:14 - INFO - train.train_snli_ve - loss is tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1049/16548 [28:55<7:07:31,  1.66s/it]11/15/2022 17:35:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:16 - INFO - train.train_snli_ve - loss is tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1050/16548 [28:57<7:04:13,  1.64s/it]11/15/2022 17:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.1875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:18 - INFO - train.train_snli_ve - loss is tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1051/16548 [28:58<7:02:59,  1.64s/it]11/15/2022 17:35:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1699e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:19 - INFO - train.train_snli_ve - loss is tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1052/16548 [29:00<7:00:30,  1.63s/it]11/15/2022 17:35:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:21 - INFO - train.train_snli_ve - loss is tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1053/16548 [29:02<7:04:44,  1.64s/it]11/15/2022 17:35:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2028e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:23 - INFO - train.train_snli_ve - loss is tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1054/16548 [29:03<7:00:57,  1.63s/it]11/15/2022 17:35:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.5569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:24 - INFO - train.train_snli_ve - loss is tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1055/16548 [29:05<6:58:40,  1.62s/it]11/15/2022 17:35:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.4958e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:26 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1056/16548 [29:06<7:00:17,  1.63s/it]11/15/2022 17:35:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:27 - INFO - train.train_snli_ve - loss is tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1057/16548 [29:08<6:57:56,  1.62s/it]11/15/2022 17:35:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:29 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1058/16548 [29:10<7:01:23,  1.63s/it]11/15/2022 17:35:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:31 - INFO - train.train_snli_ve - loss is tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1059/16548 [29:11<7:02:22,  1.64s/it]11/15/2022 17:35:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:32 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1060/16548 [29:13<7:01:20,  1.63s/it]11/15/2022 17:35:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.4739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:34 - INFO - train.train_snli_ve - loss is tensor(0.8092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1061/16548 [29:15<6:59:30,  1.63s/it]11/15/2022 17:35:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:36 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1062/16548 [29:16<7:02:15,  1.64s/it]11/15/2022 17:35:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.7282e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:37 - INFO - train.train_snli_ve - loss is tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1063/16548 [29:18<7:04:14,  1.64s/it]11/15/2022 17:35:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:39 - INFO - train.train_snli_ve - loss is tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1064/16548 [29:20<7:03:09,  1.64s/it]11/15/2022 17:35:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.7688e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:40 - INFO - train.train_snli_ve - loss is tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1065/16548 [29:21<7:01:18,  1.63s/it]11/15/2022 17:35:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.5246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:42 - INFO - train.train_snli_ve - loss is tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1066/16548 [29:23<7:04:52,  1.65s/it]11/15/2022 17:35:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2923e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:44 - INFO - train.train_snli_ve - loss is tensor(0.8988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1067/16548 [29:24<7:02:44,  1.64s/it]11/15/2022 17:35:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.0115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:45 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1068/16548 [29:26<7:01:34,  1.63s/it]11/15/2022 17:35:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.8140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:47 - INFO - train.train_snli_ve - loss is tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1069/16548 [29:28<6:58:23,  1.62s/it]11/15/2022 17:35:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.2427e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:49 - INFO - train.train_snli_ve - loss is tensor(0.9476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1070/16548 [29:29<6:59:22,  1.63s/it]11/15/2022 17:35:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.5665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:50 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1071/16548 [29:31<6:56:46,  1.62s/it]11/15/2022 17:35:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:52 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1072/16548 [29:33<6:56:34,  1.62s/it]11/15/2022 17:35:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:53 - INFO - train.train_snli_ve - loss is tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1073/16548 [29:34<7:01:59,  1.64s/it]11/15/2022 17:35:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.0990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:55 - INFO - train.train_snli_ve - loss is tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1074/16548 [29:36<7:01:09,  1.63s/it]11/15/2022 17:35:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.2017e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:57 - INFO - train.train_snli_ve - loss is tensor(0.9890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6% 1075/16548 [29:37<6:58:00,  1.62s/it]11/15/2022 17:35:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.9356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:35:58 - INFO - train.train_snli_ve - loss is tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1076/16548 [29:39<6:59:57,  1.63s/it]11/15/2022 17:36:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.5134e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:00 - INFO - train.train_snli_ve - loss is tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1077/16548 [29:41<6:59:10,  1.63s/it]11/15/2022 17:36:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.0185e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:02 - INFO - train.train_snli_ve - loss is tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1078/16548 [29:42<6:57:27,  1.62s/it]11/15/2022 17:36:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.4023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:03 - INFO - train.train_snli_ve - loss is tensor(0.7792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1079/16548 [29:44<6:56:10,  1.61s/it]11/15/2022 17:36:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.9107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:05 - INFO - train.train_snli_ve - loss is tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1080/16548 [29:46<6:56:21,  1.62s/it]11/15/2022 17:36:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.1691e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:06 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1081/16548 [29:47<6:55:09,  1.61s/it]11/15/2022 17:36:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.9812e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:08 - INFO - train.train_snli_ve - loss is tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1082/16548 [29:49<6:59:16,  1.63s/it]11/15/2022 17:36:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.1786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:10 - INFO - train.train_snli_ve - loss is tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1083/16548 [29:50<6:58:58,  1.63s/it]11/15/2022 17:36:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.2167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:11 - INFO - train.train_snli_ve - loss is tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1084/16548 [29:52<6:58:58,  1.63s/it]11/15/2022 17:36:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:13 - INFO - train.train_snli_ve - loss is tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1085/16548 [29:54<6:57:59,  1.62s/it]11/15/2022 17:36:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.6105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:15 - INFO - train.train_snli_ve - loss is tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1086/16548 [29:55<7:03:06,  1.64s/it]11/15/2022 17:36:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.0112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:16 - INFO - train.train_snli_ve - loss is tensor(0.9824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1087/16548 [29:57<7:01:38,  1.64s/it]11/15/2022 17:36:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.9599e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:18 - INFO - train.train_snli_ve - loss is tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1088/16548 [29:59<7:00:35,  1.63s/it]11/15/2022 17:36:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.8271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:19 - INFO - train.train_snli_ve - loss is tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1089/16548 [30:00<7:00:38,  1.63s/it]11/15/2022 17:36:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.8922e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:21 - INFO - train.train_snli_ve - loss is tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1090/16548 [30:02<7:03:28,  1.64s/it]11/15/2022 17:36:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.1217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:23 - INFO - train.train_snli_ve - loss is tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1091/16548 [30:04<7:02:39,  1.64s/it]11/15/2022 17:36:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.7561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:24 - INFO - train.train_snli_ve - loss is tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1092/16548 [30:05<7:02:01,  1.64s/it]11/15/2022 17:36:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:26 - INFO - train.train_snli_ve - loss is tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1093/16548 [30:07<6:59:30,  1.63s/it]11/15/2022 17:36:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:28 - INFO - train.train_snli_ve - loss is tensor(0.8468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1094/16548 [30:08<6:59:28,  1.63s/it]11/15/2022 17:36:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.9470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:29 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1095/16548 [30:10<6:57:23,  1.62s/it]11/15/2022 17:36:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.9159e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:31 - INFO - train.train_snli_ve - loss is tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1096/16548 [30:12<7:00:11,  1.63s/it]11/15/2022 17:36:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7402e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:33 - INFO - train.train_snli_ve - loss is tensor(0.9233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1097/16548 [30:13<6:58:23,  1.62s/it]11/15/2022 17:36:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1807e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:34 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1098/16548 [30:15<7:05:34,  1.65s/it]11/15/2022 17:36:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.5923e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:36 - INFO - train.train_snli_ve - loss is tensor(1.1092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1099/16548 [30:17<7:01:58,  1.64s/it]11/15/2022 17:36:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:37 - INFO - train.train_snli_ve - loss is tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1100/16548 [30:18<7:06:37,  1.66s/it]11/15/2022 17:36:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.5859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:39 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1101/16548 [30:20<7:01:34,  1.64s/it]11/15/2022 17:36:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:41 - INFO - train.train_snli_ve - loss is tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1102/16548 [30:21<6:58:28,  1.63s/it]11/15/2022 17:36:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.4438e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:42 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1103/16548 [30:23<7:00:52,  1.63s/it]11/15/2022 17:36:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.9455e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:44 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1104/16548 [30:25<7:04:17,  1.65s/it]11/15/2022 17:36:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.8920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:46 - INFO - train.train_snli_ve - loss is tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1105/16548 [30:26<7:01:12,  1.64s/it]11/15/2022 17:36:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6285e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:47 - INFO - train.train_snli_ve - loss is tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1106/16548 [30:28<6:58:52,  1.63s/it]11/15/2022 17:36:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:49 - INFO - train.train_snli_ve - loss is tensor(0.9393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1107/16548 [30:30<7:01:23,  1.64s/it]11/15/2022 17:36:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.5881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:51 - INFO - train.train_snli_ve - loss is tensor(0.8607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1108/16548 [30:31<7:01:19,  1.64s/it]11/15/2022 17:36:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.7202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:52 - INFO - train.train_snli_ve - loss is tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1109/16548 [30:33<7:03:26,  1.65s/it]11/15/2022 17:36:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.8480e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:54 - INFO - train.train_snli_ve - loss is tensor(0.8134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1110/16548 [30:35<7:05:05,  1.65s/it]11/15/2022 17:36:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.9699e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:56 - INFO - train.train_snli_ve - loss is tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1111/16548 [30:36<7:04:01,  1.65s/it]11/15/2022 17:36:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.8325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:57 - INFO - train.train_snli_ve - loss is tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1112/16548 [30:38<7:04:30,  1.65s/it]11/15/2022 17:36:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.7378e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:36:59 - INFO - train.train_snli_ve - loss is tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1113/16548 [30:40<7:03:23,  1.65s/it]11/15/2022 17:37:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.5545e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:01 - INFO - train.train_snli_ve - loss is tensor(0.9168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1114/16548 [30:41<7:06:58,  1.66s/it]11/15/2022 17:37:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.1440e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:02 - INFO - train.train_snli_ve - loss is tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1115/16548 [30:43<7:07:04,  1.66s/it]11/15/2022 17:37:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.2072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:04 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1116/16548 [30:45<7:05:37,  1.65s/it]11/15/2022 17:37:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.4618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:05 - INFO - train.train_snli_ve - loss is tensor(0.8770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1117/16548 [30:46<7:05:04,  1.65s/it]11/15/2022 17:37:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:07 - INFO - train.train_snli_ve - loss is tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1118/16548 [30:48<7:03:03,  1.65s/it]11/15/2022 17:37:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.7600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:09 - INFO - train.train_snli_ve - loss is tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1119/16548 [30:50<7:02:58,  1.64s/it]11/15/2022 17:37:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.8721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:10 - INFO - train.train_snli_ve - loss is tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1120/16548 [30:51<7:01:12,  1.64s/it]11/15/2022 17:37:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.7672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:12 - INFO - train.train_snli_ve - loss is tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1121/16548 [30:53<7:03:17,  1.65s/it]11/15/2022 17:37:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.4254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:14 - INFO - train.train_snli_ve - loss is tensor(0.9843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1122/16548 [30:54<7:00:21,  1.64s/it]11/15/2022 17:37:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.4231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:15 - INFO - train.train_snli_ve - loss is tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1123/16548 [30:56<7:00:37,  1.64s/it]11/15/2022 17:37:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:17 - INFO - train.train_snli_ve - loss is tensor(0.8132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1124/16548 [30:58<7:03:52,  1.65s/it]11/15/2022 17:37:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.3947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:19 - INFO - train.train_snli_ve - loss is tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1125/16548 [30:59<7:03:35,  1.65s/it]11/15/2022 17:37:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.3742e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:20 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1126/16548 [31:01<7:02:18,  1.64s/it]11/15/2022 17:37:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:22 - INFO - train.train_snli_ve - loss is tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1127/16548 [31:03<6:59:06,  1.63s/it]11/15/2022 17:37:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4358e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:24 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1128/16548 [31:04<7:04:37,  1.65s/it]11/15/2022 17:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.3989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:25 - INFO - train.train_snli_ve - loss is tensor(0.9621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1129/16548 [31:06<7:01:54,  1.64s/it]11/15/2022 17:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5802e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:27 - INFO - train.train_snli_ve - loss is tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1130/16548 [31:08<6:57:32,  1.62s/it]11/15/2022 17:37:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:28 - INFO - train.train_snli_ve - loss is tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1131/16548 [31:09<6:57:08,  1.62s/it]11/15/2022 17:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:30 - INFO - train.train_snli_ve - loss is tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1132/16548 [31:11<6:59:07,  1.63s/it]11/15/2022 17:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:32 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1133/16548 [31:12<6:58:17,  1.63s/it]11/15/2022 17:37:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.2390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:33 - INFO - train.train_snli_ve - loss is tensor(0.8255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1134/16548 [31:14<6:56:32,  1.62s/it]11/15/2022 17:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.0172e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:35 - INFO - train.train_snli_ve - loss is tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1135/16548 [31:16<6:56:29,  1.62s/it]11/15/2022 17:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.4211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:37 - INFO - train.train_snli_ve - loss is tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1136/16548 [31:17<6:58:38,  1.63s/it]11/15/2022 17:37:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.9155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:38 - INFO - train.train_snli_ve - loss is tensor(0.7736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1137/16548 [31:19<7:00:53,  1.64s/it]11/15/2022 17:37:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.1774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:40 - INFO - train.train_snli_ve - loss is tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1138/16548 [31:21<7:03:45,  1.65s/it]11/15/2022 17:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.2724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:42 - INFO - train.train_snli_ve - loss is tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1139/16548 [31:22<7:02:04,  1.64s/it]11/15/2022 17:37:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.7763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:43 - INFO - train.train_snli_ve - loss is tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1140/16548 [31:24<7:04:00,  1.65s/it]11/15/2022 17:37:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.9059e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:45 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1141/16548 [31:26<7:06:38,  1.66s/it]11/15/2022 17:37:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.1847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:46 - INFO - train.train_snli_ve - loss is tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1142/16548 [31:27<7:03:25,  1.65s/it]11/15/2022 17:37:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.1844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:48 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1143/16548 [31:29<7:02:02,  1.64s/it]11/15/2022 17:37:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.6439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:50 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1144/16548 [31:31<7:06:52,  1.66s/it]11/15/2022 17:37:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.2554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:51 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1145/16548 [31:32<7:04:58,  1.66s/it]11/15/2022 17:37:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.3336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:53 - INFO - train.train_snli_ve - loss is tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1146/16548 [31:34<7:12:39,  1.69s/it]11/15/2022 17:37:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.4436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:55 - INFO - train.train_snli_ve - loss is tensor(0.7798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1147/16548 [31:36<7:11:54,  1.68s/it]11/15/2022 17:37:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.5857e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:57 - INFO - train.train_snli_ve - loss is tensor(0.6609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1148/16548 [31:37<7:09:16,  1.67s/it]11/15/2022 17:37:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.1459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:37:58 - INFO - train.train_snli_ve - loss is tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1149/16548 [31:39<7:07:24,  1.67s/it]11/15/2022 17:38:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.6417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:00 - INFO - train.train_snli_ve - loss is tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1150/16548 [31:41<7:06:32,  1.66s/it]11/15/2022 17:38:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.0351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:01 - INFO - train.train_snli_ve - loss is tensor(0.9767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1151/16548 [31:42<7:01:31,  1.64s/it]11/15/2022 17:38:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.9589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:03 - INFO - train.train_snli_ve - loss is tensor(0.8431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1152/16548 [31:44<7:00:42,  1.64s/it]11/15/2022 17:38:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.1262e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:05 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1153/16548 [31:45<7:00:02,  1.64s/it]11/15/2022 17:38:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.4805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:06 - INFO - train.train_snli_ve - loss is tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1154/16548 [31:47<7:01:46,  1.64s/it]11/15/2022 17:38:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.4032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:08 - INFO - train.train_snli_ve - loss is tensor(0.9483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1155/16548 [31:49<7:06:25,  1.66s/it]11/15/2022 17:38:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.3865e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:10 - INFO - train.train_snli_ve - loss is tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1156/16548 [31:50<7:06:34,  1.66s/it]11/15/2022 17:38:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.2651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:11 - INFO - train.train_snli_ve - loss is tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1157/16548 [31:52<7:00:10,  1.64s/it]11/15/2022 17:38:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.0686e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:13 - INFO - train.train_snli_ve - loss is tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1158/16548 [31:54<7:00:50,  1.64s/it]11/15/2022 17:38:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.4968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:15 - INFO - train.train_snli_ve - loss is tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1159/16548 [31:55<7:03:34,  1.65s/it]11/15/2022 17:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.7144e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:16 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1160/16548 [31:57<7:04:31,  1.66s/it]11/15/2022 17:38:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.2318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:18 - INFO - train.train_snli_ve - loss is tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1161/16548 [31:59<7:03:40,  1.65s/it]11/15/2022 17:38:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:20 - INFO - train.train_snli_ve - loss is tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1162/16548 [32:00<7:06:02,  1.66s/it]11/15/2022 17:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.0703e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:21 - INFO - train.train_snli_ve - loss is tensor(0.8577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1163/16548 [32:02<7:04:15,  1.65s/it]11/15/2022 17:38:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.0273e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:23 - INFO - train.train_snli_ve - loss is tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1164/16548 [32:04<7:04:15,  1.65s/it]11/15/2022 17:38:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.2654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:25 - INFO - train.train_snli_ve - loss is tensor(0.9204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1165/16548 [32:05<7:04:30,  1.66s/it]11/15/2022 17:38:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.2815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:26 - INFO - train.train_snli_ve - loss is tensor(1.0285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1166/16548 [32:07<7:05:24,  1.66s/it]11/15/2022 17:38:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.7316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:28 - INFO - train.train_snli_ve - loss is tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1167/16548 [32:09<7:03:34,  1.65s/it]11/15/2022 17:38:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.0603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:30 - INFO - train.train_snli_ve - loss is tensor(0.7809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1168/16548 [32:10<7:04:29,  1.66s/it]11/15/2022 17:38:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.9201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:31 - INFO - train.train_snli_ve - loss is tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1169/16548 [32:12<7:02:52,  1.65s/it]11/15/2022 17:38:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.0346e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:33 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1170/16548 [32:14<7:01:46,  1.65s/it]11/15/2022 17:38:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.6162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:34 - INFO - train.train_snli_ve - loss is tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1171/16548 [32:15<7:01:59,  1.65s/it]11/15/2022 17:38:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.2997e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:36 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1172/16548 [32:17<7:03:28,  1.65s/it]11/15/2022 17:38:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.5603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:38 - INFO - train.train_snli_ve - loss is tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1173/16548 [32:19<7:05:08,  1.66s/it]11/15/2022 17:38:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.9771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:39 - INFO - train.train_snli_ve - loss is tensor(1.0185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1174/16548 [32:20<7:04:45,  1.66s/it]11/15/2022 17:38:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:41 - INFO - train.train_snli_ve - loss is tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1175/16548 [32:22<7:10:02,  1.68s/it]11/15/2022 17:38:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0286e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:43 - INFO - train.train_snli_ve - loss is tensor(0.9535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1176/16548 [32:24<7:06:55,  1.67s/it]11/15/2022 17:38:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.0232e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:44 - INFO - train.train_snli_ve - loss is tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1177/16548 [32:25<7:02:23,  1.65s/it]11/15/2022 17:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.2018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:46 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1178/16548 [32:27<7:06:13,  1.66s/it]11/15/2022 17:38:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.3188e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:48 - INFO - train.train_snli_ve - loss is tensor(0.9375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1179/16548 [32:29<7:06:08,  1.66s/it]11/15/2022 17:38:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.2586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:49 - INFO - train.train_snli_ve - loss is tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1180/16548 [32:30<7:02:43,  1.65s/it]11/15/2022 17:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.8236e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:51 - INFO - train.train_snli_ve - loss is tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1181/16548 [32:32<7:02:01,  1.65s/it]11/15/2022 17:38:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.7764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:53 - INFO - train.train_snli_ve - loss is tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1182/16548 [32:33<6:59:26,  1.64s/it]11/15/2022 17:38:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.2428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:54 - INFO - train.train_snli_ve - loss is tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1183/16548 [32:35<7:01:35,  1.65s/it]11/15/2022 17:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.3156e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:56 - INFO - train.train_snli_ve - loss is tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1184/16548 [32:37<7:00:25,  1.64s/it]11/15/2022 17:38:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.9586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:58 - INFO - train.train_snli_ve - loss is tensor(0.7814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1185/16548 [32:38<6:56:37,  1.63s/it]11/15/2022 17:38:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.7411e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:38:59 - INFO - train.train_snli_ve - loss is tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1186/16548 [32:40<6:56:25,  1.63s/it]11/15/2022 17:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.2605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:01 - INFO - train.train_snli_ve - loss is tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1187/16548 [32:42<6:59:15,  1.64s/it]11/15/2022 17:39:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.7164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:03 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1188/16548 [32:43<7:00:31,  1.64s/it]11/15/2022 17:39:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.1598e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:04 - INFO - train.train_snli_ve - loss is tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1189/16548 [32:45<6:58:15,  1.63s/it]11/15/2022 17:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.0144e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:06 - INFO - train.train_snli_ve - loss is tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1190/16548 [32:47<7:00:15,  1.64s/it]11/15/2022 17:39:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9770e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:07 - INFO - train.train_snli_ve - loss is tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1191/16548 [32:48<7:02:15,  1.65s/it]11/15/2022 17:39:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.7601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:09 - INFO - train.train_snli_ve - loss is tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1192/16548 [32:50<7:02:10,  1.65s/it]11/15/2022 17:39:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.0230e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:11 - INFO - train.train_snli_ve - loss is tensor(0.9847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1193/16548 [32:51<7:02:41,  1.65s/it]11/15/2022 17:39:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:12 - INFO - train.train_snli_ve - loss is tensor(0.9656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1194/16548 [32:53<6:59:56,  1.64s/it]11/15/2022 17:39:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.2521e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:14 - INFO - train.train_snli_ve - loss is tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1195/16548 [32:55<6:59:03,  1.64s/it]11/15/2022 17:39:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:16 - INFO - train.train_snli_ve - loss is tensor(0.9182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1196/16548 [32:56<6:58:40,  1.64s/it]11/15/2022 17:39:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.6443e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:17 - INFO - train.train_snli_ve - loss is tensor(0.9862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1197/16548 [32:58<7:00:15,  1.64s/it]11/15/2022 17:39:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.7409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:19 - INFO - train.train_snli_ve - loss is tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1198/16548 [33:00<7:01:26,  1.65s/it]11/15/2022 17:39:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.5863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:21 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1199/16548 [33:01<7:01:34,  1.65s/it]11/15/2022 17:39:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.9450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:22 - INFO - train.train_snli_ve - loss is tensor(1.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1200/16548 [33:03<7:04:00,  1.66s/it]11/15/2022 17:39:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.7785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:24 - INFO - train.train_snli_ve - loss is tensor(0.8084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1201/16548 [33:05<7:04:54,  1.66s/it]11/15/2022 17:39:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5711e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:26 - INFO - train.train_snli_ve - loss is tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1202/16548 [33:06<7:05:23,  1.66s/it]11/15/2022 17:39:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.7620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:27 - INFO - train.train_snli_ve - loss is tensor(0.8572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1203/16548 [33:08<7:05:31,  1.66s/it]11/15/2022 17:39:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.9653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:29 - INFO - train.train_snli_ve - loss is tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1204/16548 [33:10<7:03:28,  1.66s/it]11/15/2022 17:39:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:31 - INFO - train.train_snli_ve - loss is tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1205/16548 [33:11<6:59:57,  1.64s/it]11/15/2022 17:39:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.5111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:32 - INFO - train.train_snli_ve - loss is tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1206/16548 [33:13<6:59:39,  1.64s/it]11/15/2022 17:39:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.0240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:34 - INFO - train.train_snli_ve - loss is tensor(0.7034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1207/16548 [33:15<7:00:18,  1.64s/it]11/15/2022 17:39:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.2955e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:35 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1208/16548 [33:16<7:01:24,  1.65s/it]11/15/2022 17:39:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:37 - INFO - train.train_snli_ve - loss is tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1209/16548 [33:18<7:00:06,  1.64s/it]11/15/2022 17:39:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:39 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1210/16548 [33:19<6:58:52,  1.64s/it]11/15/2022 17:39:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:40 - INFO - train.train_snli_ve - loss is tensor(0.7613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1211/16548 [33:21<6:57:34,  1.63s/it]11/15/2022 17:39:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.9644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:42 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1212/16548 [33:23<6:57:07,  1.63s/it]11/15/2022 17:39:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5768e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:44 - INFO - train.train_snli_ve - loss is tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1213/16548 [33:24<6:54:56,  1.62s/it]11/15/2022 17:39:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.0267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:45 - INFO - train.train_snli_ve - loss is tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1214/16548 [33:26<6:57:30,  1.63s/it]11/15/2022 17:39:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.1885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:47 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1215/16548 [33:28<6:58:59,  1.64s/it]11/15/2022 17:39:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.1547e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:49 - INFO - train.train_snli_ve - loss is tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1216/16548 [33:29<6:57:53,  1.64s/it]11/15/2022 17:39:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.3761e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:50 - INFO - train.train_snli_ve - loss is tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1217/16548 [33:31<6:55:44,  1.63s/it]11/15/2022 17:39:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.4532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:52 - INFO - train.train_snli_ve - loss is tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1218/16548 [33:32<6:53:42,  1.62s/it]11/15/2022 17:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.7908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:53 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1219/16548 [33:34<6:54:59,  1.62s/it]11/15/2022 17:39:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.2913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:55 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1220/16548 [33:36<6:57:50,  1.64s/it]11/15/2022 17:39:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:57 - INFO - train.train_snli_ve - loss is tensor(0.9579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1221/16548 [33:37<6:59:56,  1.64s/it]11/15/2022 17:39:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.9401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:39:58 - INFO - train.train_snli_ve - loss is tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1222/16548 [33:39<7:02:01,  1.65s/it]11/15/2022 17:40:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.3879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:00 - INFO - train.train_snli_ve - loss is tensor(0.8634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1223/16548 [33:41<7:05:04,  1.66s/it]11/15/2022 17:40:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.6806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:02 - INFO - train.train_snli_ve - loss is tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1224/16548 [33:42<7:06:03,  1.67s/it]11/15/2022 17:40:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.1515e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:03 - INFO - train.train_snli_ve - loss is tensor(0.9791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1225/16548 [33:44<7:04:16,  1.66s/it]11/15/2022 17:40:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.8219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:05 - INFO - train.train_snli_ve - loss is tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1226/16548 [33:46<7:02:44,  1.66s/it]11/15/2022 17:40:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.8122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:07 - INFO - train.train_snli_ve - loss is tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1227/16548 [33:47<7:02:44,  1.66s/it]11/15/2022 17:40:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.1437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:08 - INFO - train.train_snli_ve - loss is tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1228/16548 [33:49<7:03:18,  1.66s/it]11/15/2022 17:40:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.4422e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:10 - INFO - train.train_snli_ve - loss is tensor(1.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1229/16548 [33:51<7:02:25,  1.65s/it]11/15/2022 17:40:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:12 - INFO - train.train_snli_ve - loss is tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1230/16548 [33:52<7:04:01,  1.66s/it]11/15/2022 17:40:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.1990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:13 - INFO - train.train_snli_ve - loss is tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1231/16548 [33:54<7:01:21,  1.65s/it]11/15/2022 17:40:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.3621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:15 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1232/16548 [33:56<7:02:18,  1.65s/it]11/15/2022 17:40:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.3800e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:17 - INFO - train.train_snli_ve - loss is tensor(0.8984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1233/16548 [33:57<7:01:05,  1.65s/it]11/15/2022 17:40:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.3974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:18 - INFO - train.train_snli_ve - loss is tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1234/16548 [33:59<6:58:02,  1.64s/it]11/15/2022 17:40:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.9856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:20 - INFO - train.train_snli_ve - loss is tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1235/16548 [34:01<6:59:57,  1.65s/it]11/15/2022 17:40:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.9522e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:22 - INFO - train.train_snli_ve - loss is tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1236/16548 [34:02<7:01:40,  1.65s/it]11/15/2022 17:40:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.6055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:23 - INFO - train.train_snli_ve - loss is tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1237/16548 [34:04<6:59:24,  1.64s/it]11/15/2022 17:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.4936e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:25 - INFO - train.train_snli_ve - loss is tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1238/16548 [34:05<6:55:34,  1.63s/it]11/15/2022 17:40:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.9824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:26 - INFO - train.train_snli_ve - loss is tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1239/16548 [34:07<6:53:08,  1.62s/it]11/15/2022 17:40:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.3904e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:28 - INFO - train.train_snli_ve - loss is tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1240/16548 [34:09<6:53:57,  1.62s/it]11/15/2022 17:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.7863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:30 - INFO - train.train_snli_ve - loss is tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7% 1241/16548 [34:10<6:53:19,  1.62s/it]11/15/2022 17:40:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.8020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:31 - INFO - train.train_snli_ve - loss is tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1242/16548 [34:12<6:57:22,  1.64s/it]11/15/2022 17:40:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:33 - INFO - train.train_snli_ve - loss is tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1243/16548 [34:14<6:57:33,  1.64s/it]11/15/2022 17:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.7570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:35 - INFO - train.train_snli_ve - loss is tensor(0.8970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1244/16548 [34:15<6:59:41,  1.65s/it]11/15/2022 17:40:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.7886e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:36 - INFO - train.train_snli_ve - loss is tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1245/16548 [34:17<6:57:13,  1.64s/it]11/15/2022 17:40:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.0669e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:38 - INFO - train.train_snli_ve - loss is tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1246/16548 [34:19<7:00:35,  1.65s/it]11/15/2022 17:40:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:39 - INFO - train.train_snli_ve - loss is tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1247/16548 [34:20<6:57:51,  1.64s/it]11/15/2022 17:40:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:41 - INFO - train.train_snli_ve - loss is tensor(0.7703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1248/16548 [34:22<7:02:08,  1.66s/it]11/15/2022 17:40:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.9260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:43 - INFO - train.train_snli_ve - loss is tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1249/16548 [34:24<7:00:29,  1.65s/it]11/15/2022 17:40:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:44 - INFO - train.train_snli_ve - loss is tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1250/16548 [34:25<6:59:04,  1.64s/it]11/15/2022 17:40:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.8484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:46 - INFO - train.train_snli_ve - loss is tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1251/16548 [34:27<6:59:12,  1.64s/it]11/15/2022 17:40:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.8758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:48 - INFO - train.train_snli_ve - loss is tensor(0.9211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1252/16548 [34:28<6:58:33,  1.64s/it]11/15/2022 17:40:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.5326e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:49 - INFO - train.train_snli_ve - loss is tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1253/16548 [34:30<6:57:49,  1.64s/it]11/15/2022 17:40:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.7842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:51 - INFO - train.train_snli_ve - loss is tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1254/16548 [34:32<6:56:32,  1.63s/it]11/15/2022 17:40:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.9153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:53 - INFO - train.train_snli_ve - loss is tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1255/16548 [34:33<6:58:13,  1.64s/it]11/15/2022 17:40:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.1342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:54 - INFO - train.train_snli_ve - loss is tensor(1.1112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1256/16548 [34:35<6:59:22,  1.65s/it]11/15/2022 17:40:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.4851e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:56 - INFO - train.train_snli_ve - loss is tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1257/16548 [34:37<6:58:28,  1.64s/it]11/15/2022 17:40:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.1814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:58 - INFO - train.train_snli_ve - loss is tensor(0.7729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1258/16548 [34:38<6:58:09,  1.64s/it]11/15/2022 17:40:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.8933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:40:59 - INFO - train.train_snli_ve - loss is tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1259/16548 [34:40<6:58:50,  1.64s/it]11/15/2022 17:41:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.9325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:01 - INFO - train.train_snli_ve - loss is tensor(1.0747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1260/16548 [34:42<6:57:26,  1.64s/it]11/15/2022 17:41:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.0353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:02 - INFO - train.train_snli_ve - loss is tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1261/16548 [34:43<6:55:12,  1.63s/it]11/15/2022 17:41:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.5910e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:04 - INFO - train.train_snli_ve - loss is tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1262/16548 [34:45<6:56:26,  1.63s/it]11/15/2022 17:41:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.1076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:06 - INFO - train.train_snli_ve - loss is tensor(0.9054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1263/16548 [34:46<6:54:35,  1.63s/it]11/15/2022 17:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.4605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:07 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1264/16548 [34:48<6:56:31,  1.64s/it]11/15/2022 17:41:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.1893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:09 - INFO - train.train_snli_ve - loss is tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1265/16548 [34:50<6:57:10,  1.64s/it]11/15/2022 17:41:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.5674e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:11 - INFO - train.train_snli_ve - loss is tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1266/16548 [34:51<7:00:51,  1.65s/it]11/15/2022 17:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.2290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:12 - INFO - train.train_snli_ve - loss is tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1267/16548 [34:53<6:57:52,  1.64s/it]11/15/2022 17:41:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.1663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:14 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1268/16548 [34:55<6:57:28,  1.64s/it]11/15/2022 17:41:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:16 - INFO - train.train_snli_ve - loss is tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1269/16548 [34:56<6:55:24,  1.63s/it]11/15/2022 17:41:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.5741e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:17 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1270/16548 [34:58<6:56:16,  1.63s/it]11/15/2022 17:41:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.3425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:19 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1271/16548 [35:00<6:56:00,  1.63s/it]11/15/2022 17:41:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:21 - INFO - train.train_snli_ve - loss is tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1272/16548 [35:01<7:01:29,  1.66s/it]11/15/2022 17:41:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.5012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:22 - INFO - train.train_snli_ve - loss is tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1273/16548 [35:03<7:00:38,  1.65s/it]11/15/2022 17:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.0806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:24 - INFO - train.train_snli_ve - loss is tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1274/16548 [35:05<7:00:30,  1.65s/it]11/15/2022 17:41:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.4557e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:25 - INFO - train.train_snli_ve - loss is tensor(0.8374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1275/16548 [35:06<7:01:51,  1.66s/it]11/15/2022 17:41:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.2277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:27 - INFO - train.train_snli_ve - loss is tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1276/16548 [35:08<7:01:14,  1.65s/it]11/15/2022 17:41:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:29 - INFO - train.train_snli_ve - loss is tensor(0.9715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1277/16548 [35:10<7:01:08,  1.65s/it]11/15/2022 17:41:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.3449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:30 - INFO - train.train_snli_ve - loss is tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1278/16548 [35:11<6:58:23,  1.64s/it]11/15/2022 17:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:32 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1279/16548 [35:13<6:59:24,  1.65s/it]11/15/2022 17:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.9292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:34 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1280/16548 [35:14<7:00:43,  1.65s/it]11/15/2022 17:41:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.5585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:35 - INFO - train.train_snli_ve - loss is tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1281/16548 [35:16<7:02:33,  1.66s/it]11/15/2022 17:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.6086e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:37 - INFO - train.train_snli_ve - loss is tensor(1.1544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1282/16548 [35:18<7:02:09,  1.66s/it]11/15/2022 17:41:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.1425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:39 - INFO - train.train_snli_ve - loss is tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1283/16548 [35:19<7:03:55,  1.67s/it]11/15/2022 17:41:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.1270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:40 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1284/16548 [35:21<7:01:36,  1.66s/it]11/15/2022 17:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.8442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:42 - INFO - train.train_snli_ve - loss is tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1285/16548 [35:23<6:59:06,  1.65s/it]11/15/2022 17:41:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:44 - INFO - train.train_snli_ve - loss is tensor(0.8001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1286/16548 [35:24<6:58:26,  1.65s/it]11/15/2022 17:41:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:45 - INFO - train.train_snli_ve - loss is tensor(0.8778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1287/16548 [35:26<6:59:28,  1.65s/it]11/15/2022 17:41:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.4185e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:47 - INFO - train.train_snli_ve - loss is tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1288/16548 [35:28<7:01:27,  1.66s/it]11/15/2022 17:41:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.1447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:49 - INFO - train.train_snli_ve - loss is tensor(0.9232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1289/16548 [35:29<6:57:42,  1.64s/it]11/15/2022 17:41:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.6235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:50 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1290/16548 [35:31<7:01:58,  1.66s/it]11/15/2022 17:41:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.7560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:52 - INFO - train.train_snli_ve - loss is tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1291/16548 [35:33<6:59:55,  1.65s/it]11/15/2022 17:41:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.8714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:54 - INFO - train.train_snli_ve - loss is tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1292/16548 [35:34<7:00:44,  1.65s/it]11/15/2022 17:41:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.7261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:55 - INFO - train.train_snli_ve - loss is tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1293/16548 [35:36<7:00:43,  1.65s/it]11/15/2022 17:41:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.3221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:57 - INFO - train.train_snli_ve - loss is tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1294/16548 [35:38<6:59:25,  1.65s/it]11/15/2022 17:41:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.9346e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:41:59 - INFO - train.train_snli_ve - loss is tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1295/16548 [35:39<7:02:11,  1.66s/it]11/15/2022 17:42:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.5308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:00 - INFO - train.train_snli_ve - loss is tensor(0.7992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1296/16548 [35:41<6:59:11,  1.65s/it]11/15/2022 17:42:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.3155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:02 - INFO - train.train_snli_ve - loss is tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1297/16548 [35:43<6:57:09,  1.64s/it]11/15/2022 17:42:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.3417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:03 - INFO - train.train_snli_ve - loss is tensor(1.1064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1298/16548 [35:44<6:57:55,  1.64s/it]11/15/2022 17:42:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.5833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:05 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1299/16548 [35:46<6:55:39,  1.64s/it]11/15/2022 17:42:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.2367e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:07 - INFO - train.train_snli_ve - loss is tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1300/16548 [35:48<7:02:45,  1.66s/it]11/15/2022 17:42:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.6259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:08 - INFO - train.train_snli_ve - loss is tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1301/16548 [35:49<6:57:25,  1.64s/it]11/15/2022 17:42:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.9109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:10 - INFO - train.train_snli_ve - loss is tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1302/16548 [35:51<6:58:16,  1.65s/it]11/15/2022 17:42:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:12 - INFO - train.train_snli_ve - loss is tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1303/16548 [35:52<6:59:01,  1.65s/it]11/15/2022 17:42:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.2403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:13 - INFO - train.train_snli_ve - loss is tensor(0.7451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1304/16548 [35:54<6:58:09,  1.65s/it]11/15/2022 17:42:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.9882e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:15 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1305/16548 [35:56<6:59:12,  1.65s/it]11/15/2022 17:42:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.3414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:17 - INFO - train.train_snli_ve - loss is tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1306/16548 [35:57<6:55:53,  1.64s/it]11/15/2022 17:42:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.0967e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:18 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1307/16548 [35:59<6:53:10,  1.63s/it]11/15/2022 17:42:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.5000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:20 - INFO - train.train_snli_ve - loss is tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1308/16548 [36:01<6:53:49,  1.63s/it]11/15/2022 17:42:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.3388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:22 - INFO - train.train_snli_ve - loss is tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1309/16548 [36:02<6:55:08,  1.63s/it]11/15/2022 17:42:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.9758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:23 - INFO - train.train_snli_ve - loss is tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1310/16548 [36:04<6:53:05,  1.63s/it]11/15/2022 17:42:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.0562e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:25 - INFO - train.train_snli_ve - loss is tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1311/16548 [36:06<7:00:31,  1.66s/it]11/15/2022 17:42:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.7930e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:27 - INFO - train.train_snli_ve - loss is tensor(1.0251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1312/16548 [36:07<7:02:14,  1.66s/it]11/15/2022 17:42:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.4007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:28 - INFO - train.train_snli_ve - loss is tensor(0.8225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1313/16548 [36:09<6:59:07,  1.65s/it]11/15/2022 17:42:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.1401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:30 - INFO - train.train_snli_ve - loss is tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1314/16548 [36:11<6:57:40,  1.65s/it]11/15/2022 17:42:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.0248e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:31 - INFO - train.train_snli_ve - loss is tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1315/16548 [36:12<6:58:04,  1.65s/it]11/15/2022 17:42:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.5207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:33 - INFO - train.train_snli_ve - loss is tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1316/16548 [36:14<6:58:53,  1.65s/it]11/15/2022 17:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.9412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:35 - INFO - train.train_snli_ve - loss is tensor(0.9046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1317/16548 [36:15<6:56:19,  1.64s/it]11/15/2022 17:42:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.2527e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:36 - INFO - train.train_snli_ve - loss is tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1318/16548 [36:17<6:59:33,  1.65s/it]11/15/2022 17:42:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.5506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:38 - INFO - train.train_snli_ve - loss is tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1319/16548 [36:19<6:56:26,  1.64s/it]11/15/2022 17:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.0399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:40 - INFO - train.train_snli_ve - loss is tensor(0.9049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1320/16548 [36:20<6:55:17,  1.64s/it]11/15/2022 17:42:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.7912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:41 - INFO - train.train_snli_ve - loss is tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1321/16548 [36:22<7:00:55,  1.66s/it]11/15/2022 17:42:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.0269e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:43 - INFO - train.train_snli_ve - loss is tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1322/16548 [36:24<6:58:40,  1.65s/it]11/15/2022 17:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.4512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:45 - INFO - train.train_snli_ve - loss is tensor(0.9998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1323/16548 [36:25<6:59:59,  1.66s/it]11/15/2022 17:42:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.4762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:46 - INFO - train.train_snli_ve - loss is tensor(1.0084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1324/16548 [36:27<6:59:07,  1.65s/it]11/15/2022 17:42:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.9137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:48 - INFO - train.train_snli_ve - loss is tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1325/16548 [36:29<6:58:11,  1.65s/it]11/15/2022 17:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.5077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:50 - INFO - train.train_snli_ve - loss is tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1326/16548 [36:30<7:03:13,  1.67s/it]11/15/2022 17:42:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.3618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:51 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1327/16548 [36:32<6:58:11,  1.65s/it]11/15/2022 17:42:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.1868e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:53 - INFO - train.train_snli_ve - loss is tensor(0.8554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1328/16548 [36:34<6:55:41,  1.64s/it]11/15/2022 17:42:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:54 - INFO - train.train_snli_ve - loss is tensor(0.8251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1329/16548 [36:35<6:52:46,  1.63s/it]11/15/2022 17:42:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.2103e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:56 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1330/16548 [36:37<6:57:55,  1.65s/it]11/15/2022 17:42:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.7329e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:58 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1331/16548 [36:39<6:57:52,  1.65s/it]11/15/2022 17:42:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.3450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:42:59 - INFO - train.train_snli_ve - loss is tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1332/16548 [36:40<6:57:10,  1.65s/it]11/15/2022 17:43:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.6461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:01 - INFO - train.train_snli_ve - loss is tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1333/16548 [36:42<6:56:28,  1.64s/it]11/15/2022 17:43:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.5287e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:03 - INFO - train.train_snli_ve - loss is tensor(0.9008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1334/16548 [36:43<6:56:38,  1.64s/it]11/15/2022 17:43:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.2648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:04 - INFO - train.train_snli_ve - loss is tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1335/16548 [36:45<6:54:19,  1.63s/it]11/15/2022 17:43:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.7815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:06 - INFO - train.train_snli_ve - loss is tensor(0.8844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1336/16548 [36:47<6:55:44,  1.64s/it]11/15/2022 17:43:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:08 - INFO - train.train_snli_ve - loss is tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1337/16548 [36:48<6:55:58,  1.64s/it]11/15/2022 17:43:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:09 - INFO - train.train_snli_ve - loss is tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1338/16548 [36:50<6:58:35,  1.65s/it]11/15/2022 17:43:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.1165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:11 - INFO - train.train_snli_ve - loss is tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1339/16548 [36:52<6:58:23,  1.65s/it]11/15/2022 17:43:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:13 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1340/16548 [36:53<6:58:17,  1.65s/it]11/15/2022 17:43:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.2537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:14 - INFO - train.train_snli_ve - loss is tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1341/16548 [36:55<6:55:22,  1.64s/it]11/15/2022 17:43:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.8335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:16 - INFO - train.train_snli_ve - loss is tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1342/16548 [36:57<6:53:07,  1.63s/it]11/15/2022 17:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.6991e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:17 - INFO - train.train_snli_ve - loss is tensor(0.8559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1343/16548 [36:58<6:53:49,  1.63s/it]11/15/2022 17:43:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.7484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:19 - INFO - train.train_snli_ve - loss is tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1344/16548 [37:00<6:51:34,  1.62s/it]11/15/2022 17:43:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.6441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:21 - INFO - train.train_snli_ve - loss is tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1345/16548 [37:01<6:50:49,  1.62s/it]11/15/2022 17:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:22 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1346/16548 [37:03<6:54:04,  1.63s/it]11/15/2022 17:43:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.1208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:24 - INFO - train.train_snli_ve - loss is tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1347/16548 [37:05<6:55:11,  1.64s/it]11/15/2022 17:43:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.2782e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:26 - INFO - train.train_snli_ve - loss is tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1348/16548 [37:06<6:58:05,  1.65s/it]11/15/2022 17:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.3694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:27 - INFO - train.train_snli_ve - loss is tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1349/16548 [37:08<6:56:26,  1.64s/it]11/15/2022 17:43:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.3133e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:29 - INFO - train.train_snli_ve - loss is tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1350/16548 [37:10<6:55:13,  1.64s/it]11/15/2022 17:43:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.7135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:31 - INFO - train.train_snli_ve - loss is tensor(0.9654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1351/16548 [37:11<6:56:16,  1.64s/it]11/15/2022 17:43:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.7344e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:32 - INFO - train.train_snli_ve - loss is tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1352/16548 [37:13<6:56:54,  1.65s/it]11/15/2022 17:43:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.3707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:34 - INFO - train.train_snli_ve - loss is tensor(0.5827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1353/16548 [37:15<6:54:02,  1.63s/it]11/15/2022 17:43:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.1780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:35 - INFO - train.train_snli_ve - loss is tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1354/16548 [37:16<6:55:06,  1.64s/it]11/15/2022 17:43:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:37 - INFO - train.train_snli_ve - loss is tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1355/16548 [37:18<6:57:25,  1.65s/it]11/15/2022 17:43:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:39 - INFO - train.train_snli_ve - loss is tensor(1.0318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1356/16548 [37:20<7:00:55,  1.66s/it]11/15/2022 17:43:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.0506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:41 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1357/16548 [37:21<7:01:01,  1.66s/it]11/15/2022 17:43:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1874e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:42 - INFO - train.train_snli_ve - loss is tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1358/16548 [37:23<7:03:01,  1.67s/it]11/15/2022 17:43:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.6587e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:44 - INFO - train.train_snli_ve - loss is tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1359/16548 [37:25<6:59:38,  1.66s/it]11/15/2022 17:43:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.6052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:45 - INFO - train.train_snli_ve - loss is tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1360/16548 [37:26<6:58:48,  1.65s/it]11/15/2022 17:43:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.7170e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:47 - INFO - train.train_snli_ve - loss is tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1361/16548 [37:28<6:59:59,  1.66s/it]11/15/2022 17:43:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:49 - INFO - train.train_snli_ve - loss is tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1362/16548 [37:29<6:56:33,  1.65s/it]11/15/2022 17:43:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:50 - INFO - train.train_snli_ve - loss is tensor(0.9610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1363/16548 [37:31<6:53:55,  1.64s/it]11/15/2022 17:43:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.8550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:52 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1364/16548 [37:33<6:58:51,  1.66s/it]11/15/2022 17:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.2309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:54 - INFO - train.train_snli_ve - loss is tensor(0.7452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1365/16548 [37:34<6:55:23,  1.64s/it]11/15/2022 17:43:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.7182e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:55 - INFO - train.train_snli_ve - loss is tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1366/16548 [37:36<6:52:18,  1.63s/it]11/15/2022 17:43:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.1411e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:57 - INFO - train.train_snli_ve - loss is tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1367/16548 [37:38<6:51:49,  1.63s/it]11/15/2022 17:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.2318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:43:59 - INFO - train.train_snli_ve - loss is tensor(0.8568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1368/16548 [37:39<6:56:23,  1.65s/it]11/15/2022 17:44:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.2123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:00 - INFO - train.train_snli_ve - loss is tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1369/16548 [37:41<6:59:32,  1.66s/it]11/15/2022 17:44:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.2847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:02 - INFO - train.train_snli_ve - loss is tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1370/16548 [37:43<7:03:15,  1.67s/it]11/15/2022 17:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.5184e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:04 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1371/16548 [37:44<6:59:20,  1.66s/it]11/15/2022 17:44:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.7050e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:05 - INFO - train.train_snli_ve - loss is tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1372/16548 [37:46<6:57:48,  1.65s/it]11/15/2022 17:44:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.1160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:07 - INFO - train.train_snli_ve - loss is tensor(0.8815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1373/16548 [37:48<7:00:09,  1.66s/it]11/15/2022 17:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.7069e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:09 - INFO - train.train_snli_ve - loss is tensor(0.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1374/16548 [37:49<7:00:34,  1.66s/it]11/15/2022 17:44:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.7856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:10 - INFO - train.train_snli_ve - loss is tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1375/16548 [37:51<6:55:54,  1.64s/it]11/15/2022 17:44:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:12 - INFO - train.train_snli_ve - loss is tensor(0.8239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1376/16548 [37:53<6:56:38,  1.65s/it]11/15/2022 17:44:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.4661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:13 - INFO - train.train_snli_ve - loss is tensor(0.9125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1377/16548 [37:54<6:55:17,  1.64s/it]11/15/2022 17:44:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.1018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:15 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1378/16548 [37:56<6:53:15,  1.63s/it]11/15/2022 17:44:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.3173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:17 - INFO - train.train_snli_ve - loss is tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1379/16548 [37:58<6:55:24,  1.64s/it]11/15/2022 17:44:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.1750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:18 - INFO - train.train_snli_ve - loss is tensor(0.7443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1380/16548 [37:59<6:59:31,  1.66s/it]11/15/2022 17:44:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.9484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:20 - INFO - train.train_snli_ve - loss is tensor(0.9069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1381/16548 [38:01<6:58:30,  1.66s/it]11/15/2022 17:44:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.9637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:22 - INFO - train.train_snli_ve - loss is tensor(0.7890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1382/16548 [38:02<6:54:21,  1.64s/it]11/15/2022 17:44:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.1014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:23 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1383/16548 [38:04<6:51:41,  1.63s/it]11/15/2022 17:44:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.2829e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:25 - INFO - train.train_snli_ve - loss is tensor(0.9708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1384/16548 [38:06<6:52:32,  1.63s/it]11/15/2022 17:44:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.0280e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:27 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1385/16548 [38:07<6:55:51,  1.65s/it]11/15/2022 17:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.9794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:28 - INFO - train.train_snli_ve - loss is tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1386/16548 [38:09<6:56:25,  1.65s/it]11/15/2022 17:44:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.1494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:30 - INFO - train.train_snli_ve - loss is tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1387/16548 [38:11<6:56:32,  1.65s/it]11/15/2022 17:44:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.0945e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:32 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1388/16548 [38:12<6:57:55,  1.65s/it]11/15/2022 17:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.8336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:33 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1389/16548 [38:14<6:55:55,  1.65s/it]11/15/2022 17:44:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.5962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:35 - INFO - train.train_snli_ve - loss is tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1390/16548 [38:16<6:59:34,  1.66s/it]11/15/2022 17:44:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.1966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:37 - INFO - train.train_snli_ve - loss is tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1391/16548 [38:17<7:04:10,  1.68s/it]11/15/2022 17:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.9170e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:38 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1392/16548 [38:19<7:01:06,  1.67s/it]11/15/2022 17:44:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8627e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:40 - INFO - train.train_snli_ve - loss is tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1393/16548 [38:21<6:58:09,  1.66s/it]11/15/2022 17:44:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.2707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:42 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1394/16548 [38:22<6:57:40,  1.65s/it]11/15/2022 17:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.9497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:43 - INFO - train.train_snli_ve - loss is tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1395/16548 [38:24<6:56:24,  1.65s/it]11/15/2022 17:44:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.9136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:45 - INFO - train.train_snli_ve - loss is tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1396/16548 [38:26<6:57:18,  1.65s/it]11/15/2022 17:44:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.7126e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:47 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1397/16548 [38:27<7:00:24,  1.66s/it]11/15/2022 17:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.8769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:48 - INFO - train.train_snli_ve - loss is tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1398/16548 [38:29<7:02:33,  1.67s/it]11/15/2022 17:44:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.1681e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:50 - INFO - train.train_snli_ve - loss is tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1399/16548 [38:31<7:04:32,  1.68s/it]11/15/2022 17:44:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.1976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:52 - INFO - train.train_snli_ve - loss is tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1400/16548 [38:32<7:13:31,  1.72s/it]11/15/2022 17:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.1591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:53 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1401/16548 [38:34<7:10:56,  1.71s/it]11/15/2022 17:44:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.0721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:55 - INFO - train.train_snli_ve - loss is tensor(0.8795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1402/16548 [38:36<7:07:17,  1.69s/it]11/15/2022 17:44:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.5339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:57 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1403/16548 [38:37<7:03:15,  1.68s/it]11/15/2022 17:44:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.4145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:44:58 - INFO - train.train_snli_ve - loss is tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1404/16548 [38:39<7:01:41,  1.67s/it]11/15/2022 17:45:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.5453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:00 - INFO - train.train_snli_ve - loss is tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1405/16548 [38:41<6:59:56,  1.66s/it]11/15/2022 17:45:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.5474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:02 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8% 1406/16548 [38:42<7:03:28,  1.68s/it]11/15/2022 17:45:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.4799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:03 - INFO - train.train_snli_ve - loss is tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1407/16548 [38:44<7:04:21,  1.68s/it]11/15/2022 17:45:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.9577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:05 - INFO - train.train_snli_ve - loss is tensor(0.8958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1408/16548 [38:46<7:06:00,  1.69s/it]11/15/2022 17:45:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.7537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:07 - INFO - train.train_snli_ve - loss is tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1409/16548 [38:48<7:07:45,  1.70s/it]11/15/2022 17:45:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.2728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:09 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1410/16548 [38:49<7:10:04,  1.70s/it]11/15/2022 17:45:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.3550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:10 - INFO - train.train_snli_ve - loss is tensor(0.9748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1411/16548 [38:51<7:09:45,  1.70s/it]11/15/2022 17:45:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.7098e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:12 - INFO - train.train_snli_ve - loss is tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1412/16548 [38:53<7:11:08,  1.71s/it]11/15/2022 17:45:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.4866e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:14 - INFO - train.train_snli_ve - loss is tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1413/16548 [38:54<7:09:02,  1.70s/it]11/15/2022 17:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.8625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:15 - INFO - train.train_snli_ve - loss is tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1414/16548 [38:56<7:11:00,  1.71s/it]11/15/2022 17:45:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.3985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:17 - INFO - train.train_snli_ve - loss is tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1415/16548 [38:58<7:11:07,  1.71s/it]11/15/2022 17:45:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.7056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:19 - INFO - train.train_snli_ve - loss is tensor(1.0995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1416/16548 [39:00<7:08:51,  1.70s/it]11/15/2022 17:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.0185e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:20 - INFO - train.train_snli_ve - loss is tensor(0.8101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1417/16548 [39:01<7:04:51,  1.68s/it]11/15/2022 17:45:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.3136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:22 - INFO - train.train_snli_ve - loss is tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1418/16548 [39:03<7:03:21,  1.68s/it]11/15/2022 17:45:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.7324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:24 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1419/16548 [39:05<7:08:40,  1.70s/it]11/15/2022 17:45:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:26 - INFO - train.train_snli_ve - loss is tensor(0.8376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1420/16548 [39:06<7:06:19,  1.69s/it]11/15/2022 17:45:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.8414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:27 - INFO - train.train_snli_ve - loss is tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1421/16548 [39:08<7:06:29,  1.69s/it]11/15/2022 17:45:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.8035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:29 - INFO - train.train_snli_ve - loss is tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1422/16548 [39:10<7:07:49,  1.70s/it]11/15/2022 17:45:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.2543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:31 - INFO - train.train_snli_ve - loss is tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1423/16548 [39:11<7:02:59,  1.68s/it]11/15/2022 17:45:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.3198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:32 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1424/16548 [39:13<7:01:44,  1.67s/it]11/15/2022 17:45:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.5121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:34 - INFO - train.train_snli_ve - loss is tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1425/16548 [39:15<7:03:02,  1.68s/it]11/15/2022 17:45:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.7585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:36 - INFO - train.train_snli_ve - loss is tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1426/16548 [39:16<6:59:44,  1.67s/it]11/15/2022 17:45:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.9638e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:37 - INFO - train.train_snli_ve - loss is tensor(0.8867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1427/16548 [39:18<6:58:24,  1.66s/it]11/15/2022 17:45:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.9926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:39 - INFO - train.train_snli_ve - loss is tensor(0.8346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1428/16548 [39:20<6:56:42,  1.65s/it]11/15/2022 17:45:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3835e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:41 - INFO - train.train_snli_ve - loss is tensor(0.7991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1429/16548 [39:21<6:59:18,  1.66s/it]11/15/2022 17:45:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1630e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:42 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1430/16548 [39:23<7:01:38,  1.67s/it]11/15/2022 17:45:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1194e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:44 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1431/16548 [39:25<7:04:17,  1.68s/it]11/15/2022 17:45:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.8439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:46 - INFO - train.train_snli_ve - loss is tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1432/16548 [39:26<7:02:22,  1.68s/it]11/15/2022 17:45:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.4343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:47 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1433/16548 [39:28<7:02:06,  1.68s/it]11/15/2022 17:45:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.0592e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:49 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1434/16548 [39:30<7:00:09,  1.67s/it]11/15/2022 17:45:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.9212e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:51 - INFO - train.train_snli_ve - loss is tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1435/16548 [39:31<7:02:38,  1.68s/it]11/15/2022 17:45:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.6971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:52 - INFO - train.train_snli_ve - loss is tensor(0.9747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1436/16548 [39:33<7:02:31,  1.68s/it]11/15/2022 17:45:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:54 - INFO - train.train_snli_ve - loss is tensor(0.8611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1437/16548 [39:35<7:04:47,  1.69s/it]11/15/2022 17:45:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.5705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:56 - INFO - train.train_snli_ve - loss is tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1438/16548 [39:36<7:02:10,  1.68s/it]11/15/2022 17:45:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.1038e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:57 - INFO - train.train_snli_ve - loss is tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1439/16548 [39:38<7:03:47,  1.68s/it]11/15/2022 17:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.8710e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:45:59 - INFO - train.train_snli_ve - loss is tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1440/16548 [39:40<7:02:30,  1.68s/it]11/15/2022 17:46:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.8672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:01 - INFO - train.train_snli_ve - loss is tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1441/16548 [39:41<7:00:11,  1.67s/it]11/15/2022 17:46:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.1974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:02 - INFO - train.train_snli_ve - loss is tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1442/16548 [39:43<7:01:45,  1.68s/it]11/15/2022 17:46:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.0165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:04 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1443/16548 [39:45<6:59:31,  1.67s/it]11/15/2022 17:46:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.8579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:06 - INFO - train.train_snli_ve - loss is tensor(0.8952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1444/16548 [39:46<7:03:39,  1.68s/it]11/15/2022 17:46:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.3702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:07 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1445/16548 [39:48<7:03:07,  1.68s/it]11/15/2022 17:46:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.7373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:09 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1446/16548 [39:50<7:01:50,  1.68s/it]11/15/2022 17:46:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.1062e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:11 - INFO - train.train_snli_ve - loss is tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1447/16548 [39:51<7:00:31,  1.67s/it]11/15/2022 17:46:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.8302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:12 - INFO - train.train_snli_ve - loss is tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1448/16548 [39:53<6:59:01,  1.66s/it]11/15/2022 17:46:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.8985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:14 - INFO - train.train_snli_ve - loss is tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1449/16548 [39:55<6:58:06,  1.66s/it]11/15/2022 17:46:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.5759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:16 - INFO - train.train_snli_ve - loss is tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1450/16548 [39:56<6:58:28,  1.66s/it]11/15/2022 17:46:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.1786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:17 - INFO - train.train_snli_ve - loss is tensor(0.7254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1451/16548 [39:58<6:58:41,  1.66s/it]11/15/2022 17:46:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.5220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:19 - INFO - train.train_snli_ve - loss is tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1452/16548 [40:00<6:57:55,  1.66s/it]11/15/2022 17:46:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.3176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:21 - INFO - train.train_snli_ve - loss is tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1453/16548 [40:01<6:58:54,  1.67s/it]11/15/2022 17:46:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.4618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:22 - INFO - train.train_snli_ve - loss is tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1454/16548 [40:03<6:56:08,  1.65s/it]11/15/2022 17:46:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.0795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:24 - INFO - train.train_snli_ve - loss is tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1455/16548 [40:05<7:01:27,  1.68s/it]11/15/2022 17:46:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:26 - INFO - train.train_snli_ve - loss is tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1456/16548 [40:06<7:00:12,  1.67s/it]11/15/2022 17:46:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.8786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:27 - INFO - train.train_snli_ve - loss is tensor(0.8394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1457/16548 [40:08<7:00:27,  1.67s/it]11/15/2022 17:46:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.5358e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:29 - INFO - train.train_snli_ve - loss is tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1458/16548 [40:10<7:01:52,  1.68s/it]11/15/2022 17:46:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:31 - INFO - train.train_snli_ve - loss is tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1459/16548 [40:11<7:01:35,  1.68s/it]11/15/2022 17:46:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.7994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:32 - INFO - train.train_snli_ve - loss is tensor(0.7348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1460/16548 [40:13<7:01:03,  1.67s/it]11/15/2022 17:46:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.0018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:34 - INFO - train.train_snli_ve - loss is tensor(0.8562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1461/16548 [40:15<7:02:58,  1.68s/it]11/15/2022 17:46:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.1760e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:36 - INFO - train.train_snli_ve - loss is tensor(0.9183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1462/16548 [40:17<7:01:47,  1.68s/it]11/15/2022 17:46:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.2034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:37 - INFO - train.train_snli_ve - loss is tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1463/16548 [40:18<6:57:45,  1.66s/it]11/15/2022 17:46:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:39 - INFO - train.train_snli_ve - loss is tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1464/16548 [40:20<6:55:09,  1.65s/it]11/15/2022 17:46:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.7968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:41 - INFO - train.train_snli_ve - loss is tensor(0.8785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1465/16548 [40:21<6:52:38,  1.64s/it]11/15/2022 17:46:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.8879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:42 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1466/16548 [40:23<6:54:32,  1.65s/it]11/15/2022 17:46:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:44 - INFO - train.train_snli_ve - loss is tensor(0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1467/16548 [40:25<6:56:43,  1.66s/it]11/15/2022 17:46:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.0348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:46 - INFO - train.train_snli_ve - loss is tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1468/16548 [40:26<7:00:53,  1.67s/it]11/15/2022 17:46:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.3357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:47 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1469/16548 [40:28<6:58:01,  1.66s/it]11/15/2022 17:46:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:49 - INFO - train.train_snli_ve - loss is tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1470/16548 [40:30<6:55:02,  1.65s/it]11/15/2022 17:46:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.5444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:51 - INFO - train.train_snli_ve - loss is tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1471/16548 [40:31<6:57:24,  1.66s/it]11/15/2022 17:46:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.5313e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:52 - INFO - train.train_snli_ve - loss is tensor(0.8038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1472/16548 [40:33<6:52:41,  1.64s/it]11/15/2022 17:46:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:54 - INFO - train.train_snli_ve - loss is tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1473/16548 [40:35<6:51:54,  1.64s/it]11/15/2022 17:46:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.5458e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:56 - INFO - train.train_snli_ve - loss is tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1474/16548 [40:36<6:54:08,  1.65s/it]11/15/2022 17:46:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.5674e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:57 - INFO - train.train_snli_ve - loss is tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1475/16548 [40:38<6:51:42,  1.64s/it]11/15/2022 17:46:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.2825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:46:59 - INFO - train.train_snli_ve - loss is tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1476/16548 [40:40<6:50:58,  1.64s/it]11/15/2022 17:47:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.1789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:00 - INFO - train.train_snli_ve - loss is tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1477/16548 [40:41<6:50:03,  1.63s/it]11/15/2022 17:47:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.8778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:02 - INFO - train.train_snli_ve - loss is tensor(0.8354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1478/16548 [40:43<6:50:45,  1.64s/it]11/15/2022 17:47:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.4192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:04 - INFO - train.train_snli_ve - loss is tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1479/16548 [40:44<6:51:09,  1.64s/it]11/15/2022 17:47:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.5553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:05 - INFO - train.train_snli_ve - loss is tensor(0.8865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1480/16548 [40:46<6:53:58,  1.65s/it]11/15/2022 17:47:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.8325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:07 - INFO - train.train_snli_ve - loss is tensor(0.9550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1481/16548 [40:48<6:55:11,  1.65s/it]11/15/2022 17:47:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.5745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:09 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1482/16548 [40:49<6:59:18,  1.67s/it]11/15/2022 17:47:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.9363e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:10 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1483/16548 [40:51<6:55:43,  1.66s/it]11/15/2022 17:47:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.7230e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:12 - INFO - train.train_snli_ve - loss is tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1484/16548 [40:53<6:53:57,  1.65s/it]11/15/2022 17:47:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.9267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:14 - INFO - train.train_snli_ve - loss is tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1485/16548 [40:54<6:53:19,  1.65s/it]11/15/2022 17:47:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.0538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:15 - INFO - train.train_snli_ve - loss is tensor(1.0010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1486/16548 [40:56<6:50:40,  1.64s/it]11/15/2022 17:47:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.7970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:17 - INFO - train.train_snli_ve - loss is tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1487/16548 [40:58<6:53:39,  1.65s/it]11/15/2022 17:47:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:19 - INFO - train.train_snli_ve - loss is tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1488/16548 [40:59<6:54:12,  1.65s/it]11/15/2022 17:47:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:20 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1489/16548 [41:01<6:52:03,  1.64s/it]11/15/2022 17:47:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.4204e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:22 - INFO - train.train_snli_ve - loss is tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1490/16548 [41:03<6:54:24,  1.65s/it]11/15/2022 17:47:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.6819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:24 - INFO - train.train_snli_ve - loss is tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1491/16548 [41:04<6:56:11,  1.66s/it]11/15/2022 17:47:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.6432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:25 - INFO - train.train_snli_ve - loss is tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1492/16548 [41:06<6:56:24,  1.66s/it]11/15/2022 17:47:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.1722e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:27 - INFO - train.train_snli_ve - loss is tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1493/16548 [41:08<6:58:07,  1.67s/it]11/15/2022 17:47:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.8041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:29 - INFO - train.train_snli_ve - loss is tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1494/16548 [41:09<6:57:55,  1.67s/it]11/15/2022 17:47:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.0009e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:30 - INFO - train.train_snli_ve - loss is tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1495/16548 [41:11<6:54:04,  1.65s/it]11/15/2022 17:47:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.1267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:32 - INFO - train.train_snli_ve - loss is tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1496/16548 [41:13<6:54:49,  1.65s/it]11/15/2022 17:47:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2983e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:34 - INFO - train.train_snli_ve - loss is tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1497/16548 [41:14<6:54:41,  1.65s/it]11/15/2022 17:47:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.8536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:35 - INFO - train.train_snli_ve - loss is tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1498/16548 [41:16<6:53:38,  1.65s/it]11/15/2022 17:47:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.1445e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:37 - INFO - train.train_snli_ve - loss is tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1499/16548 [41:17<6:50:29,  1.64s/it]11/15/2022 17:47:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.3491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:38 - INFO - train.train_snli_ve - loss is tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1500/16548 [41:19<6:55:01,  1.65s/it]11/15/2022 17:47:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.2832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:40 - INFO - train.train_snli_ve - loss is tensor(0.8294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1501/16548 [41:21<6:54:43,  1.65s/it]11/15/2022 17:47:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.6424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:42 - INFO - train.train_snli_ve - loss is tensor(0.8809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1502/16548 [41:22<6:54:10,  1.65s/it]11/15/2022 17:47:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.8387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:43 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1503/16548 [41:24<6:54:59,  1.65s/it]11/15/2022 17:47:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.4908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:45 - INFO - train.train_snli_ve - loss is tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1504/16548 [41:26<6:54:51,  1.65s/it]11/15/2022 17:47:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.1223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:47 - INFO - train.train_snli_ve - loss is tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1505/16548 [41:27<6:56:33,  1.66s/it]11/15/2022 17:47:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.2499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:48 - INFO - train.train_snli_ve - loss is tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1506/16548 [41:29<6:55:23,  1.66s/it]11/15/2022 17:47:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.8368e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:50 - INFO - train.train_snli_ve - loss is tensor(0.8890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1507/16548 [41:31<6:52:46,  1.65s/it]11/15/2022 17:47:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.9250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:52 - INFO - train.train_snli_ve - loss is tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1508/16548 [41:32<6:53:21,  1.65s/it]11/15/2022 17:47:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.2406e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:53 - INFO - train.train_snli_ve - loss is tensor(1.0721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1509/16548 [41:34<6:51:50,  1.64s/it]11/15/2022 17:47:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.9446e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:55 - INFO - train.train_snli_ve - loss is tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1510/16548 [41:36<6:49:18,  1.63s/it]11/15/2022 17:47:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.7269e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:57 - INFO - train.train_snli_ve - loss is tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1511/16548 [41:37<6:49:44,  1.63s/it]11/15/2022 17:47:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.7842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:47:58 - INFO - train.train_snli_ve - loss is tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1512/16548 [41:39<6:52:26,  1.65s/it]11/15/2022 17:48:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.5330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:00 - INFO - train.train_snli_ve - loss is tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1513/16548 [41:41<6:48:47,  1.63s/it]11/15/2022 17:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4744e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:01 - INFO - train.train_snli_ve - loss is tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1514/16548 [41:42<6:49:23,  1.63s/it]11/15/2022 17:48:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:03 - INFO - train.train_snli_ve - loss is tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1515/16548 [41:44<6:47:47,  1.63s/it]11/15/2022 17:48:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:05 - INFO - train.train_snli_ve - loss is tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1516/16548 [41:45<6:46:50,  1.62s/it]11/15/2022 17:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:06 - INFO - train.train_snli_ve - loss is tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1517/16548 [41:47<6:46:30,  1.62s/it]11/15/2022 17:48:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:08 - INFO - train.train_snli_ve - loss is tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1518/16548 [41:49<6:48:14,  1.63s/it]11/15/2022 17:48:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.4458e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:10 - INFO - train.train_snli_ve - loss is tensor(0.9518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1519/16548 [41:50<6:44:57,  1.62s/it]11/15/2022 17:48:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.5343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:11 - INFO - train.train_snli_ve - loss is tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1520/16548 [41:52<6:43:57,  1.61s/it]11/15/2022 17:48:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.4793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:13 - INFO - train.train_snli_ve - loss is tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1521/16548 [41:53<6:44:43,  1.62s/it]11/15/2022 17:48:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:14 - INFO - train.train_snli_ve - loss is tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1522/16548 [41:55<6:49:55,  1.64s/it]11/15/2022 17:48:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.0717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:16 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1523/16548 [41:57<6:45:42,  1.62s/it]11/15/2022 17:48:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.3244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:18 - INFO - train.train_snli_ve - loss is tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1524/16548 [41:58<6:44:50,  1.62s/it]11/15/2022 17:48:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.9240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:19 - INFO - train.train_snli_ve - loss is tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1525/16548 [42:00<6:43:08,  1.61s/it]11/15/2022 17:48:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.5541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:21 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1526/16548 [42:02<6:44:15,  1.61s/it]11/15/2022 17:48:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4915e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:23 - INFO - train.train_snli_ve - loss is tensor(0.9604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1527/16548 [42:03<6:48:41,  1.63s/it]11/15/2022 17:48:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.0463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:24 - INFO - train.train_snli_ve - loss is tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1528/16548 [42:05<6:45:24,  1.62s/it]11/15/2022 17:48:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.6981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:26 - INFO - train.train_snli_ve - loss is tensor(0.7875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1529/16548 [42:06<6:43:51,  1.61s/it]11/15/2022 17:48:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.1585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:27 - INFO - train.train_snli_ve - loss is tensor(0.8840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1530/16548 [42:08<6:52:28,  1.65s/it]11/15/2022 17:48:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.5681e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:29 - INFO - train.train_snli_ve - loss is tensor(0.9225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1531/16548 [42:10<6:48:52,  1.63s/it]11/15/2022 17:48:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.8735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:31 - INFO - train.train_snli_ve - loss is tensor(0.9023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1532/16548 [42:11<6:47:44,  1.63s/it]11/15/2022 17:48:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.9724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:32 - INFO - train.train_snli_ve - loss is tensor(0.7822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1533/16548 [42:13<6:46:10,  1.62s/it]11/15/2022 17:48:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.0685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:34 - INFO - train.train_snli_ve - loss is tensor(0.9427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1534/16548 [42:15<6:45:10,  1.62s/it]11/15/2022 17:48:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.9522e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:35 - INFO - train.train_snli_ve - loss is tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1535/16548 [42:16<6:44:35,  1.62s/it]11/15/2022 17:48:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.6187e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:37 - INFO - train.train_snli_ve - loss is tensor(0.8397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1536/16548 [42:18<6:43:48,  1.61s/it]11/15/2022 17:48:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.0438e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:39 - INFO - train.train_snli_ve - loss is tensor(0.8543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1537/16548 [42:19<6:41:48,  1.61s/it]11/15/2022 17:48:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.2746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:40 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1538/16548 [42:21<6:44:34,  1.62s/it]11/15/2022 17:48:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.7687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:42 - INFO - train.train_snli_ve - loss is tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1539/16548 [42:23<6:43:17,  1.61s/it]11/15/2022 17:48:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.0460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:44 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1540/16548 [42:24<6:41:12,  1.60s/it]11/15/2022 17:48:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.1052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:45 - INFO - train.train_snli_ve - loss is tensor(0.8047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1541/16548 [42:26<6:42:50,  1.61s/it]11/15/2022 17:48:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.3068e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:47 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1542/16548 [42:28<6:44:11,  1.62s/it]11/15/2022 17:48:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.9793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:48 - INFO - train.train_snli_ve - loss is tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1543/16548 [42:29<6:42:32,  1.61s/it]11/15/2022 17:48:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.8029e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:50 - INFO - train.train_snli_ve - loss is tensor(0.7827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1544/16548 [42:31<6:42:44,  1.61s/it]11/15/2022 17:48:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:52 - INFO - train.train_snli_ve - loss is tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1545/16548 [42:32<6:44:33,  1.62s/it]11/15/2022 17:48:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.4678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:53 - INFO - train.train_snli_ve - loss is tensor(0.4817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1546/16548 [42:34<6:44:44,  1.62s/it]11/15/2022 17:48:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.9677e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:55 - INFO - train.train_snli_ve - loss is tensor(0.8371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1547/16548 [42:36<6:42:14,  1.61s/it]11/15/2022 17:48:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.9130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:56 - INFO - train.train_snli_ve - loss is tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1548/16548 [42:37<6:41:08,  1.60s/it]11/15/2022 17:48:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.1485e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:48:58 - INFO - train.train_snli_ve - loss is tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1549/16548 [42:39<6:41:54,  1.61s/it]11/15/2022 17:49:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.1786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:00 - INFO - train.train_snli_ve - loss is tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1550/16548 [42:40<6:43:12,  1.61s/it]11/15/2022 17:49:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:01 - INFO - train.train_snli_ve - loss is tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1551/16548 [42:42<6:45:03,  1.62s/it]11/15/2022 17:49:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.9695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:03 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1552/16548 [42:44<6:42:06,  1.61s/it]11/15/2022 17:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.1032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:04 - INFO - train.train_snli_ve - loss is tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1553/16548 [42:45<6:40:22,  1.60s/it]11/15/2022 17:49:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0827e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:06 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1554/16548 [42:47<6:40:43,  1.60s/it]11/15/2022 17:49:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.4540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:08 - INFO - train.train_snli_ve - loss is tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1555/16548 [42:48<6:39:34,  1.60s/it]11/15/2022 17:49:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.1766e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:09 - INFO - train.train_snli_ve - loss is tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1556/16548 [42:50<6:40:48,  1.60s/it]11/15/2022 17:49:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.4732e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:11 - INFO - train.train_snli_ve - loss is tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1557/16548 [42:52<6:42:12,  1.61s/it]11/15/2022 17:49:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.7359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:13 - INFO - train.train_snli_ve - loss is tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1558/16548 [42:53<6:41:59,  1.61s/it]11/15/2022 17:49:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.3951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:14 - INFO - train.train_snli_ve - loss is tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1559/16548 [42:55<6:42:00,  1.61s/it]11/15/2022 17:49:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.5515e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:16 - INFO - train.train_snli_ve - loss is tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1560/16548 [42:56<6:43:09,  1.61s/it]11/15/2022 17:49:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.2595e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:17 - INFO - train.train_snli_ve - loss is tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1561/16548 [42:58<6:40:59,  1.61s/it]11/15/2022 17:49:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.4310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:19 - INFO - train.train_snli_ve - loss is tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1562/16548 [43:00<6:43:37,  1.62s/it]11/15/2022 17:49:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.7273e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:21 - INFO - train.train_snli_ve - loss is tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1563/16548 [43:01<6:42:24,  1.61s/it]11/15/2022 17:49:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.4566e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:22 - INFO - train.train_snli_ve - loss is tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1564/16548 [43:03<6:44:39,  1.62s/it]11/15/2022 17:49:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.7568e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:24 - INFO - train.train_snli_ve - loss is tensor(0.7608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1565/16548 [43:05<6:44:14,  1.62s/it]11/15/2022 17:49:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.8427e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:25 - INFO - train.train_snli_ve - loss is tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1566/16548 [43:06<6:43:07,  1.61s/it]11/15/2022 17:49:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.3832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:27 - INFO - train.train_snli_ve - loss is tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1567/16548 [43:08<6:43:21,  1.62s/it]11/15/2022 17:49:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.8981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:29 - INFO - train.train_snli_ve - loss is tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1568/16548 [43:09<6:45:03,  1.62s/it]11/15/2022 17:49:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.1960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:30 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1569/16548 [43:11<6:43:06,  1.61s/it]11/15/2022 17:49:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.7354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:32 - INFO - train.train_snli_ve - loss is tensor(0.7953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1570/16548 [43:13<6:42:37,  1.61s/it]11/15/2022 17:49:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.6650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:34 - INFO - train.train_snli_ve - loss is tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1571/16548 [43:14<6:42:40,  1.61s/it]11/15/2022 17:49:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.1300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:35 - INFO - train.train_snli_ve - loss is tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9% 1572/16548 [43:16<6:42:07,  1.61s/it]11/15/2022 17:49:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.8322e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:37 - INFO - train.train_snli_ve - loss is tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1573/16548 [43:17<6:40:06,  1.60s/it]11/15/2022 17:49:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.7811e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:38 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1574/16548 [43:19<6:44:59,  1.62s/it]11/15/2022 17:49:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.9750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:40 - INFO - train.train_snli_ve - loss is tensor(0.8908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1575/16548 [43:21<6:44:36,  1.62s/it]11/15/2022 17:49:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.7717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:42 - INFO - train.train_snli_ve - loss is tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1576/16548 [43:22<6:46:31,  1.63s/it]11/15/2022 17:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.1542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:43 - INFO - train.train_snli_ve - loss is tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1577/16548 [43:24<6:49:06,  1.64s/it]11/15/2022 17:49:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.2978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:45 - INFO - train.train_snli_ve - loss is tensor(0.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1578/16548 [43:26<6:47:55,  1.63s/it]11/15/2022 17:49:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.3754e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:47 - INFO - train.train_snli_ve - loss is tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1579/16548 [43:27<6:44:19,  1.62s/it]11/15/2022 17:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.7890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:48 - INFO - train.train_snli_ve - loss is tensor(1.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1580/16548 [43:29<6:43:17,  1.62s/it]11/15/2022 17:49:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.4233e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:50 - INFO - train.train_snli_ve - loss is tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1581/16548 [43:30<6:43:38,  1.62s/it]11/15/2022 17:49:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.0847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:51 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1582/16548 [43:32<6:48:02,  1.64s/it]11/15/2022 17:49:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.8964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:53 - INFO - train.train_snli_ve - loss is tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1583/16548 [43:34<6:45:50,  1.63s/it]11/15/2022 17:49:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.5512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:55 - INFO - train.train_snli_ve - loss is tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1584/16548 [43:35<6:52:16,  1.65s/it]11/15/2022 17:49:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.3879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:56 - INFO - train.train_snli_ve - loss is tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1585/16548 [43:37<6:53:43,  1.66s/it]11/15/2022 17:49:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.0207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:49:58 - INFO - train.train_snli_ve - loss is tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1586/16548 [43:39<6:55:04,  1.66s/it]11/15/2022 17:50:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.8814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:00 - INFO - train.train_snli_ve - loss is tensor(0.7518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1587/16548 [43:40<6:51:27,  1.65s/it]11/15/2022 17:50:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.2964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:01 - INFO - train.train_snli_ve - loss is tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1588/16548 [43:42<6:50:37,  1.65s/it]11/15/2022 17:50:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.8650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:03 - INFO - train.train_snli_ve - loss is tensor(0.8243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1589/16548 [43:44<6:48:54,  1.64s/it]11/15/2022 17:50:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.7162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:05 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1590/16548 [43:45<6:51:40,  1.65s/it]11/15/2022 17:50:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.8258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:06 - INFO - train.train_snli_ve - loss is tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1591/16548 [43:47<6:52:12,  1.65s/it]11/15/2022 17:50:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.6489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:08 - INFO - train.train_snli_ve - loss is tensor(0.7855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1592/16548 [43:49<6:49:24,  1.64s/it]11/15/2022 17:50:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.4274e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:10 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1593/16548 [43:50<6:47:50,  1.64s/it]11/15/2022 17:50:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.0198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:11 - INFO - train.train_snli_ve - loss is tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1594/16548 [43:52<6:48:03,  1.64s/it]11/15/2022 17:50:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.3424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:13 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1595/16548 [43:54<6:47:16,  1.63s/it]11/15/2022 17:50:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.7675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:14 - INFO - train.train_snli_ve - loss is tensor(0.7729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1596/16548 [43:55<6:51:14,  1.65s/it]11/15/2022 17:50:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.8719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:16 - INFO - train.train_snli_ve - loss is tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1597/16548 [43:57<6:50:10,  1.65s/it]11/15/2022 17:50:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.2636e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:18 - INFO - train.train_snli_ve - loss is tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1598/16548 [43:59<6:50:53,  1.65s/it]11/15/2022 17:50:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.0462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:19 - INFO - train.train_snli_ve - loss is tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1599/16548 [44:00<6:55:54,  1.67s/it]11/15/2022 17:50:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.8788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:21 - INFO - train.train_snli_ve - loss is tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1600/16548 [44:02<7:06:03,  1.71s/it]11/15/2022 17:50:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.6556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:23 - INFO - train.train_snli_ve - loss is tensor(0.9373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1601/16548 [44:04<7:01:34,  1.69s/it]11/15/2022 17:50:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.1374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:25 - INFO - train.train_snli_ve - loss is tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1602/16548 [44:05<7:02:30,  1.70s/it]11/15/2022 17:50:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.9384e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:26 - INFO - train.train_snli_ve - loss is tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1603/16548 [44:07<6:58:49,  1.68s/it]11/15/2022 17:50:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.1239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:28 - INFO - train.train_snli_ve - loss is tensor(1.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1604/16548 [44:09<6:58:37,  1.68s/it]11/15/2022 17:50:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.3746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:30 - INFO - train.train_snli_ve - loss is tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1605/16548 [44:10<6:54:25,  1.66s/it]11/15/2022 17:50:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.2705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:31 - INFO - train.train_snli_ve - loss is tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1606/16548 [44:12<6:55:47,  1.67s/it]11/15/2022 17:50:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.4186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:33 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1607/16548 [44:14<6:53:20,  1.66s/it]11/15/2022 17:50:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.4897e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:35 - INFO - train.train_snli_ve - loss is tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1608/16548 [44:15<6:54:36,  1.67s/it]11/15/2022 17:50:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.4763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:36 - INFO - train.train_snli_ve - loss is tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1609/16548 [44:17<6:59:18,  1.68s/it]11/15/2022 17:50:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.7377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:38 - INFO - train.train_snli_ve - loss is tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1610/16548 [44:19<6:59:40,  1.69s/it]11/15/2022 17:50:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.0774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:40 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1611/16548 [44:20<7:01:32,  1.69s/it]11/15/2022 17:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.7410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:42 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1612/16548 [44:22<7:09:54,  1.73s/it]11/15/2022 17:50:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:43 - INFO - train.train_snli_ve - loss is tensor(0.8361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1613/16548 [44:24<7:03:19,  1.70s/it]11/15/2022 17:50:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.2220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:45 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1614/16548 [44:26<7:02:42,  1.70s/it]11/15/2022 17:50:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.5809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:47 - INFO - train.train_snli_ve - loss is tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1615/16548 [44:27<7:02:28,  1.70s/it]11/15/2022 17:50:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.8227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:48 - INFO - train.train_snli_ve - loss is tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1616/16548 [44:29<7:00:41,  1.69s/it]11/15/2022 17:50:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:50 - INFO - train.train_snli_ve - loss is tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1617/16548 [44:31<6:57:10,  1.68s/it]11/15/2022 17:50:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.7476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:52 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1618/16548 [44:32<6:54:19,  1.67s/it]11/15/2022 17:50:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.8826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:53 - INFO - train.train_snli_ve - loss is tensor(0.9490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1619/16548 [44:34<6:53:09,  1.66s/it]11/15/2022 17:50:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.2608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:55 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1620/16548 [44:36<6:51:58,  1.66s/it]11/15/2022 17:50:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.9729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:57 - INFO - train.train_snli_ve - loss is tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1621/16548 [44:37<6:57:35,  1.68s/it]11/15/2022 17:50:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.3570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:50:58 - INFO - train.train_snli_ve - loss is tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1622/16548 [44:39<6:56:39,  1.67s/it]11/15/2022 17:51:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.7107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:00 - INFO - train.train_snli_ve - loss is tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1623/16548 [44:41<6:55:22,  1.67s/it]11/15/2022 17:51:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.7913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:02 - INFO - train.train_snli_ve - loss is tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1624/16548 [44:42<6:55:54,  1.67s/it]11/15/2022 17:51:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.3684e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:03 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1625/16548 [44:44<6:55:59,  1.67s/it]11/15/2022 17:51:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.2336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:05 - INFO - train.train_snli_ve - loss is tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1626/16548 [44:46<6:55:31,  1.67s/it]11/15/2022 17:51:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.8096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:07 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1627/16548 [44:47<6:54:20,  1.67s/it]11/15/2022 17:51:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.7738e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:08 - INFO - train.train_snli_ve - loss is tensor(1.0068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1628/16548 [44:49<6:54:55,  1.67s/it]11/15/2022 17:51:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.5303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:10 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1629/16548 [44:51<7:01:14,  1.69s/it]11/15/2022 17:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.9503e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:12 - INFO - train.train_snli_ve - loss is tensor(0.7208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1630/16548 [44:52<6:58:17,  1.68s/it]11/15/2022 17:51:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.8397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:13 - INFO - train.train_snli_ve - loss is tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1631/16548 [44:54<6:59:24,  1.69s/it]11/15/2022 17:51:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.7187e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:15 - INFO - train.train_snli_ve - loss is tensor(0.8869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1632/16548 [44:56<7:01:15,  1.69s/it]11/15/2022 17:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.5899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:17 - INFO - train.train_snli_ve - loss is tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1633/16548 [44:58<7:03:35,  1.70s/it]11/15/2022 17:51:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.8723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:18 - INFO - train.train_snli_ve - loss is tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1634/16548 [44:59<7:02:36,  1.70s/it]11/15/2022 17:51:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.3246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:20 - INFO - train.train_snli_ve - loss is tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1635/16548 [45:01<6:59:53,  1.69s/it]11/15/2022 17:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.4761e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:22 - INFO - train.train_snli_ve - loss is tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1636/16548 [45:03<7:00:42,  1.69s/it]11/15/2022 17:51:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.5478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:23 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1637/16548 [45:04<6:56:20,  1.68s/it]11/15/2022 17:51:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.3759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:25 - INFO - train.train_snli_ve - loss is tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1638/16548 [45:06<6:55:15,  1.67s/it]11/15/2022 17:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.6883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:27 - INFO - train.train_snli_ve - loss is tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1639/16548 [45:08<6:56:11,  1.67s/it]11/15/2022 17:51:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.9886e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:28 - INFO - train.train_snli_ve - loss is tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1640/16548 [45:09<6:55:17,  1.67s/it]11/15/2022 17:51:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.1831e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:30 - INFO - train.train_snli_ve - loss is tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1641/16548 [45:11<6:54:03,  1.67s/it]11/15/2022 17:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.5821e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:32 - INFO - train.train_snli_ve - loss is tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1642/16548 [45:13<6:54:36,  1.67s/it]11/15/2022 17:51:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.0357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:33 - INFO - train.train_snli_ve - loss is tensor(0.8692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1643/16548 [45:14<6:52:17,  1.66s/it]11/15/2022 17:51:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.4476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:35 - INFO - train.train_snli_ve - loss is tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1644/16548 [45:16<6:55:47,  1.67s/it]11/15/2022 17:51:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.2535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:37 - INFO - train.train_snli_ve - loss is tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1645/16548 [45:18<6:55:48,  1.67s/it]11/15/2022 17:51:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9288e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:38 - INFO - train.train_snli_ve - loss is tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1646/16548 [45:19<6:54:28,  1.67s/it]11/15/2022 17:51:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.3447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:40 - INFO - train.train_snli_ve - loss is tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1647/16548 [45:21<6:54:27,  1.67s/it]11/15/2022 17:51:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.4673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:42 - INFO - train.train_snli_ve - loss is tensor(0.5810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1648/16548 [45:23<6:55:29,  1.67s/it]11/15/2022 17:51:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.3173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:43 - INFO - train.train_snli_ve - loss is tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1649/16548 [45:24<6:55:22,  1.67s/it]11/15/2022 17:51:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.7738e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:45 - INFO - train.train_snli_ve - loss is tensor(0.6830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1650/16548 [45:26<6:53:37,  1.67s/it]11/15/2022 17:51:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.4465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:47 - INFO - train.train_snli_ve - loss is tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1651/16548 [45:28<6:50:26,  1.65s/it]11/15/2022 17:51:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.7774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:48 - INFO - train.train_snli_ve - loss is tensor(0.8829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1652/16548 [45:29<6:49:54,  1.65s/it]11/15/2022 17:51:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.8105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:50 - INFO - train.train_snli_ve - loss is tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1653/16548 [45:31<6:52:40,  1.66s/it]11/15/2022 17:51:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.7589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:52 - INFO - train.train_snli_ve - loss is tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1654/16548 [45:32<6:51:57,  1.66s/it]11/15/2022 17:51:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.5909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:53 - INFO - train.train_snli_ve - loss is tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1655/16548 [45:34<6:48:40,  1.65s/it]11/15/2022 17:51:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.0335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:55 - INFO - train.train_snli_ve - loss is tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1656/16548 [45:36<6:50:19,  1.65s/it]11/15/2022 17:51:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.8464e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:57 - INFO - train.train_snli_ve - loss is tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1657/16548 [45:37<6:49:18,  1.65s/it]11/15/2022 17:51:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.5998e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:51:58 - INFO - train.train_snli_ve - loss is tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1658/16548 [45:39<6:50:43,  1.66s/it]11/15/2022 17:52:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.9193e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:00 - INFO - train.train_snli_ve - loss is tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1659/16548 [45:41<6:51:39,  1.66s/it]11/15/2022 17:52:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.1292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:02 - INFO - train.train_snli_ve - loss is tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1660/16548 [45:42<6:53:08,  1.66s/it]11/15/2022 17:52:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.3190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:03 - INFO - train.train_snli_ve - loss is tensor(0.9148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1661/16548 [45:44<6:51:21,  1.66s/it]11/15/2022 17:52:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.5610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:05 - INFO - train.train_snli_ve - loss is tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1662/16548 [45:46<6:51:17,  1.66s/it]11/15/2022 17:52:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.4249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:07 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1663/16548 [45:47<6:51:46,  1.66s/it]11/15/2022 17:52:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.4930e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:08 - INFO - train.train_snli_ve - loss is tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1664/16548 [45:49<6:50:27,  1.65s/it]11/15/2022 17:52:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.5991e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:10 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1665/16548 [45:51<6:55:16,  1.67s/it]11/15/2022 17:52:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:12 - INFO - train.train_snli_ve - loss is tensor(0.8893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1666/16548 [45:52<6:53:03,  1.67s/it]11/15/2022 17:52:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.6356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:13 - INFO - train.train_snli_ve - loss is tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1667/16548 [45:54<6:58:11,  1.69s/it]11/15/2022 17:52:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.6296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:15 - INFO - train.train_snli_ve - loss is tensor(0.7939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1668/16548 [45:56<6:56:45,  1.68s/it]11/15/2022 17:52:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.4946e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:17 - INFO - train.train_snli_ve - loss is tensor(0.8425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1669/16548 [45:57<6:53:04,  1.67s/it]11/15/2022 17:52:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.9925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:18 - INFO - train.train_snli_ve - loss is tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1670/16548 [45:59<6:50:39,  1.66s/it]11/15/2022 17:52:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.7465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:20 - INFO - train.train_snli_ve - loss is tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1671/16548 [46:01<6:52:40,  1.66s/it]11/15/2022 17:52:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.3521e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:22 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1672/16548 [46:02<6:50:23,  1.66s/it]11/15/2022 17:52:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.3037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:23 - INFO - train.train_snli_ve - loss is tensor(0.7933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1673/16548 [46:04<6:52:01,  1.66s/it]11/15/2022 17:52:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.5272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:25 - INFO - train.train_snli_ve - loss is tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1674/16548 [46:06<6:49:25,  1.65s/it]11/15/2022 17:52:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.3410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:27 - INFO - train.train_snli_ve - loss is tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1675/16548 [46:07<6:49:49,  1.65s/it]11/15/2022 17:52:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.8542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:28 - INFO - train.train_snli_ve - loss is tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1676/16548 [46:09<6:47:35,  1.64s/it]11/15/2022 17:52:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.0290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:30 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1677/16548 [46:11<6:49:01,  1.65s/it]11/15/2022 17:52:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.0332e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:32 - INFO - train.train_snli_ve - loss is tensor(1.0541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1678/16548 [46:12<6:53:53,  1.67s/it]11/15/2022 17:52:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:33 - INFO - train.train_snli_ve - loss is tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1679/16548 [46:14<6:52:09,  1.66s/it]11/15/2022 17:52:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.8176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:35 - INFO - train.train_snli_ve - loss is tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1680/16548 [46:16<6:51:22,  1.66s/it]11/15/2022 17:52:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.1391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:37 - INFO - train.train_snli_ve - loss is tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1681/16548 [46:17<6:50:30,  1.66s/it]11/15/2022 17:52:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4691e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:38 - INFO - train.train_snli_ve - loss is tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1682/16548 [46:19<6:49:51,  1.65s/it]11/15/2022 17:52:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.1048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:40 - INFO - train.train_snli_ve - loss is tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1683/16548 [46:21<6:52:21,  1.66s/it]11/15/2022 17:52:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.7988e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:42 - INFO - train.train_snli_ve - loss is tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1684/16548 [46:22<6:52:48,  1.67s/it]11/15/2022 17:52:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.1081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:43 - INFO - train.train_snli_ve - loss is tensor(0.9138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1685/16548 [46:24<6:53:44,  1.67s/it]11/15/2022 17:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.5032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:45 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1686/16548 [46:26<6:53:45,  1.67s/it]11/15/2022 17:52:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.0757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:47 - INFO - train.train_snli_ve - loss is tensor(0.7808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1687/16548 [46:27<6:48:52,  1.65s/it]11/15/2022 17:52:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.9472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:48 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1688/16548 [46:29<6:53:13,  1.67s/it]11/15/2022 17:52:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.7270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:50 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1689/16548 [46:31<6:54:28,  1.67s/it]11/15/2022 17:52:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.4737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:52 - INFO - train.train_snli_ve - loss is tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1690/16548 [46:32<6:53:26,  1.67s/it]11/15/2022 17:52:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.0553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:53 - INFO - train.train_snli_ve - loss is tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1691/16548 [46:34<6:52:35,  1.67s/it]11/15/2022 17:52:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.3087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:55 - INFO - train.train_snli_ve - loss is tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1692/16548 [46:36<6:54:18,  1.67s/it]11/15/2022 17:52:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.5089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:57 - INFO - train.train_snli_ve - loss is tensor(0.8970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1693/16548 [46:37<6:54:19,  1.67s/it]11/15/2022 17:52:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.6625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:52:58 - INFO - train.train_snli_ve - loss is tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1694/16548 [46:39<6:57:16,  1.69s/it]11/15/2022 17:53:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.7224e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:00 - INFO - train.train_snli_ve - loss is tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1695/16548 [46:41<6:53:53,  1.67s/it]11/15/2022 17:53:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.1077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:02 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1696/16548 [46:42<6:53:41,  1.67s/it]11/15/2022 17:53:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.2241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:03 - INFO - train.train_snli_ve - loss is tensor(0.8098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1697/16548 [46:44<6:52:43,  1.67s/it]11/15/2022 17:53:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.1660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:05 - INFO - train.train_snli_ve - loss is tensor(0.8229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1698/16548 [46:46<6:52:33,  1.67s/it]11/15/2022 17:53:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.4730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:07 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1699/16548 [46:47<6:52:02,  1.66s/it]11/15/2022 17:53:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.6901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:08 - INFO - train.train_snli_ve - loss is tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1700/16548 [46:49<7:03:29,  1.71s/it]11/15/2022 17:53:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.7287e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:10 - INFO - train.train_snli_ve - loss is tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1701/16548 [46:51<7:00:04,  1.70s/it]11/15/2022 17:53:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.2258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:12 - INFO - train.train_snli_ve - loss is tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1702/16548 [46:53<7:05:00,  1.72s/it]11/15/2022 17:53:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.7780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:14 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1703/16548 [46:54<7:01:08,  1.70s/it]11/15/2022 17:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.7604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:15 - INFO - train.train_snli_ve - loss is tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1704/16548 [46:56<6:58:53,  1.69s/it]11/15/2022 17:53:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.2010e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:17 - INFO - train.train_snli_ve - loss is tensor(0.8701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1705/16548 [46:58<6:55:01,  1.68s/it]11/15/2022 17:53:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.9855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:18 - INFO - train.train_snli_ve - loss is tensor(1.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1706/16548 [46:59<6:52:43,  1.67s/it]11/15/2022 17:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.6175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:20 - INFO - train.train_snli_ve - loss is tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1707/16548 [47:01<6:55:22,  1.68s/it]11/15/2022 17:53:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.9111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:22 - INFO - train.train_snli_ve - loss is tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1708/16548 [47:03<6:53:58,  1.67s/it]11/15/2022 17:53:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.3576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:23 - INFO - train.train_snli_ve - loss is tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1709/16548 [47:04<6:51:14,  1.66s/it]11/15/2022 17:53:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.2052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:25 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1710/16548 [47:06<6:51:13,  1.66s/it]11/15/2022 17:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.9687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:27 - INFO - train.train_snli_ve - loss is tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1711/16548 [47:08<6:49:49,  1.66s/it]11/15/2022 17:53:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.7873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:28 - INFO - train.train_snli_ve - loss is tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1712/16548 [47:09<6:50:17,  1.66s/it]11/15/2022 17:53:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.3077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:30 - INFO - train.train_snli_ve - loss is tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1713/16548 [47:11<6:53:10,  1.67s/it]11/15/2022 17:53:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:32 - INFO - train.train_snli_ve - loss is tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1714/16548 [47:13<6:50:48,  1.66s/it]11/15/2022 17:53:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.6112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:33 - INFO - train.train_snli_ve - loss is tensor(0.9689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1715/16548 [47:14<6:50:22,  1.66s/it]11/15/2022 17:53:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.6909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:35 - INFO - train.train_snli_ve - loss is tensor(0.9037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1716/16548 [47:16<6:51:58,  1.67s/it]11/15/2022 17:53:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:37 - INFO - train.train_snli_ve - loss is tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1717/16548 [47:18<6:50:23,  1.66s/it]11/15/2022 17:53:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4681e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:38 - INFO - train.train_snli_ve - loss is tensor(0.8498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1718/16548 [47:19<6:51:38,  1.67s/it]11/15/2022 17:53:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8171e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:40 - INFO - train.train_snli_ve - loss is tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1719/16548 [47:21<6:49:53,  1.66s/it]11/15/2022 17:53:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.8077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:42 - INFO - train.train_snli_ve - loss is tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1720/16548 [47:22<6:49:18,  1.66s/it]11/15/2022 17:53:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.9023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:43 - INFO - train.train_snli_ve - loss is tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1721/16548 [47:24<6:50:48,  1.66s/it]11/15/2022 17:53:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.5159e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:45 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1722/16548 [47:26<6:50:23,  1.66s/it]11/15/2022 17:53:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.3603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:47 - INFO - train.train_snli_ve - loss is tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1723/16548 [47:27<6:50:34,  1.66s/it]11/15/2022 17:53:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.2813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:48 - INFO - train.train_snli_ve - loss is tensor(0.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1724/16548 [47:29<6:53:34,  1.67s/it]11/15/2022 17:53:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.4018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:50 - INFO - train.train_snli_ve - loss is tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1725/16548 [47:31<6:53:26,  1.67s/it]11/15/2022 17:53:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.7121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:52 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1726/16548 [47:33<6:52:17,  1.67s/it]11/15/2022 17:53:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.5036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:54 - INFO - train.train_snli_ve - loss is tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1727/16548 [47:34<6:57:25,  1.69s/it]11/15/2022 17:53:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.4180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:55 - INFO - train.train_snli_ve - loss is tensor(0.9435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1728/16548 [47:36<6:59:58,  1.70s/it]11/15/2022 17:53:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.4352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:57 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1729/16548 [47:38<6:57:53,  1.69s/it]11/15/2022 17:53:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.0534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:53:59 - INFO - train.train_snli_ve - loss is tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1730/16548 [47:39<6:55:28,  1.68s/it]11/15/2022 17:54:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.9810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:00 - INFO - train.train_snli_ve - loss is tensor(0.7301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1731/16548 [47:41<6:51:58,  1.67s/it]11/15/2022 17:54:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.4803e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:02 - INFO - train.train_snli_ve - loss is tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1732/16548 [47:43<6:48:47,  1.66s/it]11/15/2022 17:54:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.1226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:03 - INFO - train.train_snli_ve - loss is tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1733/16548 [47:44<6:47:11,  1.65s/it]11/15/2022 17:54:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.4347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:05 - INFO - train.train_snli_ve - loss is tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1734/16548 [47:46<6:46:06,  1.64s/it]11/15/2022 17:54:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.4560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:07 - INFO - train.train_snli_ve - loss is tensor(1.0827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1735/16548 [47:47<6:45:17,  1.64s/it]11/15/2022 17:54:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.9432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:08 - INFO - train.train_snli_ve - loss is tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1736/16548 [47:49<6:52:57,  1.67s/it]11/15/2022 17:54:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.8984e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:10 - INFO - train.train_snli_ve - loss is tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10% 1737/16548 [47:51<6:53:53,  1.68s/it]11/15/2022 17:54:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.7244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:12 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1738/16548 [47:53<6:56:58,  1.69s/it]11/15/2022 17:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.0442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:14 - INFO - train.train_snli_ve - loss is tensor(0.8043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1739/16548 [47:54<6:53:48,  1.68s/it]11/15/2022 17:54:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.7842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:15 - INFO - train.train_snli_ve - loss is tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1740/16548 [47:56<6:51:57,  1.67s/it]11/15/2022 17:54:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.9590e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:17 - INFO - train.train_snli_ve - loss is tensor(0.8223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1741/16548 [47:58<6:54:37,  1.68s/it]11/15/2022 17:54:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.5199e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:19 - INFO - train.train_snli_ve - loss is tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1742/16548 [47:59<6:53:17,  1.67s/it]11/15/2022 17:54:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.4508e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:20 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1743/16548 [48:01<6:55:14,  1.68s/it]11/15/2022 17:54:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.8160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:22 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1744/16548 [48:03<6:54:42,  1.68s/it]11/15/2022 17:54:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.9452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:24 - INFO - train.train_snli_ve - loss is tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1745/16548 [48:04<6:52:22,  1.67s/it]11/15/2022 17:54:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.2881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:25 - INFO - train.train_snli_ve - loss is tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1746/16548 [48:06<6:51:42,  1.67s/it]11/15/2022 17:54:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.6123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:27 - INFO - train.train_snli_ve - loss is tensor(0.9933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1747/16548 [48:08<6:49:39,  1.66s/it]11/15/2022 17:54:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.6429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:29 - INFO - train.train_snli_ve - loss is tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1748/16548 [48:09<6:51:05,  1.67s/it]11/15/2022 17:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.4474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:30 - INFO - train.train_snli_ve - loss is tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1749/16548 [48:11<6:54:36,  1.68s/it]11/15/2022 17:54:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.7578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:32 - INFO - train.train_snli_ve - loss is tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1750/16548 [48:13<6:51:21,  1.67s/it]11/15/2022 17:54:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.8351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:34 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1751/16548 [48:14<6:51:15,  1.67s/it]11/15/2022 17:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.4940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:35 - INFO - train.train_snli_ve - loss is tensor(0.8560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1752/16548 [48:16<6:49:18,  1.66s/it]11/15/2022 17:54:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.1979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:37 - INFO - train.train_snli_ve - loss is tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1753/16548 [48:18<6:47:14,  1.65s/it]11/15/2022 17:54:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.5292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:39 - INFO - train.train_snli_ve - loss is tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1754/16548 [48:19<6:48:17,  1.66s/it]11/15/2022 17:54:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.1366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:40 - INFO - train.train_snli_ve - loss is tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1755/16548 [48:21<6:50:37,  1.67s/it]11/15/2022 17:54:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:42 - INFO - train.train_snli_ve - loss is tensor(0.8170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1756/16548 [48:23<6:50:58,  1.67s/it]11/15/2022 17:54:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.0708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:44 - INFO - train.train_snli_ve - loss is tensor(1.0054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1757/16548 [48:24<6:53:04,  1.68s/it]11/15/2022 17:54:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.4800e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:45 - INFO - train.train_snli_ve - loss is tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1758/16548 [48:26<6:49:29,  1.66s/it]11/15/2022 17:54:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.3429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:47 - INFO - train.train_snli_ve - loss is tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1759/16548 [48:28<6:48:52,  1.66s/it]11/15/2022 17:54:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.1921e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:49 - INFO - train.train_snli_ve - loss is tensor(0.8293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1760/16548 [48:29<6:52:12,  1.67s/it]11/15/2022 17:54:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.3724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:50 - INFO - train.train_snli_ve - loss is tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1761/16548 [48:31<6:54:21,  1.68s/it]11/15/2022 17:54:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.3073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:52 - INFO - train.train_snli_ve - loss is tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1762/16548 [48:33<6:54:48,  1.68s/it]11/15/2022 17:54:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.4369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:54 - INFO - train.train_snli_ve - loss is tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1763/16548 [48:34<6:52:31,  1.67s/it]11/15/2022 17:54:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.6724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:55 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1764/16548 [48:36<6:48:58,  1.66s/it]11/15/2022 17:54:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.8537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:57 - INFO - train.train_snli_ve - loss is tensor(0.9302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1765/16548 [48:38<6:53:24,  1.68s/it]11/15/2022 17:54:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.6795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:54:59 - INFO - train.train_snli_ve - loss is tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1766/16548 [48:39<6:52:20,  1.67s/it]11/15/2022 17:55:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.6491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:00 - INFO - train.train_snli_ve - loss is tensor(0.7086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1767/16548 [48:41<6:50:52,  1.67s/it]11/15/2022 17:55:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.9224e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:02 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1768/16548 [48:43<6:47:47,  1.66s/it]11/15/2022 17:55:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.0535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:04 - INFO - train.train_snli_ve - loss is tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1769/16548 [48:44<6:46:58,  1.65s/it]11/15/2022 17:55:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.9187e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:05 - INFO - train.train_snli_ve - loss is tensor(0.8786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1770/16548 [48:46<6:49:14,  1.66s/it]11/15/2022 17:55:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.4345e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:07 - INFO - train.train_snli_ve - loss is tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1771/16548 [48:48<6:49:15,  1.66s/it]11/15/2022 17:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.1906e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:09 - INFO - train.train_snli_ve - loss is tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1772/16548 [48:49<6:49:39,  1.66s/it]11/15/2022 17:55:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.1493e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:10 - INFO - train.train_snli_ve - loss is tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1773/16548 [48:51<6:52:57,  1.68s/it]11/15/2022 17:55:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.2739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:12 - INFO - train.train_snli_ve - loss is tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1774/16548 [48:53<6:56:28,  1.69s/it]11/15/2022 17:55:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.6045e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:14 - INFO - train.train_snli_ve - loss is tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1775/16548 [48:54<6:56:35,  1.69s/it]11/15/2022 17:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:15 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1776/16548 [48:56<6:54:01,  1.68s/it]11/15/2022 17:55:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.2407e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:17 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1777/16548 [48:58<6:53:56,  1.68s/it]11/15/2022 17:55:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:19 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1778/16548 [48:59<6:55:21,  1.69s/it]11/15/2022 17:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.0113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:20 - INFO - train.train_snli_ve - loss is tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1779/16548 [49:01<6:51:41,  1.67s/it]11/15/2022 17:55:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.8030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:22 - INFO - train.train_snli_ve - loss is tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1780/16548 [49:03<6:52:24,  1.68s/it]11/15/2022 17:55:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.5872e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:24 - INFO - train.train_snli_ve - loss is tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1781/16548 [49:04<6:49:52,  1.67s/it]11/15/2022 17:55:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.7307e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:25 - INFO - train.train_snli_ve - loss is tensor(1.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1782/16548 [49:06<6:47:49,  1.66s/it]11/15/2022 17:55:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.9354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:27 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1783/16548 [49:08<6:47:31,  1.66s/it]11/15/2022 17:55:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.0501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:29 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1784/16548 [49:09<6:48:29,  1.66s/it]11/15/2022 17:55:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.0115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:30 - INFO - train.train_snli_ve - loss is tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1785/16548 [49:11<6:48:33,  1.66s/it]11/15/2022 17:55:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.8942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:32 - INFO - train.train_snli_ve - loss is tensor(0.8948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1786/16548 [49:13<6:47:47,  1.66s/it]11/15/2022 17:55:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.0880e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:34 - INFO - train.train_snli_ve - loss is tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1787/16548 [49:14<6:47:03,  1.65s/it]11/15/2022 17:55:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.1923e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:35 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1788/16548 [49:16<6:49:29,  1.66s/it]11/15/2022 17:55:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.0939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:37 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1789/16548 [49:18<6:52:24,  1.68s/it]11/15/2022 17:55:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.1644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:39 - INFO - train.train_snli_ve - loss is tensor(0.8007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1790/16548 [49:19<6:50:04,  1.67s/it]11/15/2022 17:55:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.0293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:40 - INFO - train.train_snli_ve - loss is tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1791/16548 [49:21<6:53:14,  1.68s/it]11/15/2022 17:55:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.4553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:42 - INFO - train.train_snli_ve - loss is tensor(0.9293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1792/16548 [49:23<6:52:04,  1.68s/it]11/15/2022 17:55:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.9105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:44 - INFO - train.train_snli_ve - loss is tensor(0.7614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1793/16548 [49:24<6:51:02,  1.67s/it]11/15/2022 17:55:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.2315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:45 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1794/16548 [49:26<6:52:09,  1.68s/it]11/15/2022 17:55:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.4296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:47 - INFO - train.train_snli_ve - loss is tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1795/16548 [49:28<6:49:02,  1.66s/it]11/15/2022 17:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.9030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:49 - INFO - train.train_snli_ve - loss is tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1796/16548 [49:29<6:49:44,  1.67s/it]11/15/2022 17:55:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.2179e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:50 - INFO - train.train_snli_ve - loss is tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1797/16548 [49:31<6:49:47,  1.67s/it]11/15/2022 17:55:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.7290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:52 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1798/16548 [49:33<6:51:02,  1.67s/it]11/15/2022 17:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.6580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:54 - INFO - train.train_snli_ve - loss is tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1799/16548 [49:34<6:51:28,  1.67s/it]11/15/2022 17:55:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.4206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:55 - INFO - train.train_snli_ve - loss is tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1800/16548 [49:36<6:55:01,  1.69s/it]11/15/2022 17:55:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.7631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:57 - INFO - train.train_snli_ve - loss is tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1801/16548 [49:38<6:53:01,  1.68s/it]11/15/2022 17:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.5339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:55:59 - INFO - train.train_snli_ve - loss is tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1802/16548 [49:40<6:52:30,  1.68s/it]11/15/2022 17:56:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.0631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:00 - INFO - train.train_snli_ve - loss is tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1803/16548 [49:41<6:55:46,  1.69s/it]11/15/2022 17:56:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.1542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:02 - INFO - train.train_snli_ve - loss is tensor(0.7269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1804/16548 [49:43<6:55:35,  1.69s/it]11/15/2022 17:56:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.7400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:04 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1805/16548 [49:45<6:58:13,  1.70s/it]11/15/2022 17:56:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.2635e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:06 - INFO - train.train_snli_ve - loss is tensor(1.0020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1806/16548 [49:46<6:55:35,  1.69s/it]11/15/2022 17:56:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.1540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:07 - INFO - train.train_snli_ve - loss is tensor(0.8632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1807/16548 [49:48<6:51:42,  1.68s/it]11/15/2022 17:56:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.7356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:09 - INFO - train.train_snli_ve - loss is tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1808/16548 [49:50<6:53:33,  1.68s/it]11/15/2022 17:56:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.9924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:11 - INFO - train.train_snli_ve - loss is tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1809/16548 [49:51<6:51:27,  1.67s/it]11/15/2022 17:56:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.6973e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:12 - INFO - train.train_snli_ve - loss is tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1810/16548 [49:53<6:47:57,  1.66s/it]11/15/2022 17:56:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.6213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:14 - INFO - train.train_snli_ve - loss is tensor(0.8024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1811/16548 [49:55<6:47:59,  1.66s/it]11/15/2022 17:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.9202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:16 - INFO - train.train_snli_ve - loss is tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1812/16548 [49:56<6:47:26,  1.66s/it]11/15/2022 17:56:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.1413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:17 - INFO - train.train_snli_ve - loss is tensor(0.7493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1813/16548 [49:58<6:47:14,  1.66s/it]11/15/2022 17:56:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.2804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:19 - INFO - train.train_snli_ve - loss is tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1814/16548 [50:00<6:48:53,  1.67s/it]11/15/2022 17:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.4777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:21 - INFO - train.train_snli_ve - loss is tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1815/16548 [50:01<6:49:37,  1.67s/it]11/15/2022 17:56:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.6845e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:22 - INFO - train.train_snli_ve - loss is tensor(0.8062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1816/16548 [50:03<6:50:24,  1.67s/it]11/15/2022 17:56:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.0024e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:24 - INFO - train.train_snli_ve - loss is tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1817/16548 [50:05<6:47:33,  1.66s/it]11/15/2022 17:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.8512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:26 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1818/16548 [50:06<6:48:29,  1.66s/it]11/15/2022 17:56:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.9264e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:27 - INFO - train.train_snli_ve - loss is tensor(1.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1819/16548 [50:08<6:47:43,  1.66s/it]11/15/2022 17:56:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.8779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:29 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1820/16548 [50:10<6:46:52,  1.66s/it]11/15/2022 17:56:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.9486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:30 - INFO - train.train_snli_ve - loss is tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1821/16548 [50:11<6:45:59,  1.65s/it]11/15/2022 17:56:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.2238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:32 - INFO - train.train_snli_ve - loss is tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1822/16548 [50:13<6:46:31,  1.66s/it]11/15/2022 17:56:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.4393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:34 - INFO - train.train_snli_ve - loss is tensor(0.8819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1823/16548 [50:15<6:48:32,  1.66s/it]11/15/2022 17:56:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.0464e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:36 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1824/16548 [50:16<6:51:02,  1.67s/it]11/15/2022 17:56:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.3844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:37 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1825/16548 [50:18<6:53:37,  1.69s/it]11/15/2022 17:56:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.4824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:39 - INFO - train.train_snli_ve - loss is tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1826/16548 [50:20<6:51:28,  1.68s/it]11/15/2022 17:56:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.4094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:41 - INFO - train.train_snli_ve - loss is tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1827/16548 [50:21<6:50:06,  1.67s/it]11/15/2022 17:56:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.7088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:42 - INFO - train.train_snli_ve - loss is tensor(0.8565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1828/16548 [50:23<6:47:03,  1.66s/it]11/15/2022 17:56:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.8392e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:44 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1829/16548 [50:25<6:46:51,  1.66s/it]11/15/2022 17:56:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.5580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:45 - INFO - train.train_snli_ve - loss is tensor(0.8420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1830/16548 [50:26<6:47:24,  1.66s/it]11/15/2022 17:56:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.5652e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:47 - INFO - train.train_snli_ve - loss is tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1831/16548 [50:28<6:47:24,  1.66s/it]11/15/2022 17:56:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.0239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:49 - INFO - train.train_snli_ve - loss is tensor(0.7703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1832/16548 [50:30<6:46:44,  1.66s/it]11/15/2022 17:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:50 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1833/16548 [50:31<6:46:13,  1.66s/it]11/15/2022 17:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.8496e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:52 - INFO - train.train_snli_ve - loss is tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1834/16548 [50:33<6:50:10,  1.67s/it]11/15/2022 17:56:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.3869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:54 - INFO - train.train_snli_ve - loss is tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1835/16548 [50:35<6:50:34,  1.67s/it]11/15/2022 17:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.8151e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:55 - INFO - train.train_snli_ve - loss is tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1836/16548 [50:36<6:46:45,  1.66s/it]11/15/2022 17:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.2657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:57 - INFO - train.train_snli_ve - loss is tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1837/16548 [50:38<6:48:29,  1.67s/it]11/15/2022 17:56:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:56:59 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1838/16548 [50:40<6:48:57,  1.67s/it]11/15/2022 17:57:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.1593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:01 - INFO - train.train_snli_ve - loss is tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1839/16548 [50:41<6:51:14,  1.68s/it]11/15/2022 17:57:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.3277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:02 - INFO - train.train_snli_ve - loss is tensor(0.8630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1840/16548 [50:43<6:52:15,  1.68s/it]11/15/2022 17:57:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.1806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:04 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1841/16548 [50:45<6:48:03,  1.66s/it]11/15/2022 17:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.4857e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:05 - INFO - train.train_snli_ve - loss is tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1842/16548 [50:46<6:46:10,  1.66s/it]11/15/2022 17:57:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.8731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:07 - INFO - train.train_snli_ve - loss is tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1843/16548 [50:48<6:48:32,  1.67s/it]11/15/2022 17:57:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.3978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:09 - INFO - train.train_snli_ve - loss is tensor(0.9726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1844/16548 [50:50<6:51:01,  1.68s/it]11/15/2022 17:57:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.9025e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:11 - INFO - train.train_snli_ve - loss is tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1845/16548 [50:51<6:52:18,  1.68s/it]11/15/2022 17:57:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.1472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:12 - INFO - train.train_snli_ve - loss is tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1846/16548 [50:53<6:49:44,  1.67s/it]11/15/2022 17:57:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.2319e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:14 - INFO - train.train_snli_ve - loss is tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1847/16548 [50:55<6:47:14,  1.66s/it]11/15/2022 17:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.8752e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:15 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1848/16548 [50:56<6:45:31,  1.66s/it]11/15/2022 17:57:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.4091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:17 - INFO - train.train_snli_ve - loss is tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1849/16548 [50:58<6:46:13,  1.66s/it]11/15/2022 17:57:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.9190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:19 - INFO - train.train_snli_ve - loss is tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1850/16548 [51:00<6:45:21,  1.65s/it]11/15/2022 17:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.8310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:20 - INFO - train.train_snli_ve - loss is tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1851/16548 [51:01<6:47:42,  1.66s/it]11/15/2022 17:57:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.7670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:22 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1852/16548 [51:03<6:51:09,  1.68s/it]11/15/2022 17:57:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.0696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:24 - INFO - train.train_snli_ve - loss is tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1853/16548 [51:05<6:46:13,  1.66s/it]11/15/2022 17:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.3356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:25 - INFO - train.train_snli_ve - loss is tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1854/16548 [51:06<6:44:07,  1.65s/it]11/15/2022 17:57:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.7213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:27 - INFO - train.train_snli_ve - loss is tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1855/16548 [51:08<6:46:18,  1.66s/it]11/15/2022 17:57:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.8582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:29 - INFO - train.train_snli_ve - loss is tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1856/16548 [51:09<6:44:04,  1.65s/it]11/15/2022 17:57:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.8164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:30 - INFO - train.train_snli_ve - loss is tensor(0.9873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1857/16548 [51:11<6:45:45,  1.66s/it]11/15/2022 17:57:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.1382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:32 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1858/16548 [51:13<6:45:43,  1.66s/it]11/15/2022 17:57:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.7114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:34 - INFO - train.train_snli_ve - loss is tensor(0.9775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1859/16548 [51:14<6:45:21,  1.66s/it]11/15/2022 17:57:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:35 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1860/16548 [51:16<6:44:46,  1.65s/it]11/15/2022 17:57:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.3018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:37 - INFO - train.train_snli_ve - loss is tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1861/16548 [51:18<6:45:09,  1.66s/it]11/15/2022 17:57:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.0723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:39 - INFO - train.train_snli_ve - loss is tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1862/16548 [51:19<6:43:05,  1.65s/it]11/15/2022 17:57:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.9612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:40 - INFO - train.train_snli_ve - loss is tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1863/16548 [51:21<6:42:10,  1.64s/it]11/15/2022 17:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.2065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:42 - INFO - train.train_snli_ve - loss is tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1864/16548 [51:23<6:44:04,  1.65s/it]11/15/2022 17:57:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.3100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:44 - INFO - train.train_snli_ve - loss is tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1865/16548 [51:24<6:44:24,  1.65s/it]11/15/2022 17:57:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.2111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:45 - INFO - train.train_snli_ve - loss is tensor(0.8598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1866/16548 [51:26<6:44:55,  1.65s/it]11/15/2022 17:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.8382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:47 - INFO - train.train_snli_ve - loss is tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1867/16548 [51:28<6:50:00,  1.68s/it]11/15/2022 17:57:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.4856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:49 - INFO - train.train_snli_ve - loss is tensor(0.9491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1868/16548 [51:29<6:49:30,  1.67s/it]11/15/2022 17:57:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.4929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:50 - INFO - train.train_snli_ve - loss is tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1869/16548 [51:31<6:50:26,  1.68s/it]11/15/2022 17:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.0625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:52 - INFO - train.train_snli_ve - loss is tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1870/16548 [51:33<6:49:54,  1.68s/it]11/15/2022 17:57:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.3891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:54 - INFO - train.train_snli_ve - loss is tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1871/16548 [51:34<6:48:12,  1.67s/it]11/15/2022 17:57:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.7410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:55 - INFO - train.train_snli_ve - loss is tensor(0.8354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1872/16548 [51:36<6:46:34,  1.66s/it]11/15/2022 17:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.9255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:57 - INFO - train.train_snli_ve - loss is tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1873/16548 [51:38<6:46:29,  1.66s/it]11/15/2022 17:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3152e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:57:59 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1874/16548 [51:39<6:50:02,  1.68s/it]11/15/2022 17:58:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.9557e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:00 - INFO - train.train_snli_ve - loss is tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1875/16548 [51:41<6:55:14,  1.70s/it]11/15/2022 17:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.9469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:02 - INFO - train.train_snli_ve - loss is tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1876/16548 [51:43<6:53:17,  1.69s/it]11/15/2022 17:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.8183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:04 - INFO - train.train_snli_ve - loss is tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1877/16548 [51:45<6:50:47,  1.68s/it]11/15/2022 17:58:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.6412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:05 - INFO - train.train_snli_ve - loss is tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1878/16548 [51:46<6:47:59,  1.67s/it]11/15/2022 17:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.9556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:07 - INFO - train.train_snli_ve - loss is tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1879/16548 [51:48<6:45:48,  1.66s/it]11/15/2022 17:58:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.9469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:09 - INFO - train.train_snli_ve - loss is tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1880/16548 [51:49<6:47:25,  1.67s/it]11/15/2022 17:58:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.6817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:10 - INFO - train.train_snli_ve - loss is tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1881/16548 [51:51<6:47:51,  1.67s/it]11/15/2022 17:58:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:12 - INFO - train.train_snli_ve - loss is tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1882/16548 [51:53<6:50:26,  1.68s/it]11/15/2022 17:58:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.1524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:14 - INFO - train.train_snli_ve - loss is tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1883/16548 [51:55<6:48:20,  1.67s/it]11/15/2022 17:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.9114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:16 - INFO - train.train_snli_ve - loss is tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1884/16548 [51:56<6:51:44,  1.68s/it]11/15/2022 17:58:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.9589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:17 - INFO - train.train_snli_ve - loss is tensor(0.9177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1885/16548 [51:58<6:53:24,  1.69s/it]11/15/2022 17:58:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.2749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:19 - INFO - train.train_snli_ve - loss is tensor(0.8673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1886/16548 [52:00<6:48:34,  1.67s/it]11/15/2022 17:58:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:20 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1887/16548 [52:01<6:46:20,  1.66s/it]11/15/2022 17:58:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.6453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:22 - INFO - train.train_snli_ve - loss is tensor(0.9919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1888/16548 [52:03<6:48:15,  1.67s/it]11/15/2022 17:58:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.3892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:24 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1889/16548 [52:05<6:48:47,  1.67s/it]11/15/2022 17:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.6272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:26 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1890/16548 [52:06<6:50:18,  1.68s/it]11/15/2022 17:58:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.5874e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:27 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1891/16548 [52:08<6:50:29,  1.68s/it]11/15/2022 17:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.6572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:29 - INFO - train.train_snli_ve - loss is tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1892/16548 [52:10<6:46:39,  1.66s/it]11/15/2022 17:58:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.5620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:31 - INFO - train.train_snli_ve - loss is tensor(1.1060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1893/16548 [52:11<6:47:14,  1.67s/it]11/15/2022 17:58:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.6688e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:32 - INFO - train.train_snli_ve - loss is tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1894/16548 [52:13<6:52:36,  1.69s/it]11/15/2022 17:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.8391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:34 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1895/16548 [52:15<6:49:54,  1.68s/it]11/15/2022 17:58:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.4761e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:36 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1896/16548 [52:16<6:48:46,  1.67s/it]11/15/2022 17:58:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.2785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:37 - INFO - train.train_snli_ve - loss is tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1897/16548 [52:18<6:50:00,  1.68s/it]11/15/2022 17:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.9970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:39 - INFO - train.train_snli_ve - loss is tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1898/16548 [52:20<6:50:45,  1.68s/it]11/15/2022 17:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.9842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:41 - INFO - train.train_snli_ve - loss is tensor(0.9222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1899/16548 [52:21<6:49:46,  1.68s/it]11/15/2022 17:58:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.2853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:42 - INFO - train.train_snli_ve - loss is tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1900/16548 [52:23<6:52:39,  1.69s/it]11/15/2022 17:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.6677e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:44 - INFO - train.train_snli_ve - loss is tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1901/16548 [52:25<6:52:26,  1.69s/it]11/15/2022 17:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.0223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:46 - INFO - train.train_snli_ve - loss is tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1902/16548 [52:26<6:48:57,  1.68s/it]11/15/2022 17:58:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.1447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:47 - INFO - train.train_snli_ve - loss is tensor(0.8162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11% 1903/16548 [52:28<6:48:00,  1.67s/it]11/15/2022 17:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.1543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:49 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1904/16548 [52:30<6:49:06,  1.68s/it]11/15/2022 17:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.0573e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:51 - INFO - train.train_snli_ve - loss is tensor(0.9157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1905/16548 [52:31<6:48:01,  1.67s/it]11/15/2022 17:58:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.7124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:52 - INFO - train.train_snli_ve - loss is tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1906/16548 [52:33<6:45:49,  1.66s/it]11/15/2022 17:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.5033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:54 - INFO - train.train_snli_ve - loss is tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1907/16548 [52:35<6:43:24,  1.65s/it]11/15/2022 17:58:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4443e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:56 - INFO - train.train_snli_ve - loss is tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1908/16548 [52:36<6:42:37,  1.65s/it]11/15/2022 17:58:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:57 - INFO - train.train_snli_ve - loss is tensor(0.8906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1909/16548 [52:38<6:41:02,  1.64s/it]11/15/2022 17:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.0351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:58:59 - INFO - train.train_snli_ve - loss is tensor(0.5509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1910/16548 [52:40<6:43:16,  1.65s/it]11/15/2022 17:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.3903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:01 - INFO - train.train_snli_ve - loss is tensor(0.8262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1911/16548 [52:41<6:46:56,  1.67s/it]11/15/2022 17:59:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.7807e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:02 - INFO - train.train_snli_ve - loss is tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1912/16548 [52:43<6:48:12,  1.67s/it]11/15/2022 17:59:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.4037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:04 - INFO - train.train_snli_ve - loss is tensor(0.8452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1913/16548 [52:45<6:46:44,  1.67s/it]11/15/2022 17:59:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.9692e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:06 - INFO - train.train_snli_ve - loss is tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1914/16548 [52:46<6:47:51,  1.67s/it]11/15/2022 17:59:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.7161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:07 - INFO - train.train_snli_ve - loss is tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1915/16548 [52:48<6:46:19,  1.67s/it]11/15/2022 17:59:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.5645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:09 - INFO - train.train_snli_ve - loss is tensor(0.9367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1916/16548 [52:50<6:44:28,  1.66s/it]11/15/2022 17:59:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.1561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:11 - INFO - train.train_snli_ve - loss is tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1917/16548 [52:51<6:42:20,  1.65s/it]11/15/2022 17:59:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.7970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:12 - INFO - train.train_snli_ve - loss is tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1918/16548 [52:53<6:43:38,  1.66s/it]11/15/2022 17:59:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.0603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:14 - INFO - train.train_snli_ve - loss is tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1919/16548 [52:55<6:44:05,  1.66s/it]11/15/2022 17:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.9048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:16 - INFO - train.train_snli_ve - loss is tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1920/16548 [52:56<6:44:56,  1.66s/it]11/15/2022 17:59:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.3265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:17 - INFO - train.train_snli_ve - loss is tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1921/16548 [52:58<6:43:28,  1.66s/it]11/15/2022 17:59:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.2200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:19 - INFO - train.train_snli_ve - loss is tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1922/16548 [53:00<6:43:05,  1.65s/it]11/15/2022 17:59:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.1432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:21 - INFO - train.train_snli_ve - loss is tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1923/16548 [53:01<6:43:28,  1.66s/it]11/15/2022 17:59:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.2549e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:22 - INFO - train.train_snli_ve - loss is tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1924/16548 [53:03<6:43:32,  1.66s/it]11/15/2022 17:59:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:24 - INFO - train.train_snli_ve - loss is tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1925/16548 [53:05<6:43:50,  1.66s/it]11/15/2022 17:59:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.2100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:26 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1926/16548 [53:06<6:45:33,  1.66s/it]11/15/2022 17:59:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.2997e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:27 - INFO - train.train_snli_ve - loss is tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1927/16548 [53:08<6:45:04,  1.66s/it]11/15/2022 17:59:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:29 - INFO - train.train_snli_ve - loss is tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1928/16548 [53:10<6:44:18,  1.66s/it]11/15/2022 17:59:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:30 - INFO - train.train_snli_ve - loss is tensor(0.8451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1929/16548 [53:11<6:42:33,  1.65s/it]11/15/2022 17:59:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:32 - INFO - train.train_snli_ve - loss is tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1930/16548 [53:13<6:41:06,  1.65s/it]11/15/2022 17:59:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:34 - INFO - train.train_snli_ve - loss is tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1931/16548 [53:14<6:42:40,  1.65s/it]11/15/2022 17:59:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:35 - INFO - train.train_snli_ve - loss is tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1932/16548 [53:16<6:41:49,  1.65s/it]11/15/2022 17:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.3555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:37 - INFO - train.train_snli_ve - loss is tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1933/16548 [53:18<6:42:40,  1.65s/it]11/15/2022 17:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.9497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:39 - INFO - train.train_snli_ve - loss is tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1934/16548 [53:19<6:42:47,  1.65s/it]11/15/2022 17:59:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:40 - INFO - train.train_snli_ve - loss is tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1935/16548 [53:21<6:42:33,  1.65s/it]11/15/2022 17:59:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.4914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:42 - INFO - train.train_snli_ve - loss is tensor(0.6070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1936/16548 [53:23<6:43:13,  1.66s/it]11/15/2022 17:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.6995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:44 - INFO - train.train_snli_ve - loss is tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1937/16548 [53:24<6:44:17,  1.66s/it]11/15/2022 17:59:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.6385e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:45 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1938/16548 [53:26<6:45:26,  1.67s/it]11/15/2022 17:59:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.9451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:47 - INFO - train.train_snli_ve - loss is tensor(1.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1939/16548 [53:28<6:48:57,  1.68s/it]11/15/2022 17:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:49 - INFO - train.train_snli_ve - loss is tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1940/16548 [53:30<6:49:01,  1.68s/it]11/15/2022 17:59:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0121e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:50 - INFO - train.train_snli_ve - loss is tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1941/16548 [53:31<6:46:47,  1.67s/it]11/15/2022 17:59:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.6449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:52 - INFO - train.train_snli_ve - loss is tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1942/16548 [53:33<6:44:57,  1.66s/it]11/15/2022 17:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:54 - INFO - train.train_snli_ve - loss is tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1943/16548 [53:34<6:44:03,  1.66s/it]11/15/2022 17:59:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.7026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:55 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1944/16548 [53:36<6:44:01,  1.66s/it]11/15/2022 17:59:57 - INFO - train.train_snli_ve - kd_loss is tensor(9.7962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:57 - INFO - train.train_snli_ve - loss is tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1945/16548 [53:38<6:43:29,  1.66s/it]11/15/2022 17:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0503e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 17:59:59 - INFO - train.train_snli_ve - loss is tensor(0.8286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1946/16548 [53:39<6:47:41,  1.68s/it]11/15/2022 18:00:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.3724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:00 - INFO - train.train_snli_ve - loss is tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1947/16548 [53:41<6:49:14,  1.68s/it]11/15/2022 18:00:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.7142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:02 - INFO - train.train_snli_ve - loss is tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1948/16548 [53:43<6:49:11,  1.68s/it]11/15/2022 18:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:04 - INFO - train.train_snli_ve - loss is tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1949/16548 [53:45<6:48:34,  1.68s/it]11/15/2022 18:00:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.3110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:05 - INFO - train.train_snli_ve - loss is tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1950/16548 [53:46<6:49:43,  1.68s/it]11/15/2022 18:00:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.1619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:07 - INFO - train.train_snli_ve - loss is tensor(0.9868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1951/16548 [53:48<6:54:16,  1.70s/it]11/15/2022 18:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5919e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:09 - INFO - train.train_snli_ve - loss is tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1952/16548 [53:50<6:50:26,  1.69s/it]11/15/2022 18:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.2888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:11 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1953/16548 [53:51<6:48:08,  1.68s/it]11/15/2022 18:00:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.0854e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:12 - INFO - train.train_snli_ve - loss is tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1954/16548 [53:53<6:45:47,  1.67s/it]11/15/2022 18:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.5657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:14 - INFO - train.train_snli_ve - loss is tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1955/16548 [53:55<6:43:13,  1.66s/it]11/15/2022 18:00:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.5381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:15 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1956/16548 [53:56<6:44:21,  1.66s/it]11/15/2022 18:00:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.2002e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:17 - INFO - train.train_snli_ve - loss is tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1957/16548 [53:58<6:44:25,  1.66s/it]11/15/2022 18:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.8076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:19 - INFO - train.train_snli_ve - loss is tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1958/16548 [54:00<6:45:54,  1.67s/it]11/15/2022 18:00:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.4796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:20 - INFO - train.train_snli_ve - loss is tensor(0.8360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1959/16548 [54:01<6:43:21,  1.66s/it]11/15/2022 18:00:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1942e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:22 - INFO - train.train_snli_ve - loss is tensor(0.8240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1960/16548 [54:03<6:42:12,  1.65s/it]11/15/2022 18:00:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.2577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:24 - INFO - train.train_snli_ve - loss is tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1961/16548 [54:05<6:43:33,  1.66s/it]11/15/2022 18:00:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.1389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:25 - INFO - train.train_snli_ve - loss is tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1962/16548 [54:06<6:45:08,  1.67s/it]11/15/2022 18:00:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.7958e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:27 - INFO - train.train_snli_ve - loss is tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1963/16548 [54:08<6:42:30,  1.66s/it]11/15/2022 18:00:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.6207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:29 - INFO - train.train_snli_ve - loss is tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1964/16548 [54:10<6:42:30,  1.66s/it]11/15/2022 18:00:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.8112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:30 - INFO - train.train_snli_ve - loss is tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1965/16548 [54:11<6:42:20,  1.66s/it]11/15/2022 18:00:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.7383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:32 - INFO - train.train_snli_ve - loss is tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1966/16548 [54:13<6:41:01,  1.65s/it]11/15/2022 18:00:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.9439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:34 - INFO - train.train_snli_ve - loss is tensor(0.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1967/16548 [54:15<6:45:18,  1.67s/it]11/15/2022 18:00:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.0775e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:35 - INFO - train.train_snli_ve - loss is tensor(0.8207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1968/16548 [54:16<6:44:24,  1.66s/it]11/15/2022 18:00:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.3454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:37 - INFO - train.train_snli_ve - loss is tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1969/16548 [54:18<6:44:19,  1.66s/it]11/15/2022 18:00:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.2818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:39 - INFO - train.train_snli_ve - loss is tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1970/16548 [54:20<6:45:42,  1.67s/it]11/15/2022 18:00:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.4278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:40 - INFO - train.train_snli_ve - loss is tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1971/16548 [54:21<6:43:22,  1.66s/it]11/15/2022 18:00:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:42 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1972/16548 [54:23<6:43:40,  1.66s/it]11/15/2022 18:00:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.9959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:44 - INFO - train.train_snli_ve - loss is tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1973/16548 [54:24<6:45:20,  1.67s/it]11/15/2022 18:00:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:45 - INFO - train.train_snli_ve - loss is tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1974/16548 [54:26<6:44:24,  1.66s/it]11/15/2022 18:00:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.3187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:47 - INFO - train.train_snli_ve - loss is tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1975/16548 [54:28<6:45:26,  1.67s/it]11/15/2022 18:00:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.7327e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:49 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1976/16548 [54:30<6:45:31,  1.67s/it]11/15/2022 18:00:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.6776e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:50 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1977/16548 [54:31<6:43:50,  1.66s/it]11/15/2022 18:00:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.5031e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:52 - INFO - train.train_snli_ve - loss is tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1978/16548 [54:33<6:44:27,  1.67s/it]11/15/2022 18:00:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.6241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:54 - INFO - train.train_snli_ve - loss is tensor(0.9265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1979/16548 [54:35<6:45:32,  1.67s/it]11/15/2022 18:00:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.4891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:55 - INFO - train.train_snli_ve - loss is tensor(0.9600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1980/16548 [54:36<6:44:54,  1.67s/it]11/15/2022 18:00:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:57 - INFO - train.train_snli_ve - loss is tensor(0.8605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1981/16548 [54:38<6:45:29,  1.67s/it]11/15/2022 18:00:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.8253e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:00:59 - INFO - train.train_snli_ve - loss is tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1982/16548 [54:39<6:43:30,  1.66s/it]11/15/2022 18:01:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.6889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:00 - INFO - train.train_snli_ve - loss is tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1983/16548 [54:41<6:44:48,  1.67s/it]11/15/2022 18:01:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.6370e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:02 - INFO - train.train_snli_ve - loss is tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1984/16548 [54:43<6:46:23,  1.67s/it]11/15/2022 18:01:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.7986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:04 - INFO - train.train_snli_ve - loss is tensor(0.8457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1985/16548 [54:45<6:48:39,  1.68s/it]11/15/2022 18:01:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.2913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:05 - INFO - train.train_snli_ve - loss is tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1986/16548 [54:46<6:44:33,  1.67s/it]11/15/2022 18:01:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.7719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:07 - INFO - train.train_snli_ve - loss is tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1987/16548 [54:48<6:45:10,  1.67s/it]11/15/2022 18:01:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.6585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:09 - INFO - train.train_snli_ve - loss is tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1988/16548 [54:50<6:44:14,  1.67s/it]11/15/2022 18:01:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.8762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:10 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1989/16548 [54:51<6:43:57,  1.66s/it]11/15/2022 18:01:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.7075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:12 - INFO - train.train_snli_ve - loss is tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1990/16548 [54:53<6:43:38,  1.66s/it]11/15/2022 18:01:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.8553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:14 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1991/16548 [54:55<6:44:14,  1.67s/it]11/15/2022 18:01:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.1676e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:15 - INFO - train.train_snli_ve - loss is tensor(0.8692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1992/16548 [54:56<6:45:20,  1.67s/it]11/15/2022 18:01:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.2290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:17 - INFO - train.train_snli_ve - loss is tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1993/16548 [54:58<6:46:12,  1.67s/it]11/15/2022 18:01:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.2783e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:19 - INFO - train.train_snli_ve - loss is tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1994/16548 [55:00<6:45:38,  1.67s/it]11/15/2022 18:01:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.1875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:20 - INFO - train.train_snli_ve - loss is tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1995/16548 [55:01<6:44:41,  1.67s/it]11/15/2022 18:01:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2599e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:22 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1996/16548 [55:03<6:46:04,  1.67s/it]11/15/2022 18:01:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.7817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:24 - INFO - train.train_snli_ve - loss is tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1997/16548 [55:05<6:46:23,  1.68s/it]11/15/2022 18:01:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:25 - INFO - train.train_snli_ve - loss is tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1998/16548 [55:06<6:42:44,  1.66s/it]11/15/2022 18:01:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.2571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:27 - INFO - train.train_snli_ve - loss is tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 1999/16548 [55:08<6:40:33,  1.65s/it]11/15/2022 18:01:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.1498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:29 - INFO - train.train_snli_ve - loss is tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2000/16548 [55:10<6:46:33,  1.68s/it]11/15/2022 18:01:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.7816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:31 - INFO - train.train_snli_ve - loss is tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2001/16548 [55:11<6:49:19,  1.69s/it]11/15/2022 18:01:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:32 - INFO - train.train_snli_ve - loss is tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2002/16548 [55:13<6:50:41,  1.69s/it]11/15/2022 18:01:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.0418e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:34 - INFO - train.train_snli_ve - loss is tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2003/16548 [55:15<6:47:57,  1.68s/it]11/15/2022 18:01:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.9473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:36 - INFO - train.train_snli_ve - loss is tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2004/16548 [55:16<6:47:54,  1.68s/it]11/15/2022 18:01:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.8065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:37 - INFO - train.train_snli_ve - loss is tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2005/16548 [55:18<6:46:22,  1.68s/it]11/15/2022 18:01:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.0936e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:39 - INFO - train.train_snli_ve - loss is tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2006/16548 [55:20<6:45:32,  1.67s/it]11/15/2022 18:01:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.6461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:41 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2007/16548 [55:21<6:44:06,  1.67s/it]11/15/2022 18:01:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.5912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:42 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2008/16548 [55:23<6:45:18,  1.67s/it]11/15/2022 18:01:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.3470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:44 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2009/16548 [55:25<6:45:29,  1.67s/it]11/15/2022 18:01:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.0192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:46 - INFO - train.train_snli_ve - loss is tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2010/16548 [55:26<6:43:04,  1.66s/it]11/15/2022 18:01:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.9842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:47 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2011/16548 [55:28<6:45:07,  1.67s/it]11/15/2022 18:01:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.6653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:49 - INFO - train.train_snli_ve - loss is tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2012/16548 [55:30<6:46:18,  1.68s/it]11/15/2022 18:01:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.5747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:51 - INFO - train.train_snli_ve - loss is tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2013/16548 [55:31<6:42:40,  1.66s/it]11/15/2022 18:01:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:52 - INFO - train.train_snli_ve - loss is tensor(1.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2014/16548 [55:33<6:43:54,  1.67s/it]11/15/2022 18:01:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:54 - INFO - train.train_snli_ve - loss is tensor(0.9332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2015/16548 [55:35<6:45:57,  1.68s/it]11/15/2022 18:01:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:56 - INFO - train.train_snli_ve - loss is tensor(0.4304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2016/16548 [55:36<6:47:51,  1.68s/it]11/15/2022 18:01:57 - INFO - train.train_snli_ve - kd_loss is tensor(9.9733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:57 - INFO - train.train_snli_ve - loss is tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2017/16548 [55:38<6:49:42,  1.69s/it]11/15/2022 18:01:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.8320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:01:59 - INFO - train.train_snli_ve - loss is tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2018/16548 [55:40<6:49:47,  1.69s/it]11/15/2022 18:02:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.7615e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:01 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2019/16548 [55:41<6:45:48,  1.68s/it]11/15/2022 18:02:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:02 - INFO - train.train_snli_ve - loss is tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2020/16548 [55:43<6:45:24,  1.67s/it]11/15/2022 18:02:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:04 - INFO - train.train_snli_ve - loss is tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2021/16548 [55:45<6:44:15,  1.67s/it]11/15/2022 18:02:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:06 - INFO - train.train_snli_ve - loss is tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2022/16548 [55:46<6:42:18,  1.66s/it]11/15/2022 18:02:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:07 - INFO - train.train_snli_ve - loss is tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2023/16548 [55:48<6:44:33,  1.67s/it]11/15/2022 18:02:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:09 - INFO - train.train_snli_ve - loss is tensor(0.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2024/16548 [55:50<6:45:09,  1.67s/it]11/15/2022 18:02:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.6934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:11 - INFO - train.train_snli_ve - loss is tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2025/16548 [55:51<6:44:28,  1.67s/it]11/15/2022 18:02:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:12 - INFO - train.train_snli_ve - loss is tensor(0.8757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2026/16548 [55:53<6:42:39,  1.66s/it]11/15/2022 18:02:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:14 - INFO - train.train_snli_ve - loss is tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2027/16548 [55:55<6:43:21,  1.67s/it]11/15/2022 18:02:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:16 - INFO - train.train_snli_ve - loss is tensor(0.8464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2028/16548 [55:56<6:40:46,  1.66s/it]11/15/2022 18:02:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8571e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:17 - INFO - train.train_snli_ve - loss is tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2029/16548 [55:58<6:39:13,  1.65s/it]11/15/2022 18:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.4893e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:19 - INFO - train.train_snli_ve - loss is tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2030/16548 [56:00<6:39:19,  1.65s/it]11/15/2022 18:02:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0739e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:21 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2031/16548 [56:01<6:37:55,  1.64s/it]11/15/2022 18:02:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.1745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:22 - INFO - train.train_snli_ve - loss is tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2032/16548 [56:03<6:37:27,  1.64s/it]11/15/2022 18:02:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:24 - INFO - train.train_snli_ve - loss is tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2033/16548 [56:05<6:37:45,  1.64s/it]11/15/2022 18:02:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6407e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:26 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2034/16548 [56:06<6:38:23,  1.65s/it]11/15/2022 18:02:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:27 - INFO - train.train_snli_ve - loss is tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2035/16548 [56:08<6:38:54,  1.65s/it]11/15/2022 18:02:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:29 - INFO - train.train_snli_ve - loss is tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2036/16548 [56:10<6:42:25,  1.66s/it]11/15/2022 18:02:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.3366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:31 - INFO - train.train_snli_ve - loss is tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2037/16548 [56:11<6:40:40,  1.66s/it]11/15/2022 18:02:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.9833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:32 - INFO - train.train_snli_ve - loss is tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2038/16548 [56:13<6:39:10,  1.65s/it]11/15/2022 18:02:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.7903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:34 - INFO - train.train_snli_ve - loss is tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2039/16548 [56:15<6:37:40,  1.64s/it]11/15/2022 18:02:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:35 - INFO - train.train_snli_ve - loss is tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2040/16548 [56:16<6:38:17,  1.65s/it]11/15/2022 18:02:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:37 - INFO - train.train_snli_ve - loss is tensor(0.9268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2041/16548 [56:18<6:39:35,  1.65s/it]11/15/2022 18:02:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:39 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2042/16548 [56:20<6:44:19,  1.67s/it]11/15/2022 18:02:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.3085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:40 - INFO - train.train_snli_ve - loss is tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2043/16548 [56:21<6:43:54,  1.67s/it]11/15/2022 18:02:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.3086e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:42 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2044/16548 [56:23<6:45:25,  1.68s/it]11/15/2022 18:02:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1668e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:44 - INFO - train.train_snli_ve - loss is tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2045/16548 [56:25<6:40:18,  1.66s/it]11/15/2022 18:02:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.8221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:45 - INFO - train.train_snli_ve - loss is tensor(0.9298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2046/16548 [56:26<6:40:23,  1.66s/it]11/15/2022 18:02:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.8013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:47 - INFO - train.train_snli_ve - loss is tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2047/16548 [56:28<6:38:59,  1.65s/it]11/15/2022 18:02:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.7639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:49 - INFO - train.train_snli_ve - loss is tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2048/16548 [56:29<6:39:46,  1.65s/it]11/15/2022 18:02:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.4990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:50 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2049/16548 [56:31<6:38:27,  1.65s/it]11/15/2022 18:02:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.8987e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:52 - INFO - train.train_snli_ve - loss is tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2050/16548 [56:33<6:43:22,  1.67s/it]11/15/2022 18:02:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.9985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:54 - INFO - train.train_snli_ve - loss is tensor(0.4757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2051/16548 [56:34<6:41:02,  1.66s/it]11/15/2022 18:02:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.8383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:55 - INFO - train.train_snli_ve - loss is tensor(0.8332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2052/16548 [56:36<6:45:15,  1.68s/it]11/15/2022 18:02:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.1891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:57 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2053/16548 [56:38<6:46:03,  1.68s/it]11/15/2022 18:02:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.1376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:02:59 - INFO - train.train_snli_ve - loss is tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2054/16548 [56:40<6:42:57,  1.67s/it]11/15/2022 18:03:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.7505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:01 - INFO - train.train_snli_ve - loss is tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2055/16548 [56:41<6:48:02,  1.69s/it]11/15/2022 18:03:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.4889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:02 - INFO - train.train_snli_ve - loss is tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2056/16548 [56:43<6:50:46,  1.70s/it]11/15/2022 18:03:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:04 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2057/16548 [56:45<6:48:36,  1.69s/it]11/15/2022 18:03:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2276e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:06 - INFO - train.train_snli_ve - loss is tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2058/16548 [56:46<6:47:43,  1.69s/it]11/15/2022 18:03:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.8739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:07 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2059/16548 [56:48<6:51:33,  1.70s/it]11/15/2022 18:03:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.2700e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:09 - INFO - train.train_snli_ve - loss is tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2060/16548 [56:50<6:49:02,  1.69s/it]11/15/2022 18:03:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.6147e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:11 - INFO - train.train_snli_ve - loss is tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2061/16548 [56:51<6:46:22,  1.68s/it]11/15/2022 18:03:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.9353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:12 - INFO - train.train_snli_ve - loss is tensor(0.8161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2062/16548 [56:53<6:44:55,  1.68s/it]11/15/2022 18:03:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.3293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:14 - INFO - train.train_snli_ve - loss is tensor(0.8670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2063/16548 [56:55<6:44:27,  1.68s/it]11/15/2022 18:03:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.3638e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:16 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2064/16548 [56:56<6:43:30,  1.67s/it]11/15/2022 18:03:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.8020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:17 - INFO - train.train_snli_ve - loss is tensor(0.8760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2065/16548 [56:58<6:42:12,  1.67s/it]11/15/2022 18:03:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.5785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:19 - INFO - train.train_snli_ve - loss is tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2066/16548 [57:00<6:40:26,  1.66s/it]11/15/2022 18:03:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.2693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:21 - INFO - train.train_snli_ve - loss is tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2067/16548 [57:01<6:41:07,  1.66s/it]11/15/2022 18:03:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.8046e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:22 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12% 2068/16548 [57:03<6:40:40,  1.66s/it]11/15/2022 18:03:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.0318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:24 - INFO - train.train_snli_ve - loss is tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2069/16548 [57:05<6:44:12,  1.67s/it]11/15/2022 18:03:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.6036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:26 - INFO - train.train_snli_ve - loss is tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2070/16548 [57:06<6:46:51,  1.69s/it]11/15/2022 18:03:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.3608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:27 - INFO - train.train_snli_ve - loss is tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2071/16548 [57:08<6:45:44,  1.68s/it]11/15/2022 18:03:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.9102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:29 - INFO - train.train_snli_ve - loss is tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2072/16548 [57:10<6:47:23,  1.69s/it]11/15/2022 18:03:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.7943e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:31 - INFO - train.train_snli_ve - loss is tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2073/16548 [57:11<6:44:25,  1.68s/it]11/15/2022 18:03:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.6484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:32 - INFO - train.train_snli_ve - loss is tensor(1.2398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2074/16548 [57:13<6:51:41,  1.71s/it]11/15/2022 18:03:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.0183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:34 - INFO - train.train_snli_ve - loss is tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2075/16548 [57:15<6:48:07,  1.69s/it]11/15/2022 18:03:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.3886e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:36 - INFO - train.train_snli_ve - loss is tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2076/16548 [57:17<6:51:01,  1.70s/it]11/15/2022 18:03:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.5301e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:38 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2077/16548 [57:18<6:48:35,  1.69s/it]11/15/2022 18:03:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.2466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:39 - INFO - train.train_snli_ve - loss is tensor(0.9184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2078/16548 [57:20<6:46:07,  1.68s/it]11/15/2022 18:03:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.0806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:41 - INFO - train.train_snli_ve - loss is tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2079/16548 [57:22<6:46:24,  1.69s/it]11/15/2022 18:03:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.1379e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:43 - INFO - train.train_snli_ve - loss is tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2080/16548 [57:23<6:48:22,  1.69s/it]11/15/2022 18:03:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.2160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:44 - INFO - train.train_snli_ve - loss is tensor(0.9220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2081/16548 [57:25<6:46:10,  1.68s/it]11/15/2022 18:03:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.9081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:46 - INFO - train.train_snli_ve - loss is tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2082/16548 [57:27<6:49:43,  1.70s/it]11/15/2022 18:03:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.2307e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:48 - INFO - train.train_snli_ve - loss is tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2083/16548 [57:28<6:45:28,  1.68s/it]11/15/2022 18:03:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.3805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:49 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2084/16548 [57:30<6:44:53,  1.68s/it]11/15/2022 18:03:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.4214e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:51 - INFO - train.train_snli_ve - loss is tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2085/16548 [57:32<6:43:22,  1.67s/it]11/15/2022 18:03:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:53 - INFO - train.train_snli_ve - loss is tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2086/16548 [57:33<6:46:04,  1.68s/it]11/15/2022 18:03:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.8362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:54 - INFO - train.train_snli_ve - loss is tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2087/16548 [57:35<6:45:07,  1.68s/it]11/15/2022 18:03:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.0467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:56 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2088/16548 [57:37<6:42:21,  1.67s/it]11/15/2022 18:03:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.2825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:58 - INFO - train.train_snli_ve - loss is tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2089/16548 [57:38<6:43:54,  1.68s/it]11/15/2022 18:03:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.8291e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:03:59 - INFO - train.train_snli_ve - loss is tensor(0.8332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2090/16548 [57:40<6:40:26,  1.66s/it]11/15/2022 18:04:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.3349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:01 - INFO - train.train_snli_ve - loss is tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2091/16548 [57:42<6:39:09,  1.66s/it]11/15/2022 18:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.6916e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:03 - INFO - train.train_snli_ve - loss is tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2092/16548 [57:43<6:39:18,  1.66s/it]11/15/2022 18:04:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.2355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:04 - INFO - train.train_snli_ve - loss is tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2093/16548 [57:45<6:42:24,  1.67s/it]11/15/2022 18:04:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:06 - INFO - train.train_snli_ve - loss is tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2094/16548 [57:47<6:47:05,  1.69s/it]11/15/2022 18:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.5011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:08 - INFO - train.train_snli_ve - loss is tensor(1.0248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2095/16548 [57:48<6:43:29,  1.68s/it]11/15/2022 18:04:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.1861e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:09 - INFO - train.train_snli_ve - loss is tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2096/16548 [57:50<6:42:00,  1.67s/it]11/15/2022 18:04:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1543e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:11 - INFO - train.train_snli_ve - loss is tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2097/16548 [57:52<6:41:08,  1.67s/it]11/15/2022 18:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.6390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:13 - INFO - train.train_snli_ve - loss is tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2098/16548 [57:53<6:41:10,  1.67s/it]11/15/2022 18:04:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.4226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:14 - INFO - train.train_snli_ve - loss is tensor(0.9029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2099/16548 [57:55<6:41:53,  1.67s/it]11/15/2022 18:04:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.1653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:16 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2100/16548 [57:57<6:47:26,  1.69s/it]11/15/2022 18:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.6645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:18 - INFO - train.train_snli_ve - loss is tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2101/16548 [57:58<6:42:39,  1.67s/it]11/15/2022 18:04:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.0387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:19 - INFO - train.train_snli_ve - loss is tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2102/16548 [58:00<6:42:30,  1.67s/it]11/15/2022 18:04:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:21 - INFO - train.train_snli_ve - loss is tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2103/16548 [58:02<6:41:57,  1.67s/it]11/15/2022 18:04:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.2973e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:23 - INFO - train.train_snli_ve - loss is tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2104/16548 [58:03<6:41:42,  1.67s/it]11/15/2022 18:04:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:24 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2105/16548 [58:05<6:40:00,  1.66s/it]11/15/2022 18:04:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.6720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:26 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2106/16548 [58:07<6:37:20,  1.65s/it]11/15/2022 18:04:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.6324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:28 - INFO - train.train_snli_ve - loss is tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2107/16548 [58:08<6:39:41,  1.66s/it]11/15/2022 18:04:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.5950e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:29 - INFO - train.train_snli_ve - loss is tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2108/16548 [58:10<6:42:22,  1.67s/it]11/15/2022 18:04:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:31 - INFO - train.train_snli_ve - loss is tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2109/16548 [58:12<6:40:11,  1.66s/it]11/15/2022 18:04:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:33 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2110/16548 [58:13<6:40:48,  1.67s/it]11/15/2022 18:04:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.4227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:34 - INFO - train.train_snli_ve - loss is tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2111/16548 [58:15<6:43:18,  1.68s/it]11/15/2022 18:04:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.4535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:36 - INFO - train.train_snli_ve - loss is tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2112/16548 [58:17<6:47:31,  1.69s/it]11/15/2022 18:04:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.7206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:38 - INFO - train.train_snli_ve - loss is tensor(1.0141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2113/16548 [58:19<6:45:47,  1.69s/it]11/15/2022 18:04:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:39 - INFO - train.train_snli_ve - loss is tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2114/16548 [58:20<6:41:56,  1.67s/it]11/15/2022 18:04:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.3963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:41 - INFO - train.train_snli_ve - loss is tensor(0.8510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2115/16548 [58:22<6:43:11,  1.68s/it]11/15/2022 18:04:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.5019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:43 - INFO - train.train_snli_ve - loss is tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2116/16548 [58:24<6:43:26,  1.68s/it]11/15/2022 18:04:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.5099e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:44 - INFO - train.train_snli_ve - loss is tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2117/16548 [58:25<6:41:49,  1.67s/it]11/15/2022 18:04:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.7637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:46 - INFO - train.train_snli_ve - loss is tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2118/16548 [58:27<6:43:06,  1.68s/it]11/15/2022 18:04:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.8993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:48 - INFO - train.train_snli_ve - loss is tensor(0.4763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2119/16548 [58:29<6:44:16,  1.68s/it]11/15/2022 18:04:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.6080e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:50 - INFO - train.train_snli_ve - loss is tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2120/16548 [58:30<6:44:11,  1.68s/it]11/15/2022 18:04:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.7227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:51 - INFO - train.train_snli_ve - loss is tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2121/16548 [58:32<6:42:54,  1.68s/it]11/15/2022 18:04:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.8663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:53 - INFO - train.train_snli_ve - loss is tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2122/16548 [58:34<6:43:26,  1.68s/it]11/15/2022 18:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.9426e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:55 - INFO - train.train_snli_ve - loss is tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2123/16548 [58:35<6:42:50,  1.68s/it]11/15/2022 18:04:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:56 - INFO - train.train_snli_ve - loss is tensor(0.7731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2124/16548 [58:37<6:40:45,  1.67s/it]11/15/2022 18:04:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.6842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:04:58 - INFO - train.train_snli_ve - loss is tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2125/16548 [58:39<6:39:48,  1.66s/it]11/15/2022 18:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.6969e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:00 - INFO - train.train_snli_ve - loss is tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2126/16548 [58:40<6:38:29,  1.66s/it]11/15/2022 18:05:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.7026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:01 - INFO - train.train_snli_ve - loss is tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2127/16548 [58:42<6:40:21,  1.67s/it]11/15/2022 18:05:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.2341e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:03 - INFO - train.train_snli_ve - loss is tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2128/16548 [58:44<6:43:55,  1.68s/it]11/15/2022 18:05:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:05 - INFO - train.train_snli_ve - loss is tensor(0.8117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2129/16548 [58:45<6:41:47,  1.67s/it]11/15/2022 18:05:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.7835e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:06 - INFO - train.train_snli_ve - loss is tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2130/16548 [58:47<6:44:26,  1.68s/it]11/15/2022 18:05:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.4424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:08 - INFO - train.train_snli_ve - loss is tensor(0.8742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2131/16548 [58:49<6:45:41,  1.69s/it]11/15/2022 18:05:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.0557e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:10 - INFO - train.train_snli_ve - loss is tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2132/16548 [58:50<6:43:55,  1.68s/it]11/15/2022 18:05:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.7853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:11 - INFO - train.train_snli_ve - loss is tensor(0.7645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2133/16548 [58:52<6:41:42,  1.67s/it]11/15/2022 18:05:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.0728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:13 - INFO - train.train_snli_ve - loss is tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2134/16548 [58:54<6:41:50,  1.67s/it]11/15/2022 18:05:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.1019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:15 - INFO - train.train_snli_ve - loss is tensor(0.9168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2135/16548 [58:55<6:43:52,  1.68s/it]11/15/2022 18:05:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.6686e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:16 - INFO - train.train_snli_ve - loss is tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2136/16548 [58:57<6:40:02,  1.67s/it]11/15/2022 18:05:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.2556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:18 - INFO - train.train_snli_ve - loss is tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2137/16548 [58:59<6:40:17,  1.67s/it]11/15/2022 18:05:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.2945e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:20 - INFO - train.train_snli_ve - loss is tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2138/16548 [59:00<6:36:31,  1.65s/it]11/15/2022 18:05:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.6727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:21 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2139/16548 [59:02<6:39:41,  1.66s/it]11/15/2022 18:05:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.3007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:23 - INFO - train.train_snli_ve - loss is tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2140/16548 [59:04<6:40:16,  1.67s/it]11/15/2022 18:05:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:25 - INFO - train.train_snli_ve - loss is tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2141/16548 [59:05<6:39:36,  1.66s/it]11/15/2022 18:05:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.0218e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:26 - INFO - train.train_snli_ve - loss is tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2142/16548 [59:07<6:42:12,  1.68s/it]11/15/2022 18:05:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.1654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:28 - INFO - train.train_snli_ve - loss is tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2143/16548 [59:09<6:42:47,  1.68s/it]11/15/2022 18:05:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:30 - INFO - train.train_snli_ve - loss is tensor(1.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2144/16548 [59:10<6:45:40,  1.69s/it]11/15/2022 18:05:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.8186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:31 - INFO - train.train_snli_ve - loss is tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2145/16548 [59:12<6:42:32,  1.68s/it]11/15/2022 18:05:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.5923e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:33 - INFO - train.train_snli_ve - loss is tensor(0.8110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2146/16548 [59:14<6:44:33,  1.69s/it]11/15/2022 18:05:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.6434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:35 - INFO - train.train_snli_ve - loss is tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2147/16548 [59:15<6:41:21,  1.67s/it]11/15/2022 18:05:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.0107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:36 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2148/16548 [59:17<6:40:46,  1.67s/it]11/15/2022 18:05:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.9494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:38 - INFO - train.train_snli_ve - loss is tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2149/16548 [59:19<6:43:31,  1.68s/it]11/15/2022 18:05:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.0713e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:40 - INFO - train.train_snli_ve - loss is tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2150/16548 [59:20<6:40:11,  1.67s/it]11/15/2022 18:05:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:41 - INFO - train.train_snli_ve - loss is tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2151/16548 [59:22<6:39:53,  1.67s/it]11/15/2022 18:05:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.7010e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:43 - INFO - train.train_snli_ve - loss is tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2152/16548 [59:24<6:41:46,  1.67s/it]11/15/2022 18:05:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.4824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:45 - INFO - train.train_snli_ve - loss is tensor(1.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2153/16548 [59:25<6:43:15,  1.68s/it]11/15/2022 18:05:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.0419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:46 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2154/16548 [59:27<6:42:24,  1.68s/it]11/15/2022 18:05:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.2942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:48 - INFO - train.train_snli_ve - loss is tensor(0.9706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2155/16548 [59:29<6:41:12,  1.67s/it]11/15/2022 18:05:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.0435e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:50 - INFO - train.train_snli_ve - loss is tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2156/16548 [59:30<6:39:16,  1.66s/it]11/15/2022 18:05:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.8456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:51 - INFO - train.train_snli_ve - loss is tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2157/16548 [59:32<6:38:29,  1.66s/it]11/15/2022 18:05:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.3190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:53 - INFO - train.train_snli_ve - loss is tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2158/16548 [59:34<6:42:21,  1.68s/it]11/15/2022 18:05:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.4098e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:55 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2159/16548 [59:36<6:41:02,  1.67s/it]11/15/2022 18:05:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.0555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:56 - INFO - train.train_snli_ve - loss is tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2160/16548 [59:37<6:41:25,  1.67s/it]11/15/2022 18:05:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.4013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:05:58 - INFO - train.train_snli_ve - loss is tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2161/16548 [59:39<6:44:50,  1.69s/it]11/15/2022 18:06:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0129e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:00 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2162/16548 [59:41<6:42:38,  1.68s/it]11/15/2022 18:06:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:01 - INFO - train.train_snli_ve - loss is tensor(0.8065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2163/16548 [59:42<6:42:00,  1.68s/it]11/15/2022 18:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.8473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:03 - INFO - train.train_snli_ve - loss is tensor(0.5712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2164/16548 [59:44<6:45:25,  1.69s/it]11/15/2022 18:06:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.1413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:05 - INFO - train.train_snli_ve - loss is tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2165/16548 [59:46<6:48:51,  1.71s/it]11/15/2022 18:06:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.6794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:07 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2166/16548 [59:47<6:48:26,  1.70s/it]11/15/2022 18:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.4629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:08 - INFO - train.train_snli_ve - loss is tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2167/16548 [59:49<6:49:29,  1.71s/it]11/15/2022 18:06:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.8537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:10 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2168/16548 [59:51<6:46:29,  1.70s/it]11/15/2022 18:06:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.0110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:12 - INFO - train.train_snli_ve - loss is tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2169/16548 [59:52<6:42:50,  1.68s/it]11/15/2022 18:06:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.5203e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:13 - INFO - train.train_snli_ve - loss is tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2170/16548 [59:54<6:43:18,  1.68s/it]11/15/2022 18:06:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.7724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:15 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2171/16548 [59:56<6:42:28,  1.68s/it]11/15/2022 18:06:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.9291e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:17 - INFO - train.train_snli_ve - loss is tensor(0.9954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2172/16548 [59:57<6:40:10,  1.67s/it]11/15/2022 18:06:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.8435e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:18 - INFO - train.train_snli_ve - loss is tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2173/16548 [59:59<6:42:13,  1.68s/it]11/15/2022 18:06:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.9004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:20 - INFO - train.train_snli_ve - loss is tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2174/16548 [1:00:01<6:43:54,  1.69s/it]11/15/2022 18:06:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.6653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:22 - INFO - train.train_snli_ve - loss is tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2175/16548 [1:00:02<6:39:37,  1.67s/it]11/15/2022 18:06:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.5501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:23 - INFO - train.train_snli_ve - loss is tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2176/16548 [1:00:04<6:37:39,  1.66s/it]11/15/2022 18:06:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.2290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:25 - INFO - train.train_snli_ve - loss is tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2177/16548 [1:00:06<6:37:10,  1.66s/it]11/15/2022 18:06:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.0096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:27 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2178/16548 [1:00:07<6:38:43,  1.66s/it]11/15/2022 18:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.6690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:28 - INFO - train.train_snli_ve - loss is tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2179/16548 [1:00:09<6:37:39,  1.66s/it]11/15/2022 18:06:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.1373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:30 - INFO - train.train_snli_ve - loss is tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2180/16548 [1:00:11<6:38:22,  1.66s/it]11/15/2022 18:06:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.5930e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:32 - INFO - train.train_snli_ve - loss is tensor(0.8626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2181/16548 [1:00:12<6:38:27,  1.66s/it]11/15/2022 18:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.2639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:33 - INFO - train.train_snli_ve - loss is tensor(0.8749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2182/16548 [1:00:14<6:38:12,  1.66s/it]11/15/2022 18:06:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.1523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:35 - INFO - train.train_snli_ve - loss is tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2183/16548 [1:00:16<6:36:52,  1.66s/it]11/15/2022 18:06:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.3421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:37 - INFO - train.train_snli_ve - loss is tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2184/16548 [1:00:17<6:36:19,  1.66s/it]11/15/2022 18:06:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:38 - INFO - train.train_snli_ve - loss is tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2185/16548 [1:00:19<6:37:15,  1.66s/it]11/15/2022 18:06:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:40 - INFO - train.train_snli_ve - loss is tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2186/16548 [1:00:21<6:35:55,  1.65s/it]11/15/2022 18:06:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.5529e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:42 - INFO - train.train_snli_ve - loss is tensor(0.5167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2187/16548 [1:00:22<6:34:38,  1.65s/it]11/15/2022 18:06:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5778e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:43 - INFO - train.train_snli_ve - loss is tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2188/16548 [1:00:24<6:37:11,  1.66s/it]11/15/2022 18:06:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:45 - INFO - train.train_snli_ve - loss is tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2189/16548 [1:00:26<6:38:18,  1.66s/it]11/15/2022 18:06:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.1695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:47 - INFO - train.train_snli_ve - loss is tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2190/16548 [1:00:27<6:39:52,  1.67s/it]11/15/2022 18:06:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.8236e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:48 - INFO - train.train_snli_ve - loss is tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2191/16548 [1:00:29<6:41:56,  1.68s/it]11/15/2022 18:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.6453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:50 - INFO - train.train_snli_ve - loss is tensor(1.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2192/16548 [1:00:31<6:42:37,  1.68s/it]11/15/2022 18:06:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.3776e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:52 - INFO - train.train_snli_ve - loss is tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2193/16548 [1:00:32<6:41:08,  1.68s/it]11/15/2022 18:06:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:53 - INFO - train.train_snli_ve - loss is tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2194/16548 [1:00:34<6:39:11,  1.67s/it]11/15/2022 18:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.1746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:55 - INFO - train.train_snli_ve - loss is tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2195/16548 [1:00:36<6:36:51,  1.66s/it]11/15/2022 18:06:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:57 - INFO - train.train_snli_ve - loss is tensor(0.5217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2196/16548 [1:00:37<6:36:34,  1.66s/it]11/15/2022 18:06:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:06:58 - INFO - train.train_snli_ve - loss is tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2197/16548 [1:00:39<6:39:29,  1.67s/it]11/15/2022 18:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:00 - INFO - train.train_snli_ve - loss is tensor(0.8638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2198/16548 [1:00:41<6:39:16,  1.67s/it]11/15/2022 18:07:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.6675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:02 - INFO - train.train_snli_ve - loss is tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2199/16548 [1:00:42<6:38:53,  1.67s/it]11/15/2022 18:07:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4026e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:03 - INFO - train.train_snli_ve - loss is tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2200/16548 [1:00:44<6:44:55,  1.69s/it]11/15/2022 18:07:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.3544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:05 - INFO - train.train_snli_ve - loss is tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2201/16548 [1:00:46<6:48:25,  1.71s/it]11/15/2022 18:07:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.0960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:07 - INFO - train.train_snli_ve - loss is tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2202/16548 [1:00:48<6:45:13,  1.69s/it]11/15/2022 18:07:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.8438e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:08 - INFO - train.train_snli_ve - loss is tensor(1.0733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2203/16548 [1:00:49<6:43:44,  1.69s/it]11/15/2022 18:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:10 - INFO - train.train_snli_ve - loss is tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2204/16548 [1:00:51<6:43:38,  1.69s/it]11/15/2022 18:07:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.5211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:12 - INFO - train.train_snli_ve - loss is tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2205/16548 [1:00:53<6:41:52,  1.68s/it]11/15/2022 18:07:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:14 - INFO - train.train_snli_ve - loss is tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2206/16548 [1:00:54<6:43:24,  1.69s/it]11/15/2022 18:07:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.3257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:15 - INFO - train.train_snli_ve - loss is tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2207/16548 [1:00:56<6:42:53,  1.69s/it]11/15/2022 18:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.2811e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:17 - INFO - train.train_snli_ve - loss is tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2208/16548 [1:00:58<6:41:51,  1.68s/it]11/15/2022 18:07:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.3302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:19 - INFO - train.train_snli_ve - loss is tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2209/16548 [1:00:59<6:43:28,  1.69s/it]11/15/2022 18:07:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.7275e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:20 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2210/16548 [1:01:01<6:43:01,  1.69s/it]11/15/2022 18:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.0920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:22 - INFO - train.train_snli_ve - loss is tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2211/16548 [1:01:03<6:39:01,  1.67s/it]11/15/2022 18:07:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3048e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:24 - INFO - train.train_snli_ve - loss is tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2212/16548 [1:01:04<6:39:46,  1.67s/it]11/15/2022 18:07:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.1013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:25 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2213/16548 [1:01:06<6:39:47,  1.67s/it]11/15/2022 18:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.0016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:27 - INFO - train.train_snli_ve - loss is tensor(0.6249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2214/16548 [1:01:08<6:38:01,  1.67s/it]11/15/2022 18:07:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.6116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:29 - INFO - train.train_snli_ve - loss is tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2215/16548 [1:01:09<6:38:32,  1.67s/it]11/15/2022 18:07:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.9918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:30 - INFO - train.train_snli_ve - loss is tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2216/16548 [1:01:11<6:41:46,  1.68s/it]11/15/2022 18:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.3630e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:32 - INFO - train.train_snli_ve - loss is tensor(0.9890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2217/16548 [1:01:13<6:41:33,  1.68s/it]11/15/2022 18:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.1008e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:34 - INFO - train.train_snli_ve - loss is tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2218/16548 [1:01:14<6:42:49,  1.69s/it]11/15/2022 18:07:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.3660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:35 - INFO - train.train_snli_ve - loss is tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2219/16548 [1:01:16<6:38:28,  1.67s/it]11/15/2022 18:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.0207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:37 - INFO - train.train_snli_ve - loss is tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2220/16548 [1:01:18<6:39:14,  1.67s/it]11/15/2022 18:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.6900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:39 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2221/16548 [1:01:19<6:39:25,  1.67s/it]11/15/2022 18:07:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:40 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2222/16548 [1:01:21<6:40:05,  1.68s/it]11/15/2022 18:07:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.7420e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:42 - INFO - train.train_snli_ve - loss is tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2223/16548 [1:01:23<6:41:18,  1.68s/it]11/15/2022 18:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.2766e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:44 - INFO - train.train_snli_ve - loss is tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2224/16548 [1:01:25<6:48:57,  1.71s/it]11/15/2022 18:07:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.4549e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:45 - INFO - train.train_snli_ve - loss is tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2225/16548 [1:01:26<6:44:23,  1.69s/it]11/15/2022 18:07:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.3997e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:47 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2226/16548 [1:01:28<6:47:01,  1.71s/it]11/15/2022 18:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.1645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:49 - INFO - train.train_snli_ve - loss is tensor(0.8467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2227/16548 [1:01:30<6:44:26,  1.69s/it]11/15/2022 18:07:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.3350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:51 - INFO - train.train_snli_ve - loss is tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2228/16548 [1:01:31<6:41:51,  1.68s/it]11/15/2022 18:07:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.0213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:52 - INFO - train.train_snli_ve - loss is tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2229/16548 [1:01:33<6:40:21,  1.68s/it]11/15/2022 18:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.2341e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:54 - INFO - train.train_snli_ve - loss is tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2230/16548 [1:01:35<6:42:49,  1.69s/it]11/15/2022 18:07:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.7004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:56 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2231/16548 [1:01:36<6:41:37,  1.68s/it]11/15/2022 18:07:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.1472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:57 - INFO - train.train_snli_ve - loss is tensor(0.8859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2232/16548 [1:01:38<6:38:41,  1.67s/it]11/15/2022 18:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.7296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:07:59 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13% 2233/16548 [1:01:40<6:41:05,  1.68s/it]11/15/2022 18:08:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.9584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:01 - INFO - train.train_snli_ve - loss is tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2234/16548 [1:01:41<6:41:41,  1.68s/it]11/15/2022 18:08:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.1024e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:02 - INFO - train.train_snli_ve - loss is tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2235/16548 [1:01:43<6:42:05,  1.69s/it]11/15/2022 18:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.8123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:04 - INFO - train.train_snli_ve - loss is tensor(1.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2236/16548 [1:01:45<6:42:11,  1.69s/it]11/15/2022 18:08:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.8556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:06 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2237/16548 [1:01:46<6:38:41,  1.67s/it]11/15/2022 18:08:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.1497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:07 - INFO - train.train_snli_ve - loss is tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2238/16548 [1:01:48<6:41:36,  1.68s/it]11/15/2022 18:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.2028e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:09 - INFO - train.train_snli_ve - loss is tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2239/16548 [1:01:50<6:42:32,  1.69s/it]11/15/2022 18:08:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.1659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:11 - INFO - train.train_snli_ve - loss is tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2240/16548 [1:01:52<6:44:22,  1.70s/it]11/15/2022 18:08:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.4286e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:12 - INFO - train.train_snli_ve - loss is tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2241/16548 [1:01:53<6:41:06,  1.68s/it]11/15/2022 18:08:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.5864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:14 - INFO - train.train_snli_ve - loss is tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2242/16548 [1:01:55<6:40:25,  1.68s/it]11/15/2022 18:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:16 - INFO - train.train_snli_ve - loss is tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2243/16548 [1:01:56<6:37:20,  1.67s/it]11/15/2022 18:08:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:17 - INFO - train.train_snli_ve - loss is tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2244/16548 [1:01:58<6:37:41,  1.67s/it]11/15/2022 18:08:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:19 - INFO - train.train_snli_ve - loss is tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2245/16548 [1:02:00<6:41:48,  1.69s/it]11/15/2022 18:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1580e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:21 - INFO - train.train_snli_ve - loss is tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2246/16548 [1:02:02<6:43:32,  1.69s/it]11/15/2022 18:08:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.8130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:22 - INFO - train.train_snli_ve - loss is tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2247/16548 [1:02:03<6:38:58,  1.67s/it]11/15/2022 18:08:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1472e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:24 - INFO - train.train_snli_ve - loss is tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2248/16548 [1:02:05<6:37:24,  1.67s/it]11/15/2022 18:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.8132e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:26 - INFO - train.train_snli_ve - loss is tensor(0.8243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2249/16548 [1:02:07<6:36:05,  1.66s/it]11/15/2022 18:08:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.8990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:27 - INFO - train.train_snli_ve - loss is tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2250/16548 [1:02:08<6:37:08,  1.67s/it]11/15/2022 18:08:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.5818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:29 - INFO - train.train_snli_ve - loss is tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2251/16548 [1:02:10<6:41:32,  1.69s/it]11/15/2022 18:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:31 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2252/16548 [1:02:12<6:41:56,  1.69s/it]11/15/2022 18:08:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:33 - INFO - train.train_snli_ve - loss is tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2253/16548 [1:02:13<6:39:56,  1.68s/it]11/15/2022 18:08:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.7049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:34 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2254/16548 [1:02:15<6:37:05,  1.67s/it]11/15/2022 18:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:36 - INFO - train.train_snli_ve - loss is tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2255/16548 [1:02:17<6:39:45,  1.68s/it]11/15/2022 18:08:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:38 - INFO - train.train_snli_ve - loss is tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2256/16548 [1:02:18<6:40:05,  1.68s/it]11/15/2022 18:08:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:39 - INFO - train.train_snli_ve - loss is tensor(0.8256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2257/16548 [1:02:20<6:38:04,  1.67s/it]11/15/2022 18:08:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.4802e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:41 - INFO - train.train_snli_ve - loss is tensor(0.7636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2258/16548 [1:02:22<6:39:04,  1.68s/it]11/15/2022 18:08:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.4707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:42 - INFO - train.train_snli_ve - loss is tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2259/16548 [1:02:23<6:34:13,  1.66s/it]11/15/2022 18:08:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.5304e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:44 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2260/16548 [1:02:25<6:35:07,  1.66s/it]11/15/2022 18:08:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.1647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:46 - INFO - train.train_snli_ve - loss is tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2261/16548 [1:02:27<6:37:52,  1.67s/it]11/15/2022 18:08:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.7863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:48 - INFO - train.train_snli_ve - loss is tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2262/16548 [1:02:28<6:38:33,  1.67s/it]11/15/2022 18:08:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.1459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:49 - INFO - train.train_snli_ve - loss is tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2263/16548 [1:02:30<6:41:32,  1.69s/it]11/15/2022 18:08:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:51 - INFO - train.train_snli_ve - loss is tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2264/16548 [1:02:32<6:42:29,  1.69s/it]11/15/2022 18:08:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.1056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:53 - INFO - train.train_snli_ve - loss is tensor(0.8413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2265/16548 [1:02:33<6:38:08,  1.67s/it]11/15/2022 18:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2755e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:54 - INFO - train.train_snli_ve - loss is tensor(0.9869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2266/16548 [1:02:35<6:39:14,  1.68s/it]11/15/2022 18:08:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.2397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:56 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2267/16548 [1:02:37<6:37:08,  1.67s/it]11/15/2022 18:08:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.5794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:58 - INFO - train.train_snli_ve - loss is tensor(0.4810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2268/16548 [1:02:38<6:39:10,  1.68s/it]11/15/2022 18:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:08:59 - INFO - train.train_snli_ve - loss is tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2269/16548 [1:02:40<6:40:43,  1.68s/it]11/15/2022 18:09:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:01 - INFO - train.train_snli_ve - loss is tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2270/16548 [1:02:42<6:42:43,  1.69s/it]11/15/2022 18:09:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:03 - INFO - train.train_snli_ve - loss is tensor(0.8248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2271/16548 [1:02:43<6:43:10,  1.69s/it]11/15/2022 18:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.4252e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:04 - INFO - train.train_snli_ve - loss is tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2272/16548 [1:02:45<6:41:47,  1.69s/it]11/15/2022 18:09:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.4078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:06 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2273/16548 [1:02:47<6:42:19,  1.69s/it]11/15/2022 18:09:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.2064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:08 - INFO - train.train_snli_ve - loss is tensor(0.8925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2274/16548 [1:02:49<6:43:30,  1.70s/it]11/15/2022 18:09:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6778e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:10 - INFO - train.train_snli_ve - loss is tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2275/16548 [1:02:50<6:44:20,  1.70s/it]11/15/2022 18:09:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.9688e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:11 - INFO - train.train_snli_ve - loss is tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2276/16548 [1:02:52<6:40:57,  1.69s/it]11/15/2022 18:09:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0439e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:13 - INFO - train.train_snli_ve - loss is tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2277/16548 [1:02:54<6:38:06,  1.67s/it]11/15/2022 18:09:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.3507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:14 - INFO - train.train_snli_ve - loss is tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2278/16548 [1:02:55<6:36:46,  1.67s/it]11/15/2022 18:09:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.8741e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:16 - INFO - train.train_snli_ve - loss is tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2279/16548 [1:02:57<6:37:09,  1.67s/it]11/15/2022 18:09:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.5684e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:18 - INFO - train.train_snli_ve - loss is tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2280/16548 [1:02:59<6:34:37,  1.66s/it]11/15/2022 18:09:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.5590e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:19 - INFO - train.train_snli_ve - loss is tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2281/16548 [1:03:00<6:32:11,  1.65s/it]11/15/2022 18:09:21 - INFO - train.train_snli_ve - kd_loss is tensor(9.1355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:21 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2282/16548 [1:03:02<6:32:29,  1.65s/it]11/15/2022 18:09:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:23 - INFO - train.train_snli_ve - loss is tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2283/16548 [1:03:03<6:32:44,  1.65s/it]11/15/2022 18:09:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.7308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:24 - INFO - train.train_snli_ve - loss is tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2284/16548 [1:03:05<6:36:28,  1.67s/it]11/15/2022 18:09:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:26 - INFO - train.train_snli_ve - loss is tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2285/16548 [1:03:07<6:34:49,  1.66s/it]11/15/2022 18:09:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.6682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:28 - INFO - train.train_snli_ve - loss is tensor(0.9225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2286/16548 [1:03:09<6:38:04,  1.67s/it]11/15/2022 18:09:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:29 - INFO - train.train_snli_ve - loss is tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2287/16548 [1:03:10<6:39:01,  1.68s/it]11/15/2022 18:09:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.7346e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:31 - INFO - train.train_snli_ve - loss is tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2288/16548 [1:03:12<6:38:25,  1.68s/it]11/15/2022 18:09:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.9385e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:33 - INFO - train.train_snli_ve - loss is tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2289/16548 [1:03:14<6:41:53,  1.69s/it]11/15/2022 18:09:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:35 - INFO - train.train_snli_ve - loss is tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2290/16548 [1:03:15<6:39:38,  1.68s/it]11/15/2022 18:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:36 - INFO - train.train_snli_ve - loss is tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2291/16548 [1:03:17<6:43:07,  1.70s/it]11/15/2022 18:09:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.8349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:38 - INFO - train.train_snli_ve - loss is tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2292/16548 [1:03:19<6:42:27,  1.69s/it]11/15/2022 18:09:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.0651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:40 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2293/16548 [1:03:20<6:40:21,  1.69s/it]11/15/2022 18:09:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.3463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:41 - INFO - train.train_snli_ve - loss is tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2294/16548 [1:03:22<6:40:08,  1.68s/it]11/15/2022 18:09:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:43 - INFO - train.train_snli_ve - loss is tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2295/16548 [1:03:24<6:38:13,  1.68s/it]11/15/2022 18:09:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.2523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:45 - INFO - train.train_snli_ve - loss is tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2296/16548 [1:03:25<6:38:52,  1.68s/it]11/15/2022 18:09:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:46 - INFO - train.train_snli_ve - loss is tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2297/16548 [1:03:27<6:40:07,  1.68s/it]11/15/2022 18:09:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.0020e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:48 - INFO - train.train_snli_ve - loss is tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2298/16548 [1:03:29<6:41:28,  1.69s/it]11/15/2022 18:09:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:50 - INFO - train.train_snli_ve - loss is tensor(0.9211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2299/16548 [1:03:30<6:38:16,  1.68s/it]11/15/2022 18:09:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:51 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2300/16548 [1:03:32<6:43:28,  1.70s/it]11/15/2022 18:09:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:53 - INFO - train.train_snli_ve - loss is tensor(1.0412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2301/16548 [1:03:34<6:43:06,  1.70s/it]11/15/2022 18:09:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:55 - INFO - train.train_snli_ve - loss is tensor(0.7809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2302/16548 [1:03:36<6:43:23,  1.70s/it]11/15/2022 18:09:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.2077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:56 - INFO - train.train_snli_ve - loss is tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2303/16548 [1:03:37<6:38:56,  1.68s/it]11/15/2022 18:09:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:09:58 - INFO - train.train_snli_ve - loss is tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2304/16548 [1:03:39<6:38:39,  1.68s/it]11/15/2022 18:10:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.0554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:00 - INFO - train.train_snli_ve - loss is tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2305/16548 [1:03:40<6:34:38,  1.66s/it]11/15/2022 18:10:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.2988e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:01 - INFO - train.train_snli_ve - loss is tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2306/16548 [1:03:42<6:35:30,  1.67s/it]11/15/2022 18:10:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.9141e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:03 - INFO - train.train_snli_ve - loss is tensor(0.9710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2307/16548 [1:03:44<6:39:27,  1.68s/it]11/15/2022 18:10:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.7024e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:05 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2308/16548 [1:03:46<6:43:18,  1.70s/it]11/15/2022 18:10:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.1354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:07 - INFO - train.train_snli_ve - loss is tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2309/16548 [1:03:47<6:42:04,  1.69s/it]11/15/2022 18:10:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.7241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:08 - INFO - train.train_snli_ve - loss is tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2310/16548 [1:03:49<6:39:53,  1.69s/it]11/15/2022 18:10:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.2703e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:10 - INFO - train.train_snli_ve - loss is tensor(0.8346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2311/16548 [1:03:51<6:37:48,  1.68s/it]11/15/2022 18:10:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.0899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:12 - INFO - train.train_snli_ve - loss is tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2312/16548 [1:03:52<6:42:19,  1.70s/it]11/15/2022 18:10:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.0659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:13 - INFO - train.train_snli_ve - loss is tensor(0.8844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2313/16548 [1:03:54<6:38:12,  1.68s/it]11/15/2022 18:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.0875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:15 - INFO - train.train_snli_ve - loss is tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2314/16548 [1:03:56<6:36:46,  1.67s/it]11/15/2022 18:10:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:17 - INFO - train.train_snli_ve - loss is tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2315/16548 [1:03:57<6:35:40,  1.67s/it]11/15/2022 18:10:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.9072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:18 - INFO - train.train_snli_ve - loss is tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2316/16548 [1:03:59<6:36:25,  1.67s/it]11/15/2022 18:10:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.0563e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:20 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2317/16548 [1:04:01<6:35:07,  1.67s/it]11/15/2022 18:10:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.6690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:22 - INFO - train.train_snli_ve - loss is tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2318/16548 [1:04:02<6:34:19,  1.66s/it]11/15/2022 18:10:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.0169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:23 - INFO - train.train_snli_ve - loss is tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2319/16548 [1:04:04<6:33:47,  1.66s/it]11/15/2022 18:10:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.2143e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:25 - INFO - train.train_snli_ve - loss is tensor(1.0392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2320/16548 [1:04:06<6:34:28,  1.66s/it]11/15/2022 18:10:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.5999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:27 - INFO - train.train_snli_ve - loss is tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2321/16548 [1:04:07<6:34:25,  1.66s/it]11/15/2022 18:10:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.3824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:28 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2322/16548 [1:04:09<6:37:17,  1.68s/it]11/15/2022 18:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.7532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:30 - INFO - train.train_snli_ve - loss is tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2323/16548 [1:04:11<6:36:26,  1.67s/it]11/15/2022 18:10:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.8612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:32 - INFO - train.train_snli_ve - loss is tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2324/16548 [1:04:12<6:40:52,  1.69s/it]11/15/2022 18:10:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.0262e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:33 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2325/16548 [1:04:14<6:41:51,  1.70s/it]11/15/2022 18:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:35 - INFO - train.train_snli_ve - loss is tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2326/16548 [1:04:16<6:45:51,  1.71s/it]11/15/2022 18:10:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.8239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:37 - INFO - train.train_snli_ve - loss is tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2327/16548 [1:04:18<6:44:43,  1.71s/it]11/15/2022 18:10:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.9684e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:39 - INFO - train.train_snli_ve - loss is tensor(0.8032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2328/16548 [1:04:19<6:46:31,  1.72s/it]11/15/2022 18:10:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.2073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:40 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2329/16548 [1:04:21<6:43:49,  1.70s/it]11/15/2022 18:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.8404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:42 - INFO - train.train_snli_ve - loss is tensor(0.7720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2330/16548 [1:04:23<6:41:29,  1.69s/it]11/15/2022 18:10:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:44 - INFO - train.train_snli_ve - loss is tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2331/16548 [1:04:24<6:41:42,  1.70s/it]11/15/2022 18:10:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.8842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:45 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2332/16548 [1:04:26<6:43:06,  1.70s/it]11/15/2022 18:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.2396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:47 - INFO - train.train_snli_ve - loss is tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2333/16548 [1:04:28<6:41:28,  1.69s/it]11/15/2022 18:10:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.0542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:49 - INFO - train.train_snli_ve - loss is tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2334/16548 [1:04:29<6:40:19,  1.69s/it]11/15/2022 18:10:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.7882e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:50 - INFO - train.train_snli_ve - loss is tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2335/16548 [1:04:31<6:36:31,  1.67s/it]11/15/2022 18:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.0853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:52 - INFO - train.train_snli_ve - loss is tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2336/16548 [1:04:33<6:35:49,  1.67s/it]11/15/2022 18:10:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.9314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:54 - INFO - train.train_snli_ve - loss is tensor(0.8569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2337/16548 [1:04:34<6:38:31,  1.68s/it]11/15/2022 18:10:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.7577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:55 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2338/16548 [1:04:36<6:34:07,  1.66s/it]11/15/2022 18:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.3826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:57 - INFO - train.train_snli_ve - loss is tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2339/16548 [1:04:38<6:35:08,  1.67s/it]11/15/2022 18:10:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:10:59 - INFO - train.train_snli_ve - loss is tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2340/16548 [1:04:39<6:34:00,  1.66s/it]11/15/2022 18:11:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.8297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:00 - INFO - train.train_snli_ve - loss is tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2341/16548 [1:04:41<6:33:27,  1.66s/it]11/15/2022 18:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.2712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:02 - INFO - train.train_snli_ve - loss is tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2342/16548 [1:04:43<6:35:30,  1.67s/it]11/15/2022 18:11:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.0635e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:04 - INFO - train.train_snli_ve - loss is tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2343/16548 [1:04:44<6:35:03,  1.67s/it]11/15/2022 18:11:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.4051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:05 - INFO - train.train_snli_ve - loss is tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2344/16548 [1:04:46<6:34:52,  1.67s/it]11/15/2022 18:11:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.0427e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:07 - INFO - train.train_snli_ve - loss is tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2345/16548 [1:04:48<6:33:29,  1.66s/it]11/15/2022 18:11:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.3472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:09 - INFO - train.train_snli_ve - loss is tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2346/16548 [1:04:49<6:32:51,  1.66s/it]11/15/2022 18:11:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.3892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:10 - INFO - train.train_snli_ve - loss is tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2347/16548 [1:04:51<6:36:19,  1.67s/it]11/15/2022 18:11:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.7498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:12 - INFO - train.train_snli_ve - loss is tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2348/16548 [1:04:53<6:35:31,  1.67s/it]11/15/2022 18:11:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1895e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:14 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2349/16548 [1:04:54<6:33:22,  1.66s/it]11/15/2022 18:11:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.3235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:15 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2350/16548 [1:04:56<6:42:24,  1.70s/it]11/15/2022 18:11:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.6572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:17 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2351/16548 [1:04:58<6:39:04,  1.69s/it]11/15/2022 18:11:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.4736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:19 - INFO - train.train_snli_ve - loss is tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2352/16548 [1:04:59<6:38:03,  1.68s/it]11/15/2022 18:11:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.0803e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:20 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2353/16548 [1:05:01<6:35:33,  1.67s/it]11/15/2022 18:11:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:22 - INFO - train.train_snli_ve - loss is tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2354/16548 [1:05:03<6:32:55,  1.66s/it]11/15/2022 18:11:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.7857e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:24 - INFO - train.train_snli_ve - loss is tensor(0.8903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2355/16548 [1:05:04<6:34:24,  1.67s/it]11/15/2022 18:11:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.9917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:25 - INFO - train.train_snli_ve - loss is tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2356/16548 [1:05:06<6:32:05,  1.66s/it]11/15/2022 18:11:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.1074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:27 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2357/16548 [1:05:08<6:33:10,  1.66s/it]11/15/2022 18:11:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.5602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:29 - INFO - train.train_snli_ve - loss is tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2358/16548 [1:05:09<6:35:26,  1.67s/it]11/15/2022 18:11:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.7353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:30 - INFO - train.train_snli_ve - loss is tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2359/16548 [1:05:11<6:36:15,  1.68s/it]11/15/2022 18:11:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.6288e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:32 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2360/16548 [1:05:13<6:34:30,  1.67s/it]11/15/2022 18:11:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.0239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:34 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2361/16548 [1:05:14<6:35:15,  1.67s/it]11/15/2022 18:11:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.4611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:35 - INFO - train.train_snli_ve - loss is tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2362/16548 [1:05:16<6:37:11,  1.68s/it]11/15/2022 18:11:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.0214e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:37 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2363/16548 [1:05:18<6:35:38,  1.67s/it]11/15/2022 18:11:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.2398e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:39 - INFO - train.train_snli_ve - loss is tensor(0.4799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2364/16548 [1:05:19<6:33:42,  1.67s/it]11/15/2022 18:11:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.9556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:40 - INFO - train.train_snli_ve - loss is tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2365/16548 [1:05:21<6:31:43,  1.66s/it]11/15/2022 18:11:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:42 - INFO - train.train_snli_ve - loss is tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2366/16548 [1:05:23<6:31:11,  1.66s/it]11/15/2022 18:11:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.9618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:44 - INFO - train.train_snli_ve - loss is tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2367/16548 [1:05:24<6:33:40,  1.67s/it]11/15/2022 18:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.2866e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:45 - INFO - train.train_snli_ve - loss is tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2368/16548 [1:05:26<6:35:31,  1.67s/it]11/15/2022 18:11:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.7446e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:47 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2369/16548 [1:05:28<6:34:32,  1.67s/it]11/15/2022 18:11:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.1173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:49 - INFO - train.train_snli_ve - loss is tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2370/16548 [1:05:30<6:40:51,  1.70s/it]11/15/2022 18:11:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.8479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:51 - INFO - train.train_snli_ve - loss is tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2371/16548 [1:05:31<6:39:11,  1.69s/it]11/15/2022 18:11:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.3449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:52 - INFO - train.train_snli_ve - loss is tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2372/16548 [1:05:33<6:40:05,  1.69s/it]11/15/2022 18:11:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.9575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:54 - INFO - train.train_snli_ve - loss is tensor(0.8240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2373/16548 [1:05:35<6:41:06,  1.70s/it]11/15/2022 18:11:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.0012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:56 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2374/16548 [1:05:36<6:40:52,  1.70s/it]11/15/2022 18:11:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:57 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2375/16548 [1:05:38<6:37:33,  1.68s/it]11/15/2022 18:11:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.5207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:11:59 - INFO - train.train_snli_ve - loss is tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2376/16548 [1:05:40<6:39:04,  1.69s/it]11/15/2022 18:12:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.6825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:01 - INFO - train.train_snli_ve - loss is tensor(0.9842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2377/16548 [1:05:41<6:39:04,  1.69s/it]11/15/2022 18:12:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.0913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:02 - INFO - train.train_snli_ve - loss is tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2378/16548 [1:05:43<6:38:05,  1.69s/it]11/15/2022 18:12:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.9786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:04 - INFO - train.train_snli_ve - loss is tensor(0.8862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2379/16548 [1:05:45<6:41:23,  1.70s/it]11/15/2022 18:12:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.6762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:06 - INFO - train.train_snli_ve - loss is tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2380/16548 [1:05:46<6:39:38,  1.69s/it]11/15/2022 18:12:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.9731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:07 - INFO - train.train_snli_ve - loss is tensor(0.9747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2381/16548 [1:05:48<6:37:42,  1.68s/it]11/15/2022 18:12:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.2176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:09 - INFO - train.train_snli_ve - loss is tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2382/16548 [1:05:50<6:37:04,  1.68s/it]11/15/2022 18:12:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.0915e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:11 - INFO - train.train_snli_ve - loss is tensor(0.8172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2383/16548 [1:05:51<6:34:03,  1.67s/it]11/15/2022 18:12:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.0071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:12 - INFO - train.train_snli_ve - loss is tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2384/16548 [1:05:53<6:33:39,  1.67s/it]11/15/2022 18:12:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.5349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:14 - INFO - train.train_snli_ve - loss is tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2385/16548 [1:05:55<6:37:26,  1.68s/it]11/15/2022 18:12:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.0257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:16 - INFO - train.train_snli_ve - loss is tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2386/16548 [1:05:57<6:37:57,  1.69s/it]11/15/2022 18:12:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.5021e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:17 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2387/16548 [1:05:58<6:35:28,  1.68s/it]11/15/2022 18:12:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.0815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:19 - INFO - train.train_snli_ve - loss is tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2388/16548 [1:06:00<6:38:44,  1.69s/it]11/15/2022 18:12:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.0369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:21 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2389/16548 [1:06:02<6:39:33,  1.69s/it]11/15/2022 18:12:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.4448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:23 - INFO - train.train_snli_ve - loss is tensor(0.7616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2390/16548 [1:06:03<6:37:46,  1.69s/it]11/15/2022 18:12:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.8237e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:24 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2391/16548 [1:06:05<6:35:24,  1.68s/it]11/15/2022 18:12:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.5117e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:26 - INFO - train.train_snli_ve - loss is tensor(0.8987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2392/16548 [1:06:07<6:41:14,  1.70s/it]11/15/2022 18:12:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.8127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:28 - INFO - train.train_snli_ve - loss is tensor(0.8494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2393/16548 [1:06:08<6:37:21,  1.68s/it]11/15/2022 18:12:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.1872e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:29 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2394/16548 [1:06:10<6:35:55,  1.68s/it]11/15/2022 18:12:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.8771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:31 - INFO - train.train_snli_ve - loss is tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2395/16548 [1:06:12<6:35:04,  1.67s/it]11/15/2022 18:12:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.7029e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:33 - INFO - train.train_snli_ve - loss is tensor(0.9113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2396/16548 [1:06:13<6:33:32,  1.67s/it]11/15/2022 18:12:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.4920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:34 - INFO - train.train_snli_ve - loss is tensor(0.9273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2397/16548 [1:06:15<6:32:22,  1.66s/it]11/15/2022 18:12:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.7361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:36 - INFO - train.train_snli_ve - loss is tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2398/16548 [1:06:17<6:33:55,  1.67s/it]11/15/2022 18:12:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.4651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:38 - INFO - train.train_snli_ve - loss is tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14% 2399/16548 [1:06:18<6:32:37,  1.66s/it]11/15/2022 18:12:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.3784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:39 - INFO - train.train_snli_ve - loss is tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2400/16548 [1:06:20<6:39:57,  1.70s/it]11/15/2022 18:12:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.8132e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:41 - INFO - train.train_snli_ve - loss is tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2401/16548 [1:06:22<6:38:39,  1.69s/it]11/15/2022 18:12:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.4164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:43 - INFO - train.train_snli_ve - loss is tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2402/16548 [1:06:23<6:35:40,  1.68s/it]11/15/2022 18:12:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.8405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:44 - INFO - train.train_snli_ve - loss is tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2403/16548 [1:06:25<6:36:59,  1.68s/it]11/15/2022 18:12:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.2312e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:46 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2404/16548 [1:06:27<6:40:06,  1.70s/it]11/15/2022 18:12:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.6383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:48 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2405/16548 [1:06:28<6:35:20,  1.68s/it]11/15/2022 18:12:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.6619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:49 - INFO - train.train_snli_ve - loss is tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2406/16548 [1:06:30<6:37:31,  1.69s/it]11/15/2022 18:12:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.5032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:51 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2407/16548 [1:06:32<6:36:50,  1.68s/it]11/15/2022 18:12:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.4246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:53 - INFO - train.train_snli_ve - loss is tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2408/16548 [1:06:34<6:36:39,  1.68s/it]11/15/2022 18:12:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.7107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:54 - INFO - train.train_snli_ve - loss is tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2409/16548 [1:06:35<6:35:13,  1.68s/it]11/15/2022 18:12:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.7247e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:56 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2410/16548 [1:06:37<6:34:27,  1.67s/it]11/15/2022 18:12:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.7007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:58 - INFO - train.train_snli_ve - loss is tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2411/16548 [1:06:39<6:36:08,  1.68s/it]11/15/2022 18:12:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.9624e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:12:59 - INFO - train.train_snli_ve - loss is tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2412/16548 [1:06:40<6:36:18,  1.68s/it]11/15/2022 18:13:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.4949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:01 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2413/16548 [1:06:42<6:36:11,  1.68s/it]11/15/2022 18:13:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.7268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:03 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2414/16548 [1:06:44<6:35:05,  1.68s/it]11/15/2022 18:13:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.3560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:05 - INFO - train.train_snli_ve - loss is tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2415/16548 [1:06:45<6:34:28,  1.67s/it]11/15/2022 18:13:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.3105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:06 - INFO - train.train_snli_ve - loss is tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2416/16548 [1:06:47<6:34:25,  1.67s/it]11/15/2022 18:13:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.0645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:08 - INFO - train.train_snli_ve - loss is tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2417/16548 [1:06:49<6:35:32,  1.68s/it]11/15/2022 18:13:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.1258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:10 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2418/16548 [1:06:50<6:38:35,  1.69s/it]11/15/2022 18:13:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.9485e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:11 - INFO - train.train_snli_ve - loss is tensor(0.5450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2419/16548 [1:06:52<6:34:44,  1.68s/it]11/15/2022 18:13:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.2567e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:13 - INFO - train.train_snli_ve - loss is tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2420/16548 [1:06:54<6:36:27,  1.68s/it]11/15/2022 18:13:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.4094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:15 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2421/16548 [1:06:55<6:37:28,  1.69s/it]11/15/2022 18:13:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.6846e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:16 - INFO - train.train_snli_ve - loss is tensor(0.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2422/16548 [1:06:57<6:34:56,  1.68s/it]11/15/2022 18:13:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.3403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:18 - INFO - train.train_snli_ve - loss is tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2423/16548 [1:06:59<6:32:30,  1.67s/it]11/15/2022 18:13:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.2044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:20 - INFO - train.train_snli_ve - loss is tensor(0.8221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2424/16548 [1:07:00<6:33:48,  1.67s/it]11/15/2022 18:13:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.1795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:21 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2425/16548 [1:07:02<6:36:28,  1.68s/it]11/15/2022 18:13:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.2336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:23 - INFO - train.train_snli_ve - loss is tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2426/16548 [1:07:04<6:35:58,  1.68s/it]11/15/2022 18:13:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.6219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:25 - INFO - train.train_snli_ve - loss is tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2427/16548 [1:07:05<6:37:09,  1.69s/it]11/15/2022 18:13:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.0192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:26 - INFO - train.train_snli_ve - loss is tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2428/16548 [1:07:07<6:36:03,  1.68s/it]11/15/2022 18:13:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.7683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:28 - INFO - train.train_snli_ve - loss is tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2429/16548 [1:07:09<6:35:03,  1.68s/it]11/15/2022 18:13:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:30 - INFO - train.train_snli_ve - loss is tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2430/16548 [1:07:10<6:36:15,  1.68s/it]11/15/2022 18:13:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.0481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:31 - INFO - train.train_snli_ve - loss is tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2431/16548 [1:07:12<6:31:54,  1.67s/it]11/15/2022 18:13:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.4855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:33 - INFO - train.train_snli_ve - loss is tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2432/16548 [1:07:14<6:32:28,  1.67s/it]11/15/2022 18:13:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.9318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:35 - INFO - train.train_snli_ve - loss is tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2433/16548 [1:07:15<6:30:59,  1.66s/it]11/15/2022 18:13:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:36 - INFO - train.train_snli_ve - loss is tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2434/16548 [1:07:17<6:31:50,  1.67s/it]11/15/2022 18:13:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.9634e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:38 - INFO - train.train_snli_ve - loss is tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2435/16548 [1:07:19<6:31:18,  1.66s/it]11/15/2022 18:13:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:40 - INFO - train.train_snli_ve - loss is tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2436/16548 [1:07:20<6:30:19,  1.66s/it]11/15/2022 18:13:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.7961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:41 - INFO - train.train_snli_ve - loss is tensor(1.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2437/16548 [1:07:22<6:30:20,  1.66s/it]11/15/2022 18:13:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.1782e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:43 - INFO - train.train_snli_ve - loss is tensor(0.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2438/16548 [1:07:24<6:29:55,  1.66s/it]11/15/2022 18:13:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:45 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2439/16548 [1:07:25<6:30:16,  1.66s/it]11/15/2022 18:13:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.1289e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:46 - INFO - train.train_snli_ve - loss is tensor(1.2248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2440/16548 [1:07:27<6:32:10,  1.67s/it]11/15/2022 18:13:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.5295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:48 - INFO - train.train_snli_ve - loss is tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2441/16548 [1:07:29<6:29:42,  1.66s/it]11/15/2022 18:13:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.2750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:50 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2442/16548 [1:07:30<6:32:17,  1.67s/it]11/15/2022 18:13:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.1361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:51 - INFO - train.train_snli_ve - loss is tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2443/16548 [1:07:32<6:31:08,  1.66s/it]11/15/2022 18:13:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.1602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:53 - INFO - train.train_snli_ve - loss is tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2444/16548 [1:07:34<6:30:18,  1.66s/it]11/15/2022 18:13:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.1468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:55 - INFO - train.train_snli_ve - loss is tensor(0.8709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2445/16548 [1:07:35<6:31:40,  1.67s/it]11/15/2022 18:13:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.6786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:56 - INFO - train.train_snli_ve - loss is tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2446/16548 [1:07:37<6:31:20,  1.67s/it]11/15/2022 18:13:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.8355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:13:58 - INFO - train.train_snli_ve - loss is tensor(0.9474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2447/16548 [1:07:39<6:30:08,  1.66s/it]11/15/2022 18:14:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.9208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:00 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2448/16548 [1:07:40<6:29:22,  1.66s/it]11/15/2022 18:14:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.7816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:01 - INFO - train.train_snli_ve - loss is tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2449/16548 [1:07:42<6:33:21,  1.67s/it]11/15/2022 18:14:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.9759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:03 - INFO - train.train_snli_ve - loss is tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2450/16548 [1:07:44<6:35:55,  1.69s/it]11/15/2022 18:14:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.8098e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:05 - INFO - train.train_snli_ve - loss is tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2451/16548 [1:07:45<6:33:22,  1.67s/it]11/15/2022 18:14:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.8982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:06 - INFO - train.train_snli_ve - loss is tensor(0.4980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2452/16548 [1:07:47<6:37:49,  1.69s/it]11/15/2022 18:14:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.3125e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:08 - INFO - train.train_snli_ve - loss is tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2453/16548 [1:07:49<6:37:21,  1.69s/it]11/15/2022 18:14:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:10 - INFO - train.train_snli_ve - loss is tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2454/16548 [1:07:51<6:37:29,  1.69s/it]11/15/2022 18:14:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.2998e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:11 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2455/16548 [1:07:52<6:35:52,  1.69s/it]11/15/2022 18:14:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.9444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:13 - INFO - train.train_snli_ve - loss is tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2456/16548 [1:07:54<6:32:52,  1.67s/it]11/15/2022 18:14:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.5412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:15 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2457/16548 [1:07:56<6:32:39,  1.67s/it]11/15/2022 18:14:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.5082e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:17 - INFO - train.train_snli_ve - loss is tensor(0.9366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2458/16548 [1:07:57<6:36:26,  1.69s/it]11/15/2022 18:14:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.5494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:18 - INFO - train.train_snli_ve - loss is tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2459/16548 [1:07:59<6:38:06,  1.70s/it]11/15/2022 18:14:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.6632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:20 - INFO - train.train_snli_ve - loss is tensor(0.8120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2460/16548 [1:08:01<6:39:34,  1.70s/it]11/15/2022 18:14:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.7776e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:22 - INFO - train.train_snli_ve - loss is tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2461/16548 [1:08:02<6:37:38,  1.69s/it]11/15/2022 18:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.4844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:23 - INFO - train.train_snli_ve - loss is tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2462/16548 [1:08:04<6:38:17,  1.70s/it]11/15/2022 18:14:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:25 - INFO - train.train_snli_ve - loss is tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2463/16548 [1:08:06<6:38:42,  1.70s/it]11/15/2022 18:14:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.6600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:27 - INFO - train.train_snli_ve - loss is tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2464/16548 [1:08:07<6:38:06,  1.70s/it]11/15/2022 18:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.5498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:28 - INFO - train.train_snli_ve - loss is tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2465/16548 [1:08:09<6:35:07,  1.68s/it]11/15/2022 18:14:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:30 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2466/16548 [1:08:11<6:34:15,  1.68s/it]11/15/2022 18:14:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.9877e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:32 - INFO - train.train_snli_ve - loss is tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2467/16548 [1:08:12<6:30:50,  1.67s/it]11/15/2022 18:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:33 - INFO - train.train_snli_ve - loss is tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2468/16548 [1:08:14<6:31:12,  1.67s/it]11/15/2022 18:14:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:35 - INFO - train.train_snli_ve - loss is tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2469/16548 [1:08:16<6:30:04,  1.66s/it]11/15/2022 18:14:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.2890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:37 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2470/16548 [1:08:17<6:30:12,  1.66s/it]11/15/2022 18:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.5876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:38 - INFO - train.train_snli_ve - loss is tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2471/16548 [1:08:19<6:33:44,  1.68s/it]11/15/2022 18:14:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:40 - INFO - train.train_snli_ve - loss is tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2472/16548 [1:08:21<6:34:54,  1.68s/it]11/15/2022 18:14:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:42 - INFO - train.train_snli_ve - loss is tensor(0.8457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2473/16548 [1:08:23<6:35:52,  1.69s/it]11/15/2022 18:14:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.8856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:43 - INFO - train.train_snli_ve - loss is tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2474/16548 [1:08:24<6:35:29,  1.69s/it]11/15/2022 18:14:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:45 - INFO - train.train_snli_ve - loss is tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2475/16548 [1:08:26<6:34:38,  1.68s/it]11/15/2022 18:14:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:47 - INFO - train.train_snli_ve - loss is tensor(0.8199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2476/16548 [1:08:28<6:35:16,  1.69s/it]11/15/2022 18:14:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.1805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:48 - INFO - train.train_snli_ve - loss is tensor(0.8318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2477/16548 [1:08:29<6:33:11,  1.68s/it]11/15/2022 18:14:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.4749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:50 - INFO - train.train_snli_ve - loss is tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2478/16548 [1:08:31<6:35:46,  1.69s/it]11/15/2022 18:14:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.7334e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:52 - INFO - train.train_snli_ve - loss is tensor(0.9440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2479/16548 [1:08:33<6:31:03,  1.67s/it]11/15/2022 18:14:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.0864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:53 - INFO - train.train_snli_ve - loss is tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2480/16548 [1:08:34<6:32:22,  1.67s/it]11/15/2022 18:14:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.2186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:55 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2481/16548 [1:08:36<6:35:23,  1.69s/it]11/15/2022 18:14:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4435e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:57 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2482/16548 [1:08:38<6:41:47,  1.71s/it]11/15/2022 18:14:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:14:59 - INFO - train.train_snli_ve - loss is tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2483/16548 [1:08:39<6:42:28,  1.72s/it]11/15/2022 18:15:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.6462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:00 - INFO - train.train_snli_ve - loss is tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2484/16548 [1:08:41<6:40:19,  1.71s/it]11/15/2022 18:15:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.1770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:02 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2485/16548 [1:08:43<6:39:05,  1.70s/it]11/15/2022 18:15:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.8892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:04 - INFO - train.train_snli_ve - loss is tensor(0.9283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2486/16548 [1:08:45<6:38:43,  1.70s/it]11/15/2022 18:15:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.3511e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:05 - INFO - train.train_snli_ve - loss is tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2487/16548 [1:08:46<6:39:21,  1.70s/it]11/15/2022 18:15:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.2575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:07 - INFO - train.train_snli_ve - loss is tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2488/16548 [1:08:48<6:35:21,  1.69s/it]11/15/2022 18:15:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.8541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:09 - INFO - train.train_snli_ve - loss is tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2489/16548 [1:08:50<6:37:58,  1.70s/it]11/15/2022 18:15:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.6212e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:11 - INFO - train.train_snli_ve - loss is tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2490/16548 [1:08:51<6:37:17,  1.70s/it]11/15/2022 18:15:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.2322e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:12 - INFO - train.train_snli_ve - loss is tensor(0.8685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2491/16548 [1:08:53<6:33:53,  1.68s/it]11/15/2022 18:15:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.4054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:14 - INFO - train.train_snli_ve - loss is tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2492/16548 [1:08:55<6:34:46,  1.69s/it]11/15/2022 18:15:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.0586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:16 - INFO - train.train_snli_ve - loss is tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2493/16548 [1:08:56<6:34:21,  1.68s/it]11/15/2022 18:15:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.6442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:17 - INFO - train.train_snli_ve - loss is tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2494/16548 [1:08:58<6:36:39,  1.69s/it]11/15/2022 18:15:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.7950e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:19 - INFO - train.train_snli_ve - loss is tensor(0.9865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2495/16548 [1:09:00<6:33:58,  1.68s/it]11/15/2022 18:15:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.1826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:21 - INFO - train.train_snli_ve - loss is tensor(0.5271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2496/16548 [1:09:01<6:34:20,  1.68s/it]11/15/2022 18:15:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.1562e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:22 - INFO - train.train_snli_ve - loss is tensor(0.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2497/16548 [1:09:03<6:35:18,  1.69s/it]11/15/2022 18:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.2993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:24 - INFO - train.train_snli_ve - loss is tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2498/16548 [1:09:05<6:32:34,  1.68s/it]11/15/2022 18:15:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.1260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:26 - INFO - train.train_snli_ve - loss is tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2499/16548 [1:09:06<6:30:48,  1.67s/it]11/15/2022 18:15:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.4588e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:27 - INFO - train.train_snli_ve - loss is tensor(0.4817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2500/16548 [1:09:08<6:35:40,  1.69s/it]11/15/2022 18:15:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.3087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:29 - INFO - train.train_snli_ve - loss is tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2501/16548 [1:09:10<6:33:58,  1.68s/it]11/15/2022 18:15:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.6892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:31 - INFO - train.train_snli_ve - loss is tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2502/16548 [1:09:11<6:33:02,  1.68s/it]11/15/2022 18:15:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:32 - INFO - train.train_snli_ve - loss is tensor(0.5314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2503/16548 [1:09:13<6:32:47,  1.68s/it]11/15/2022 18:15:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.1484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:34 - INFO - train.train_snli_ve - loss is tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2504/16548 [1:09:15<6:34:23,  1.68s/it]11/15/2022 18:15:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.7057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:36 - INFO - train.train_snli_ve - loss is tensor(1.0533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2505/16548 [1:09:17<6:32:59,  1.68s/it]11/15/2022 18:15:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.5629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:37 - INFO - train.train_snli_ve - loss is tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2506/16548 [1:09:18<6:31:40,  1.67s/it]11/15/2022 18:15:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.6524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:39 - INFO - train.train_snli_ve - loss is tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2507/16548 [1:09:20<6:35:12,  1.69s/it]11/15/2022 18:15:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.6780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:41 - INFO - train.train_snli_ve - loss is tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2508/16548 [1:09:22<6:35:32,  1.69s/it]11/15/2022 18:15:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.6456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:43 - INFO - train.train_snli_ve - loss is tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2509/16548 [1:09:23<6:34:08,  1.68s/it]11/15/2022 18:15:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.0365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:44 - INFO - train.train_snli_ve - loss is tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2510/16548 [1:09:25<6:35:42,  1.69s/it]11/15/2022 18:15:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.7763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:46 - INFO - train.train_snli_ve - loss is tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2511/16548 [1:09:27<6:35:37,  1.69s/it]11/15/2022 18:15:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:48 - INFO - train.train_snli_ve - loss is tensor(0.7900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2512/16548 [1:09:28<6:35:25,  1.69s/it]11/15/2022 18:15:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.1249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:49 - INFO - train.train_snli_ve - loss is tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2513/16548 [1:09:30<6:39:14,  1.71s/it]11/15/2022 18:15:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:51 - INFO - train.train_snli_ve - loss is tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2514/16548 [1:09:32<6:38:11,  1.70s/it]11/15/2022 18:15:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:53 - INFO - train.train_snli_ve - loss is tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2515/16548 [1:09:34<6:41:32,  1.72s/it]11/15/2022 18:15:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:54 - INFO - train.train_snli_ve - loss is tensor(0.7506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2516/16548 [1:09:35<6:40:16,  1.71s/it]11/15/2022 18:15:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:56 - INFO - train.train_snli_ve - loss is tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2517/16548 [1:09:37<6:38:17,  1.70s/it]11/15/2022 18:15:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.5912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:15:58 - INFO - train.train_snli_ve - loss is tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2518/16548 [1:09:39<6:38:25,  1.70s/it]11/15/2022 18:16:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.1502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:00 - INFO - train.train_snli_ve - loss is tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2519/16548 [1:09:40<6:39:48,  1.71s/it]11/15/2022 18:16:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:01 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2520/16548 [1:09:42<6:36:51,  1.70s/it]11/15/2022 18:16:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:03 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2521/16548 [1:09:44<6:39:59,  1.71s/it]11/15/2022 18:16:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:05 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2522/16548 [1:09:45<6:36:55,  1.70s/it]11/15/2022 18:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:06 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2523/16548 [1:09:47<6:39:37,  1.71s/it]11/15/2022 18:16:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.1100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:08 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2524/16548 [1:09:49<6:36:10,  1.70s/it]11/15/2022 18:16:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.9130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:10 - INFO - train.train_snli_ve - loss is tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2525/16548 [1:09:51<6:36:03,  1.69s/it]11/15/2022 18:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.1164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:11 - INFO - train.train_snli_ve - loss is tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2526/16548 [1:09:52<6:34:19,  1.69s/it]11/15/2022 18:16:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.2145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:13 - INFO - train.train_snli_ve - loss is tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2527/16548 [1:09:54<6:34:04,  1.69s/it]11/15/2022 18:16:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.9726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:15 - INFO - train.train_snli_ve - loss is tensor(0.8873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2528/16548 [1:09:56<6:34:20,  1.69s/it]11/15/2022 18:16:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.6533e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:17 - INFO - train.train_snli_ve - loss is tensor(0.9352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2529/16548 [1:09:57<6:37:23,  1.70s/it]11/15/2022 18:16:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.9206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:18 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2530/16548 [1:09:59<6:38:35,  1.71s/it]11/15/2022 18:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.1594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:20 - INFO - train.train_snli_ve - loss is tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2531/16548 [1:10:01<6:33:23,  1.68s/it]11/15/2022 18:16:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.2453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:22 - INFO - train.train_snli_ve - loss is tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2532/16548 [1:10:02<6:34:11,  1.69s/it]11/15/2022 18:16:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:23 - INFO - train.train_snli_ve - loss is tensor(0.8681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2533/16548 [1:10:04<6:33:39,  1.69s/it]11/15/2022 18:16:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.7805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:25 - INFO - train.train_snli_ve - loss is tensor(0.8923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2534/16548 [1:10:06<6:33:14,  1.68s/it]11/15/2022 18:16:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.0091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:27 - INFO - train.train_snli_ve - loss is tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2535/16548 [1:10:07<6:35:24,  1.69s/it]11/15/2022 18:16:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.4353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:28 - INFO - train.train_snli_ve - loss is tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2536/16548 [1:10:09<6:39:19,  1.71s/it]11/15/2022 18:16:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1959e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:30 - INFO - train.train_snli_ve - loss is tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2537/16548 [1:10:11<6:36:30,  1.70s/it]11/15/2022 18:16:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.8836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:32 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2538/16548 [1:10:13<6:35:28,  1.69s/it]11/15/2022 18:16:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.8357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:33 - INFO - train.train_snli_ve - loss is tensor(0.8526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2539/16548 [1:10:14<6:33:02,  1.68s/it]11/15/2022 18:16:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.2933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:35 - INFO - train.train_snli_ve - loss is tensor(0.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2540/16548 [1:10:16<6:31:03,  1.68s/it]11/15/2022 18:16:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.2044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:37 - INFO - train.train_snli_ve - loss is tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2541/16548 [1:10:17<6:30:41,  1.67s/it]11/15/2022 18:16:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.5809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:38 - INFO - train.train_snli_ve - loss is tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2542/16548 [1:10:19<6:31:48,  1.68s/it]11/15/2022 18:16:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:40 - INFO - train.train_snli_ve - loss is tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2543/16548 [1:10:21<6:32:59,  1.68s/it]11/15/2022 18:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.1302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:42 - INFO - train.train_snli_ve - loss is tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2544/16548 [1:10:23<6:31:21,  1.68s/it]11/15/2022 18:16:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.8665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:43 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2545/16548 [1:10:24<6:32:42,  1.68s/it]11/15/2022 18:16:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.8722e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:45 - INFO - train.train_snli_ve - loss is tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2546/16548 [1:10:26<6:35:44,  1.70s/it]11/15/2022 18:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.3458e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:47 - INFO - train.train_snli_ve - loss is tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2547/16548 [1:10:28<6:33:32,  1.69s/it]11/15/2022 18:16:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.2234e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:49 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2548/16548 [1:10:29<6:30:42,  1.67s/it]11/15/2022 18:16:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1250e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:50 - INFO - train.train_snli_ve - loss is tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2549/16548 [1:10:31<6:31:53,  1.68s/it]11/15/2022 18:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.2191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:52 - INFO - train.train_snli_ve - loss is tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2550/16548 [1:10:33<6:30:19,  1.67s/it]11/15/2022 18:16:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.5644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:54 - INFO - train.train_snli_ve - loss is tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2551/16548 [1:10:34<6:29:49,  1.67s/it]11/15/2022 18:16:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.3202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:55 - INFO - train.train_snli_ve - loss is tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2552/16548 [1:10:36<6:28:25,  1.67s/it]11/15/2022 18:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.6145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:57 - INFO - train.train_snli_ve - loss is tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2553/16548 [1:10:38<6:30:26,  1.67s/it]11/15/2022 18:16:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:16:59 - INFO - train.train_snli_ve - loss is tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2554/16548 [1:10:39<6:30:57,  1.68s/it]11/15/2022 18:17:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.6141e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:00 - INFO - train.train_snli_ve - loss is tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2555/16548 [1:10:41<6:33:34,  1.69s/it]11/15/2022 18:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.4583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:02 - INFO - train.train_snli_ve - loss is tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2556/16548 [1:10:43<6:32:34,  1.68s/it]11/15/2022 18:17:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.8139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:04 - INFO - train.train_snli_ve - loss is tensor(0.9473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2557/16548 [1:10:44<6:32:35,  1.68s/it]11/15/2022 18:17:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.3408e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:05 - INFO - train.train_snli_ve - loss is tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2558/16548 [1:10:46<6:30:42,  1.68s/it]11/15/2022 18:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.0636e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:07 - INFO - train.train_snli_ve - loss is tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2559/16548 [1:10:48<6:30:50,  1.68s/it]11/15/2022 18:17:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:09 - INFO - train.train_snli_ve - loss is tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2560/16548 [1:10:49<6:30:22,  1.67s/it]11/15/2022 18:17:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.2099e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:10 - INFO - train.train_snli_ve - loss is tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2561/16548 [1:10:51<6:26:35,  1.66s/it]11/15/2022 18:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:12 - INFO - train.train_snli_ve - loss is tensor(0.7643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2562/16548 [1:10:53<6:27:03,  1.66s/it]11/15/2022 18:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.0296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:14 - INFO - train.train_snli_ve - loss is tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2563/16548 [1:10:54<6:31:55,  1.68s/it]11/15/2022 18:17:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.9550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:15 - INFO - train.train_snli_ve - loss is tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15% 2564/16548 [1:10:56<6:31:56,  1.68s/it]11/15/2022 18:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.5822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:17 - INFO - train.train_snli_ve - loss is tensor(0.8065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2565/16548 [1:10:58<6:33:01,  1.69s/it]11/15/2022 18:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.8196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:19 - INFO - train.train_snli_ve - loss is tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2566/16548 [1:10:59<6:29:26,  1.67s/it]11/15/2022 18:17:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0594e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:20 - INFO - train.train_snli_ve - loss is tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2567/16548 [1:11:01<6:30:28,  1.68s/it]11/15/2022 18:17:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:22 - INFO - train.train_snli_ve - loss is tensor(0.8009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2568/16548 [1:11:03<6:28:00,  1.67s/it]11/15/2022 18:17:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:24 - INFO - train.train_snli_ve - loss is tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2569/16548 [1:11:04<6:30:18,  1.68s/it]11/15/2022 18:17:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.0697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:25 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2570/16548 [1:11:06<6:31:34,  1.68s/it]11/15/2022 18:17:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.3523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:27 - INFO - train.train_snli_ve - loss is tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2571/16548 [1:11:08<6:29:05,  1.67s/it]11/15/2022 18:17:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.4764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:29 - INFO - train.train_snli_ve - loss is tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2572/16548 [1:11:09<6:29:56,  1.67s/it]11/15/2022 18:17:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.4917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:30 - INFO - train.train_snli_ve - loss is tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2573/16548 [1:11:11<6:30:35,  1.68s/it]11/15/2022 18:17:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.5366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:32 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2574/16548 [1:11:13<6:30:37,  1.68s/it]11/15/2022 18:17:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.9618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:34 - INFO - train.train_snli_ve - loss is tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2575/16548 [1:11:14<6:27:04,  1.66s/it]11/15/2022 18:17:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.0893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:35 - INFO - train.train_snli_ve - loss is tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2576/16548 [1:11:16<6:28:27,  1.67s/it]11/15/2022 18:17:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:37 - INFO - train.train_snli_ve - loss is tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2577/16548 [1:11:18<6:25:27,  1.66s/it]11/15/2022 18:17:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.0029e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:39 - INFO - train.train_snli_ve - loss is tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2578/16548 [1:11:19<6:23:47,  1.65s/it]11/15/2022 18:17:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.0890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:40 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2579/16548 [1:11:21<6:23:30,  1.65s/it]11/15/2022 18:17:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.3875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:42 - INFO - train.train_snli_ve - loss is tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2580/16548 [1:11:23<6:25:57,  1.66s/it]11/15/2022 18:17:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:44 - INFO - train.train_snli_ve - loss is tensor(0.9585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2581/16548 [1:11:24<6:25:22,  1.66s/it]11/15/2022 18:17:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:45 - INFO - train.train_snli_ve - loss is tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2582/16548 [1:11:26<6:27:20,  1.66s/it]11/15/2022 18:17:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.0652e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:47 - INFO - train.train_snli_ve - loss is tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2583/16548 [1:11:28<6:26:28,  1.66s/it]11/15/2022 18:17:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:49 - INFO - train.train_snli_ve - loss is tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2584/16548 [1:11:29<6:27:29,  1.66s/it]11/15/2022 18:17:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.4869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:50 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2585/16548 [1:11:31<6:28:46,  1.67s/it]11/15/2022 18:17:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.8181e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:52 - INFO - train.train_snli_ve - loss is tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2586/16548 [1:11:33<6:29:22,  1.67s/it]11/15/2022 18:17:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.3496e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:54 - INFO - train.train_snli_ve - loss is tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2587/16548 [1:11:34<6:28:43,  1.67s/it]11/15/2022 18:17:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.9027e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:55 - INFO - train.train_snli_ve - loss is tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2588/16548 [1:11:36<6:28:36,  1.67s/it]11/15/2022 18:17:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.4903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:57 - INFO - train.train_snli_ve - loss is tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2589/16548 [1:11:38<6:27:11,  1.66s/it]11/15/2022 18:17:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.8953e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:17:59 - INFO - train.train_snli_ve - loss is tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2590/16548 [1:11:39<6:26:51,  1.66s/it]11/15/2022 18:18:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.6266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:00 - INFO - train.train_snli_ve - loss is tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2591/16548 [1:11:41<6:28:41,  1.67s/it]11/15/2022 18:18:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.2235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:02 - INFO - train.train_snli_ve - loss is tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2592/16548 [1:11:43<6:30:35,  1.68s/it]11/15/2022 18:18:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.5292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:04 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2593/16548 [1:11:44<6:28:47,  1.67s/it]11/15/2022 18:18:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.9202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:05 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2594/16548 [1:11:46<6:28:03,  1.67s/it]11/15/2022 18:18:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.0109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:07 - INFO - train.train_snli_ve - loss is tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2595/16548 [1:11:48<6:26:14,  1.66s/it]11/15/2022 18:18:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.2176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:09 - INFO - train.train_snli_ve - loss is tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2596/16548 [1:11:49<6:24:54,  1.66s/it]11/15/2022 18:18:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.1189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:10 - INFO - train.train_snli_ve - loss is tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2597/16548 [1:11:51<6:28:01,  1.67s/it]11/15/2022 18:18:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:12 - INFO - train.train_snli_ve - loss is tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2598/16548 [1:11:53<6:29:00,  1.67s/it]11/15/2022 18:18:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.6582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:14 - INFO - train.train_snli_ve - loss is tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2599/16548 [1:11:54<6:28:47,  1.67s/it]11/15/2022 18:18:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.3118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:15 - INFO - train.train_snli_ve - loss is tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2600/16548 [1:11:56<6:33:34,  1.69s/it]11/15/2022 18:18:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:17 - INFO - train.train_snli_ve - loss is tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2601/16548 [1:11:58<6:32:04,  1.69s/it]11/15/2022 18:18:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0449e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:19 - INFO - train.train_snli_ve - loss is tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2602/16548 [1:12:00<6:30:48,  1.68s/it]11/15/2022 18:18:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.7578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:20 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2603/16548 [1:12:01<6:29:00,  1.67s/it]11/15/2022 18:18:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:22 - INFO - train.train_snli_ve - loss is tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2604/16548 [1:12:03<6:28:27,  1.67s/it]11/15/2022 18:18:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:24 - INFO - train.train_snli_ve - loss is tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2605/16548 [1:12:05<6:28:38,  1.67s/it]11/15/2022 18:18:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.0822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:25 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2606/16548 [1:12:06<6:28:00,  1.67s/it]11/15/2022 18:18:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.7482e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:27 - INFO - train.train_snli_ve - loss is tensor(0.9360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2607/16548 [1:12:08<6:33:04,  1.69s/it]11/15/2022 18:18:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.0140e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:29 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2608/16548 [1:12:10<6:31:40,  1.69s/it]11/15/2022 18:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:31 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2609/16548 [1:12:11<6:29:40,  1.68s/it]11/15/2022 18:18:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:32 - INFO - train.train_snli_ve - loss is tensor(1.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2610/16548 [1:12:13<6:29:32,  1.68s/it]11/15/2022 18:18:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:34 - INFO - train.train_snli_ve - loss is tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2611/16548 [1:12:15<6:26:14,  1.66s/it]11/15/2022 18:18:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3276e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:35 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2612/16548 [1:12:16<6:25:08,  1.66s/it]11/15/2022 18:18:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:37 - INFO - train.train_snli_ve - loss is tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2613/16548 [1:12:18<6:22:44,  1.65s/it]11/15/2022 18:18:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:39 - INFO - train.train_snli_ve - loss is tensor(0.8725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2614/16548 [1:12:20<6:28:24,  1.67s/it]11/15/2022 18:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:40 - INFO - train.train_snli_ve - loss is tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2615/16548 [1:12:21<6:28:08,  1.67s/it]11/15/2022 18:18:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.9843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:42 - INFO - train.train_snli_ve - loss is tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2616/16548 [1:12:23<6:26:44,  1.67s/it]11/15/2022 18:18:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.3063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:44 - INFO - train.train_snli_ve - loss is tensor(0.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2617/16548 [1:12:25<6:29:18,  1.68s/it]11/15/2022 18:18:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.8107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:46 - INFO - train.train_snli_ve - loss is tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2618/16548 [1:12:26<6:30:25,  1.68s/it]11/15/2022 18:18:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:47 - INFO - train.train_snli_ve - loss is tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2619/16548 [1:12:28<6:32:08,  1.69s/it]11/15/2022 18:18:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5773e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:49 - INFO - train.train_snli_ve - loss is tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2620/16548 [1:12:30<6:32:33,  1.69s/it]11/15/2022 18:18:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.4049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:51 - INFO - train.train_snli_ve - loss is tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2621/16548 [1:12:31<6:30:22,  1.68s/it]11/15/2022 18:18:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.0824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:52 - INFO - train.train_snli_ve - loss is tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2622/16548 [1:12:33<6:30:46,  1.68s/it]11/15/2022 18:18:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:54 - INFO - train.train_snli_ve - loss is tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2623/16548 [1:12:35<6:27:59,  1.67s/it]11/15/2022 18:18:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.9704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:56 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2624/16548 [1:12:36<6:26:54,  1.67s/it]11/15/2022 18:18:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.3600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:57 - INFO - train.train_snli_ve - loss is tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2625/16548 [1:12:38<6:30:45,  1.68s/it]11/15/2022 18:18:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.3170e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:18:59 - INFO - train.train_snli_ve - loss is tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2626/16548 [1:12:40<6:35:10,  1.70s/it]11/15/2022 18:19:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.4408e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:01 - INFO - train.train_snli_ve - loss is tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2627/16548 [1:12:41<6:28:19,  1.67s/it]11/15/2022 18:19:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.4325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:02 - INFO - train.train_snli_ve - loss is tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2628/16548 [1:12:43<6:26:37,  1.67s/it]11/15/2022 18:19:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.8778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:04 - INFO - train.train_snli_ve - loss is tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2629/16548 [1:12:45<6:25:12,  1.66s/it]11/15/2022 18:19:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.3400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:06 - INFO - train.train_snli_ve - loss is tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2630/16548 [1:12:46<6:24:57,  1.66s/it]11/15/2022 18:19:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.1072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:07 - INFO - train.train_snli_ve - loss is tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2631/16548 [1:12:48<6:25:07,  1.66s/it]11/15/2022 18:19:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.5257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:09 - INFO - train.train_snli_ve - loss is tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2632/16548 [1:12:50<6:25:54,  1.66s/it]11/15/2022 18:19:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.5292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:11 - INFO - train.train_snli_ve - loss is tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2633/16548 [1:12:51<6:27:07,  1.67s/it]11/15/2022 18:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.1151e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:12 - INFO - train.train_snli_ve - loss is tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2634/16548 [1:12:53<6:31:38,  1.69s/it]11/15/2022 18:19:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.0777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:14 - INFO - train.train_snli_ve - loss is tensor(1.1495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2635/16548 [1:12:55<6:34:10,  1.70s/it]11/15/2022 18:19:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.5749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:16 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2636/16548 [1:12:57<6:34:19,  1.70s/it]11/15/2022 18:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:17 - INFO - train.train_snli_ve - loss is tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2637/16548 [1:12:58<6:30:29,  1.68s/it]11/15/2022 18:19:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.1628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:19 - INFO - train.train_snli_ve - loss is tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2638/16548 [1:13:00<6:27:41,  1.67s/it]11/15/2022 18:19:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.6121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:21 - INFO - train.train_snli_ve - loss is tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2639/16548 [1:13:02<6:27:53,  1.67s/it]11/15/2022 18:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.4589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:22 - INFO - train.train_snli_ve - loss is tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2640/16548 [1:13:03<6:27:19,  1.67s/it]11/15/2022 18:19:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.3460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:24 - INFO - train.train_snli_ve - loss is tensor(0.7835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2641/16548 [1:13:05<6:24:47,  1.66s/it]11/15/2022 18:19:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.0733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:26 - INFO - train.train_snli_ve - loss is tensor(0.8410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2642/16548 [1:13:06<6:25:54,  1.67s/it]11/15/2022 18:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.2575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:27 - INFO - train.train_snli_ve - loss is tensor(0.9432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2643/16548 [1:13:08<6:24:13,  1.66s/it]11/15/2022 18:19:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.1735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:29 - INFO - train.train_snli_ve - loss is tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2644/16548 [1:13:10<6:27:43,  1.67s/it]11/15/2022 18:19:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.6914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:31 - INFO - train.train_snli_ve - loss is tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2645/16548 [1:13:12<6:29:51,  1.68s/it]11/15/2022 18:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.3791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:33 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2646/16548 [1:13:13<6:31:32,  1.69s/it]11/15/2022 18:19:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:34 - INFO - train.train_snli_ve - loss is tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2647/16548 [1:13:15<6:29:10,  1.68s/it]11/15/2022 18:19:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.9708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:36 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2648/16548 [1:13:17<6:31:58,  1.69s/it]11/15/2022 18:19:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.6231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:38 - INFO - train.train_snli_ve - loss is tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2649/16548 [1:13:18<6:31:15,  1.69s/it]11/15/2022 18:19:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.9442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:39 - INFO - train.train_snli_ve - loss is tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2650/16548 [1:13:20<6:34:46,  1.70s/it]11/15/2022 18:19:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.4162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:41 - INFO - train.train_snli_ve - loss is tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2651/16548 [1:13:22<6:32:14,  1.69s/it]11/15/2022 18:19:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:43 - INFO - train.train_snli_ve - loss is tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2652/16548 [1:13:23<6:30:10,  1.68s/it]11/15/2022 18:19:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.8817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:44 - INFO - train.train_snli_ve - loss is tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2653/16548 [1:13:25<6:28:47,  1.68s/it]11/15/2022 18:19:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.3979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:46 - INFO - train.train_snli_ve - loss is tensor(0.9333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2654/16548 [1:13:27<6:27:53,  1.68s/it]11/15/2022 18:19:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.4283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:48 - INFO - train.train_snli_ve - loss is tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2655/16548 [1:13:28<6:28:08,  1.68s/it]11/15/2022 18:19:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.1628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:49 - INFO - train.train_snli_ve - loss is tensor(0.9079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2656/16548 [1:13:30<6:27:00,  1.67s/it]11/15/2022 18:19:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.4340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:51 - INFO - train.train_snli_ve - loss is tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2657/16548 [1:13:32<6:27:23,  1.67s/it]11/15/2022 18:19:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.4861e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:53 - INFO - train.train_snli_ve - loss is tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2658/16548 [1:13:33<6:30:06,  1.69s/it]11/15/2022 18:19:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.4005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:54 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2659/16548 [1:13:35<6:28:56,  1.68s/it]11/15/2022 18:19:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.8221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:56 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2660/16548 [1:13:37<6:29:59,  1.68s/it]11/15/2022 18:19:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0473e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:58 - INFO - train.train_snli_ve - loss is tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2661/16548 [1:13:38<6:29:18,  1.68s/it]11/15/2022 18:19:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.5720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:19:59 - INFO - train.train_snli_ve - loss is tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2662/16548 [1:13:40<6:28:05,  1.68s/it]11/15/2022 18:20:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:01 - INFO - train.train_snli_ve - loss is tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2663/16548 [1:13:42<6:30:09,  1.69s/it]11/15/2022 18:20:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.2345e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:03 - INFO - train.train_snli_ve - loss is tensor(0.7589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2664/16548 [1:13:44<6:32:45,  1.70s/it]11/15/2022 18:20:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.9057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:05 - INFO - train.train_snli_ve - loss is tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2665/16548 [1:13:45<6:32:17,  1.70s/it]11/15/2022 18:20:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.4028e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:06 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2666/16548 [1:13:47<6:31:07,  1.69s/it]11/15/2022 18:20:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.6848e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:08 - INFO - train.train_snli_ve - loss is tensor(0.8555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2667/16548 [1:13:49<6:29:30,  1.68s/it]11/15/2022 18:20:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.6452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:10 - INFO - train.train_snli_ve - loss is tensor(0.4635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2668/16548 [1:13:50<6:28:03,  1.68s/it]11/15/2022 18:20:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.3312e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:11 - INFO - train.train_snli_ve - loss is tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2669/16548 [1:13:52<6:28:45,  1.68s/it]11/15/2022 18:20:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.8252e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:13 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2670/16548 [1:13:54<6:25:24,  1.67s/it]11/15/2022 18:20:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.5126e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:15 - INFO - train.train_snli_ve - loss is tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2671/16548 [1:13:55<6:24:30,  1.66s/it]11/15/2022 18:20:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.9424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:16 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2672/16548 [1:13:57<6:25:07,  1.67s/it]11/15/2022 18:20:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.6018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:18 - INFO - train.train_snli_ve - loss is tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2673/16548 [1:13:59<6:26:04,  1.67s/it]11/15/2022 18:20:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.9089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:20 - INFO - train.train_snli_ve - loss is tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2674/16548 [1:14:00<6:27:58,  1.68s/it]11/15/2022 18:20:21 - INFO - train.train_snli_ve - kd_loss is tensor(9.7719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:21 - INFO - train.train_snli_ve - loss is tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2675/16548 [1:14:02<6:27:57,  1.68s/it]11/15/2022 18:20:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:23 - INFO - train.train_snli_ve - loss is tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2676/16548 [1:14:04<6:26:10,  1.67s/it]11/15/2022 18:20:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:25 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2677/16548 [1:14:05<6:27:30,  1.68s/it]11/15/2022 18:20:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.4224e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:26 - INFO - train.train_snli_ve - loss is tensor(0.9024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2678/16548 [1:14:07<6:26:41,  1.67s/it]11/15/2022 18:20:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.7597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:28 - INFO - train.train_snli_ve - loss is tensor(0.8009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2679/16548 [1:14:09<6:26:27,  1.67s/it]11/15/2022 18:20:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.8245e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:30 - INFO - train.train_snli_ve - loss is tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2680/16548 [1:14:10<6:28:26,  1.68s/it]11/15/2022 18:20:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.2066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:31 - INFO - train.train_snli_ve - loss is tensor(0.8560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2681/16548 [1:14:12<6:28:08,  1.68s/it]11/15/2022 18:20:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.6348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:33 - INFO - train.train_snli_ve - loss is tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2682/16548 [1:14:14<6:27:06,  1.68s/it]11/15/2022 18:20:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.1506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:35 - INFO - train.train_snli_ve - loss is tensor(0.8706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2683/16548 [1:14:15<6:24:32,  1.66s/it]11/15/2022 18:20:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:36 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2684/16548 [1:14:17<6:22:37,  1.66s/it]11/15/2022 18:20:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.8576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:38 - INFO - train.train_snli_ve - loss is tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2685/16548 [1:14:19<6:25:00,  1.67s/it]11/15/2022 18:20:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:40 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2686/16548 [1:14:20<6:29:03,  1.68s/it]11/15/2022 18:20:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.8953e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:41 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2687/16548 [1:14:22<6:29:05,  1.68s/it]11/15/2022 18:20:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:43 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2688/16548 [1:14:24<6:26:44,  1.67s/it]11/15/2022 18:20:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.8871e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:45 - INFO - train.train_snli_ve - loss is tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2689/16548 [1:14:25<6:31:25,  1.69s/it]11/15/2022 18:20:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:46 - INFO - train.train_snli_ve - loss is tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2690/16548 [1:14:27<6:28:27,  1.68s/it]11/15/2022 18:20:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.7281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:48 - INFO - train.train_snli_ve - loss is tensor(0.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2691/16548 [1:14:29<6:26:01,  1.67s/it]11/15/2022 18:20:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.3460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:50 - INFO - train.train_snli_ve - loss is tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2692/16548 [1:14:30<6:28:16,  1.68s/it]11/15/2022 18:20:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:51 - INFO - train.train_snli_ve - loss is tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2693/16548 [1:14:32<6:29:10,  1.69s/it]11/15/2022 18:20:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.9129e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:53 - INFO - train.train_snli_ve - loss is tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2694/16548 [1:14:34<6:31:13,  1.69s/it]11/15/2022 18:20:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.5183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:55 - INFO - train.train_snli_ve - loss is tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2695/16548 [1:14:36<6:35:04,  1.71s/it]11/15/2022 18:20:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.7213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:57 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2696/16548 [1:14:37<6:34:16,  1.71s/it]11/15/2022 18:20:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.1115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:20:58 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2697/16548 [1:14:39<6:31:47,  1.70s/it]11/15/2022 18:21:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.7371e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:00 - INFO - train.train_snli_ve - loss is tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2698/16548 [1:14:41<6:31:12,  1.69s/it]11/15/2022 18:21:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.8270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:02 - INFO - train.train_snli_ve - loss is tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2699/16548 [1:14:42<6:29:16,  1.69s/it]11/15/2022 18:21:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.3313e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:03 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2700/16548 [1:14:44<6:35:31,  1.71s/it]11/15/2022 18:21:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.8056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:05 - INFO - train.train_snli_ve - loss is tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2701/16548 [1:14:46<6:32:17,  1.70s/it]11/15/2022 18:21:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.6145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:07 - INFO - train.train_snli_ve - loss is tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2702/16548 [1:14:48<6:33:10,  1.70s/it]11/15/2022 18:21:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.8900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:08 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2703/16548 [1:14:49<6:33:26,  1.71s/it]11/15/2022 18:21:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.0964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:10 - INFO - train.train_snli_ve - loss is tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2704/16548 [1:14:51<6:32:49,  1.70s/it]11/15/2022 18:21:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.6533e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:12 - INFO - train.train_snli_ve - loss is tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2705/16548 [1:14:53<6:30:42,  1.69s/it]11/15/2022 18:21:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.1570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:13 - INFO - train.train_snli_ve - loss is tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2706/16548 [1:14:54<6:26:51,  1.68s/it]11/15/2022 18:21:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.2114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:15 - INFO - train.train_snli_ve - loss is tensor(0.8768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2707/16548 [1:14:56<6:25:26,  1.67s/it]11/15/2022 18:21:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.4051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:17 - INFO - train.train_snli_ve - loss is tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2708/16548 [1:14:58<6:26:50,  1.68s/it]11/15/2022 18:21:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0915e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:18 - INFO - train.train_snli_ve - loss is tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2709/16548 [1:14:59<6:25:07,  1.67s/it]11/15/2022 18:21:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.7426e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:20 - INFO - train.train_snli_ve - loss is tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2710/16548 [1:15:01<6:26:43,  1.68s/it]11/15/2022 18:21:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:22 - INFO - train.train_snli_ve - loss is tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2711/16548 [1:15:03<6:24:47,  1.67s/it]11/15/2022 18:21:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.1936e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:24 - INFO - train.train_snli_ve - loss is tensor(0.8410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2712/16548 [1:15:04<6:27:52,  1.68s/it]11/15/2022 18:21:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.4775e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:25 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2713/16548 [1:15:06<6:25:40,  1.67s/it]11/15/2022 18:21:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.0325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:27 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2714/16548 [1:15:08<6:25:42,  1.67s/it]11/15/2022 18:21:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.4234e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:29 - INFO - train.train_snli_ve - loss is tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2715/16548 [1:15:09<6:26:10,  1.68s/it]11/15/2022 18:21:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.1931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:30 - INFO - train.train_snli_ve - loss is tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2716/16548 [1:15:11<6:27:03,  1.68s/it]11/15/2022 18:21:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.5359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:32 - INFO - train.train_snli_ve - loss is tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2717/16548 [1:15:13<6:27:26,  1.68s/it]11/15/2022 18:21:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.8145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:34 - INFO - train.train_snli_ve - loss is tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2718/16548 [1:15:14<6:29:29,  1.69s/it]11/15/2022 18:21:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.0762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:35 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2719/16548 [1:15:16<6:27:23,  1.68s/it]11/15/2022 18:21:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.2724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:37 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2720/16548 [1:15:18<6:25:09,  1.67s/it]11/15/2022 18:21:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2287e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:39 - INFO - train.train_snli_ve - loss is tensor(0.8707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2721/16548 [1:15:19<6:24:22,  1.67s/it]11/15/2022 18:21:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.4145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:40 - INFO - train.train_snli_ve - loss is tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2722/16548 [1:15:21<6:28:07,  1.68s/it]11/15/2022 18:21:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.8890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:42 - INFO - train.train_snli_ve - loss is tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2723/16548 [1:15:23<6:26:28,  1.68s/it]11/15/2022 18:21:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.7629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:44 - INFO - train.train_snli_ve - loss is tensor(0.4990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2724/16548 [1:15:24<6:25:18,  1.67s/it]11/15/2022 18:21:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.9978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:45 - INFO - train.train_snli_ve - loss is tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2725/16548 [1:15:26<6:26:49,  1.68s/it]11/15/2022 18:21:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.9647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:47 - INFO - train.train_snli_ve - loss is tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2726/16548 [1:15:28<6:30:12,  1.69s/it]11/15/2022 18:21:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.4112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:49 - INFO - train.train_snli_ve - loss is tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2727/16548 [1:15:29<6:28:12,  1.69s/it]11/15/2022 18:21:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.2756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:50 - INFO - train.train_snli_ve - loss is tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2728/16548 [1:15:31<6:27:04,  1.68s/it]11/15/2022 18:21:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.9091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:52 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2729/16548 [1:15:33<6:28:40,  1.69s/it]11/15/2022 18:21:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.8271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:54 - INFO - train.train_snli_ve - loss is tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16% 2730/16548 [1:15:35<6:28:29,  1.69s/it]11/15/2022 18:21:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.0307e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:55 - INFO - train.train_snli_ve - loss is tensor(0.7771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2731/16548 [1:15:36<6:27:08,  1.68s/it]11/15/2022 18:21:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.7893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:57 - INFO - train.train_snli_ve - loss is tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2732/16548 [1:15:38<6:29:45,  1.69s/it]11/15/2022 18:21:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.8796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:21:59 - INFO - train.train_snli_ve - loss is tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2733/16548 [1:15:40<6:28:03,  1.69s/it]11/15/2022 18:22:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.5480e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:01 - INFO - train.train_snli_ve - loss is tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2734/16548 [1:15:41<6:27:29,  1.68s/it]11/15/2022 18:22:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.7024e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:02 - INFO - train.train_snli_ve - loss is tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2735/16548 [1:15:43<6:28:13,  1.69s/it]11/15/2022 18:22:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.5221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:04 - INFO - train.train_snli_ve - loss is tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2736/16548 [1:15:45<6:28:03,  1.69s/it]11/15/2022 18:22:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.5386e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:06 - INFO - train.train_snli_ve - loss is tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2737/16548 [1:15:46<6:29:52,  1.69s/it]11/15/2022 18:22:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.4053e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:07 - INFO - train.train_snli_ve - loss is tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2738/16548 [1:15:48<6:31:56,  1.70s/it]11/15/2022 18:22:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:09 - INFO - train.train_snli_ve - loss is tensor(0.8338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2739/16548 [1:15:50<6:28:58,  1.69s/it]11/15/2022 18:22:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.6497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:11 - INFO - train.train_snli_ve - loss is tensor(0.6020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2740/16548 [1:15:51<6:27:52,  1.69s/it]11/15/2022 18:22:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:12 - INFO - train.train_snli_ve - loss is tensor(1.0593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2741/16548 [1:15:53<6:24:57,  1.67s/it]11/15/2022 18:22:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1889e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:14 - INFO - train.train_snli_ve - loss is tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2742/16548 [1:15:55<6:25:44,  1.68s/it]11/15/2022 18:22:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.0894e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:16 - INFO - train.train_snli_ve - loss is tensor(0.8606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2743/16548 [1:15:56<6:22:39,  1.66s/it]11/15/2022 18:22:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.2939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:17 - INFO - train.train_snli_ve - loss is tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2744/16548 [1:15:58<6:22:49,  1.66s/it]11/15/2022 18:22:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.6528e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:19 - INFO - train.train_snli_ve - loss is tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2745/16548 [1:16:00<6:23:10,  1.67s/it]11/15/2022 18:22:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.2165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:21 - INFO - train.train_snli_ve - loss is tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2746/16548 [1:16:01<6:23:05,  1.67s/it]11/15/2022 18:22:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.5380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:22 - INFO - train.train_snli_ve - loss is tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2747/16548 [1:16:03<6:26:54,  1.68s/it]11/15/2022 18:22:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.0310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:24 - INFO - train.train_snli_ve - loss is tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2748/16548 [1:16:05<6:27:55,  1.69s/it]11/15/2022 18:22:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.2618e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:26 - INFO - train.train_snli_ve - loss is tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2749/16548 [1:16:06<6:23:24,  1.67s/it]11/15/2022 18:22:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.8664e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:27 - INFO - train.train_snli_ve - loss is tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2750/16548 [1:16:08<6:30:20,  1.70s/it]11/15/2022 18:22:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.6052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:29 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2751/16548 [1:16:10<6:26:36,  1.68s/it]11/15/2022 18:22:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.2161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:31 - INFO - train.train_snli_ve - loss is tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2752/16548 [1:16:11<6:24:13,  1.67s/it]11/15/2022 18:22:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.6254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:32 - INFO - train.train_snli_ve - loss is tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2753/16548 [1:16:13<6:21:11,  1.66s/it]11/15/2022 18:22:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.7901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:34 - INFO - train.train_snli_ve - loss is tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2754/16548 [1:16:15<6:20:21,  1.65s/it]11/15/2022 18:22:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.8150e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:36 - INFO - train.train_snli_ve - loss is tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2755/16548 [1:16:16<6:18:26,  1.65s/it]11/15/2022 18:22:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.4729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:37 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2756/16548 [1:16:18<6:19:54,  1.65s/it]11/15/2022 18:22:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.2509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:39 - INFO - train.train_snli_ve - loss is tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2757/16548 [1:16:20<6:20:01,  1.65s/it]11/15/2022 18:22:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.7038e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:41 - INFO - train.train_snli_ve - loss is tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2758/16548 [1:16:21<6:20:29,  1.66s/it]11/15/2022 18:22:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.2297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:42 - INFO - train.train_snli_ve - loss is tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2759/16548 [1:16:23<6:17:25,  1.64s/it]11/15/2022 18:22:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.5509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:44 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2760/16548 [1:16:25<6:20:54,  1.66s/it]11/15/2022 18:22:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.1654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:46 - INFO - train.train_snli_ve - loss is tensor(1.0702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2761/16548 [1:16:26<6:22:17,  1.66s/it]11/15/2022 18:22:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.3612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:47 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2762/16548 [1:16:28<6:23:55,  1.67s/it]11/15/2022 18:22:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.7352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:49 - INFO - train.train_snli_ve - loss is tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2763/16548 [1:16:30<6:25:17,  1.68s/it]11/15/2022 18:22:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.3507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:51 - INFO - train.train_snli_ve - loss is tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2764/16548 [1:16:31<6:26:54,  1.68s/it]11/15/2022 18:22:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.9125e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:52 - INFO - train.train_snli_ve - loss is tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2765/16548 [1:16:33<6:24:38,  1.67s/it]11/15/2022 18:22:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.8465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:54 - INFO - train.train_snli_ve - loss is tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2766/16548 [1:16:35<6:23:44,  1.67s/it]11/15/2022 18:22:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.0717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:56 - INFO - train.train_snli_ve - loss is tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2767/16548 [1:16:36<6:25:28,  1.68s/it]11/15/2022 18:22:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4242e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:57 - INFO - train.train_snli_ve - loss is tensor(0.7305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2768/16548 [1:16:38<6:21:42,  1.66s/it]11/15/2022 18:22:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.3867e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:22:59 - INFO - train.train_snli_ve - loss is tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2769/16548 [1:16:40<6:23:10,  1.67s/it]11/15/2022 18:23:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.5441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:01 - INFO - train.train_snli_ve - loss is tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2770/16548 [1:16:41<6:22:07,  1.66s/it]11/15/2022 18:23:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.8315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:02 - INFO - train.train_snli_ve - loss is tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2771/16548 [1:16:43<6:20:17,  1.66s/it]11/15/2022 18:23:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.3502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:04 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2772/16548 [1:16:45<6:21:10,  1.66s/it]11/15/2022 18:23:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.8436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:06 - INFO - train.train_snli_ve - loss is tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2773/16548 [1:16:46<6:21:57,  1.66s/it]11/15/2022 18:23:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.8027e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:07 - INFO - train.train_snli_ve - loss is tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2774/16548 [1:16:48<6:24:38,  1.68s/it]11/15/2022 18:23:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.9354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:09 - INFO - train.train_snli_ve - loss is tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2775/16548 [1:16:50<6:24:46,  1.68s/it]11/15/2022 18:23:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.8750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:11 - INFO - train.train_snli_ve - loss is tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2776/16548 [1:16:51<6:26:48,  1.69s/it]11/15/2022 18:23:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.8913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:12 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2777/16548 [1:16:53<6:25:44,  1.68s/it]11/15/2022 18:23:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.5756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:14 - INFO - train.train_snli_ve - loss is tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2778/16548 [1:16:55<6:29:00,  1.70s/it]11/15/2022 18:23:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.1996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:16 - INFO - train.train_snli_ve - loss is tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2779/16548 [1:16:57<6:27:02,  1.69s/it]11/15/2022 18:23:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.9864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:17 - INFO - train.train_snli_ve - loss is tensor(0.8734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2780/16548 [1:16:58<6:26:25,  1.68s/it]11/15/2022 18:23:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.2712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:19 - INFO - train.train_snli_ve - loss is tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2781/16548 [1:17:00<6:24:34,  1.68s/it]11/15/2022 18:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.1285e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:21 - INFO - train.train_snli_ve - loss is tensor(0.9132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2782/16548 [1:17:02<6:23:42,  1.67s/it]11/15/2022 18:23:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.0902e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:22 - INFO - train.train_snli_ve - loss is tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2783/16548 [1:17:03<6:21:41,  1.66s/it]11/15/2022 18:23:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.7611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:24 - INFO - train.train_snli_ve - loss is tensor(0.8473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2784/16548 [1:17:05<6:19:48,  1.66s/it]11/15/2022 18:23:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.3626e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:26 - INFO - train.train_snli_ve - loss is tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2785/16548 [1:17:06<6:22:23,  1.67s/it]11/15/2022 18:23:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.7155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:27 - INFO - train.train_snli_ve - loss is tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2786/16548 [1:17:08<6:22:58,  1.67s/it]11/15/2022 18:23:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.8290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:29 - INFO - train.train_snli_ve - loss is tensor(0.8991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2787/16548 [1:17:10<6:25:05,  1.68s/it]11/15/2022 18:23:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.7243e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:31 - INFO - train.train_snli_ve - loss is tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2788/16548 [1:17:12<6:24:02,  1.67s/it]11/15/2022 18:23:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.1359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:32 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2789/16548 [1:17:13<6:23:26,  1.67s/it]11/15/2022 18:23:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.3980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:34 - INFO - train.train_snli_ve - loss is tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2790/16548 [1:17:15<6:24:17,  1.68s/it]11/15/2022 18:23:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.3865e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:36 - INFO - train.train_snli_ve - loss is tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2791/16548 [1:17:17<6:22:52,  1.67s/it]11/15/2022 18:23:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.2810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:37 - INFO - train.train_snli_ve - loss is tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2792/16548 [1:17:18<6:22:07,  1.67s/it]11/15/2022 18:23:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.6141e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:39 - INFO - train.train_snli_ve - loss is tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2793/16548 [1:17:20<6:23:02,  1.67s/it]11/15/2022 18:23:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.0725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:41 - INFO - train.train_snli_ve - loss is tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2794/16548 [1:17:22<6:22:27,  1.67s/it]11/15/2022 18:23:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.2550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:42 - INFO - train.train_snli_ve - loss is tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2795/16548 [1:17:23<6:19:24,  1.66s/it]11/15/2022 18:23:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.1023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:44 - INFO - train.train_snli_ve - loss is tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2796/16548 [1:17:25<6:18:56,  1.65s/it]11/15/2022 18:23:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.2300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:46 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2797/16548 [1:17:26<6:20:09,  1.66s/it]11/15/2022 18:23:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.0598e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:47 - INFO - train.train_snli_ve - loss is tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2798/16548 [1:17:28<6:21:50,  1.67s/it]11/15/2022 18:23:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.9255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:49 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2799/16548 [1:17:30<6:21:51,  1.67s/it]11/15/2022 18:23:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.7298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:51 - INFO - train.train_snli_ve - loss is tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2800/16548 [1:17:32<6:27:34,  1.69s/it]11/15/2022 18:23:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.4632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:53 - INFO - train.train_snli_ve - loss is tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2801/16548 [1:17:33<6:25:29,  1.68s/it]11/15/2022 18:23:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.5717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:54 - INFO - train.train_snli_ve - loss is tensor(0.9291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2802/16548 [1:17:35<6:23:37,  1.67s/it]11/15/2022 18:23:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.6717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:56 - INFO - train.train_snli_ve - loss is tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2803/16548 [1:17:37<6:22:58,  1.67s/it]11/15/2022 18:23:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.3352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:57 - INFO - train.train_snli_ve - loss is tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2804/16548 [1:17:38<6:21:36,  1.67s/it]11/15/2022 18:23:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.0532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:23:59 - INFO - train.train_snli_ve - loss is tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2805/16548 [1:17:40<6:21:28,  1.67s/it]11/15/2022 18:24:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.1660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:01 - INFO - train.train_snli_ve - loss is tensor(0.8794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2806/16548 [1:17:42<6:22:16,  1.67s/it]11/15/2022 18:24:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.6448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:03 - INFO - train.train_snli_ve - loss is tensor(0.5801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2807/16548 [1:17:43<6:23:16,  1.67s/it]11/15/2022 18:24:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.5449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:04 - INFO - train.train_snli_ve - loss is tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2808/16548 [1:17:45<6:23:57,  1.68s/it]11/15/2022 18:24:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.5880e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:06 - INFO - train.train_snli_ve - loss is tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2809/16548 [1:17:47<6:24:39,  1.68s/it]11/15/2022 18:24:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.9146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:08 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2810/16548 [1:17:48<6:23:26,  1.67s/it]11/15/2022 18:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.1581e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:09 - INFO - train.train_snli_ve - loss is tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2811/16548 [1:17:50<6:22:32,  1.67s/it]11/15/2022 18:24:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.7776e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:11 - INFO - train.train_snli_ve - loss is tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2812/16548 [1:17:52<6:24:41,  1.68s/it]11/15/2022 18:24:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.3709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:13 - INFO - train.train_snli_ve - loss is tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2813/16548 [1:17:53<6:27:15,  1.69s/it]11/15/2022 18:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.0843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:14 - INFO - train.train_snli_ve - loss is tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2814/16548 [1:17:55<6:26:17,  1.69s/it]11/15/2022 18:24:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.1210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:16 - INFO - train.train_snli_ve - loss is tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2815/16548 [1:17:57<6:25:40,  1.69s/it]11/15/2022 18:24:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.6607e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:18 - INFO - train.train_snli_ve - loss is tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2816/16548 [1:17:58<6:23:27,  1.68s/it]11/15/2022 18:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.6319e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:19 - INFO - train.train_snli_ve - loss is tensor(0.7629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2817/16548 [1:18:00<6:24:33,  1.68s/it]11/15/2022 18:24:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.9897e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:21 - INFO - train.train_snli_ve - loss is tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2818/16548 [1:18:02<6:25:06,  1.68s/it]11/15/2022 18:24:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.0685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:23 - INFO - train.train_snli_ve - loss is tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2819/16548 [1:18:03<6:27:10,  1.69s/it]11/15/2022 18:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.0356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:24 - INFO - train.train_snli_ve - loss is tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2820/16548 [1:18:05<6:30:53,  1.71s/it]11/15/2022 18:24:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.8298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:26 - INFO - train.train_snli_ve - loss is tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2821/16548 [1:18:07<6:30:45,  1.71s/it]11/15/2022 18:24:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0299e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:28 - INFO - train.train_snli_ve - loss is tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2822/16548 [1:18:09<6:30:30,  1.71s/it]11/15/2022 18:24:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.4514e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:30 - INFO - train.train_snli_ve - loss is tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2823/16548 [1:18:10<6:25:36,  1.69s/it]11/15/2022 18:24:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.0996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:31 - INFO - train.train_snli_ve - loss is tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2824/16548 [1:18:12<6:27:25,  1.69s/it]11/15/2022 18:24:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.1109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:33 - INFO - train.train_snli_ve - loss is tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2825/16548 [1:18:14<6:26:06,  1.69s/it]11/15/2022 18:24:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.9883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:35 - INFO - train.train_snli_ve - loss is tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2826/16548 [1:18:15<6:25:10,  1.68s/it]11/15/2022 18:24:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.8625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:36 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2827/16548 [1:18:17<6:26:45,  1.69s/it]11/15/2022 18:24:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.4230e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:38 - INFO - train.train_snli_ve - loss is tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2828/16548 [1:18:19<6:27:53,  1.70s/it]11/15/2022 18:24:40 - INFO - train.train_snli_ve - kd_loss is tensor(9.6254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:40 - INFO - train.train_snli_ve - loss is tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2829/16548 [1:18:20<6:25:59,  1.69s/it]11/15/2022 18:24:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.8941e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:41 - INFO - train.train_snli_ve - loss is tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2830/16548 [1:18:22<6:23:15,  1.68s/it]11/15/2022 18:24:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.2014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:43 - INFO - train.train_snli_ve - loss is tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2831/16548 [1:18:24<6:23:02,  1.68s/it]11/15/2022 18:24:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.9479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:45 - INFO - train.train_snli_ve - loss is tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2832/16548 [1:18:25<6:22:09,  1.67s/it]11/15/2022 18:24:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.4838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:46 - INFO - train.train_snli_ve - loss is tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2833/16548 [1:18:27<6:21:54,  1.67s/it]11/15/2022 18:24:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.7735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:48 - INFO - train.train_snli_ve - loss is tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2834/16548 [1:18:29<6:20:28,  1.66s/it]11/15/2022 18:24:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.9787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:50 - INFO - train.train_snli_ve - loss is tensor(1.0933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2835/16548 [1:18:30<6:18:40,  1.66s/it]11/15/2022 18:24:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.4511e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:51 - INFO - train.train_snli_ve - loss is tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2836/16548 [1:18:32<6:18:18,  1.66s/it]11/15/2022 18:24:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.7246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:53 - INFO - train.train_snli_ve - loss is tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2837/16548 [1:18:34<6:21:37,  1.67s/it]11/15/2022 18:24:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.6978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:55 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2838/16548 [1:18:35<6:22:24,  1.67s/it]11/15/2022 18:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.1840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:56 - INFO - train.train_snli_ve - loss is tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2839/16548 [1:18:37<6:20:26,  1.67s/it]11/15/2022 18:24:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.6786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:24:58 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2840/16548 [1:18:39<6:21:50,  1.67s/it]11/15/2022 18:25:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.8647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:00 - INFO - train.train_snli_ve - loss is tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2841/16548 [1:18:40<6:26:01,  1.69s/it]11/15/2022 18:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.7073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:01 - INFO - train.train_snli_ve - loss is tensor(0.5228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2842/16548 [1:18:42<6:24:34,  1.68s/it]11/15/2022 18:25:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0897e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:03 - INFO - train.train_snli_ve - loss is tensor(0.9978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2843/16548 [1:18:44<6:21:55,  1.67s/it]11/15/2022 18:25:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.1881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:05 - INFO - train.train_snli_ve - loss is tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2844/16548 [1:18:45<6:21:47,  1.67s/it]11/15/2022 18:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.6840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:06 - INFO - train.train_snli_ve - loss is tensor(0.8018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2845/16548 [1:18:47<6:19:17,  1.66s/it]11/15/2022 18:25:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.4638e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:08 - INFO - train.train_snli_ve - loss is tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2846/16548 [1:18:49<6:21:40,  1.67s/it]11/15/2022 18:25:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.8856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:10 - INFO - train.train_snli_ve - loss is tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2847/16548 [1:18:50<6:23:38,  1.68s/it]11/15/2022 18:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.6568e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:11 - INFO - train.train_snli_ve - loss is tensor(1.1336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2848/16548 [1:18:52<6:23:01,  1.68s/it]11/15/2022 18:25:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.8882e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:13 - INFO - train.train_snli_ve - loss is tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2849/16548 [1:18:54<6:24:40,  1.68s/it]11/15/2022 18:25:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.4054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:15 - INFO - train.train_snli_ve - loss is tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2850/16548 [1:18:56<6:23:27,  1.68s/it]11/15/2022 18:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.4605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:16 - INFO - train.train_snli_ve - loss is tensor(0.8704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2851/16548 [1:18:57<6:20:41,  1.67s/it]11/15/2022 18:25:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.7714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:18 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2852/16548 [1:18:59<6:21:46,  1.67s/it]11/15/2022 18:25:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.7094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:20 - INFO - train.train_snli_ve - loss is tensor(0.8106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2853/16548 [1:19:01<6:21:21,  1.67s/it]11/15/2022 18:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.7250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:21 - INFO - train.train_snli_ve - loss is tensor(0.9847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2854/16548 [1:19:02<6:19:07,  1.66s/it]11/15/2022 18:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.7542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:23 - INFO - train.train_snli_ve - loss is tensor(0.8868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2855/16548 [1:19:04<6:20:58,  1.67s/it]11/15/2022 18:25:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.6803e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:25 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2856/16548 [1:19:06<6:21:04,  1.67s/it]11/15/2022 18:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.8483e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:26 - INFO - train.train_snli_ve - loss is tensor(0.7967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2857/16548 [1:19:07<6:21:44,  1.67s/it]11/15/2022 18:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.0911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:28 - INFO - train.train_snli_ve - loss is tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2858/16548 [1:19:09<6:26:03,  1.69s/it]11/15/2022 18:25:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.2727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:30 - INFO - train.train_snli_ve - loss is tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2859/16548 [1:19:11<6:25:36,  1.69s/it]11/15/2022 18:25:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.3130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:32 - INFO - train.train_snli_ve - loss is tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2860/16548 [1:19:12<6:28:02,  1.70s/it]11/15/2022 18:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.6201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:33 - INFO - train.train_snli_ve - loss is tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2861/16548 [1:19:14<6:29:08,  1.71s/it]11/15/2022 18:25:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.6073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:35 - INFO - train.train_snli_ve - loss is tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2862/16548 [1:19:16<6:34:26,  1.73s/it]11/15/2022 18:25:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.6238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:37 - INFO - train.train_snli_ve - loss is tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2863/16548 [1:19:18<6:30:54,  1.71s/it]11/15/2022 18:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.2875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:38 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2864/16548 [1:19:19<6:31:30,  1.72s/it]11/15/2022 18:25:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:40 - INFO - train.train_snli_ve - loss is tensor(0.7685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2865/16548 [1:19:21<6:28:57,  1.71s/it]11/15/2022 18:25:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.1432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:42 - INFO - train.train_snli_ve - loss is tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2866/16548 [1:19:23<6:28:10,  1.70s/it]11/15/2022 18:25:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.6405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:44 - INFO - train.train_snli_ve - loss is tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2867/16548 [1:19:24<6:27:49,  1.70s/it]11/15/2022 18:25:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.7857e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:45 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2868/16548 [1:19:26<6:29:14,  1.71s/it]11/15/2022 18:25:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.9929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:47 - INFO - train.train_snli_ve - loss is tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2869/16548 [1:19:28<6:25:35,  1.69s/it]11/15/2022 18:25:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.8277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:49 - INFO - train.train_snli_ve - loss is tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2870/16548 [1:19:29<6:24:22,  1.69s/it]11/15/2022 18:25:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.7145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:50 - INFO - train.train_snli_ve - loss is tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2871/16548 [1:19:31<6:26:04,  1.69s/it]11/15/2022 18:25:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.5124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:52 - INFO - train.train_snli_ve - loss is tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2872/16548 [1:19:33<6:24:21,  1.69s/it]11/15/2022 18:25:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.3712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:54 - INFO - train.train_snli_ve - loss is tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2873/16548 [1:19:34<6:22:52,  1.68s/it]11/15/2022 18:25:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.7323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:55 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2874/16548 [1:19:36<6:22:13,  1.68s/it]11/15/2022 18:25:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.0837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:57 - INFO - train.train_snli_ve - loss is tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2875/16548 [1:19:38<6:20:24,  1.67s/it]11/15/2022 18:25:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.6396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:25:59 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2876/16548 [1:19:39<6:23:16,  1.68s/it]11/15/2022 18:26:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:00 - INFO - train.train_snli_ve - loss is tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2877/16548 [1:19:41<6:22:38,  1.68s/it]11/15/2022 18:26:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.2652e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:02 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2878/16548 [1:19:43<6:26:03,  1.69s/it]11/15/2022 18:26:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.8701e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:04 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2879/16548 [1:19:45<6:26:52,  1.70s/it]11/15/2022 18:26:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.4251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:05 - INFO - train.train_snli_ve - loss is tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2880/16548 [1:19:46<6:25:47,  1.69s/it]11/15/2022 18:26:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.9270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:07 - INFO - train.train_snli_ve - loss is tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2881/16548 [1:19:48<6:25:46,  1.69s/it]11/15/2022 18:26:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.9675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:09 - INFO - train.train_snli_ve - loss is tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2882/16548 [1:19:50<6:23:31,  1.68s/it]11/15/2022 18:26:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.8580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:11 - INFO - train.train_snli_ve - loss is tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2883/16548 [1:19:51<6:21:38,  1.68s/it]11/15/2022 18:26:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:12 - INFO - train.train_snli_ve - loss is tensor(0.5147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2884/16548 [1:19:53<6:23:51,  1.69s/it]11/15/2022 18:26:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.0333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:14 - INFO - train.train_snli_ve - loss is tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2885/16548 [1:19:55<6:25:24,  1.69s/it]11/15/2022 18:26:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.9064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:16 - INFO - train.train_snli_ve - loss is tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2886/16548 [1:19:56<6:26:43,  1.70s/it]11/15/2022 18:26:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.5331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:17 - INFO - train.train_snli_ve - loss is tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2887/16548 [1:19:58<6:25:00,  1.69s/it]11/15/2022 18:26:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:19 - INFO - train.train_snli_ve - loss is tensor(0.7887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2888/16548 [1:20:00<6:24:07,  1.69s/it]11/15/2022 18:26:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.0829e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:21 - INFO - train.train_snli_ve - loss is tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2889/16548 [1:20:01<6:25:51,  1.69s/it]11/15/2022 18:26:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:22 - INFO - train.train_snli_ve - loss is tensor(0.9088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2890/16548 [1:20:03<6:28:14,  1.71s/it]11/15/2022 18:26:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.8202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:24 - INFO - train.train_snli_ve - loss is tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2891/16548 [1:20:05<6:26:32,  1.70s/it]11/15/2022 18:26:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:26 - INFO - train.train_snli_ve - loss is tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2892/16548 [1:20:07<6:27:40,  1.70s/it]11/15/2022 18:26:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.0814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:27 - INFO - train.train_snli_ve - loss is tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2893/16548 [1:20:08<6:24:24,  1.69s/it]11/15/2022 18:26:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.5864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:29 - INFO - train.train_snli_ve - loss is tensor(0.8214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2894/16548 [1:20:10<6:24:39,  1.69s/it]11/15/2022 18:26:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.4269e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:31 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17% 2895/16548 [1:20:12<6:23:47,  1.69s/it]11/15/2022 18:26:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.5662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:33 - INFO - train.train_snli_ve - loss is tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2896/16548 [1:20:13<6:23:26,  1.69s/it]11/15/2022 18:26:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.9925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:34 - INFO - train.train_snli_ve - loss is tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2897/16548 [1:20:15<6:22:43,  1.68s/it]11/15/2022 18:26:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.1882e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:36 - INFO - train.train_snli_ve - loss is tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2898/16548 [1:20:17<6:24:18,  1.69s/it]11/15/2022 18:26:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.4164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:38 - INFO - train.train_snli_ve - loss is tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2899/16548 [1:20:18<6:22:19,  1.68s/it]11/15/2022 18:26:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.8376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:39 - INFO - train.train_snli_ve - loss is tensor(0.5291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2900/16548 [1:20:20<6:23:04,  1.68s/it]11/15/2022 18:26:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.1121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:41 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2901/16548 [1:20:22<6:23:35,  1.69s/it]11/15/2022 18:26:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.4671e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:43 - INFO - train.train_snli_ve - loss is tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2902/16548 [1:20:23<6:24:01,  1.69s/it]11/15/2022 18:26:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.1810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:44 - INFO - train.train_snli_ve - loss is tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2903/16548 [1:20:25<6:21:40,  1.68s/it]11/15/2022 18:26:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.0120e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:46 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2904/16548 [1:20:27<6:18:39,  1.67s/it]11/15/2022 18:26:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.7382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:48 - INFO - train.train_snli_ve - loss is tensor(0.7827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2905/16548 [1:20:28<6:18:11,  1.66s/it]11/15/2022 18:26:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.7078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:49 - INFO - train.train_snli_ve - loss is tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2906/16548 [1:20:30<6:19:17,  1.67s/it]11/15/2022 18:26:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.4546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:51 - INFO - train.train_snli_ve - loss is tensor(0.8847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2907/16548 [1:20:32<6:18:42,  1.67s/it]11/15/2022 18:26:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.9708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:53 - INFO - train.train_snli_ve - loss is tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2908/16548 [1:20:33<6:21:15,  1.68s/it]11/15/2022 18:26:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.0832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:54 - INFO - train.train_snli_ve - loss is tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2909/16548 [1:20:35<6:18:05,  1.66s/it]11/15/2022 18:26:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.5274e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:56 - INFO - train.train_snli_ve - loss is tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2910/16548 [1:20:37<6:22:08,  1.68s/it]11/15/2022 18:26:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.8526e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:58 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2911/16548 [1:20:38<6:23:28,  1.69s/it]11/15/2022 18:26:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.9954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:26:59 - INFO - train.train_snli_ve - loss is tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2912/16548 [1:20:40<6:21:55,  1.68s/it]11/15/2022 18:27:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.4673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:01 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2913/16548 [1:20:42<6:23:45,  1.69s/it]11/15/2022 18:27:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.6140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:03 - INFO - train.train_snli_ve - loss is tensor(0.8062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2914/16548 [1:20:43<6:21:51,  1.68s/it]11/15/2022 18:27:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.9279e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:04 - INFO - train.train_snli_ve - loss is tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2915/16548 [1:20:45<6:19:49,  1.67s/it]11/15/2022 18:27:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.6103e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:06 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2916/16548 [1:20:47<6:21:55,  1.68s/it]11/15/2022 18:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.9081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:08 - INFO - train.train_snli_ve - loss is tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2917/16548 [1:20:48<6:19:55,  1.67s/it]11/15/2022 18:27:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.3106e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:09 - INFO - train.train_snli_ve - loss is tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2918/16548 [1:20:50<6:20:20,  1.67s/it]11/15/2022 18:27:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.8782e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:11 - INFO - train.train_snli_ve - loss is tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2919/16548 [1:20:52<6:25:15,  1.70s/it]11/15/2022 18:27:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.8319e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:13 - INFO - train.train_snli_ve - loss is tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2920/16548 [1:20:54<6:25:17,  1.70s/it]11/15/2022 18:27:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.4928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:15 - INFO - train.train_snli_ve - loss is tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2921/16548 [1:20:55<6:24:32,  1.69s/it]11/15/2022 18:27:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.4792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:16 - INFO - train.train_snli_ve - loss is tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2922/16548 [1:20:57<6:23:40,  1.69s/it]11/15/2022 18:27:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.1153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:18 - INFO - train.train_snli_ve - loss is tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2923/16548 [1:20:59<6:21:48,  1.68s/it]11/15/2022 18:27:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.4347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:20 - INFO - train.train_snli_ve - loss is tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2924/16548 [1:21:00<6:22:44,  1.69s/it]11/15/2022 18:27:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.1010e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:21 - INFO - train.train_snli_ve - loss is tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2925/16548 [1:21:02<6:22:15,  1.68s/it]11/15/2022 18:27:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.4870e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:23 - INFO - train.train_snli_ve - loss is tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2926/16548 [1:21:04<6:21:00,  1.68s/it]11/15/2022 18:27:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.8595e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:25 - INFO - train.train_snli_ve - loss is tensor(0.4712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2927/16548 [1:21:05<6:21:09,  1.68s/it]11/15/2022 18:27:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.3078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:26 - INFO - train.train_snli_ve - loss is tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2928/16548 [1:21:07<6:22:09,  1.68s/it]11/15/2022 18:27:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.5920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:28 - INFO - train.train_snli_ve - loss is tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2929/16548 [1:21:09<6:18:13,  1.67s/it]11/15/2022 18:27:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.9226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:30 - INFO - train.train_snli_ve - loss is tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2930/16548 [1:21:10<6:18:59,  1.67s/it]11/15/2022 18:27:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.2467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:31 - INFO - train.train_snli_ve - loss is tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2931/16548 [1:21:12<6:19:23,  1.67s/it]11/15/2022 18:27:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.4295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:33 - INFO - train.train_snli_ve - loss is tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2932/16548 [1:21:14<6:18:18,  1.67s/it]11/15/2022 18:27:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.3927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:35 - INFO - train.train_snli_ve - loss is tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2933/16548 [1:21:15<6:19:28,  1.67s/it]11/15/2022 18:27:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:36 - INFO - train.train_snli_ve - loss is tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2934/16548 [1:21:17<6:19:13,  1.67s/it]11/15/2022 18:27:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.8743e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:38 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2935/16548 [1:21:19<6:17:18,  1.66s/it]11/15/2022 18:27:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.8152e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:40 - INFO - train.train_snli_ve - loss is tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2936/16548 [1:21:20<6:17:12,  1.66s/it]11/15/2022 18:27:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.5935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:41 - INFO - train.train_snli_ve - loss is tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2937/16548 [1:21:22<6:18:57,  1.67s/it]11/15/2022 18:27:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1933e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:43 - INFO - train.train_snli_ve - loss is tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2938/16548 [1:21:24<6:19:56,  1.68s/it]11/15/2022 18:27:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.9725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:45 - INFO - train.train_snli_ve - loss is tensor(0.8478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2939/16548 [1:21:25<6:22:45,  1.69s/it]11/15/2022 18:27:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:46 - INFO - train.train_snli_ve - loss is tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2940/16548 [1:21:27<6:24:29,  1.70s/it]11/15/2022 18:27:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.9769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:48 - INFO - train.train_snli_ve - loss is tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2941/16548 [1:21:29<6:22:14,  1.69s/it]11/15/2022 18:27:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.5940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:50 - INFO - train.train_snli_ve - loss is tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2942/16548 [1:21:30<6:18:35,  1.67s/it]11/15/2022 18:27:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:51 - INFO - train.train_snli_ve - loss is tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2943/16548 [1:21:32<6:16:47,  1.66s/it]11/15/2022 18:27:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:53 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2944/16548 [1:21:34<6:18:59,  1.67s/it]11/15/2022 18:27:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.0965e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:55 - INFO - train.train_snli_ve - loss is tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2945/16548 [1:21:35<6:20:13,  1.68s/it]11/15/2022 18:27:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.4288e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:56 - INFO - train.train_snli_ve - loss is tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2946/16548 [1:21:37<6:19:19,  1.67s/it]11/15/2022 18:27:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.3229e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:27:58 - INFO - train.train_snli_ve - loss is tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2947/16548 [1:21:39<6:20:30,  1.68s/it]11/15/2022 18:28:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.1496e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:00 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2948/16548 [1:21:40<6:18:57,  1.67s/it]11/15/2022 18:28:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.2302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:01 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2949/16548 [1:21:42<6:17:57,  1.67s/it]11/15/2022 18:28:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.8836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:03 - INFO - train.train_snli_ve - loss is tensor(0.5325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2950/16548 [1:21:44<6:18:49,  1.67s/it]11/15/2022 18:28:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.8460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:05 - INFO - train.train_snli_ve - loss is tensor(0.8140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2951/16548 [1:21:45<6:17:46,  1.67s/it]11/15/2022 18:28:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.4851e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:06 - INFO - train.train_snli_ve - loss is tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2952/16548 [1:21:47<6:19:49,  1.68s/it]11/15/2022 18:28:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.5531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:08 - INFO - train.train_snli_ve - loss is tensor(0.4593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2953/16548 [1:21:49<6:20:11,  1.68s/it]11/15/2022 18:28:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.6071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:10 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2954/16548 [1:21:51<6:21:42,  1.68s/it]11/15/2022 18:28:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.2258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:11 - INFO - train.train_snli_ve - loss is tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2955/16548 [1:21:52<6:21:18,  1.68s/it]11/15/2022 18:28:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.9197e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:13 - INFO - train.train_snli_ve - loss is tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2956/16548 [1:21:54<6:19:32,  1.68s/it]11/15/2022 18:28:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.4755e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:15 - INFO - train.train_snli_ve - loss is tensor(0.9165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2957/16548 [1:21:56<6:19:28,  1.68s/it]11/15/2022 18:28:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.7387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:17 - INFO - train.train_snli_ve - loss is tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2958/16548 [1:21:57<6:24:05,  1.70s/it]11/15/2022 18:28:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4565e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:18 - INFO - train.train_snli_ve - loss is tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2959/16548 [1:21:59<6:27:39,  1.71s/it]11/15/2022 18:28:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.1288e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:20 - INFO - train.train_snli_ve - loss is tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2960/16548 [1:22:01<6:24:17,  1.70s/it]11/15/2022 18:28:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:22 - INFO - train.train_snli_ve - loss is tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2961/16548 [1:22:02<6:24:27,  1.70s/it]11/15/2022 18:28:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:23 - INFO - train.train_snli_ve - loss is tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2962/16548 [1:22:04<6:24:29,  1.70s/it]11/15/2022 18:28:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.3896e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:25 - INFO - train.train_snli_ve - loss is tensor(0.9257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2963/16548 [1:22:06<6:21:50,  1.69s/it]11/15/2022 18:28:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.3939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:27 - INFO - train.train_snli_ve - loss is tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2964/16548 [1:22:07<6:21:10,  1.68s/it]11/15/2022 18:28:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.9146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:28 - INFO - train.train_snli_ve - loss is tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2965/16548 [1:22:09<6:25:05,  1.70s/it]11/15/2022 18:28:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.1829e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:30 - INFO - train.train_snli_ve - loss is tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2966/16548 [1:22:11<6:24:25,  1.70s/it]11/15/2022 18:28:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.0956e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:32 - INFO - train.train_snli_ve - loss is tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2967/16548 [1:22:13<6:25:37,  1.70s/it]11/15/2022 18:28:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.2683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:34 - INFO - train.train_snli_ve - loss is tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2968/16548 [1:22:14<6:24:36,  1.70s/it]11/15/2022 18:28:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.1049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:35 - INFO - train.train_snli_ve - loss is tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2969/16548 [1:22:16<6:22:00,  1.69s/it]11/15/2022 18:28:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.3801e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:37 - INFO - train.train_snli_ve - loss is tensor(0.5004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2970/16548 [1:22:18<6:21:21,  1.69s/it]11/15/2022 18:28:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0190e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:39 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2971/16548 [1:22:19<6:20:52,  1.68s/it]11/15/2022 18:28:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.8607e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:40 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2972/16548 [1:22:21<6:20:03,  1.68s/it]11/15/2022 18:28:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.9509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:42 - INFO - train.train_snli_ve - loss is tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2973/16548 [1:22:23<6:19:25,  1.68s/it]11/15/2022 18:28:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.4140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:44 - INFO - train.train_snli_ve - loss is tensor(0.8416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2974/16548 [1:22:24<6:19:36,  1.68s/it]11/15/2022 18:28:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.0786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:45 - INFO - train.train_snli_ve - loss is tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2975/16548 [1:22:26<6:21:58,  1.69s/it]11/15/2022 18:28:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.0526e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:47 - INFO - train.train_snli_ve - loss is tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2976/16548 [1:22:28<6:21:32,  1.69s/it]11/15/2022 18:28:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.9340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:49 - INFO - train.train_snli_ve - loss is tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2977/16548 [1:22:29<6:22:39,  1.69s/it]11/15/2022 18:28:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.9701e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:50 - INFO - train.train_snli_ve - loss is tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2978/16548 [1:22:31<6:21:58,  1.69s/it]11/15/2022 18:28:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.0471e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:52 - INFO - train.train_snli_ve - loss is tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2979/16548 [1:22:33<6:18:56,  1.68s/it]11/15/2022 18:28:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.8370e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:54 - INFO - train.train_snli_ve - loss is tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2980/16548 [1:22:34<6:21:13,  1.69s/it]11/15/2022 18:28:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1523e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:55 - INFO - train.train_snli_ve - loss is tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2981/16548 [1:22:36<6:20:11,  1.68s/it]11/15/2022 18:28:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:57 - INFO - train.train_snli_ve - loss is tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2982/16548 [1:22:38<6:19:01,  1.68s/it]11/15/2022 18:28:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.7614e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:28:59 - INFO - train.train_snli_ve - loss is tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2983/16548 [1:22:39<6:18:45,  1.68s/it]11/15/2022 18:29:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.7449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:00 - INFO - train.train_snli_ve - loss is tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2984/16548 [1:22:41<6:18:32,  1.67s/it]11/15/2022 18:29:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.4958e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:02 - INFO - train.train_snli_ve - loss is tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2985/16548 [1:22:43<6:16:21,  1.66s/it]11/15/2022 18:29:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.1659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:04 - INFO - train.train_snli_ve - loss is tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2986/16548 [1:22:45<6:19:54,  1.68s/it]11/15/2022 18:29:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.8487e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:05 - INFO - train.train_snli_ve - loss is tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2987/16548 [1:22:46<6:18:42,  1.68s/it]11/15/2022 18:29:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.4027e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:07 - INFO - train.train_snli_ve - loss is tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2988/16548 [1:22:48<6:18:31,  1.67s/it]11/15/2022 18:29:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.3717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:09 - INFO - train.train_snli_ve - loss is tensor(0.4904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2989/16548 [1:22:50<6:20:11,  1.68s/it]11/15/2022 18:29:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.8727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:10 - INFO - train.train_snli_ve - loss is tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2990/16548 [1:22:51<6:18:35,  1.68s/it]11/15/2022 18:29:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.8114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:12 - INFO - train.train_snli_ve - loss is tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2991/16548 [1:22:53<6:19:26,  1.68s/it]11/15/2022 18:29:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.3517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:14 - INFO - train.train_snli_ve - loss is tensor(0.8761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2992/16548 [1:22:55<6:18:30,  1.68s/it]11/15/2022 18:29:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.2378e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:15 - INFO - train.train_snli_ve - loss is tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2993/16548 [1:22:56<6:17:06,  1.67s/it]11/15/2022 18:29:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.9597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:17 - INFO - train.train_snli_ve - loss is tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2994/16548 [1:22:58<6:19:57,  1.68s/it]11/15/2022 18:29:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.1199e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:19 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2995/16548 [1:23:00<6:19:48,  1.68s/it]11/15/2022 18:29:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.9639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:21 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2996/16548 [1:23:01<6:19:58,  1.68s/it]11/15/2022 18:29:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.0924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:22 - INFO - train.train_snli_ve - loss is tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2997/16548 [1:23:03<6:20:22,  1.68s/it]11/15/2022 18:29:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.4263e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:24 - INFO - train.train_snli_ve - loss is tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2998/16548 [1:23:05<6:20:33,  1.69s/it]11/15/2022 18:29:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:26 - INFO - train.train_snli_ve - loss is tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 2999/16548 [1:23:06<6:21:39,  1.69s/it]11/15/2022 18:29:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.8425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:27 - INFO - train.train_snli_ve - loss is tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3000/16548 [1:23:08<6:25:52,  1.71s/it]11/15/2022 18:29:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:29 - INFO - train.train_snli_ve - loss is tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3001/16548 [1:23:10<6:22:12,  1.69s/it]11/15/2022 18:29:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.6602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:31 - INFO - train.train_snli_ve - loss is tensor(0.4476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3002/16548 [1:23:11<6:18:35,  1.68s/it]11/15/2022 18:29:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.5318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:32 - INFO - train.train_snli_ve - loss is tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3003/16548 [1:23:13<6:23:38,  1.70s/it]11/15/2022 18:29:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.1335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:34 - INFO - train.train_snli_ve - loss is tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3004/16548 [1:23:15<6:27:19,  1.72s/it]11/15/2022 18:29:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.1383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:36 - INFO - train.train_snli_ve - loss is tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3005/16548 [1:23:17<6:24:58,  1.71s/it]11/15/2022 18:29:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.0934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:38 - INFO - train.train_snli_ve - loss is tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3006/16548 [1:23:18<6:24:27,  1.70s/it]11/15/2022 18:29:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.0917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:39 - INFO - train.train_snli_ve - loss is tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3007/16548 [1:23:20<6:22:30,  1.69s/it]11/15/2022 18:29:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.8045e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:41 - INFO - train.train_snli_ve - loss is tensor(0.8754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3008/16548 [1:23:22<6:23:16,  1.70s/it]11/15/2022 18:29:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.9543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:43 - INFO - train.train_snli_ve - loss is tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3009/16548 [1:23:23<6:20:16,  1.69s/it]11/15/2022 18:29:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.7084e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:44 - INFO - train.train_snli_ve - loss is tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3010/16548 [1:23:25<6:22:32,  1.70s/it]11/15/2022 18:29:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.1260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:46 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3011/16548 [1:23:27<6:23:25,  1.70s/it]11/15/2022 18:29:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:48 - INFO - train.train_snli_ve - loss is tensor(0.7426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3012/16548 [1:23:28<6:25:11,  1.71s/it]11/15/2022 18:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.8764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:49 - INFO - train.train_snli_ve - loss is tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3013/16548 [1:23:30<6:25:02,  1.71s/it]11/15/2022 18:29:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.6929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:51 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3014/16548 [1:23:32<6:21:21,  1.69s/it]11/15/2022 18:29:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:53 - INFO - train.train_snli_ve - loss is tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3015/16548 [1:23:34<6:20:17,  1.69s/it]11/15/2022 18:29:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.9699e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:54 - INFO - train.train_snli_ve - loss is tensor(0.8450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3016/16548 [1:23:35<6:17:29,  1.67s/it]11/15/2022 18:29:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.5107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:56 - INFO - train.train_snli_ve - loss is tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3017/16548 [1:23:37<6:19:49,  1.68s/it]11/15/2022 18:29:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.5668e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:58 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3018/16548 [1:23:39<6:18:29,  1.68s/it]11/15/2022 18:29:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.6972e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:29:59 - INFO - train.train_snli_ve - loss is tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3019/16548 [1:23:40<6:14:13,  1.66s/it]11/15/2022 18:30:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.3006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:01 - INFO - train.train_snli_ve - loss is tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3020/16548 [1:23:42<6:17:59,  1.68s/it]11/15/2022 18:30:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.0424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:03 - INFO - train.train_snli_ve - loss is tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3021/16548 [1:23:44<6:16:09,  1.67s/it]11/15/2022 18:30:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.8873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:04 - INFO - train.train_snli_ve - loss is tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3022/16548 [1:23:45<6:16:08,  1.67s/it]11/15/2022 18:30:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.3725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:06 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3023/16548 [1:23:47<6:15:58,  1.67s/it]11/15/2022 18:30:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.9831e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:08 - INFO - train.train_snli_ve - loss is tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3024/16548 [1:23:49<6:14:15,  1.66s/it]11/15/2022 18:30:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.3597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:09 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3025/16548 [1:23:50<6:16:08,  1.67s/it]11/15/2022 18:30:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:11 - INFO - train.train_snli_ve - loss is tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3026/16548 [1:23:52<6:16:54,  1.67s/it]11/15/2022 18:30:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.3294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:13 - INFO - train.train_snli_ve - loss is tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3027/16548 [1:23:54<6:18:14,  1.68s/it]11/15/2022 18:30:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:14 - INFO - train.train_snli_ve - loss is tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3028/16548 [1:23:55<6:16:36,  1.67s/it]11/15/2022 18:30:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.2372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:16 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3029/16548 [1:23:57<6:15:59,  1.67s/it]11/15/2022 18:30:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.4146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:18 - INFO - train.train_snli_ve - loss is tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3030/16548 [1:23:59<6:15:22,  1.67s/it]11/15/2022 18:30:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.1680e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:20 - INFO - train.train_snli_ve - loss is tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3031/16548 [1:24:00<6:19:23,  1.68s/it]11/15/2022 18:30:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:21 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3032/16548 [1:24:02<6:21:50,  1.70s/it]11/15/2022 18:30:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.9582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:23 - INFO - train.train_snli_ve - loss is tensor(0.8478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3033/16548 [1:24:04<6:21:49,  1.70s/it]11/15/2022 18:30:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.4167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:25 - INFO - train.train_snli_ve - loss is tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3034/16548 [1:24:05<6:22:35,  1.70s/it]11/15/2022 18:30:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.1064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:26 - INFO - train.train_snli_ve - loss is tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3035/16548 [1:24:07<6:23:13,  1.70s/it]11/15/2022 18:30:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.6350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:28 - INFO - train.train_snli_ve - loss is tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3036/16548 [1:24:09<6:25:03,  1.71s/it]11/15/2022 18:30:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.0848e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:30 - INFO - train.train_snli_ve - loss is tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3037/16548 [1:24:10<6:21:00,  1.69s/it]11/15/2022 18:30:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.2265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:31 - INFO - train.train_snli_ve - loss is tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3038/16548 [1:24:12<6:20:14,  1.69s/it]11/15/2022 18:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.8879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:33 - INFO - train.train_snli_ve - loss is tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3039/16548 [1:24:14<6:21:09,  1.69s/it]11/15/2022 18:30:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.1537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:35 - INFO - train.train_snli_ve - loss is tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3040/16548 [1:24:16<6:19:24,  1.69s/it]11/15/2022 18:30:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.4410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:36 - INFO - train.train_snli_ve - loss is tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3041/16548 [1:24:17<6:18:46,  1.68s/it]11/15/2022 18:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.4548e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:38 - INFO - train.train_snli_ve - loss is tensor(0.9220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3042/16548 [1:24:19<6:17:30,  1.68s/it]11/15/2022 18:30:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.8165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:40 - INFO - train.train_snli_ve - loss is tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3043/16548 [1:24:21<6:18:29,  1.68s/it]11/15/2022 18:30:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.0016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:41 - INFO - train.train_snli_ve - loss is tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3044/16548 [1:24:22<6:16:11,  1.67s/it]11/15/2022 18:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5230e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:43 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3045/16548 [1:24:24<6:16:31,  1.67s/it]11/15/2022 18:30:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.0647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:45 - INFO - train.train_snli_ve - loss is tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3046/16548 [1:24:26<6:16:24,  1.67s/it]11/15/2022 18:30:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.6086e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:46 - INFO - train.train_snli_ve - loss is tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3047/16548 [1:24:27<6:14:07,  1.66s/it]11/15/2022 18:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.7101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:48 - INFO - train.train_snli_ve - loss is tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3048/16548 [1:24:29<6:17:33,  1.68s/it]11/15/2022 18:30:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.4515e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:50 - INFO - train.train_snli_ve - loss is tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3049/16548 [1:24:31<6:17:06,  1.68s/it]11/15/2022 18:30:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.2816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:52 - INFO - train.train_snli_ve - loss is tensor(0.8031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3050/16548 [1:24:32<6:15:31,  1.67s/it]11/15/2022 18:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.3352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:53 - INFO - train.train_snli_ve - loss is tensor(0.8958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3051/16548 [1:24:34<6:17:05,  1.68s/it]11/15/2022 18:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.8520e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:55 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3052/16548 [1:24:36<6:15:59,  1.67s/it]11/15/2022 18:30:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.2530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:57 - INFO - train.train_snli_ve - loss is tensor(1.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3053/16548 [1:24:37<6:15:10,  1.67s/it]11/15/2022 18:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.7955e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:30:58 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3054/16548 [1:24:39<6:15:26,  1.67s/it]11/15/2022 18:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.5292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:00 - INFO - train.train_snli_ve - loss is tensor(0.9409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3055/16548 [1:24:41<6:15:08,  1.67s/it]11/15/2022 18:31:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.2924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:02 - INFO - train.train_snli_ve - loss is tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3056/16548 [1:24:42<6:16:28,  1.67s/it]11/15/2022 18:31:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.9795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:03 - INFO - train.train_snli_ve - loss is tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3057/16548 [1:24:44<6:18:48,  1.68s/it]11/15/2022 18:31:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.3888e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:05 - INFO - train.train_snli_ve - loss is tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3058/16548 [1:24:46<6:20:16,  1.69s/it]11/15/2022 18:31:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.2909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:07 - INFO - train.train_snli_ve - loss is tensor(0.9351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3059/16548 [1:24:47<6:18:53,  1.69s/it]11/15/2022 18:31:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.5032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:08 - INFO - train.train_snli_ve - loss is tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3060/16548 [1:24:49<6:19:44,  1.69s/it]11/15/2022 18:31:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.1779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:10 - INFO - train.train_snli_ve - loss is tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18% 3061/16548 [1:24:51<6:19:38,  1.69s/it]11/15/2022 18:31:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.0980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:12 - INFO - train.train_snli_ve - loss is tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3062/16548 [1:24:52<6:17:57,  1.68s/it]11/15/2022 18:31:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.8164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:13 - INFO - train.train_snli_ve - loss is tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3063/16548 [1:24:54<6:19:22,  1.69s/it]11/15/2022 18:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.1994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:15 - INFO - train.train_snli_ve - loss is tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3064/16548 [1:24:56<6:22:10,  1.70s/it]11/15/2022 18:31:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.1490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:17 - INFO - train.train_snli_ve - loss is tensor(0.7157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3065/16548 [1:24:58<6:23:08,  1.70s/it]11/15/2022 18:31:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.7560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:19 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3066/16548 [1:24:59<6:24:41,  1.71s/it]11/15/2022 18:31:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.2627e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:20 - INFO - train.train_snli_ve - loss is tensor(0.9581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3067/16548 [1:25:01<6:23:30,  1.71s/it]11/15/2022 18:31:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.2706e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:22 - INFO - train.train_snli_ve - loss is tensor(0.6546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3068/16548 [1:25:03<6:24:23,  1.71s/it]11/15/2022 18:31:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.5817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:24 - INFO - train.train_snli_ve - loss is tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3069/16548 [1:25:04<6:20:37,  1.69s/it]11/15/2022 18:31:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.8980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:25 - INFO - train.train_snli_ve - loss is tensor(0.8129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3070/16548 [1:25:06<6:16:40,  1.68s/it]11/15/2022 18:31:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.5228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:27 - INFO - train.train_snli_ve - loss is tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3071/16548 [1:25:08<6:17:34,  1.68s/it]11/15/2022 18:31:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.2482e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:29 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3072/16548 [1:25:09<6:14:22,  1.67s/it]11/15/2022 18:31:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.9579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:30 - INFO - train.train_snli_ve - loss is tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3073/16548 [1:25:11<6:12:45,  1.66s/it]11/15/2022 18:31:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.8806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:32 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3074/16548 [1:25:13<6:13:35,  1.66s/it]11/15/2022 18:31:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.1536e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:34 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3075/16548 [1:25:14<6:13:51,  1.66s/it]11/15/2022 18:31:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.9781e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:35 - INFO - train.train_snli_ve - loss is tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3076/16548 [1:25:16<6:15:04,  1.67s/it]11/15/2022 18:31:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.3112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:37 - INFO - train.train_snli_ve - loss is tensor(0.8895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3077/16548 [1:25:18<6:13:56,  1.67s/it]11/15/2022 18:31:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.3916e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:39 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3078/16548 [1:25:19<6:15:51,  1.67s/it]11/15/2022 18:31:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.9110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:40 - INFO - train.train_snli_ve - loss is tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3079/16548 [1:25:21<6:15:16,  1.67s/it]11/15/2022 18:31:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.2541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:42 - INFO - train.train_snli_ve - loss is tensor(0.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3080/16548 [1:25:23<6:14:52,  1.67s/it]11/15/2022 18:31:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.5184e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:44 - INFO - train.train_snli_ve - loss is tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3081/16548 [1:25:24<6:16:28,  1.68s/it]11/15/2022 18:31:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.7434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:45 - INFO - train.train_snli_ve - loss is tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3082/16548 [1:25:26<6:22:03,  1.70s/it]11/15/2022 18:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.8964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:47 - INFO - train.train_snli_ve - loss is tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3083/16548 [1:25:28<6:21:02,  1.70s/it]11/15/2022 18:31:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.4303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:49 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3084/16548 [1:25:30<6:23:17,  1.71s/it]11/15/2022 18:31:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.1818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:50 - INFO - train.train_snli_ve - loss is tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3085/16548 [1:25:31<6:21:42,  1.70s/it]11/15/2022 18:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.7781e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:52 - INFO - train.train_snli_ve - loss is tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3086/16548 [1:25:33<6:18:21,  1.69s/it]11/15/2022 18:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.9303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:54 - INFO - train.train_snli_ve - loss is tensor(0.4918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3087/16548 [1:25:35<6:20:41,  1.70s/it]11/15/2022 18:31:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.2109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:56 - INFO - train.train_snli_ve - loss is tensor(0.7709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3088/16548 [1:25:36<6:16:53,  1.68s/it]11/15/2022 18:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.5105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:57 - INFO - train.train_snli_ve - loss is tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3089/16548 [1:25:38<6:18:18,  1.69s/it]11/15/2022 18:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.7753e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:31:59 - INFO - train.train_snli_ve - loss is tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3090/16548 [1:25:40<6:18:09,  1.69s/it]11/15/2022 18:32:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.0396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:01 - INFO - train.train_snli_ve - loss is tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3091/16548 [1:25:41<6:20:23,  1.70s/it]11/15/2022 18:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.1553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:02 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3092/16548 [1:25:43<6:18:54,  1.69s/it]11/15/2022 18:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.1071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:04 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3093/16548 [1:25:45<6:19:13,  1.69s/it]11/15/2022 18:32:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.5008e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:06 - INFO - train.train_snli_ve - loss is tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3094/16548 [1:25:46<6:19:27,  1.69s/it]11/15/2022 18:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.6268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:07 - INFO - train.train_snli_ve - loss is tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3095/16548 [1:25:48<6:22:17,  1.71s/it]11/15/2022 18:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:09 - INFO - train.train_snli_ve - loss is tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3096/16548 [1:25:50<6:20:03,  1.70s/it]11/15/2022 18:32:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.2360e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:11 - INFO - train.train_snli_ve - loss is tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3097/16548 [1:25:52<6:21:08,  1.70s/it]11/15/2022 18:32:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2997e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:13 - INFO - train.train_snli_ve - loss is tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3098/16548 [1:25:53<6:21:14,  1.70s/it]11/15/2022 18:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.1834e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:14 - INFO - train.train_snli_ve - loss is tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3099/16548 [1:25:55<6:19:30,  1.69s/it]11/15/2022 18:32:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.9118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:16 - INFO - train.train_snli_ve - loss is tensor(1.3328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3100/16548 [1:25:57<6:20:41,  1.70s/it]11/15/2022 18:32:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.2356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:18 - INFO - train.train_snli_ve - loss is tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3101/16548 [1:25:58<6:17:50,  1.69s/it]11/15/2022 18:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.6030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:19 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3102/16548 [1:26:00<6:15:25,  1.68s/it]11/15/2022 18:32:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.4201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:21 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3103/16548 [1:26:02<6:13:28,  1.67s/it]11/15/2022 18:32:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.3494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:23 - INFO - train.train_snli_ve - loss is tensor(0.9375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3104/16548 [1:26:03<6:14:27,  1.67s/it]11/15/2022 18:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.0457e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:24 - INFO - train.train_snli_ve - loss is tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3105/16548 [1:26:05<6:13:18,  1.67s/it]11/15/2022 18:32:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.1769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:26 - INFO - train.train_snli_ve - loss is tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3106/16548 [1:26:07<6:12:59,  1.66s/it]11/15/2022 18:32:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.1469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:28 - INFO - train.train_snli_ve - loss is tensor(0.7684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3107/16548 [1:26:08<6:15:12,  1.67s/it]11/15/2022 18:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.1709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:29 - INFO - train.train_snli_ve - loss is tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3108/16548 [1:26:10<6:15:15,  1.68s/it]11/15/2022 18:32:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.0767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:31 - INFO - train.train_snli_ve - loss is tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3109/16548 [1:26:12<6:18:29,  1.69s/it]11/15/2022 18:32:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.7876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:33 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3110/16548 [1:26:13<6:21:52,  1.71s/it]11/15/2022 18:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.6685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:34 - INFO - train.train_snli_ve - loss is tensor(0.9633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3111/16548 [1:26:15<6:19:51,  1.70s/it]11/15/2022 18:32:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.1216e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:36 - INFO - train.train_snli_ve - loss is tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3112/16548 [1:26:17<6:18:09,  1.69s/it]11/15/2022 18:32:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.6524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:38 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3113/16548 [1:26:18<6:19:27,  1.69s/it]11/15/2022 18:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.2663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:39 - INFO - train.train_snli_ve - loss is tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3114/16548 [1:26:20<6:19:38,  1.70s/it]11/15/2022 18:32:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.1052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:41 - INFO - train.train_snli_ve - loss is tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3115/16548 [1:26:22<6:16:24,  1.68s/it]11/15/2022 18:32:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.8253e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:43 - INFO - train.train_snli_ve - loss is tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3116/16548 [1:26:24<6:17:27,  1.69s/it]11/15/2022 18:32:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.7605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:44 - INFO - train.train_snli_ve - loss is tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3117/16548 [1:26:25<6:16:17,  1.68s/it]11/15/2022 18:32:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.4810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:46 - INFO - train.train_snli_ve - loss is tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3118/16548 [1:26:27<6:15:13,  1.68s/it]11/15/2022 18:32:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.4390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:48 - INFO - train.train_snli_ve - loss is tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3119/16548 [1:26:29<6:16:20,  1.68s/it]11/15/2022 18:32:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.0477e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:49 - INFO - train.train_snli_ve - loss is tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3120/16548 [1:26:30<6:15:24,  1.68s/it]11/15/2022 18:32:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3472e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:51 - INFO - train.train_snli_ve - loss is tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3121/16548 [1:26:32<6:14:41,  1.67s/it]11/15/2022 18:32:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0338e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:53 - INFO - train.train_snli_ve - loss is tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3122/16548 [1:26:34<6:15:48,  1.68s/it]11/15/2022 18:32:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.6990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:55 - INFO - train.train_snli_ve - loss is tensor(0.4686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3123/16548 [1:26:35<6:18:06,  1.69s/it]11/15/2022 18:32:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:56 - INFO - train.train_snli_ve - loss is tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3124/16548 [1:26:37<6:19:45,  1.70s/it]11/15/2022 18:32:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.1355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:32:58 - INFO - train.train_snli_ve - loss is tensor(0.9669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3125/16548 [1:26:39<6:16:25,  1.68s/it]11/15/2022 18:33:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.8532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:00 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3126/16548 [1:26:40<6:20:47,  1.70s/it]11/15/2022 18:33:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.7612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:01 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3127/16548 [1:26:42<6:17:46,  1.69s/it]11/15/2022 18:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:03 - INFO - train.train_snli_ve - loss is tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3128/16548 [1:26:44<6:16:42,  1.68s/it]11/15/2022 18:33:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.5808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:05 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3129/16548 [1:26:45<6:16:36,  1.68s/it]11/15/2022 18:33:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.2992e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:06 - INFO - train.train_snli_ve - loss is tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3130/16548 [1:26:47<6:15:31,  1.68s/it]11/15/2022 18:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.1532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:08 - INFO - train.train_snli_ve - loss is tensor(0.4315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3131/16548 [1:26:49<6:17:36,  1.69s/it]11/15/2022 18:33:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.8189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:10 - INFO - train.train_snli_ve - loss is tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3132/16548 [1:26:50<6:16:57,  1.69s/it]11/15/2022 18:33:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.1283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:11 - INFO - train.train_snli_ve - loss is tensor(0.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3133/16548 [1:26:52<6:14:39,  1.68s/it]11/15/2022 18:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.8821e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:13 - INFO - train.train_snli_ve - loss is tensor(0.9003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3134/16548 [1:26:54<6:14:36,  1.68s/it]11/15/2022 18:33:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.6807e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:15 - INFO - train.train_snli_ve - loss is tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3135/16548 [1:26:55<6:14:57,  1.68s/it]11/15/2022 18:33:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.0116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:16 - INFO - train.train_snli_ve - loss is tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3136/16548 [1:26:57<6:16:30,  1.68s/it]11/15/2022 18:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.6363e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:18 - INFO - train.train_snli_ve - loss is tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3137/16548 [1:26:59<6:18:22,  1.69s/it]11/15/2022 18:33:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.4875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:20 - INFO - train.train_snli_ve - loss is tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3138/16548 [1:27:01<6:18:17,  1.69s/it]11/15/2022 18:33:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.2342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:22 - INFO - train.train_snli_ve - loss is tensor(0.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3139/16548 [1:27:02<6:18:52,  1.70s/it]11/15/2022 18:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3248e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:23 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3140/16548 [1:27:04<6:18:54,  1.70s/it]11/15/2022 18:33:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:25 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3141/16548 [1:27:06<6:15:57,  1.68s/it]11/15/2022 18:33:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:27 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3142/16548 [1:27:07<6:14:51,  1.68s/it]11/15/2022 18:33:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.6506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:28 - INFO - train.train_snli_ve - loss is tensor(0.8769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3143/16548 [1:27:09<6:14:12,  1.67s/it]11/15/2022 18:33:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.1775e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:30 - INFO - train.train_snli_ve - loss is tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3144/16548 [1:27:11<6:16:50,  1.69s/it]11/15/2022 18:33:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.3569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:32 - INFO - train.train_snli_ve - loss is tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3145/16548 [1:27:12<6:15:52,  1.68s/it]11/15/2022 18:33:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.7089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:33 - INFO - train.train_snli_ve - loss is tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3146/16548 [1:27:14<6:15:18,  1.68s/it]11/15/2022 18:33:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.1657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:35 - INFO - train.train_snli_ve - loss is tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3147/16548 [1:27:16<6:16:33,  1.69s/it]11/15/2022 18:33:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.7888e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:37 - INFO - train.train_snli_ve - loss is tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3148/16548 [1:27:17<6:15:13,  1.68s/it]11/15/2022 18:33:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:38 - INFO - train.train_snli_ve - loss is tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3149/16548 [1:27:19<6:12:07,  1.67s/it]11/15/2022 18:33:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.6399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:40 - INFO - train.train_snli_ve - loss is tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3150/16548 [1:27:21<6:13:38,  1.67s/it]11/15/2022 18:33:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.0317e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:42 - INFO - train.train_snli_ve - loss is tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3151/16548 [1:27:22<6:19:50,  1.70s/it]11/15/2022 18:33:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1007e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:43 - INFO - train.train_snli_ve - loss is tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3152/16548 [1:27:24<6:19:15,  1.70s/it]11/15/2022 18:33:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:45 - INFO - train.train_snli_ve - loss is tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3153/16548 [1:27:26<6:17:36,  1.69s/it]11/15/2022 18:33:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.9796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:47 - INFO - train.train_snli_ve - loss is tensor(0.4758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3154/16548 [1:27:28<6:15:07,  1.68s/it]11/15/2022 18:33:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.9365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:48 - INFO - train.train_snli_ve - loss is tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3155/16548 [1:27:29<6:16:09,  1.69s/it]11/15/2022 18:33:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:50 - INFO - train.train_snli_ve - loss is tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3156/16548 [1:27:31<6:16:57,  1.69s/it]11/15/2022 18:33:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:52 - INFO - train.train_snli_ve - loss is tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3157/16548 [1:27:33<6:12:44,  1.67s/it]11/15/2022 18:33:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:53 - INFO - train.train_snli_ve - loss is tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3158/16548 [1:27:34<6:10:35,  1.66s/it]11/15/2022 18:33:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:55 - INFO - train.train_snli_ve - loss is tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3159/16548 [1:27:36<6:14:49,  1.68s/it]11/15/2022 18:33:57 - INFO - train.train_snli_ve - kd_loss is tensor(9.4935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:57 - INFO - train.train_snli_ve - loss is tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3160/16548 [1:27:38<6:16:32,  1.69s/it]11/15/2022 18:33:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.4353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:33:59 - INFO - train.train_snli_ve - loss is tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3161/16548 [1:27:39<6:14:44,  1.68s/it]11/15/2022 18:34:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.4479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:00 - INFO - train.train_snli_ve - loss is tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3162/16548 [1:27:41<6:13:31,  1.67s/it]11/15/2022 18:34:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.4807e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:02 - INFO - train.train_snli_ve - loss is tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3163/16548 [1:27:43<6:16:27,  1.69s/it]11/15/2022 18:34:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.2044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:04 - INFO - train.train_snli_ve - loss is tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3164/16548 [1:27:44<6:15:16,  1.68s/it]11/15/2022 18:34:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.6054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:05 - INFO - train.train_snli_ve - loss is tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3165/16548 [1:27:46<6:16:20,  1.69s/it]11/15/2022 18:34:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.4130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:07 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3166/16548 [1:27:48<6:19:31,  1.70s/it]11/15/2022 18:34:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.8397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:09 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3167/16548 [1:27:49<6:14:19,  1.68s/it]11/15/2022 18:34:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:10 - INFO - train.train_snli_ve - loss is tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3168/16548 [1:27:51<6:15:18,  1.68s/it]11/15/2022 18:34:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:12 - INFO - train.train_snli_ve - loss is tensor(1.3410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3169/16548 [1:27:53<6:15:21,  1.68s/it]11/15/2022 18:34:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.2923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:14 - INFO - train.train_snli_ve - loss is tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3170/16548 [1:27:54<6:15:27,  1.68s/it]11/15/2022 18:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.4664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:15 - INFO - train.train_snli_ve - loss is tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3171/16548 [1:27:56<6:13:50,  1.68s/it]11/15/2022 18:34:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:17 - INFO - train.train_snli_ve - loss is tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3172/16548 [1:27:58<6:14:09,  1.68s/it]11/15/2022 18:34:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.5066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:19 - INFO - train.train_snli_ve - loss is tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3173/16548 [1:27:59<6:13:48,  1.68s/it]11/15/2022 18:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.1186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:20 - INFO - train.train_snli_ve - loss is tensor(0.9339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3174/16548 [1:28:01<6:15:54,  1.69s/it]11/15/2022 18:34:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.3361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:22 - INFO - train.train_snli_ve - loss is tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3175/16548 [1:28:03<6:14:28,  1.68s/it]11/15/2022 18:34:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.2127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:24 - INFO - train.train_snli_ve - loss is tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3176/16548 [1:28:04<6:12:34,  1.67s/it]11/15/2022 18:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:25 - INFO - train.train_snli_ve - loss is tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3177/16548 [1:28:06<6:11:14,  1.67s/it]11/15/2022 18:34:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.4541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:27 - INFO - train.train_snli_ve - loss is tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3178/16548 [1:28:08<6:09:59,  1.66s/it]11/15/2022 18:34:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.2873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:29 - INFO - train.train_snli_ve - loss is tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3179/16548 [1:28:09<6:13:31,  1.68s/it]11/15/2022 18:34:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.3090e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:30 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3180/16548 [1:28:11<6:15:10,  1.68s/it]11/15/2022 18:34:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.5637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:32 - INFO - train.train_snli_ve - loss is tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3181/16548 [1:28:13<6:13:05,  1.67s/it]11/15/2022 18:34:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.5202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:34 - INFO - train.train_snli_ve - loss is tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3182/16548 [1:28:14<6:12:01,  1.67s/it]11/15/2022 18:34:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.8911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:35 - INFO - train.train_snli_ve - loss is tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3183/16548 [1:28:16<6:12:19,  1.67s/it]11/15/2022 18:34:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.4069e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:37 - INFO - train.train_snli_ve - loss is tensor(0.9973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3184/16548 [1:28:18<6:14:05,  1.68s/it]11/15/2022 18:34:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.8120e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:39 - INFO - train.train_snli_ve - loss is tensor(0.8263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3185/16548 [1:28:20<6:14:52,  1.68s/it]11/15/2022 18:34:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.7769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:41 - INFO - train.train_snli_ve - loss is tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3186/16548 [1:28:21<6:16:51,  1.69s/it]11/15/2022 18:34:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.5332e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:42 - INFO - train.train_snli_ve - loss is tensor(0.7754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3187/16548 [1:28:23<6:13:49,  1.68s/it]11/15/2022 18:34:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.4018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:44 - INFO - train.train_snli_ve - loss is tensor(0.9457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3188/16548 [1:28:25<6:15:40,  1.69s/it]11/15/2022 18:34:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.0077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:46 - INFO - train.train_snli_ve - loss is tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3189/16548 [1:28:26<6:15:00,  1.68s/it]11/15/2022 18:34:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.0160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:47 - INFO - train.train_snli_ve - loss is tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3190/16548 [1:28:28<6:12:10,  1.67s/it]11/15/2022 18:34:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.3979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:49 - INFO - train.train_snli_ve - loss is tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3191/16548 [1:28:30<6:12:09,  1.67s/it]11/15/2022 18:34:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.8860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:51 - INFO - train.train_snli_ve - loss is tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3192/16548 [1:28:31<6:14:38,  1.68s/it]11/15/2022 18:34:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.9058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:52 - INFO - train.train_snli_ve - loss is tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3193/16548 [1:28:33<6:14:57,  1.68s/it]11/15/2022 18:34:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.8200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:54 - INFO - train.train_snli_ve - loss is tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3194/16548 [1:28:35<6:19:00,  1.70s/it]11/15/2022 18:34:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.3040e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:56 - INFO - train.train_snli_ve - loss is tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3195/16548 [1:28:36<6:16:07,  1.69s/it]11/15/2022 18:34:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.1328e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:57 - INFO - train.train_snli_ve - loss is tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3196/16548 [1:28:38<6:17:07,  1.69s/it]11/15/2022 18:34:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.1555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:34:59 - INFO - train.train_snli_ve - loss is tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3197/16548 [1:28:40<6:13:18,  1.68s/it]11/15/2022 18:35:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.0752e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:01 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3198/16548 [1:28:41<6:13:12,  1.68s/it]11/15/2022 18:35:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.0276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:02 - INFO - train.train_snli_ve - loss is tensor(0.5578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3199/16548 [1:28:43<6:10:23,  1.66s/it]11/15/2022 18:35:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.2001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:04 - INFO - train.train_snli_ve - loss is tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3200/16548 [1:28:45<6:14:48,  1.68s/it]11/15/2022 18:35:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.0872e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:06 - INFO - train.train_snli_ve - loss is tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3201/16548 [1:28:47<6:16:58,  1.69s/it]11/15/2022 18:35:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.6108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:07 - INFO - train.train_snli_ve - loss is tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3202/16548 [1:28:48<6:13:25,  1.68s/it]11/15/2022 18:35:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.7947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:09 - INFO - train.train_snli_ve - loss is tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3203/16548 [1:28:50<6:14:05,  1.68s/it]11/15/2022 18:35:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.0799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:11 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3204/16548 [1:28:52<6:12:21,  1.67s/it]11/15/2022 18:35:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0668e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:12 - INFO - train.train_snli_ve - loss is tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3205/16548 [1:28:53<6:14:44,  1.69s/it]11/15/2022 18:35:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.2728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:14 - INFO - train.train_snli_ve - loss is tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3206/16548 [1:28:55<6:18:16,  1.70s/it]11/15/2022 18:35:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.7957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:16 - INFO - train.train_snli_ve - loss is tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3207/16548 [1:28:57<6:17:56,  1.70s/it]11/15/2022 18:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:18 - INFO - train.train_snli_ve - loss is tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3208/16548 [1:28:58<6:16:36,  1.69s/it]11/15/2022 18:35:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:19 - INFO - train.train_snli_ve - loss is tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3209/16548 [1:29:00<6:17:17,  1.70s/it]11/15/2022 18:35:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:21 - INFO - train.train_snli_ve - loss is tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3210/16548 [1:29:02<6:16:38,  1.69s/it]11/15/2022 18:35:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:23 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3211/16548 [1:29:03<6:13:19,  1.68s/it]11/15/2022 18:35:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.3403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:24 - INFO - train.train_snli_ve - loss is tensor(0.8946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3212/16548 [1:29:05<6:17:39,  1.70s/it]11/15/2022 18:35:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.5765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:26 - INFO - train.train_snli_ve - loss is tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3213/16548 [1:29:07<6:16:46,  1.70s/it]11/15/2022 18:35:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.2249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:28 - INFO - train.train_snli_ve - loss is tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3214/16548 [1:29:08<6:16:08,  1.69s/it]11/15/2022 18:35:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.5059e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:29 - INFO - train.train_snli_ve - loss is tensor(0.8190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3215/16548 [1:29:10<6:14:41,  1.69s/it]11/15/2022 18:35:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.2559e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:31 - INFO - train.train_snli_ve - loss is tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3216/16548 [1:29:12<6:15:17,  1.69s/it]11/15/2022 18:35:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.2808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:33 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3217/16548 [1:29:14<6:16:19,  1.69s/it]11/15/2022 18:35:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.4736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:35 - INFO - train.train_snli_ve - loss is tensor(0.8774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3218/16548 [1:29:15<6:15:55,  1.69s/it]11/15/2022 18:35:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2216e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:36 - INFO - train.train_snli_ve - loss is tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3219/16548 [1:29:17<6:13:38,  1.68s/it]11/15/2022 18:35:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.8631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:38 - INFO - train.train_snli_ve - loss is tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3220/16548 [1:29:19<6:14:01,  1.68s/it]11/15/2022 18:35:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.1657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:40 - INFO - train.train_snli_ve - loss is tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3221/16548 [1:29:20<6:13:19,  1.68s/it]11/15/2022 18:35:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:41 - INFO - train.train_snli_ve - loss is tensor(0.5656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3222/16548 [1:29:22<6:14:36,  1.69s/it]11/15/2022 18:35:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.3403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:43 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3223/16548 [1:29:24<6:13:19,  1.68s/it]11/15/2022 18:35:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:45 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3224/16548 [1:29:25<6:13:40,  1.68s/it]11/15/2022 18:35:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.2840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:46 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3225/16548 [1:29:27<6:15:59,  1.69s/it]11/15/2022 18:35:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.4831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:48 - INFO - train.train_snli_ve - loss is tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19% 3226/16548 [1:29:29<6:14:08,  1.69s/it]11/15/2022 18:35:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:50 - INFO - train.train_snli_ve - loss is tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3227/16548 [1:29:30<6:11:46,  1.67s/it]11/15/2022 18:35:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.3670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:51 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3228/16548 [1:29:32<6:11:41,  1.67s/it]11/15/2022 18:35:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.8523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:53 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3229/16548 [1:29:34<6:10:46,  1.67s/it]11/15/2022 18:35:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.9765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:55 - INFO - train.train_snli_ve - loss is tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3230/16548 [1:29:35<6:10:20,  1.67s/it]11/15/2022 18:35:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.5813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:56 - INFO - train.train_snli_ve - loss is tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3231/16548 [1:29:37<6:10:03,  1.67s/it]11/15/2022 18:35:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:35:58 - INFO - train.train_snli_ve - loss is tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3232/16548 [1:29:39<6:10:02,  1.67s/it]11/15/2022 18:36:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.3158e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:00 - INFO - train.train_snli_ve - loss is tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3233/16548 [1:29:40<6:08:54,  1.66s/it]11/15/2022 18:36:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.4707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:01 - INFO - train.train_snli_ve - loss is tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3234/16548 [1:29:42<6:13:00,  1.68s/it]11/15/2022 18:36:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.6281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:03 - INFO - train.train_snli_ve - loss is tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3235/16548 [1:29:44<6:12:03,  1.68s/it]11/15/2022 18:36:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.8331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:05 - INFO - train.train_snli_ve - loss is tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3236/16548 [1:29:45<6:10:17,  1.67s/it]11/15/2022 18:36:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.6040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:06 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3237/16548 [1:29:47<6:11:51,  1.68s/it]11/15/2022 18:36:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.9307e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:08 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3238/16548 [1:29:49<6:14:53,  1.69s/it]11/15/2022 18:36:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.9067e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:10 - INFO - train.train_snli_ve - loss is tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3239/16548 [1:29:51<6:15:05,  1.69s/it]11/15/2022 18:36:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:11 - INFO - train.train_snli_ve - loss is tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3240/16548 [1:29:52<6:14:27,  1.69s/it]11/15/2022 18:36:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.7974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:13 - INFO - train.train_snli_ve - loss is tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3241/16548 [1:29:54<6:14:18,  1.69s/it]11/15/2022 18:36:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.9729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:15 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3242/16548 [1:29:56<6:12:52,  1.68s/it]11/15/2022 18:36:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.7960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:16 - INFO - train.train_snli_ve - loss is tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3243/16548 [1:29:57<6:11:49,  1.68s/it]11/15/2022 18:36:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.8432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:18 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3244/16548 [1:29:59<6:09:00,  1.66s/it]11/15/2022 18:36:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.9915e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:20 - INFO - train.train_snli_ve - loss is tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3245/16548 [1:30:01<6:10:26,  1.67s/it]11/15/2022 18:36:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.8136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:21 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3246/16548 [1:30:02<6:11:15,  1.67s/it]11/15/2022 18:36:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.3088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:23 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3247/16548 [1:30:04<6:10:26,  1.67s/it]11/15/2022 18:36:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:25 - INFO - train.train_snli_ve - loss is tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3248/16548 [1:30:06<6:09:46,  1.67s/it]11/15/2022 18:36:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.3885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:26 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3249/16548 [1:30:07<6:09:22,  1.67s/it]11/15/2022 18:36:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.5393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:28 - INFO - train.train_snli_ve - loss is tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3250/16548 [1:30:09<6:10:34,  1.67s/it]11/15/2022 18:36:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.7489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:30 - INFO - train.train_snli_ve - loss is tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3251/16548 [1:30:11<6:09:42,  1.67s/it]11/15/2022 18:36:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.8019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:31 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3252/16548 [1:30:12<6:09:25,  1.67s/it]11/15/2022 18:36:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.4245e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:33 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3253/16548 [1:30:14<6:07:50,  1.66s/it]11/15/2022 18:36:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.1162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:35 - INFO - train.train_snli_ve - loss is tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3254/16548 [1:30:15<6:05:38,  1.65s/it]11/15/2022 18:36:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.9808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:36 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3255/16548 [1:30:17<6:08:06,  1.66s/it]11/15/2022 18:36:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.1665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:38 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3256/16548 [1:30:19<6:10:00,  1.67s/it]11/15/2022 18:36:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.7451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:40 - INFO - train.train_snli_ve - loss is tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3257/16548 [1:30:21<6:10:49,  1.67s/it]11/15/2022 18:36:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.1478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:41 - INFO - train.train_snli_ve - loss is tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3258/16548 [1:30:22<6:10:23,  1.67s/it]11/15/2022 18:36:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.4253e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:43 - INFO - train.train_snli_ve - loss is tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3259/16548 [1:30:24<6:10:56,  1.67s/it]11/15/2022 18:36:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.6492e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:45 - INFO - train.train_snli_ve - loss is tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3260/16548 [1:30:26<6:10:48,  1.67s/it]11/15/2022 18:36:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.9565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:47 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3261/16548 [1:30:27<6:12:16,  1.68s/it]11/15/2022 18:36:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.3632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:48 - INFO - train.train_snli_ve - loss is tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3262/16548 [1:30:29<6:10:34,  1.67s/it]11/15/2022 18:36:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.0198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:50 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3263/16548 [1:30:31<6:07:40,  1.66s/it]11/15/2022 18:36:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.9937e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:51 - INFO - train.train_snli_ve - loss is tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3264/16548 [1:30:32<6:07:38,  1.66s/it]11/15/2022 18:36:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.9088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:53 - INFO - train.train_snli_ve - loss is tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3265/16548 [1:30:34<6:08:20,  1.66s/it]11/15/2022 18:36:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.9556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:55 - INFO - train.train_snli_ve - loss is tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3266/16548 [1:30:36<6:07:38,  1.66s/it]11/15/2022 18:36:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.5250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:56 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3267/16548 [1:30:37<6:06:13,  1.65s/it]11/15/2022 18:36:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.8874e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:36:58 - INFO - train.train_snli_ve - loss is tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3268/16548 [1:30:39<6:07:05,  1.66s/it]11/15/2022 18:37:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.1530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:00 - INFO - train.train_snli_ve - loss is tensor(0.9086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3269/16548 [1:30:40<6:06:28,  1.66s/it]11/15/2022 18:37:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2528e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:01 - INFO - train.train_snli_ve - loss is tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3270/16548 [1:30:42<6:06:46,  1.66s/it]11/15/2022 18:37:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.8372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:03 - INFO - train.train_snli_ve - loss is tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3271/16548 [1:30:44<6:07:19,  1.66s/it]11/15/2022 18:37:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.7788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:05 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3272/16548 [1:30:46<6:11:10,  1.68s/it]11/15/2022 18:37:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.2019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:06 - INFO - train.train_snli_ve - loss is tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3273/16548 [1:30:47<6:12:26,  1.68s/it]11/15/2022 18:37:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.3097e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:08 - INFO - train.train_snli_ve - loss is tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3274/16548 [1:30:49<6:11:21,  1.68s/it]11/15/2022 18:37:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.2013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:10 - INFO - train.train_snli_ve - loss is tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3275/16548 [1:30:51<6:10:25,  1.67s/it]11/15/2022 18:37:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.6717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:11 - INFO - train.train_snli_ve - loss is tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3276/16548 [1:30:52<6:08:41,  1.67s/it]11/15/2022 18:37:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.7266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:13 - INFO - train.train_snli_ve - loss is tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3277/16548 [1:30:54<6:10:13,  1.67s/it]11/15/2022 18:37:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.6354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:15 - INFO - train.train_snli_ve - loss is tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3278/16548 [1:30:56<6:10:13,  1.67s/it]11/15/2022 18:37:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.1235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:17 - INFO - train.train_snli_ve - loss is tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3279/16548 [1:30:57<6:10:17,  1.67s/it]11/15/2022 18:37:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.7705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:18 - INFO - train.train_snli_ve - loss is tensor(1.0703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3280/16548 [1:30:59<6:11:21,  1.68s/it]11/15/2022 18:37:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.6787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:20 - INFO - train.train_snli_ve - loss is tensor(0.8205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3281/16548 [1:31:01<6:08:13,  1.67s/it]11/15/2022 18:37:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.9468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:22 - INFO - train.train_snli_ve - loss is tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3282/16548 [1:31:02<6:08:56,  1.67s/it]11/15/2022 18:37:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.8910e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:23 - INFO - train.train_snli_ve - loss is tensor(1.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3283/16548 [1:31:04<6:07:23,  1.66s/it]11/15/2022 18:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.4488e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:25 - INFO - train.train_snli_ve - loss is tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3284/16548 [1:31:06<6:09:48,  1.67s/it]11/15/2022 18:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2663e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:27 - INFO - train.train_snli_ve - loss is tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3285/16548 [1:31:07<6:08:31,  1.67s/it]11/15/2022 18:37:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.4399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:28 - INFO - train.train_snli_ve - loss is tensor(0.8940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3286/16548 [1:31:09<6:11:02,  1.68s/it]11/15/2022 18:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:30 - INFO - train.train_snli_ve - loss is tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3287/16548 [1:31:11<6:15:20,  1.70s/it]11/15/2022 18:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.9351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:32 - INFO - train.train_snli_ve - loss is tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3288/16548 [1:31:12<6:16:29,  1.70s/it]11/15/2022 18:37:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.2893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:33 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3289/16548 [1:31:14<6:14:19,  1.69s/it]11/15/2022 18:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.2100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:35 - INFO - train.train_snli_ve - loss is tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3290/16548 [1:31:16<6:11:43,  1.68s/it]11/15/2022 18:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.0041e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:37 - INFO - train.train_snli_ve - loss is tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3291/16548 [1:31:17<6:13:34,  1.69s/it]11/15/2022 18:37:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.4546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:38 - INFO - train.train_snli_ve - loss is tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3292/16548 [1:31:19<6:14:18,  1.69s/it]11/15/2022 18:37:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.5346e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:40 - INFO - train.train_snli_ve - loss is tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3293/16548 [1:31:21<6:17:03,  1.71s/it]11/15/2022 18:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.5298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:42 - INFO - train.train_snli_ve - loss is tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3294/16548 [1:31:23<6:13:01,  1.69s/it]11/15/2022 18:37:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.5051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:43 - INFO - train.train_snli_ve - loss is tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3295/16548 [1:31:24<6:12:09,  1.68s/it]11/15/2022 18:37:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.1708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:45 - INFO - train.train_snli_ve - loss is tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3296/16548 [1:31:26<6:13:31,  1.69s/it]11/15/2022 18:37:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.6586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:47 - INFO - train.train_snli_ve - loss is tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3297/16548 [1:31:28<6:12:02,  1.68s/it]11/15/2022 18:37:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.7180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:49 - INFO - train.train_snli_ve - loss is tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3298/16548 [1:31:29<6:11:03,  1.68s/it]11/15/2022 18:37:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.5605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:50 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3299/16548 [1:31:31<6:09:36,  1.67s/it]11/15/2022 18:37:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.7855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:52 - INFO - train.train_snli_ve - loss is tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3300/16548 [1:31:33<6:13:34,  1.69s/it]11/15/2022 18:37:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.3894e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:54 - INFO - train.train_snli_ve - loss is tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3301/16548 [1:31:34<6:13:59,  1.69s/it]11/15/2022 18:37:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.6000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:55 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3302/16548 [1:31:36<6:14:37,  1.70s/it]11/15/2022 18:37:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.8049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:57 - INFO - train.train_snli_ve - loss is tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3303/16548 [1:31:38<6:09:50,  1.68s/it]11/15/2022 18:37:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.4372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:37:59 - INFO - train.train_snli_ve - loss is tensor(0.8107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3304/16548 [1:31:39<6:08:52,  1.67s/it]11/15/2022 18:38:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.8735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:00 - INFO - train.train_snli_ve - loss is tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3305/16548 [1:31:41<6:10:54,  1.68s/it]11/15/2022 18:38:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:02 - INFO - train.train_snli_ve - loss is tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3306/16548 [1:31:43<6:09:00,  1.67s/it]11/15/2022 18:38:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.7685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:04 - INFO - train.train_snli_ve - loss is tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3307/16548 [1:31:44<6:08:01,  1.67s/it]11/15/2022 18:38:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.2637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:05 - INFO - train.train_snli_ve - loss is tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3308/16548 [1:31:46<6:10:24,  1.68s/it]11/15/2022 18:38:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.2758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:07 - INFO - train.train_snli_ve - loss is tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3309/16548 [1:31:48<6:13:05,  1.69s/it]11/15/2022 18:38:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.5858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:09 - INFO - train.train_snli_ve - loss is tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3310/16548 [1:31:49<6:12:09,  1.69s/it]11/15/2022 18:38:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.1623e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:10 - INFO - train.train_snli_ve - loss is tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3311/16548 [1:31:51<6:17:38,  1.71s/it]11/15/2022 18:38:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.5933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:12 - INFO - train.train_snli_ve - loss is tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3312/16548 [1:31:53<6:15:09,  1.70s/it]11/15/2022 18:38:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.2922e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:14 - INFO - train.train_snli_ve - loss is tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3313/16548 [1:31:55<6:14:58,  1.70s/it]11/15/2022 18:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.7532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:16 - INFO - train.train_snli_ve - loss is tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3314/16548 [1:31:56<6:16:55,  1.71s/it]11/15/2022 18:38:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.1233e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:17 - INFO - train.train_snli_ve - loss is tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3315/16548 [1:31:58<6:13:48,  1.69s/it]11/15/2022 18:38:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.5428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:19 - INFO - train.train_snli_ve - loss is tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3316/16548 [1:32:00<6:12:17,  1.69s/it]11/15/2022 18:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.4135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:21 - INFO - train.train_snli_ve - loss is tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3317/16548 [1:32:01<6:14:30,  1.70s/it]11/15/2022 18:38:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.5048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:22 - INFO - train.train_snli_ve - loss is tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3318/16548 [1:32:03<6:15:26,  1.70s/it]11/15/2022 18:38:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.7657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:24 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3319/16548 [1:32:05<6:13:07,  1.69s/it]11/15/2022 18:38:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.1648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:26 - INFO - train.train_snli_ve - loss is tensor(1.0219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3320/16548 [1:32:06<6:16:19,  1.71s/it]11/15/2022 18:38:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.5047e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:27 - INFO - train.train_snli_ve - loss is tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3321/16548 [1:32:08<6:14:23,  1.70s/it]11/15/2022 18:38:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.3270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:29 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3322/16548 [1:32:10<6:14:19,  1.70s/it]11/15/2022 18:38:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.0197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:31 - INFO - train.train_snli_ve - loss is tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3323/16548 [1:32:12<6:13:02,  1.69s/it]11/15/2022 18:38:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:32 - INFO - train.train_snli_ve - loss is tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3324/16548 [1:32:13<6:09:46,  1.68s/it]11/15/2022 18:38:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.1315e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:34 - INFO - train.train_snli_ve - loss is tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3325/16548 [1:32:15<6:11:38,  1.69s/it]11/15/2022 18:38:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:36 - INFO - train.train_snli_ve - loss is tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3326/16548 [1:32:17<6:10:09,  1.68s/it]11/15/2022 18:38:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.1107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:37 - INFO - train.train_snli_ve - loss is tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3327/16548 [1:32:18<6:08:20,  1.67s/it]11/15/2022 18:38:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.7534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:39 - INFO - train.train_snli_ve - loss is tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3328/16548 [1:32:20<6:08:20,  1.67s/it]11/15/2022 18:38:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.1283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:41 - INFO - train.train_snli_ve - loss is tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3329/16548 [1:32:22<6:10:00,  1.68s/it]11/15/2022 18:38:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.2672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:43 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3330/16548 [1:32:23<6:08:00,  1.67s/it]11/15/2022 18:38:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.0210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:44 - INFO - train.train_snli_ve - loss is tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3331/16548 [1:32:25<6:10:01,  1.68s/it]11/15/2022 18:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.4381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:46 - INFO - train.train_snli_ve - loss is tensor(0.8606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3332/16548 [1:32:27<6:08:26,  1.67s/it]11/15/2022 18:38:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.2391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:48 - INFO - train.train_snli_ve - loss is tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3333/16548 [1:32:28<6:11:41,  1.69s/it]11/15/2022 18:38:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.4949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:49 - INFO - train.train_snli_ve - loss is tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3334/16548 [1:32:30<6:10:49,  1.68s/it]11/15/2022 18:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.1903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:51 - INFO - train.train_snli_ve - loss is tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3335/16548 [1:32:32<6:08:17,  1.67s/it]11/15/2022 18:38:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.6367e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:53 - INFO - train.train_snli_ve - loss is tensor(0.8571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3336/16548 [1:32:33<6:11:48,  1.69s/it]11/15/2022 18:38:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.2715e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:54 - INFO - train.train_snli_ve - loss is tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3337/16548 [1:32:35<6:12:10,  1.69s/it]11/15/2022 18:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.6693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:56 - INFO - train.train_snli_ve - loss is tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3338/16548 [1:32:37<6:11:37,  1.69s/it]11/15/2022 18:38:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.9940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:58 - INFO - train.train_snli_ve - loss is tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3339/16548 [1:32:38<6:10:16,  1.68s/it]11/15/2022 18:38:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.2176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:38:59 - INFO - train.train_snli_ve - loss is tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3340/16548 [1:32:40<6:08:14,  1.67s/it]11/15/2022 18:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.3449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:01 - INFO - train.train_snli_ve - loss is tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3341/16548 [1:32:42<6:09:21,  1.68s/it]11/15/2022 18:39:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.5524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:03 - INFO - train.train_snli_ve - loss is tensor(0.9969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3342/16548 [1:32:43<6:09:04,  1.68s/it]11/15/2022 18:39:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.5266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:04 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3343/16548 [1:32:45<6:09:05,  1.68s/it]11/15/2022 18:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.3622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:06 - INFO - train.train_snli_ve - loss is tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3344/16548 [1:32:47<6:08:00,  1.67s/it]11/15/2022 18:39:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.1353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:08 - INFO - train.train_snli_ve - loss is tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3345/16548 [1:32:48<6:08:29,  1.67s/it]11/15/2022 18:39:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:09 - INFO - train.train_snli_ve - loss is tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3346/16548 [1:32:50<6:07:05,  1.67s/it]11/15/2022 18:39:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.2083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:11 - INFO - train.train_snli_ve - loss is tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3347/16548 [1:32:52<6:09:20,  1.68s/it]11/15/2022 18:39:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.8513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:13 - INFO - train.train_snli_ve - loss is tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3348/16548 [1:32:53<6:07:27,  1.67s/it]11/15/2022 18:39:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.5530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:14 - INFO - train.train_snli_ve - loss is tensor(0.7770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3349/16548 [1:32:55<6:09:32,  1.68s/it]11/15/2022 18:39:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.3143e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:16 - INFO - train.train_snli_ve - loss is tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3350/16548 [1:32:57<6:06:57,  1.67s/it]11/15/2022 18:39:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.4555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:18 - INFO - train.train_snli_ve - loss is tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3351/16548 [1:32:58<6:07:35,  1.67s/it]11/15/2022 18:39:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.7604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:19 - INFO - train.train_snli_ve - loss is tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3352/16548 [1:33:00<6:07:45,  1.67s/it]11/15/2022 18:39:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.0663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:21 - INFO - train.train_snli_ve - loss is tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3353/16548 [1:33:02<6:09:00,  1.68s/it]11/15/2022 18:39:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.8756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:23 - INFO - train.train_snli_ve - loss is tensor(0.7500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3354/16548 [1:33:04<6:09:00,  1.68s/it]11/15/2022 18:39:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.0840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:24 - INFO - train.train_snli_ve - loss is tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3355/16548 [1:33:05<6:10:27,  1.68s/it]11/15/2022 18:39:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.1132e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:26 - INFO - train.train_snli_ve - loss is tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3356/16548 [1:33:07<6:08:40,  1.68s/it]11/15/2022 18:39:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.1420e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:28 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3357/16548 [1:33:09<6:07:19,  1.67s/it]11/15/2022 18:39:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:29 - INFO - train.train_snli_ve - loss is tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3358/16548 [1:33:10<6:07:46,  1.67s/it]11/15/2022 18:39:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.2574e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:31 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3359/16548 [1:33:12<6:06:40,  1.67s/it]11/15/2022 18:39:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.6077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:33 - INFO - train.train_snli_ve - loss is tensor(0.8609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3360/16548 [1:33:14<6:06:17,  1.67s/it]11/15/2022 18:39:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.8305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:34 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3361/16548 [1:33:15<6:06:51,  1.67s/it]11/15/2022 18:39:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.6629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:36 - INFO - train.train_snli_ve - loss is tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3362/16548 [1:33:17<6:06:38,  1.67s/it]11/15/2022 18:39:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.1537e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:38 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3363/16548 [1:33:19<6:04:21,  1.66s/it]11/15/2022 18:39:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.4703e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:39 - INFO - train.train_snli_ve - loss is tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3364/16548 [1:33:20<6:03:49,  1.66s/it]11/15/2022 18:39:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.9077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:41 - INFO - train.train_snli_ve - loss is tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3365/16548 [1:33:22<6:04:11,  1.66s/it]11/15/2022 18:39:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.1293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:43 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3366/16548 [1:33:24<6:06:07,  1.67s/it]11/15/2022 18:39:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.7452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:44 - INFO - train.train_snli_ve - loss is tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3367/16548 [1:33:25<6:04:47,  1.66s/it]11/15/2022 18:39:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.7978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:46 - INFO - train.train_snli_ve - loss is tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3368/16548 [1:33:27<6:07:34,  1.67s/it]11/15/2022 18:39:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.0793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:48 - INFO - train.train_snli_ve - loss is tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3369/16548 [1:33:29<6:10:05,  1.68s/it]11/15/2022 18:39:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:50 - INFO - train.train_snli_ve - loss is tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3370/16548 [1:33:30<6:10:14,  1.69s/it]11/15/2022 18:39:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.6724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:51 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3371/16548 [1:33:32<6:09:42,  1.68s/it]11/15/2022 18:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.1538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:53 - INFO - train.train_snli_ve - loss is tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3372/16548 [1:33:34<6:07:20,  1.67s/it]11/15/2022 18:39:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:55 - INFO - train.train_snli_ve - loss is tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3373/16548 [1:33:35<6:06:14,  1.67s/it]11/15/2022 18:39:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.1948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:56 - INFO - train.train_snli_ve - loss is tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3374/16548 [1:33:37<6:08:59,  1.68s/it]11/15/2022 18:39:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.8796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:39:58 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3375/16548 [1:33:39<6:08:14,  1.68s/it]11/15/2022 18:40:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.8260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:00 - INFO - train.train_snli_ve - loss is tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3376/16548 [1:33:40<6:10:44,  1.69s/it]11/15/2022 18:40:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.3332e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:01 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3377/16548 [1:33:42<6:11:16,  1.69s/it]11/15/2022 18:40:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.6298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:03 - INFO - train.train_snli_ve - loss is tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3378/16548 [1:33:44<6:09:45,  1.68s/it]11/15/2022 18:40:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.4734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:05 - INFO - train.train_snli_ve - loss is tensor(0.9438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3379/16548 [1:33:45<6:07:38,  1.68s/it]11/15/2022 18:40:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.4382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:06 - INFO - train.train_snli_ve - loss is tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3380/16548 [1:33:47<6:07:32,  1.67s/it]11/15/2022 18:40:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.3959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:08 - INFO - train.train_snli_ve - loss is tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3381/16548 [1:33:49<6:06:41,  1.67s/it]11/15/2022 18:40:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.7832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:10 - INFO - train.train_snli_ve - loss is tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3382/16548 [1:33:50<6:06:22,  1.67s/it]11/15/2022 18:40:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.9241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:11 - INFO - train.train_snli_ve - loss is tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3383/16548 [1:33:52<6:07:12,  1.67s/it]11/15/2022 18:40:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.0506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:13 - INFO - train.train_snli_ve - loss is tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3384/16548 [1:33:54<6:07:56,  1.68s/it]11/15/2022 18:40:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.4597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:15 - INFO - train.train_snli_ve - loss is tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3385/16548 [1:33:55<6:08:52,  1.68s/it]11/15/2022 18:40:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.8713e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:16 - INFO - train.train_snli_ve - loss is tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3386/16548 [1:33:57<6:08:22,  1.68s/it]11/15/2022 18:40:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.1831e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:18 - INFO - train.train_snli_ve - loss is tensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3387/16548 [1:33:59<6:10:37,  1.69s/it]11/15/2022 18:40:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.9177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:20 - INFO - train.train_snli_ve - loss is tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3388/16548 [1:34:00<6:08:14,  1.68s/it]11/15/2022 18:40:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.6037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:21 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3389/16548 [1:34:02<6:09:24,  1.68s/it]11/15/2022 18:40:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.8556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:23 - INFO - train.train_snli_ve - loss is tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3390/16548 [1:34:04<6:07:58,  1.68s/it]11/15/2022 18:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.6074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:25 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3391/16548 [1:34:05<6:06:40,  1.67s/it]11/15/2022 18:40:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.4637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:26 - INFO - train.train_snli_ve - loss is tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20% 3392/16548 [1:34:07<6:05:53,  1.67s/it]11/15/2022 18:40:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.8305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:28 - INFO - train.train_snli_ve - loss is tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3393/16548 [1:34:09<6:05:52,  1.67s/it]11/15/2022 18:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.6258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:30 - INFO - train.train_snli_ve - loss is tensor(0.8490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3394/16548 [1:34:10<6:04:35,  1.66s/it]11/15/2022 18:40:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.3012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:31 - INFO - train.train_snli_ve - loss is tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3395/16548 [1:34:12<6:03:02,  1.66s/it]11/15/2022 18:40:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.3359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:33 - INFO - train.train_snli_ve - loss is tensor(0.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3396/16548 [1:34:14<6:03:34,  1.66s/it]11/15/2022 18:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0919e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:35 - INFO - train.train_snli_ve - loss is tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3397/16548 [1:34:15<6:05:02,  1.67s/it]11/15/2022 18:40:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.7754e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:36 - INFO - train.train_snli_ve - loss is tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3398/16548 [1:34:17<6:06:55,  1.67s/it]11/15/2022 18:40:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.3576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:38 - INFO - train.train_snli_ve - loss is tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3399/16548 [1:34:19<6:09:43,  1.69s/it]11/15/2022 18:40:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.9258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:40 - INFO - train.train_snli_ve - loss is tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3400/16548 [1:34:21<6:15:16,  1.71s/it]11/15/2022 18:40:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:42 - INFO - train.train_snli_ve - loss is tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3401/16548 [1:34:22<6:12:45,  1.70s/it]11/15/2022 18:40:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.2175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:43 - INFO - train.train_snli_ve - loss is tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3402/16548 [1:34:24<6:10:28,  1.69s/it]11/15/2022 18:40:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.7517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:45 - INFO - train.train_snli_ve - loss is tensor(0.5034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3403/16548 [1:34:26<6:08:08,  1.68s/it]11/15/2022 18:40:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.0762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:47 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3404/16548 [1:34:27<6:06:57,  1.68s/it]11/15/2022 18:40:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.3176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:48 - INFO - train.train_snli_ve - loss is tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3405/16548 [1:34:29<6:12:28,  1.70s/it]11/15/2022 18:40:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.5026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:50 - INFO - train.train_snli_ve - loss is tensor(0.4958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3406/16548 [1:34:31<6:12:14,  1.70s/it]11/15/2022 18:40:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:52 - INFO - train.train_snli_ve - loss is tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3407/16548 [1:34:32<6:10:02,  1.69s/it]11/15/2022 18:40:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:53 - INFO - train.train_snli_ve - loss is tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3408/16548 [1:34:34<6:11:33,  1.70s/it]11/15/2022 18:40:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.6410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:55 - INFO - train.train_snli_ve - loss is tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3409/16548 [1:34:36<6:10:17,  1.69s/it]11/15/2022 18:40:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.3176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:57 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3410/16548 [1:34:38<6:11:52,  1.70s/it]11/15/2022 18:40:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.9300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:40:58 - INFO - train.train_snli_ve - loss is tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3411/16548 [1:34:39<6:08:14,  1.68s/it]11/15/2022 18:41:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.5934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:00 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3412/16548 [1:34:41<6:08:41,  1.68s/it]11/15/2022 18:41:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.5343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:02 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3413/16548 [1:34:43<6:08:07,  1.68s/it]11/15/2022 18:41:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7993e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:03 - INFO - train.train_snli_ve - loss is tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3414/16548 [1:34:44<6:06:50,  1.68s/it]11/15/2022 18:41:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.6609e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:05 - INFO - train.train_snli_ve - loss is tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3415/16548 [1:34:46<6:06:37,  1.68s/it]11/15/2022 18:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.3828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:07 - INFO - train.train_snli_ve - loss is tensor(0.8310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3416/16548 [1:34:48<6:07:20,  1.68s/it]11/15/2022 18:41:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.9600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:09 - INFO - train.train_snli_ve - loss is tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3417/16548 [1:34:49<6:09:36,  1.69s/it]11/15/2022 18:41:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.4838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:10 - INFO - train.train_snli_ve - loss is tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3418/16548 [1:34:51<6:08:26,  1.68s/it]11/15/2022 18:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.2773e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:12 - INFO - train.train_snli_ve - loss is tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3419/16548 [1:34:53<6:07:38,  1.68s/it]11/15/2022 18:41:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.2536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:13 - INFO - train.train_snli_ve - loss is tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3420/16548 [1:34:54<6:04:26,  1.67s/it]11/15/2022 18:41:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.0193e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:15 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3421/16548 [1:34:56<6:04:34,  1.67s/it]11/15/2022 18:41:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8719e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:17 - INFO - train.train_snli_ve - loss is tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3422/16548 [1:34:58<6:05:22,  1.67s/it]11/15/2022 18:41:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:18 - INFO - train.train_snli_ve - loss is tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3423/16548 [1:34:59<6:04:20,  1.67s/it]11/15/2022 18:41:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.2199e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:20 - INFO - train.train_snli_ve - loss is tensor(0.5391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3424/16548 [1:35:01<6:02:51,  1.66s/it]11/15/2022 18:41:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.9213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:22 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3425/16548 [1:35:03<6:05:50,  1.67s/it]11/15/2022 18:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0004e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:24 - INFO - train.train_snli_ve - loss is tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3426/16548 [1:35:04<6:06:22,  1.68s/it]11/15/2022 18:41:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:25 - INFO - train.train_snli_ve - loss is tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3427/16548 [1:35:06<6:04:14,  1.67s/it]11/15/2022 18:41:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5292e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:27 - INFO - train.train_snli_ve - loss is tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3428/16548 [1:35:08<6:02:15,  1.66s/it]11/15/2022 18:41:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:28 - INFO - train.train_snli_ve - loss is tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3429/16548 [1:35:09<6:04:09,  1.67s/it]11/15/2022 18:41:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:30 - INFO - train.train_snli_ve - loss is tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3430/16548 [1:35:11<6:05:27,  1.67s/it]11/15/2022 18:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.8058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:32 - INFO - train.train_snli_ve - loss is tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3431/16548 [1:35:13<6:05:09,  1.67s/it]11/15/2022 18:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:34 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3432/16548 [1:35:14<6:04:41,  1.67s/it]11/15/2022 18:41:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:35 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3433/16548 [1:35:16<6:03:09,  1.66s/it]11/15/2022 18:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:37 - INFO - train.train_snli_ve - loss is tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3434/16548 [1:35:18<6:03:16,  1.66s/it]11/15/2022 18:41:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:38 - INFO - train.train_snli_ve - loss is tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3435/16548 [1:35:19<6:04:00,  1.67s/it]11/15/2022 18:41:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:40 - INFO - train.train_snli_ve - loss is tensor(0.9511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3436/16548 [1:35:21<6:03:23,  1.66s/it]11/15/2022 18:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:42 - INFO - train.train_snli_ve - loss is tensor(0.9193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3437/16548 [1:35:23<6:01:26,  1.65s/it]11/15/2022 18:41:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.3784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:43 - INFO - train.train_snli_ve - loss is tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3438/16548 [1:35:24<6:00:32,  1.65s/it]11/15/2022 18:41:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.3265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:45 - INFO - train.train_snli_ve - loss is tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3439/16548 [1:35:26<6:01:37,  1.66s/it]11/15/2022 18:41:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.8414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:47 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3440/16548 [1:35:27<6:01:42,  1.66s/it]11/15/2022 18:41:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.9494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:48 - INFO - train.train_snli_ve - loss is tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3441/16548 [1:35:29<6:05:31,  1.67s/it]11/15/2022 18:41:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.7556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:50 - INFO - train.train_snli_ve - loss is tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3442/16548 [1:35:31<6:06:04,  1.68s/it]11/15/2022 18:41:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:52 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3443/16548 [1:35:33<6:03:46,  1.67s/it]11/15/2022 18:41:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.3473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:53 - INFO - train.train_snli_ve - loss is tensor(0.8826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3444/16548 [1:35:34<6:02:12,  1.66s/it]11/15/2022 18:41:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.4908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:55 - INFO - train.train_snli_ve - loss is tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3445/16548 [1:35:36<6:03:43,  1.67s/it]11/15/2022 18:41:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:57 - INFO - train.train_snli_ve - loss is tensor(0.8931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3446/16548 [1:35:37<6:02:34,  1.66s/it]11/15/2022 18:41:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.5751e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:41:58 - INFO - train.train_snli_ve - loss is tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3447/16548 [1:35:39<6:03:53,  1.67s/it]11/15/2022 18:42:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.5853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:00 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3448/16548 [1:35:41<6:07:03,  1.68s/it]11/15/2022 18:42:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.6003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:02 - INFO - train.train_snli_ve - loss is tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3449/16548 [1:35:43<6:06:20,  1.68s/it]11/15/2022 18:42:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.1224e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:03 - INFO - train.train_snli_ve - loss is tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3450/16548 [1:35:44<6:05:47,  1.68s/it]11/15/2022 18:42:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.9087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:05 - INFO - train.train_snli_ve - loss is tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3451/16548 [1:35:46<6:05:16,  1.67s/it]11/15/2022 18:42:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.0405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:07 - INFO - train.train_snli_ve - loss is tensor(0.8400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3452/16548 [1:35:48<6:03:23,  1.66s/it]11/15/2022 18:42:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.6682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:08 - INFO - train.train_snli_ve - loss is tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3453/16548 [1:35:49<6:03:21,  1.66s/it]11/15/2022 18:42:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.8898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:10 - INFO - train.train_snli_ve - loss is tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3454/16548 [1:35:51<6:07:20,  1.68s/it]11/15/2022 18:42:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.8957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:12 - INFO - train.train_snli_ve - loss is tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3455/16548 [1:35:53<6:05:08,  1.67s/it]11/15/2022 18:42:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.7800e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:14 - INFO - train.train_snli_ve - loss is tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3456/16548 [1:35:54<6:08:37,  1.69s/it]11/15/2022 18:42:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.8107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:15 - INFO - train.train_snli_ve - loss is tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3457/16548 [1:35:56<6:06:05,  1.68s/it]11/15/2022 18:42:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.9619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:17 - INFO - train.train_snli_ve - loss is tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3458/16548 [1:35:58<6:05:06,  1.67s/it]11/15/2022 18:42:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.0963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:19 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3459/16548 [1:35:59<6:01:58,  1.66s/it]11/15/2022 18:42:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.1571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:20 - INFO - train.train_snli_ve - loss is tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3460/16548 [1:36:01<6:01:04,  1.66s/it]11/15/2022 18:42:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.2788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:22 - INFO - train.train_snli_ve - loss is tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3461/16548 [1:36:03<6:00:40,  1.65s/it]11/15/2022 18:42:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.4689e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:23 - INFO - train.train_snli_ve - loss is tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3462/16548 [1:36:04<5:59:19,  1.65s/it]11/15/2022 18:42:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.2248e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:25 - INFO - train.train_snli_ve - loss is tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3463/16548 [1:36:06<6:01:40,  1.66s/it]11/15/2022 18:42:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.8630e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:27 - INFO - train.train_snli_ve - loss is tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3464/16548 [1:36:08<6:05:33,  1.68s/it]11/15/2022 18:42:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.3597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:29 - INFO - train.train_snli_ve - loss is tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3465/16548 [1:36:09<6:04:24,  1.67s/it]11/15/2022 18:42:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.6260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:30 - INFO - train.train_snli_ve - loss is tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3466/16548 [1:36:11<6:02:13,  1.66s/it]11/15/2022 18:42:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.3479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:32 - INFO - train.train_snli_ve - loss is tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3467/16548 [1:36:13<6:01:09,  1.66s/it]11/15/2022 18:42:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:33 - INFO - train.train_snli_ve - loss is tensor(0.5167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3468/16548 [1:36:14<6:02:32,  1.66s/it]11/15/2022 18:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:35 - INFO - train.train_snli_ve - loss is tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3469/16548 [1:36:16<6:04:42,  1.67s/it]11/15/2022 18:42:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.7577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:37 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3470/16548 [1:36:18<6:04:10,  1.67s/it]11/15/2022 18:42:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.7798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:39 - INFO - train.train_snli_ve - loss is tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3471/16548 [1:36:19<6:06:11,  1.68s/it]11/15/2022 18:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:40 - INFO - train.train_snli_ve - loss is tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3472/16548 [1:36:21<6:04:23,  1.67s/it]11/15/2022 18:42:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:42 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3473/16548 [1:36:23<6:04:58,  1.67s/it]11/15/2022 18:42:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.2906e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:44 - INFO - train.train_snli_ve - loss is tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3474/16548 [1:36:24<6:05:13,  1.68s/it]11/15/2022 18:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.6708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:45 - INFO - train.train_snli_ve - loss is tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3475/16548 [1:36:26<6:03:15,  1.67s/it]11/15/2022 18:42:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:47 - INFO - train.train_snli_ve - loss is tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3476/16548 [1:36:28<6:07:43,  1.69s/it]11/15/2022 18:42:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.9519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:49 - INFO - train.train_snli_ve - loss is tensor(0.9008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3477/16548 [1:36:29<6:08:35,  1.69s/it]11/15/2022 18:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:50 - INFO - train.train_snli_ve - loss is tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3478/16548 [1:36:31<6:07:26,  1.69s/it]11/15/2022 18:42:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.4514e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:52 - INFO - train.train_snli_ve - loss is tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3479/16548 [1:36:33<6:05:51,  1.68s/it]11/15/2022 18:42:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:54 - INFO - train.train_snli_ve - loss is tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3480/16548 [1:36:34<6:05:30,  1.68s/it]11/15/2022 18:42:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.2323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:55 - INFO - train.train_snli_ve - loss is tensor(0.8592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3481/16548 [1:36:36<6:04:46,  1.67s/it]11/15/2022 18:42:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.2478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:57 - INFO - train.train_snli_ve - loss is tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3482/16548 [1:36:38<6:05:36,  1.68s/it]11/15/2022 18:42:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:42:59 - INFO - train.train_snli_ve - loss is tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3483/16548 [1:36:39<6:06:47,  1.68s/it]11/15/2022 18:43:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:00 - INFO - train.train_snli_ve - loss is tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3484/16548 [1:36:41<6:06:10,  1.68s/it]11/15/2022 18:43:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.3545e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:02 - INFO - train.train_snli_ve - loss is tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3485/16548 [1:36:43<6:06:36,  1.68s/it]11/15/2022 18:43:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.5784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:04 - INFO - train.train_snli_ve - loss is tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3486/16548 [1:36:45<6:08:47,  1.69s/it]11/15/2022 18:43:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.4343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:05 - INFO - train.train_snli_ve - loss is tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3487/16548 [1:36:46<6:10:53,  1.70s/it]11/15/2022 18:43:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.0369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:07 - INFO - train.train_snli_ve - loss is tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3488/16548 [1:36:48<6:08:53,  1.69s/it]11/15/2022 18:43:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.7731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:09 - INFO - train.train_snli_ve - loss is tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3489/16548 [1:36:50<6:06:20,  1.68s/it]11/15/2022 18:43:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.8056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:11 - INFO - train.train_snli_ve - loss is tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3490/16548 [1:36:51<6:06:37,  1.68s/it]11/15/2022 18:43:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.9515e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:12 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3491/16548 [1:36:53<6:07:00,  1.69s/it]11/15/2022 18:43:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.4354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:14 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3492/16548 [1:36:55<6:08:16,  1.69s/it]11/15/2022 18:43:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.0992e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:16 - INFO - train.train_snli_ve - loss is tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3493/16548 [1:36:56<6:08:18,  1.69s/it]11/15/2022 18:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.2918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:17 - INFO - train.train_snli_ve - loss is tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3494/16548 [1:36:58<6:09:26,  1.70s/it]11/15/2022 18:43:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.0713e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:19 - INFO - train.train_snli_ve - loss is tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3495/16548 [1:37:00<6:07:50,  1.69s/it]11/15/2022 18:43:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.8303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:21 - INFO - train.train_snli_ve - loss is tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3496/16548 [1:37:01<6:11:13,  1.71s/it]11/15/2022 18:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.4036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:22 - INFO - train.train_snli_ve - loss is tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3497/16548 [1:37:03<6:10:57,  1.71s/it]11/15/2022 18:43:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.1251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:24 - INFO - train.train_snli_ve - loss is tensor(0.8664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3498/16548 [1:37:05<6:11:47,  1.71s/it]11/15/2022 18:43:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.5118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:26 - INFO - train.train_snli_ve - loss is tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3499/16548 [1:37:07<6:07:09,  1.69s/it]11/15/2022 18:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.9541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:27 - INFO - train.train_snli_ve - loss is tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3500/16548 [1:37:08<6:09:20,  1.70s/it]11/15/2022 18:43:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.7019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:29 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3501/16548 [1:37:10<6:09:46,  1.70s/it]11/15/2022 18:43:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.2018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:31 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3502/16548 [1:37:12<6:08:43,  1.70s/it]11/15/2022 18:43:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.4414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:33 - INFO - train.train_snli_ve - loss is tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3503/16548 [1:37:13<6:06:22,  1.69s/it]11/15/2022 18:43:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.2112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:34 - INFO - train.train_snli_ve - loss is tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3504/16548 [1:37:15<6:03:23,  1.67s/it]11/15/2022 18:43:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.4099e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:36 - INFO - train.train_snli_ve - loss is tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3505/16548 [1:37:17<6:02:21,  1.67s/it]11/15/2022 18:43:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.1527e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:38 - INFO - train.train_snli_ve - loss is tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3506/16548 [1:37:18<6:04:05,  1.68s/it]11/15/2022 18:43:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.8928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:39 - INFO - train.train_snli_ve - loss is tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3507/16548 [1:37:20<6:03:49,  1.67s/it]11/15/2022 18:43:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.8436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:41 - INFO - train.train_snli_ve - loss is tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3508/16548 [1:37:22<6:02:06,  1.67s/it]11/15/2022 18:43:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.1783e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:43 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3509/16548 [1:37:23<6:04:31,  1.68s/it]11/15/2022 18:43:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.0382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:44 - INFO - train.train_snli_ve - loss is tensor(0.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3510/16548 [1:37:25<6:03:17,  1.67s/it]11/15/2022 18:43:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.4580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:46 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3511/16548 [1:37:27<6:02:56,  1.67s/it]11/15/2022 18:43:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.2136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:48 - INFO - train.train_snli_ve - loss is tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3512/16548 [1:37:28<6:02:14,  1.67s/it]11/15/2022 18:43:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.5174e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:49 - INFO - train.train_snli_ve - loss is tensor(0.8351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3513/16548 [1:37:30<6:01:36,  1.66s/it]11/15/2022 18:43:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.8959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:51 - INFO - train.train_snli_ve - loss is tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3514/16548 [1:37:32<6:06:10,  1.69s/it]11/15/2022 18:43:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.3760e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:53 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3515/16548 [1:37:33<6:06:32,  1.69s/it]11/15/2022 18:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.6327e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:54 - INFO - train.train_snli_ve - loss is tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3516/16548 [1:37:35<6:06:38,  1.69s/it]11/15/2022 18:43:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.4302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:56 - INFO - train.train_snli_ve - loss is tensor(0.7435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3517/16548 [1:37:37<6:03:26,  1.67s/it]11/15/2022 18:43:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:58 - INFO - train.train_snli_ve - loss is tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3518/16548 [1:37:38<6:03:33,  1.67s/it]11/15/2022 18:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.2943e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:43:59 - INFO - train.train_snli_ve - loss is tensor(0.8376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3519/16548 [1:37:40<6:05:32,  1.68s/it]11/15/2022 18:44:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.3491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:01 - INFO - train.train_snli_ve - loss is tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3520/16548 [1:37:42<6:06:50,  1.69s/it]11/15/2022 18:44:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.8231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:03 - INFO - train.train_snli_ve - loss is tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3521/16548 [1:37:43<6:02:40,  1.67s/it]11/15/2022 18:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:04 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3522/16548 [1:37:45<6:03:27,  1.67s/it]11/15/2022 18:44:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.0472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:06 - INFO - train.train_snli_ve - loss is tensor(0.7034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3523/16548 [1:37:47<6:01:34,  1.67s/it]11/15/2022 18:44:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.9579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:08 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3524/16548 [1:37:48<6:01:03,  1.66s/it]11/15/2022 18:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.2619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:09 - INFO - train.train_snli_ve - loss is tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3525/16548 [1:37:50<6:02:59,  1.67s/it]11/15/2022 18:44:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.5138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:11 - INFO - train.train_snli_ve - loss is tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3526/16548 [1:37:52<6:01:30,  1.67s/it]11/15/2022 18:44:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.1512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:13 - INFO - train.train_snli_ve - loss is tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3527/16548 [1:37:53<6:03:18,  1.67s/it]11/15/2022 18:44:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.2729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:14 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3528/16548 [1:37:55<6:00:42,  1.66s/it]11/15/2022 18:44:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.8862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:16 - INFO - train.train_snli_ve - loss is tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3529/16548 [1:37:57<6:02:17,  1.67s/it]11/15/2022 18:44:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.2714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:18 - INFO - train.train_snli_ve - loss is tensor(0.8001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3530/16548 [1:37:58<6:04:12,  1.68s/it]11/15/2022 18:44:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.0276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:19 - INFO - train.train_snli_ve - loss is tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3531/16548 [1:38:00<6:02:46,  1.67s/it]11/15/2022 18:44:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.9883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:21 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3532/16548 [1:38:02<6:01:06,  1.66s/it]11/15/2022 18:44:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.7799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:23 - INFO - train.train_snli_ve - loss is tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3533/16548 [1:38:03<5:59:19,  1.66s/it]11/15/2022 18:44:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:24 - INFO - train.train_snli_ve - loss is tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3534/16548 [1:38:05<5:59:34,  1.66s/it]11/15/2022 18:44:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.9550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:26 - INFO - train.train_snli_ve - loss is tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3535/16548 [1:38:07<5:59:24,  1.66s/it]11/15/2022 18:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.6911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:28 - INFO - train.train_snli_ve - loss is tensor(0.9562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3536/16548 [1:38:08<6:02:00,  1.67s/it]11/15/2022 18:44:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.4101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:29 - INFO - train.train_snli_ve - loss is tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3537/16548 [1:38:10<6:01:13,  1.67s/it]11/15/2022 18:44:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.1342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:31 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3538/16548 [1:38:12<6:00:29,  1.66s/it]11/15/2022 18:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.9124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:33 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3539/16548 [1:38:13<5:59:08,  1.66s/it]11/15/2022 18:44:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.6137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:34 - INFO - train.train_snli_ve - loss is tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3540/16548 [1:38:15<6:01:32,  1.67s/it]11/15/2022 18:44:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.2987e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:36 - INFO - train.train_snli_ve - loss is tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3541/16548 [1:38:17<6:00:40,  1.66s/it]11/15/2022 18:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.5452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:38 - INFO - train.train_snli_ve - loss is tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3542/16548 [1:38:18<6:01:57,  1.67s/it]11/15/2022 18:44:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.4476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:39 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3543/16548 [1:38:20<6:02:03,  1.67s/it]11/15/2022 18:44:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.3415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:41 - INFO - train.train_snli_ve - loss is tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3544/16548 [1:38:22<6:01:15,  1.67s/it]11/15/2022 18:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.4714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:43 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3545/16548 [1:38:23<6:01:26,  1.67s/it]11/15/2022 18:44:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.7167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:44 - INFO - train.train_snli_ve - loss is tensor(0.8167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3546/16548 [1:38:25<6:00:43,  1.66s/it]11/15/2022 18:44:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.0834e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:46 - INFO - train.train_snli_ve - loss is tensor(0.8331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3547/16548 [1:38:27<6:00:27,  1.66s/it]11/15/2022 18:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.4356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:48 - INFO - train.train_snli_ve - loss is tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3548/16548 [1:38:28<6:01:21,  1.67s/it]11/15/2022 18:44:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.4947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:49 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3549/16548 [1:38:30<6:00:47,  1.67s/it]11/15/2022 18:44:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.2644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:51 - INFO - train.train_snli_ve - loss is tensor(0.9042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3550/16548 [1:38:32<6:01:22,  1.67s/it]11/15/2022 18:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.3778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:53 - INFO - train.train_snli_ve - loss is tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3551/16548 [1:38:33<6:00:30,  1.66s/it]11/15/2022 18:44:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.4169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:54 - INFO - train.train_snli_ve - loss is tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3552/16548 [1:38:35<6:03:18,  1.68s/it]11/15/2022 18:44:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.8755e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:56 - INFO - train.train_snli_ve - loss is tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3553/16548 [1:38:37<6:03:24,  1.68s/it]11/15/2022 18:44:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.8087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:58 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3554/16548 [1:38:38<6:02:01,  1.67s/it]11/15/2022 18:44:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.0575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:44:59 - INFO - train.train_snli_ve - loss is tensor(0.8214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3555/16548 [1:38:40<6:04:20,  1.68s/it]11/15/2022 18:45:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.0680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:01 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3556/16548 [1:38:42<6:06:27,  1.69s/it]11/15/2022 18:45:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.1448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:03 - INFO - train.train_snli_ve - loss is tensor(0.5325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21% 3557/16548 [1:38:44<6:04:46,  1.68s/it]11/15/2022 18:45:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.0211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:04 - INFO - train.train_snli_ve - loss is tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3558/16548 [1:38:45<6:04:45,  1.68s/it]11/15/2022 18:45:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.7294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:06 - INFO - train.train_snli_ve - loss is tensor(0.7850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3559/16548 [1:38:47<6:02:11,  1.67s/it]11/15/2022 18:45:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.6993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:08 - INFO - train.train_snli_ve - loss is tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3560/16548 [1:38:49<6:01:53,  1.67s/it]11/15/2022 18:45:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.9408e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:09 - INFO - train.train_snli_ve - loss is tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3561/16548 [1:38:50<6:01:34,  1.67s/it]11/15/2022 18:45:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.8867e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:11 - INFO - train.train_snli_ve - loss is tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3562/16548 [1:38:52<6:03:48,  1.68s/it]11/15/2022 18:45:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.7510e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:13 - INFO - train.train_snli_ve - loss is tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3563/16548 [1:38:54<6:04:19,  1.68s/it]11/15/2022 18:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.0526e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:15 - INFO - train.train_snli_ve - loss is tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3564/16548 [1:38:55<6:05:15,  1.69s/it]11/15/2022 18:45:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.6326e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:16 - INFO - train.train_snli_ve - loss is tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3565/16548 [1:38:57<6:06:08,  1.69s/it]11/15/2022 18:45:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.3190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:18 - INFO - train.train_snli_ve - loss is tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3566/16548 [1:38:59<6:05:32,  1.69s/it]11/15/2022 18:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.3597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:20 - INFO - train.train_snli_ve - loss is tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3567/16548 [1:39:00<6:09:21,  1.71s/it]11/15/2022 18:45:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.9186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:21 - INFO - train.train_snli_ve - loss is tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3568/16548 [1:39:02<6:09:20,  1.71s/it]11/15/2022 18:45:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.5379e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:23 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3569/16548 [1:39:04<6:08:01,  1.70s/it]11/15/2022 18:45:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.7317e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:25 - INFO - train.train_snli_ve - loss is tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3570/16548 [1:39:06<6:06:32,  1.69s/it]11/15/2022 18:45:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.5551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:26 - INFO - train.train_snli_ve - loss is tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3571/16548 [1:39:07<6:03:35,  1.68s/it]11/15/2022 18:45:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.2947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:28 - INFO - train.train_snli_ve - loss is tensor(0.9269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3572/16548 [1:39:09<6:06:21,  1.69s/it]11/15/2022 18:45:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.9073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:30 - INFO - train.train_snli_ve - loss is tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3573/16548 [1:39:11<6:05:14,  1.69s/it]11/15/2022 18:45:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.1350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:31 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3574/16548 [1:39:12<6:02:57,  1.68s/it]11/15/2022 18:45:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.2766e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:33 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3575/16548 [1:39:14<6:09:32,  1.71s/it]11/15/2022 18:45:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.7673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:35 - INFO - train.train_snli_ve - loss is tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3576/16548 [1:39:16<6:05:51,  1.69s/it]11/15/2022 18:45:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.1251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:37 - INFO - train.train_snli_ve - loss is tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3577/16548 [1:39:17<6:05:17,  1.69s/it]11/15/2022 18:45:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.5611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:38 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3578/16548 [1:39:19<6:07:48,  1.70s/it]11/15/2022 18:45:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.7723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:40 - INFO - train.train_snli_ve - loss is tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3579/16548 [1:39:21<6:02:30,  1.68s/it]11/15/2022 18:45:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.1804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:42 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3580/16548 [1:39:22<6:03:26,  1.68s/it]11/15/2022 18:45:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.6436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:43 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3581/16548 [1:39:24<6:04:27,  1.69s/it]11/15/2022 18:45:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.8916e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:45 - INFO - train.train_snli_ve - loss is tensor(0.8787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3582/16548 [1:39:26<6:06:11,  1.69s/it]11/15/2022 18:45:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.4819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:47 - INFO - train.train_snli_ve - loss is tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3583/16548 [1:39:27<6:06:24,  1.70s/it]11/15/2022 18:45:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.8436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:48 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3584/16548 [1:39:29<6:10:03,  1.71s/it]11/15/2022 18:45:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:50 - INFO - train.train_snli_ve - loss is tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3585/16548 [1:39:31<6:10:24,  1.71s/it]11/15/2022 18:45:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.3832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:52 - INFO - train.train_snli_ve - loss is tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3586/16548 [1:39:33<6:09:39,  1.71s/it]11/15/2022 18:45:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.2674e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:54 - INFO - train.train_snli_ve - loss is tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3587/16548 [1:39:34<6:10:23,  1.71s/it]11/15/2022 18:45:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.1059e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:55 - INFO - train.train_snli_ve - loss is tensor(0.5375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3588/16548 [1:39:36<6:10:43,  1.72s/it]11/15/2022 18:45:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.2672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:57 - INFO - train.train_snli_ve - loss is tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3589/16548 [1:39:38<6:08:44,  1.71s/it]11/15/2022 18:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.4517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:45:59 - INFO - train.train_snli_ve - loss is tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3590/16548 [1:39:39<6:06:47,  1.70s/it]11/15/2022 18:46:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.6594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:00 - INFO - train.train_snli_ve - loss is tensor(0.8473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3591/16548 [1:39:41<6:05:14,  1.69s/it]11/15/2022 18:46:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.8924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:02 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3592/16548 [1:39:43<6:03:55,  1.69s/it]11/15/2022 18:46:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.7907e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:04 - INFO - train.train_snli_ve - loss is tensor(0.8315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3593/16548 [1:39:44<6:03:02,  1.68s/it]11/15/2022 18:46:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.9196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:05 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3594/16548 [1:39:46<6:03:07,  1.68s/it]11/15/2022 18:46:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:07 - INFO - train.train_snli_ve - loss is tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3595/16548 [1:39:48<6:02:10,  1.68s/it]11/15/2022 18:46:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.3136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:09 - INFO - train.train_snli_ve - loss is tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3596/16548 [1:39:50<6:04:36,  1.69s/it]11/15/2022 18:46:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.6060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:10 - INFO - train.train_snli_ve - loss is tensor(0.8127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3597/16548 [1:39:51<6:04:46,  1.69s/it]11/15/2022 18:46:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.7871e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:12 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3598/16548 [1:39:53<6:03:52,  1.69s/it]11/15/2022 18:46:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.7680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:14 - INFO - train.train_snli_ve - loss is tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3599/16548 [1:39:55<6:03:56,  1.69s/it]11/15/2022 18:46:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.7163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:16 - INFO - train.train_snli_ve - loss is tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3600/16548 [1:39:56<6:06:43,  1.70s/it]11/15/2022 18:46:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.6372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:17 - INFO - train.train_snli_ve - loss is tensor(0.5876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3601/16548 [1:39:58<6:03:58,  1.69s/it]11/15/2022 18:46:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.5982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:19 - INFO - train.train_snli_ve - loss is tensor(0.9421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3602/16548 [1:40:00<6:03:14,  1.68s/it]11/15/2022 18:46:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.4129e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:21 - INFO - train.train_snli_ve - loss is tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3603/16548 [1:40:01<6:01:31,  1.68s/it]11/15/2022 18:46:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.7858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:22 - INFO - train.train_snli_ve - loss is tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3604/16548 [1:40:03<6:02:00,  1.68s/it]11/15/2022 18:46:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.0461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:24 - INFO - train.train_snli_ve - loss is tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3605/16548 [1:40:05<6:00:11,  1.67s/it]11/15/2022 18:46:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.1478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:26 - INFO - train.train_snli_ve - loss is tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3606/16548 [1:40:06<6:03:16,  1.68s/it]11/15/2022 18:46:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.2256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:27 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3607/16548 [1:40:08<6:00:14,  1.67s/it]11/15/2022 18:46:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.4584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:29 - INFO - train.train_snli_ve - loss is tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3608/16548 [1:40:10<5:59:52,  1.67s/it]11/15/2022 18:46:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.0301e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:31 - INFO - train.train_snli_ve - loss is tensor(0.5908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3609/16548 [1:40:11<6:01:02,  1.67s/it]11/15/2022 18:46:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.0546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:32 - INFO - train.train_snli_ve - loss is tensor(0.4547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3610/16548 [1:40:13<6:05:45,  1.70s/it]11/15/2022 18:46:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.8284e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:34 - INFO - train.train_snli_ve - loss is tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3611/16548 [1:40:15<6:05:25,  1.69s/it]11/15/2022 18:46:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.7016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:36 - INFO - train.train_snli_ve - loss is tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3612/16548 [1:40:17<6:08:25,  1.71s/it]11/15/2022 18:46:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.9397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:37 - INFO - train.train_snli_ve - loss is tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3613/16548 [1:40:18<6:06:10,  1.70s/it]11/15/2022 18:46:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.9450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:39 - INFO - train.train_snli_ve - loss is tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3614/16548 [1:40:20<6:02:42,  1.68s/it]11/15/2022 18:46:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.2705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:41 - INFO - train.train_snli_ve - loss is tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3615/16548 [1:40:22<6:02:13,  1.68s/it]11/15/2022 18:46:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.8681e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:42 - INFO - train.train_snli_ve - loss is tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3616/16548 [1:40:23<6:03:17,  1.69s/it]11/15/2022 18:46:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.1387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:44 - INFO - train.train_snli_ve - loss is tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3617/16548 [1:40:25<6:04:38,  1.69s/it]11/15/2022 18:46:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.3658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:46 - INFO - train.train_snli_ve - loss is tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3618/16548 [1:40:27<6:03:42,  1.69s/it]11/15/2022 18:46:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.1316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:48 - INFO - train.train_snli_ve - loss is tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3619/16548 [1:40:28<6:03:17,  1.69s/it]11/15/2022 18:46:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.6904e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:49 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3620/16548 [1:40:30<6:06:39,  1.70s/it]11/15/2022 18:46:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.5797e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:51 - INFO - train.train_snli_ve - loss is tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3621/16548 [1:40:32<6:04:52,  1.69s/it]11/15/2022 18:46:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.4297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:53 - INFO - train.train_snli_ve - loss is tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3622/16548 [1:40:33<6:05:12,  1.70s/it]11/15/2022 18:46:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.3824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:54 - INFO - train.train_snli_ve - loss is tensor(0.7955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3623/16548 [1:40:35<6:06:48,  1.70s/it]11/15/2022 18:46:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.9259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:56 - INFO - train.train_snli_ve - loss is tensor(0.9058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3624/16548 [1:40:37<6:08:44,  1.71s/it]11/15/2022 18:46:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.4361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:46:58 - INFO - train.train_snli_ve - loss is tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3625/16548 [1:40:39<6:08:27,  1.71s/it]11/15/2022 18:47:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.9616e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:00 - INFO - train.train_snli_ve - loss is tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3626/16548 [1:40:40<6:09:28,  1.72s/it]11/15/2022 18:47:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.4074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:01 - INFO - train.train_snli_ve - loss is tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3627/16548 [1:40:42<6:07:01,  1.70s/it]11/15/2022 18:47:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.1353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:03 - INFO - train.train_snli_ve - loss is tensor(0.9144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3628/16548 [1:40:44<6:03:06,  1.69s/it]11/15/2022 18:47:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.3662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:05 - INFO - train.train_snli_ve - loss is tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3629/16548 [1:40:45<6:00:24,  1.67s/it]11/15/2022 18:47:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.2694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:06 - INFO - train.train_snli_ve - loss is tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3630/16548 [1:40:47<6:04:36,  1.69s/it]11/15/2022 18:47:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.6874e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:08 - INFO - train.train_snli_ve - loss is tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3631/16548 [1:40:49<6:03:20,  1.69s/it]11/15/2022 18:47:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.2597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:10 - INFO - train.train_snli_ve - loss is tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3632/16548 [1:40:50<6:02:47,  1.69s/it]11/15/2022 18:47:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.6103e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:11 - INFO - train.train_snli_ve - loss is tensor(0.8103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3633/16548 [1:40:52<6:01:53,  1.68s/it]11/15/2022 18:47:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.1468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:13 - INFO - train.train_snli_ve - loss is tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3634/16548 [1:40:54<6:02:43,  1.69s/it]11/15/2022 18:47:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.5000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:15 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3635/16548 [1:40:55<6:05:51,  1.70s/it]11/15/2022 18:47:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.2863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:16 - INFO - train.train_snli_ve - loss is tensor(0.5404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3636/16548 [1:40:57<6:07:41,  1.71s/it]11/15/2022 18:47:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.8223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:18 - INFO - train.train_snli_ve - loss is tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3637/16548 [1:40:59<6:07:57,  1.71s/it]11/15/2022 18:47:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.2461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:20 - INFO - train.train_snli_ve - loss is tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3638/16548 [1:41:01<6:06:07,  1.70s/it]11/15/2022 18:47:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.5979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:22 - INFO - train.train_snli_ve - loss is tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3639/16548 [1:41:02<6:04:15,  1.69s/it]11/15/2022 18:47:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.3404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:23 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3640/16548 [1:41:04<6:05:12,  1.70s/it]11/15/2022 18:47:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.2826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:25 - INFO - train.train_snli_ve - loss is tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3641/16548 [1:41:06<6:01:42,  1.68s/it]11/15/2022 18:47:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.8182e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:27 - INFO - train.train_snli_ve - loss is tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3642/16548 [1:41:07<6:05:21,  1.70s/it]11/15/2022 18:47:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.9940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:28 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3643/16548 [1:41:09<6:04:51,  1.70s/it]11/15/2022 18:47:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.7815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:30 - INFO - train.train_snli_ve - loss is tensor(0.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3644/16548 [1:41:11<6:03:06,  1.69s/it]11/15/2022 18:47:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.0883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:32 - INFO - train.train_snli_ve - loss is tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3645/16548 [1:41:12<5:59:41,  1.67s/it]11/15/2022 18:47:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.5192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:33 - INFO - train.train_snli_ve - loss is tensor(0.9049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3646/16548 [1:41:14<6:00:57,  1.68s/it]11/15/2022 18:47:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.2011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:35 - INFO - train.train_snli_ve - loss is tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3647/16548 [1:41:16<5:59:58,  1.67s/it]11/15/2022 18:47:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.9801e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:37 - INFO - train.train_snli_ve - loss is tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3648/16548 [1:41:17<6:00:37,  1.68s/it]11/15/2022 18:47:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.3654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:38 - INFO - train.train_snli_ve - loss is tensor(0.8915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3649/16548 [1:41:19<5:58:50,  1.67s/it]11/15/2022 18:47:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.6453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:40 - INFO - train.train_snli_ve - loss is tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3650/16548 [1:41:21<6:02:20,  1.69s/it]11/15/2022 18:47:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.8913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:42 - INFO - train.train_snli_ve - loss is tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3651/16548 [1:41:22<6:04:07,  1.69s/it]11/15/2022 18:47:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.1687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:43 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3652/16548 [1:41:24<6:03:43,  1.69s/it]11/15/2022 18:47:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.9122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:45 - INFO - train.train_snli_ve - loss is tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3653/16548 [1:41:26<6:03:38,  1.69s/it]11/15/2022 18:47:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.0548e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:47 - INFO - train.train_snli_ve - loss is tensor(0.8329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3654/16548 [1:41:28<6:03:44,  1.69s/it]11/15/2022 18:47:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.1512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:49 - INFO - train.train_snli_ve - loss is tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3655/16548 [1:41:29<6:04:13,  1.69s/it]11/15/2022 18:47:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.1809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:50 - INFO - train.train_snli_ve - loss is tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3656/16548 [1:41:31<6:03:26,  1.69s/it]11/15/2022 18:47:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.0947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:52 - INFO - train.train_snli_ve - loss is tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3657/16548 [1:41:33<6:06:02,  1.70s/it]11/15/2022 18:47:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.2026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:54 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3658/16548 [1:41:34<6:10:42,  1.73s/it]11/15/2022 18:47:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.3352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:55 - INFO - train.train_snli_ve - loss is tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3659/16548 [1:41:36<6:06:23,  1.71s/it]11/15/2022 18:47:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.5622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:57 - INFO - train.train_snli_ve - loss is tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3660/16548 [1:41:38<6:09:44,  1.72s/it]11/15/2022 18:47:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.9047e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:47:59 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3661/16548 [1:41:40<6:05:51,  1.70s/it]11/15/2022 18:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.5003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:01 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3662/16548 [1:41:41<6:06:41,  1.71s/it]11/15/2022 18:48:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.5387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:02 - INFO - train.train_snli_ve - loss is tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3663/16548 [1:41:43<6:04:14,  1.70s/it]11/15/2022 18:48:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.4250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:04 - INFO - train.train_snli_ve - loss is tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3664/16548 [1:41:45<6:02:43,  1.69s/it]11/15/2022 18:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.9538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:06 - INFO - train.train_snli_ve - loss is tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3665/16548 [1:41:46<6:02:21,  1.69s/it]11/15/2022 18:48:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.1610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:07 - INFO - train.train_snli_ve - loss is tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3666/16548 [1:41:48<5:58:58,  1.67s/it]11/15/2022 18:48:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.9486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:09 - INFO - train.train_snli_ve - loss is tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3667/16548 [1:41:50<5:59:45,  1.68s/it]11/15/2022 18:48:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.4347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:11 - INFO - train.train_snli_ve - loss is tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3668/16548 [1:41:51<5:58:44,  1.67s/it]11/15/2022 18:48:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:12 - INFO - train.train_snli_ve - loss is tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3669/16548 [1:41:53<6:00:06,  1.68s/it]11/15/2022 18:48:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:14 - INFO - train.train_snli_ve - loss is tensor(0.9369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3670/16548 [1:41:55<6:01:16,  1.68s/it]11/15/2022 18:48:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.4631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:16 - INFO - train.train_snli_ve - loss is tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3671/16548 [1:41:56<6:02:20,  1.69s/it]11/15/2022 18:48:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.6004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:17 - INFO - train.train_snli_ve - loss is tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3672/16548 [1:41:58<6:02:21,  1.69s/it]11/15/2022 18:48:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.0361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:19 - INFO - train.train_snli_ve - loss is tensor(0.4590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3673/16548 [1:42:00<6:02:51,  1.69s/it]11/15/2022 18:48:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2007e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:21 - INFO - train.train_snli_ve - loss is tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3674/16548 [1:42:01<6:04:08,  1.70s/it]11/15/2022 18:48:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.8516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:22 - INFO - train.train_snli_ve - loss is tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3675/16548 [1:42:03<6:01:06,  1.68s/it]11/15/2022 18:48:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.2828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:24 - INFO - train.train_snli_ve - loss is tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3676/16548 [1:42:05<6:00:42,  1.68s/it]11/15/2022 18:48:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.7995e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:26 - INFO - train.train_snli_ve - loss is tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3677/16548 [1:42:06<6:00:59,  1.68s/it]11/15/2022 18:48:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.0951e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:27 - INFO - train.train_snli_ve - loss is tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3678/16548 [1:42:08<6:02:09,  1.69s/it]11/15/2022 18:48:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.4322e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:29 - INFO - train.train_snli_ve - loss is tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3679/16548 [1:42:10<6:00:50,  1.68s/it]11/15/2022 18:48:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.4621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:31 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3680/16548 [1:42:12<6:01:00,  1.68s/it]11/15/2022 18:48:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.9777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:32 - INFO - train.train_snli_ve - loss is tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3681/16548 [1:42:13<6:00:14,  1.68s/it]11/15/2022 18:48:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:34 - INFO - train.train_snli_ve - loss is tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3682/16548 [1:42:15<5:57:56,  1.67s/it]11/15/2022 18:48:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.4680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:36 - INFO - train.train_snli_ve - loss is tensor(0.9775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3683/16548 [1:42:16<5:57:23,  1.67s/it]11/15/2022 18:48:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.0523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:37 - INFO - train.train_snli_ve - loss is tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3684/16548 [1:42:18<5:56:42,  1.66s/it]11/15/2022 18:48:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1342e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:39 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3685/16548 [1:42:20<5:58:55,  1.67s/it]11/15/2022 18:48:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.6007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:41 - INFO - train.train_snli_ve - loss is tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3686/16548 [1:42:22<5:59:29,  1.68s/it]11/15/2022 18:48:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.8389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:42 - INFO - train.train_snli_ve - loss is tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3687/16548 [1:42:23<5:59:57,  1.68s/it]11/15/2022 18:48:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.9330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:44 - INFO - train.train_snli_ve - loss is tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3688/16548 [1:42:25<5:56:52,  1.67s/it]11/15/2022 18:48:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.8340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:46 - INFO - train.train_snli_ve - loss is tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3689/16548 [1:42:27<5:57:52,  1.67s/it]11/15/2022 18:48:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.0582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:47 - INFO - train.train_snli_ve - loss is tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3690/16548 [1:42:28<5:57:59,  1.67s/it]11/15/2022 18:48:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.7898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:49 - INFO - train.train_snli_ve - loss is tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3691/16548 [1:42:30<5:59:14,  1.68s/it]11/15/2022 18:48:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:51 - INFO - train.train_snli_ve - loss is tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3692/16548 [1:42:32<6:01:41,  1.69s/it]11/15/2022 18:48:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:53 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3693/16548 [1:42:33<6:00:58,  1.68s/it]11/15/2022 18:48:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:54 - INFO - train.train_snli_ve - loss is tensor(0.8537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3694/16548 [1:42:35<5:59:50,  1.68s/it]11/15/2022 18:48:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.3848e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:56 - INFO - train.train_snli_ve - loss is tensor(0.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3695/16548 [1:42:37<6:01:42,  1.69s/it]11/15/2022 18:48:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.7298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:58 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3696/16548 [1:42:38<6:02:05,  1.69s/it]11/15/2022 18:48:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.2828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:48:59 - INFO - train.train_snli_ve - loss is tensor(0.9280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3697/16548 [1:42:40<6:02:50,  1.69s/it]11/15/2022 18:49:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.3231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:01 - INFO - train.train_snli_ve - loss is tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3698/16548 [1:42:42<6:02:44,  1.69s/it]11/15/2022 18:49:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.6994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:03 - INFO - train.train_snli_ve - loss is tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3699/16548 [1:42:43<6:04:03,  1.70s/it]11/15/2022 18:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.3652e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:04 - INFO - train.train_snli_ve - loss is tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3700/16548 [1:42:45<6:14:21,  1.75s/it]11/15/2022 18:49:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.2730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:06 - INFO - train.train_snli_ve - loss is tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3701/16548 [1:42:47<6:09:42,  1.73s/it]11/15/2022 18:49:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.8661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:08 - INFO - train.train_snli_ve - loss is tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3702/16548 [1:42:49<6:05:59,  1.71s/it]11/15/2022 18:49:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:10 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3703/16548 [1:42:50<6:02:06,  1.69s/it]11/15/2022 18:49:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.0412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:11 - INFO - train.train_snli_ve - loss is tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3704/16548 [1:42:52<6:00:10,  1.68s/it]11/15/2022 18:49:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:13 - INFO - train.train_snli_ve - loss is tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3705/16548 [1:42:54<5:57:59,  1.67s/it]11/15/2022 18:49:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.2567e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:15 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3706/16548 [1:42:55<5:55:18,  1.66s/it]11/15/2022 18:49:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.2907e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:16 - INFO - train.train_snli_ve - loss is tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3707/16548 [1:42:57<5:58:28,  1.67s/it]11/15/2022 18:49:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1775e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:18 - INFO - train.train_snli_ve - loss is tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3708/16548 [1:42:59<5:57:39,  1.67s/it]11/15/2022 18:49:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:20 - INFO - train.train_snli_ve - loss is tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3709/16548 [1:43:00<5:59:57,  1.68s/it]11/15/2022 18:49:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.4203e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:21 - INFO - train.train_snli_ve - loss is tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3710/16548 [1:43:02<6:01:50,  1.69s/it]11/15/2022 18:49:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.2542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:23 - INFO - train.train_snli_ve - loss is tensor(0.9537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3711/16548 [1:43:04<6:02:41,  1.70s/it]11/15/2022 18:49:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:25 - INFO - train.train_snli_ve - loss is tensor(0.9855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3712/16548 [1:43:05<6:04:31,  1.70s/it]11/15/2022 18:49:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.1224e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:26 - INFO - train.train_snli_ve - loss is tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3713/16548 [1:43:07<6:03:49,  1.70s/it]11/15/2022 18:49:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.8280e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:28 - INFO - train.train_snli_ve - loss is tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3714/16548 [1:43:09<6:04:04,  1.70s/it]11/15/2022 18:49:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:30 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3715/16548 [1:43:11<6:03:40,  1.70s/it]11/15/2022 18:49:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.0770e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:31 - INFO - train.train_snli_ve - loss is tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3716/16548 [1:43:12<6:01:22,  1.69s/it]11/15/2022 18:49:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.0774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:33 - INFO - train.train_snli_ve - loss is tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3717/16548 [1:43:14<5:59:19,  1.68s/it]11/15/2022 18:49:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.6400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:35 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3718/16548 [1:43:16<6:01:22,  1.69s/it]11/15/2022 18:49:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.9994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:37 - INFO - train.train_snli_ve - loss is tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3719/16548 [1:43:17<5:57:55,  1.67s/it]11/15/2022 18:49:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.0632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:38 - INFO - train.train_snli_ve - loss is tensor(0.8910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3720/16548 [1:43:19<5:58:42,  1.68s/it]11/15/2022 18:49:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.4462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:40 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3721/16548 [1:43:21<5:57:39,  1.67s/it]11/15/2022 18:49:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.5283e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:42 - INFO - train.train_snli_ve - loss is tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3722/16548 [1:43:22<5:56:39,  1.67s/it]11/15/2022 18:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.1147e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:43 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22% 3723/16548 [1:43:24<6:00:49,  1.69s/it]11/15/2022 18:49:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.9667e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:45 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3724/16548 [1:43:26<5:58:50,  1.68s/it]11/15/2022 18:49:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.7824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:47 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3725/16548 [1:43:27<5:56:09,  1.67s/it]11/15/2022 18:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.5296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:48 - INFO - train.train_snli_ve - loss is tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3726/16548 [1:43:29<5:58:53,  1.68s/it]11/15/2022 18:49:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:50 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3727/16548 [1:43:31<6:01:59,  1.69s/it]11/15/2022 18:49:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.8297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:52 - INFO - train.train_snli_ve - loss is tensor(0.9249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3728/16548 [1:43:32<6:00:38,  1.69s/it]11/15/2022 18:49:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.1262e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:53 - INFO - train.train_snli_ve - loss is tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3729/16548 [1:43:34<5:59:36,  1.68s/it]11/15/2022 18:49:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.8386e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:55 - INFO - train.train_snli_ve - loss is tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3730/16548 [1:43:36<5:59:20,  1.68s/it]11/15/2022 18:49:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.3292e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:57 - INFO - train.train_snli_ve - loss is tensor(0.8120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3731/16548 [1:43:37<5:59:04,  1.68s/it]11/15/2022 18:49:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.1276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:49:58 - INFO - train.train_snli_ve - loss is tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3732/16548 [1:43:39<5:58:48,  1.68s/it]11/15/2022 18:50:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.0200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:00 - INFO - train.train_snli_ve - loss is tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3733/16548 [1:43:41<5:59:57,  1.69s/it]11/15/2022 18:50:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.2572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:02 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3734/16548 [1:43:43<6:03:33,  1.70s/it]11/15/2022 18:50:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.9434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:03 - INFO - train.train_snli_ve - loss is tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3735/16548 [1:43:44<6:01:51,  1.69s/it]11/15/2022 18:50:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.8925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:05 - INFO - train.train_snli_ve - loss is tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3736/16548 [1:43:46<6:02:41,  1.70s/it]11/15/2022 18:50:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.0547e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:07 - INFO - train.train_snli_ve - loss is tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3737/16548 [1:43:48<6:05:53,  1.71s/it]11/15/2022 18:50:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.5502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:09 - INFO - train.train_snli_ve - loss is tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3738/16548 [1:43:49<6:02:50,  1.70s/it]11/15/2022 18:50:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.5677e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:10 - INFO - train.train_snli_ve - loss is tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3739/16548 [1:43:51<6:05:07,  1.71s/it]11/15/2022 18:50:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.6560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:12 - INFO - train.train_snli_ve - loss is tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3740/16548 [1:43:53<6:02:15,  1.70s/it]11/15/2022 18:50:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.0980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:14 - INFO - train.train_snli_ve - loss is tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3741/16548 [1:43:54<5:58:37,  1.68s/it]11/15/2022 18:50:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:15 - INFO - train.train_snli_ve - loss is tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3742/16548 [1:43:56<6:01:25,  1.69s/it]11/15/2022 18:50:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.0297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:17 - INFO - train.train_snli_ve - loss is tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3743/16548 [1:43:58<6:02:20,  1.70s/it]11/15/2022 18:50:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:19 - INFO - train.train_snli_ve - loss is tensor(1.0685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3744/16548 [1:43:59<6:00:53,  1.69s/it]11/15/2022 18:50:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.1368e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:20 - INFO - train.train_snli_ve - loss is tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3745/16548 [1:44:01<6:03:02,  1.70s/it]11/15/2022 18:50:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.1787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:22 - INFO - train.train_snli_ve - loss is tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3746/16548 [1:44:03<6:01:45,  1.70s/it]11/15/2022 18:50:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.0704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:24 - INFO - train.train_snli_ve - loss is tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3747/16548 [1:44:05<6:00:26,  1.69s/it]11/15/2022 18:50:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.5670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:26 - INFO - train.train_snli_ve - loss is tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3748/16548 [1:44:06<6:02:08,  1.70s/it]11/15/2022 18:50:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.2723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:27 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3749/16548 [1:44:08<5:57:57,  1.68s/it]11/15/2022 18:50:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.0177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:29 - INFO - train.train_snli_ve - loss is tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3750/16548 [1:44:10<5:58:17,  1.68s/it]11/15/2022 18:50:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.8582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:31 - INFO - train.train_snli_ve - loss is tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3751/16548 [1:44:11<6:01:57,  1.70s/it]11/15/2022 18:50:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.8057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:32 - INFO - train.train_snli_ve - loss is tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3752/16548 [1:44:13<6:01:45,  1.70s/it]11/15/2022 18:50:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.7483e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:34 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3753/16548 [1:44:15<5:56:33,  1.67s/it]11/15/2022 18:50:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.2852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:36 - INFO - train.train_snli_ve - loss is tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3754/16548 [1:44:16<5:58:04,  1.68s/it]11/15/2022 18:50:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.4316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:37 - INFO - train.train_snli_ve - loss is tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3755/16548 [1:44:18<5:56:35,  1.67s/it]11/15/2022 18:50:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.7474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:39 - INFO - train.train_snli_ve - loss is tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3756/16548 [1:44:20<5:57:37,  1.68s/it]11/15/2022 18:50:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.3304e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:41 - INFO - train.train_snli_ve - loss is tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3757/16548 [1:44:21<5:56:36,  1.67s/it]11/15/2022 18:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.6778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:42 - INFO - train.train_snli_ve - loss is tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3758/16548 [1:44:23<5:56:09,  1.67s/it]11/15/2022 18:50:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:44 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3759/16548 [1:44:25<5:55:16,  1.67s/it]11/15/2022 18:50:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.2464e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:46 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3760/16548 [1:44:26<5:55:42,  1.67s/it]11/15/2022 18:50:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.3223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:47 - INFO - train.train_snli_ve - loss is tensor(0.5816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3761/16548 [1:44:28<5:54:29,  1.66s/it]11/15/2022 18:50:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.9022e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:49 - INFO - train.train_snli_ve - loss is tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3762/16548 [1:44:30<5:54:14,  1.66s/it]11/15/2022 18:50:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.9653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:51 - INFO - train.train_snli_ve - loss is tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3763/16548 [1:44:31<5:56:19,  1.67s/it]11/15/2022 18:50:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.3984e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:52 - INFO - train.train_snli_ve - loss is tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3764/16548 [1:44:33<5:57:08,  1.68s/it]11/15/2022 18:50:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.7361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:54 - INFO - train.train_snli_ve - loss is tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3765/16548 [1:44:35<5:55:36,  1.67s/it]11/15/2022 18:50:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:56 - INFO - train.train_snli_ve - loss is tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3766/16548 [1:44:36<5:54:23,  1.66s/it]11/15/2022 18:50:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.7034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:57 - INFO - train.train_snli_ve - loss is tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3767/16548 [1:44:38<5:57:32,  1.68s/it]11/15/2022 18:50:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.6421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:50:59 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3768/16548 [1:44:40<5:57:12,  1.68s/it]11/15/2022 18:51:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.1265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:01 - INFO - train.train_snli_ve - loss is tensor(0.7760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3769/16548 [1:44:41<5:57:02,  1.68s/it]11/15/2022 18:51:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:02 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3770/16548 [1:44:43<5:56:42,  1.67s/it]11/15/2022 18:51:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:04 - INFO - train.train_snli_ve - loss is tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3771/16548 [1:44:45<5:55:55,  1.67s/it]11/15/2022 18:51:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.3552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:06 - INFO - train.train_snli_ve - loss is tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3772/16548 [1:44:46<5:56:31,  1.67s/it]11/15/2022 18:51:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.6611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:07 - INFO - train.train_snli_ve - loss is tensor(0.8963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3773/16548 [1:44:48<5:55:35,  1.67s/it]11/15/2022 18:51:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:09 - INFO - train.train_snli_ve - loss is tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3774/16548 [1:44:50<5:56:17,  1.67s/it]11/15/2022 18:51:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.2429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:11 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3775/16548 [1:44:51<5:56:46,  1.68s/it]11/15/2022 18:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:12 - INFO - train.train_snli_ve - loss is tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3776/16548 [1:44:53<5:55:35,  1.67s/it]11/15/2022 18:51:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:14 - INFO - train.train_snli_ve - loss is tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3777/16548 [1:44:55<5:56:49,  1.68s/it]11/15/2022 18:51:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:16 - INFO - train.train_snli_ve - loss is tensor(0.8413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3778/16548 [1:44:56<5:55:13,  1.67s/it]11/15/2022 18:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:17 - INFO - train.train_snli_ve - loss is tensor(0.5074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3779/16548 [1:44:58<5:53:58,  1.66s/it]11/15/2022 18:51:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.6805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:19 - INFO - train.train_snli_ve - loss is tensor(0.8318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3780/16548 [1:45:00<5:57:09,  1.68s/it]11/15/2022 18:51:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.0367e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:21 - INFO - train.train_snli_ve - loss is tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3781/16548 [1:45:01<5:55:35,  1.67s/it]11/15/2022 18:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.5552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:22 - INFO - train.train_snli_ve - loss is tensor(0.9664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3782/16548 [1:45:03<5:55:26,  1.67s/it]11/15/2022 18:51:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.3721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:24 - INFO - train.train_snli_ve - loss is tensor(0.8582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3783/16548 [1:45:05<5:56:14,  1.67s/it]11/15/2022 18:51:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.9252e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:26 - INFO - train.train_snli_ve - loss is tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3784/16548 [1:45:06<5:54:34,  1.67s/it]11/15/2022 18:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.8762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:27 - INFO - train.train_snli_ve - loss is tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3785/16548 [1:45:08<5:52:48,  1.66s/it]11/15/2022 18:51:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.6696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:29 - INFO - train.train_snli_ve - loss is tensor(0.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3786/16548 [1:45:10<5:51:59,  1.65s/it]11/15/2022 18:51:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.0293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:31 - INFO - train.train_snli_ve - loss is tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3787/16548 [1:45:11<5:51:29,  1.65s/it]11/15/2022 18:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.5967e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:32 - INFO - train.train_snli_ve - loss is tensor(0.5237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3788/16548 [1:45:13<5:55:19,  1.67s/it]11/15/2022 18:51:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.8501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:34 - INFO - train.train_snli_ve - loss is tensor(0.9012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3789/16548 [1:45:15<5:57:26,  1.68s/it]11/15/2022 18:51:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.2608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:36 - INFO - train.train_snli_ve - loss is tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3790/16548 [1:45:17<6:00:07,  1.69s/it]11/15/2022 18:51:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.8884e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:37 - INFO - train.train_snli_ve - loss is tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3791/16548 [1:45:18<5:58:55,  1.69s/it]11/15/2022 18:51:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.8487e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:39 - INFO - train.train_snli_ve - loss is tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3792/16548 [1:45:20<5:56:59,  1.68s/it]11/15/2022 18:51:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.9516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:41 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3793/16548 [1:45:22<5:57:20,  1.68s/it]11/15/2022 18:51:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:42 - INFO - train.train_snli_ve - loss is tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3794/16548 [1:45:23<5:55:40,  1.67s/it]11/15/2022 18:51:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.0600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:44 - INFO - train.train_snli_ve - loss is tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3795/16548 [1:45:25<5:54:44,  1.67s/it]11/15/2022 18:51:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.2793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:46 - INFO - train.train_snli_ve - loss is tensor(0.8436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3796/16548 [1:45:27<5:54:53,  1.67s/it]11/15/2022 18:51:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.5323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:47 - INFO - train.train_snli_ve - loss is tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3797/16548 [1:45:28<5:54:04,  1.67s/it]11/15/2022 18:51:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.3318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:49 - INFO - train.train_snli_ve - loss is tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3798/16548 [1:45:30<5:56:06,  1.68s/it]11/15/2022 18:51:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.5552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:51 - INFO - train.train_snli_ve - loss is tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3799/16548 [1:45:32<5:56:21,  1.68s/it]11/15/2022 18:51:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.5592e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:53 - INFO - train.train_snli_ve - loss is tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3800/16548 [1:45:33<6:00:50,  1.70s/it]11/15/2022 18:51:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:54 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3801/16548 [1:45:35<6:00:21,  1.70s/it]11/15/2022 18:51:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.6537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:56 - INFO - train.train_snli_ve - loss is tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3802/16548 [1:45:37<5:59:02,  1.69s/it]11/15/2022 18:51:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.5431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:58 - INFO - train.train_snli_ve - loss is tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3803/16548 [1:45:38<5:56:42,  1.68s/it]11/15/2022 18:51:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.7421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:51:59 - INFO - train.train_snli_ve - loss is tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3804/16548 [1:45:40<5:56:43,  1.68s/it]11/15/2022 18:52:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.2762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:01 - INFO - train.train_snli_ve - loss is tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3805/16548 [1:45:42<5:55:57,  1.68s/it]11/15/2022 18:52:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.3585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:03 - INFO - train.train_snli_ve - loss is tensor(0.8451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3806/16548 [1:45:43<5:54:07,  1.67s/it]11/15/2022 18:52:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.0559e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:04 - INFO - train.train_snli_ve - loss is tensor(0.9518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3807/16548 [1:45:45<5:54:50,  1.67s/it]11/15/2022 18:52:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.3020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:06 - INFO - train.train_snli_ve - loss is tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3808/16548 [1:45:47<5:55:59,  1.68s/it]11/15/2022 18:52:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.3277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:08 - INFO - train.train_snli_ve - loss is tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3809/16548 [1:45:48<5:57:01,  1.68s/it]11/15/2022 18:52:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:09 - INFO - train.train_snli_ve - loss is tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3810/16548 [1:45:50<5:58:02,  1.69s/it]11/15/2022 18:52:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.3837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:11 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3811/16548 [1:45:52<5:57:14,  1.68s/it]11/15/2022 18:52:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.0157e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:13 - INFO - train.train_snli_ve - loss is tensor(1.0910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3812/16548 [1:45:53<5:56:33,  1.68s/it]11/15/2022 18:52:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:14 - INFO - train.train_snli_ve - loss is tensor(0.7715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3813/16548 [1:45:55<5:54:06,  1.67s/it]11/15/2022 18:52:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.0178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:16 - INFO - train.train_snli_ve - loss is tensor(0.8705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3814/16548 [1:45:57<5:53:10,  1.66s/it]11/15/2022 18:52:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.0964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:18 - INFO - train.train_snli_ve - loss is tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3815/16548 [1:45:58<5:52:47,  1.66s/it]11/15/2022 18:52:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.2433e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:19 - INFO - train.train_snli_ve - loss is tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3816/16548 [1:46:00<5:57:25,  1.68s/it]11/15/2022 18:52:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.6095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:21 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3817/16548 [1:46:02<5:57:19,  1.68s/it]11/15/2022 18:52:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.9631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:23 - INFO - train.train_snli_ve - loss is tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3818/16548 [1:46:04<5:57:20,  1.68s/it]11/15/2022 18:52:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.9839e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:24 - INFO - train.train_snli_ve - loss is tensor(0.5207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3819/16548 [1:46:05<5:54:28,  1.67s/it]11/15/2022 18:52:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.1441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:26 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3820/16548 [1:46:07<5:55:54,  1.68s/it]11/15/2022 18:52:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.6392e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:28 - INFO - train.train_snli_ve - loss is tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3821/16548 [1:46:09<6:01:17,  1.70s/it]11/15/2022 18:52:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.1538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:29 - INFO - train.train_snli_ve - loss is tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3822/16548 [1:46:10<5:56:52,  1.68s/it]11/15/2022 18:52:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.4264e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:31 - INFO - train.train_snli_ve - loss is tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3823/16548 [1:46:12<5:56:36,  1.68s/it]11/15/2022 18:52:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.9284e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:33 - INFO - train.train_snli_ve - loss is tensor(0.8538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3824/16548 [1:46:14<5:53:30,  1.67s/it]11/15/2022 18:52:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.0621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:35 - INFO - train.train_snli_ve - loss is tensor(0.9387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3825/16548 [1:46:15<5:55:47,  1.68s/it]11/15/2022 18:52:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.9031e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:36 - INFO - train.train_snli_ve - loss is tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3826/16548 [1:46:17<5:55:33,  1.68s/it]11/15/2022 18:52:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.4502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:38 - INFO - train.train_snli_ve - loss is tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3827/16548 [1:46:19<5:52:28,  1.66s/it]11/15/2022 18:52:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:39 - INFO - train.train_snli_ve - loss is tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3828/16548 [1:46:20<5:53:15,  1.67s/it]11/15/2022 18:52:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.5388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:41 - INFO - train.train_snli_ve - loss is tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3829/16548 [1:46:22<5:54:57,  1.67s/it]11/15/2022 18:52:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.5335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:43 - INFO - train.train_snli_ve - loss is tensor(0.7910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3830/16548 [1:46:24<5:55:28,  1.68s/it]11/15/2022 18:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.5819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:45 - INFO - train.train_snli_ve - loss is tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3831/16548 [1:46:25<5:55:38,  1.68s/it]11/15/2022 18:52:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.8041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:46 - INFO - train.train_snli_ve - loss is tensor(0.4705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3832/16548 [1:46:27<5:58:29,  1.69s/it]11/15/2022 18:52:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.3354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:48 - INFO - train.train_snli_ve - loss is tensor(1.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3833/16548 [1:46:29<6:00:05,  1.70s/it]11/15/2022 18:52:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.4984e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:50 - INFO - train.train_snli_ve - loss is tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3834/16548 [1:46:30<5:59:21,  1.70s/it]11/15/2022 18:52:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.2291e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:51 - INFO - train.train_snli_ve - loss is tensor(0.9114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3835/16548 [1:46:32<5:56:54,  1.68s/it]11/15/2022 18:52:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1642e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:53 - INFO - train.train_snli_ve - loss is tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3836/16548 [1:46:34<5:56:22,  1.68s/it]11/15/2022 18:52:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.3055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:55 - INFO - train.train_snli_ve - loss is tensor(0.7888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3837/16548 [1:46:35<5:53:38,  1.67s/it]11/15/2022 18:52:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.6873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:56 - INFO - train.train_snli_ve - loss is tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3838/16548 [1:46:37<5:53:07,  1.67s/it]11/15/2022 18:52:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.4221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:52:58 - INFO - train.train_snli_ve - loss is tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3839/16548 [1:46:39<5:55:28,  1.68s/it]11/15/2022 18:53:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:00 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3840/16548 [1:46:40<5:53:43,  1.67s/it]11/15/2022 18:53:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.6804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:01 - INFO - train.train_snli_ve - loss is tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3841/16548 [1:46:42<5:57:00,  1.69s/it]11/15/2022 18:53:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.4881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:03 - INFO - train.train_snli_ve - loss is tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3842/16548 [1:46:44<5:52:15,  1.66s/it]11/15/2022 18:53:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.3466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:05 - INFO - train.train_snli_ve - loss is tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3843/16548 [1:46:45<5:54:01,  1.67s/it]11/15/2022 18:53:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.0244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:06 - INFO - train.train_snli_ve - loss is tensor(0.9329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3844/16548 [1:46:47<5:54:55,  1.68s/it]11/15/2022 18:53:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.5481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:08 - INFO - train.train_snli_ve - loss is tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3845/16548 [1:46:49<5:54:20,  1.67s/it]11/15/2022 18:53:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.3020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:10 - INFO - train.train_snli_ve - loss is tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3846/16548 [1:46:50<5:52:56,  1.67s/it]11/15/2022 18:53:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.9777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:11 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3847/16548 [1:46:52<5:52:33,  1.67s/it]11/15/2022 18:53:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.7272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:13 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3848/16548 [1:46:54<5:53:35,  1.67s/it]11/15/2022 18:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.7041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:15 - INFO - train.train_snli_ve - loss is tensor(0.7955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3849/16548 [1:46:55<5:53:48,  1.67s/it]11/15/2022 18:53:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.4876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:16 - INFO - train.train_snli_ve - loss is tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3850/16548 [1:46:57<5:53:34,  1.67s/it]11/15/2022 18:53:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.1028e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:18 - INFO - train.train_snli_ve - loss is tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3851/16548 [1:46:59<5:53:52,  1.67s/it]11/15/2022 18:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.3226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:20 - INFO - train.train_snli_ve - loss is tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3852/16548 [1:47:01<5:56:00,  1.68s/it]11/15/2022 18:53:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.2631e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:21 - INFO - train.train_snli_ve - loss is tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3853/16548 [1:47:02<5:58:36,  1.69s/it]11/15/2022 18:53:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.3023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:23 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3854/16548 [1:47:04<5:59:01,  1.70s/it]11/15/2022 18:53:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.1405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:25 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3855/16548 [1:47:06<5:55:54,  1.68s/it]11/15/2022 18:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.8078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:27 - INFO - train.train_snli_ve - loss is tensor(1.0430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3856/16548 [1:47:07<5:56:42,  1.69s/it]11/15/2022 18:53:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.1253e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:28 - INFO - train.train_snli_ve - loss is tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3857/16548 [1:47:09<5:54:01,  1.67s/it]11/15/2022 18:53:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.5878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:30 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3858/16548 [1:47:11<5:53:45,  1.67s/it]11/15/2022 18:53:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:31 - INFO - train.train_snli_ve - loss is tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3859/16548 [1:47:12<5:51:18,  1.66s/it]11/15/2022 18:53:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.8306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:33 - INFO - train.train_snli_ve - loss is tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3860/16548 [1:47:14<5:54:10,  1.67s/it]11/15/2022 18:53:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.2272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:35 - INFO - train.train_snli_ve - loss is tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3861/16548 [1:47:16<5:52:56,  1.67s/it]11/15/2022 18:53:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.3104e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:36 - INFO - train.train_snli_ve - loss is tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3862/16548 [1:47:17<5:50:54,  1.66s/it]11/15/2022 18:53:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.5391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:38 - INFO - train.train_snli_ve - loss is tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3863/16548 [1:47:19<5:54:03,  1.67s/it]11/15/2022 18:53:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.4943e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:40 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3864/16548 [1:47:21<5:54:44,  1.68s/it]11/15/2022 18:53:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.6682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:42 - INFO - train.train_snli_ve - loss is tensor(0.4599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3865/16548 [1:47:22<5:53:17,  1.67s/it]11/15/2022 18:53:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.8832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:43 - INFO - train.train_snli_ve - loss is tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3866/16548 [1:47:24<5:52:41,  1.67s/it]11/15/2022 18:53:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.3920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:45 - INFO - train.train_snli_ve - loss is tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3867/16548 [1:47:26<5:53:29,  1.67s/it]11/15/2022 18:53:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.6733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:47 - INFO - train.train_snli_ve - loss is tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3868/16548 [1:47:27<5:52:20,  1.67s/it]11/15/2022 18:53:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.5500e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:48 - INFO - train.train_snli_ve - loss is tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3869/16548 [1:47:29<5:50:27,  1.66s/it]11/15/2022 18:53:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.1964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:50 - INFO - train.train_snli_ve - loss is tensor(0.8779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3870/16548 [1:47:31<5:54:24,  1.68s/it]11/15/2022 18:53:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.9380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:52 - INFO - train.train_snli_ve - loss is tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3871/16548 [1:47:32<5:54:02,  1.68s/it]11/15/2022 18:53:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.6271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:53 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3872/16548 [1:47:34<5:53:43,  1.67s/it]11/15/2022 18:53:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.0392e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:55 - INFO - train.train_snli_ve - loss is tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3873/16548 [1:47:36<5:54:41,  1.68s/it]11/15/2022 18:53:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.9524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:57 - INFO - train.train_snli_ve - loss is tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3874/16548 [1:47:37<5:59:47,  1.70s/it]11/15/2022 18:53:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.0678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:53:58 - INFO - train.train_snli_ve - loss is tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3875/16548 [1:47:39<5:55:57,  1.69s/it]11/15/2022 18:54:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.8005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:00 - INFO - train.train_snli_ve - loss is tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3876/16548 [1:47:41<5:56:43,  1.69s/it]11/15/2022 18:54:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.0455e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:02 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3877/16548 [1:47:42<5:53:27,  1.67s/it]11/15/2022 18:54:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.1602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:03 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3878/16548 [1:47:44<5:52:02,  1.67s/it]11/15/2022 18:54:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.0096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:05 - INFO - train.train_snli_ve - loss is tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3879/16548 [1:47:46<5:50:56,  1.66s/it]11/15/2022 18:54:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.4090e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:07 - INFO - train.train_snli_ve - loss is tensor(0.8383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3880/16548 [1:47:47<5:51:22,  1.66s/it]11/15/2022 18:54:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.4998e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:08 - INFO - train.train_snli_ve - loss is tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3881/16548 [1:47:49<5:49:21,  1.65s/it]11/15/2022 18:54:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.6679e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:10 - INFO - train.train_snli_ve - loss is tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3882/16548 [1:47:51<5:51:23,  1.66s/it]11/15/2022 18:54:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.5149e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:12 - INFO - train.train_snli_ve - loss is tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3883/16548 [1:47:52<5:51:41,  1.67s/it]11/15/2022 18:54:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.6870e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:13 - INFO - train.train_snli_ve - loss is tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3884/16548 [1:47:54<5:55:36,  1.68s/it]11/15/2022 18:54:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.8107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:15 - INFO - train.train_snli_ve - loss is tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3885/16548 [1:47:56<5:55:08,  1.68s/it]11/15/2022 18:54:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.1227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:17 - INFO - train.train_snli_ve - loss is tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3886/16548 [1:47:57<5:54:43,  1.68s/it]11/15/2022 18:54:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.3937e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:18 - INFO - train.train_snli_ve - loss is tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3887/16548 [1:47:59<5:53:45,  1.68s/it]11/15/2022 18:54:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.3025e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:20 - INFO - train.train_snli_ve - loss is tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23% 3888/16548 [1:48:01<5:52:57,  1.67s/it]11/15/2022 18:54:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.0740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:22 - INFO - train.train_snli_ve - loss is tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3889/16548 [1:48:02<5:51:43,  1.67s/it]11/15/2022 18:54:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:23 - INFO - train.train_snli_ve - loss is tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3890/16548 [1:48:04<5:50:54,  1.66s/it]11/15/2022 18:54:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:25 - INFO - train.train_snli_ve - loss is tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3891/16548 [1:48:06<5:51:48,  1.67s/it]11/15/2022 18:54:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.9163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:27 - INFO - train.train_snli_ve - loss is tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3892/16548 [1:48:07<5:50:10,  1.66s/it]11/15/2022 18:54:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.1165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:28 - INFO - train.train_snli_ve - loss is tensor(0.5934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3893/16548 [1:48:09<5:50:40,  1.66s/it]11/15/2022 18:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:30 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3894/16548 [1:48:11<5:52:37,  1.67s/it]11/15/2022 18:54:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.8451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:32 - INFO - train.train_snli_ve - loss is tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3895/16548 [1:48:12<5:52:43,  1.67s/it]11/15/2022 18:54:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.4859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:33 - INFO - train.train_snli_ve - loss is tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3896/16548 [1:48:14<5:56:57,  1.69s/it]11/15/2022 18:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.3624e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:35 - INFO - train.train_snli_ve - loss is tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3897/16548 [1:48:16<5:58:30,  1.70s/it]11/15/2022 18:54:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.5431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:37 - INFO - train.train_snli_ve - loss is tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3898/16548 [1:48:18<5:57:41,  1.70s/it]11/15/2022 18:54:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.3906e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:39 - INFO - train.train_snli_ve - loss is tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3899/16548 [1:48:19<5:54:56,  1.68s/it]11/15/2022 18:54:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.5918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:40 - INFO - train.train_snli_ve - loss is tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3900/16548 [1:48:21<5:57:37,  1.70s/it]11/15/2022 18:54:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.4655e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:42 - INFO - train.train_snli_ve - loss is tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3901/16548 [1:48:23<5:57:34,  1.70s/it]11/15/2022 18:54:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.2470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:44 - INFO - train.train_snli_ve - loss is tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3902/16548 [1:48:24<5:58:05,  1.70s/it]11/15/2022 18:54:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.9985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:45 - INFO - train.train_snli_ve - loss is tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3903/16548 [1:48:26<5:58:59,  1.70s/it]11/15/2022 18:54:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.2035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:47 - INFO - train.train_snli_ve - loss is tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3904/16548 [1:48:28<5:55:57,  1.69s/it]11/15/2022 18:54:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.6559e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:49 - INFO - train.train_snli_ve - loss is tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3905/16548 [1:48:29<5:58:01,  1.70s/it]11/15/2022 18:54:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.8950e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:50 - INFO - train.train_snli_ve - loss is tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3906/16548 [1:48:31<5:59:18,  1.71s/it]11/15/2022 18:54:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.3622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:52 - INFO - train.train_snli_ve - loss is tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3907/16548 [1:48:33<5:57:37,  1.70s/it]11/15/2022 18:54:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.0383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:54 - INFO - train.train_snli_ve - loss is tensor(0.4787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3908/16548 [1:48:35<5:57:24,  1.70s/it]11/15/2022 18:54:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.9372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:56 - INFO - train.train_snli_ve - loss is tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3909/16548 [1:48:36<5:56:38,  1.69s/it]11/15/2022 18:54:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:57 - INFO - train.train_snli_ve - loss is tensor(0.8518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3910/16548 [1:48:38<5:55:10,  1.69s/it]11/15/2022 18:54:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.1836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:54:59 - INFO - train.train_snli_ve - loss is tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3911/16548 [1:48:40<5:54:01,  1.68s/it]11/15/2022 18:55:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.4954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:01 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3912/16548 [1:48:41<5:53:10,  1.68s/it]11/15/2022 18:55:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.0491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:02 - INFO - train.train_snli_ve - loss is tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3913/16548 [1:48:43<5:51:43,  1.67s/it]11/15/2022 18:55:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.0381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:04 - INFO - train.train_snli_ve - loss is tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3914/16548 [1:48:45<5:53:38,  1.68s/it]11/15/2022 18:55:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.4030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:06 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3915/16548 [1:48:46<5:51:48,  1.67s/it]11/15/2022 18:55:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.0686e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:07 - INFO - train.train_snli_ve - loss is tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3916/16548 [1:48:48<5:57:18,  1.70s/it]11/15/2022 18:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.3841e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:09 - INFO - train.train_snli_ve - loss is tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3917/16548 [1:48:50<5:54:05,  1.68s/it]11/15/2022 18:55:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.1361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:11 - INFO - train.train_snli_ve - loss is tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3918/16548 [1:48:51<5:52:38,  1.68s/it]11/15/2022 18:55:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:12 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3919/16548 [1:48:53<5:51:25,  1.67s/it]11/15/2022 18:55:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.7454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:14 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3920/16548 [1:48:55<5:52:00,  1.67s/it]11/15/2022 18:55:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.0648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:16 - INFO - train.train_snli_ve - loss is tensor(0.9963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3921/16548 [1:48:56<5:49:49,  1.66s/it]11/15/2022 18:55:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:17 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3922/16548 [1:48:58<5:53:33,  1.68s/it]11/15/2022 18:55:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.9728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:19 - INFO - train.train_snli_ve - loss is tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3923/16548 [1:49:00<5:55:53,  1.69s/it]11/15/2022 18:55:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.8726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:21 - INFO - train.train_snli_ve - loss is tensor(0.9245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3924/16548 [1:49:01<5:55:31,  1.69s/it]11/15/2022 18:55:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.9866e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:22 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3925/16548 [1:49:03<5:54:38,  1.69s/it]11/15/2022 18:55:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.8550e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:24 - INFO - train.train_snli_ve - loss is tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3926/16548 [1:49:05<5:55:08,  1.69s/it]11/15/2022 18:55:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.3704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:26 - INFO - train.train_snli_ve - loss is tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3927/16548 [1:49:06<5:53:03,  1.68s/it]11/15/2022 18:55:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.8748e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:27 - INFO - train.train_snli_ve - loss is tensor(0.7258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3928/16548 [1:49:08<5:52:33,  1.68s/it]11/15/2022 18:55:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.2272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:29 - INFO - train.train_snli_ve - loss is tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3929/16548 [1:49:10<5:53:14,  1.68s/it]11/15/2022 18:55:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.1875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:31 - INFO - train.train_snli_ve - loss is tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3930/16548 [1:49:11<5:53:29,  1.68s/it]11/15/2022 18:55:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.3696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:32 - INFO - train.train_snli_ve - loss is tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3931/16548 [1:49:13<5:53:06,  1.68s/it]11/15/2022 18:55:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.0913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:34 - INFO - train.train_snli_ve - loss is tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3932/16548 [1:49:15<5:57:08,  1.70s/it]11/15/2022 18:55:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.2503e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:36 - INFO - train.train_snli_ve - loss is tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3933/16548 [1:49:17<5:56:48,  1.70s/it]11/15/2022 18:55:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.8750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:38 - INFO - train.train_snli_ve - loss is tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3934/16548 [1:49:18<5:56:11,  1.69s/it]11/15/2022 18:55:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.9847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:39 - INFO - train.train_snli_ve - loss is tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3935/16548 [1:49:20<5:58:10,  1.70s/it]11/15/2022 18:55:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.2995e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:41 - INFO - train.train_snli_ve - loss is tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3936/16548 [1:49:22<5:55:09,  1.69s/it]11/15/2022 18:55:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.1890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:43 - INFO - train.train_snli_ve - loss is tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3937/16548 [1:49:23<5:53:43,  1.68s/it]11/15/2022 18:55:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.8064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:44 - INFO - train.train_snli_ve - loss is tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3938/16548 [1:49:25<5:53:01,  1.68s/it]11/15/2022 18:55:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.4146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:46 - INFO - train.train_snli_ve - loss is tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3939/16548 [1:49:27<5:53:12,  1.68s/it]11/15/2022 18:55:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.7023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:48 - INFO - train.train_snli_ve - loss is tensor(0.8394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3940/16548 [1:49:28<5:51:17,  1.67s/it]11/15/2022 18:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.0692e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:49 - INFO - train.train_snli_ve - loss is tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3941/16548 [1:49:30<5:50:13,  1.67s/it]11/15/2022 18:55:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.0153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:51 - INFO - train.train_snli_ve - loss is tensor(0.8751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3942/16548 [1:49:32<5:54:08,  1.69s/it]11/15/2022 18:55:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:53 - INFO - train.train_snli_ve - loss is tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3943/16548 [1:49:33<5:54:01,  1.69s/it]11/15/2022 18:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.9837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:54 - INFO - train.train_snli_ve - loss is tensor(0.7826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3944/16548 [1:49:35<5:56:07,  1.70s/it]11/15/2022 18:55:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.8582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:56 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3945/16548 [1:49:37<5:58:35,  1.71s/it]11/15/2022 18:55:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.6633e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:58 - INFO - train.train_snli_ve - loss is tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3946/16548 [1:49:39<5:56:35,  1.70s/it]11/15/2022 18:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.5965e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:55:59 - INFO - train.train_snli_ve - loss is tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3947/16548 [1:49:40<5:56:35,  1.70s/it]11/15/2022 18:56:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.8078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:01 - INFO - train.train_snli_ve - loss is tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3948/16548 [1:49:42<5:57:18,  1.70s/it]11/15/2022 18:56:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.8806e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:03 - INFO - train.train_snli_ve - loss is tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3949/16548 [1:49:44<5:54:52,  1.69s/it]11/15/2022 18:56:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.3238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:05 - INFO - train.train_snli_ve - loss is tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3950/16548 [1:49:45<5:55:29,  1.69s/it]11/15/2022 18:56:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.4126e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:06 - INFO - train.train_snli_ve - loss is tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3951/16548 [1:49:47<5:54:37,  1.69s/it]11/15/2022 18:56:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.2092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:08 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3952/16548 [1:49:49<5:54:06,  1.69s/it]11/15/2022 18:56:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.0892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:10 - INFO - train.train_snli_ve - loss is tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3953/16548 [1:49:50<5:52:47,  1.68s/it]11/15/2022 18:56:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:11 - INFO - train.train_snli_ve - loss is tensor(0.4904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3954/16548 [1:49:52<5:53:17,  1.68s/it]11/15/2022 18:56:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.3049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:13 - INFO - train.train_snli_ve - loss is tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3955/16548 [1:49:54<5:53:58,  1.69s/it]11/15/2022 18:56:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.0601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:15 - INFO - train.train_snli_ve - loss is tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3956/16548 [1:49:55<5:53:15,  1.68s/it]11/15/2022 18:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.5343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:16 - INFO - train.train_snli_ve - loss is tensor(0.4245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3957/16548 [1:49:57<5:53:34,  1.68s/it]11/15/2022 18:56:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:18 - INFO - train.train_snli_ve - loss is tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3958/16548 [1:49:59<5:56:05,  1.70s/it]11/15/2022 18:56:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.4475e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:20 - INFO - train.train_snli_ve - loss is tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3959/16548 [1:50:00<5:55:20,  1.69s/it]11/15/2022 18:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.1630e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:21 - INFO - train.train_snli_ve - loss is tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3960/16548 [1:50:02<5:53:18,  1.68s/it]11/15/2022 18:56:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.1875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:23 - INFO - train.train_snli_ve - loss is tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3961/16548 [1:50:04<5:51:57,  1.68s/it]11/15/2022 18:56:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.9782e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:25 - INFO - train.train_snli_ve - loss is tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3962/16548 [1:50:05<5:49:08,  1.66s/it]11/15/2022 18:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.2869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:26 - INFO - train.train_snli_ve - loss is tensor(0.8263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3963/16548 [1:50:07<5:49:38,  1.67s/it]11/15/2022 18:56:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.4259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:28 - INFO - train.train_snli_ve - loss is tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3964/16548 [1:50:09<5:53:48,  1.69s/it]11/15/2022 18:56:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.2257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:30 - INFO - train.train_snli_ve - loss is tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3965/16548 [1:50:11<6:00:44,  1.72s/it]11/15/2022 18:56:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.3257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:32 - INFO - train.train_snli_ve - loss is tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3966/16548 [1:50:12<5:57:43,  1.71s/it]11/15/2022 18:56:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.5204e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:33 - INFO - train.train_snli_ve - loss is tensor(0.7886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3967/16548 [1:50:14<5:55:00,  1.69s/it]11/15/2022 18:56:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.3208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:35 - INFO - train.train_snli_ve - loss is tensor(0.4725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3968/16548 [1:50:16<5:56:04,  1.70s/it]11/15/2022 18:56:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.5308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:37 - INFO - train.train_snli_ve - loss is tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3969/16548 [1:50:17<5:54:16,  1.69s/it]11/15/2022 18:56:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.8700e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:38 - INFO - train.train_snli_ve - loss is tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3970/16548 [1:50:19<5:53:32,  1.69s/it]11/15/2022 18:56:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.3854e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:40 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3971/16548 [1:50:21<5:52:21,  1.68s/it]11/15/2022 18:56:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:42 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3972/16548 [1:50:22<5:50:57,  1.67s/it]11/15/2022 18:56:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.3964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:43 - INFO - train.train_snli_ve - loss is tensor(0.8858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3973/16548 [1:50:24<5:48:51,  1.66s/it]11/15/2022 18:56:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.4831e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:45 - INFO - train.train_snli_ve - loss is tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3974/16548 [1:50:26<5:50:50,  1.67s/it]11/15/2022 18:56:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.7990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:47 - INFO - train.train_snli_ve - loss is tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3975/16548 [1:50:27<5:48:38,  1.66s/it]11/15/2022 18:56:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.0771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:48 - INFO - train.train_snli_ve - loss is tensor(0.7900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3976/16548 [1:50:29<5:47:47,  1.66s/it]11/15/2022 18:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.0583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:50 - INFO - train.train_snli_ve - loss is tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3977/16548 [1:50:31<5:46:47,  1.66s/it]11/15/2022 18:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.6439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:52 - INFO - train.train_snli_ve - loss is tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3978/16548 [1:50:32<5:49:46,  1.67s/it]11/15/2022 18:56:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.5200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:53 - INFO - train.train_snli_ve - loss is tensor(0.8249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3979/16548 [1:50:34<5:52:38,  1.68s/it]11/15/2022 18:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.3990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:55 - INFO - train.train_snli_ve - loss is tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3980/16548 [1:50:36<5:50:09,  1.67s/it]11/15/2022 18:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.4818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:57 - INFO - train.train_snli_ve - loss is tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3981/16548 [1:50:37<5:49:55,  1.67s/it]11/15/2022 18:56:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.1142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:56:58 - INFO - train.train_snli_ve - loss is tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3982/16548 [1:50:39<5:51:41,  1.68s/it]11/15/2022 18:57:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:00 - INFO - train.train_snli_ve - loss is tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3983/16548 [1:50:41<5:56:52,  1.70s/it]11/15/2022 18:57:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.1971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:02 - INFO - train.train_snli_ve - loss is tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3984/16548 [1:50:43<5:58:00,  1.71s/it]11/15/2022 18:57:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.3415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:03 - INFO - train.train_snli_ve - loss is tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3985/16548 [1:50:44<5:54:29,  1.69s/it]11/15/2022 18:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.7793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:05 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3986/16548 [1:50:46<5:54:35,  1.69s/it]11/15/2022 18:57:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.1442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:07 - INFO - train.train_snli_ve - loss is tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3987/16548 [1:50:48<5:53:02,  1.69s/it]11/15/2022 18:57:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.3611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:09 - INFO - train.train_snli_ve - loss is tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3988/16548 [1:50:49<5:53:38,  1.69s/it]11/15/2022 18:57:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.8314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:10 - INFO - train.train_snli_ve - loss is tensor(0.5817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3989/16548 [1:50:51<5:53:05,  1.69s/it]11/15/2022 18:57:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.0277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:12 - INFO - train.train_snli_ve - loss is tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3990/16548 [1:50:53<5:51:44,  1.68s/it]11/15/2022 18:57:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0342e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:14 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3991/16548 [1:50:54<5:49:37,  1.67s/it]11/15/2022 18:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1843e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:15 - INFO - train.train_snli_ve - loss is tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3992/16548 [1:50:56<5:48:47,  1.67s/it]11/15/2022 18:57:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.4437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:17 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3993/16548 [1:50:58<5:46:50,  1.66s/it]11/15/2022 18:57:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.5531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:19 - INFO - train.train_snli_ve - loss is tensor(0.8022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3994/16548 [1:50:59<5:49:29,  1.67s/it]11/15/2022 18:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:20 - INFO - train.train_snli_ve - loss is tensor(0.4808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3995/16548 [1:51:01<5:49:23,  1.67s/it]11/15/2022 18:57:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.4034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:22 - INFO - train.train_snli_ve - loss is tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3996/16548 [1:51:03<5:52:21,  1.68s/it]11/15/2022 18:57:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.9175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:24 - INFO - train.train_snli_ve - loss is tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3997/16548 [1:51:04<5:50:38,  1.68s/it]11/15/2022 18:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.1192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:25 - INFO - train.train_snli_ve - loss is tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3998/16548 [1:51:06<5:50:26,  1.68s/it]11/15/2022 18:57:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.0339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:27 - INFO - train.train_snli_ve - loss is tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 3999/16548 [1:51:08<5:51:20,  1.68s/it]11/15/2022 18:57:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.0324e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:29 - INFO - train.train_snli_ve - loss is tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4000/16548 [1:51:09<5:53:50,  1.69s/it]11/15/2022 18:57:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.4043e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:30 - INFO - train.train_snli_ve - loss is tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4001/16548 [1:51:11<5:55:33,  1.70s/it]11/15/2022 18:57:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.1077e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:32 - INFO - train.train_snli_ve - loss is tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4002/16548 [1:51:13<5:52:42,  1.69s/it]11/15/2022 18:57:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.1357e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:34 - INFO - train.train_snli_ve - loss is tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4003/16548 [1:51:14<5:53:50,  1.69s/it]11/15/2022 18:57:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.1426e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:35 - INFO - train.train_snli_ve - loss is tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4004/16548 [1:51:16<5:50:44,  1.68s/it]11/15/2022 18:57:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.6726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:37 - INFO - train.train_snli_ve - loss is tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4005/16548 [1:51:18<5:54:19,  1.69s/it]11/15/2022 18:57:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.1349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:39 - INFO - train.train_snli_ve - loss is tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4006/16548 [1:51:20<5:51:52,  1.68s/it]11/15/2022 18:57:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.0527e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:40 - INFO - train.train_snli_ve - loss is tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4007/16548 [1:51:21<5:52:43,  1.69s/it]11/15/2022 18:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.4122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:42 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4008/16548 [1:51:23<5:51:27,  1.68s/it]11/15/2022 18:57:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.8196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:44 - INFO - train.train_snli_ve - loss is tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4009/16548 [1:51:25<5:51:37,  1.68s/it]11/15/2022 18:57:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.6507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:46 - INFO - train.train_snli_ve - loss is tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4010/16548 [1:51:26<5:52:02,  1.68s/it]11/15/2022 18:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.1248e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:47 - INFO - train.train_snli_ve - loss is tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4011/16548 [1:51:28<5:53:56,  1.69s/it]11/15/2022 18:57:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.6612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:49 - INFO - train.train_snli_ve - loss is tensor(0.8429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4012/16548 [1:51:30<5:51:31,  1.68s/it]11/15/2022 18:57:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:51 - INFO - train.train_snli_ve - loss is tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4013/16548 [1:51:31<5:51:30,  1.68s/it]11/15/2022 18:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.2596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:52 - INFO - train.train_snli_ve - loss is tensor(0.9290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4014/16548 [1:51:33<5:52:24,  1.69s/it]11/15/2022 18:57:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.5179e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:54 - INFO - train.train_snli_ve - loss is tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4015/16548 [1:51:35<5:52:32,  1.69s/it]11/15/2022 18:57:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:56 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4016/16548 [1:51:36<5:53:15,  1.69s/it]11/15/2022 18:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.7189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:57 - INFO - train.train_snli_ve - loss is tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4017/16548 [1:51:38<5:53:02,  1.69s/it]11/15/2022 18:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.0885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:57:59 - INFO - train.train_snli_ve - loss is tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4018/16548 [1:51:40<5:53:45,  1.69s/it]11/15/2022 18:58:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.2026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:01 - INFO - train.train_snli_ve - loss is tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4019/16548 [1:51:41<5:52:03,  1.69s/it]11/15/2022 18:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.4328e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:02 - INFO - train.train_snli_ve - loss is tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4020/16548 [1:51:43<5:52:48,  1.69s/it]11/15/2022 18:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.3073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:04 - INFO - train.train_snli_ve - loss is tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4021/16548 [1:51:45<5:48:34,  1.67s/it]11/15/2022 18:58:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.0875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:06 - INFO - train.train_snli_ve - loss is tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4022/16548 [1:51:46<5:49:51,  1.68s/it]11/15/2022 18:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.9153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:07 - INFO - train.train_snli_ve - loss is tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4023/16548 [1:51:48<5:52:11,  1.69s/it]11/15/2022 18:58:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.7662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:09 - INFO - train.train_snli_ve - loss is tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4024/16548 [1:51:50<5:55:23,  1.70s/it]11/15/2022 18:58:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.6142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:11 - INFO - train.train_snli_ve - loss is tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4025/16548 [1:51:52<5:54:50,  1.70s/it]11/15/2022 18:58:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.6094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:13 - INFO - train.train_snli_ve - loss is tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4026/16548 [1:51:53<5:53:02,  1.69s/it]11/15/2022 18:58:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.9989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:14 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4027/16548 [1:51:55<5:49:51,  1.68s/it]11/15/2022 18:58:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:16 - INFO - train.train_snli_ve - loss is tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4028/16548 [1:51:57<5:49:55,  1.68s/it]11/15/2022 18:58:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:18 - INFO - train.train_snli_ve - loss is tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4029/16548 [1:51:58<5:49:41,  1.68s/it]11/15/2022 18:58:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:19 - INFO - train.train_snli_ve - loss is tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4030/16548 [1:52:00<5:51:45,  1.69s/it]11/15/2022 18:58:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.1967e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:21 - INFO - train.train_snli_ve - loss is tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4031/16548 [1:52:02<5:51:29,  1.68s/it]11/15/2022 18:58:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:23 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4032/16548 [1:52:03<5:47:52,  1.67s/it]11/15/2022 18:58:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:24 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4033/16548 [1:52:05<5:45:57,  1.66s/it]11/15/2022 18:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.1560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:26 - INFO - train.train_snli_ve - loss is tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4034/16548 [1:52:07<5:48:06,  1.67s/it]11/15/2022 18:58:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:28 - INFO - train.train_snli_ve - loss is tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4035/16548 [1:52:08<5:47:33,  1.67s/it]11/15/2022 18:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:29 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4036/16548 [1:52:10<5:50:57,  1.68s/it]11/15/2022 18:58:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.6452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:31 - INFO - train.train_snli_ve - loss is tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4037/16548 [1:52:12<5:49:52,  1.68s/it]11/15/2022 18:58:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.1589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:33 - INFO - train.train_snli_ve - loss is tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4038/16548 [1:52:13<5:50:54,  1.68s/it]11/15/2022 18:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.1825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:34 - INFO - train.train_snli_ve - loss is tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4039/16548 [1:52:15<5:49:34,  1.68s/it]11/15/2022 18:58:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.9377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:36 - INFO - train.train_snli_ve - loss is tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4040/16548 [1:52:17<5:50:13,  1.68s/it]11/15/2022 18:58:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0220e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:38 - INFO - train.train_snli_ve - loss is tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4041/16548 [1:52:18<5:49:51,  1.68s/it]11/15/2022 18:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.2862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:39 - INFO - train.train_snli_ve - loss is tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4042/16548 [1:52:20<5:50:06,  1.68s/it]11/15/2022 18:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.3353e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:41 - INFO - train.train_snli_ve - loss is tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4043/16548 [1:52:22<5:51:53,  1.69s/it]11/15/2022 18:58:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:43 - INFO - train.train_snli_ve - loss is tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4044/16548 [1:52:23<5:50:49,  1.68s/it]11/15/2022 18:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.7721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:44 - INFO - train.train_snli_ve - loss is tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4045/16548 [1:52:25<5:50:58,  1.68s/it]11/15/2022 18:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.7903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:46 - INFO - train.train_snli_ve - loss is tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4046/16548 [1:52:27<5:51:44,  1.69s/it]11/15/2022 18:58:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.9690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:48 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4047/16548 [1:52:28<5:49:34,  1.68s/it]11/15/2022 18:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:49 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4048/16548 [1:52:30<5:49:40,  1.68s/it]11/15/2022 18:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.9861e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:51 - INFO - train.train_snli_ve - loss is tensor(0.8423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4049/16548 [1:52:32<5:51:11,  1.69s/it]11/15/2022 18:58:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:53 - INFO - train.train_snli_ve - loss is tensor(0.7491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4050/16548 [1:52:34<5:48:44,  1.67s/it]11/15/2022 18:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.5821e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:54 - INFO - train.train_snli_ve - loss is tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4051/16548 [1:52:35<5:45:54,  1.66s/it]11/15/2022 18:58:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2830e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:56 - INFO - train.train_snli_ve - loss is tensor(0.8185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4052/16548 [1:52:37<5:46:45,  1.67s/it]11/15/2022 18:58:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.2310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:58 - INFO - train.train_snli_ve - loss is tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4053/16548 [1:52:38<5:46:46,  1.67s/it]11/15/2022 18:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.4177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:58:59 - INFO - train.train_snli_ve - loss is tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24% 4054/16548 [1:52:40<5:48:50,  1.68s/it]11/15/2022 18:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.8666e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:01 - INFO - train.train_snli_ve - loss is tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4055/16548 [1:52:42<5:49:02,  1.68s/it]11/15/2022 18:59:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.4612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:03 - INFO - train.train_snli_ve - loss is tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4056/16548 [1:52:44<5:48:25,  1.67s/it]11/15/2022 18:59:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.9109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:05 - INFO - train.train_snli_ve - loss is tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4057/16548 [1:52:45<5:51:01,  1.69s/it]11/15/2022 18:59:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.6730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:06 - INFO - train.train_snli_ve - loss is tensor(0.4603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4058/16548 [1:52:47<5:50:38,  1.68s/it]11/15/2022 18:59:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.0944e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:08 - INFO - train.train_snli_ve - loss is tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4059/16548 [1:52:49<5:49:23,  1.68s/it]11/15/2022 18:59:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:10 - INFO - train.train_snli_ve - loss is tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4060/16548 [1:52:50<5:50:01,  1.68s/it]11/15/2022 18:59:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.6596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:11 - INFO - train.train_snli_ve - loss is tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4061/16548 [1:52:52<5:51:38,  1.69s/it]11/15/2022 18:59:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.7924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:13 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4062/16548 [1:52:54<5:52:30,  1.69s/it]11/15/2022 18:59:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.7887e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:15 - INFO - train.train_snli_ve - loss is tensor(0.7791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4063/16548 [1:52:55<5:50:03,  1.68s/it]11/15/2022 18:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.1725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:16 - INFO - train.train_snli_ve - loss is tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4064/16548 [1:52:57<5:49:37,  1.68s/it]11/15/2022 18:59:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.2012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:18 - INFO - train.train_snli_ve - loss is tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4065/16548 [1:52:59<5:49:44,  1.68s/it]11/15/2022 18:59:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.0219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:20 - INFO - train.train_snli_ve - loss is tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4066/16548 [1:53:00<5:48:31,  1.68s/it]11/15/2022 18:59:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.1975e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:21 - INFO - train.train_snli_ve - loss is tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4067/16548 [1:53:02<5:47:07,  1.67s/it]11/15/2022 18:59:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.2037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:23 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4068/16548 [1:53:04<5:45:13,  1.66s/it]11/15/2022 18:59:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:25 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4069/16548 [1:53:05<5:53:56,  1.70s/it]11/15/2022 18:59:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.7091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:26 - INFO - train.train_snli_ve - loss is tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4070/16548 [1:53:07<5:51:51,  1.69s/it]11/15/2022 18:59:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.6963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:28 - INFO - train.train_snli_ve - loss is tensor(0.7913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4071/16548 [1:53:09<5:51:27,  1.69s/it]11/15/2022 18:59:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.5450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:30 - INFO - train.train_snli_ve - loss is tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4072/16548 [1:53:11<5:55:58,  1.71s/it]11/15/2022 18:59:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4192e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:32 - INFO - train.train_snli_ve - loss is tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4073/16548 [1:53:12<5:56:28,  1.71s/it]11/15/2022 18:59:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.5140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:33 - INFO - train.train_snli_ve - loss is tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4074/16548 [1:53:14<5:53:24,  1.70s/it]11/15/2022 18:59:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.1289e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:35 - INFO - train.train_snli_ve - loss is tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4075/16548 [1:53:16<5:56:54,  1.72s/it]11/15/2022 18:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.2139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:37 - INFO - train.train_snli_ve - loss is tensor(0.7888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4076/16548 [1:53:17<5:55:43,  1.71s/it]11/15/2022 18:59:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.1476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:38 - INFO - train.train_snli_ve - loss is tensor(0.8912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4077/16548 [1:53:19<5:52:45,  1.70s/it]11/15/2022 18:59:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.1293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:40 - INFO - train.train_snli_ve - loss is tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4078/16548 [1:53:21<5:52:49,  1.70s/it]11/15/2022 18:59:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.1245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:42 - INFO - train.train_snli_ve - loss is tensor(0.4932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4079/16548 [1:53:22<5:52:46,  1.70s/it]11/15/2022 18:59:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.9529e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:43 - INFO - train.train_snli_ve - loss is tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4080/16548 [1:53:24<5:53:42,  1.70s/it]11/15/2022 18:59:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:45 - INFO - train.train_snli_ve - loss is tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4081/16548 [1:53:26<5:51:24,  1.69s/it]11/15/2022 18:59:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.8079e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:47 - INFO - train.train_snli_ve - loss is tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4082/16548 [1:53:28<5:52:20,  1.70s/it]11/15/2022 18:59:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.3271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:48 - INFO - train.train_snli_ve - loss is tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4083/16548 [1:53:29<5:47:45,  1.67s/it]11/15/2022 18:59:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.2403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:50 - INFO - train.train_snli_ve - loss is tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4084/16548 [1:53:31<5:48:26,  1.68s/it]11/15/2022 18:59:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:52 - INFO - train.train_snli_ve - loss is tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4085/16548 [1:53:33<5:48:34,  1.68s/it]11/15/2022 18:59:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.8515e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:53 - INFO - train.train_snli_ve - loss is tensor(0.9901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4086/16548 [1:53:34<5:48:04,  1.68s/it]11/15/2022 18:59:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.7439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:55 - INFO - train.train_snli_ve - loss is tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4087/16548 [1:53:36<5:48:22,  1.68s/it]11/15/2022 18:59:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.8693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:57 - INFO - train.train_snli_ve - loss is tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4088/16548 [1:53:38<5:49:39,  1.68s/it]11/15/2022 18:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.1196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 18:59:59 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4089/16548 [1:53:39<5:49:47,  1.68s/it]11/15/2022 19:00:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.1934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:00 - INFO - train.train_snli_ve - loss is tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4090/16548 [1:53:41<5:53:16,  1.70s/it]11/15/2022 19:00:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.2580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:02 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4091/16548 [1:53:43<5:55:03,  1.71s/it]11/15/2022 19:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.7072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:04 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4092/16548 [1:53:44<5:53:14,  1.70s/it]11/15/2022 19:00:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.2714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:05 - INFO - train.train_snli_ve - loss is tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4093/16548 [1:53:46<5:52:01,  1.70s/it]11/15/2022 19:00:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.5817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:07 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4094/16548 [1:53:48<5:53:46,  1.70s/it]11/15/2022 19:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.5778e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:09 - INFO - train.train_snli_ve - loss is tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4095/16548 [1:53:50<5:51:50,  1.70s/it]11/15/2022 19:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.2046e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:11 - INFO - train.train_snli_ve - loss is tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4096/16548 [1:53:51<5:52:51,  1.70s/it]11/15/2022 19:00:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.3497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:12 - INFO - train.train_snli_ve - loss is tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4097/16548 [1:53:53<5:50:40,  1.69s/it]11/15/2022 19:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.0348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:14 - INFO - train.train_snli_ve - loss is tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4098/16548 [1:53:55<5:52:50,  1.70s/it]11/15/2022 19:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.6741e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:16 - INFO - train.train_snli_ve - loss is tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4099/16548 [1:53:56<5:50:30,  1.69s/it]11/15/2022 19:00:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.2611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:17 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4100/16548 [1:53:58<5:56:09,  1.72s/it]11/15/2022 19:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.4501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:19 - INFO - train.train_snli_ve - loss is tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4101/16548 [1:54:00<5:53:49,  1.71s/it]11/15/2022 19:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.4793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:21 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4102/16548 [1:54:01<5:53:49,  1.71s/it]11/15/2022 19:00:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.2750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:22 - INFO - train.train_snli_ve - loss is tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4103/16548 [1:54:03<5:52:03,  1.70s/it]11/15/2022 19:00:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.6102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:24 - INFO - train.train_snli_ve - loss is tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4104/16548 [1:54:05<5:50:48,  1.69s/it]11/15/2022 19:00:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.4651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:26 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4105/16548 [1:54:06<5:48:38,  1.68s/it]11/15/2022 19:00:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.0669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:27 - INFO - train.train_snli_ve - loss is tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4106/16548 [1:54:08<5:47:46,  1.68s/it]11/15/2022 19:00:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.6169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:29 - INFO - train.train_snli_ve - loss is tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4107/16548 [1:54:10<5:49:37,  1.69s/it]11/15/2022 19:00:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.5891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:31 - INFO - train.train_snli_ve - loss is tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4108/16548 [1:54:12<5:52:49,  1.70s/it]11/15/2022 19:00:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.5163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:33 - INFO - train.train_snli_ve - loss is tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4109/16548 [1:54:13<5:50:51,  1.69s/it]11/15/2022 19:00:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:34 - INFO - train.train_snli_ve - loss is tensor(0.9568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4110/16548 [1:54:15<5:47:43,  1.68s/it]11/15/2022 19:00:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.3236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:36 - INFO - train.train_snli_ve - loss is tensor(0.7258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4111/16548 [1:54:17<5:49:55,  1.69s/it]11/15/2022 19:00:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.1018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:38 - INFO - train.train_snli_ve - loss is tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4112/16548 [1:54:18<5:50:24,  1.69s/it]11/15/2022 19:00:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.9968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:39 - INFO - train.train_snli_ve - loss is tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4113/16548 [1:54:20<5:51:30,  1.70s/it]11/15/2022 19:00:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.3912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:41 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4114/16548 [1:54:22<5:52:30,  1.70s/it]11/15/2022 19:00:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:43 - INFO - train.train_snli_ve - loss is tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4115/16548 [1:54:23<5:51:28,  1.70s/it]11/15/2022 19:00:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.0525e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:44 - INFO - train.train_snli_ve - loss is tensor(0.9381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4116/16548 [1:54:25<5:50:22,  1.69s/it]11/15/2022 19:00:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.0573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:46 - INFO - train.train_snli_ve - loss is tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4117/16548 [1:54:27<5:48:19,  1.68s/it]11/15/2022 19:00:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.0347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:48 - INFO - train.train_snli_ve - loss is tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4118/16548 [1:54:28<5:50:02,  1.69s/it]11/15/2022 19:00:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:49 - INFO - train.train_snli_ve - loss is tensor(0.4320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4119/16548 [1:54:30<5:47:15,  1.68s/it]11/15/2022 19:00:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.3404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:51 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4120/16548 [1:54:32<5:52:59,  1.70s/it]11/15/2022 19:00:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.1405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:53 - INFO - train.train_snli_ve - loss is tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4121/16548 [1:54:34<5:52:29,  1.70s/it]11/15/2022 19:00:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.8466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:55 - INFO - train.train_snli_ve - loss is tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4122/16548 [1:54:35<5:50:36,  1.69s/it]11/15/2022 19:00:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.1240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:56 - INFO - train.train_snli_ve - loss is tensor(0.7428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4123/16548 [1:54:37<5:49:04,  1.69s/it]11/15/2022 19:00:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.0776e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:00:58 - INFO - train.train_snli_ve - loss is tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4124/16548 [1:54:39<5:49:52,  1.69s/it]11/15/2022 19:01:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.2270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:00 - INFO - train.train_snli_ve - loss is tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4125/16548 [1:54:40<5:47:43,  1.68s/it]11/15/2022 19:01:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4315e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:01 - INFO - train.train_snli_ve - loss is tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4126/16548 [1:54:42<5:48:34,  1.68s/it]11/15/2022 19:01:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.0789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:03 - INFO - train.train_snli_ve - loss is tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4127/16548 [1:54:44<5:47:17,  1.68s/it]11/15/2022 19:01:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.6152e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:05 - INFO - train.train_snli_ve - loss is tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4128/16548 [1:54:45<5:45:17,  1.67s/it]11/15/2022 19:01:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.3662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:06 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4129/16548 [1:54:47<5:44:16,  1.66s/it]11/15/2022 19:01:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.1514e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:08 - INFO - train.train_snli_ve - loss is tensor(0.8581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4130/16548 [1:54:49<5:46:15,  1.67s/it]11/15/2022 19:01:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.6717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:10 - INFO - train.train_snli_ve - loss is tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4131/16548 [1:54:50<5:49:29,  1.69s/it]11/15/2022 19:01:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:11 - INFO - train.train_snli_ve - loss is tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4132/16548 [1:54:52<5:49:44,  1.69s/it]11/15/2022 19:01:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.7744e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:13 - INFO - train.train_snli_ve - loss is tensor(0.8276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4133/16548 [1:54:54<5:54:11,  1.71s/it]11/15/2022 19:01:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.2788e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:15 - INFO - train.train_snli_ve - loss is tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4134/16548 [1:54:55<5:50:58,  1.70s/it]11/15/2022 19:01:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.2202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:16 - INFO - train.train_snli_ve - loss is tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4135/16548 [1:54:57<5:47:12,  1.68s/it]11/15/2022 19:01:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.9212e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:18 - INFO - train.train_snli_ve - loss is tensor(0.9132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4136/16548 [1:54:59<5:45:03,  1.67s/it]11/15/2022 19:01:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.2687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:20 - INFO - train.train_snli_ve - loss is tensor(0.9462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4137/16548 [1:55:00<5:43:10,  1.66s/it]11/15/2022 19:01:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.0698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:21 - INFO - train.train_snli_ve - loss is tensor(0.8802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4138/16548 [1:55:02<5:45:11,  1.67s/it]11/15/2022 19:01:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.5657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:23 - INFO - train.train_snli_ve - loss is tensor(0.7933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4139/16548 [1:55:04<5:47:56,  1.68s/it]11/15/2022 19:01:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.7780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:25 - INFO - train.train_snli_ve - loss is tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4140/16548 [1:55:05<5:49:13,  1.69s/it]11/15/2022 19:01:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.6323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:26 - INFO - train.train_snli_ve - loss is tensor(0.8849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4141/16548 [1:55:07<5:48:06,  1.68s/it]11/15/2022 19:01:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.7347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:28 - INFO - train.train_snli_ve - loss is tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4142/16548 [1:55:09<5:48:04,  1.68s/it]11/15/2022 19:01:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.5221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:30 - INFO - train.train_snli_ve - loss is tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4143/16548 [1:55:11<5:48:48,  1.69s/it]11/15/2022 19:01:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.0954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:31 - INFO - train.train_snli_ve - loss is tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4144/16548 [1:55:12<5:47:57,  1.68s/it]11/15/2022 19:01:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.0050e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:33 - INFO - train.train_snli_ve - loss is tensor(0.8747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4145/16548 [1:55:14<5:47:30,  1.68s/it]11/15/2022 19:01:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.2231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:35 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4146/16548 [1:55:16<5:44:58,  1.67s/it]11/15/2022 19:01:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.1512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:36 - INFO - train.train_snli_ve - loss is tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4147/16548 [1:55:17<5:44:30,  1.67s/it]11/15/2022 19:01:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.9190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:38 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4148/16548 [1:55:19<5:46:57,  1.68s/it]11/15/2022 19:01:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.8724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:40 - INFO - train.train_snli_ve - loss is tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4149/16548 [1:55:21<5:49:50,  1.69s/it]11/15/2022 19:01:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1488e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:42 - INFO - train.train_snli_ve - loss is tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4150/16548 [1:55:22<5:52:14,  1.70s/it]11/15/2022 19:01:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.4702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:43 - INFO - train.train_snli_ve - loss is tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4151/16548 [1:55:24<5:50:31,  1.70s/it]11/15/2022 19:01:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:45 - INFO - train.train_snli_ve - loss is tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4152/16548 [1:55:26<5:49:21,  1.69s/it]11/15/2022 19:01:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.2444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:47 - INFO - train.train_snli_ve - loss is tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4153/16548 [1:55:27<5:46:58,  1.68s/it]11/15/2022 19:01:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.5817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:48 - INFO - train.train_snli_ve - loss is tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4154/16548 [1:55:29<5:45:03,  1.67s/it]11/15/2022 19:01:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.7063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:50 - INFO - train.train_snli_ve - loss is tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4155/16548 [1:55:31<5:42:41,  1.66s/it]11/15/2022 19:01:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.3594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:52 - INFO - train.train_snli_ve - loss is tensor(0.8904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4156/16548 [1:55:32<5:45:01,  1.67s/it]11/15/2022 19:01:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.7137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:53 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4157/16548 [1:55:34<5:46:18,  1.68s/it]11/15/2022 19:01:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.1315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:55 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4158/16548 [1:55:36<5:46:50,  1.68s/it]11/15/2022 19:01:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.9437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:57 - INFO - train.train_snli_ve - loss is tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4159/16548 [1:55:37<5:47:16,  1.68s/it]11/15/2022 19:01:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.4530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:01:58 - INFO - train.train_snli_ve - loss is tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4160/16548 [1:55:39<5:48:03,  1.69s/it]11/15/2022 19:02:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.3683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:00 - INFO - train.train_snli_ve - loss is tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4161/16548 [1:55:41<5:46:12,  1.68s/it]11/15/2022 19:02:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.8074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:02 - INFO - train.train_snli_ve - loss is tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4162/16548 [1:55:42<5:45:02,  1.67s/it]11/15/2022 19:02:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.8651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:03 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4163/16548 [1:55:44<5:45:47,  1.68s/it]11/15/2022 19:02:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.9359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:05 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4164/16548 [1:55:46<5:45:34,  1.67s/it]11/15/2022 19:02:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.2724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:07 - INFO - train.train_snli_ve - loss is tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4165/16548 [1:55:47<5:47:14,  1.68s/it]11/15/2022 19:02:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.3635e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:08 - INFO - train.train_snli_ve - loss is tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4166/16548 [1:55:49<5:46:57,  1.68s/it]11/15/2022 19:02:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.9843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:10 - INFO - train.train_snli_ve - loss is tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4167/16548 [1:55:51<5:45:31,  1.67s/it]11/15/2022 19:02:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.6297e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:12 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4168/16548 [1:55:53<5:50:38,  1.70s/it]11/15/2022 19:02:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.3756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:13 - INFO - train.train_snli_ve - loss is tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4169/16548 [1:55:54<5:47:34,  1.68s/it]11/15/2022 19:02:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.3428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:15 - INFO - train.train_snli_ve - loss is tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4170/16548 [1:55:56<5:48:00,  1.69s/it]11/15/2022 19:02:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.6095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:17 - INFO - train.train_snli_ve - loss is tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4171/16548 [1:55:58<5:48:34,  1.69s/it]11/15/2022 19:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.1431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:19 - INFO - train.train_snli_ve - loss is tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4172/16548 [1:55:59<5:48:59,  1.69s/it]11/15/2022 19:02:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.4683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:20 - INFO - train.train_snli_ve - loss is tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4173/16548 [1:56:01<5:51:25,  1.70s/it]11/15/2022 19:02:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.6961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:22 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4174/16548 [1:56:03<5:51:17,  1.70s/it]11/15/2022 19:02:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.7960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:24 - INFO - train.train_snli_ve - loss is tensor(0.8718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4175/16548 [1:56:04<5:51:27,  1.70s/it]11/15/2022 19:02:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.5926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:25 - INFO - train.train_snli_ve - loss is tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4176/16548 [1:56:06<5:51:34,  1.70s/it]11/15/2022 19:02:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.5962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:27 - INFO - train.train_snli_ve - loss is tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4177/16548 [1:56:08<5:51:12,  1.70s/it]11/15/2022 19:02:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.6931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:29 - INFO - train.train_snli_ve - loss is tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4178/16548 [1:56:10<5:49:38,  1.70s/it]11/15/2022 19:02:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.4804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:30 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4179/16548 [1:56:11<5:46:29,  1.68s/it]11/15/2022 19:02:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.9499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:32 - INFO - train.train_snli_ve - loss is tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4180/16548 [1:56:13<5:45:44,  1.68s/it]11/15/2022 19:02:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.6914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:34 - INFO - train.train_snli_ve - loss is tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4181/16548 [1:56:15<5:46:01,  1.68s/it]11/15/2022 19:02:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.4705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:35 - INFO - train.train_snli_ve - loss is tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4182/16548 [1:56:16<5:45:58,  1.68s/it]11/15/2022 19:02:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.5479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:37 - INFO - train.train_snli_ve - loss is tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4183/16548 [1:56:18<5:44:14,  1.67s/it]11/15/2022 19:02:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.9484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:39 - INFO - train.train_snli_ve - loss is tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4184/16548 [1:56:20<5:44:14,  1.67s/it]11/15/2022 19:02:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.3884e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:40 - INFO - train.train_snli_ve - loss is tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4185/16548 [1:56:21<5:44:24,  1.67s/it]11/15/2022 19:02:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.9932e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:42 - INFO - train.train_snli_ve - loss is tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4186/16548 [1:56:23<5:46:27,  1.68s/it]11/15/2022 19:02:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.6740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:44 - INFO - train.train_snli_ve - loss is tensor(0.8748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4187/16548 [1:56:25<5:45:22,  1.68s/it]11/15/2022 19:02:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.6897e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:46 - INFO - train.train_snli_ve - loss is tensor(0.8787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4188/16548 [1:56:26<5:44:59,  1.67s/it]11/15/2022 19:02:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.1964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:47 - INFO - train.train_snli_ve - loss is tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4189/16548 [1:56:28<5:45:11,  1.68s/it]11/15/2022 19:02:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.6042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:49 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4190/16548 [1:56:30<5:45:27,  1.68s/it]11/15/2022 19:02:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.5460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:50 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4191/16548 [1:56:31<5:42:12,  1.66s/it]11/15/2022 19:02:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.9946e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:52 - INFO - train.train_snli_ve - loss is tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4192/16548 [1:56:33<5:42:01,  1.66s/it]11/15/2022 19:02:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.8488e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:54 - INFO - train.train_snli_ve - loss is tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4193/16548 [1:56:35<5:44:11,  1.67s/it]11/15/2022 19:02:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.0091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:56 - INFO - train.train_snli_ve - loss is tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4194/16548 [1:56:36<5:48:25,  1.69s/it]11/15/2022 19:02:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.5687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:57 - INFO - train.train_snli_ve - loss is tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4195/16548 [1:56:38<5:48:43,  1.69s/it]11/15/2022 19:02:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.0094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:02:59 - INFO - train.train_snli_ve - loss is tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4196/16548 [1:56:40<5:48:13,  1.69s/it]11/15/2022 19:03:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.1769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:01 - INFO - train.train_snli_ve - loss is tensor(0.9150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4197/16548 [1:56:41<5:53:11,  1.72s/it]11/15/2022 19:03:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.7715e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:02 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4198/16548 [1:56:43<5:53:46,  1.72s/it]11/15/2022 19:03:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.4773e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:04 - INFO - train.train_snli_ve - loss is tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4199/16548 [1:56:45<5:51:44,  1.71s/it]11/15/2022 19:03:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.4044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:06 - INFO - train.train_snli_ve - loss is tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4200/16548 [1:56:47<5:51:34,  1.71s/it]11/15/2022 19:03:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.8576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:08 - INFO - train.train_snli_ve - loss is tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4201/16548 [1:56:48<5:49:11,  1.70s/it]11/15/2022 19:03:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.6238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:09 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4202/16548 [1:56:50<5:46:16,  1.68s/it]11/15/2022 19:03:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.3179e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:11 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4203/16548 [1:56:52<5:44:27,  1.67s/it]11/15/2022 19:03:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.1554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:12 - INFO - train.train_snli_ve - loss is tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4204/16548 [1:56:53<5:42:03,  1.66s/it]11/15/2022 19:03:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.9276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:14 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4205/16548 [1:56:55<5:44:56,  1.68s/it]11/15/2022 19:03:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.7351e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:16 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4206/16548 [1:56:57<5:46:24,  1.68s/it]11/15/2022 19:03:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.6927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:18 - INFO - train.train_snli_ve - loss is tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4207/16548 [1:56:58<5:46:26,  1.68s/it]11/15/2022 19:03:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.5599e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:19 - INFO - train.train_snli_ve - loss is tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4208/16548 [1:57:00<5:45:12,  1.68s/it]11/15/2022 19:03:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.0038e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:21 - INFO - train.train_snli_ve - loss is tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4209/16548 [1:57:02<5:45:01,  1.68s/it]11/15/2022 19:03:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.2300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:23 - INFO - train.train_snli_ve - loss is tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4210/16548 [1:57:03<5:46:20,  1.68s/it]11/15/2022 19:03:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.2035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:24 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4211/16548 [1:57:05<5:47:07,  1.69s/it]11/15/2022 19:03:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.3217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:26 - INFO - train.train_snli_ve - loss is tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4212/16548 [1:57:07<5:47:23,  1.69s/it]11/15/2022 19:03:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.7740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:28 - INFO - train.train_snli_ve - loss is tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4213/16548 [1:57:08<5:46:13,  1.68s/it]11/15/2022 19:03:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.0693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:29 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4214/16548 [1:57:10<5:47:37,  1.69s/it]11/15/2022 19:03:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1547e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:31 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4215/16548 [1:57:12<5:47:22,  1.69s/it]11/15/2022 19:03:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.6536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:33 - INFO - train.train_snli_ve - loss is tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4216/16548 [1:57:14<5:48:42,  1.70s/it]11/15/2022 19:03:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.3601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:34 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4217/16548 [1:57:15<5:46:03,  1.68s/it]11/15/2022 19:03:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.1703e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:36 - INFO - train.train_snli_ve - loss is tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4218/16548 [1:57:17<5:44:58,  1.68s/it]11/15/2022 19:03:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.9644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:38 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25% 4219/16548 [1:57:19<5:47:49,  1.69s/it]11/15/2022 19:03:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.4321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:40 - INFO - train.train_snli_ve - loss is tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4220/16548 [1:57:20<5:46:49,  1.69s/it]11/15/2022 19:03:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.1569e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:41 - INFO - train.train_snli_ve - loss is tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4221/16548 [1:57:22<5:46:16,  1.69s/it]11/15/2022 19:03:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.8212e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:43 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4222/16548 [1:57:24<5:43:37,  1.67s/it]11/15/2022 19:03:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.2338e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:45 - INFO - train.train_snli_ve - loss is tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4223/16548 [1:57:25<5:46:01,  1.68s/it]11/15/2022 19:03:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.3498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:46 - INFO - train.train_snli_ve - loss is tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4224/16548 [1:57:27<5:44:54,  1.68s/it]11/15/2022 19:03:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.2837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:48 - INFO - train.train_snli_ve - loss is tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4225/16548 [1:57:29<5:42:57,  1.67s/it]11/15/2022 19:03:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.8889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:49 - INFO - train.train_snli_ve - loss is tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4226/16548 [1:57:30<5:41:17,  1.66s/it]11/15/2022 19:03:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.3175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:51 - INFO - train.train_snli_ve - loss is tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4227/16548 [1:57:32<5:44:16,  1.68s/it]11/15/2022 19:03:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.2279e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:53 - INFO - train.train_snli_ve - loss is tensor(0.8713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4228/16548 [1:57:34<5:48:23,  1.70s/it]11/15/2022 19:03:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:55 - INFO - train.train_snli_ve - loss is tensor(0.5874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4229/16548 [1:57:35<5:47:03,  1.69s/it]11/15/2022 19:03:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:56 - INFO - train.train_snli_ve - loss is tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4230/16548 [1:57:37<5:45:25,  1.68s/it]11/15/2022 19:03:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.5316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:03:58 - INFO - train.train_snli_ve - loss is tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4231/16548 [1:57:39<5:44:44,  1.68s/it]11/15/2022 19:04:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8415e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:00 - INFO - train.train_snli_ve - loss is tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4232/16548 [1:57:40<5:45:24,  1.68s/it]11/15/2022 19:04:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.4022e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:01 - INFO - train.train_snli_ve - loss is tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4233/16548 [1:57:42<5:43:57,  1.68s/it]11/15/2022 19:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.4409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:03 - INFO - train.train_snli_ve - loss is tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4234/16548 [1:57:44<5:45:04,  1.68s/it]11/15/2022 19:04:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.6220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:05 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4235/16548 [1:57:46<5:51:08,  1.71s/it]11/15/2022 19:04:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.1947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:06 - INFO - train.train_snli_ve - loss is tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4236/16548 [1:57:47<5:50:21,  1.71s/it]11/15/2022 19:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.2586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:08 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4237/16548 [1:57:49<5:50:53,  1.71s/it]11/15/2022 19:04:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.3116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:10 - INFO - train.train_snli_ve - loss is tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4238/16548 [1:57:51<5:48:23,  1.70s/it]11/15/2022 19:04:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.9969e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:12 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4239/16548 [1:57:52<5:49:49,  1.71s/it]11/15/2022 19:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2362e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:13 - INFO - train.train_snli_ve - loss is tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4240/16548 [1:57:54<5:45:31,  1.68s/it]11/15/2022 19:04:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3690e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:15 - INFO - train.train_snli_ve - loss is tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4241/16548 [1:57:56<5:43:27,  1.67s/it]11/15/2022 19:04:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.8808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:17 - INFO - train.train_snli_ve - loss is tensor(0.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4242/16548 [1:57:57<5:42:45,  1.67s/it]11/15/2022 19:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:18 - INFO - train.train_snli_ve - loss is tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4243/16548 [1:57:59<5:43:44,  1.68s/it]11/15/2022 19:04:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:20 - INFO - train.train_snli_ve - loss is tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4244/16548 [1:58:01<5:45:10,  1.68s/it]11/15/2022 19:04:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.3218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:22 - INFO - train.train_snli_ve - loss is tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4245/16548 [1:58:02<5:41:47,  1.67s/it]11/15/2022 19:04:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.7071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:23 - INFO - train.train_snli_ve - loss is tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4246/16548 [1:58:04<5:45:45,  1.69s/it]11/15/2022 19:04:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4845e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:25 - INFO - train.train_snli_ve - loss is tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4247/16548 [1:58:06<5:44:11,  1.68s/it]11/15/2022 19:04:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8168e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:27 - INFO - train.train_snli_ve - loss is tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4248/16548 [1:58:07<5:49:52,  1.71s/it]11/15/2022 19:04:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:28 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4249/16548 [1:58:09<5:44:55,  1.68s/it]11/15/2022 19:04:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.2125e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:30 - INFO - train.train_snli_ve - loss is tensor(0.8334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4250/16548 [1:58:11<5:42:33,  1.67s/it]11/15/2022 19:04:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:32 - INFO - train.train_snli_ve - loss is tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4251/16548 [1:58:12<5:41:41,  1.67s/it]11/15/2022 19:04:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.8445e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:33 - INFO - train.train_snli_ve - loss is tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4252/16548 [1:58:14<5:41:58,  1.67s/it]11/15/2022 19:04:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.2455e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:35 - INFO - train.train_snli_ve - loss is tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4253/16548 [1:58:16<5:40:26,  1.66s/it]11/15/2022 19:04:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:37 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4254/16548 [1:58:17<5:42:18,  1.67s/it]11/15/2022 19:04:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.4899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:38 - INFO - train.train_snli_ve - loss is tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4255/16548 [1:58:19<5:45:08,  1.68s/it]11/15/2022 19:04:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:40 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4256/16548 [1:58:21<5:45:39,  1.69s/it]11/15/2022 19:04:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.6697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:42 - INFO - train.train_snli_ve - loss is tensor(0.9564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4257/16548 [1:58:22<5:43:37,  1.68s/it]11/15/2022 19:04:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.6236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:43 - INFO - train.train_snli_ve - loss is tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4258/16548 [1:58:24<5:43:40,  1.68s/it]11/15/2022 19:04:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5449e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:45 - INFO - train.train_snli_ve - loss is tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4259/16548 [1:58:26<5:44:56,  1.68s/it]11/15/2022 19:04:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.3073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:47 - INFO - train.train_snli_ve - loss is tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4260/16548 [1:58:28<5:44:14,  1.68s/it]11/15/2022 19:04:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:48 - INFO - train.train_snli_ve - loss is tensor(0.4620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4261/16548 [1:58:29<5:46:34,  1.69s/it]11/15/2022 19:04:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:50 - INFO - train.train_snli_ve - loss is tensor(0.7629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4262/16548 [1:58:31<5:49:40,  1.71s/it]11/15/2022 19:04:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.9863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:52 - INFO - train.train_snli_ve - loss is tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4263/16548 [1:58:33<5:50:34,  1.71s/it]11/15/2022 19:04:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:54 - INFO - train.train_snli_ve - loss is tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4264/16548 [1:58:34<5:52:18,  1.72s/it]11/15/2022 19:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:55 - INFO - train.train_snli_ve - loss is tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4265/16548 [1:58:36<5:50:17,  1.71s/it]11/15/2022 19:04:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.9754e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:57 - INFO - train.train_snli_ve - loss is tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4266/16548 [1:58:38<5:48:34,  1.70s/it]11/15/2022 19:04:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.5042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:04:59 - INFO - train.train_snli_ve - loss is tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4267/16548 [1:58:39<5:46:43,  1.69s/it]11/15/2022 19:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.3934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:00 - INFO - train.train_snli_ve - loss is tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4268/16548 [1:58:41<5:46:56,  1.70s/it]11/15/2022 19:05:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.8821e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:02 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4269/16548 [1:58:43<5:46:42,  1.69s/it]11/15/2022 19:05:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.6651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:04 - INFO - train.train_snli_ve - loss is tensor(0.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4270/16548 [1:58:45<5:44:28,  1.68s/it]11/15/2022 19:05:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:06 - INFO - train.train_snli_ve - loss is tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4271/16548 [1:58:46<5:46:34,  1.69s/it]11/15/2022 19:05:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.5031e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:07 - INFO - train.train_snli_ve - loss is tensor(0.8196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4272/16548 [1:58:48<5:47:03,  1.70s/it]11/15/2022 19:05:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.9377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:09 - INFO - train.train_snli_ve - loss is tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4273/16548 [1:58:50<5:43:58,  1.68s/it]11/15/2022 19:05:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.0582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:11 - INFO - train.train_snli_ve - loss is tensor(0.9719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4274/16548 [1:58:51<5:41:16,  1.67s/it]11/15/2022 19:05:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.9919e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:12 - INFO - train.train_snli_ve - loss is tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4275/16548 [1:58:53<5:41:19,  1.67s/it]11/15/2022 19:05:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.2073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:14 - INFO - train.train_snli_ve - loss is tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4276/16548 [1:58:55<5:41:05,  1.67s/it]11/15/2022 19:05:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.4531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:15 - INFO - train.train_snli_ve - loss is tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4277/16548 [1:58:56<5:39:26,  1.66s/it]11/15/2022 19:05:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.5168e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:17 - INFO - train.train_snli_ve - loss is tensor(0.4590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4278/16548 [1:58:58<5:40:08,  1.66s/it]11/15/2022 19:05:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.1538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:19 - INFO - train.train_snli_ve - loss is tensor(0.7841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4279/16548 [1:59:00<5:39:56,  1.66s/it]11/15/2022 19:05:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.3160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:20 - INFO - train.train_snli_ve - loss is tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4280/16548 [1:59:01<5:40:11,  1.66s/it]11/15/2022 19:05:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.4697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:22 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4281/16548 [1:59:03<5:38:18,  1.65s/it]11/15/2022 19:05:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.2494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:24 - INFO - train.train_snli_ve - loss is tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4282/16548 [1:59:05<5:40:30,  1.67s/it]11/15/2022 19:05:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.1123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:25 - INFO - train.train_snli_ve - loss is tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4283/16548 [1:59:06<5:41:52,  1.67s/it]11/15/2022 19:05:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.2616e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:27 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4284/16548 [1:59:08<5:44:05,  1.68s/it]11/15/2022 19:05:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.8144e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:29 - INFO - train.train_snli_ve - loss is tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4285/16548 [1:59:10<5:45:27,  1.69s/it]11/15/2022 19:05:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:31 - INFO - train.train_snli_ve - loss is tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4286/16548 [1:59:11<5:45:30,  1.69s/it]11/15/2022 19:05:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.2528e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:32 - INFO - train.train_snli_ve - loss is tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4287/16548 [1:59:13<5:43:31,  1.68s/it]11/15/2022 19:05:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.6602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:34 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4288/16548 [1:59:15<5:46:06,  1.69s/it]11/15/2022 19:05:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.2434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:36 - INFO - train.train_snli_ve - loss is tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4289/16548 [1:59:16<5:48:35,  1.71s/it]11/15/2022 19:05:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.2341e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:37 - INFO - train.train_snli_ve - loss is tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4290/16548 [1:59:18<5:46:24,  1.70s/it]11/15/2022 19:05:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.5724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:39 - INFO - train.train_snli_ve - loss is tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4291/16548 [1:59:20<5:44:57,  1.69s/it]11/15/2022 19:05:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.2454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:41 - INFO - train.train_snli_ve - loss is tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4292/16548 [1:59:21<5:45:34,  1.69s/it]11/15/2022 19:05:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.2739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:42 - INFO - train.train_snli_ve - loss is tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4293/16548 [1:59:23<5:44:45,  1.69s/it]11/15/2022 19:05:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.9847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:44 - INFO - train.train_snli_ve - loss is tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4294/16548 [1:59:25<5:45:37,  1.69s/it]11/15/2022 19:05:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.0462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:46 - INFO - train.train_snli_ve - loss is tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4295/16548 [1:59:27<5:44:13,  1.69s/it]11/15/2022 19:05:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.4795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:47 - INFO - train.train_snli_ve - loss is tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4296/16548 [1:59:28<5:44:06,  1.69s/it]11/15/2022 19:05:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.6644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:49 - INFO - train.train_snli_ve - loss is tensor(1.1874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4297/16548 [1:59:30<5:40:35,  1.67s/it]11/15/2022 19:05:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:51 - INFO - train.train_snli_ve - loss is tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4298/16548 [1:59:31<5:38:32,  1.66s/it]11/15/2022 19:05:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.5704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:52 - INFO - train.train_snli_ve - loss is tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4299/16548 [1:59:33<5:39:55,  1.67s/it]11/15/2022 19:05:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.7318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:54 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4300/16548 [1:59:35<5:42:12,  1.68s/it]11/15/2022 19:05:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.5546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:56 - INFO - train.train_snli_ve - loss is tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4301/16548 [1:59:37<5:44:25,  1.69s/it]11/15/2022 19:05:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:58 - INFO - train.train_snli_ve - loss is tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4302/16548 [1:59:38<5:44:50,  1.69s/it]11/15/2022 19:05:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.5197e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:05:59 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4303/16548 [1:59:40<5:43:43,  1.68s/it]11/15/2022 19:06:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.2564e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:01 - INFO - train.train_snli_ve - loss is tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4304/16548 [1:59:42<5:42:12,  1.68s/it]11/15/2022 19:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:03 - INFO - train.train_snli_ve - loss is tensor(0.5382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4305/16548 [1:59:43<5:43:10,  1.68s/it]11/15/2022 19:06:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3728e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:04 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4306/16548 [1:59:45<5:43:16,  1.68s/it]11/15/2022 19:06:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.6218e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:06 - INFO - train.train_snli_ve - loss is tensor(0.9881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4307/16548 [1:59:47<5:42:52,  1.68s/it]11/15/2022 19:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.0591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:08 - INFO - train.train_snli_ve - loss is tensor(0.5228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4308/16548 [1:59:48<5:45:16,  1.69s/it]11/15/2022 19:06:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:09 - INFO - train.train_snli_ve - loss is tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4309/16548 [1:59:50<5:42:23,  1.68s/it]11/15/2022 19:06:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.5193e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:11 - INFO - train.train_snli_ve - loss is tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4310/16548 [1:59:52<5:40:21,  1.67s/it]11/15/2022 19:06:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:13 - INFO - train.train_snli_ve - loss is tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4311/16548 [1:59:53<5:37:53,  1.66s/it]11/15/2022 19:06:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.8183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:14 - INFO - train.train_snli_ve - loss is tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4312/16548 [1:59:55<5:39:26,  1.66s/it]11/15/2022 19:06:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.7007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:16 - INFO - train.train_snli_ve - loss is tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4313/16548 [1:59:57<5:38:55,  1.66s/it]11/15/2022 19:06:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.0620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:18 - INFO - train.train_snli_ve - loss is tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4314/16548 [1:59:58<5:41:26,  1.67s/it]11/15/2022 19:06:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.9814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:19 - INFO - train.train_snli_ve - loss is tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4315/16548 [2:00:00<5:42:17,  1.68s/it]11/15/2022 19:06:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.7253e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:21 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4316/16548 [2:00:02<5:45:49,  1.70s/it]11/15/2022 19:06:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.9982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:23 - INFO - train.train_snli_ve - loss is tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4317/16548 [2:00:03<5:44:06,  1.69s/it]11/15/2022 19:06:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.6105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:24 - INFO - train.train_snli_ve - loss is tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4318/16548 [2:00:05<5:43:26,  1.68s/it]11/15/2022 19:06:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:26 - INFO - train.train_snli_ve - loss is tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4319/16548 [2:00:07<5:41:51,  1.68s/it]11/15/2022 19:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.4358e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:28 - INFO - train.train_snli_ve - loss is tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4320/16548 [2:00:08<5:42:48,  1.68s/it]11/15/2022 19:06:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:29 - INFO - train.train_snli_ve - loss is tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4321/16548 [2:00:10<5:41:19,  1.67s/it]11/15/2022 19:06:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.4412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:31 - INFO - train.train_snli_ve - loss is tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4322/16548 [2:00:12<5:39:30,  1.67s/it]11/15/2022 19:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.7918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:33 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4323/16548 [2:00:13<5:38:46,  1.66s/it]11/15/2022 19:06:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.9394e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:34 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4324/16548 [2:00:15<5:37:24,  1.66s/it]11/15/2022 19:06:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:36 - INFO - train.train_snli_ve - loss is tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4325/16548 [2:00:17<5:37:58,  1.66s/it]11/15/2022 19:06:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.1736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:38 - INFO - train.train_snli_ve - loss is tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4326/16548 [2:00:18<5:38:28,  1.66s/it]11/15/2022 19:06:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.3397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:39 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4327/16548 [2:00:20<5:40:56,  1.67s/it]11/15/2022 19:06:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.6647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:41 - INFO - train.train_snli_ve - loss is tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4328/16548 [2:00:22<5:42:45,  1.68s/it]11/15/2022 19:06:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.5718e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:43 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4329/16548 [2:00:23<5:41:31,  1.68s/it]11/15/2022 19:06:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.7128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:44 - INFO - train.train_snli_ve - loss is tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4330/16548 [2:00:25<5:43:15,  1.69s/it]11/15/2022 19:06:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.4137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:46 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4331/16548 [2:00:27<5:41:43,  1.68s/it]11/15/2022 19:06:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:48 - INFO - train.train_snli_ve - loss is tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4332/16548 [2:00:29<5:42:14,  1.68s/it]11/15/2022 19:06:49 - INFO - train.train_snli_ve - kd_loss is tensor(9.5047e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:49 - INFO - train.train_snli_ve - loss is tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4333/16548 [2:00:30<5:39:58,  1.67s/it]11/15/2022 19:06:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.7217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:51 - INFO - train.train_snli_ve - loss is tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4334/16548 [2:00:32<5:41:07,  1.68s/it]11/15/2022 19:06:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.4584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:53 - INFO - train.train_snli_ve - loss is tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4335/16548 [2:00:34<5:38:34,  1.66s/it]11/15/2022 19:06:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.0278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:54 - INFO - train.train_snli_ve - loss is tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4336/16548 [2:00:35<5:40:07,  1.67s/it]11/15/2022 19:06:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.6755e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:56 - INFO - train.train_snli_ve - loss is tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4337/16548 [2:00:37<5:39:16,  1.67s/it]11/15/2022 19:06:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.4390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:58 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4338/16548 [2:00:39<5:38:32,  1.66s/it]11/15/2022 19:06:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:06:59 - INFO - train.train_snli_ve - loss is tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4339/16548 [2:00:40<5:41:09,  1.68s/it]11/15/2022 19:07:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6781e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:01 - INFO - train.train_snli_ve - loss is tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4340/16548 [2:00:42<5:41:35,  1.68s/it]11/15/2022 19:07:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.9080e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:03 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4341/16548 [2:00:44<5:39:56,  1.67s/it]11/15/2022 19:07:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.9560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:04 - INFO - train.train_snli_ve - loss is tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4342/16548 [2:00:45<5:40:30,  1.67s/it]11/15/2022 19:07:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.3440e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:06 - INFO - train.train_snli_ve - loss is tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4343/16548 [2:00:47<5:39:50,  1.67s/it]11/15/2022 19:07:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.8575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:08 - INFO - train.train_snli_ve - loss is tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4344/16548 [2:00:49<5:42:05,  1.68s/it]11/15/2022 19:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(7.6676e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:10 - INFO - train.train_snli_ve - loss is tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4345/16548 [2:00:50<5:43:57,  1.69s/it]11/15/2022 19:07:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.5733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:11 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4346/16548 [2:00:52<5:43:08,  1.69s/it]11/15/2022 19:07:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.6773e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:13 - INFO - train.train_snli_ve - loss is tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4347/16548 [2:00:54<5:40:39,  1.68s/it]11/15/2022 19:07:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:15 - INFO - train.train_snli_ve - loss is tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4348/16548 [2:00:55<5:40:20,  1.67s/it]11/15/2022 19:07:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.8810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:16 - INFO - train.train_snli_ve - loss is tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4349/16548 [2:00:57<5:39:20,  1.67s/it]11/15/2022 19:07:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.5939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:18 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4350/16548 [2:00:59<5:41:45,  1.68s/it]11/15/2022 19:07:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.3090e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:20 - INFO - train.train_snli_ve - loss is tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4351/16548 [2:01:00<5:42:26,  1.68s/it]11/15/2022 19:07:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.2714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:21 - INFO - train.train_snli_ve - loss is tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4352/16548 [2:01:02<5:41:12,  1.68s/it]11/15/2022 19:07:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.8235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:23 - INFO - train.train_snli_ve - loss is tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4353/16548 [2:01:04<5:39:24,  1.67s/it]11/15/2022 19:07:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.8232e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:25 - INFO - train.train_snli_ve - loss is tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4354/16548 [2:01:05<5:39:09,  1.67s/it]11/15/2022 19:07:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.7246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:26 - INFO - train.train_snli_ve - loss is tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4355/16548 [2:01:07<5:36:39,  1.66s/it]11/15/2022 19:07:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.3731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:28 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4356/16548 [2:01:09<5:35:38,  1.65s/it]11/15/2022 19:07:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.1616e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:30 - INFO - train.train_snli_ve - loss is tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4357/16548 [2:01:10<5:34:53,  1.65s/it]11/15/2022 19:07:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.3357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:31 - INFO - train.train_snli_ve - loss is tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4358/16548 [2:01:12<5:37:51,  1.66s/it]11/15/2022 19:07:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.1401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:33 - INFO - train.train_snli_ve - loss is tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4359/16548 [2:01:14<5:36:21,  1.66s/it]11/15/2022 19:07:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.7935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:35 - INFO - train.train_snli_ve - loss is tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4360/16548 [2:01:15<5:37:50,  1.66s/it]11/15/2022 19:07:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.3841e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:36 - INFO - train.train_snli_ve - loss is tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4361/16548 [2:01:17<5:37:48,  1.66s/it]11/15/2022 19:07:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.8636e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:38 - INFO - train.train_snli_ve - loss is tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4362/16548 [2:01:19<5:36:19,  1.66s/it]11/15/2022 19:07:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:40 - INFO - train.train_snli_ve - loss is tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4363/16548 [2:01:20<5:40:23,  1.68s/it]11/15/2022 19:07:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.1163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:41 - INFO - train.train_snli_ve - loss is tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4364/16548 [2:01:22<5:40:08,  1.68s/it]11/15/2022 19:07:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.8097e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:43 - INFO - train.train_snli_ve - loss is tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4365/16548 [2:01:24<5:39:34,  1.67s/it]11/15/2022 19:07:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.1620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:45 - INFO - train.train_snli_ve - loss is tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4366/16548 [2:01:25<5:39:16,  1.67s/it]11/15/2022 19:07:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.0465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:46 - INFO - train.train_snli_ve - loss is tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4367/16548 [2:01:27<5:39:20,  1.67s/it]11/15/2022 19:07:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.3907e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:48 - INFO - train.train_snli_ve - loss is tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4368/16548 [2:01:29<5:38:48,  1.67s/it]11/15/2022 19:07:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:50 - INFO - train.train_snli_ve - loss is tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4369/16548 [2:01:30<5:42:35,  1.69s/it]11/15/2022 19:07:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.4036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:51 - INFO - train.train_snli_ve - loss is tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4370/16548 [2:01:32<5:42:30,  1.69s/it]11/15/2022 19:07:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:53 - INFO - train.train_snli_ve - loss is tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4371/16548 [2:01:34<5:40:35,  1.68s/it]11/15/2022 19:07:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.3229e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:55 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4372/16548 [2:01:35<5:39:07,  1.67s/it]11/15/2022 19:07:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.3298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:56 - INFO - train.train_snli_ve - loss is tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4373/16548 [2:01:37<5:40:45,  1.68s/it]11/15/2022 19:07:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:07:58 - INFO - train.train_snli_ve - loss is tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4374/16548 [2:01:39<5:39:29,  1.67s/it]11/15/2022 19:08:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.9954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:00 - INFO - train.train_snli_ve - loss is tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4375/16548 [2:01:40<5:39:03,  1.67s/it]11/15/2022 19:08:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.9456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:01 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4376/16548 [2:01:42<5:39:31,  1.67s/it]11/15/2022 19:08:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.0083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:03 - INFO - train.train_snli_ve - loss is tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4377/16548 [2:01:44<5:37:44,  1.67s/it]11/15/2022 19:08:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.9364e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:05 - INFO - train.train_snli_ve - loss is tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4378/16548 [2:01:45<5:40:10,  1.68s/it]11/15/2022 19:08:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.7161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:06 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4379/16548 [2:01:47<5:41:47,  1.69s/it]11/15/2022 19:08:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.7835e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:08 - INFO - train.train_snli_ve - loss is tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4380/16548 [2:01:49<5:41:29,  1.68s/it]11/15/2022 19:08:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.4936e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:10 - INFO - train.train_snli_ve - loss is tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4381/16548 [2:01:50<5:39:50,  1.68s/it]11/15/2022 19:08:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.5869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:11 - INFO - train.train_snli_ve - loss is tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4382/16548 [2:01:52<5:41:16,  1.68s/it]11/15/2022 19:08:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.5418e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:13 - INFO - train.train_snli_ve - loss is tensor(0.5228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4383/16548 [2:01:54<5:40:45,  1.68s/it]11/15/2022 19:08:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:15 - INFO - train.train_snli_ve - loss is tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4384/16548 [2:01:56<5:39:21,  1.67s/it]11/15/2022 19:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.8064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:16 - INFO - train.train_snli_ve - loss is tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26% 4385/16548 [2:01:57<5:39:52,  1.68s/it]11/15/2022 19:08:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0103e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:18 - INFO - train.train_snli_ve - loss is tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4386/16548 [2:01:59<5:39:09,  1.67s/it]11/15/2022 19:08:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.3330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:20 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4387/16548 [2:02:01<5:39:49,  1.68s/it]11/15/2022 19:08:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.2222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:22 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4388/16548 [2:02:02<5:42:48,  1.69s/it]11/15/2022 19:08:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.1646e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:23 - INFO - train.train_snli_ve - loss is tensor(0.9196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4389/16548 [2:02:04<5:40:45,  1.68s/it]11/15/2022 19:08:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.3808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:25 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4390/16548 [2:02:06<5:42:26,  1.69s/it]11/15/2022 19:08:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.9189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:27 - INFO - train.train_snli_ve - loss is tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4391/16548 [2:02:07<5:42:13,  1.69s/it]11/15/2022 19:08:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.6364e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:28 - INFO - train.train_snli_ve - loss is tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4392/16548 [2:02:09<5:41:25,  1.69s/it]11/15/2022 19:08:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:30 - INFO - train.train_snli_ve - loss is tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4393/16548 [2:02:11<5:39:20,  1.68s/it]11/15/2022 19:08:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.9176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:32 - INFO - train.train_snli_ve - loss is tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4394/16548 [2:02:12<5:41:45,  1.69s/it]11/15/2022 19:08:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.7256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:33 - INFO - train.train_snli_ve - loss is tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4395/16548 [2:02:14<5:40:08,  1.68s/it]11/15/2022 19:08:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1593e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:35 - INFO - train.train_snli_ve - loss is tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4396/16548 [2:02:16<5:42:27,  1.69s/it]11/15/2022 19:08:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.0299e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:37 - INFO - train.train_snli_ve - loss is tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4397/16548 [2:02:17<5:39:28,  1.68s/it]11/15/2022 19:08:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.5220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:38 - INFO - train.train_snli_ve - loss is tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4398/16548 [2:02:19<5:39:18,  1.68s/it]11/15/2022 19:08:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.1933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:40 - INFO - train.train_snli_ve - loss is tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4399/16548 [2:02:21<5:42:27,  1.69s/it]11/15/2022 19:08:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.5777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:42 - INFO - train.train_snli_ve - loss is tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4400/16548 [2:02:23<5:45:24,  1.71s/it]11/15/2022 19:08:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.1478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:43 - INFO - train.train_snli_ve - loss is tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4401/16548 [2:02:24<5:43:34,  1.70s/it]11/15/2022 19:08:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.4624e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:45 - INFO - train.train_snli_ve - loss is tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4402/16548 [2:02:26<5:44:03,  1.70s/it]11/15/2022 19:08:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.8817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:47 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4403/16548 [2:02:28<5:41:55,  1.69s/it]11/15/2022 19:08:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.8726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:49 - INFO - train.train_snli_ve - loss is tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4404/16548 [2:02:29<5:42:53,  1.69s/it]11/15/2022 19:08:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.5228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:50 - INFO - train.train_snli_ve - loss is tensor(0.5342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4405/16548 [2:02:31<5:41:18,  1.69s/it]11/15/2022 19:08:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:52 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4406/16548 [2:02:33<5:37:52,  1.67s/it]11/15/2022 19:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.5790e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:54 - INFO - train.train_snli_ve - loss is tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4407/16548 [2:02:34<5:40:04,  1.68s/it]11/15/2022 19:08:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:55 - INFO - train.train_snli_ve - loss is tensor(0.8832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4408/16548 [2:02:36<5:38:47,  1.67s/it]11/15/2022 19:08:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.5932e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:57 - INFO - train.train_snli_ve - loss is tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4409/16548 [2:02:38<5:43:00,  1.70s/it]11/15/2022 19:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.3467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:08:59 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4410/16548 [2:02:39<5:42:30,  1.69s/it]11/15/2022 19:09:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.3901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:00 - INFO - train.train_snli_ve - loss is tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4411/16548 [2:02:41<5:42:39,  1.69s/it]11/15/2022 19:09:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.2260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:02 - INFO - train.train_snli_ve - loss is tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4412/16548 [2:02:43<5:38:53,  1.68s/it]11/15/2022 19:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.5553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:04 - INFO - train.train_snli_ve - loss is tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4413/16548 [2:02:44<5:37:26,  1.67s/it]11/15/2022 19:09:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.4015e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:05 - INFO - train.train_snli_ve - loss is tensor(0.8730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4414/16548 [2:02:46<5:36:28,  1.66s/it]11/15/2022 19:09:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.1227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:07 - INFO - train.train_snli_ve - loss is tensor(0.7418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4415/16548 [2:02:48<5:35:34,  1.66s/it]11/15/2022 19:09:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.6176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:09 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4416/16548 [2:02:49<5:38:18,  1.67s/it]11/15/2022 19:09:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.2900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:10 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4417/16548 [2:02:51<5:37:05,  1.67s/it]11/15/2022 19:09:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:12 - INFO - train.train_snli_ve - loss is tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4418/16548 [2:02:53<5:38:16,  1.67s/it]11/15/2022 19:09:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.0601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:14 - INFO - train.train_snli_ve - loss is tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4419/16548 [2:02:54<5:36:37,  1.67s/it]11/15/2022 19:09:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.6870e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:15 - INFO - train.train_snli_ve - loss is tensor(0.8411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4420/16548 [2:02:56<5:38:34,  1.68s/it]11/15/2022 19:09:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.4819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:17 - INFO - train.train_snli_ve - loss is tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4421/16548 [2:02:58<5:40:55,  1.69s/it]11/15/2022 19:09:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.2485e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:19 - INFO - train.train_snli_ve - loss is tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4422/16548 [2:02:59<5:38:55,  1.68s/it]11/15/2022 19:09:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.3483e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:20 - INFO - train.train_snli_ve - loss is tensor(0.8931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4423/16548 [2:03:01<5:37:26,  1.67s/it]11/15/2022 19:09:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.4787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:22 - INFO - train.train_snli_ve - loss is tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4424/16548 [2:03:03<5:37:39,  1.67s/it]11/15/2022 19:09:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.1507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:24 - INFO - train.train_snli_ve - loss is tensor(0.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4425/16548 [2:03:04<5:36:59,  1.67s/it]11/15/2022 19:09:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.4539e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:25 - INFO - train.train_snli_ve - loss is tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4426/16548 [2:03:06<5:37:26,  1.67s/it]11/15/2022 19:09:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.0621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:27 - INFO - train.train_snli_ve - loss is tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4427/16548 [2:03:08<5:39:27,  1.68s/it]11/15/2022 19:09:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.8694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:29 - INFO - train.train_snli_ve - loss is tensor(0.4804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4428/16548 [2:03:09<5:40:09,  1.68s/it]11/15/2022 19:09:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.1872e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:30 - INFO - train.train_snli_ve - loss is tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4429/16548 [2:03:11<5:42:21,  1.69s/it]11/15/2022 19:09:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.2577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:32 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4430/16548 [2:03:13<5:44:46,  1.71s/it]11/15/2022 19:09:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.2627e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:34 - INFO - train.train_snli_ve - loss is tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4431/16548 [2:03:15<5:41:07,  1.69s/it]11/15/2022 19:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.4396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:36 - INFO - train.train_snli_ve - loss is tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4432/16548 [2:03:16<5:41:24,  1.69s/it]11/15/2022 19:09:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.1470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:37 - INFO - train.train_snli_ve - loss is tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4433/16548 [2:03:18<5:38:33,  1.68s/it]11/15/2022 19:09:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.8303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:39 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4434/16548 [2:03:20<5:35:52,  1.66s/it]11/15/2022 19:09:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.9370e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:40 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4435/16548 [2:03:21<5:33:37,  1.65s/it]11/15/2022 19:09:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.7093e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:42 - INFO - train.train_snli_ve - loss is tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4436/16548 [2:03:23<5:35:29,  1.66s/it]11/15/2022 19:09:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.3217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:44 - INFO - train.train_snli_ve - loss is tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4437/16548 [2:03:24<5:33:37,  1.65s/it]11/15/2022 19:09:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.1444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:45 - INFO - train.train_snli_ve - loss is tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4438/16548 [2:03:26<5:36:32,  1.67s/it]11/15/2022 19:09:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.6108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:47 - INFO - train.train_snli_ve - loss is tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4439/16548 [2:03:28<5:36:46,  1.67s/it]11/15/2022 19:09:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.8825e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:49 - INFO - train.train_snli_ve - loss is tensor(0.8365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4440/16548 [2:03:30<5:35:11,  1.66s/it]11/15/2022 19:09:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.3625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:50 - INFO - train.train_snli_ve - loss is tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4441/16548 [2:03:31<5:36:44,  1.67s/it]11/15/2022 19:09:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.1511e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:52 - INFO - train.train_snli_ve - loss is tensor(0.7997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4442/16548 [2:03:33<5:39:29,  1.68s/it]11/15/2022 19:09:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.2159e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:54 - INFO - train.train_snli_ve - loss is tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4443/16548 [2:03:35<5:36:51,  1.67s/it]11/15/2022 19:09:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.6845e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:56 - INFO - train.train_snli_ve - loss is tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4444/16548 [2:03:36<5:38:49,  1.68s/it]11/15/2022 19:09:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.3561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:57 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4445/16548 [2:03:38<5:37:37,  1.67s/it]11/15/2022 19:09:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.8139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:09:59 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4446/16548 [2:03:40<5:35:21,  1.66s/it]11/15/2022 19:10:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.1627e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:00 - INFO - train.train_snli_ve - loss is tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4447/16548 [2:03:41<5:34:51,  1.66s/it]11/15/2022 19:10:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.2380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:02 - INFO - train.train_snli_ve - loss is tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4448/16548 [2:03:43<5:34:14,  1.66s/it]11/15/2022 19:10:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.9447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:04 - INFO - train.train_snli_ve - loss is tensor(0.8218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4449/16548 [2:03:45<5:34:30,  1.66s/it]11/15/2022 19:10:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.6376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:05 - INFO - train.train_snli_ve - loss is tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4450/16548 [2:03:46<5:33:09,  1.65s/it]11/15/2022 19:10:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.1403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:07 - INFO - train.train_snli_ve - loss is tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4451/16548 [2:03:48<5:35:07,  1.66s/it]11/15/2022 19:10:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.7812e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:09 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4452/16548 [2:03:49<5:32:48,  1.65s/it]11/15/2022 19:10:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.2316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:10 - INFO - train.train_snli_ve - loss is tensor(0.7691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4453/16548 [2:03:51<5:35:18,  1.66s/it]11/15/2022 19:10:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.0430e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:12 - INFO - train.train_snli_ve - loss is tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4454/16548 [2:03:53<5:34:49,  1.66s/it]11/15/2022 19:10:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.4422e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:14 - INFO - train.train_snli_ve - loss is tensor(0.7999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4455/16548 [2:03:54<5:34:46,  1.66s/it]11/15/2022 19:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.5702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:15 - INFO - train.train_snli_ve - loss is tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4456/16548 [2:03:56<5:39:00,  1.68s/it]11/15/2022 19:10:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.8228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:17 - INFO - train.train_snli_ve - loss is tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4457/16548 [2:03:58<5:38:59,  1.68s/it]11/15/2022 19:10:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.3421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:19 - INFO - train.train_snli_ve - loss is tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4458/16548 [2:04:00<5:39:06,  1.68s/it]11/15/2022 19:10:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.0091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:21 - INFO - train.train_snli_ve - loss is tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4459/16548 [2:04:01<5:38:46,  1.68s/it]11/15/2022 19:10:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:22 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4460/16548 [2:04:03<5:40:09,  1.69s/it]11/15/2022 19:10:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.5085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:24 - INFO - train.train_snli_ve - loss is tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4461/16548 [2:04:05<5:39:33,  1.69s/it]11/15/2022 19:10:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.7308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:26 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4462/16548 [2:04:06<5:37:26,  1.68s/it]11/15/2022 19:10:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.3985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:27 - INFO - train.train_snli_ve - loss is tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4463/16548 [2:04:08<5:35:51,  1.67s/it]11/15/2022 19:10:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.3961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:29 - INFO - train.train_snli_ve - loss is tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4464/16548 [2:04:10<5:35:34,  1.67s/it]11/15/2022 19:10:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2461e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:31 - INFO - train.train_snli_ve - loss is tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4465/16548 [2:04:11<5:36:33,  1.67s/it]11/15/2022 19:10:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:32 - INFO - train.train_snli_ve - loss is tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4466/16548 [2:04:13<5:34:37,  1.66s/it]11/15/2022 19:10:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.4461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:34 - INFO - train.train_snli_ve - loss is tensor(0.5419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4467/16548 [2:04:15<5:34:57,  1.66s/it]11/15/2022 19:10:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.1582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:36 - INFO - train.train_snli_ve - loss is tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4468/16548 [2:04:16<5:36:13,  1.67s/it]11/15/2022 19:10:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.0048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:37 - INFO - train.train_snli_ve - loss is tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4469/16548 [2:04:18<5:34:28,  1.66s/it]11/15/2022 19:10:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.3571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:39 - INFO - train.train_snli_ve - loss is tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4470/16548 [2:04:20<5:34:47,  1.66s/it]11/15/2022 19:10:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.1086e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:41 - INFO - train.train_snli_ve - loss is tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4471/16548 [2:04:21<5:35:19,  1.67s/it]11/15/2022 19:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.2321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:42 - INFO - train.train_snli_ve - loss is tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4472/16548 [2:04:23<5:36:26,  1.67s/it]11/15/2022 19:10:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.1557e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:44 - INFO - train.train_snli_ve - loss is tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4473/16548 [2:04:25<5:37:55,  1.68s/it]11/15/2022 19:10:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1393e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:46 - INFO - train.train_snli_ve - loss is tensor(0.8155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4474/16548 [2:04:26<5:40:08,  1.69s/it]11/15/2022 19:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:47 - INFO - train.train_snli_ve - loss is tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4475/16548 [2:04:28<5:37:53,  1.68s/it]11/15/2022 19:10:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.3679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:49 - INFO - train.train_snli_ve - loss is tensor(0.8814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4476/16548 [2:04:30<5:42:20,  1.70s/it]11/15/2022 19:10:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:51 - INFO - train.train_snli_ve - loss is tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4477/16548 [2:04:31<5:39:36,  1.69s/it]11/15/2022 19:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:52 - INFO - train.train_snli_ve - loss is tensor(0.8943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4478/16548 [2:04:33<5:41:40,  1.70s/it]11/15/2022 19:10:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.8729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:54 - INFO - train.train_snli_ve - loss is tensor(0.7137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4479/16548 [2:04:35<5:38:58,  1.69s/it]11/15/2022 19:10:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.2884e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:56 - INFO - train.train_snli_ve - loss is tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4480/16548 [2:04:37<5:41:30,  1.70s/it]11/15/2022 19:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.0960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:57 - INFO - train.train_snli_ve - loss is tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4481/16548 [2:04:38<5:38:41,  1.68s/it]11/15/2022 19:10:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.2647e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:10:59 - INFO - train.train_snli_ve - loss is tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4482/16548 [2:04:40<5:36:59,  1.68s/it]11/15/2022 19:11:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.6544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:01 - INFO - train.train_snli_ve - loss is tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4483/16548 [2:04:41<5:35:47,  1.67s/it]11/15/2022 19:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.9811e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:02 - INFO - train.train_snli_ve - loss is tensor(1.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4484/16548 [2:04:43<5:37:03,  1.68s/it]11/15/2022 19:11:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.1113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:04 - INFO - train.train_snli_ve - loss is tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4485/16548 [2:04:45<5:38:03,  1.68s/it]11/15/2022 19:11:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.1584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:06 - INFO - train.train_snli_ve - loss is tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4486/16548 [2:04:47<5:38:07,  1.68s/it]11/15/2022 19:11:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.5009e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:08 - INFO - train.train_snli_ve - loss is tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4487/16548 [2:04:48<5:39:06,  1.69s/it]11/15/2022 19:11:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.2610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:09 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4488/16548 [2:04:50<5:40:43,  1.70s/it]11/15/2022 19:11:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.8704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:11 - INFO - train.train_snli_ve - loss is tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4489/16548 [2:04:52<5:42:43,  1.71s/it]11/15/2022 19:11:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.3032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:13 - INFO - train.train_snli_ve - loss is tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4490/16548 [2:04:53<5:41:49,  1.70s/it]11/15/2022 19:11:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.8840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:14 - INFO - train.train_snli_ve - loss is tensor(1.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4491/16548 [2:04:55<5:39:15,  1.69s/it]11/15/2022 19:11:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.7406e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:16 - INFO - train.train_snli_ve - loss is tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4492/16548 [2:04:57<5:39:59,  1.69s/it]11/15/2022 19:11:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.8572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:18 - INFO - train.train_snli_ve - loss is tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4493/16548 [2:04:58<5:39:16,  1.69s/it]11/15/2022 19:11:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.7690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:19 - INFO - train.train_snli_ve - loss is tensor(0.9056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4494/16548 [2:05:00<5:39:05,  1.69s/it]11/15/2022 19:11:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.5944e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:21 - INFO - train.train_snli_ve - loss is tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4495/16548 [2:05:02<5:40:24,  1.69s/it]11/15/2022 19:11:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.5986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:23 - INFO - train.train_snli_ve - loss is tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4496/16548 [2:05:03<5:39:25,  1.69s/it]11/15/2022 19:11:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.8989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:24 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4497/16548 [2:05:05<5:36:00,  1.67s/it]11/15/2022 19:11:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.8502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:26 - INFO - train.train_snli_ve - loss is tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4498/16548 [2:05:07<5:38:28,  1.69s/it]11/15/2022 19:11:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.3518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:28 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4499/16548 [2:05:08<5:34:28,  1.67s/it]11/15/2022 19:11:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.3903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:29 - INFO - train.train_snli_ve - loss is tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4500/16548 [2:05:10<5:48:38,  1.74s/it]11/15/2022 19:11:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.2574e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:31 - INFO - train.train_snli_ve - loss is tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4501/16548 [2:05:12<5:46:34,  1.73s/it]11/15/2022 19:11:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.0772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:33 - INFO - train.train_snli_ve - loss is tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4502/16548 [2:05:14<5:42:56,  1.71s/it]11/15/2022 19:11:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5604e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:35 - INFO - train.train_snli_ve - loss is tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4503/16548 [2:05:15<5:44:11,  1.71s/it]11/15/2022 19:11:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.2381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:36 - INFO - train.train_snli_ve - loss is tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4504/16548 [2:05:17<5:43:20,  1.71s/it]11/15/2022 19:11:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.1306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:38 - INFO - train.train_snli_ve - loss is tensor(0.8609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4505/16548 [2:05:19<5:45:00,  1.72s/it]11/15/2022 19:11:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.6138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:40 - INFO - train.train_snli_ve - loss is tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4506/16548 [2:05:21<5:41:45,  1.70s/it]11/15/2022 19:11:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.1987e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:41 - INFO - train.train_snli_ve - loss is tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4507/16548 [2:05:22<5:39:16,  1.69s/it]11/15/2022 19:11:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.5221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:43 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4508/16548 [2:05:24<5:37:00,  1.68s/it]11/15/2022 19:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.4484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:45 - INFO - train.train_snli_ve - loss is tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4509/16548 [2:05:26<5:36:33,  1.68s/it]11/15/2022 19:11:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.7957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:46 - INFO - train.train_snli_ve - loss is tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4510/16548 [2:05:27<5:36:17,  1.68s/it]11/15/2022 19:11:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1895e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:48 - INFO - train.train_snli_ve - loss is tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4511/16548 [2:05:29<5:38:29,  1.69s/it]11/15/2022 19:11:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.4976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:50 - INFO - train.train_snli_ve - loss is tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4512/16548 [2:05:31<5:43:08,  1.71s/it]11/15/2022 19:11:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.2708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:52 - INFO - train.train_snli_ve - loss is tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4513/16548 [2:05:32<5:39:42,  1.69s/it]11/15/2022 19:11:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.1479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:53 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4514/16548 [2:05:34<5:38:11,  1.69s/it]11/15/2022 19:11:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.5178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:55 - INFO - train.train_snli_ve - loss is tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4515/16548 [2:05:36<5:39:22,  1.69s/it]11/15/2022 19:11:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:57 - INFO - train.train_snli_ve - loss is tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4516/16548 [2:05:37<5:38:15,  1.69s/it]11/15/2022 19:11:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.6873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:11:58 - INFO - train.train_snli_ve - loss is tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4517/16548 [2:05:39<5:36:26,  1.68s/it]11/15/2022 19:12:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.2856e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:00 - INFO - train.train_snli_ve - loss is tensor(0.8893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4518/16548 [2:05:41<5:36:03,  1.68s/it]11/15/2022 19:12:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.6749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:02 - INFO - train.train_snli_ve - loss is tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4519/16548 [2:05:42<5:33:16,  1.66s/it]11/15/2022 19:12:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.1512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:03 - INFO - train.train_snli_ve - loss is tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4520/16548 [2:05:44<5:34:01,  1.67s/it]11/15/2022 19:12:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.4128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:05 - INFO - train.train_snli_ve - loss is tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4521/16548 [2:05:46<5:31:24,  1.65s/it]11/15/2022 19:12:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.5456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:07 - INFO - train.train_snli_ve - loss is tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4522/16548 [2:05:47<5:30:39,  1.65s/it]11/15/2022 19:12:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.0378e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:08 - INFO - train.train_snli_ve - loss is tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4523/16548 [2:05:49<5:30:43,  1.65s/it]11/15/2022 19:12:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.4798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:10 - INFO - train.train_snli_ve - loss is tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4524/16548 [2:05:51<5:36:11,  1.68s/it]11/15/2022 19:12:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.7512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:12 - INFO - train.train_snli_ve - loss is tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4525/16548 [2:05:52<5:36:02,  1.68s/it]11/15/2022 19:12:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.2376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:13 - INFO - train.train_snli_ve - loss is tensor(0.7916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4526/16548 [2:05:54<5:36:53,  1.68s/it]11/15/2022 19:12:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.8090e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:15 - INFO - train.train_snli_ve - loss is tensor(0.8684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4527/16548 [2:05:56<5:37:42,  1.69s/it]11/15/2022 19:12:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.2596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:17 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4528/16548 [2:05:57<5:37:14,  1.68s/it]11/15/2022 19:12:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.0004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:18 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4529/16548 [2:05:59<5:35:24,  1.67s/it]11/15/2022 19:12:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.2434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:20 - INFO - train.train_snli_ve - loss is tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4530/16548 [2:06:01<5:34:36,  1.67s/it]11/15/2022 19:12:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.2379e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:22 - INFO - train.train_snli_ve - loss is tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4531/16548 [2:06:02<5:34:20,  1.67s/it]11/15/2022 19:12:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.0401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:23 - INFO - train.train_snli_ve - loss is tensor(1.0118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4532/16548 [2:06:04<5:36:48,  1.68s/it]11/15/2022 19:12:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.7336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:25 - INFO - train.train_snli_ve - loss is tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4533/16548 [2:06:06<5:35:27,  1.68s/it]11/15/2022 19:12:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:27 - INFO - train.train_snli_ve - loss is tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4534/16548 [2:06:08<5:38:46,  1.69s/it]11/15/2022 19:12:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.9862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:28 - INFO - train.train_snli_ve - loss is tensor(0.4932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4535/16548 [2:06:09<5:38:26,  1.69s/it]11/15/2022 19:12:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.5639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:30 - INFO - train.train_snli_ve - loss is tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4536/16548 [2:06:11<5:37:58,  1.69s/it]11/15/2022 19:12:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.1620e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:32 - INFO - train.train_snli_ve - loss is tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4537/16548 [2:06:13<5:35:33,  1.68s/it]11/15/2022 19:12:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.1786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:33 - INFO - train.train_snli_ve - loss is tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4538/16548 [2:06:14<5:34:33,  1.67s/it]11/15/2022 19:12:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.0760e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:35 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4539/16548 [2:06:16<5:32:33,  1.66s/it]11/15/2022 19:12:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.2007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:37 - INFO - train.train_snli_ve - loss is tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4540/16548 [2:06:18<5:35:18,  1.68s/it]11/15/2022 19:12:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.9309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:38 - INFO - train.train_snli_ve - loss is tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4541/16548 [2:06:19<5:34:29,  1.67s/it]11/15/2022 19:12:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.7970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:40 - INFO - train.train_snli_ve - loss is tensor(0.9132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4542/16548 [2:06:21<5:34:36,  1.67s/it]11/15/2022 19:12:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.0785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:42 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4543/16548 [2:06:23<5:36:03,  1.68s/it]11/15/2022 19:12:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.6984e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:44 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4544/16548 [2:06:24<5:35:41,  1.68s/it]11/15/2022 19:12:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.2810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:45 - INFO - train.train_snli_ve - loss is tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4545/16548 [2:06:26<5:36:19,  1.68s/it]11/15/2022 19:12:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.9871e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:47 - INFO - train.train_snli_ve - loss is tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4546/16548 [2:06:28<5:34:54,  1.67s/it]11/15/2022 19:12:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.8394e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:49 - INFO - train.train_snli_ve - loss is tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4547/16548 [2:06:29<5:34:57,  1.67s/it]11/15/2022 19:12:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.6452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:50 - INFO - train.train_snli_ve - loss is tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4548/16548 [2:06:31<5:34:55,  1.67s/it]11/15/2022 19:12:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.3722e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:52 - INFO - train.train_snli_ve - loss is tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4549/16548 [2:06:33<5:34:23,  1.67s/it]11/15/2022 19:12:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.3505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:54 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27% 4550/16548 [2:06:34<5:36:04,  1.68s/it]11/15/2022 19:12:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.7696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:55 - INFO - train.train_snli_ve - loss is tensor(1.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4551/16548 [2:06:36<5:34:51,  1.67s/it]11/15/2022 19:12:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.5418e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:57 - INFO - train.train_snli_ve - loss is tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4552/16548 [2:06:38<5:33:06,  1.67s/it]11/15/2022 19:12:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.4891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:12:59 - INFO - train.train_snli_ve - loss is tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4553/16548 [2:06:39<5:35:27,  1.68s/it]11/15/2022 19:13:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.2135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:00 - INFO - train.train_snli_ve - loss is tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4554/16548 [2:06:41<5:34:12,  1.67s/it]11/15/2022 19:13:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.7459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:02 - INFO - train.train_snli_ve - loss is tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4555/16548 [2:06:43<5:32:42,  1.66s/it]11/15/2022 19:13:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.4470e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:04 - INFO - train.train_snli_ve - loss is tensor(0.8575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4556/16548 [2:06:44<5:33:27,  1.67s/it]11/15/2022 19:13:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.7789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:05 - INFO - train.train_snli_ve - loss is tensor(0.9338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4557/16548 [2:06:46<5:31:50,  1.66s/it]11/15/2022 19:13:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.9247e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:07 - INFO - train.train_snli_ve - loss is tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4558/16548 [2:06:48<5:30:31,  1.65s/it]11/15/2022 19:13:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.5134e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:09 - INFO - train.train_snli_ve - loss is tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4559/16548 [2:06:49<5:30:39,  1.65s/it]11/15/2022 19:13:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.8001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:10 - INFO - train.train_snli_ve - loss is tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4560/16548 [2:06:51<5:31:19,  1.66s/it]11/15/2022 19:13:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.4340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:12 - INFO - train.train_snli_ve - loss is tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4561/16548 [2:06:53<5:33:59,  1.67s/it]11/15/2022 19:13:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.3356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:14 - INFO - train.train_snli_ve - loss is tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4562/16548 [2:06:54<5:34:15,  1.67s/it]11/15/2022 19:13:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.2069e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:15 - INFO - train.train_snli_ve - loss is tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4563/16548 [2:06:56<5:35:50,  1.68s/it]11/15/2022 19:13:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.8250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:17 - INFO - train.train_snli_ve - loss is tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4564/16548 [2:06:58<5:35:24,  1.68s/it]11/15/2022 19:13:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.4813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:19 - INFO - train.train_snli_ve - loss is tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4565/16548 [2:06:59<5:34:36,  1.68s/it]11/15/2022 19:13:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.0463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:20 - INFO - train.train_snli_ve - loss is tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4566/16548 [2:07:01<5:32:01,  1.66s/it]11/15/2022 19:13:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.1812e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:22 - INFO - train.train_snli_ve - loss is tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4567/16548 [2:07:03<5:30:40,  1.66s/it]11/15/2022 19:13:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1428e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:24 - INFO - train.train_snli_ve - loss is tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4568/16548 [2:07:04<5:33:10,  1.67s/it]11/15/2022 19:13:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.3771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:25 - INFO - train.train_snli_ve - loss is tensor(0.4637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4569/16548 [2:07:06<5:35:24,  1.68s/it]11/15/2022 19:13:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.9996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:27 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4570/16548 [2:07:08<5:33:42,  1.67s/it]11/15/2022 19:13:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.9137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:29 - INFO - train.train_snli_ve - loss is tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4571/16548 [2:07:09<5:30:52,  1.66s/it]11/15/2022 19:13:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.9298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:30 - INFO - train.train_snli_ve - loss is tensor(0.4518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4572/16548 [2:07:11<5:33:48,  1.67s/it]11/15/2022 19:13:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.3330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:32 - INFO - train.train_snli_ve - loss is tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4573/16548 [2:07:13<5:33:27,  1.67s/it]11/15/2022 19:13:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.1073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:34 - INFO - train.train_snli_ve - loss is tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4574/16548 [2:07:14<5:34:26,  1.68s/it]11/15/2022 19:13:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.9650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:35 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4575/16548 [2:07:16<5:32:26,  1.67s/it]11/15/2022 19:13:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.9530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:37 - INFO - train.train_snli_ve - loss is tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4576/16548 [2:07:18<5:30:30,  1.66s/it]11/15/2022 19:13:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.1011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:39 - INFO - train.train_snli_ve - loss is tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4577/16548 [2:07:19<5:29:34,  1.65s/it]11/15/2022 19:13:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.5384e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:40 - INFO - train.train_snli_ve - loss is tensor(0.5497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4578/16548 [2:07:21<5:33:51,  1.67s/it]11/15/2022 19:13:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.7276e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:42 - INFO - train.train_snli_ve - loss is tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4579/16548 [2:07:23<5:33:03,  1.67s/it]11/15/2022 19:13:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:44 - INFO - train.train_snli_ve - loss is tensor(0.4799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4580/16548 [2:07:24<5:33:51,  1.67s/it]11/15/2022 19:13:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.3842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:45 - INFO - train.train_snli_ve - loss is tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4581/16548 [2:07:26<5:31:17,  1.66s/it]11/15/2022 19:13:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.7591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:47 - INFO - train.train_snli_ve - loss is tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4582/16548 [2:07:28<5:29:33,  1.65s/it]11/15/2022 19:13:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.2959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:49 - INFO - train.train_snli_ve - loss is tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4583/16548 [2:07:29<5:36:31,  1.69s/it]11/15/2022 19:13:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.3787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:50 - INFO - train.train_snli_ve - loss is tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4584/16548 [2:07:31<5:39:13,  1.70s/it]11/15/2022 19:13:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:52 - INFO - train.train_snli_ve - loss is tensor(1.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4585/16548 [2:07:33<5:36:50,  1.69s/it]11/15/2022 19:13:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.9388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:54 - INFO - train.train_snli_ve - loss is tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4586/16548 [2:07:34<5:37:26,  1.69s/it]11/15/2022 19:13:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.3116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:55 - INFO - train.train_snli_ve - loss is tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4587/16548 [2:07:36<5:35:40,  1.68s/it]11/15/2022 19:13:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.3290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:57 - INFO - train.train_snli_ve - loss is tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4588/16548 [2:07:38<5:35:22,  1.68s/it]11/15/2022 19:13:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:13:59 - INFO - train.train_snli_ve - loss is tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4589/16548 [2:07:39<5:34:09,  1.68s/it]11/15/2022 19:14:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.7414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:00 - INFO - train.train_snli_ve - loss is tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4590/16548 [2:07:41<5:37:55,  1.70s/it]11/15/2022 19:14:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.7538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:02 - INFO - train.train_snli_ve - loss is tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4591/16548 [2:07:43<5:38:26,  1.70s/it]11/15/2022 19:14:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0050e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:04 - INFO - train.train_snli_ve - loss is tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4592/16548 [2:07:45<5:38:40,  1.70s/it]11/15/2022 19:14:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.0042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:06 - INFO - train.train_snli_ve - loss is tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4593/16548 [2:07:46<5:40:13,  1.71s/it]11/15/2022 19:14:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.3106e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:07 - INFO - train.train_snli_ve - loss is tensor(0.7232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4594/16548 [2:07:48<5:44:01,  1.73s/it]11/15/2022 19:14:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.1942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:09 - INFO - train.train_snli_ve - loss is tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4595/16548 [2:07:50<5:41:28,  1.71s/it]11/15/2022 19:14:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.6756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:11 - INFO - train.train_snli_ve - loss is tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4596/16548 [2:07:51<5:39:29,  1.70s/it]11/15/2022 19:14:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.4198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:12 - INFO - train.train_snli_ve - loss is tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4597/16548 [2:07:53<5:35:45,  1.69s/it]11/15/2022 19:14:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.3915e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:14 - INFO - train.train_snli_ve - loss is tensor(0.5089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4598/16548 [2:07:55<5:38:17,  1.70s/it]11/15/2022 19:14:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.7804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:16 - INFO - train.train_snli_ve - loss is tensor(0.8883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4599/16548 [2:07:57<5:36:49,  1.69s/it]11/15/2022 19:14:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.3100e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:18 - INFO - train.train_snli_ve - loss is tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4600/16548 [2:07:58<5:44:23,  1.73s/it]11/15/2022 19:14:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:19 - INFO - train.train_snli_ve - loss is tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4601/16548 [2:08:00<5:39:39,  1.71s/it]11/15/2022 19:14:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.7053e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:21 - INFO - train.train_snli_ve - loss is tensor(1.0867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4602/16548 [2:08:02<5:37:54,  1.70s/it]11/15/2022 19:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.2983e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:23 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4603/16548 [2:08:03<5:35:51,  1.69s/it]11/15/2022 19:14:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.0606e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:24 - INFO - train.train_snli_ve - loss is tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4604/16548 [2:08:05<5:38:26,  1.70s/it]11/15/2022 19:14:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.6148e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:26 - INFO - train.train_snli_ve - loss is tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4605/16548 [2:08:07<5:36:45,  1.69s/it]11/15/2022 19:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.3387e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:28 - INFO - train.train_snli_ve - loss is tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4606/16548 [2:08:08<5:34:11,  1.68s/it]11/15/2022 19:14:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.6211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:29 - INFO - train.train_snli_ve - loss is tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4607/16548 [2:08:10<5:32:15,  1.67s/it]11/15/2022 19:14:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.9542e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:31 - INFO - train.train_snli_ve - loss is tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4608/16548 [2:08:12<5:31:55,  1.67s/it]11/15/2022 19:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.6007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:33 - INFO - train.train_snli_ve - loss is tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4609/16548 [2:08:13<5:31:58,  1.67s/it]11/15/2022 19:14:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.3447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:34 - INFO - train.train_snli_ve - loss is tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4610/16548 [2:08:15<5:30:37,  1.66s/it]11/15/2022 19:14:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.4999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:36 - INFO - train.train_snli_ve - loss is tensor(0.8684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4611/16548 [2:08:17<5:29:57,  1.66s/it]11/15/2022 19:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.8543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:38 - INFO - train.train_snli_ve - loss is tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4612/16548 [2:08:18<5:30:39,  1.66s/it]11/15/2022 19:14:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:39 - INFO - train.train_snli_ve - loss is tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4613/16548 [2:08:20<5:29:13,  1.66s/it]11/15/2022 19:14:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.1627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:41 - INFO - train.train_snli_ve - loss is tensor(0.5249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4614/16548 [2:08:22<5:29:47,  1.66s/it]11/15/2022 19:14:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.3343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:43 - INFO - train.train_snli_ve - loss is tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4615/16548 [2:08:23<5:30:00,  1.66s/it]11/15/2022 19:14:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.5191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:44 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4616/16548 [2:08:25<5:28:01,  1.65s/it]11/15/2022 19:14:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.4601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:46 - INFO - train.train_snli_ve - loss is tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4617/16548 [2:08:27<5:29:44,  1.66s/it]11/15/2022 19:14:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.9063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:48 - INFO - train.train_snli_ve - loss is tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4618/16548 [2:08:28<5:30:35,  1.66s/it]11/15/2022 19:14:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.3302e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:49 - INFO - train.train_snli_ve - loss is tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4619/16548 [2:08:30<5:31:42,  1.67s/it]11/15/2022 19:14:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.8654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:51 - INFO - train.train_snli_ve - loss is tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4620/16548 [2:08:32<5:30:20,  1.66s/it]11/15/2022 19:14:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.7249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:53 - INFO - train.train_snli_ve - loss is tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4621/16548 [2:08:33<5:33:30,  1.68s/it]11/15/2022 19:14:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6276e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:54 - INFO - train.train_snli_ve - loss is tensor(0.9442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4622/16548 [2:08:35<5:31:51,  1.67s/it]11/15/2022 19:14:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.3614e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:56 - INFO - train.train_snli_ve - loss is tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4623/16548 [2:08:37<5:32:40,  1.67s/it]11/15/2022 19:14:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.6084e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:58 - INFO - train.train_snli_ve - loss is tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4624/16548 [2:08:38<5:31:42,  1.67s/it]11/15/2022 19:14:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.6659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:14:59 - INFO - train.train_snli_ve - loss is tensor(0.8830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4625/16548 [2:08:40<5:33:35,  1.68s/it]11/15/2022 19:15:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:01 - INFO - train.train_snli_ve - loss is tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4626/16548 [2:08:42<5:35:04,  1.69s/it]11/15/2022 19:15:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.4826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:03 - INFO - train.train_snli_ve - loss is tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4627/16548 [2:08:43<5:31:42,  1.67s/it]11/15/2022 19:15:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.9606e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:04 - INFO - train.train_snli_ve - loss is tensor(0.7999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4628/16548 [2:08:45<5:31:54,  1.67s/it]11/15/2022 19:15:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.4024e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:06 - INFO - train.train_snli_ve - loss is tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4629/16548 [2:08:47<5:31:26,  1.67s/it]11/15/2022 19:15:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.3518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:08 - INFO - train.train_snli_ve - loss is tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4630/16548 [2:08:48<5:29:42,  1.66s/it]11/15/2022 19:15:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:09 - INFO - train.train_snli_ve - loss is tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4631/16548 [2:08:50<5:29:26,  1.66s/it]11/15/2022 19:15:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.1662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:11 - INFO - train.train_snli_ve - loss is tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4632/16548 [2:08:52<5:29:38,  1.66s/it]11/15/2022 19:15:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.9214e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:13 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4633/16548 [2:08:53<5:31:07,  1.67s/it]11/15/2022 19:15:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.6514e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:14 - INFO - train.train_snli_ve - loss is tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4634/16548 [2:08:55<5:30:45,  1.67s/it]11/15/2022 19:15:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.1131e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:16 - INFO - train.train_snli_ve - loss is tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4635/16548 [2:08:57<5:35:19,  1.69s/it]11/15/2022 19:15:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.6008e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:18 - INFO - train.train_snli_ve - loss is tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4636/16548 [2:08:58<5:35:31,  1.69s/it]11/15/2022 19:15:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.9694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:19 - INFO - train.train_snli_ve - loss is tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4637/16548 [2:09:00<5:34:36,  1.69s/it]11/15/2022 19:15:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.7100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:21 - INFO - train.train_snli_ve - loss is tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4638/16548 [2:09:02<5:36:19,  1.69s/it]11/15/2022 19:15:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:23 - INFO - train.train_snli_ve - loss is tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4639/16548 [2:09:04<5:36:39,  1.70s/it]11/15/2022 19:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.2957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:24 - INFO - train.train_snli_ve - loss is tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4640/16548 [2:09:05<5:35:24,  1.69s/it]11/15/2022 19:15:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:26 - INFO - train.train_snli_ve - loss is tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4641/16548 [2:09:07<5:35:51,  1.69s/it]11/15/2022 19:15:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.6834e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:28 - INFO - train.train_snli_ve - loss is tensor(0.7470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4642/16548 [2:09:09<5:36:50,  1.70s/it]11/15/2022 19:15:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.6729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:30 - INFO - train.train_snli_ve - loss is tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4643/16548 [2:09:10<5:36:11,  1.69s/it]11/15/2022 19:15:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:31 - INFO - train.train_snli_ve - loss is tensor(0.8398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4644/16548 [2:09:12<5:36:36,  1.70s/it]11/15/2022 19:15:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.1259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:33 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4645/16548 [2:09:14<5:36:46,  1.70s/it]11/15/2022 19:15:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.0119e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:35 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4646/16548 [2:09:15<5:37:32,  1.70s/it]11/15/2022 19:15:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.1194e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:36 - INFO - train.train_snli_ve - loss is tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4647/16548 [2:09:17<5:35:12,  1.69s/it]11/15/2022 19:15:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.2643e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:38 - INFO - train.train_snli_ve - loss is tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4648/16548 [2:09:19<5:40:12,  1.72s/it]11/15/2022 19:15:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:40 - INFO - train.train_snli_ve - loss is tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4649/16548 [2:09:21<5:37:47,  1.70s/it]11/15/2022 19:15:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.8727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:41 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4650/16548 [2:09:22<5:37:36,  1.70s/it]11/15/2022 19:15:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.1144e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:43 - INFO - train.train_snli_ve - loss is tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4651/16548 [2:09:24<5:35:21,  1.69s/it]11/15/2022 19:15:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0410e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:45 - INFO - train.train_snli_ve - loss is tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4652/16548 [2:09:26<5:33:33,  1.68s/it]11/15/2022 19:15:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:46 - INFO - train.train_snli_ve - loss is tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4653/16548 [2:09:27<5:32:55,  1.68s/it]11/15/2022 19:15:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.8033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:48 - INFO - train.train_snli_ve - loss is tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4654/16548 [2:09:29<5:33:28,  1.68s/it]11/15/2022 19:15:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:50 - INFO - train.train_snli_ve - loss is tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4655/16548 [2:09:31<5:32:07,  1.68s/it]11/15/2022 19:15:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.2100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:51 - INFO - train.train_snli_ve - loss is tensor(0.8706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4656/16548 [2:09:32<5:30:52,  1.67s/it]11/15/2022 19:15:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.5905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:53 - INFO - train.train_snli_ve - loss is tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4657/16548 [2:09:34<5:31:34,  1.67s/it]11/15/2022 19:15:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.2917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:55 - INFO - train.train_snli_ve - loss is tensor(0.8807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4658/16548 [2:09:36<5:32:01,  1.68s/it]11/15/2022 19:15:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.6119e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:57 - INFO - train.train_snli_ve - loss is tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4659/16548 [2:09:37<5:32:03,  1.68s/it]11/15/2022 19:15:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.9956e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:15:58 - INFO - train.train_snli_ve - loss is tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4660/16548 [2:09:39<5:29:51,  1.66s/it]11/15/2022 19:16:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.3463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:00 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4661/16548 [2:09:41<5:30:07,  1.67s/it]11/15/2022 19:16:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.9747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:02 - INFO - train.train_snli_ve - loss is tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4662/16548 [2:09:42<5:30:56,  1.67s/it]11/15/2022 19:16:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.9289e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:03 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4663/16548 [2:09:44<5:29:57,  1.67s/it]11/15/2022 19:16:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.8964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:05 - INFO - train.train_snli_ve - loss is tensor(0.7492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4664/16548 [2:09:46<5:30:01,  1.67s/it]11/15/2022 19:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:06 - INFO - train.train_snli_ve - loss is tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4665/16548 [2:09:47<5:28:04,  1.66s/it]11/15/2022 19:16:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.7855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:08 - INFO - train.train_snli_ve - loss is tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4666/16548 [2:09:49<5:29:14,  1.66s/it]11/15/2022 19:16:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.7935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:10 - INFO - train.train_snli_ve - loss is tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4667/16548 [2:09:51<5:26:42,  1.65s/it]11/15/2022 19:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.2080e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:11 - INFO - train.train_snli_ve - loss is tensor(0.8622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4668/16548 [2:09:52<5:27:22,  1.65s/it]11/15/2022 19:16:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.0499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:13 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4669/16548 [2:09:54<5:29:14,  1.66s/it]11/15/2022 19:16:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.9737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:15 - INFO - train.train_snli_ve - loss is tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4670/16548 [2:09:55<5:27:23,  1.65s/it]11/15/2022 19:16:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.1037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:16 - INFO - train.train_snli_ve - loss is tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4671/16548 [2:09:57<5:29:41,  1.67s/it]11/15/2022 19:16:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.0375e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:18 - INFO - train.train_snli_ve - loss is tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4672/16548 [2:09:59<5:29:11,  1.66s/it]11/15/2022 19:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.0798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:20 - INFO - train.train_snli_ve - loss is tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4673/16548 [2:10:00<5:28:09,  1.66s/it]11/15/2022 19:16:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.6223e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:21 - INFO - train.train_snli_ve - loss is tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4674/16548 [2:10:02<5:29:45,  1.67s/it]11/15/2022 19:16:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.4814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:23 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4675/16548 [2:10:04<5:28:52,  1.66s/it]11/15/2022 19:16:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.9486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:25 - INFO - train.train_snli_ve - loss is tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4676/16548 [2:10:06<5:31:25,  1.68s/it]11/15/2022 19:16:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.1119e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:26 - INFO - train.train_snli_ve - loss is tensor(0.9201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4677/16548 [2:10:07<5:31:54,  1.68s/it]11/15/2022 19:16:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:28 - INFO - train.train_snli_ve - loss is tensor(0.8187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4678/16548 [2:10:09<5:30:23,  1.67s/it]11/15/2022 19:16:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.2093e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:30 - INFO - train.train_snli_ve - loss is tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4679/16548 [2:10:11<5:33:36,  1.69s/it]11/15/2022 19:16:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.7575e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:32 - INFO - train.train_snli_ve - loss is tensor(0.9428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4680/16548 [2:10:12<5:33:54,  1.69s/it]11/15/2022 19:16:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.6244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:33 - INFO - train.train_snli_ve - loss is tensor(0.8926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4681/16548 [2:10:14<5:32:33,  1.68s/it]11/15/2022 19:16:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.9925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:35 - INFO - train.train_snli_ve - loss is tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4682/16548 [2:10:16<5:32:39,  1.68s/it]11/15/2022 19:16:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.2980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:37 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4683/16548 [2:10:17<5:30:32,  1.67s/it]11/15/2022 19:16:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.9809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:38 - INFO - train.train_snli_ve - loss is tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4684/16548 [2:10:19<5:30:55,  1.67s/it]11/15/2022 19:16:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.8939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:40 - INFO - train.train_snli_ve - loss is tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4685/16548 [2:10:21<5:31:02,  1.67s/it]11/15/2022 19:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.3858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:42 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4686/16548 [2:10:22<5:30:27,  1.67s/it]11/15/2022 19:16:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.0447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:43 - INFO - train.train_snli_ve - loss is tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4687/16548 [2:10:24<5:31:23,  1.68s/it]11/15/2022 19:16:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.7165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:45 - INFO - train.train_snli_ve - loss is tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4688/16548 [2:10:26<5:28:35,  1.66s/it]11/15/2022 19:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.9532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:47 - INFO - train.train_snli_ve - loss is tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4689/16548 [2:10:27<5:28:43,  1.66s/it]11/15/2022 19:16:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.7153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:48 - INFO - train.train_snli_ve - loss is tensor(0.6249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4690/16548 [2:10:29<5:29:23,  1.67s/it]11/15/2022 19:16:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.3301e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:50 - INFO - train.train_snli_ve - loss is tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4691/16548 [2:10:31<5:28:38,  1.66s/it]11/15/2022 19:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.9282e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:52 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4692/16548 [2:10:32<5:29:19,  1.67s/it]11/15/2022 19:16:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.3486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:53 - INFO - train.train_snli_ve - loss is tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4693/16548 [2:10:34<5:29:55,  1.67s/it]11/15/2022 19:16:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.2905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:55 - INFO - train.train_snli_ve - loss is tensor(0.8842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4694/16548 [2:10:36<5:32:24,  1.68s/it]11/15/2022 19:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(5.9153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:57 - INFO - train.train_snli_ve - loss is tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4695/16548 [2:10:37<5:34:04,  1.69s/it]11/15/2022 19:16:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.8893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:16:58 - INFO - train.train_snli_ve - loss is tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4696/16548 [2:10:39<5:34:09,  1.69s/it]11/15/2022 19:17:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.9997e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:00 - INFO - train.train_snli_ve - loss is tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4697/16548 [2:10:41<5:32:30,  1.68s/it]11/15/2022 19:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.1371e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:02 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4698/16548 [2:10:42<5:30:06,  1.67s/it]11/15/2022 19:17:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.9770e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:03 - INFO - train.train_snli_ve - loss is tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4699/16548 [2:10:44<5:29:32,  1.67s/it]11/15/2022 19:17:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.7700e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:05 - INFO - train.train_snli_ve - loss is tensor(0.8867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4700/16548 [2:10:46<5:35:26,  1.70s/it]11/15/2022 19:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.0518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:07 - INFO - train.train_snli_ve - loss is tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4701/16548 [2:10:48<5:37:11,  1.71s/it]11/15/2022 19:17:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.0066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:08 - INFO - train.train_snli_ve - loss is tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4702/16548 [2:10:49<5:34:46,  1.70s/it]11/15/2022 19:17:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.6705e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:10 - INFO - train.train_snli_ve - loss is tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4703/16548 [2:10:51<5:33:09,  1.69s/it]11/15/2022 19:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.9591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:12 - INFO - train.train_snli_ve - loss is tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4704/16548 [2:10:53<5:32:49,  1.69s/it]11/15/2022 19:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.5335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:14 - INFO - train.train_snli_ve - loss is tensor(0.8526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4705/16548 [2:10:54<5:32:42,  1.69s/it]11/15/2022 19:17:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.2849e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:15 - INFO - train.train_snli_ve - loss is tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4706/16548 [2:10:56<5:32:54,  1.69s/it]11/15/2022 19:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.3000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:17 - INFO - train.train_snli_ve - loss is tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4707/16548 [2:10:58<5:32:01,  1.68s/it]11/15/2022 19:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.1453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:19 - INFO - train.train_snli_ve - loss is tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4708/16548 [2:10:59<5:32:48,  1.69s/it]11/15/2022 19:17:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.1533e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:20 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4709/16548 [2:11:01<5:30:41,  1.68s/it]11/15/2022 19:17:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.9926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:22 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4710/16548 [2:11:03<5:32:16,  1.68s/it]11/15/2022 19:17:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.6586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:24 - INFO - train.train_snli_ve - loss is tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4711/16548 [2:11:04<5:32:49,  1.69s/it]11/15/2022 19:17:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.5284e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:25 - INFO - train.train_snli_ve - loss is tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4712/16548 [2:11:06<5:31:26,  1.68s/it]11/15/2022 19:17:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.7934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:27 - INFO - train.train_snli_ve - loss is tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4713/16548 [2:11:08<5:32:29,  1.69s/it]11/15/2022 19:17:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.5624e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:29 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4714/16548 [2:11:09<5:30:13,  1.67s/it]11/15/2022 19:17:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.9273e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:30 - INFO - train.train_snli_ve - loss is tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4715/16548 [2:11:11<5:30:57,  1.68s/it]11/15/2022 19:17:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.6911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:32 - INFO - train.train_snli_ve - loss is tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28% 4716/16548 [2:11:13<5:28:45,  1.67s/it]11/15/2022 19:17:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.8100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:34 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4717/16548 [2:11:14<5:27:24,  1.66s/it]11/15/2022 19:17:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0519e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:35 - INFO - train.train_snli_ve - loss is tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4718/16548 [2:11:16<5:27:40,  1.66s/it]11/15/2022 19:17:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.9586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:37 - INFO - train.train_snli_ve - loss is tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4719/16548 [2:11:18<5:29:25,  1.67s/it]11/15/2022 19:17:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.8792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:39 - INFO - train.train_snli_ve - loss is tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4720/16548 [2:11:19<5:28:36,  1.67s/it]11/15/2022 19:17:40 - INFO - train.train_snli_ve - kd_loss is tensor(9.6026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:40 - INFO - train.train_snli_ve - loss is tensor(0.9236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4721/16548 [2:11:21<5:31:37,  1.68s/it]11/15/2022 19:17:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.1099e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:42 - INFO - train.train_snli_ve - loss is tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4722/16548 [2:11:23<5:28:52,  1.67s/it]11/15/2022 19:17:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.5033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:44 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4723/16548 [2:11:24<5:27:06,  1.66s/it]11/15/2022 19:17:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:45 - INFO - train.train_snli_ve - loss is tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4724/16548 [2:11:26<5:26:25,  1.66s/it]11/15/2022 19:17:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.0546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:47 - INFO - train.train_snli_ve - loss is tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4725/16548 [2:11:28<5:26:42,  1.66s/it]11/15/2022 19:17:49 - INFO - train.train_snli_ve - kd_loss is tensor(9.0427e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:49 - INFO - train.train_snli_ve - loss is tensor(0.9425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4726/16548 [2:11:29<5:26:43,  1.66s/it]11/15/2022 19:17:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.0960e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:50 - INFO - train.train_snli_ve - loss is tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4727/16548 [2:11:31<5:27:02,  1.66s/it]11/15/2022 19:17:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.2094e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:52 - INFO - train.train_snli_ve - loss is tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4728/16548 [2:11:33<5:26:38,  1.66s/it]11/15/2022 19:17:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.3391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:54 - INFO - train.train_snli_ve - loss is tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4729/16548 [2:11:34<5:29:12,  1.67s/it]11/15/2022 19:17:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.3168e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:55 - INFO - train.train_snli_ve - loss is tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4730/16548 [2:11:36<5:27:43,  1.66s/it]11/15/2022 19:17:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.4162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:57 - INFO - train.train_snli_ve - loss is tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4731/16548 [2:11:38<5:29:06,  1.67s/it]11/15/2022 19:17:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.9990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:17:59 - INFO - train.train_snli_ve - loss is tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4732/16548 [2:11:39<5:29:14,  1.67s/it]11/15/2022 19:18:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.7336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:00 - INFO - train.train_snli_ve - loss is tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4733/16548 [2:11:41<5:31:03,  1.68s/it]11/15/2022 19:18:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.2231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:02 - INFO - train.train_snli_ve - loss is tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4734/16548 [2:11:43<5:29:07,  1.67s/it]11/15/2022 19:18:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.1167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:04 - INFO - train.train_snli_ve - loss is tensor(0.7834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4735/16548 [2:11:44<5:27:52,  1.67s/it]11/15/2022 19:18:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.5986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:05 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4736/16548 [2:11:46<5:31:15,  1.68s/it]11/15/2022 19:18:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.8817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:07 - INFO - train.train_snli_ve - loss is tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4737/16548 [2:11:48<5:31:11,  1.68s/it]11/15/2022 19:18:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.8011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:09 - INFO - train.train_snli_ve - loss is tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4738/16548 [2:11:49<5:32:04,  1.69s/it]11/15/2022 19:18:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.6676e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:10 - INFO - train.train_snli_ve - loss is tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4739/16548 [2:11:51<5:33:36,  1.70s/it]11/15/2022 19:18:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.0524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:12 - INFO - train.train_snli_ve - loss is tensor(0.5930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4740/16548 [2:11:53<5:32:12,  1.69s/it]11/15/2022 19:18:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.6844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:14 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4741/16548 [2:11:54<5:28:52,  1.67s/it]11/15/2022 19:18:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.8054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:15 - INFO - train.train_snli_ve - loss is tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4742/16548 [2:11:56<5:31:44,  1.69s/it]11/15/2022 19:18:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.1344e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:17 - INFO - train.train_snli_ve - loss is tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4743/16548 [2:11:58<5:34:58,  1.70s/it]11/15/2022 19:18:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:19 - INFO - train.train_snli_ve - loss is tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4744/16548 [2:12:00<5:33:46,  1.70s/it]11/15/2022 19:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(9.2138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:21 - INFO - train.train_snli_ve - loss is tensor(0.5289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4745/16548 [2:12:01<5:32:04,  1.69s/it]11/15/2022 19:18:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:22 - INFO - train.train_snli_ve - loss is tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4746/16548 [2:12:03<5:29:44,  1.68s/it]11/15/2022 19:18:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:24 - INFO - train.train_snli_ve - loss is tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4747/16548 [2:12:05<5:28:21,  1.67s/it]11/15/2022 19:18:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0323e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:26 - INFO - train.train_snli_ve - loss is tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4748/16548 [2:12:06<5:28:43,  1.67s/it]11/15/2022 19:18:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:27 - INFO - train.train_snli_ve - loss is tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4749/16548 [2:12:08<5:28:31,  1.67s/it]11/15/2022 19:18:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.3248e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:29 - INFO - train.train_snli_ve - loss is tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4750/16548 [2:12:10<5:28:10,  1.67s/it]11/15/2022 19:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:31 - INFO - train.train_snli_ve - loss is tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4751/16548 [2:12:11<5:27:10,  1.66s/it]11/15/2022 19:18:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:32 - INFO - train.train_snli_ve - loss is tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4752/16548 [2:12:13<5:27:04,  1.66s/it]11/15/2022 19:18:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:34 - INFO - train.train_snli_ve - loss is tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4753/16548 [2:12:15<5:28:21,  1.67s/it]11/15/2022 19:18:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0628e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:35 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4754/16548 [2:12:16<5:25:44,  1.66s/it]11/15/2022 19:18:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.3610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:37 - INFO - train.train_snli_ve - loss is tensor(0.8625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4755/16548 [2:12:18<5:24:40,  1.65s/it]11/15/2022 19:18:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.1386e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:39 - INFO - train.train_snli_ve - loss is tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4756/16548 [2:12:20<5:24:46,  1.65s/it]11/15/2022 19:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:40 - INFO - train.train_snli_ve - loss is tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4757/16548 [2:12:21<5:28:25,  1.67s/it]11/15/2022 19:18:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.1025e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:42 - INFO - train.train_snli_ve - loss is tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4758/16548 [2:12:23<5:30:11,  1.68s/it]11/15/2022 19:18:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.0902e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:44 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4759/16548 [2:12:25<5:29:17,  1.68s/it]11/15/2022 19:18:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2061e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:46 - INFO - train.train_snli_ve - loss is tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4760/16548 [2:12:26<5:30:16,  1.68s/it]11/15/2022 19:18:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.3329e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:47 - INFO - train.train_snli_ve - loss is tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4761/16548 [2:12:28<5:32:19,  1.69s/it]11/15/2022 19:18:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.7689e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:49 - INFO - train.train_snli_ve - loss is tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4762/16548 [2:12:30<5:32:39,  1.69s/it]11/15/2022 19:18:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.4956e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:51 - INFO - train.train_snli_ve - loss is tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4763/16548 [2:12:31<5:30:25,  1.68s/it]11/15/2022 19:18:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.7453e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:52 - INFO - train.train_snli_ve - loss is tensor(0.4384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4764/16548 [2:12:33<5:30:48,  1.68s/it]11/15/2022 19:18:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.2303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:54 - INFO - train.train_snli_ve - loss is tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4765/16548 [2:12:35<5:31:57,  1.69s/it]11/15/2022 19:18:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.1722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:56 - INFO - train.train_snli_ve - loss is tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4766/16548 [2:12:37<5:36:58,  1.72s/it]11/15/2022 19:18:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.1773e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:57 - INFO - train.train_snli_ve - loss is tensor(0.7821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4767/16548 [2:12:38<5:34:20,  1.70s/it]11/15/2022 19:18:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.3736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:18:59 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4768/16548 [2:12:40<5:32:10,  1.69s/it]11/15/2022 19:19:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.3412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:01 - INFO - train.train_snli_ve - loss is tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4769/16548 [2:12:42<5:31:33,  1.69s/it]11/15/2022 19:19:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.6512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:02 - INFO - train.train_snli_ve - loss is tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4770/16548 [2:12:43<5:29:52,  1.68s/it]11/15/2022 19:19:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.5912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:04 - INFO - train.train_snli_ve - loss is tensor(0.9037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4771/16548 [2:12:45<5:31:09,  1.69s/it]11/15/2022 19:19:06 - INFO - train.train_snli_ve - kd_loss is tensor(8.9552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:06 - INFO - train.train_snli_ve - loss is tensor(0.8551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4772/16548 [2:12:47<5:31:12,  1.69s/it]11/15/2022 19:19:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.4088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:08 - INFO - train.train_snli_ve - loss is tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4773/16548 [2:12:48<5:28:52,  1.68s/it]11/15/2022 19:19:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.8252e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:09 - INFO - train.train_snli_ve - loss is tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4774/16548 [2:12:50<5:29:14,  1.68s/it]11/15/2022 19:19:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:11 - INFO - train.train_snli_ve - loss is tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4775/16548 [2:12:52<5:27:22,  1.67s/it]11/15/2022 19:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.6000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:12 - INFO - train.train_snli_ve - loss is tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4776/16548 [2:12:53<5:25:38,  1.66s/it]11/15/2022 19:19:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.5019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:14 - INFO - train.train_snli_ve - loss is tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4777/16548 [2:12:55<5:23:21,  1.65s/it]11/15/2022 19:19:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:16 - INFO - train.train_snli_ve - loss is tensor(0.8670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4778/16548 [2:12:56<5:22:27,  1.64s/it]11/15/2022 19:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:17 - INFO - train.train_snli_ve - loss is tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4779/16548 [2:12:58<5:22:44,  1.65s/it]11/15/2022 19:19:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.7836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:19 - INFO - train.train_snli_ve - loss is tensor(0.7850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4780/16548 [2:13:00<5:25:19,  1.66s/it]11/15/2022 19:19:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.8294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:21 - INFO - train.train_snli_ve - loss is tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4781/16548 [2:13:01<5:25:55,  1.66s/it]11/15/2022 19:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.7888e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:22 - INFO - train.train_snli_ve - loss is tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4782/16548 [2:13:03<5:27:38,  1.67s/it]11/15/2022 19:19:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.4757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:24 - INFO - train.train_snli_ve - loss is tensor(1.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4783/16548 [2:13:05<5:32:01,  1.69s/it]11/15/2022 19:19:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.5879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:26 - INFO - train.train_snli_ve - loss is tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4784/16548 [2:13:07<5:30:50,  1.69s/it]11/15/2022 19:19:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.6417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:28 - INFO - train.train_snli_ve - loss is tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4785/16548 [2:13:08<5:32:09,  1.69s/it]11/15/2022 19:19:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.1894e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:29 - INFO - train.train_snli_ve - loss is tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4786/16548 [2:13:10<5:32:44,  1.70s/it]11/15/2022 19:19:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.3696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:31 - INFO - train.train_snli_ve - loss is tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4787/16548 [2:13:12<5:30:44,  1.69s/it]11/15/2022 19:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.0933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:33 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4788/16548 [2:13:13<5:32:00,  1.69s/it]11/15/2022 19:19:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.5608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:34 - INFO - train.train_snli_ve - loss is tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4789/16548 [2:13:15<5:31:59,  1.69s/it]11/15/2022 19:19:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.5409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:36 - INFO - train.train_snli_ve - loss is tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4790/16548 [2:13:17<5:32:34,  1.70s/it]11/15/2022 19:19:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.1658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:38 - INFO - train.train_snli_ve - loss is tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4791/16548 [2:13:18<5:32:13,  1.70s/it]11/15/2022 19:19:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.5164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:39 - INFO - train.train_snli_ve - loss is tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4792/16548 [2:13:20<5:29:13,  1.68s/it]11/15/2022 19:19:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.8438e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:41 - INFO - train.train_snli_ve - loss is tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4793/16548 [2:13:22<5:32:06,  1.70s/it]11/15/2022 19:19:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.1683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:43 - INFO - train.train_snli_ve - loss is tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4794/16548 [2:13:24<5:29:52,  1.68s/it]11/15/2022 19:19:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.7862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:45 - INFO - train.train_snli_ve - loss is tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4795/16548 [2:13:25<5:33:13,  1.70s/it]11/15/2022 19:19:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.5458e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:46 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4796/16548 [2:13:27<5:29:49,  1.68s/it]11/15/2022 19:19:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.2250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:48 - INFO - train.train_snli_ve - loss is tensor(0.8587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4797/16548 [2:13:29<5:28:58,  1.68s/it]11/15/2022 19:19:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.4241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:50 - INFO - train.train_snli_ve - loss is tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4798/16548 [2:13:30<5:31:06,  1.69s/it]11/15/2022 19:19:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.4278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:51 - INFO - train.train_snli_ve - loss is tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4799/16548 [2:13:32<5:28:11,  1.68s/it]11/15/2022 19:19:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.8141e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:53 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4800/16548 [2:13:34<5:44:08,  1.76s/it]11/15/2022 19:19:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.6622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:55 - INFO - train.train_snli_ve - loss is tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4801/16548 [2:13:35<5:35:47,  1.72s/it]11/15/2022 19:19:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.8934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:56 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4802/16548 [2:13:37<5:31:38,  1.69s/it]11/15/2022 19:19:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.6134e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:19:58 - INFO - train.train_snli_ve - loss is tensor(0.8039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4803/16548 [2:13:39<5:29:02,  1.68s/it]11/15/2022 19:20:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.1365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:00 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4804/16548 [2:13:40<5:27:32,  1.67s/it]11/15/2022 19:20:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.8549e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:01 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4805/16548 [2:13:42<5:31:44,  1.69s/it]11/15/2022 19:20:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.9794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:03 - INFO - train.train_snli_ve - loss is tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4806/16548 [2:13:44<5:31:31,  1.69s/it]11/15/2022 19:20:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.4676e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:05 - INFO - train.train_snli_ve - loss is tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4807/16548 [2:13:46<5:28:23,  1.68s/it]11/15/2022 19:20:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.4256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:06 - INFO - train.train_snli_ve - loss is tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4808/16548 [2:13:47<5:29:09,  1.68s/it]11/15/2022 19:20:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.1851e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:08 - INFO - train.train_snli_ve - loss is tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4809/16548 [2:13:49<5:29:27,  1.68s/it]11/15/2022 19:20:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.3234e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:10 - INFO - train.train_snli_ve - loss is tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4810/16548 [2:13:51<5:28:30,  1.68s/it]11/15/2022 19:20:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.3784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:12 - INFO - train.train_snli_ve - loss is tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4811/16548 [2:13:52<5:30:41,  1.69s/it]11/15/2022 19:20:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.9441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:13 - INFO - train.train_snli_ve - loss is tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4812/16548 [2:13:54<5:32:44,  1.70s/it]11/15/2022 19:20:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.2393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:15 - INFO - train.train_snli_ve - loss is tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4813/16548 [2:13:56<5:34:36,  1.71s/it]11/15/2022 19:20:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.9068e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:17 - INFO - train.train_snli_ve - loss is tensor(0.5650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4814/16548 [2:13:57<5:31:41,  1.70s/it]11/15/2022 19:20:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.2538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:18 - INFO - train.train_snli_ve - loss is tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4815/16548 [2:13:59<5:32:56,  1.70s/it]11/15/2022 19:20:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2521e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:20 - INFO - train.train_snli_ve - loss is tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4816/16548 [2:14:01<5:33:55,  1.71s/it]11/15/2022 19:20:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.1642e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:22 - INFO - train.train_snli_ve - loss is tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4817/16548 [2:14:02<5:31:22,  1.69s/it]11/15/2022 19:20:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.7317e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:23 - INFO - train.train_snli_ve - loss is tensor(0.8815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4818/16548 [2:14:04<5:31:06,  1.69s/it]11/15/2022 19:20:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.8716e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:25 - INFO - train.train_snli_ve - loss is tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4819/16548 [2:14:06<5:28:53,  1.68s/it]11/15/2022 19:20:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.0284e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:27 - INFO - train.train_snli_ve - loss is tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4820/16548 [2:14:08<5:28:28,  1.68s/it]11/15/2022 19:20:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.1382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:29 - INFO - train.train_snli_ve - loss is tensor(0.9121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4821/16548 [2:14:09<5:31:39,  1.70s/it]11/15/2022 19:20:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.9929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:30 - INFO - train.train_snli_ve - loss is tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4822/16548 [2:14:11<5:30:05,  1.69s/it]11/15/2022 19:20:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4203e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:32 - INFO - train.train_snli_ve - loss is tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4823/16548 [2:14:13<5:30:38,  1.69s/it]11/15/2022 19:20:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.3377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:34 - INFO - train.train_snli_ve - loss is tensor(0.8043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4824/16548 [2:14:14<5:29:12,  1.68s/it]11/15/2022 19:20:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.4116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:35 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4825/16548 [2:14:16<5:29:25,  1.69s/it]11/15/2022 19:20:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.3434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:37 - INFO - train.train_snli_ve - loss is tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4826/16548 [2:14:18<5:26:14,  1.67s/it]11/15/2022 19:20:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.2177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:39 - INFO - train.train_snli_ve - loss is tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4827/16548 [2:14:19<5:23:46,  1.66s/it]11/15/2022 19:20:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0374e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:40 - INFO - train.train_snli_ve - loss is tensor(0.5807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4828/16548 [2:14:21<5:23:29,  1.66s/it]11/15/2022 19:20:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:42 - INFO - train.train_snli_ve - loss is tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4829/16548 [2:14:23<5:23:13,  1.65s/it]11/15/2022 19:20:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.7708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:43 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4830/16548 [2:14:24<5:23:10,  1.65s/it]11/15/2022 19:20:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.9766e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:45 - INFO - train.train_snli_ve - loss is tensor(0.5388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4831/16548 [2:14:26<5:23:15,  1.66s/it]11/15/2022 19:20:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.8426e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:47 - INFO - train.train_snli_ve - loss is tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4832/16548 [2:14:28<5:23:23,  1.66s/it]11/15/2022 19:20:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.5184e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:48 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4833/16548 [2:14:29<5:25:20,  1.67s/it]11/15/2022 19:20:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.0258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:50 - INFO - train.train_snli_ve - loss is tensor(0.8294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4834/16548 [2:14:31<5:24:19,  1.66s/it]11/15/2022 19:20:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.6240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:52 - INFO - train.train_snli_ve - loss is tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4835/16548 [2:14:33<5:24:07,  1.66s/it]11/15/2022 19:20:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.2715e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:53 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4836/16548 [2:14:34<5:27:00,  1.68s/it]11/15/2022 19:20:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:55 - INFO - train.train_snli_ve - loss is tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4837/16548 [2:14:36<5:26:03,  1.67s/it]11/15/2022 19:20:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.2040e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:57 - INFO - train.train_snli_ve - loss is tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4838/16548 [2:14:38<5:25:32,  1.67s/it]11/15/2022 19:20:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.7899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:20:59 - INFO - train.train_snli_ve - loss is tensor(0.7864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4839/16548 [2:14:39<5:27:37,  1.68s/it]11/15/2022 19:21:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.4657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:00 - INFO - train.train_snli_ve - loss is tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4840/16548 [2:14:41<5:27:50,  1.68s/it]11/15/2022 19:21:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.6519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:02 - INFO - train.train_snli_ve - loss is tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4841/16548 [2:14:43<5:26:50,  1.68s/it]11/15/2022 19:21:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.4217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:04 - INFO - train.train_snli_ve - loss is tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4842/16548 [2:14:44<5:26:00,  1.67s/it]11/15/2022 19:21:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.4027e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:05 - INFO - train.train_snli_ve - loss is tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4843/16548 [2:14:46<5:27:50,  1.68s/it]11/15/2022 19:21:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.9582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:07 - INFO - train.train_snli_ve - loss is tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4844/16548 [2:14:48<5:26:28,  1.67s/it]11/15/2022 19:21:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.1338e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:09 - INFO - train.train_snli_ve - loss is tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4845/16548 [2:14:49<5:26:02,  1.67s/it]11/15/2022 19:21:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.0160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:10 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4846/16548 [2:14:51<5:26:08,  1.67s/it]11/15/2022 19:21:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.5061e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:12 - INFO - train.train_snli_ve - loss is tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4847/16548 [2:14:53<5:27:26,  1.68s/it]11/15/2022 19:21:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.0797e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:14 - INFO - train.train_snli_ve - loss is tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4848/16548 [2:14:54<5:29:42,  1.69s/it]11/15/2022 19:21:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.7360e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:15 - INFO - train.train_snli_ve - loss is tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4849/16548 [2:14:56<5:27:41,  1.68s/it]11/15/2022 19:21:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.3709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:17 - INFO - train.train_snli_ve - loss is tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4850/16548 [2:14:58<5:27:53,  1.68s/it]11/15/2022 19:21:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.6536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:19 - INFO - train.train_snli_ve - loss is tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4851/16548 [2:14:59<5:28:27,  1.68s/it]11/15/2022 19:21:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:20 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4852/16548 [2:15:01<5:27:35,  1.68s/it]11/15/2022 19:21:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:22 - INFO - train.train_snli_ve - loss is tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4853/16548 [2:15:03<5:29:25,  1.69s/it]11/15/2022 19:21:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.9320e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:24 - INFO - train.train_snli_ve - loss is tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4854/16548 [2:15:05<5:31:25,  1.70s/it]11/15/2022 19:21:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:25 - INFO - train.train_snli_ve - loss is tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4855/16548 [2:15:06<5:30:04,  1.69s/it]11/15/2022 19:21:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:27 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4856/16548 [2:15:08<5:27:30,  1.68s/it]11/15/2022 19:21:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.0063e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:29 - INFO - train.train_snli_ve - loss is tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4857/16548 [2:15:10<5:26:36,  1.68s/it]11/15/2022 19:21:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.2156e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:30 - INFO - train.train_snli_ve - loss is tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4858/16548 [2:15:11<5:27:27,  1.68s/it]11/15/2022 19:21:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:32 - INFO - train.train_snli_ve - loss is tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4859/16548 [2:15:13<5:24:30,  1.67s/it]11/15/2022 19:21:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:34 - INFO - train.train_snli_ve - loss is tensor(0.4812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4860/16548 [2:15:15<5:25:06,  1.67s/it]11/15/2022 19:21:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:35 - INFO - train.train_snli_ve - loss is tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4861/16548 [2:15:16<5:25:21,  1.67s/it]11/15/2022 19:21:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.7000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:37 - INFO - train.train_snli_ve - loss is tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4862/16548 [2:15:18<5:28:57,  1.69s/it]11/15/2022 19:21:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.0080e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:39 - INFO - train.train_snli_ve - loss is tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4863/16548 [2:15:20<5:29:28,  1.69s/it]11/15/2022 19:21:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.0491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:41 - INFO - train.train_snli_ve - loss is tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4864/16548 [2:15:21<5:29:59,  1.69s/it]11/15/2022 19:21:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.6629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:42 - INFO - train.train_snli_ve - loss is tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4865/16548 [2:15:23<5:29:37,  1.69s/it]11/15/2022 19:21:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:44 - INFO - train.train_snli_ve - loss is tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4866/16548 [2:15:25<5:31:28,  1.70s/it]11/15/2022 19:21:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:46 - INFO - train.train_snli_ve - loss is tensor(0.9774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4867/16548 [2:15:26<5:27:48,  1.68s/it]11/15/2022 19:21:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:47 - INFO - train.train_snli_ve - loss is tensor(0.4548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4868/16548 [2:15:28<5:29:58,  1.70s/it]11/15/2022 19:21:49 - INFO - train.train_snli_ve - kd_loss is tensor(9.1804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:49 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4869/16548 [2:15:30<5:29:39,  1.69s/it]11/15/2022 19:21:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:51 - INFO - train.train_snli_ve - loss is tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4870/16548 [2:15:31<5:30:59,  1.70s/it]11/15/2022 19:21:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:52 - INFO - train.train_snli_ve - loss is tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4871/16548 [2:15:33<5:28:35,  1.69s/it]11/15/2022 19:21:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.0101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:54 - INFO - train.train_snli_ve - loss is tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4872/16548 [2:15:35<5:31:23,  1.70s/it]11/15/2022 19:21:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.4979e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:56 - INFO - train.train_snli_ve - loss is tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4873/16548 [2:15:37<5:29:50,  1.70s/it]11/15/2022 19:21:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.0168e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:58 - INFO - train.train_snli_ve - loss is tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4874/16548 [2:15:38<5:29:43,  1.69s/it]11/15/2022 19:21:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.2596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:21:59 - INFO - train.train_snli_ve - loss is tensor(0.8457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4875/16548 [2:15:40<5:26:13,  1.68s/it]11/15/2022 19:22:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:01 - INFO - train.train_snli_ve - loss is tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4876/16548 [2:15:42<5:27:46,  1.68s/it]11/15/2022 19:22:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.1492e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:03 - INFO - train.train_snli_ve - loss is tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4877/16548 [2:15:43<5:27:44,  1.68s/it]11/15/2022 19:22:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.2719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:04 - INFO - train.train_snli_ve - loss is tensor(0.4810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4878/16548 [2:15:45<5:28:12,  1.69s/it]11/15/2022 19:22:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2251e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:06 - INFO - train.train_snli_ve - loss is tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4879/16548 [2:15:47<5:25:53,  1.68s/it]11/15/2022 19:22:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.4051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:08 - INFO - train.train_snli_ve - loss is tensor(0.7443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4880/16548 [2:15:48<5:24:26,  1.67s/it]11/15/2022 19:22:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0568e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:09 - INFO - train.train_snli_ve - loss is tensor(0.7942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29% 4881/16548 [2:15:50<5:22:49,  1.66s/it]11/15/2022 19:22:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.1698e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:11 - INFO - train.train_snli_ve - loss is tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4882/16548 [2:15:52<5:28:42,  1.69s/it]11/15/2022 19:22:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.9244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:13 - INFO - train.train_snli_ve - loss is tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4883/16548 [2:15:53<5:26:31,  1.68s/it]11/15/2022 19:22:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.0030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:14 - INFO - train.train_snli_ve - loss is tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4884/16548 [2:15:55<5:25:34,  1.67s/it]11/15/2022 19:22:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.6860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:16 - INFO - train.train_snli_ve - loss is tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4885/16548 [2:15:57<5:24:39,  1.67s/it]11/15/2022 19:22:18 - INFO - train.train_snli_ve - kd_loss is tensor(8.0065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:18 - INFO - train.train_snli_ve - loss is tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4886/16548 [2:15:58<5:23:25,  1.66s/it]11/15/2022 19:22:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.9006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:19 - INFO - train.train_snli_ve - loss is tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4887/16548 [2:16:00<5:24:03,  1.67s/it]11/15/2022 19:22:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.5209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:21 - INFO - train.train_snli_ve - loss is tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4888/16548 [2:16:02<5:26:12,  1.68s/it]11/15/2022 19:22:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:23 - INFO - train.train_snli_ve - loss is tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4889/16548 [2:16:03<5:24:42,  1.67s/it]11/15/2022 19:22:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.8848e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:24 - INFO - train.train_snli_ve - loss is tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4890/16548 [2:16:05<5:28:25,  1.69s/it]11/15/2022 19:22:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.6622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:26 - INFO - train.train_snli_ve - loss is tensor(0.8062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4891/16548 [2:16:07<5:26:19,  1.68s/it]11/15/2022 19:22:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0899e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:28 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4892/16548 [2:16:08<5:23:35,  1.67s/it]11/15/2022 19:22:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:29 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4893/16548 [2:16:10<5:24:22,  1.67s/it]11/15/2022 19:22:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.0644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:31 - INFO - train.train_snli_ve - loss is tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4894/16548 [2:16:12<5:24:28,  1.67s/it]11/15/2022 19:22:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:33 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4895/16548 [2:16:13<5:28:02,  1.69s/it]11/15/2022 19:22:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.8611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:34 - INFO - train.train_snli_ve - loss is tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4896/16548 [2:16:15<5:31:56,  1.71s/it]11/15/2022 19:22:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.5508e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:36 - INFO - train.train_snli_ve - loss is tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4897/16548 [2:16:17<5:34:11,  1.72s/it]11/15/2022 19:22:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.1100e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:38 - INFO - train.train_snli_ve - loss is tensor(0.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4898/16548 [2:16:19<5:31:45,  1.71s/it]11/15/2022 19:22:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.2799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:40 - INFO - train.train_snli_ve - loss is tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4899/16548 [2:16:20<5:30:06,  1.70s/it]11/15/2022 19:22:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.3412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:41 - INFO - train.train_snli_ve - loss is tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4900/16548 [2:16:22<5:32:10,  1.71s/it]11/15/2022 19:22:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.3519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:43 - INFO - train.train_snli_ve - loss is tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4901/16548 [2:16:24<5:30:11,  1.70s/it]11/15/2022 19:22:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.4664e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:45 - INFO - train.train_snli_ve - loss is tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4902/16548 [2:16:25<5:28:10,  1.69s/it]11/15/2022 19:22:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.2835e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:46 - INFO - train.train_snli_ve - loss is tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4903/16548 [2:16:27<5:25:03,  1.67s/it]11/15/2022 19:22:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.6007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:48 - INFO - train.train_snli_ve - loss is tensor(0.8143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4904/16548 [2:16:29<5:23:04,  1.66s/it]11/15/2022 19:22:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.9146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:50 - INFO - train.train_snli_ve - loss is tensor(0.8514, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4905/16548 [2:16:30<5:24:52,  1.67s/it]11/15/2022 19:22:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.3308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:51 - INFO - train.train_snli_ve - loss is tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4906/16548 [2:16:32<5:24:26,  1.67s/it]11/15/2022 19:22:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.5500e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:53 - INFO - train.train_snli_ve - loss is tensor(0.9875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4907/16548 [2:16:34<5:25:25,  1.68s/it]11/15/2022 19:22:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:55 - INFO - train.train_snli_ve - loss is tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4908/16548 [2:16:35<5:25:03,  1.68s/it]11/15/2022 19:22:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.2333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:56 - INFO - train.train_snli_ve - loss is tensor(0.8076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4909/16548 [2:16:37<5:26:56,  1.69s/it]11/15/2022 19:22:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.4236e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:22:58 - INFO - train.train_snli_ve - loss is tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4910/16548 [2:16:39<5:21:27,  1.66s/it]11/15/2022 19:23:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.1356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:00 - INFO - train.train_snli_ve - loss is tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4911/16548 [2:16:40<5:21:46,  1.66s/it]11/15/2022 19:23:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:01 - INFO - train.train_snli_ve - loss is tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4912/16548 [2:16:42<5:23:31,  1.67s/it]11/15/2022 19:23:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.5644e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:03 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4913/16548 [2:16:44<5:23:04,  1.67s/it]11/15/2022 19:23:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.1400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:05 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4914/16548 [2:16:45<5:23:10,  1.67s/it]11/15/2022 19:23:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.4817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:06 - INFO - train.train_snli_ve - loss is tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4915/16548 [2:16:47<5:21:01,  1.66s/it]11/15/2022 19:23:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.9763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:08 - INFO - train.train_snli_ve - loss is tensor(0.4518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4916/16548 [2:16:49<5:23:16,  1.67s/it]11/15/2022 19:23:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.4594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:10 - INFO - train.train_snli_ve - loss is tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4917/16548 [2:16:50<5:23:06,  1.67s/it]11/15/2022 19:23:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:11 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4918/16548 [2:16:52<5:26:15,  1.68s/it]11/15/2022 19:23:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.5333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:13 - INFO - train.train_snli_ve - loss is tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4919/16548 [2:16:54<5:26:32,  1.68s/it]11/15/2022 19:23:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.8418e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:15 - INFO - train.train_snli_ve - loss is tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4920/16548 [2:16:55<5:27:25,  1.69s/it]11/15/2022 19:23:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.5798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:16 - INFO - train.train_snli_ve - loss is tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4921/16548 [2:16:57<5:29:26,  1.70s/it]11/15/2022 19:23:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:18 - INFO - train.train_snli_ve - loss is tensor(0.5057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4922/16548 [2:16:59<5:27:15,  1.69s/it]11/15/2022 19:23:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.8638e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:20 - INFO - train.train_snli_ve - loss is tensor(0.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4923/16548 [2:17:01<5:25:48,  1.68s/it]11/15/2022 19:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:21 - INFO - train.train_snli_ve - loss is tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4924/16548 [2:17:02<5:24:46,  1.68s/it]11/15/2022 19:23:23 - INFO - train.train_snli_ve - kd_loss is tensor(5.5207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:23 - INFO - train.train_snli_ve - loss is tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4925/16548 [2:17:04<5:25:33,  1.68s/it]11/15/2022 19:23:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.3316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:25 - INFO - train.train_snli_ve - loss is tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4926/16548 [2:17:06<5:28:04,  1.69s/it]11/15/2022 19:23:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.1556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:27 - INFO - train.train_snli_ve - loss is tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4927/16548 [2:17:07<5:27:20,  1.69s/it]11/15/2022 19:23:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:28 - INFO - train.train_snli_ve - loss is tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4928/16548 [2:17:09<5:28:39,  1.70s/it]11/15/2022 19:23:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.7905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:30 - INFO - train.train_snli_ve - loss is tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4929/16548 [2:17:11<5:29:14,  1.70s/it]11/15/2022 19:23:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.4300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:32 - INFO - train.train_snli_ve - loss is tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4930/16548 [2:17:12<5:29:22,  1.70s/it]11/15/2022 19:23:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:33 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4931/16548 [2:17:14<5:28:21,  1.70s/it]11/15/2022 19:23:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.6571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:35 - INFO - train.train_snli_ve - loss is tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4932/16548 [2:17:16<5:27:12,  1.69s/it]11/15/2022 19:23:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:37 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4933/16548 [2:17:17<5:24:14,  1.67s/it]11/15/2022 19:23:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:38 - INFO - train.train_snli_ve - loss is tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4934/16548 [2:17:19<5:26:34,  1.69s/it]11/15/2022 19:23:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.9517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:40 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4935/16548 [2:17:21<5:27:06,  1.69s/it]11/15/2022 19:23:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.9675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:42 - INFO - train.train_snli_ve - loss is tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4936/16548 [2:17:23<5:28:56,  1.70s/it]11/15/2022 19:23:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:43 - INFO - train.train_snli_ve - loss is tensor(0.4874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4937/16548 [2:17:24<5:28:28,  1.70s/it]11/15/2022 19:23:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:45 - INFO - train.train_snli_ve - loss is tensor(0.7671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4938/16548 [2:17:26<5:28:05,  1.70s/it]11/15/2022 19:23:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4034e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:47 - INFO - train.train_snli_ve - loss is tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4939/16548 [2:17:28<5:27:08,  1.69s/it]11/15/2022 19:23:49 - INFO - train.train_snli_ve - kd_loss is tensor(7.2006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:49 - INFO - train.train_snli_ve - loss is tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4940/16548 [2:17:29<5:30:00,  1.71s/it]11/15/2022 19:23:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.7962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:50 - INFO - train.train_snli_ve - loss is tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4941/16548 [2:17:31<5:27:46,  1.69s/it]11/15/2022 19:23:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0905e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:52 - INFO - train.train_snli_ve - loss is tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4942/16548 [2:17:33<5:27:05,  1.69s/it]11/15/2022 19:23:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.9197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:54 - INFO - train.train_snli_ve - loss is tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4943/16548 [2:17:34<5:27:57,  1.70s/it]11/15/2022 19:23:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.5700e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:55 - INFO - train.train_snli_ve - loss is tensor(0.9146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4944/16548 [2:17:36<5:26:06,  1.69s/it]11/15/2022 19:23:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.0448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:57 - INFO - train.train_snli_ve - loss is tensor(0.9145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4945/16548 [2:17:38<5:23:24,  1.67s/it]11/15/2022 19:23:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2089e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:23:59 - INFO - train.train_snli_ve - loss is tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4946/16548 [2:17:39<5:24:20,  1.68s/it]11/15/2022 19:24:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:00 - INFO - train.train_snli_ve - loss is tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4947/16548 [2:17:41<5:22:38,  1.67s/it]11/15/2022 19:24:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:02 - INFO - train.train_snli_ve - loss is tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4948/16548 [2:17:43<5:24:54,  1.68s/it]11/15/2022 19:24:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.5123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:04 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4949/16548 [2:17:44<5:23:08,  1.67s/it]11/15/2022 19:24:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:05 - INFO - train.train_snli_ve - loss is tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4950/16548 [2:17:46<5:24:36,  1.68s/it]11/15/2022 19:24:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.5277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:07 - INFO - train.train_snli_ve - loss is tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4951/16548 [2:17:48<5:22:51,  1.67s/it]11/15/2022 19:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.4157e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:09 - INFO - train.train_snli_ve - loss is tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4952/16548 [2:17:49<5:24:56,  1.68s/it]11/15/2022 19:24:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0953e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:10 - INFO - train.train_snli_ve - loss is tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4953/16548 [2:17:51<5:25:08,  1.68s/it]11/15/2022 19:24:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.4751e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:12 - INFO - train.train_snli_ve - loss is tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4954/16548 [2:17:53<5:24:52,  1.68s/it]11/15/2022 19:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.4717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:14 - INFO - train.train_snli_ve - loss is tensor(0.8536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4955/16548 [2:17:54<5:24:14,  1.68s/it]11/15/2022 19:24:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.1966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:15 - INFO - train.train_snli_ve - loss is tensor(0.4502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4956/16548 [2:17:56<5:24:39,  1.68s/it]11/15/2022 19:24:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.3472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:17 - INFO - train.train_snli_ve - loss is tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4957/16548 [2:17:58<5:23:17,  1.67s/it]11/15/2022 19:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.9546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:19 - INFO - train.train_snli_ve - loss is tensor(0.8803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4958/16548 [2:18:00<5:22:44,  1.67s/it]11/15/2022 19:24:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.2161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:20 - INFO - train.train_snli_ve - loss is tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4959/16548 [2:18:01<5:20:50,  1.66s/it]11/15/2022 19:24:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.1553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:22 - INFO - train.train_snli_ve - loss is tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4960/16548 [2:18:03<5:23:00,  1.67s/it]11/15/2022 19:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.9019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:24 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4961/16548 [2:18:04<5:21:31,  1.66s/it]11/15/2022 19:24:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.6318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:25 - INFO - train.train_snli_ve - loss is tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4962/16548 [2:18:06<5:23:25,  1.67s/it]11/15/2022 19:24:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.6914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:27 - INFO - train.train_snli_ve - loss is tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4963/16548 [2:18:08<5:25:10,  1.68s/it]11/15/2022 19:24:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.3016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:29 - INFO - train.train_snli_ve - loss is tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4964/16548 [2:18:10<5:27:11,  1.69s/it]11/15/2022 19:24:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.0018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:31 - INFO - train.train_snli_ve - loss is tensor(0.8773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4965/16548 [2:18:11<5:24:30,  1.68s/it]11/15/2022 19:24:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.8441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:32 - INFO - train.train_snli_ve - loss is tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4966/16548 [2:18:13<5:25:15,  1.69s/it]11/15/2022 19:24:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.8472e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:34 - INFO - train.train_snli_ve - loss is tensor(0.8907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4967/16548 [2:18:15<5:24:06,  1.68s/it]11/15/2022 19:24:36 - INFO - train.train_snli_ve - kd_loss is tensor(6.3749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:36 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4968/16548 [2:18:16<5:25:20,  1.69s/it]11/15/2022 19:24:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.0441e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:37 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4969/16548 [2:18:18<5:26:42,  1.69s/it]11/15/2022 19:24:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.0133e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:39 - INFO - train.train_snli_ve - loss is tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4970/16548 [2:18:20<5:25:05,  1.68s/it]11/15/2022 19:24:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.7209e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:41 - INFO - train.train_snli_ve - loss is tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4971/16548 [2:18:21<5:21:12,  1.66s/it]11/15/2022 19:24:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.4903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:42 - INFO - train.train_snli_ve - loss is tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4972/16548 [2:18:23<5:21:35,  1.67s/it]11/15/2022 19:24:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.2005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:44 - INFO - train.train_snli_ve - loss is tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4973/16548 [2:18:25<5:22:13,  1.67s/it]11/15/2022 19:24:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:46 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4974/16548 [2:18:26<5:21:25,  1.67s/it]11/15/2022 19:24:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.3763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:47 - INFO - train.train_snli_ve - loss is tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4975/16548 [2:18:28<5:22:27,  1.67s/it]11/15/2022 19:24:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.0990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:49 - INFO - train.train_snli_ve - loss is tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4976/16548 [2:18:30<5:22:40,  1.67s/it]11/15/2022 19:24:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.3663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:51 - INFO - train.train_snli_ve - loss is tensor(0.9730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4977/16548 [2:18:31<5:21:47,  1.67s/it]11/15/2022 19:24:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:52 - INFO - train.train_snli_ve - loss is tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4978/16548 [2:18:33<5:22:51,  1.67s/it]11/15/2022 19:24:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.8920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:54 - INFO - train.train_snli_ve - loss is tensor(0.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4979/16548 [2:18:35<5:22:42,  1.67s/it]11/15/2022 19:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.6460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:56 - INFO - train.train_snli_ve - loss is tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4980/16548 [2:18:36<5:20:46,  1.66s/it]11/15/2022 19:24:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:57 - INFO - train.train_snli_ve - loss is tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4981/16548 [2:18:38<5:20:09,  1.66s/it]11/15/2022 19:24:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.0342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:24:59 - INFO - train.train_snli_ve - loss is tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4982/16548 [2:18:40<5:22:03,  1.67s/it]11/15/2022 19:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.8160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:01 - INFO - train.train_snli_ve - loss is tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4983/16548 [2:18:41<5:26:02,  1.69s/it]11/15/2022 19:25:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.4399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:02 - INFO - train.train_snli_ve - loss is tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4984/16548 [2:18:43<5:24:48,  1.69s/it]11/15/2022 19:25:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.4399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:04 - INFO - train.train_snli_ve - loss is tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4985/16548 [2:18:45<5:24:20,  1.68s/it]11/15/2022 19:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.6917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:06 - INFO - train.train_snli_ve - loss is tensor(0.9041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4986/16548 [2:18:46<5:24:52,  1.69s/it]11/15/2022 19:25:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.1147e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:07 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4987/16548 [2:18:48<5:23:58,  1.68s/it]11/15/2022 19:25:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.2257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:09 - INFO - train.train_snli_ve - loss is tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4988/16548 [2:18:50<5:25:47,  1.69s/it]11/15/2022 19:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.5672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:11 - INFO - train.train_snli_ve - loss is tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4989/16548 [2:18:52<5:23:04,  1.68s/it]11/15/2022 19:25:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.8118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:12 - INFO - train.train_snli_ve - loss is tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4990/16548 [2:18:53<5:22:56,  1.68s/it]11/15/2022 19:25:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.1058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:14 - INFO - train.train_snli_ve - loss is tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4991/16548 [2:18:55<5:25:57,  1.69s/it]11/15/2022 19:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.7671e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:16 - INFO - train.train_snli_ve - loss is tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4992/16548 [2:18:57<5:24:55,  1.69s/it]11/15/2022 19:25:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.1459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:17 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4993/16548 [2:18:58<5:21:27,  1.67s/it]11/15/2022 19:25:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.5836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:19 - INFO - train.train_snli_ve - loss is tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4994/16548 [2:19:00<5:25:41,  1.69s/it]11/15/2022 19:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.9952e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:21 - INFO - train.train_snli_ve - loss is tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4995/16548 [2:19:02<5:27:30,  1.70s/it]11/15/2022 19:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.7739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:23 - INFO - train.train_snli_ve - loss is tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4996/16548 [2:19:03<5:28:12,  1.70s/it]11/15/2022 19:25:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.8535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:24 - INFO - train.train_snli_ve - loss is tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4997/16548 [2:19:05<5:25:31,  1.69s/it]11/15/2022 19:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.5078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:26 - INFO - train.train_snli_ve - loss is tensor(0.8610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4998/16548 [2:19:07<5:26:37,  1.70s/it]11/15/2022 19:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.2057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:28 - INFO - train.train_snli_ve - loss is tensor(0.7999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 4999/16548 [2:19:08<5:22:38,  1.68s/it]11/15/2022 19:25:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.5699e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:29 - INFO - train.train_snli_ve - loss is tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5000/16548 [2:19:10<5:26:22,  1.70s/it]11/15/2022 19:25:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.0951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:31 - INFO - train.train_snli_ve - loss is tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5001/16548 [2:19:12<5:23:08,  1.68s/it]11/15/2022 19:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.1207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:33 - INFO - train.train_snli_ve - loss is tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5002/16548 [2:19:13<5:20:17,  1.66s/it]11/15/2022 19:25:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.6457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:34 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5003/16548 [2:19:15<5:21:30,  1.67s/it]11/15/2022 19:25:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.1673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:36 - INFO - train.train_snli_ve - loss is tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5004/16548 [2:19:17<5:21:14,  1.67s/it]11/15/2022 19:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.5961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:38 - INFO - train.train_snli_ve - loss is tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5005/16548 [2:19:18<5:21:31,  1.67s/it]11/15/2022 19:25:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.6605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:39 - INFO - train.train_snli_ve - loss is tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5006/16548 [2:19:20<5:21:15,  1.67s/it]11/15/2022 19:25:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.3121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:41 - INFO - train.train_snli_ve - loss is tensor(0.8928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5007/16548 [2:19:22<5:23:52,  1.68s/it]11/15/2022 19:25:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.6058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:43 - INFO - train.train_snli_ve - loss is tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5008/16548 [2:19:24<5:27:10,  1.70s/it]11/15/2022 19:25:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.2477e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:44 - INFO - train.train_snli_ve - loss is tensor(0.9916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5009/16548 [2:19:25<5:25:50,  1.69s/it]11/15/2022 19:25:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.8973e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:46 - INFO - train.train_snli_ve - loss is tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5010/16548 [2:19:27<5:24:50,  1.69s/it]11/15/2022 19:25:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.4718e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:48 - INFO - train.train_snli_ve - loss is tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5011/16548 [2:19:29<5:24:54,  1.69s/it]11/15/2022 19:25:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.6583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:50 - INFO - train.train_snli_ve - loss is tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5012/16548 [2:19:30<5:23:41,  1.68s/it]11/15/2022 19:25:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.8959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:51 - INFO - train.train_snli_ve - loss is tensor(0.9873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5013/16548 [2:19:32<5:23:08,  1.68s/it]11/15/2022 19:25:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.9486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:53 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5014/16548 [2:19:34<5:22:05,  1.68s/it]11/15/2022 19:25:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:55 - INFO - train.train_snli_ve - loss is tensor(0.8671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5015/16548 [2:19:35<5:20:09,  1.67s/it]11/15/2022 19:25:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.8749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:56 - INFO - train.train_snli_ve - loss is tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5016/16548 [2:19:37<5:21:02,  1.67s/it]11/15/2022 19:25:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.5421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:25:58 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5017/16548 [2:19:39<5:25:20,  1.69s/it]11/15/2022 19:26:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.9885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:00 - INFO - train.train_snli_ve - loss is tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5018/16548 [2:19:40<5:22:14,  1.68s/it]11/15/2022 19:26:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.3740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:01 - INFO - train.train_snli_ve - loss is tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5019/16548 [2:19:42<5:23:04,  1.68s/it]11/15/2022 19:26:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.0481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:03 - INFO - train.train_snli_ve - loss is tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5020/16548 [2:19:44<5:23:18,  1.68s/it]11/15/2022 19:26:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.8222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:05 - INFO - train.train_snli_ve - loss is tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5021/16548 [2:19:45<5:20:55,  1.67s/it]11/15/2022 19:26:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.5563e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:06 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5022/16548 [2:19:47<5:20:01,  1.67s/it]11/15/2022 19:26:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.1914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:08 - INFO - train.train_snli_ve - loss is tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5023/16548 [2:19:49<5:22:34,  1.68s/it]11/15/2022 19:26:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.7972e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:10 - INFO - train.train_snli_ve - loss is tensor(0.5167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5024/16548 [2:19:50<5:21:53,  1.68s/it]11/15/2022 19:26:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.5127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:11 - INFO - train.train_snli_ve - loss is tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5025/16548 [2:19:52<5:23:57,  1.69s/it]11/15/2022 19:26:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.0810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:13 - INFO - train.train_snli_ve - loss is tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5026/16548 [2:19:54<5:23:56,  1.69s/it]11/15/2022 19:26:15 - INFO - train.train_snli_ve - kd_loss is tensor(5.8037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:15 - INFO - train.train_snli_ve - loss is tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5027/16548 [2:19:56<5:27:09,  1.70s/it]11/15/2022 19:26:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:16 - INFO - train.train_snli_ve - loss is tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5028/16548 [2:19:57<5:24:46,  1.69s/it]11/15/2022 19:26:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.2237e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:18 - INFO - train.train_snli_ve - loss is tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5029/16548 [2:19:59<5:22:10,  1.68s/it]11/15/2022 19:26:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.6092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:20 - INFO - train.train_snli_ve - loss is tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5030/16548 [2:20:00<5:21:31,  1.67s/it]11/15/2022 19:26:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.1270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:21 - INFO - train.train_snli_ve - loss is tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5031/16548 [2:20:02<5:21:35,  1.68s/it]11/15/2022 19:26:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:23 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5032/16548 [2:20:04<5:21:27,  1.67s/it]11/15/2022 19:26:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.0195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:25 - INFO - train.train_snli_ve - loss is tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5033/16548 [2:20:05<5:19:39,  1.67s/it]11/15/2022 19:26:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:26 - INFO - train.train_snli_ve - loss is tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5034/16548 [2:20:07<5:19:18,  1.66s/it]11/15/2022 19:26:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.1607e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:28 - INFO - train.train_snli_ve - loss is tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5035/16548 [2:20:09<5:17:16,  1.65s/it]11/15/2022 19:26:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4801e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:30 - INFO - train.train_snli_ve - loss is tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5036/16548 [2:20:10<5:16:35,  1.65s/it]11/15/2022 19:26:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.0797e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:31 - INFO - train.train_snli_ve - loss is tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5037/16548 [2:20:12<5:19:33,  1.67s/it]11/15/2022 19:26:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.9111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:33 - INFO - train.train_snli_ve - loss is tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5038/16548 [2:20:14<5:22:29,  1.68s/it]11/15/2022 19:26:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.1373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:35 - INFO - train.train_snli_ve - loss is tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5039/16548 [2:20:15<5:21:49,  1.68s/it]11/15/2022 19:26:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.6919e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:36 - INFO - train.train_snli_ve - loss is tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5040/16548 [2:20:17<5:21:08,  1.67s/it]11/15/2022 19:26:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:38 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5041/16548 [2:20:19<5:19:49,  1.67s/it]11/15/2022 19:26:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:40 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5042/16548 [2:20:20<5:19:17,  1.67s/it]11/15/2022 19:26:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:41 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5043/16548 [2:20:22<5:21:39,  1.68s/it]11/15/2022 19:26:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:43 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5044/16548 [2:20:24<5:19:56,  1.67s/it]11/15/2022 19:26:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.9833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:45 - INFO - train.train_snli_ve - loss is tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5045/16548 [2:20:26<5:20:33,  1.67s/it]11/15/2022 19:26:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.0089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:46 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5046/16548 [2:20:27<5:22:09,  1.68s/it]11/15/2022 19:26:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:48 - INFO - train.train_snli_ve - loss is tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30% 5047/16548 [2:20:29<5:21:08,  1.68s/it]11/15/2022 19:26:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.2238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:50 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5048/16548 [2:20:31<5:18:45,  1.66s/it]11/15/2022 19:26:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.2452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:51 - INFO - train.train_snli_ve - loss is tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5049/16548 [2:20:32<5:20:14,  1.67s/it]11/15/2022 19:26:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:53 - INFO - train.train_snli_ve - loss is tensor(0.4687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5050/16548 [2:20:34<5:22:18,  1.68s/it]11/15/2022 19:26:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.7082e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:55 - INFO - train.train_snli_ve - loss is tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5051/16548 [2:20:36<5:23:32,  1.69s/it]11/15/2022 19:26:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.7728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:57 - INFO - train.train_snli_ve - loss is tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5052/16548 [2:20:37<5:22:56,  1.69s/it]11/15/2022 19:26:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:26:58 - INFO - train.train_snli_ve - loss is tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5053/16548 [2:20:39<5:23:18,  1.69s/it]11/15/2022 19:27:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.7558e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:00 - INFO - train.train_snli_ve - loss is tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5054/16548 [2:20:41<5:26:05,  1.70s/it]11/15/2022 19:27:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:02 - INFO - train.train_snli_ve - loss is tensor(0.8592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5055/16548 [2:20:42<5:25:26,  1.70s/it]11/15/2022 19:27:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.3136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:03 - INFO - train.train_snli_ve - loss is tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5056/16548 [2:20:44<5:24:38,  1.69s/it]11/15/2022 19:27:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.8988e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:05 - INFO - train.train_snli_ve - loss is tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5057/16548 [2:20:46<5:21:19,  1.68s/it]11/15/2022 19:27:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.7823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:07 - INFO - train.train_snli_ve - loss is tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5058/16548 [2:20:47<5:21:21,  1.68s/it]11/15/2022 19:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:08 - INFO - train.train_snli_ve - loss is tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5059/16548 [2:20:49<5:21:46,  1.68s/it]11/15/2022 19:27:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:10 - INFO - train.train_snli_ve - loss is tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5060/16548 [2:20:51<5:21:04,  1.68s/it]11/15/2022 19:27:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:12 - INFO - train.train_snli_ve - loss is tensor(0.8234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5061/16548 [2:20:52<5:22:40,  1.69s/it]11/15/2022 19:27:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:13 - INFO - train.train_snli_ve - loss is tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5062/16548 [2:20:54<5:22:47,  1.69s/it]11/15/2022 19:27:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6755e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:15 - INFO - train.train_snli_ve - loss is tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5063/16548 [2:20:56<5:19:57,  1.67s/it]11/15/2022 19:27:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.5794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:17 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5064/16548 [2:20:57<5:20:46,  1.68s/it]11/15/2022 19:27:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.3601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:18 - INFO - train.train_snli_ve - loss is tensor(0.8832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5065/16548 [2:20:59<5:20:38,  1.68s/it]11/15/2022 19:27:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:20 - INFO - train.train_snli_ve - loss is tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5066/16548 [2:21:01<5:19:42,  1.67s/it]11/15/2022 19:27:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:22 - INFO - train.train_snli_ve - loss is tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5067/16548 [2:21:02<5:17:56,  1.66s/it]11/15/2022 19:27:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4036e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:23 - INFO - train.train_snli_ve - loss is tensor(0.9431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5068/16548 [2:21:04<5:18:23,  1.66s/it]11/15/2022 19:27:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:25 - INFO - train.train_snli_ve - loss is tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5069/16548 [2:21:06<5:19:57,  1.67s/it]11/15/2022 19:27:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:27 - INFO - train.train_snli_ve - loss is tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5070/16548 [2:21:07<5:19:48,  1.67s/it]11/15/2022 19:27:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:28 - INFO - train.train_snli_ve - loss is tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5071/16548 [2:21:09<5:19:00,  1.67s/it]11/15/2022 19:27:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.4312e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:30 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5072/16548 [2:21:11<5:21:07,  1.68s/it]11/15/2022 19:27:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:32 - INFO - train.train_snli_ve - loss is tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5073/16548 [2:21:12<5:18:10,  1.66s/it]11/15/2022 19:27:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2521e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:33 - INFO - train.train_snli_ve - loss is tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5074/16548 [2:21:14<5:19:27,  1.67s/it]11/15/2022 19:27:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:35 - INFO - train.train_snli_ve - loss is tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5075/16548 [2:21:16<5:18:01,  1.66s/it]11/15/2022 19:27:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.5603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:37 - INFO - train.train_snli_ve - loss is tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5076/16548 [2:21:17<5:18:27,  1.67s/it]11/15/2022 19:27:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:38 - INFO - train.train_snli_ve - loss is tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5077/16548 [2:21:19<5:19:47,  1.67s/it]11/15/2022 19:27:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.5654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:40 - INFO - train.train_snli_ve - loss is tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5078/16548 [2:21:21<5:20:09,  1.67s/it]11/15/2022 19:27:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.8556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:42 - INFO - train.train_snli_ve - loss is tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5079/16548 [2:21:23<5:21:19,  1.68s/it]11/15/2022 19:27:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:44 - INFO - train.train_snli_ve - loss is tensor(0.5807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5080/16548 [2:21:24<5:22:16,  1.69s/it]11/15/2022 19:27:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.2679e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:45 - INFO - train.train_snli_ve - loss is tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5081/16548 [2:21:26<5:20:29,  1.68s/it]11/15/2022 19:27:47 - INFO - train.train_snli_ve - kd_loss is tensor(7.7885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:47 - INFO - train.train_snli_ve - loss is tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5082/16548 [2:21:28<5:19:57,  1.67s/it]11/15/2022 19:27:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.8933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:49 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5083/16548 [2:21:29<5:21:36,  1.68s/it]11/15/2022 19:27:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.8495e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:50 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5084/16548 [2:21:31<5:20:02,  1.67s/it]11/15/2022 19:27:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.9071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:52 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5085/16548 [2:21:33<5:20:32,  1.68s/it]11/15/2022 19:27:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:54 - INFO - train.train_snli_ve - loss is tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5086/16548 [2:21:34<5:20:21,  1.68s/it]11/15/2022 19:27:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.7429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:55 - INFO - train.train_snli_ve - loss is tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5087/16548 [2:21:36<5:17:50,  1.66s/it]11/15/2022 19:27:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.0683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:57 - INFO - train.train_snli_ve - loss is tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5088/16548 [2:21:38<5:18:49,  1.67s/it]11/15/2022 19:27:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.0452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:27:59 - INFO - train.train_snli_ve - loss is tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5089/16548 [2:21:39<5:22:01,  1.69s/it]11/15/2022 19:28:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.2217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:00 - INFO - train.train_snli_ve - loss is tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5090/16548 [2:21:41<5:21:26,  1.68s/it]11/15/2022 19:28:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:02 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5091/16548 [2:21:43<5:19:46,  1.67s/it]11/15/2022 19:28:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.8980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:04 - INFO - train.train_snli_ve - loss is tensor(0.8671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5092/16548 [2:21:44<5:19:34,  1.67s/it]11/15/2022 19:28:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:05 - INFO - train.train_snli_ve - loss is tensor(0.7601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5093/16548 [2:21:46<5:18:11,  1.67s/it]11/15/2022 19:28:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.5504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:07 - INFO - train.train_snli_ve - loss is tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5094/16548 [2:21:48<5:17:06,  1.66s/it]11/15/2022 19:28:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.6473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:09 - INFO - train.train_snli_ve - loss is tensor(0.8995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5095/16548 [2:21:49<5:17:33,  1.66s/it]11/15/2022 19:28:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.2012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:10 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5096/16548 [2:21:51<5:20:32,  1.68s/it]11/15/2022 19:28:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:12 - INFO - train.train_snli_ve - loss is tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5097/16548 [2:21:53<5:19:31,  1.67s/it]11/15/2022 19:28:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0959e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:14 - INFO - train.train_snli_ve - loss is tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5098/16548 [2:21:54<5:21:01,  1.68s/it]11/15/2022 19:28:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.7483e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:15 - INFO - train.train_snli_ve - loss is tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5099/16548 [2:21:56<5:23:12,  1.69s/it]11/15/2022 19:28:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:17 - INFO - train.train_snli_ve - loss is tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5100/16548 [2:21:58<5:26:07,  1.71s/it]11/15/2022 19:28:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:19 - INFO - train.train_snli_ve - loss is tensor(0.9850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5101/16548 [2:21:59<5:22:46,  1.69s/it]11/15/2022 19:28:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.5594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:20 - INFO - train.train_snli_ve - loss is tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5102/16548 [2:22:01<5:19:41,  1.68s/it]11/15/2022 19:28:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.6520e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:22 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5103/16548 [2:22:03<5:19:19,  1.67s/it]11/15/2022 19:28:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.0763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:24 - INFO - train.train_snli_ve - loss is tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5104/16548 [2:22:05<5:22:36,  1.69s/it]11/15/2022 19:28:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.9758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:25 - INFO - train.train_snli_ve - loss is tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5105/16548 [2:22:06<5:20:12,  1.68s/it]11/15/2022 19:28:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.6241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:27 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5106/16548 [2:22:08<5:21:43,  1.69s/it]11/15/2022 19:28:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.2046e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:29 - INFO - train.train_snli_ve - loss is tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5107/16548 [2:22:10<5:21:35,  1.69s/it]11/15/2022 19:28:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.3758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:30 - INFO - train.train_snli_ve - loss is tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5108/16548 [2:22:11<5:19:23,  1.68s/it]11/15/2022 19:28:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.9069e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:32 - INFO - train.train_snli_ve - loss is tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5109/16548 [2:22:13<5:17:46,  1.67s/it]11/15/2022 19:28:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.7001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:34 - INFO - train.train_snli_ve - loss is tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5110/16548 [2:22:15<5:16:57,  1.66s/it]11/15/2022 19:28:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.2966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:35 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5111/16548 [2:22:16<5:14:13,  1.65s/it]11/15/2022 19:28:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.6562e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:37 - INFO - train.train_snli_ve - loss is tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5112/16548 [2:22:18<5:13:39,  1.65s/it]11/15/2022 19:28:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.3138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:39 - INFO - train.train_snli_ve - loss is tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5113/16548 [2:22:20<5:18:34,  1.67s/it]11/15/2022 19:28:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0638e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:40 - INFO - train.train_snli_ve - loss is tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5114/16548 [2:22:21<5:17:11,  1.66s/it]11/15/2022 19:28:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.1435e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:42 - INFO - train.train_snli_ve - loss is tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5115/16548 [2:22:23<5:15:48,  1.66s/it]11/15/2022 19:28:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.3789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:44 - INFO - train.train_snli_ve - loss is tensor(0.7438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5116/16548 [2:22:24<5:15:44,  1.66s/it]11/15/2022 19:28:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.3408e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:45 - INFO - train.train_snli_ve - loss is tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5117/16548 [2:22:26<5:14:24,  1.65s/it]11/15/2022 19:28:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.0284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:47 - INFO - train.train_snli_ve - loss is tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5118/16548 [2:22:28<5:13:13,  1.64s/it]11/15/2022 19:28:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.0841e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:49 - INFO - train.train_snli_ve - loss is tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5119/16548 [2:22:29<5:17:03,  1.66s/it]11/15/2022 19:28:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.2767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:50 - INFO - train.train_snli_ve - loss is tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5120/16548 [2:22:31<5:19:01,  1.67s/it]11/15/2022 19:28:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.4202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:52 - INFO - train.train_snli_ve - loss is tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5121/16548 [2:22:33<5:18:35,  1.67s/it]11/15/2022 19:28:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.5482e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:54 - INFO - train.train_snli_ve - loss is tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5122/16548 [2:22:34<5:17:50,  1.67s/it]11/15/2022 19:28:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:55 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5123/16548 [2:22:36<5:18:23,  1.67s/it]11/15/2022 19:28:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4655e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:57 - INFO - train.train_snli_ve - loss is tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5124/16548 [2:22:38<5:18:36,  1.67s/it]11/15/2022 19:28:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:28:59 - INFO - train.train_snli_ve - loss is tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5125/16548 [2:22:39<5:18:58,  1.68s/it]11/15/2022 19:29:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.4535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:00 - INFO - train.train_snli_ve - loss is tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5126/16548 [2:22:41<5:18:52,  1.68s/it]11/15/2022 19:29:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2897e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:02 - INFO - train.train_snli_ve - loss is tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5127/16548 [2:22:43<5:18:40,  1.67s/it]11/15/2022 19:29:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.2065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:04 - INFO - train.train_snli_ve - loss is tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5128/16548 [2:22:45<5:20:53,  1.69s/it]11/15/2022 19:29:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.2694e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:05 - INFO - train.train_snli_ve - loss is tensor(0.4966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5129/16548 [2:22:46<5:18:41,  1.67s/it]11/15/2022 19:29:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.8906e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:07 - INFO - train.train_snli_ve - loss is tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5130/16548 [2:22:48<5:18:14,  1.67s/it]11/15/2022 19:29:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2323e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:09 - INFO - train.train_snli_ve - loss is tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5131/16548 [2:22:50<5:20:58,  1.69s/it]11/15/2022 19:29:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.5723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:11 - INFO - train.train_snli_ve - loss is tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5132/16548 [2:22:51<5:21:36,  1.69s/it]11/15/2022 19:29:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.6369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:12 - INFO - train.train_snli_ve - loss is tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5133/16548 [2:22:53<5:21:17,  1.69s/it]11/15/2022 19:29:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.4732e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:14 - INFO - train.train_snli_ve - loss is tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5134/16548 [2:22:55<5:21:04,  1.69s/it]11/15/2022 19:29:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2230e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:16 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5135/16548 [2:22:56<5:19:41,  1.68s/it]11/15/2022 19:29:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.3877e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:17 - INFO - train.train_snli_ve - loss is tensor(0.8446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5136/16548 [2:22:58<5:20:11,  1.68s/it]11/15/2022 19:29:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:19 - INFO - train.train_snli_ve - loss is tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5137/16548 [2:23:00<5:20:03,  1.68s/it]11/15/2022 19:29:21 - INFO - train.train_snli_ve - kd_loss is tensor(9.7954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:21 - INFO - train.train_snli_ve - loss is tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5138/16548 [2:23:01<5:18:40,  1.68s/it]11/15/2022 19:29:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:22 - INFO - train.train_snli_ve - loss is tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5139/16548 [2:23:03<5:17:44,  1.67s/it]11/15/2022 19:29:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.3537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:24 - INFO - train.train_snli_ve - loss is tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5140/16548 [2:23:05<5:16:34,  1.66s/it]11/15/2022 19:29:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:26 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5141/16548 [2:23:06<5:19:53,  1.68s/it]11/15/2022 19:29:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.2115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:27 - INFO - train.train_snli_ve - loss is tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5142/16548 [2:23:08<5:21:01,  1.69s/it]11/15/2022 19:29:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.6794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:29 - INFO - train.train_snli_ve - loss is tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5143/16548 [2:23:10<5:21:54,  1.69s/it]11/15/2022 19:29:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.0399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:31 - INFO - train.train_snli_ve - loss is tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5144/16548 [2:23:12<5:24:17,  1.71s/it]11/15/2022 19:29:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.7390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:32 - INFO - train.train_snli_ve - loss is tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5145/16548 [2:23:13<5:21:31,  1.69s/it]11/15/2022 19:29:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:34 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5146/16548 [2:23:15<5:21:40,  1.69s/it]11/15/2022 19:29:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.5365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:36 - INFO - train.train_snli_ve - loss is tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5147/16548 [2:23:17<5:22:43,  1.70s/it]11/15/2022 19:29:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.9810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:37 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5148/16548 [2:23:18<5:19:03,  1.68s/it]11/15/2022 19:29:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.5757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:39 - INFO - train.train_snli_ve - loss is tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5149/16548 [2:23:20<5:19:27,  1.68s/it]11/15/2022 19:29:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:41 - INFO - train.train_snli_ve - loss is tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5150/16548 [2:23:22<5:21:18,  1.69s/it]11/15/2022 19:29:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:43 - INFO - train.train_snli_ve - loss is tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5151/16548 [2:23:23<5:22:34,  1.70s/it]11/15/2022 19:29:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.9034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:44 - INFO - train.train_snli_ve - loss is tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5152/16548 [2:23:25<5:21:13,  1.69s/it]11/15/2022 19:29:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.5753e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:46 - INFO - train.train_snli_ve - loss is tensor(0.8551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5153/16548 [2:23:27<5:19:16,  1.68s/it]11/15/2022 19:29:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.6686e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:48 - INFO - train.train_snli_ve - loss is tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5154/16548 [2:23:28<5:17:30,  1.67s/it]11/15/2022 19:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(9.3926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:49 - INFO - train.train_snli_ve - loss is tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5155/16548 [2:23:30<5:17:41,  1.67s/it]11/15/2022 19:29:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:51 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5156/16548 [2:23:32<5:15:53,  1.66s/it]11/15/2022 19:29:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1750e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:53 - INFO - train.train_snli_ve - loss is tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5157/16548 [2:23:33<5:15:05,  1.66s/it]11/15/2022 19:29:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.1589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:54 - INFO - train.train_snli_ve - loss is tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5158/16548 [2:23:35<5:17:55,  1.67s/it]11/15/2022 19:29:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.2058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:56 - INFO - train.train_snli_ve - loss is tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5159/16548 [2:23:37<5:16:49,  1.67s/it]11/15/2022 19:29:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2523e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:58 - INFO - train.train_snli_ve - loss is tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5160/16548 [2:23:38<5:19:56,  1.69s/it]11/15/2022 19:29:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:29:59 - INFO - train.train_snli_ve - loss is tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5161/16548 [2:23:40<5:18:50,  1.68s/it]11/15/2022 19:30:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:01 - INFO - train.train_snli_ve - loss is tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5162/16548 [2:23:42<5:17:48,  1.67s/it]11/15/2022 19:30:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.2003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:03 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5163/16548 [2:23:43<5:19:52,  1.69s/it]11/15/2022 19:30:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.8146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:04 - INFO - train.train_snli_ve - loss is tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5164/16548 [2:23:45<5:21:06,  1.69s/it]11/15/2022 19:30:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1928e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:06 - INFO - train.train_snli_ve - loss is tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5165/16548 [2:23:47<5:18:21,  1.68s/it]11/15/2022 19:30:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3201e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:08 - INFO - train.train_snli_ve - loss is tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5166/16548 [2:23:48<5:16:25,  1.67s/it]11/15/2022 19:30:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.6149e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:09 - INFO - train.train_snli_ve - loss is tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5167/16548 [2:23:50<5:17:01,  1.67s/it]11/15/2022 19:30:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.8434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:11 - INFO - train.train_snli_ve - loss is tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5168/16548 [2:23:52<5:16:18,  1.67s/it]11/15/2022 19:30:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:13 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5169/16548 [2:23:53<5:15:01,  1.66s/it]11/15/2022 19:30:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.0315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:14 - INFO - train.train_snli_ve - loss is tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5170/16548 [2:23:55<5:16:50,  1.67s/it]11/15/2022 19:30:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.0544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:16 - INFO - train.train_snli_ve - loss is tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5171/16548 [2:23:57<5:16:25,  1.67s/it]11/15/2022 19:30:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.6374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:18 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5172/16548 [2:23:59<5:20:07,  1.69s/it]11/15/2022 19:30:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2636e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:19 - INFO - train.train_snli_ve - loss is tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5173/16548 [2:24:00<5:21:59,  1.70s/it]11/15/2022 19:30:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:21 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5174/16548 [2:24:02<5:19:48,  1.69s/it]11/15/2022 19:30:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.8114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:23 - INFO - train.train_snli_ve - loss is tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5175/16548 [2:24:04<5:17:16,  1.67s/it]11/15/2022 19:30:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.7117e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:25 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5176/16548 [2:24:05<5:19:42,  1.69s/it]11/15/2022 19:30:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:26 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5177/16548 [2:24:07<5:20:34,  1.69s/it]11/15/2022 19:30:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.9314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:28 - INFO - train.train_snli_ve - loss is tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5178/16548 [2:24:09<5:18:57,  1.68s/it]11/15/2022 19:30:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.0160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:30 - INFO - train.train_snli_ve - loss is tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5179/16548 [2:24:10<5:18:21,  1.68s/it]11/15/2022 19:30:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.8172e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:31 - INFO - train.train_snli_ve - loss is tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5180/16548 [2:24:12<5:18:27,  1.68s/it]11/15/2022 19:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:33 - INFO - train.train_snli_ve - loss is tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5181/16548 [2:24:14<5:16:14,  1.67s/it]11/15/2022 19:30:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.3997e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:35 - INFO - train.train_snli_ve - loss is tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5182/16548 [2:24:15<5:17:04,  1.67s/it]11/15/2022 19:30:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:36 - INFO - train.train_snli_ve - loss is tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5183/16548 [2:24:17<5:19:04,  1.68s/it]11/15/2022 19:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:38 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5184/16548 [2:24:19<5:21:06,  1.70s/it]11/15/2022 19:30:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.4578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:40 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5185/16548 [2:24:20<5:20:00,  1.69s/it]11/15/2022 19:30:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:41 - INFO - train.train_snli_ve - loss is tensor(0.8787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5186/16548 [2:24:22<5:18:18,  1.68s/it]11/15/2022 19:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.5208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:43 - INFO - train.train_snli_ve - loss is tensor(0.8912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5187/16548 [2:24:24<5:21:35,  1.70s/it]11/15/2022 19:30:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.2129e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:45 - INFO - train.train_snli_ve - loss is tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5188/16548 [2:24:25<5:21:28,  1.70s/it]11/15/2022 19:30:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.6789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:46 - INFO - train.train_snli_ve - loss is tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5189/16548 [2:24:27<5:19:28,  1.69s/it]11/15/2022 19:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.4055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:48 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5190/16548 [2:24:29<5:21:43,  1.70s/it]11/15/2022 19:30:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:50 - INFO - train.train_snli_ve - loss is tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5191/16548 [2:24:31<5:22:03,  1.70s/it]11/15/2022 19:30:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.6368e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:52 - INFO - train.train_snli_ve - loss is tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5192/16548 [2:24:32<5:22:13,  1.70s/it]11/15/2022 19:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2913e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:53 - INFO - train.train_snli_ve - loss is tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5193/16548 [2:24:34<5:18:43,  1.68s/it]11/15/2022 19:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.5531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:55 - INFO - train.train_snli_ve - loss is tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5194/16548 [2:24:36<5:18:10,  1.68s/it]11/15/2022 19:30:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.4826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:57 - INFO - train.train_snli_ve - loss is tensor(0.8527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5195/16548 [2:24:37<5:16:52,  1.67s/it]11/15/2022 19:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.9473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:30:58 - INFO - train.train_snli_ve - loss is tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5196/16548 [2:24:39<5:16:37,  1.67s/it]11/15/2022 19:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.7019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:00 - INFO - train.train_snli_ve - loss is tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5197/16548 [2:24:41<5:19:20,  1.69s/it]11/15/2022 19:31:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.9722e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:02 - INFO - train.train_snli_ve - loss is tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5198/16548 [2:24:42<5:16:50,  1.67s/it]11/15/2022 19:31:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.1609e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:03 - INFO - train.train_snli_ve - loss is tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5199/16548 [2:24:44<5:17:57,  1.68s/it]11/15/2022 19:31:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.1393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:05 - INFO - train.train_snli_ve - loss is tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5200/16548 [2:24:46<5:23:39,  1.71s/it]11/15/2022 19:31:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.2175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:07 - INFO - train.train_snli_ve - loss is tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5201/16548 [2:24:47<5:20:34,  1.70s/it]11/15/2022 19:31:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.8808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:08 - INFO - train.train_snli_ve - loss is tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5202/16548 [2:24:49<5:17:56,  1.68s/it]11/15/2022 19:31:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.8646e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:10 - INFO - train.train_snli_ve - loss is tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5203/16548 [2:24:51<5:17:20,  1.68s/it]11/15/2022 19:31:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0052e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:12 - INFO - train.train_snli_ve - loss is tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5204/16548 [2:24:52<5:20:07,  1.69s/it]11/15/2022 19:31:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.9003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:13 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5205/16548 [2:24:54<5:20:42,  1.70s/it]11/15/2022 19:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.2914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:15 - INFO - train.train_snli_ve - loss is tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5206/16548 [2:24:56<5:17:48,  1.68s/it]11/15/2022 19:31:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.5267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:17 - INFO - train.train_snli_ve - loss is tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5207/16548 [2:24:58<5:18:16,  1.68s/it]11/15/2022 19:31:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.3054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:18 - INFO - train.train_snli_ve - loss is tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5208/16548 [2:24:59<5:16:54,  1.68s/it]11/15/2022 19:31:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.0007e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:20 - INFO - train.train_snli_ve - loss is tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5209/16548 [2:25:01<5:19:13,  1.69s/it]11/15/2022 19:31:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.5261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:22 - INFO - train.train_snli_ve - loss is tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5210/16548 [2:25:03<5:17:32,  1.68s/it]11/15/2022 19:31:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.2061e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:24 - INFO - train.train_snli_ve - loss is tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5211/16548 [2:25:04<5:18:27,  1.69s/it]11/15/2022 19:31:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:25 - INFO - train.train_snli_ve - loss is tensor(1.1221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31% 5212/16548 [2:25:06<5:16:14,  1.67s/it]11/15/2022 19:31:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.1747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:27 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5213/16548 [2:25:08<5:15:18,  1.67s/it]11/15/2022 19:31:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.3218e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:29 - INFO - train.train_snli_ve - loss is tensor(0.9073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5214/16548 [2:25:09<5:15:39,  1.67s/it]11/15/2022 19:31:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.7805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:30 - INFO - train.train_snli_ve - loss is tensor(0.8211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5215/16548 [2:25:11<5:16:10,  1.67s/it]11/15/2022 19:31:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:32 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5216/16548 [2:25:13<5:12:26,  1.65s/it]11/15/2022 19:31:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.3776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:33 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5217/16548 [2:25:14<5:12:23,  1.65s/it]11/15/2022 19:31:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:35 - INFO - train.train_snli_ve - loss is tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5218/16548 [2:25:16<5:13:55,  1.66s/it]11/15/2022 19:31:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2428e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:37 - INFO - train.train_snli_ve - loss is tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5219/16548 [2:25:18<5:14:36,  1.67s/it]11/15/2022 19:31:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0580e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:39 - INFO - train.train_snli_ve - loss is tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5220/16548 [2:25:19<5:16:29,  1.68s/it]11/15/2022 19:31:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.1867e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:40 - INFO - train.train_snli_ve - loss is tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5221/16548 [2:25:21<5:16:17,  1.68s/it]11/15/2022 19:31:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.7183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:42 - INFO - train.train_snli_ve - loss is tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5222/16548 [2:25:23<5:15:26,  1.67s/it]11/15/2022 19:31:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:43 - INFO - train.train_snli_ve - loss is tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5223/16548 [2:25:24<5:13:50,  1.66s/it]11/15/2022 19:31:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.0838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:45 - INFO - train.train_snli_ve - loss is tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5224/16548 [2:25:26<5:14:25,  1.67s/it]11/15/2022 19:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.2308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:47 - INFO - train.train_snli_ve - loss is tensor(0.8218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5225/16548 [2:25:28<5:15:50,  1.67s/it]11/15/2022 19:31:49 - INFO - train.train_snli_ve - kd_loss is tensor(5.5583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:49 - INFO - train.train_snli_ve - loss is tensor(0.8265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5226/16548 [2:25:29<5:15:52,  1.67s/it]11/15/2022 19:31:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.9651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:50 - INFO - train.train_snli_ve - loss is tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5227/16548 [2:25:31<5:16:03,  1.68s/it]11/15/2022 19:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.9804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:52 - INFO - train.train_snli_ve - loss is tensor(0.7086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5228/16548 [2:25:33<5:15:16,  1.67s/it]11/15/2022 19:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.3703e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:54 - INFO - train.train_snli_ve - loss is tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5229/16548 [2:25:34<5:15:09,  1.67s/it]11/15/2022 19:31:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.0059e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:55 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5230/16548 [2:25:36<5:15:30,  1.67s/it]11/15/2022 19:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.2116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:57 - INFO - train.train_snli_ve - loss is tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5231/16548 [2:25:38<5:16:28,  1.68s/it]11/15/2022 19:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.4183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:31:59 - INFO - train.train_snli_ve - loss is tensor(0.8223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5232/16548 [2:25:39<5:16:47,  1.68s/it]11/15/2022 19:32:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.5244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:00 - INFO - train.train_snli_ve - loss is tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5233/16548 [2:25:41<5:16:37,  1.68s/it]11/15/2022 19:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.0377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:02 - INFO - train.train_snli_ve - loss is tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5234/16548 [2:25:43<5:15:45,  1.67s/it]11/15/2022 19:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.2050e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:04 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5235/16548 [2:25:44<5:19:16,  1.69s/it]11/15/2022 19:32:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.4374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:05 - INFO - train.train_snli_ve - loss is tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5236/16548 [2:25:46<5:19:56,  1.70s/it]11/15/2022 19:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.4734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:07 - INFO - train.train_snli_ve - loss is tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5237/16548 [2:25:48<5:20:32,  1.70s/it]11/15/2022 19:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.8389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:09 - INFO - train.train_snli_ve - loss is tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5238/16548 [2:25:49<5:18:02,  1.69s/it]11/15/2022 19:32:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.8632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:10 - INFO - train.train_snli_ve - loss is tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5239/16548 [2:25:51<5:18:11,  1.69s/it]11/15/2022 19:32:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.3546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:12 - INFO - train.train_snli_ve - loss is tensor(0.8179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5240/16548 [2:25:53<5:15:53,  1.68s/it]11/15/2022 19:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:14 - INFO - train.train_snli_ve - loss is tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5241/16548 [2:25:54<5:15:46,  1.68s/it]11/15/2022 19:32:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.1295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:15 - INFO - train.train_snli_ve - loss is tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5242/16548 [2:25:56<5:14:49,  1.67s/it]11/15/2022 19:32:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:17 - INFO - train.train_snli_ve - loss is tensor(0.4718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5243/16548 [2:25:58<5:13:27,  1.66s/it]11/15/2022 19:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.7830e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:19 - INFO - train.train_snli_ve - loss is tensor(0.8367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5244/16548 [2:25:59<5:14:32,  1.67s/it]11/15/2022 19:32:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:20 - INFO - train.train_snli_ve - loss is tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5245/16548 [2:26:01<5:15:09,  1.67s/it]11/15/2022 19:32:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.1900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:22 - INFO - train.train_snli_ve - loss is tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5246/16548 [2:26:03<5:16:10,  1.68s/it]11/15/2022 19:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.5505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:24 - INFO - train.train_snli_ve - loss is tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5247/16548 [2:26:05<5:15:30,  1.68s/it]11/15/2022 19:32:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.8604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:25 - INFO - train.train_snli_ve - loss is tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5248/16548 [2:26:06<5:14:18,  1.67s/it]11/15/2022 19:32:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.1949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:27 - INFO - train.train_snli_ve - loss is tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5249/16548 [2:26:08<5:13:05,  1.66s/it]11/15/2022 19:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.9680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:29 - INFO - train.train_snli_ve - loss is tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5250/16548 [2:26:10<5:15:17,  1.67s/it]11/15/2022 19:32:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.9269e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:30 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5251/16548 [2:26:11<5:16:03,  1.68s/it]11/15/2022 19:32:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.1164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:32 - INFO - train.train_snli_ve - loss is tensor(0.7798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5252/16548 [2:26:13<5:16:27,  1.68s/it]11/15/2022 19:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.1883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:34 - INFO - train.train_snli_ve - loss is tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5253/16548 [2:26:15<5:15:39,  1.68s/it]11/15/2022 19:32:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.8377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:36 - INFO - train.train_snli_ve - loss is tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5254/16548 [2:26:16<5:15:38,  1.68s/it]11/15/2022 19:32:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.5681e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:37 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5255/16548 [2:26:18<5:15:14,  1.67s/it]11/15/2022 19:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.1003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:39 - INFO - train.train_snli_ve - loss is tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5256/16548 [2:26:20<5:15:51,  1.68s/it]11/15/2022 19:32:41 - INFO - train.train_snli_ve - kd_loss is tensor(5.3512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:41 - INFO - train.train_snli_ve - loss is tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5257/16548 [2:26:21<5:13:56,  1.67s/it]11/15/2022 19:32:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.4310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:42 - INFO - train.train_snli_ve - loss is tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5258/16548 [2:26:23<5:13:41,  1.67s/it]11/15/2022 19:32:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.5947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:44 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5259/16548 [2:26:25<5:14:34,  1.67s/it]11/15/2022 19:32:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.9337e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:46 - INFO - train.train_snli_ve - loss is tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5260/16548 [2:26:26<5:18:25,  1.69s/it]11/15/2022 19:32:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.8116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:47 - INFO - train.train_snli_ve - loss is tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5261/16548 [2:26:28<5:19:26,  1.70s/it]11/15/2022 19:32:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:49 - INFO - train.train_snli_ve - loss is tensor(0.9539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5262/16548 [2:26:30<5:17:41,  1.69s/it]11/15/2022 19:32:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.7767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:51 - INFO - train.train_snli_ve - loss is tensor(0.9693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5263/16548 [2:26:31<5:19:56,  1.70s/it]11/15/2022 19:32:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.3804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:52 - INFO - train.train_snli_ve - loss is tensor(0.8384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5264/16548 [2:26:33<5:19:52,  1.70s/it]11/15/2022 19:32:54 - INFO - train.train_snli_ve - kd_loss is tensor(5.6436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:54 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5265/16548 [2:26:35<5:18:57,  1.70s/it]11/15/2022 19:32:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.2257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:56 - INFO - train.train_snli_ve - loss is tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5266/16548 [2:26:36<5:16:47,  1.68s/it]11/15/2022 19:32:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.8343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:57 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5267/16548 [2:26:38<5:15:18,  1.68s/it]11/15/2022 19:32:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:32:59 - INFO - train.train_snli_ve - loss is tensor(0.8533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5268/16548 [2:26:40<5:18:54,  1.70s/it]11/15/2022 19:33:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.1235e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:01 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5269/16548 [2:26:42<5:19:20,  1.70s/it]11/15/2022 19:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:03 - INFO - train.train_snli_ve - loss is tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5270/16548 [2:26:43<5:18:45,  1.70s/it]11/15/2022 19:33:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.9211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:04 - INFO - train.train_snli_ve - loss is tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5271/16548 [2:26:45<5:18:17,  1.69s/it]11/15/2022 19:33:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.6459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:06 - INFO - train.train_snli_ve - loss is tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5272/16548 [2:26:47<5:18:23,  1.69s/it]11/15/2022 19:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.3143e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:08 - INFO - train.train_snli_ve - loss is tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5273/16548 [2:26:48<5:16:54,  1.69s/it]11/15/2022 19:33:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:09 - INFO - train.train_snli_ve - loss is tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5274/16548 [2:26:50<5:14:25,  1.67s/it]11/15/2022 19:33:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.8787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:11 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5275/16548 [2:26:52<5:15:43,  1.68s/it]11/15/2022 19:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.0239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:13 - INFO - train.train_snli_ve - loss is tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5276/16548 [2:26:53<5:15:00,  1.68s/it]11/15/2022 19:33:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.4259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:14 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5277/16548 [2:26:55<5:17:52,  1.69s/it]11/15/2022 19:33:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.9357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:16 - INFO - train.train_snli_ve - loss is tensor(0.8176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5278/16548 [2:26:57<5:14:59,  1.68s/it]11/15/2022 19:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.2864e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:18 - INFO - train.train_snli_ve - loss is tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5279/16548 [2:26:58<5:14:35,  1.67s/it]11/15/2022 19:33:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.4822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:19 - INFO - train.train_snli_ve - loss is tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5280/16548 [2:27:00<5:14:44,  1.68s/it]11/15/2022 19:33:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.6228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:21 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5281/16548 [2:27:02<5:14:10,  1.67s/it]11/15/2022 19:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.5730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:23 - INFO - train.train_snli_ve - loss is tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5282/16548 [2:27:03<5:15:22,  1.68s/it]11/15/2022 19:33:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.4108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:24 - INFO - train.train_snli_ve - loss is tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5283/16548 [2:27:05<5:16:26,  1.69s/it]11/15/2022 19:33:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:26 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5284/16548 [2:27:07<5:16:47,  1.69s/it]11/15/2022 19:33:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.5658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:28 - INFO - train.train_snli_ve - loss is tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5285/16548 [2:27:08<5:13:40,  1.67s/it]11/15/2022 19:33:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.1062e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:29 - INFO - train.train_snli_ve - loss is tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5286/16548 [2:27:10<5:14:11,  1.67s/it]11/15/2022 19:33:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.0150e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:31 - INFO - train.train_snli_ve - loss is tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5287/16548 [2:27:12<5:16:03,  1.68s/it]11/15/2022 19:33:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.0976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:33 - INFO - train.train_snli_ve - loss is tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5288/16548 [2:27:13<5:14:09,  1.67s/it]11/15/2022 19:33:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:34 - INFO - train.train_snli_ve - loss is tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5289/16548 [2:27:15<5:14:17,  1.67s/it]11/15/2022 19:33:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:36 - INFO - train.train_snli_ve - loss is tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5290/16548 [2:27:17<5:13:36,  1.67s/it]11/15/2022 19:33:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.6938e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:38 - INFO - train.train_snli_ve - loss is tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5291/16548 [2:27:19<5:14:57,  1.68s/it]11/15/2022 19:33:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.2505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:40 - INFO - train.train_snli_ve - loss is tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5292/16548 [2:27:20<5:18:43,  1.70s/it]11/15/2022 19:33:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.8940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:41 - INFO - train.train_snli_ve - loss is tensor(0.5400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5293/16548 [2:27:22<5:15:15,  1.68s/it]11/15/2022 19:33:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.8493e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:43 - INFO - train.train_snli_ve - loss is tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5294/16548 [2:27:24<5:17:24,  1.69s/it]11/15/2022 19:33:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.7372e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:45 - INFO - train.train_snli_ve - loss is tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5295/16548 [2:27:25<5:15:28,  1.68s/it]11/15/2022 19:33:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:46 - INFO - train.train_snli_ve - loss is tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5296/16548 [2:27:27<5:15:37,  1.68s/it]11/15/2022 19:33:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.6751e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:48 - INFO - train.train_snli_ve - loss is tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5297/16548 [2:27:29<5:17:49,  1.69s/it]11/15/2022 19:33:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.2392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:50 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5298/16548 [2:27:30<5:16:29,  1.69s/it]11/15/2022 19:33:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4473e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:51 - INFO - train.train_snli_ve - loss is tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5299/16548 [2:27:32<5:14:49,  1.68s/it]11/15/2022 19:33:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.9006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:53 - INFO - train.train_snli_ve - loss is tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5300/16548 [2:27:34<5:19:48,  1.71s/it]11/15/2022 19:33:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:55 - INFO - train.train_snli_ve - loss is tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5301/16548 [2:27:35<5:19:06,  1.70s/it]11/15/2022 19:33:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.6196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:56 - INFO - train.train_snli_ve - loss is tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5302/16548 [2:27:37<5:15:57,  1.69s/it]11/15/2022 19:33:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:33:58 - INFO - train.train_snli_ve - loss is tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5303/16548 [2:27:39<5:14:55,  1.68s/it]11/15/2022 19:34:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:00 - INFO - train.train_snli_ve - loss is tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5304/16548 [2:27:40<5:14:55,  1.68s/it]11/15/2022 19:34:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5479e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:01 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5305/16548 [2:27:42<5:14:47,  1.68s/it]11/15/2022 19:34:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:03 - INFO - train.train_snli_ve - loss is tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5306/16548 [2:27:44<5:12:53,  1.67s/it]11/15/2022 19:34:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:05 - INFO - train.train_snli_ve - loss is tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5307/16548 [2:27:45<5:14:19,  1.68s/it]11/15/2022 19:34:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:06 - INFO - train.train_snli_ve - loss is tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5308/16548 [2:27:47<5:11:53,  1.66s/it]11/15/2022 19:34:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:08 - INFO - train.train_snli_ve - loss is tensor(0.8835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5309/16548 [2:27:49<5:12:08,  1.67s/it]11/15/2022 19:34:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:10 - INFO - train.train_snli_ve - loss is tensor(0.5042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5310/16548 [2:27:50<5:10:53,  1.66s/it]11/15/2022 19:34:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0036e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:11 - INFO - train.train_snli_ve - loss is tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5311/16548 [2:27:52<5:12:35,  1.67s/it]11/15/2022 19:34:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.0765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:13 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5312/16548 [2:27:54<5:13:02,  1.67s/it]11/15/2022 19:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.2116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:15 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5313/16548 [2:27:55<5:13:00,  1.67s/it]11/15/2022 19:34:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:16 - INFO - train.train_snli_ve - loss is tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5314/16548 [2:27:57<5:11:32,  1.66s/it]11/15/2022 19:34:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.6865e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:18 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5315/16548 [2:27:59<5:11:12,  1.66s/it]11/15/2022 19:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.9222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:20 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5316/16548 [2:28:00<5:10:54,  1.66s/it]11/15/2022 19:34:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.4036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:21 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5317/16548 [2:28:02<5:13:45,  1.68s/it]11/15/2022 19:34:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:23 - INFO - train.train_snli_ve - loss is tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5318/16548 [2:28:04<5:16:02,  1.69s/it]11/15/2022 19:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.3208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:25 - INFO - train.train_snli_ve - loss is tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5319/16548 [2:28:06<5:14:15,  1.68s/it]11/15/2022 19:34:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.9042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:26 - INFO - train.train_snli_ve - loss is tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5320/16548 [2:28:07<5:13:57,  1.68s/it]11/15/2022 19:34:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4071e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:28 - INFO - train.train_snli_ve - loss is tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5321/16548 [2:28:09<5:12:50,  1.67s/it]11/15/2022 19:34:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.2014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:30 - INFO - train.train_snli_ve - loss is tensor(0.8897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5322/16548 [2:28:11<5:14:28,  1.68s/it]11/15/2022 19:34:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8818e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:32 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5323/16548 [2:28:12<5:14:19,  1.68s/it]11/15/2022 19:34:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:33 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5324/16548 [2:28:14<5:15:26,  1.69s/it]11/15/2022 19:34:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:35 - INFO - train.train_snli_ve - loss is tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5325/16548 [2:28:16<5:14:29,  1.68s/it]11/15/2022 19:34:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.0796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:37 - INFO - train.train_snli_ve - loss is tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5326/16548 [2:28:17<5:14:08,  1.68s/it]11/15/2022 19:34:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0615e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:38 - INFO - train.train_snli_ve - loss is tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5327/16548 [2:28:19<5:15:48,  1.69s/it]11/15/2022 19:34:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:40 - INFO - train.train_snli_ve - loss is tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5328/16548 [2:28:21<5:14:12,  1.68s/it]11/15/2022 19:34:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.1250e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:42 - INFO - train.train_snli_ve - loss is tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5329/16548 [2:28:22<5:12:05,  1.67s/it]11/15/2022 19:34:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:43 - INFO - train.train_snli_ve - loss is tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5330/16548 [2:28:24<5:10:54,  1.66s/it]11/15/2022 19:34:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:45 - INFO - train.train_snli_ve - loss is tensor(0.8512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5331/16548 [2:28:26<5:14:18,  1.68s/it]11/15/2022 19:34:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:47 - INFO - train.train_snli_ve - loss is tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5332/16548 [2:28:27<5:12:38,  1.67s/it]11/15/2022 19:34:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:48 - INFO - train.train_snli_ve - loss is tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5333/16548 [2:28:29<5:11:15,  1.67s/it]11/15/2022 19:34:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:50 - INFO - train.train_snli_ve - loss is tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5334/16548 [2:28:31<5:12:31,  1.67s/it]11/15/2022 19:34:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.2813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:52 - INFO - train.train_snli_ve - loss is tensor(0.5509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5335/16548 [2:28:32<5:11:03,  1.66s/it]11/15/2022 19:34:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.2517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:53 - INFO - train.train_snli_ve - loss is tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5336/16548 [2:28:34<5:12:46,  1.67s/it]11/15/2022 19:34:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:55 - INFO - train.train_snli_ve - loss is tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5337/16548 [2:28:36<5:10:40,  1.66s/it]11/15/2022 19:34:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:57 - INFO - train.train_snli_ve - loss is tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5338/16548 [2:28:37<5:10:50,  1.66s/it]11/15/2022 19:34:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:34:58 - INFO - train.train_snli_ve - loss is tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5339/16548 [2:28:39<5:11:48,  1.67s/it]11/15/2022 19:35:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:00 - INFO - train.train_snli_ve - loss is tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5340/16548 [2:28:41<5:14:56,  1.69s/it]11/15/2022 19:35:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.3913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:02 - INFO - train.train_snli_ve - loss is tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5341/16548 [2:28:42<5:16:22,  1.69s/it]11/15/2022 19:35:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:03 - INFO - train.train_snli_ve - loss is tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5342/16548 [2:28:44<5:13:28,  1.68s/it]11/15/2022 19:35:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:05 - INFO - train.train_snli_ve - loss is tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5343/16548 [2:28:46<5:14:12,  1.68s/it]11/15/2022 19:35:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:07 - INFO - train.train_snli_ve - loss is tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5344/16548 [2:28:47<5:14:26,  1.68s/it]11/15/2022 19:35:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.9871e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:08 - INFO - train.train_snli_ve - loss is tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5345/16548 [2:28:49<5:14:37,  1.69s/it]11/15/2022 19:35:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.5332e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:10 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5346/16548 [2:28:51<5:14:27,  1.68s/it]11/15/2022 19:35:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:12 - INFO - train.train_snli_ve - loss is tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5347/16548 [2:28:52<5:12:43,  1.68s/it]11/15/2022 19:35:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:13 - INFO - train.train_snli_ve - loss is tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5348/16548 [2:28:54<5:10:05,  1.66s/it]11/15/2022 19:35:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:15 - INFO - train.train_snli_ve - loss is tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5349/16548 [2:28:56<5:12:43,  1.68s/it]11/15/2022 19:35:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:17 - INFO - train.train_snli_ve - loss is tensor(0.9226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5350/16548 [2:28:57<5:11:29,  1.67s/it]11/15/2022 19:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.4330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:18 - INFO - train.train_snli_ve - loss is tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5351/16548 [2:28:59<5:13:52,  1.68s/it]11/15/2022 19:35:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:20 - INFO - train.train_snli_ve - loss is tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5352/16548 [2:29:01<5:12:51,  1.68s/it]11/15/2022 19:35:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.7413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:22 - INFO - train.train_snli_ve - loss is tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5353/16548 [2:29:02<5:10:34,  1.66s/it]11/15/2022 19:35:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.6227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:23 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5354/16548 [2:29:04<5:12:10,  1.67s/it]11/15/2022 19:35:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.5225e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:25 - INFO - train.train_snli_ve - loss is tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5355/16548 [2:29:06<5:12:14,  1.67s/it]11/15/2022 19:35:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.7593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:27 - INFO - train.train_snli_ve - loss is tensor(0.7800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5356/16548 [2:29:07<5:09:36,  1.66s/it]11/15/2022 19:35:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.9216e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:28 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5357/16548 [2:29:09<5:08:40,  1.65s/it]11/15/2022 19:35:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.4670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:30 - INFO - train.train_snli_ve - loss is tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5358/16548 [2:29:11<5:10:41,  1.67s/it]11/15/2022 19:35:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.9079e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:32 - INFO - train.train_snli_ve - loss is tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5359/16548 [2:29:13<5:12:12,  1.67s/it]11/15/2022 19:35:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.7745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:33 - INFO - train.train_snli_ve - loss is tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5360/16548 [2:29:14<5:12:12,  1.67s/it]11/15/2022 19:35:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:35 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5361/16548 [2:29:16<5:10:13,  1.66s/it]11/15/2022 19:35:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.1071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:37 - INFO - train.train_snli_ve - loss is tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5362/16548 [2:29:17<5:09:05,  1.66s/it]11/15/2022 19:35:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.6318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:38 - INFO - train.train_snli_ve - loss is tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5363/16548 [2:29:19<5:07:58,  1.65s/it]11/15/2022 19:35:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.8066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:40 - INFO - train.train_snli_ve - loss is tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5364/16548 [2:29:21<5:11:08,  1.67s/it]11/15/2022 19:35:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.0565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:42 - INFO - train.train_snli_ve - loss is tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5365/16548 [2:29:22<5:10:22,  1.67s/it]11/15/2022 19:35:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.7622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:43 - INFO - train.train_snli_ve - loss is tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5366/16548 [2:29:24<5:10:27,  1.67s/it]11/15/2022 19:35:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4149e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:45 - INFO - train.train_snli_ve - loss is tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5367/16548 [2:29:26<5:11:13,  1.67s/it]11/15/2022 19:35:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.8446e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:47 - INFO - train.train_snli_ve - loss is tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5368/16548 [2:29:27<5:10:08,  1.66s/it]11/15/2022 19:35:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2719e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:48 - INFO - train.train_snli_ve - loss is tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5369/16548 [2:29:29<5:12:52,  1.68s/it]11/15/2022 19:35:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.2373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:50 - INFO - train.train_snli_ve - loss is tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5370/16548 [2:29:31<5:14:41,  1.69s/it]11/15/2022 19:35:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.0970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:52 - INFO - train.train_snli_ve - loss is tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5371/16548 [2:29:33<5:13:07,  1.68s/it]11/15/2022 19:35:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.3298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:53 - INFO - train.train_snli_ve - loss is tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5372/16548 [2:29:34<5:12:16,  1.68s/it]11/15/2022 19:35:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:55 - INFO - train.train_snli_ve - loss is tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5373/16548 [2:29:36<5:12:52,  1.68s/it]11/15/2022 19:35:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:57 - INFO - train.train_snli_ve - loss is tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5374/16548 [2:29:38<5:11:49,  1.67s/it]11/15/2022 19:35:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0119e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:35:59 - INFO - train.train_snli_ve - loss is tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5375/16548 [2:29:39<5:12:19,  1.68s/it]11/15/2022 19:36:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.5551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:00 - INFO - train.train_snli_ve - loss is tensor(0.9181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5376/16548 [2:29:41<5:11:45,  1.67s/it]11/15/2022 19:36:02 - INFO - train.train_snli_ve - kd_loss is tensor(7.4518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:02 - INFO - train.train_snli_ve - loss is tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5377/16548 [2:29:43<5:09:45,  1.66s/it]11/15/2022 19:36:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:04 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32% 5378/16548 [2:29:44<5:12:34,  1.68s/it]11/15/2022 19:36:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.2192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:05 - INFO - train.train_snli_ve - loss is tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5379/16548 [2:29:46<5:13:14,  1.68s/it]11/15/2022 19:36:07 - INFO - train.train_snli_ve - kd_loss is tensor(8.7568e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:07 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5380/16548 [2:29:48<5:13:10,  1.68s/it]11/15/2022 19:36:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:09 - INFO - train.train_snli_ve - loss is tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5381/16548 [2:29:49<5:15:34,  1.70s/it]11/15/2022 19:36:10 - INFO - train.train_snli_ve - kd_loss is tensor(6.9103e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:10 - INFO - train.train_snli_ve - loss is tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5382/16548 [2:29:51<5:12:59,  1.68s/it]11/15/2022 19:36:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.9133e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:12 - INFO - train.train_snli_ve - loss is tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5383/16548 [2:29:53<5:12:26,  1.68s/it]11/15/2022 19:36:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.1789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:14 - INFO - train.train_snli_ve - loss is tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5384/16548 [2:29:54<5:11:06,  1.67s/it]11/15/2022 19:36:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.2219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:15 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5385/16548 [2:29:56<5:09:26,  1.66s/it]11/15/2022 19:36:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.7712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:17 - INFO - train.train_snli_ve - loss is tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5386/16548 [2:29:58<5:08:42,  1.66s/it]11/15/2022 19:36:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2551e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:19 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5387/16548 [2:29:59<5:13:24,  1.68s/it]11/15/2022 19:36:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.1955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:20 - INFO - train.train_snli_ve - loss is tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5388/16548 [2:30:01<5:11:32,  1.67s/it]11/15/2022 19:36:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:22 - INFO - train.train_snli_ve - loss is tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5389/16548 [2:30:03<5:08:29,  1.66s/it]11/15/2022 19:36:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.1817e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:24 - INFO - train.train_snli_ve - loss is tensor(0.7716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5390/16548 [2:30:04<5:08:54,  1.66s/it]11/15/2022 19:36:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.3285e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:25 - INFO - train.train_snli_ve - loss is tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5391/16548 [2:30:06<5:07:22,  1.65s/it]11/15/2022 19:36:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:27 - INFO - train.train_snli_ve - loss is tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5392/16548 [2:30:08<5:10:06,  1.67s/it]11/15/2022 19:36:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.7604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:29 - INFO - train.train_snli_ve - loss is tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5393/16548 [2:30:09<5:08:22,  1.66s/it]11/15/2022 19:36:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.2665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:30 - INFO - train.train_snli_ve - loss is tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5394/16548 [2:30:11<5:11:02,  1.67s/it]11/15/2022 19:36:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:32 - INFO - train.train_snli_ve - loss is tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5395/16548 [2:30:13<5:11:16,  1.67s/it]11/15/2022 19:36:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.3759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:34 - INFO - train.train_snli_ve - loss is tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5396/16548 [2:30:14<5:10:32,  1.67s/it]11/15/2022 19:36:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2216e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:35 - INFO - train.train_snli_ve - loss is tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5397/16548 [2:30:16<5:12:52,  1.68s/it]11/15/2022 19:36:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:37 - INFO - train.train_snli_ve - loss is tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5398/16548 [2:30:18<5:09:42,  1.67s/it]11/15/2022 19:36:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2366e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:39 - INFO - train.train_snli_ve - loss is tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5399/16548 [2:30:19<5:11:56,  1.68s/it]11/15/2022 19:36:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.8928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:40 - INFO - train.train_snli_ve - loss is tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5400/16548 [2:30:21<5:16:25,  1.70s/it]11/15/2022 19:36:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.1785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:42 - INFO - train.train_snli_ve - loss is tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5401/16548 [2:30:23<5:15:13,  1.70s/it]11/15/2022 19:36:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.3408e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:44 - INFO - train.train_snli_ve - loss is tensor(0.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5402/16548 [2:30:24<5:12:06,  1.68s/it]11/15/2022 19:36:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.7568e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:45 - INFO - train.train_snli_ve - loss is tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5403/16548 [2:30:26<5:11:06,  1.67s/it]11/15/2022 19:36:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.3079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:47 - INFO - train.train_snli_ve - loss is tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5404/16548 [2:30:28<5:10:17,  1.67s/it]11/15/2022 19:36:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:49 - INFO - train.train_snli_ve - loss is tensor(0.8311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5405/16548 [2:30:29<5:09:48,  1.67s/it]11/15/2022 19:36:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:50 - INFO - train.train_snli_ve - loss is tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5406/16548 [2:30:31<5:11:01,  1.67s/it]11/15/2022 19:36:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.2120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:52 - INFO - train.train_snli_ve - loss is tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5407/16548 [2:30:33<5:09:57,  1.67s/it]11/15/2022 19:36:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:54 - INFO - train.train_snli_ve - loss is tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5408/16548 [2:30:34<5:09:09,  1.67s/it]11/15/2022 19:36:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.0016e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:55 - INFO - train.train_snli_ve - loss is tensor(0.8613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5409/16548 [2:30:36<5:09:17,  1.67s/it]11/15/2022 19:36:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.6964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:57 - INFO - train.train_snli_ve - loss is tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5410/16548 [2:30:38<5:12:27,  1.68s/it]11/15/2022 19:36:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.3025e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:36:59 - INFO - train.train_snli_ve - loss is tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5411/16548 [2:30:39<5:09:51,  1.67s/it]11/15/2022 19:37:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:00 - INFO - train.train_snli_ve - loss is tensor(0.9581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5412/16548 [2:30:41<5:11:43,  1.68s/it]11/15/2022 19:37:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.5498e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:02 - INFO - train.train_snli_ve - loss is tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5413/16548 [2:30:43<5:08:14,  1.66s/it]11/15/2022 19:37:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.1986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:04 - INFO - train.train_snli_ve - loss is tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5414/16548 [2:30:44<5:07:41,  1.66s/it]11/15/2022 19:37:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.1236e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:05 - INFO - train.train_snli_ve - loss is tensor(0.9155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5415/16548 [2:30:46<5:07:46,  1.66s/it]11/15/2022 19:37:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:07 - INFO - train.train_snli_ve - loss is tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5416/16548 [2:30:48<5:08:51,  1.66s/it]11/15/2022 19:37:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:09 - INFO - train.train_snli_ve - loss is tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5417/16548 [2:30:49<5:08:49,  1.66s/it]11/15/2022 19:37:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:10 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5418/16548 [2:30:51<5:07:33,  1.66s/it]11/15/2022 19:37:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:12 - INFO - train.train_snli_ve - loss is tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5419/16548 [2:30:53<5:07:25,  1.66s/it]11/15/2022 19:37:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.8447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:14 - INFO - train.train_snli_ve - loss is tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5420/16548 [2:30:54<5:08:52,  1.67s/it]11/15/2022 19:37:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.2436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:15 - INFO - train.train_snli_ve - loss is tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5421/16548 [2:30:56<5:08:19,  1.66s/it]11/15/2022 19:37:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.2527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:17 - INFO - train.train_snli_ve - loss is tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5422/16548 [2:30:58<5:09:09,  1.67s/it]11/15/2022 19:37:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:19 - INFO - train.train_snli_ve - loss is tensor(0.9635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5423/16548 [2:30:59<5:08:09,  1.66s/it]11/15/2022 19:37:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.6850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:20 - INFO - train.train_snli_ve - loss is tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5424/16548 [2:31:01<5:13:40,  1.69s/it]11/15/2022 19:37:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1678e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:22 - INFO - train.train_snli_ve - loss is tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5425/16548 [2:31:03<5:09:32,  1.67s/it]11/15/2022 19:37:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3485e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:24 - INFO - train.train_snli_ve - loss is tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5426/16548 [2:31:04<5:10:07,  1.67s/it]11/15/2022 19:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:25 - INFO - train.train_snli_ve - loss is tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5427/16548 [2:31:06<5:10:55,  1.68s/it]11/15/2022 19:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6344e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:27 - INFO - train.train_snli_ve - loss is tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5428/16548 [2:31:08<5:10:05,  1.67s/it]11/15/2022 19:37:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4475e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:29 - INFO - train.train_snli_ve - loss is tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5429/16548 [2:31:10<5:10:18,  1.67s/it]11/15/2022 19:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:30 - INFO - train.train_snli_ve - loss is tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5430/16548 [2:31:11<5:09:53,  1.67s/it]11/15/2022 19:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.8075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:32 - INFO - train.train_snli_ve - loss is tensor(0.8482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5431/16548 [2:31:13<5:09:06,  1.67s/it]11/15/2022 19:37:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.6153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:34 - INFO - train.train_snli_ve - loss is tensor(0.4358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5432/16548 [2:31:15<5:08:43,  1.67s/it]11/15/2022 19:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:35 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5433/16548 [2:31:16<5:09:24,  1.67s/it]11/15/2022 19:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:37 - INFO - train.train_snli_ve - loss is tensor(0.7800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5434/16548 [2:31:18<5:12:16,  1.69s/it]11/15/2022 19:37:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:39 - INFO - train.train_snli_ve - loss is tensor(0.8106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5435/16548 [2:31:20<5:11:34,  1.68s/it]11/15/2022 19:37:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.8358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:41 - INFO - train.train_snli_ve - loss is tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5436/16548 [2:31:21<5:11:09,  1.68s/it]11/15/2022 19:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:42 - INFO - train.train_snli_ve - loss is tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5437/16548 [2:31:23<5:11:41,  1.68s/it]11/15/2022 19:37:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.7508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:44 - INFO - train.train_snli_ve - loss is tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5438/16548 [2:31:25<5:12:19,  1.69s/it]11/15/2022 19:37:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:46 - INFO - train.train_snli_ve - loss is tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5439/16548 [2:31:26<5:12:58,  1.69s/it]11/15/2022 19:37:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:47 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5440/16548 [2:31:28<5:11:21,  1.68s/it]11/15/2022 19:37:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.1980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:49 - INFO - train.train_snli_ve - loss is tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5441/16548 [2:31:30<5:13:21,  1.69s/it]11/15/2022 19:37:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:51 - INFO - train.train_snli_ve - loss is tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5442/16548 [2:31:31<5:14:13,  1.70s/it]11/15/2022 19:37:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:52 - INFO - train.train_snli_ve - loss is tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5443/16548 [2:31:33<5:12:15,  1.69s/it]11/15/2022 19:37:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:54 - INFO - train.train_snli_ve - loss is tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5444/16548 [2:31:35<5:11:04,  1.68s/it]11/15/2022 19:37:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2203e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:56 - INFO - train.train_snli_ve - loss is tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5445/16548 [2:31:36<5:09:44,  1.67s/it]11/15/2022 19:37:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:57 - INFO - train.train_snli_ve - loss is tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5446/16548 [2:31:38<5:08:44,  1.67s/it]11/15/2022 19:37:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:37:59 - INFO - train.train_snli_ve - loss is tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5447/16548 [2:31:40<5:09:52,  1.67s/it]11/15/2022 19:38:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.2195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:01 - INFO - train.train_snli_ve - loss is tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5448/16548 [2:31:41<5:11:00,  1.68s/it]11/15/2022 19:38:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:02 - INFO - train.train_snli_ve - loss is tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5449/16548 [2:31:43<5:10:52,  1.68s/it]11/15/2022 19:38:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:04 - INFO - train.train_snli_ve - loss is tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5450/16548 [2:31:45<5:09:54,  1.68s/it]11/15/2022 19:38:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.9342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:06 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5451/16548 [2:31:46<5:09:25,  1.67s/it]11/15/2022 19:38:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.9888e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:07 - INFO - train.train_snli_ve - loss is tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5452/16548 [2:31:48<5:11:32,  1.68s/it]11/15/2022 19:38:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.7527e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:09 - INFO - train.train_snli_ve - loss is tensor(0.4564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5453/16548 [2:31:50<5:13:20,  1.69s/it]11/15/2022 19:38:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:11 - INFO - train.train_snli_ve - loss is tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5454/16548 [2:31:52<5:15:46,  1.71s/it]11/15/2022 19:38:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:13 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5455/16548 [2:31:53<5:12:39,  1.69s/it]11/15/2022 19:38:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:14 - INFO - train.train_snli_ve - loss is tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5456/16548 [2:31:55<5:11:45,  1.69s/it]11/15/2022 19:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.6342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:16 - INFO - train.train_snli_ve - loss is tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5457/16548 [2:31:57<5:13:08,  1.69s/it]11/15/2022 19:38:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2316e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:18 - INFO - train.train_snli_ve - loss is tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5458/16548 [2:31:58<5:10:00,  1.68s/it]11/15/2022 19:38:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:19 - INFO - train.train_snli_ve - loss is tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5459/16548 [2:32:00<5:10:15,  1.68s/it]11/15/2022 19:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:21 - INFO - train.train_snli_ve - loss is tensor(0.8536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5460/16548 [2:32:02<5:10:52,  1.68s/it]11/15/2022 19:38:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:23 - INFO - train.train_snli_ve - loss is tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5461/16548 [2:32:03<5:11:50,  1.69s/it]11/15/2022 19:38:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.8890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:24 - INFO - train.train_snli_ve - loss is tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5462/16548 [2:32:05<5:11:03,  1.68s/it]11/15/2022 19:38:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.5210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:26 - INFO - train.train_snli_ve - loss is tensor(0.4760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5463/16548 [2:32:07<5:11:26,  1.69s/it]11/15/2022 19:38:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.4216e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:28 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5464/16548 [2:32:08<5:10:31,  1.68s/it]11/15/2022 19:38:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:29 - INFO - train.train_snli_ve - loss is tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5465/16548 [2:32:10<5:11:13,  1.68s/it]11/15/2022 19:38:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.6429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:31 - INFO - train.train_snli_ve - loss is tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5466/16548 [2:32:12<5:10:54,  1.68s/it]11/15/2022 19:38:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:33 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5467/16548 [2:32:13<5:08:49,  1.67s/it]11/15/2022 19:38:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.1636e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:34 - INFO - train.train_snli_ve - loss is tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5468/16548 [2:32:15<5:08:20,  1.67s/it]11/15/2022 19:38:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.3445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:36 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5469/16548 [2:32:17<5:11:40,  1.69s/it]11/15/2022 19:38:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:38 - INFO - train.train_snli_ve - loss is tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5470/16548 [2:32:18<5:09:45,  1.68s/it]11/15/2022 19:38:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.5335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:39 - INFO - train.train_snli_ve - loss is tensor(0.9913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5471/16548 [2:32:20<5:07:35,  1.67s/it]11/15/2022 19:38:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.6432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:41 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5472/16548 [2:32:22<5:10:46,  1.68s/it]11/15/2022 19:38:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:43 - INFO - train.train_snli_ve - loss is tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5473/16548 [2:32:24<5:11:03,  1.69s/it]11/15/2022 19:38:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.3617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:44 - INFO - train.train_snli_ve - loss is tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5474/16548 [2:32:25<5:10:03,  1.68s/it]11/15/2022 19:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.4628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:46 - INFO - train.train_snli_ve - loss is tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5475/16548 [2:32:27<5:10:11,  1.68s/it]11/15/2022 19:38:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.7737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:48 - INFO - train.train_snli_ve - loss is tensor(0.4685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5476/16548 [2:32:29<5:10:15,  1.68s/it]11/15/2022 19:38:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.1518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:50 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5477/16548 [2:32:30<5:10:41,  1.68s/it]11/15/2022 19:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.0296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:51 - INFO - train.train_snli_ve - loss is tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5478/16548 [2:32:32<5:10:22,  1.68s/it]11/15/2022 19:38:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.8118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:53 - INFO - train.train_snli_ve - loss is tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5479/16548 [2:32:34<5:10:23,  1.68s/it]11/15/2022 19:38:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.9807e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:55 - INFO - train.train_snli_ve - loss is tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5480/16548 [2:32:35<5:09:38,  1.68s/it]11/15/2022 19:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.9511e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:56 - INFO - train.train_snli_ve - loss is tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5481/16548 [2:32:37<5:09:39,  1.68s/it]11/15/2022 19:38:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.6653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:38:58 - INFO - train.train_snli_ve - loss is tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5482/16548 [2:32:39<5:09:04,  1.68s/it]11/15/2022 19:39:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.7719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:00 - INFO - train.train_snli_ve - loss is tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5483/16548 [2:32:40<5:09:58,  1.68s/it]11/15/2022 19:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.4067e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:01 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5484/16548 [2:32:42<5:09:32,  1.68s/it]11/15/2022 19:39:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:03 - INFO - train.train_snli_ve - loss is tensor(0.4995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5485/16548 [2:32:44<5:07:27,  1.67s/it]11/15/2022 19:39:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:05 - INFO - train.train_snli_ve - loss is tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5486/16548 [2:32:45<5:07:36,  1.67s/it]11/15/2022 19:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.9834e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:06 - INFO - train.train_snli_ve - loss is tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5487/16548 [2:32:47<5:08:33,  1.67s/it]11/15/2022 19:39:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.5195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:08 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5488/16548 [2:32:49<5:07:42,  1.67s/it]11/15/2022 19:39:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.9369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:10 - INFO - train.train_snli_ve - loss is tensor(0.5119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5489/16548 [2:32:50<5:07:06,  1.67s/it]11/15/2022 19:39:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.4931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:11 - INFO - train.train_snli_ve - loss is tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5490/16548 [2:32:52<5:05:48,  1.66s/it]11/15/2022 19:39:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.1015e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:13 - INFO - train.train_snli_ve - loss is tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5491/16548 [2:32:54<5:07:39,  1.67s/it]11/15/2022 19:39:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:15 - INFO - train.train_snli_ve - loss is tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5492/16548 [2:32:55<5:08:38,  1.67s/it]11/15/2022 19:39:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.2314e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:16 - INFO - train.train_snli_ve - loss is tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5493/16548 [2:32:57<5:07:01,  1.67s/it]11/15/2022 19:39:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0680e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:18 - INFO - train.train_snli_ve - loss is tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5494/16548 [2:32:59<5:07:11,  1.67s/it]11/15/2022 19:39:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.9603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:20 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5495/16548 [2:33:00<5:08:04,  1.67s/it]11/15/2022 19:39:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.0266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:21 - INFO - train.train_snli_ve - loss is tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5496/16548 [2:33:02<5:05:32,  1.66s/it]11/15/2022 19:39:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.9086e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:23 - INFO - train.train_snli_ve - loss is tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5497/16548 [2:33:04<5:06:59,  1.67s/it]11/15/2022 19:39:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:25 - INFO - train.train_snli_ve - loss is tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5498/16548 [2:33:05<5:04:28,  1.65s/it]11/15/2022 19:39:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.9676e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:26 - INFO - train.train_snli_ve - loss is tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5499/16548 [2:33:07<5:04:55,  1.66s/it]11/15/2022 19:39:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.4893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:28 - INFO - train.train_snli_ve - loss is tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5500/16548 [2:33:09<5:12:38,  1.70s/it]11/15/2022 19:39:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:30 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5501/16548 [2:33:10<5:11:46,  1.69s/it]11/15/2022 19:39:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.9853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:31 - INFO - train.train_snli_ve - loss is tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5502/16548 [2:33:12<5:09:27,  1.68s/it]11/15/2022 19:39:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:33 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5503/16548 [2:33:14<5:07:46,  1.67s/it]11/15/2022 19:39:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:35 - INFO - train.train_snli_ve - loss is tensor(0.8261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5504/16548 [2:33:15<5:10:55,  1.69s/it]11/15/2022 19:39:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.3173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:36 - INFO - train.train_snli_ve - loss is tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5505/16548 [2:33:17<5:10:23,  1.69s/it]11/15/2022 19:39:38 - INFO - train.train_snli_ve - kd_loss is tensor(6.6762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:38 - INFO - train.train_snli_ve - loss is tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5506/16548 [2:33:19<5:10:59,  1.69s/it]11/15/2022 19:39:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.3068e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:40 - INFO - train.train_snli_ve - loss is tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5507/16548 [2:33:20<5:08:22,  1.68s/it]11/15/2022 19:39:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.5861e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:41 - INFO - train.train_snli_ve - loss is tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5508/16548 [2:33:22<5:08:09,  1.67s/it]11/15/2022 19:39:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:43 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5509/16548 [2:33:24<5:06:51,  1.67s/it]11/15/2022 19:39:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:45 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5510/16548 [2:33:26<5:08:10,  1.68s/it]11/15/2022 19:39:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.9957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:46 - INFO - train.train_snli_ve - loss is tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5511/16548 [2:33:27<5:08:09,  1.68s/it]11/15/2022 19:39:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.7693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:48 - INFO - train.train_snli_ve - loss is tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5512/16548 [2:33:29<5:05:52,  1.66s/it]11/15/2022 19:39:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.0928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:50 - INFO - train.train_snli_ve - loss is tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5513/16548 [2:33:30<5:06:02,  1.66s/it]11/15/2022 19:39:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:51 - INFO - train.train_snli_ve - loss is tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5514/16548 [2:33:32<5:05:38,  1.66s/it]11/15/2022 19:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.4143e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:53 - INFO - train.train_snli_ve - loss is tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5515/16548 [2:33:34<5:04:48,  1.66s/it]11/15/2022 19:39:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:55 - INFO - train.train_snli_ve - loss is tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5516/16548 [2:33:35<5:05:44,  1.66s/it]11/15/2022 19:39:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.1510e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:56 - INFO - train.train_snli_ve - loss is tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5517/16548 [2:33:37<5:03:58,  1.65s/it]11/15/2022 19:39:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:39:58 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5518/16548 [2:33:39<5:02:04,  1.64s/it]11/15/2022 19:40:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.6095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:00 - INFO - train.train_snli_ve - loss is tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5519/16548 [2:33:40<5:02:30,  1.65s/it]11/15/2022 19:40:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:01 - INFO - train.train_snli_ve - loss is tensor(0.8890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5520/16548 [2:33:42<5:02:47,  1.65s/it]11/15/2022 19:40:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.7295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:03 - INFO - train.train_snli_ve - loss is tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5521/16548 [2:33:44<5:03:14,  1.65s/it]11/15/2022 19:40:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:05 - INFO - train.train_snli_ve - loss is tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5522/16548 [2:33:45<5:03:56,  1.65s/it]11/15/2022 19:40:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.3767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:06 - INFO - train.train_snli_ve - loss is tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5523/16548 [2:33:47<5:04:51,  1.66s/it]11/15/2022 19:40:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.0498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:08 - INFO - train.train_snli_ve - loss is tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5524/16548 [2:33:49<5:06:32,  1.67s/it]11/15/2022 19:40:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.6986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:10 - INFO - train.train_snli_ve - loss is tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5525/16548 [2:33:50<5:06:13,  1.67s/it]11/15/2022 19:40:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.8456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:11 - INFO - train.train_snli_ve - loss is tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5526/16548 [2:33:52<5:05:04,  1.66s/it]11/15/2022 19:40:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.1430e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:13 - INFO - train.train_snli_ve - loss is tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5527/16548 [2:33:54<5:09:51,  1.69s/it]11/15/2022 19:40:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.2888e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:15 - INFO - train.train_snli_ve - loss is tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5528/16548 [2:33:55<5:09:58,  1.69s/it]11/15/2022 19:40:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.3628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:16 - INFO - train.train_snli_ve - loss is tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5529/16548 [2:33:57<5:10:18,  1.69s/it]11/15/2022 19:40:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.2362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:18 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5530/16548 [2:33:59<5:08:59,  1.68s/it]11/15/2022 19:40:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.9403e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:20 - INFO - train.train_snli_ve - loss is tensor(0.5143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5531/16548 [2:34:00<5:06:15,  1.67s/it]11/15/2022 19:40:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.2996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:21 - INFO - train.train_snli_ve - loss is tensor(0.8178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5532/16548 [2:34:02<5:06:11,  1.67s/it]11/15/2022 19:40:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.1146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:23 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5533/16548 [2:34:04<5:06:01,  1.67s/it]11/15/2022 19:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:25 - INFO - train.train_snli_ve - loss is tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5534/16548 [2:34:05<5:05:34,  1.66s/it]11/15/2022 19:40:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.0968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:26 - INFO - train.train_snli_ve - loss is tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5535/16548 [2:34:07<5:06:23,  1.67s/it]11/15/2022 19:40:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.5654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:28 - INFO - train.train_snli_ve - loss is tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5536/16548 [2:34:09<5:07:50,  1.68s/it]11/15/2022 19:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.3673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:30 - INFO - train.train_snli_ve - loss is tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5537/16548 [2:34:10<5:08:18,  1.68s/it]11/15/2022 19:40:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.3869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:31 - INFO - train.train_snli_ve - loss is tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5538/16548 [2:34:12<5:07:13,  1.67s/it]11/15/2022 19:40:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.0359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:33 - INFO - train.train_snli_ve - loss is tensor(0.8110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5539/16548 [2:34:14<5:07:39,  1.68s/it]11/15/2022 19:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.9508e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:35 - INFO - train.train_snli_ve - loss is tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5540/16548 [2:34:15<5:06:56,  1.67s/it]11/15/2022 19:40:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.0918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:36 - INFO - train.train_snli_ve - loss is tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5541/16548 [2:34:17<5:08:39,  1.68s/it]11/15/2022 19:40:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4263e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:38 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5542/16548 [2:34:19<5:08:08,  1.68s/it]11/15/2022 19:40:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:40 - INFO - train.train_snli_ve - loss is tensor(0.5138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33% 5543/16548 [2:34:21<5:08:22,  1.68s/it]11/15/2022 19:40:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.4973e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:41 - INFO - train.train_snli_ve - loss is tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5544/16548 [2:34:22<5:05:06,  1.66s/it]11/15/2022 19:40:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1088e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:43 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5545/16548 [2:34:24<5:03:53,  1.66s/it]11/15/2022 19:40:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.8189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:45 - INFO - train.train_snli_ve - loss is tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5546/16548 [2:34:26<5:08:51,  1.68s/it]11/15/2022 19:40:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:47 - INFO - train.train_snli_ve - loss is tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5547/16548 [2:34:27<5:08:40,  1.68s/it]11/15/2022 19:40:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.5552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:48 - INFO - train.train_snli_ve - loss is tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5548/16548 [2:34:29<5:09:59,  1.69s/it]11/15/2022 19:40:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.7855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:50 - INFO - train.train_snli_ve - loss is tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5549/16548 [2:34:31<5:08:31,  1.68s/it]11/15/2022 19:40:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.6884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:52 - INFO - train.train_snli_ve - loss is tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5550/16548 [2:34:32<5:09:49,  1.69s/it]11/15/2022 19:40:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:53 - INFO - train.train_snli_ve - loss is tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5551/16548 [2:34:34<5:08:18,  1.68s/it]11/15/2022 19:40:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:55 - INFO - train.train_snli_ve - loss is tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5552/16548 [2:34:36<5:10:00,  1.69s/it]11/15/2022 19:40:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:57 - INFO - train.train_snli_ve - loss is tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5553/16548 [2:34:37<5:13:22,  1.71s/it]11/15/2022 19:40:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.3333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:40:58 - INFO - train.train_snli_ve - loss is tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5554/16548 [2:34:39<5:15:15,  1.72s/it]11/15/2022 19:41:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.5342e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:00 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5555/16548 [2:34:41<5:14:09,  1.71s/it]11/15/2022 19:41:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.7968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:02 - INFO - train.train_snli_ve - loss is tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5556/16548 [2:34:43<5:12:08,  1.70s/it]11/15/2022 19:41:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:04 - INFO - train.train_snli_ve - loss is tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5557/16548 [2:34:44<5:12:17,  1.70s/it]11/15/2022 19:41:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:05 - INFO - train.train_snli_ve - loss is tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5558/16548 [2:34:46<5:10:03,  1.69s/it]11/15/2022 19:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:07 - INFO - train.train_snli_ve - loss is tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5559/16548 [2:34:48<5:07:19,  1.68s/it]11/15/2022 19:41:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:09 - INFO - train.train_snli_ve - loss is tensor(0.9625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5560/16548 [2:34:49<5:07:26,  1.68s/it]11/15/2022 19:41:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:10 - INFO - train.train_snli_ve - loss is tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5561/16548 [2:34:51<5:04:47,  1.66s/it]11/15/2022 19:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:12 - INFO - train.train_snli_ve - loss is tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5562/16548 [2:34:53<5:09:43,  1.69s/it]11/15/2022 19:41:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:14 - INFO - train.train_snli_ve - loss is tensor(0.5278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5563/16548 [2:34:54<5:07:53,  1.68s/it]11/15/2022 19:41:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.2979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:15 - INFO - train.train_snli_ve - loss is tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5564/16548 [2:34:56<5:05:37,  1.67s/it]11/15/2022 19:41:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.2352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:17 - INFO - train.train_snli_ve - loss is tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5565/16548 [2:34:58<5:07:03,  1.68s/it]11/15/2022 19:41:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.7153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:19 - INFO - train.train_snli_ve - loss is tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5566/16548 [2:34:59<5:07:21,  1.68s/it]11/15/2022 19:41:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.6533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:20 - INFO - train.train_snli_ve - loss is tensor(0.8633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5567/16548 [2:35:01<5:06:45,  1.68s/it]11/15/2022 19:41:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6372e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:22 - INFO - train.train_snli_ve - loss is tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5568/16548 [2:35:03<5:08:09,  1.68s/it]11/15/2022 19:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.5730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:24 - INFO - train.train_snli_ve - loss is tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5569/16548 [2:35:04<5:07:57,  1.68s/it]11/15/2022 19:41:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:25 - INFO - train.train_snli_ve - loss is tensor(0.7636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5570/16548 [2:35:06<5:08:49,  1.69s/it]11/15/2022 19:41:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.4178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:27 - INFO - train.train_snli_ve - loss is tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5571/16548 [2:35:08<5:09:00,  1.69s/it]11/15/2022 19:41:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.9932e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:29 - INFO - train.train_snli_ve - loss is tensor(0.9864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5572/16548 [2:35:09<5:06:06,  1.67s/it]11/15/2022 19:41:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.9468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:30 - INFO - train.train_snli_ve - loss is tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5573/16548 [2:35:11<5:06:36,  1.68s/it]11/15/2022 19:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4363e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:32 - INFO - train.train_snli_ve - loss is tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5574/16548 [2:35:13<5:06:00,  1.67s/it]11/15/2022 19:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.2007e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:34 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5575/16548 [2:35:14<5:08:25,  1.69s/it]11/15/2022 19:41:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:35 - INFO - train.train_snli_ve - loss is tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5576/16548 [2:35:16<5:07:33,  1.68s/it]11/15/2022 19:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.1385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:37 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5577/16548 [2:35:18<5:06:06,  1.67s/it]11/15/2022 19:41:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.4260e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:39 - INFO - train.train_snli_ve - loss is tensor(0.5810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5578/16548 [2:35:19<5:04:19,  1.66s/it]11/15/2022 19:41:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:40 - INFO - train.train_snli_ve - loss is tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5579/16548 [2:35:21<5:03:52,  1.66s/it]11/15/2022 19:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.2494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:42 - INFO - train.train_snli_ve - loss is tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5580/16548 [2:35:23<5:04:58,  1.67s/it]11/15/2022 19:41:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:44 - INFO - train.train_snli_ve - loss is tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5581/16548 [2:35:24<5:05:36,  1.67s/it]11/15/2022 19:41:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.5303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:45 - INFO - train.train_snli_ve - loss is tensor(0.8842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5582/16548 [2:35:26<5:04:51,  1.67s/it]11/15/2022 19:41:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:47 - INFO - train.train_snli_ve - loss is tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5583/16548 [2:35:28<5:05:54,  1.67s/it]11/15/2022 19:41:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:49 - INFO - train.train_snli_ve - loss is tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5584/16548 [2:35:29<5:02:58,  1.66s/it]11/15/2022 19:41:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.8820e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:50 - INFO - train.train_snli_ve - loss is tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5585/16548 [2:35:31<5:04:14,  1.67s/it]11/15/2022 19:41:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0818e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:52 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5586/16548 [2:35:33<5:05:57,  1.67s/it]11/15/2022 19:41:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:54 - INFO - train.train_snli_ve - loss is tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5587/16548 [2:35:35<5:06:02,  1.68s/it]11/15/2022 19:41:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:55 - INFO - train.train_snli_ve - loss is tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5588/16548 [2:35:36<5:04:45,  1.67s/it]11/15/2022 19:41:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2341e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:57 - INFO - train.train_snli_ve - loss is tensor(0.5637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5589/16548 [2:35:38<5:05:43,  1.67s/it]11/15/2022 19:41:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:41:59 - INFO - train.train_snli_ve - loss is tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5590/16548 [2:35:40<5:04:49,  1.67s/it]11/15/2022 19:42:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.0785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:00 - INFO - train.train_snli_ve - loss is tensor(0.8293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5591/16548 [2:35:41<5:03:29,  1.66s/it]11/15/2022 19:42:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.7604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:02 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5592/16548 [2:35:43<5:02:51,  1.66s/it]11/15/2022 19:42:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.8232e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:04 - INFO - train.train_snli_ve - loss is tensor(1.0354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5593/16548 [2:35:44<5:02:35,  1.66s/it]11/15/2022 19:42:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:05 - INFO - train.train_snli_ve - loss is tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5594/16548 [2:35:46<5:01:05,  1.65s/it]11/15/2022 19:42:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.9404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:07 - INFO - train.train_snli_ve - loss is tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5595/16548 [2:35:48<5:02:29,  1.66s/it]11/15/2022 19:42:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.8692e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:09 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5596/16548 [2:35:49<5:03:01,  1.66s/it]11/15/2022 19:42:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.1325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:10 - INFO - train.train_snli_ve - loss is tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5597/16548 [2:35:51<5:06:55,  1.68s/it]11/15/2022 19:42:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.0660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:12 - INFO - train.train_snli_ve - loss is tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5598/16548 [2:35:53<5:07:07,  1.68s/it]11/15/2022 19:42:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:14 - INFO - train.train_snli_ve - loss is tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5599/16548 [2:35:54<5:04:52,  1.67s/it]11/15/2022 19:42:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0444e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:15 - INFO - train.train_snli_ve - loss is tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5600/16548 [2:35:56<5:07:18,  1.68s/it]11/15/2022 19:42:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1458e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:17 - INFO - train.train_snli_ve - loss is tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5601/16548 [2:35:58<5:04:35,  1.67s/it]11/15/2022 19:42:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.1787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:19 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5602/16548 [2:35:59<5:03:38,  1.66s/it]11/15/2022 19:42:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.4972e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:20 - INFO - train.train_snli_ve - loss is tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5603/16548 [2:36:01<5:02:14,  1.66s/it]11/15/2022 19:42:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:22 - INFO - train.train_snli_ve - loss is tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5604/16548 [2:36:03<5:03:10,  1.66s/it]11/15/2022 19:42:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:24 - INFO - train.train_snli_ve - loss is tensor(0.7426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5605/16548 [2:36:04<5:04:16,  1.67s/it]11/15/2022 19:42:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.5769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:25 - INFO - train.train_snli_ve - loss is tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5606/16548 [2:36:06<5:02:50,  1.66s/it]11/15/2022 19:42:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.0135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:27 - INFO - train.train_snli_ve - loss is tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5607/16548 [2:36:08<5:02:38,  1.66s/it]11/15/2022 19:42:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:29 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5608/16548 [2:36:09<5:01:44,  1.65s/it]11/15/2022 19:42:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.4115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:30 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5609/16548 [2:36:11<5:02:25,  1.66s/it]11/15/2022 19:42:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:32 - INFO - train.train_snli_ve - loss is tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5610/16548 [2:36:13<5:02:49,  1.66s/it]11/15/2022 19:42:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.8261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:34 - INFO - train.train_snli_ve - loss is tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5611/16548 [2:36:14<5:02:37,  1.66s/it]11/15/2022 19:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.6258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:35 - INFO - train.train_snli_ve - loss is tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5612/16548 [2:36:16<5:02:17,  1.66s/it]11/15/2022 19:42:37 - INFO - train.train_snli_ve - kd_loss is tensor(8.0071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:37 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5613/16548 [2:36:18<5:01:06,  1.65s/it]11/15/2022 19:42:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.7746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:39 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5614/16548 [2:36:19<5:01:58,  1.66s/it]11/15/2022 19:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.1343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:40 - INFO - train.train_snli_ve - loss is tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5615/16548 [2:36:21<5:02:01,  1.66s/it]11/15/2022 19:42:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.6980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:42 - INFO - train.train_snli_ve - loss is tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5616/16548 [2:36:23<5:02:06,  1.66s/it]11/15/2022 19:42:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:44 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5617/16548 [2:36:24<5:00:14,  1.65s/it]11/15/2022 19:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:45 - INFO - train.train_snli_ve - loss is tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5618/16548 [2:36:26<5:00:30,  1.65s/it]11/15/2022 19:42:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.8943e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:47 - INFO - train.train_snli_ve - loss is tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5619/16548 [2:36:28<5:00:43,  1.65s/it]11/15/2022 19:42:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:49 - INFO - train.train_snli_ve - loss is tensor(0.4337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5620/16548 [2:36:29<5:01:23,  1.65s/it]11/15/2022 19:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:50 - INFO - train.train_snli_ve - loss is tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5621/16548 [2:36:31<5:04:31,  1.67s/it]11/15/2022 19:42:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.6715e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:52 - INFO - train.train_snli_ve - loss is tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5622/16548 [2:36:33<5:04:18,  1.67s/it]11/15/2022 19:42:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:54 - INFO - train.train_snli_ve - loss is tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5623/16548 [2:36:34<5:03:06,  1.66s/it]11/15/2022 19:42:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.8740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:55 - INFO - train.train_snli_ve - loss is tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5624/16548 [2:36:36<5:03:42,  1.67s/it]11/15/2022 19:42:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.1697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:57 - INFO - train.train_snli_ve - loss is tensor(0.9419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5625/16548 [2:36:38<5:02:11,  1.66s/it]11/15/2022 19:42:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:42:59 - INFO - train.train_snli_ve - loss is tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5626/16548 [2:36:39<5:03:30,  1.67s/it]11/15/2022 19:43:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:00 - INFO - train.train_snli_ve - loss is tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5627/16548 [2:36:41<5:03:47,  1.67s/it]11/15/2022 19:43:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.7737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:02 - INFO - train.train_snli_ve - loss is tensor(1.0049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5628/16548 [2:36:43<5:06:04,  1.68s/it]11/15/2022 19:43:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9011e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:04 - INFO - train.train_snli_ve - loss is tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5629/16548 [2:36:44<5:06:58,  1.69s/it]11/15/2022 19:43:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.2579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:05 - INFO - train.train_snli_ve - loss is tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5630/16548 [2:36:46<5:05:35,  1.68s/it]11/15/2022 19:43:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.2222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:07 - INFO - train.train_snli_ve - loss is tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5631/16548 [2:36:48<5:04:38,  1.67s/it]11/15/2022 19:43:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:09 - INFO - train.train_snli_ve - loss is tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5632/16548 [2:36:49<5:07:52,  1.69s/it]11/15/2022 19:43:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.9902e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:10 - INFO - train.train_snli_ve - loss is tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5633/16548 [2:36:51<5:04:03,  1.67s/it]11/15/2022 19:43:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:12 - INFO - train.train_snli_ve - loss is tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5634/16548 [2:36:53<5:04:28,  1.67s/it]11/15/2022 19:43:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:14 - INFO - train.train_snli_ve - loss is tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5635/16548 [2:36:54<5:05:06,  1.68s/it]11/15/2022 19:43:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:15 - INFO - train.train_snli_ve - loss is tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5636/16548 [2:36:56<5:06:45,  1.69s/it]11/15/2022 19:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:17 - INFO - train.train_snli_ve - loss is tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5637/16548 [2:36:58<5:06:28,  1.69s/it]11/15/2022 19:43:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:19 - INFO - train.train_snli_ve - loss is tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5638/16548 [2:37:00<5:06:06,  1.68s/it]11/15/2022 19:43:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.1628e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:20 - INFO - train.train_snli_ve - loss is tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5639/16548 [2:37:01<5:05:13,  1.68s/it]11/15/2022 19:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.2304e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:22 - INFO - train.train_snli_ve - loss is tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5640/16548 [2:37:03<5:06:28,  1.69s/it]11/15/2022 19:43:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.7227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:24 - INFO - train.train_snli_ve - loss is tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5641/16548 [2:37:05<5:06:28,  1.69s/it]11/15/2022 19:43:26 - INFO - train.train_snli_ve - kd_loss is tensor(9.9621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:26 - INFO - train.train_snli_ve - loss is tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5642/16548 [2:37:06<5:06:00,  1.68s/it]11/15/2022 19:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:27 - INFO - train.train_snli_ve - loss is tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5643/16548 [2:37:08<5:07:49,  1.69s/it]11/15/2022 19:43:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:29 - INFO - train.train_snli_ve - loss is tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5644/16548 [2:37:10<5:03:56,  1.67s/it]11/15/2022 19:43:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.4151e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:31 - INFO - train.train_snli_ve - loss is tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5645/16548 [2:37:11<5:04:51,  1.68s/it]11/15/2022 19:43:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.1033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:32 - INFO - train.train_snli_ve - loss is tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5646/16548 [2:37:13<5:03:08,  1.67s/it]11/15/2022 19:43:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:34 - INFO - train.train_snli_ve - loss is tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5647/16548 [2:37:15<5:03:32,  1.67s/it]11/15/2022 19:43:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.3876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:36 - INFO - train.train_snli_ve - loss is tensor(0.5693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5648/16548 [2:37:16<5:05:19,  1.68s/it]11/15/2022 19:43:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.3134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:37 - INFO - train.train_snli_ve - loss is tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5649/16548 [2:37:18<5:06:19,  1.69s/it]11/15/2022 19:43:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:39 - INFO - train.train_snli_ve - loss is tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5650/16548 [2:37:20<5:06:39,  1.69s/it]11/15/2022 19:43:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.0702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:41 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5651/16548 [2:37:21<5:05:40,  1.68s/it]11/15/2022 19:43:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.4220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:42 - INFO - train.train_snli_ve - loss is tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5652/16548 [2:37:23<5:06:10,  1.69s/it]11/15/2022 19:43:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.5821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:44 - INFO - train.train_snli_ve - loss is tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5653/16548 [2:37:25<5:07:20,  1.69s/it]11/15/2022 19:43:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1660e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:46 - INFO - train.train_snli_ve - loss is tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5654/16548 [2:37:26<5:06:13,  1.69s/it]11/15/2022 19:43:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:47 - INFO - train.train_snli_ve - loss is tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5655/16548 [2:37:28<5:04:57,  1.68s/it]11/15/2022 19:43:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:49 - INFO - train.train_snli_ve - loss is tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5656/16548 [2:37:30<5:07:34,  1.69s/it]11/15/2022 19:43:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:51 - INFO - train.train_snli_ve - loss is tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5657/16548 [2:37:31<5:04:18,  1.68s/it]11/15/2022 19:43:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:52 - INFO - train.train_snli_ve - loss is tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5658/16548 [2:37:33<5:02:12,  1.67s/it]11/15/2022 19:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:54 - INFO - train.train_snli_ve - loss is tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5659/16548 [2:37:35<5:03:55,  1.67s/it]11/15/2022 19:43:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.5315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:56 - INFO - train.train_snli_ve - loss is tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5660/16548 [2:37:37<5:05:32,  1.68s/it]11/15/2022 19:43:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:57 - INFO - train.train_snli_ve - loss is tensor(0.8638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5661/16548 [2:37:38<5:05:59,  1.69s/it]11/15/2022 19:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:43:59 - INFO - train.train_snli_ve - loss is tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5662/16548 [2:37:40<5:06:30,  1.69s/it]11/15/2022 19:44:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.8060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:01 - INFO - train.train_snli_ve - loss is tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5663/16548 [2:37:42<5:04:20,  1.68s/it]11/15/2022 19:44:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:03 - INFO - train.train_snli_ve - loss is tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5664/16548 [2:37:43<5:04:27,  1.68s/it]11/15/2022 19:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:04 - INFO - train.train_snli_ve - loss is tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5665/16548 [2:37:45<5:04:27,  1.68s/it]11/15/2022 19:44:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:06 - INFO - train.train_snli_ve - loss is tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5666/16548 [2:37:47<5:05:13,  1.68s/it]11/15/2022 19:44:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:08 - INFO - train.train_snli_ve - loss is tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5667/16548 [2:37:48<5:07:06,  1.69s/it]11/15/2022 19:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.7619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:09 - INFO - train.train_snli_ve - loss is tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5668/16548 [2:37:50<5:06:57,  1.69s/it]11/15/2022 19:44:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:11 - INFO - train.train_snli_ve - loss is tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5669/16548 [2:37:52<5:06:49,  1.69s/it]11/15/2022 19:44:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.3128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:13 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5670/16548 [2:37:53<5:03:38,  1.67s/it]11/15/2022 19:44:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:14 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5671/16548 [2:37:55<5:03:29,  1.67s/it]11/15/2022 19:44:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:16 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5672/16548 [2:37:57<5:03:40,  1.68s/it]11/15/2022 19:44:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:18 - INFO - train.train_snli_ve - loss is tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5673/16548 [2:37:58<5:01:24,  1.66s/it]11/15/2022 19:44:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.6282e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:19 - INFO - train.train_snli_ve - loss is tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5674/16548 [2:38:00<5:00:20,  1.66s/it]11/15/2022 19:44:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.9666e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:21 - INFO - train.train_snli_ve - loss is tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5675/16548 [2:38:02<5:03:14,  1.67s/it]11/15/2022 19:44:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1507e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:23 - INFO - train.train_snli_ve - loss is tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5676/16548 [2:38:03<5:03:11,  1.67s/it]11/15/2022 19:44:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.9954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:24 - INFO - train.train_snli_ve - loss is tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5677/16548 [2:38:05<5:04:51,  1.68s/it]11/15/2022 19:44:26 - INFO - train.train_snli_ve - kd_loss is tensor(7.8629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:26 - INFO - train.train_snli_ve - loss is tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5678/16548 [2:38:07<5:03:54,  1.68s/it]11/15/2022 19:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.7468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:28 - INFO - train.train_snli_ve - loss is tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5679/16548 [2:38:08<5:02:15,  1.67s/it]11/15/2022 19:44:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.4805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:29 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5680/16548 [2:38:10<5:01:00,  1.66s/it]11/15/2022 19:44:31 - INFO - train.train_snli_ve - kd_loss is tensor(5.2320e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:31 - INFO - train.train_snli_ve - loss is tensor(0.7402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5681/16548 [2:38:12<5:05:04,  1.68s/it]11/15/2022 19:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.1373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:33 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5682/16548 [2:38:13<5:07:56,  1.70s/it]11/15/2022 19:44:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.6525e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:34 - INFO - train.train_snli_ve - loss is tensor(0.8802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5683/16548 [2:38:15<5:06:27,  1.69s/it]11/15/2022 19:44:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:36 - INFO - train.train_snli_ve - loss is tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5684/16548 [2:38:17<5:05:05,  1.68s/it]11/15/2022 19:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.0900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:38 - INFO - train.train_snli_ve - loss is tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5685/16548 [2:38:18<5:02:52,  1.67s/it]11/15/2022 19:44:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.2933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:39 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5686/16548 [2:38:20<5:02:14,  1.67s/it]11/15/2022 19:44:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.1116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:41 - INFO - train.train_snli_ve - loss is tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5687/16548 [2:38:22<5:03:19,  1.68s/it]11/15/2022 19:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.1909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:43 - INFO - train.train_snli_ve - loss is tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5688/16548 [2:38:24<5:03:13,  1.68s/it]11/15/2022 19:44:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.6432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:44 - INFO - train.train_snli_ve - loss is tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5689/16548 [2:38:25<5:01:02,  1.66s/it]11/15/2022 19:44:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.9784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:46 - INFO - train.train_snli_ve - loss is tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5690/16548 [2:38:27<5:00:54,  1.66s/it]11/15/2022 19:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.0403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:48 - INFO - train.train_snli_ve - loss is tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5691/16548 [2:38:29<5:03:01,  1.67s/it]11/15/2022 19:44:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.7957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:50 - INFO - train.train_snli_ve - loss is tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5692/16548 [2:38:30<5:07:02,  1.70s/it]11/15/2022 19:44:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:51 - INFO - train.train_snli_ve - loss is tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5693/16548 [2:38:32<5:07:32,  1.70s/it]11/15/2022 19:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(8.9907e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:53 - INFO - train.train_snli_ve - loss is tensor(0.7754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5694/16548 [2:38:34<5:06:54,  1.70s/it]11/15/2022 19:44:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.7076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:55 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5695/16548 [2:38:35<5:05:35,  1.69s/it]11/15/2022 19:44:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.0348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:56 - INFO - train.train_snli_ve - loss is tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5696/16548 [2:38:37<5:04:23,  1.68s/it]11/15/2022 19:44:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.3206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:44:58 - INFO - train.train_snli_ve - loss is tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5697/16548 [2:38:39<5:03:41,  1.68s/it]11/15/2022 19:45:00 - INFO - train.train_snli_ve - kd_loss is tensor(6.1139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:00 - INFO - train.train_snli_ve - loss is tensor(1.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5698/16548 [2:38:40<5:03:23,  1.68s/it]11/15/2022 19:45:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.5203e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:01 - INFO - train.train_snli_ve - loss is tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5699/16548 [2:38:42<5:03:04,  1.68s/it]11/15/2022 19:45:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:03 - INFO - train.train_snli_ve - loss is tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5700/16548 [2:38:44<5:06:32,  1.70s/it]11/15/2022 19:45:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.9742e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:05 - INFO - train.train_snli_ve - loss is tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5701/16548 [2:38:45<5:05:20,  1.69s/it]11/15/2022 19:45:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.7101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:06 - INFO - train.train_snli_ve - loss is tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5702/16548 [2:38:47<5:03:22,  1.68s/it]11/15/2022 19:45:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.8984e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:08 - INFO - train.train_snli_ve - loss is tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5703/16548 [2:38:49<5:02:10,  1.67s/it]11/15/2022 19:45:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:10 - INFO - train.train_snli_ve - loss is tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5704/16548 [2:38:50<5:00:52,  1.66s/it]11/15/2022 19:45:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.5074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:11 - INFO - train.train_snli_ve - loss is tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5705/16548 [2:38:52<5:01:57,  1.67s/it]11/15/2022 19:45:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.4336e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:13 - INFO - train.train_snli_ve - loss is tensor(0.7427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5706/16548 [2:38:54<5:00:43,  1.66s/it]11/15/2022 19:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.1858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:15 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5707/16548 [2:38:55<5:00:23,  1.66s/it]11/15/2022 19:45:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.6237e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:16 - INFO - train.train_snli_ve - loss is tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5708/16548 [2:38:57<5:01:53,  1.67s/it]11/15/2022 19:45:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.4505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:18 - INFO - train.train_snli_ve - loss is tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34% 5709/16548 [2:38:59<5:03:43,  1.68s/it]11/15/2022 19:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.9380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:20 - INFO - train.train_snli_ve - loss is tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5710/16548 [2:39:00<5:04:23,  1.69s/it]11/15/2022 19:45:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.9504e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:21 - INFO - train.train_snli_ve - loss is tensor(0.4593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5711/16548 [2:39:02<5:03:04,  1.68s/it]11/15/2022 19:45:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.1501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:23 - INFO - train.train_snli_ve - loss is tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5712/16548 [2:39:04<5:01:16,  1.67s/it]11/15/2022 19:45:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.6840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:25 - INFO - train.train_snli_ve - loss is tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5713/16548 [2:39:06<5:05:49,  1.69s/it]11/15/2022 19:45:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.7243e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:26 - INFO - train.train_snli_ve - loss is tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5714/16548 [2:39:07<5:04:31,  1.69s/it]11/15/2022 19:45:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.1128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:28 - INFO - train.train_snli_ve - loss is tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5715/16548 [2:39:09<5:06:42,  1.70s/it]11/15/2022 19:45:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.3879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:30 - INFO - train.train_snli_ve - loss is tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5716/16548 [2:39:11<5:06:30,  1.70s/it]11/15/2022 19:45:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.0324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:32 - INFO - train.train_snli_ve - loss is tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5717/16548 [2:39:12<5:05:51,  1.69s/it]11/15/2022 19:45:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.4485e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:33 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5718/16548 [2:39:14<5:04:11,  1.69s/it]11/15/2022 19:45:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.7133e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:35 - INFO - train.train_snli_ve - loss is tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5719/16548 [2:39:16<5:02:00,  1.67s/it]11/15/2022 19:45:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.9000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:37 - INFO - train.train_snli_ve - loss is tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5720/16548 [2:39:17<5:03:30,  1.68s/it]11/15/2022 19:45:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:38 - INFO - train.train_snli_ve - loss is tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5721/16548 [2:39:19<5:01:40,  1.67s/it]11/15/2022 19:45:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:40 - INFO - train.train_snli_ve - loss is tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5722/16548 [2:39:21<5:04:52,  1.69s/it]11/15/2022 19:45:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.4473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:42 - INFO - train.train_snli_ve - loss is tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5723/16548 [2:39:22<5:05:17,  1.69s/it]11/15/2022 19:45:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0251e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:43 - INFO - train.train_snli_ve - loss is tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5724/16548 [2:39:24<5:03:47,  1.68s/it]11/15/2022 19:45:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.7096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:45 - INFO - train.train_snli_ve - loss is tensor(0.7689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5725/16548 [2:39:26<5:05:28,  1.69s/it]11/15/2022 19:45:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:47 - INFO - train.train_snli_ve - loss is tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5726/16548 [2:39:27<5:04:29,  1.69s/it]11/15/2022 19:45:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:48 - INFO - train.train_snli_ve - loss is tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5727/16548 [2:39:29<5:02:03,  1.67s/it]11/15/2022 19:45:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:50 - INFO - train.train_snli_ve - loss is tensor(0.8962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5728/16548 [2:39:31<5:03:32,  1.68s/it]11/15/2022 19:45:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:52 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5729/16548 [2:39:32<5:04:10,  1.69s/it]11/15/2022 19:45:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.8303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:53 - INFO - train.train_snli_ve - loss is tensor(0.9159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5730/16548 [2:39:34<5:03:17,  1.68s/it]11/15/2022 19:45:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:55 - INFO - train.train_snli_ve - loss is tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5731/16548 [2:39:36<5:02:55,  1.68s/it]11/15/2022 19:45:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:57 - INFO - train.train_snli_ve - loss is tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5732/16548 [2:39:38<5:04:38,  1.69s/it]11/15/2022 19:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.4949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:45:59 - INFO - train.train_snli_ve - loss is tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5733/16548 [2:39:39<5:04:40,  1.69s/it]11/15/2022 19:46:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:00 - INFO - train.train_snli_ve - loss is tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5734/16548 [2:39:41<5:03:29,  1.68s/it]11/15/2022 19:46:02 - INFO - train.train_snli_ve - kd_loss is tensor(8.2717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:02 - INFO - train.train_snli_ve - loss is tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5735/16548 [2:39:43<5:01:55,  1.68s/it]11/15/2022 19:46:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.4065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:03 - INFO - train.train_snli_ve - loss is tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5736/16548 [2:39:44<5:00:04,  1.67s/it]11/15/2022 19:46:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0446e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:05 - INFO - train.train_snli_ve - loss is tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5737/16548 [2:39:46<5:01:26,  1.67s/it]11/15/2022 19:46:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.9462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:07 - INFO - train.train_snli_ve - loss is tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5738/16548 [2:39:48<5:02:24,  1.68s/it]11/15/2022 19:46:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.1268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:09 - INFO - train.train_snli_ve - loss is tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5739/16548 [2:39:49<5:02:45,  1.68s/it]11/15/2022 19:46:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0281e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:10 - INFO - train.train_snli_ve - loss is tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5740/16548 [2:39:51<5:04:00,  1.69s/it]11/15/2022 19:46:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.5738e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:12 - INFO - train.train_snli_ve - loss is tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5741/16548 [2:39:53<5:03:23,  1.68s/it]11/15/2022 19:46:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.4505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:14 - INFO - train.train_snli_ve - loss is tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5742/16548 [2:39:54<5:03:01,  1.68s/it]11/15/2022 19:46:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.9964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:15 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5743/16548 [2:39:56<5:02:38,  1.68s/it]11/15/2022 19:46:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:17 - INFO - train.train_snli_ve - loss is tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5744/16548 [2:39:58<4:59:59,  1.67s/it]11/15/2022 19:46:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:19 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5745/16548 [2:39:59<4:59:22,  1.66s/it]11/15/2022 19:46:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.9819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:20 - INFO - train.train_snli_ve - loss is tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5746/16548 [2:40:01<5:04:04,  1.69s/it]11/15/2022 19:46:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:22 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5747/16548 [2:40:03<5:04:45,  1.69s/it]11/15/2022 19:46:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.5133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:24 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5748/16548 [2:40:04<5:03:01,  1.68s/it]11/15/2022 19:46:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:25 - INFO - train.train_snli_ve - loss is tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5749/16548 [2:40:06<5:03:38,  1.69s/it]11/15/2022 19:46:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.5599e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:27 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5750/16548 [2:40:08<5:00:54,  1.67s/it]11/15/2022 19:46:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1052e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:29 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5751/16548 [2:40:09<5:03:56,  1.69s/it]11/15/2022 19:46:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.4859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:30 - INFO - train.train_snli_ve - loss is tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5752/16548 [2:40:11<5:02:42,  1.68s/it]11/15/2022 19:46:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.0828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:32 - INFO - train.train_snli_ve - loss is tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5753/16548 [2:40:13<5:03:47,  1.69s/it]11/15/2022 19:46:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:34 - INFO - train.train_snli_ve - loss is tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5754/16548 [2:40:15<5:02:20,  1.68s/it]11/15/2022 19:46:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:35 - INFO - train.train_snli_ve - loss is tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5755/16548 [2:40:16<4:59:52,  1.67s/it]11/15/2022 19:46:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.1562e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:37 - INFO - train.train_snli_ve - loss is tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5756/16548 [2:40:18<4:57:59,  1.66s/it]11/15/2022 19:46:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.3616e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:39 - INFO - train.train_snli_ve - loss is tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5757/16548 [2:40:19<4:59:07,  1.66s/it]11/15/2022 19:46:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.4109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:40 - INFO - train.train_snli_ve - loss is tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5758/16548 [2:40:21<5:03:10,  1.69s/it]11/15/2022 19:46:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.1492e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:42 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5759/16548 [2:40:23<5:02:12,  1.68s/it]11/15/2022 19:46:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:44 - INFO - train.train_snli_ve - loss is tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5760/16548 [2:40:25<5:01:25,  1.68s/it]11/15/2022 19:46:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4916e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:45 - INFO - train.train_snli_ve - loss is tensor(0.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5761/16548 [2:40:26<5:01:44,  1.68s/it]11/15/2022 19:46:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:47 - INFO - train.train_snli_ve - loss is tensor(0.7968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5762/16548 [2:40:28<5:00:22,  1.67s/it]11/15/2022 19:46:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:49 - INFO - train.train_snli_ve - loss is tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5763/16548 [2:40:30<5:04:58,  1.70s/it]11/15/2022 19:46:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:51 - INFO - train.train_snli_ve - loss is tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5764/16548 [2:40:31<5:05:19,  1.70s/it]11/15/2022 19:46:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:52 - INFO - train.train_snli_ve - loss is tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5765/16548 [2:40:33<5:03:53,  1.69s/it]11/15/2022 19:46:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:54 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5766/16548 [2:40:35<5:04:09,  1.69s/it]11/15/2022 19:46:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:56 - INFO - train.train_snli_ve - loss is tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5767/16548 [2:40:36<5:02:03,  1.68s/it]11/15/2022 19:46:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:57 - INFO - train.train_snli_ve - loss is tensor(0.4808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5768/16548 [2:40:38<5:02:32,  1.68s/it]11/15/2022 19:46:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.3131e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:46:59 - INFO - train.train_snli_ve - loss is tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5769/16548 [2:40:40<5:02:54,  1.69s/it]11/15/2022 19:47:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.4891e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:01 - INFO - train.train_snli_ve - loss is tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5770/16548 [2:40:41<5:01:40,  1.68s/it]11/15/2022 19:47:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:02 - INFO - train.train_snli_ve - loss is tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5771/16548 [2:40:43<5:01:26,  1.68s/it]11/15/2022 19:47:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1097e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:04 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5772/16548 [2:40:45<5:03:28,  1.69s/it]11/15/2022 19:47:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.6191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:06 - INFO - train.train_snli_ve - loss is tensor(0.8187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5773/16548 [2:40:46<5:03:07,  1.69s/it]11/15/2022 19:47:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.3448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:07 - INFO - train.train_snli_ve - loss is tensor(0.7691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5774/16548 [2:40:48<5:04:10,  1.69s/it]11/15/2022 19:47:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.0855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:09 - INFO - train.train_snli_ve - loss is tensor(0.9681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5775/16548 [2:40:50<5:04:08,  1.69s/it]11/15/2022 19:47:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:11 - INFO - train.train_snli_ve - loss is tensor(0.4745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5776/16548 [2:40:52<5:05:53,  1.70s/it]11/15/2022 19:47:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:13 - INFO - train.train_snli_ve - loss is tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5777/16548 [2:40:53<5:03:38,  1.69s/it]11/15/2022 19:47:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.4534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:14 - INFO - train.train_snli_ve - loss is tensor(0.9000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5778/16548 [2:40:55<5:01:47,  1.68s/it]11/15/2022 19:47:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:16 - INFO - train.train_snli_ve - loss is tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5779/16548 [2:40:57<5:00:31,  1.67s/it]11/15/2022 19:47:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.2986e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:17 - INFO - train.train_snli_ve - loss is tensor(0.9504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5780/16548 [2:40:58<4:58:06,  1.66s/it]11/15/2022 19:47:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:19 - INFO - train.train_snli_ve - loss is tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5781/16548 [2:41:00<5:00:46,  1.68s/it]11/15/2022 19:47:21 - INFO - train.train_snli_ve - kd_loss is tensor(9.3866e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:21 - INFO - train.train_snli_ve - loss is tensor(0.8507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5782/16548 [2:41:02<4:59:53,  1.67s/it]11/15/2022 19:47:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.5842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:23 - INFO - train.train_snli_ve - loss is tensor(0.9601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5783/16548 [2:41:03<4:59:57,  1.67s/it]11/15/2022 19:47:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.3145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:24 - INFO - train.train_snli_ve - loss is tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5784/16548 [2:41:05<4:58:58,  1.67s/it]11/15/2022 19:47:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.3556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:26 - INFO - train.train_snli_ve - loss is tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5785/16548 [2:41:07<5:01:37,  1.68s/it]11/15/2022 19:47:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:28 - INFO - train.train_snli_ve - loss is tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5786/16548 [2:41:08<5:00:48,  1.68s/it]11/15/2022 19:47:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.6509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:29 - INFO - train.train_snli_ve - loss is tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5787/16548 [2:41:10<5:00:27,  1.68s/it]11/15/2022 19:47:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:31 - INFO - train.train_snli_ve - loss is tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5788/16548 [2:41:12<5:00:18,  1.67s/it]11/15/2022 19:47:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.8416e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:33 - INFO - train.train_snli_ve - loss is tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5789/16548 [2:41:13<4:58:39,  1.67s/it]11/15/2022 19:47:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.1965e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:34 - INFO - train.train_snli_ve - loss is tensor(0.7967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5790/16548 [2:41:15<4:58:19,  1.66s/it]11/15/2022 19:47:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:36 - INFO - train.train_snli_ve - loss is tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5791/16548 [2:41:17<4:58:29,  1.66s/it]11/15/2022 19:47:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.5308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:38 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5792/16548 [2:41:18<4:58:44,  1.67s/it]11/15/2022 19:47:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.8750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:39 - INFO - train.train_snli_ve - loss is tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5793/16548 [2:41:20<4:58:27,  1.67s/it]11/15/2022 19:47:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.0146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:41 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5794/16548 [2:41:22<4:57:59,  1.66s/it]11/15/2022 19:47:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.5487e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:42 - INFO - train.train_snli_ve - loss is tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5795/16548 [2:41:23<4:55:51,  1.65s/it]11/15/2022 19:47:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:44 - INFO - train.train_snli_ve - loss is tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5796/16548 [2:41:25<4:54:44,  1.64s/it]11/15/2022 19:47:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.2615e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:46 - INFO - train.train_snli_ve - loss is tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5797/16548 [2:41:27<4:57:27,  1.66s/it]11/15/2022 19:47:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.6600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:48 - INFO - train.train_snli_ve - loss is tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5798/16548 [2:41:28<5:01:27,  1.68s/it]11/15/2022 19:47:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.8713e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:49 - INFO - train.train_snli_ve - loss is tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5799/16548 [2:41:30<4:59:00,  1.67s/it]11/15/2022 19:47:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.1996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:51 - INFO - train.train_snli_ve - loss is tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5800/16548 [2:41:32<5:01:09,  1.68s/it]11/15/2022 19:47:53 - INFO - train.train_snli_ve - kd_loss is tensor(9.9685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:53 - INFO - train.train_snli_ve - loss is tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5801/16548 [2:41:33<5:02:45,  1.69s/it]11/15/2022 19:47:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:54 - INFO - train.train_snli_ve - loss is tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5802/16548 [2:41:35<5:01:54,  1.69s/it]11/15/2022 19:47:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.3467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:56 - INFO - train.train_snli_ve - loss is tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5803/16548 [2:41:37<5:02:00,  1.69s/it]11/15/2022 19:47:58 - INFO - train.train_snli_ve - kd_loss is tensor(7.1369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:58 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5804/16548 [2:41:38<5:01:18,  1.68s/it]11/15/2022 19:47:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:47:59 - INFO - train.train_snli_ve - loss is tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5805/16548 [2:41:40<5:03:25,  1.69s/it]11/15/2022 19:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.8424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:01 - INFO - train.train_snli_ve - loss is tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5806/16548 [2:41:42<5:02:42,  1.69s/it]11/15/2022 19:48:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:03 - INFO - train.train_snli_ve - loss is tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5807/16548 [2:41:43<5:00:37,  1.68s/it]11/15/2022 19:48:04 - INFO - train.train_snli_ve - kd_loss is tensor(8.9809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:04 - INFO - train.train_snli_ve - loss is tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5808/16548 [2:41:45<5:00:45,  1.68s/it]11/15/2022 19:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.2903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:06 - INFO - train.train_snli_ve - loss is tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5809/16548 [2:41:47<4:59:33,  1.67s/it]11/15/2022 19:48:08 - INFO - train.train_snli_ve - kd_loss is tensor(8.1530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:08 - INFO - train.train_snli_ve - loss is tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5810/16548 [2:41:48<4:58:24,  1.67s/it]11/15/2022 19:48:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3041e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:09 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5811/16548 [2:41:50<4:57:44,  1.66s/it]11/15/2022 19:48:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.5534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:11 - INFO - train.train_snli_ve - loss is tensor(0.9476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5812/16548 [2:41:52<4:59:38,  1.67s/it]11/15/2022 19:48:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.5051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:13 - INFO - train.train_snli_ve - loss is tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5813/16548 [2:41:53<4:58:38,  1.67s/it]11/15/2022 19:48:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.3690e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:14 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5814/16548 [2:41:55<4:58:45,  1.67s/it]11/15/2022 19:48:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.6829e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:16 - INFO - train.train_snli_ve - loss is tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5815/16548 [2:41:57<4:57:21,  1.66s/it]11/15/2022 19:48:18 - INFO - train.train_snli_ve - kd_loss is tensor(7.2673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:18 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5816/16548 [2:41:58<4:58:23,  1.67s/it]11/15/2022 19:48:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:19 - INFO - train.train_snli_ve - loss is tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5817/16548 [2:42:00<4:55:41,  1.65s/it]11/15/2022 19:48:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:21 - INFO - train.train_snli_ve - loss is tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5818/16548 [2:42:02<4:56:04,  1.66s/it]11/15/2022 19:48:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.5072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:23 - INFO - train.train_snli_ve - loss is tensor(0.7286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5819/16548 [2:42:03<4:55:16,  1.65s/it]11/15/2022 19:48:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.3348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:24 - INFO - train.train_snli_ve - loss is tensor(0.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5820/16548 [2:42:05<4:56:14,  1.66s/it]11/15/2022 19:48:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.5779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:26 - INFO - train.train_snli_ve - loss is tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5821/16548 [2:42:07<4:56:32,  1.66s/it]11/15/2022 19:48:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.5989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:28 - INFO - train.train_snli_ve - loss is tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5822/16548 [2:42:08<4:56:22,  1.66s/it]11/15/2022 19:48:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.0343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:29 - INFO - train.train_snli_ve - loss is tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5823/16548 [2:42:10<4:58:03,  1.67s/it]11/15/2022 19:48:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.0697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:31 - INFO - train.train_snli_ve - loss is tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5824/16548 [2:42:12<4:57:12,  1.66s/it]11/15/2022 19:48:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.0405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:33 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5825/16548 [2:42:13<4:55:31,  1.65s/it]11/15/2022 19:48:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:34 - INFO - train.train_snli_ve - loss is tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5826/16548 [2:42:15<4:59:27,  1.68s/it]11/15/2022 19:48:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:36 - INFO - train.train_snli_ve - loss is tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5827/16548 [2:42:17<4:57:02,  1.66s/it]11/15/2022 19:48:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.5413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:38 - INFO - train.train_snli_ve - loss is tensor(0.4672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5828/16548 [2:42:18<4:56:49,  1.66s/it]11/15/2022 19:48:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:39 - INFO - train.train_snli_ve - loss is tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5829/16548 [2:42:20<4:57:53,  1.67s/it]11/15/2022 19:48:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.6613e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:41 - INFO - train.train_snli_ve - loss is tensor(0.4507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5830/16548 [2:42:22<4:56:46,  1.66s/it]11/15/2022 19:48:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:43 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5831/16548 [2:42:23<4:56:01,  1.66s/it]11/15/2022 19:48:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.1961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:44 - INFO - train.train_snli_ve - loss is tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5832/16548 [2:42:25<4:54:54,  1.65s/it]11/15/2022 19:48:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.5081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:46 - INFO - train.train_snli_ve - loss is tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5833/16548 [2:42:27<4:54:36,  1.65s/it]11/15/2022 19:48:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.8525e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:47 - INFO - train.train_snli_ve - loss is tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5834/16548 [2:42:28<4:53:15,  1.64s/it]11/15/2022 19:48:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:49 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5835/16548 [2:42:30<4:56:45,  1.66s/it]11/15/2022 19:48:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:51 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5836/16548 [2:42:32<4:58:00,  1.67s/it]11/15/2022 19:48:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1072e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:52 - INFO - train.train_snli_ve - loss is tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5837/16548 [2:42:33<4:54:51,  1.65s/it]11/15/2022 19:48:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:54 - INFO - train.train_snli_ve - loss is tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5838/16548 [2:42:35<4:59:48,  1.68s/it]11/15/2022 19:48:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4346e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:56 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5839/16548 [2:42:37<5:00:05,  1.68s/it]11/15/2022 19:48:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:58 - INFO - train.train_snli_ve - loss is tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5840/16548 [2:42:38<4:58:22,  1.67s/it]11/15/2022 19:48:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:48:59 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5841/16548 [2:42:40<4:57:24,  1.67s/it]11/15/2022 19:49:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8324e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:01 - INFO - train.train_snli_ve - loss is tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5842/16548 [2:42:42<4:57:50,  1.67s/it]11/15/2022 19:49:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.3015e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:03 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5843/16548 [2:42:43<4:55:05,  1.65s/it]11/15/2022 19:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:04 - INFO - train.train_snli_ve - loss is tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5844/16548 [2:42:45<4:58:05,  1.67s/it]11/15/2022 19:49:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.1097e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:06 - INFO - train.train_snli_ve - loss is tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5845/16548 [2:42:47<4:58:45,  1.67s/it]11/15/2022 19:49:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:08 - INFO - train.train_snli_ve - loss is tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5846/16548 [2:42:48<4:58:14,  1.67s/it]11/15/2022 19:49:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:09 - INFO - train.train_snli_ve - loss is tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5847/16548 [2:42:50<5:00:07,  1.68s/it]11/15/2022 19:49:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:11 - INFO - train.train_snli_ve - loss is tensor(0.5880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5848/16548 [2:42:52<5:01:01,  1.69s/it]11/15/2022 19:49:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:13 - INFO - train.train_snli_ve - loss is tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5849/16548 [2:42:53<4:59:10,  1.68s/it]11/15/2022 19:49:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:14 - INFO - train.train_snli_ve - loss is tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5850/16548 [2:42:55<4:58:38,  1.67s/it]11/15/2022 19:49:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.2706e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:16 - INFO - train.train_snli_ve - loss is tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5851/16548 [2:42:57<4:57:19,  1.67s/it]11/15/2022 19:49:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:18 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5852/16548 [2:42:58<4:56:19,  1.66s/it]11/15/2022 19:49:19 - INFO - train.train_snli_ve - kd_loss is tensor(8.7772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:19 - INFO - train.train_snli_ve - loss is tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5853/16548 [2:43:00<4:58:42,  1.68s/it]11/15/2022 19:49:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.3789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:21 - INFO - train.train_snli_ve - loss is tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5854/16548 [2:43:02<4:58:01,  1.67s/it]11/15/2022 19:49:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:23 - INFO - train.train_snli_ve - loss is tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5855/16548 [2:43:03<4:57:27,  1.67s/it]11/15/2022 19:49:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.7682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:24 - INFO - train.train_snli_ve - loss is tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5856/16548 [2:43:05<5:00:11,  1.68s/it]11/15/2022 19:49:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4547e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:26 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5857/16548 [2:43:07<5:00:36,  1.69s/it]11/15/2022 19:49:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:28 - INFO - train.train_snli_ve - loss is tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5858/16548 [2:43:08<4:59:27,  1.68s/it]11/15/2022 19:49:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:29 - INFO - train.train_snli_ve - loss is tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5859/16548 [2:43:10<4:57:18,  1.67s/it]11/15/2022 19:49:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:31 - INFO - train.train_snli_ve - loss is tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5860/16548 [2:43:12<4:58:20,  1.67s/it]11/15/2022 19:49:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5793e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:33 - INFO - train.train_snli_ve - loss is tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5861/16548 [2:43:14<5:00:50,  1.69s/it]11/15/2022 19:49:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.6496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:34 - INFO - train.train_snli_ve - loss is tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5862/16548 [2:43:15<5:00:58,  1.69s/it]11/15/2022 19:49:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:36 - INFO - train.train_snli_ve - loss is tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5863/16548 [2:43:17<4:59:57,  1.68s/it]11/15/2022 19:49:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:38 - INFO - train.train_snli_ve - loss is tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5864/16548 [2:43:19<4:58:22,  1.68s/it]11/15/2022 19:49:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0213e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:39 - INFO - train.train_snli_ve - loss is tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5865/16548 [2:43:20<4:59:45,  1.68s/it]11/15/2022 19:49:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.3319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:41 - INFO - train.train_snli_ve - loss is tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5866/16548 [2:43:22<5:02:31,  1.70s/it]11/15/2022 19:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:43 - INFO - train.train_snli_ve - loss is tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5867/16548 [2:43:24<5:02:02,  1.70s/it]11/15/2022 19:49:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:45 - INFO - train.train_snli_ve - loss is tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5868/16548 [2:43:25<4:59:37,  1.68s/it]11/15/2022 19:49:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:46 - INFO - train.train_snli_ve - loss is tensor(0.8411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5869/16548 [2:43:27<4:58:28,  1.68s/it]11/15/2022 19:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:48 - INFO - train.train_snli_ve - loss is tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5870/16548 [2:43:29<5:00:49,  1.69s/it]11/15/2022 19:49:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:50 - INFO - train.train_snli_ve - loss is tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5871/16548 [2:43:30<5:00:13,  1.69s/it]11/15/2022 19:49:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3034e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:51 - INFO - train.train_snli_ve - loss is tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5872/16548 [2:43:32<4:59:21,  1.68s/it]11/15/2022 19:49:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:53 - INFO - train.train_snli_ve - loss is tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5873/16548 [2:43:34<5:00:11,  1.69s/it]11/15/2022 19:49:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:55 - INFO - train.train_snli_ve - loss is tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35% 5874/16548 [2:43:35<4:57:26,  1.67s/it]11/15/2022 19:49:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.3678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:56 - INFO - train.train_snli_ve - loss is tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5875/16548 [2:43:37<4:58:47,  1.68s/it]11/15/2022 19:49:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:49:58 - INFO - train.train_snli_ve - loss is tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5876/16548 [2:43:39<4:57:46,  1.67s/it]11/15/2022 19:50:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:00 - INFO - train.train_snli_ve - loss is tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5877/16548 [2:43:40<5:00:37,  1.69s/it]11/15/2022 19:50:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.3546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:01 - INFO - train.train_snli_ve - loss is tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5878/16548 [2:43:42<5:00:53,  1.69s/it]11/15/2022 19:50:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.4900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:03 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5879/16548 [2:43:44<5:03:17,  1.71s/it]11/15/2022 19:50:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2281e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:05 - INFO - train.train_snli_ve - loss is tensor(0.9247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5880/16548 [2:43:46<5:03:30,  1.71s/it]11/15/2022 19:50:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:07 - INFO - train.train_snli_ve - loss is tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5881/16548 [2:43:47<5:06:24,  1.72s/it]11/15/2022 19:50:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.4599e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:08 - INFO - train.train_snli_ve - loss is tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5882/16548 [2:43:49<5:03:35,  1.71s/it]11/15/2022 19:50:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:10 - INFO - train.train_snli_ve - loss is tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5883/16548 [2:43:51<5:01:10,  1.69s/it]11/15/2022 19:50:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:12 - INFO - train.train_snli_ve - loss is tensor(0.8852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5884/16548 [2:43:52<5:00:23,  1.69s/it]11/15/2022 19:50:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:13 - INFO - train.train_snli_ve - loss is tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5885/16548 [2:43:54<4:59:37,  1.69s/it]11/15/2022 19:50:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.2403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:15 - INFO - train.train_snli_ve - loss is tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5886/16548 [2:43:56<4:57:42,  1.68s/it]11/15/2022 19:50:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:17 - INFO - train.train_snli_ve - loss is tensor(1.0032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5887/16548 [2:43:57<4:57:56,  1.68s/it]11/15/2022 19:50:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:18 - INFO - train.train_snli_ve - loss is tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5888/16548 [2:43:59<4:59:56,  1.69s/it]11/15/2022 19:50:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.4547e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:20 - INFO - train.train_snli_ve - loss is tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5889/16548 [2:44:01<5:01:44,  1.70s/it]11/15/2022 19:50:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:22 - INFO - train.train_snli_ve - loss is tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5890/16548 [2:44:02<4:59:54,  1.69s/it]11/15/2022 19:50:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:23 - INFO - train.train_snli_ve - loss is tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5891/16548 [2:44:04<4:56:58,  1.67s/it]11/15/2022 19:50:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:25 - INFO - train.train_snli_ve - loss is tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5892/16548 [2:44:06<4:56:36,  1.67s/it]11/15/2022 19:50:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:27 - INFO - train.train_snli_ve - loss is tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5893/16548 [2:44:07<4:54:57,  1.66s/it]11/15/2022 19:50:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.4813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:28 - INFO - train.train_snli_ve - loss is tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5894/16548 [2:44:09<4:54:41,  1.66s/it]11/15/2022 19:50:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.8607e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:30 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5895/16548 [2:44:11<4:52:57,  1.65s/it]11/15/2022 19:50:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.3654e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:32 - INFO - train.train_snli_ve - loss is tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5896/16548 [2:44:12<4:56:48,  1.67s/it]11/15/2022 19:50:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.2095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:33 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5897/16548 [2:44:14<4:56:44,  1.67s/it]11/15/2022 19:50:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.4254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:35 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5898/16548 [2:44:16<4:56:30,  1.67s/it]11/15/2022 19:50:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.0470e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:37 - INFO - train.train_snli_ve - loss is tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5899/16548 [2:44:17<4:58:16,  1.68s/it]11/15/2022 19:50:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2295e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:38 - INFO - train.train_snli_ve - loss is tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5900/16548 [2:44:19<5:05:29,  1.72s/it]11/15/2022 19:50:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.0187e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:40 - INFO - train.train_snli_ve - loss is tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5901/16548 [2:44:21<5:02:54,  1.71s/it]11/15/2022 19:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:42 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5902/16548 [2:44:23<4:59:22,  1.69s/it]11/15/2022 19:50:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:44 - INFO - train.train_snli_ve - loss is tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5903/16548 [2:44:24<4:59:13,  1.69s/it]11/15/2022 19:50:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:45 - INFO - train.train_snli_ve - loss is tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5904/16548 [2:44:26<5:02:21,  1.70s/it]11/15/2022 19:50:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:47 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5905/16548 [2:44:28<5:00:06,  1.69s/it]11/15/2022 19:50:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:49 - INFO - train.train_snli_ve - loss is tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5906/16548 [2:44:29<4:57:07,  1.68s/it]11/15/2022 19:50:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:50 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5907/16548 [2:44:31<4:55:02,  1.66s/it]11/15/2022 19:50:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:52 - INFO - train.train_snli_ve - loss is tensor(0.7104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5908/16548 [2:44:33<4:52:58,  1.65s/it]11/15/2022 19:50:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:54 - INFO - train.train_snli_ve - loss is tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5909/16548 [2:44:34<4:54:33,  1.66s/it]11/15/2022 19:50:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.2584e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:55 - INFO - train.train_snli_ve - loss is tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5910/16548 [2:44:36<4:58:28,  1.68s/it]11/15/2022 19:50:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:57 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5911/16548 [2:44:38<4:58:46,  1.69s/it]11/15/2022 19:50:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.6067e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:50:59 - INFO - train.train_snli_ve - loss is tensor(0.5801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5912/16548 [2:44:39<4:57:53,  1.68s/it]11/15/2022 19:51:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.4824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:00 - INFO - train.train_snli_ve - loss is tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5913/16548 [2:44:41<5:01:11,  1.70s/it]11/15/2022 19:51:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:02 - INFO - train.train_snli_ve - loss is tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5914/16548 [2:44:43<4:59:28,  1.69s/it]11/15/2022 19:51:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:04 - INFO - train.train_snli_ve - loss is tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5915/16548 [2:44:45<5:00:38,  1.70s/it]11/15/2022 19:51:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:05 - INFO - train.train_snli_ve - loss is tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5916/16548 [2:44:46<4:59:22,  1.69s/it]11/15/2022 19:51:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:07 - INFO - train.train_snli_ve - loss is tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5917/16548 [2:44:48<4:57:32,  1.68s/it]11/15/2022 19:51:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:09 - INFO - train.train_snli_ve - loss is tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5918/16548 [2:44:50<5:00:14,  1.69s/it]11/15/2022 19:51:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.8109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:11 - INFO - train.train_snli_ve - loss is tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5919/16548 [2:44:51<4:59:13,  1.69s/it]11/15/2022 19:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0067e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:12 - INFO - train.train_snli_ve - loss is tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5920/16548 [2:44:53<4:58:41,  1.69s/it]11/15/2022 19:51:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:14 - INFO - train.train_snli_ve - loss is tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5921/16548 [2:44:55<4:58:48,  1.69s/it]11/15/2022 19:51:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:16 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5922/16548 [2:44:56<4:58:14,  1.68s/it]11/15/2022 19:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:17 - INFO - train.train_snli_ve - loss is tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5923/16548 [2:44:58<4:56:29,  1.67s/it]11/15/2022 19:51:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:19 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5924/16548 [2:45:00<4:57:25,  1.68s/it]11/15/2022 19:51:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.6172e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:21 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5925/16548 [2:45:01<4:58:59,  1.69s/it]11/15/2022 19:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:22 - INFO - train.train_snli_ve - loss is tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5926/16548 [2:45:03<5:02:13,  1.71s/it]11/15/2022 19:51:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:24 - INFO - train.train_snli_ve - loss is tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5927/16548 [2:45:05<5:01:49,  1.71s/it]11/15/2022 19:51:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:26 - INFO - train.train_snli_ve - loss is tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5928/16548 [2:45:06<5:00:47,  1.70s/it]11/15/2022 19:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:27 - INFO - train.train_snli_ve - loss is tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5929/16548 [2:45:08<5:00:22,  1.70s/it]11/15/2022 19:51:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:29 - INFO - train.train_snli_ve - loss is tensor(0.4599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5930/16548 [2:45:10<5:01:16,  1.70s/it]11/15/2022 19:51:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:31 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5931/16548 [2:45:12<4:57:53,  1.68s/it]11/15/2022 19:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:32 - INFO - train.train_snli_ve - loss is tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5932/16548 [2:45:13<4:57:49,  1.68s/it]11/15/2022 19:51:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.3887e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:34 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5933/16548 [2:45:15<4:58:35,  1.69s/it]11/15/2022 19:51:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.7514e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:36 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5934/16548 [2:45:17<4:57:21,  1.68s/it]11/15/2022 19:51:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:38 - INFO - train.train_snli_ve - loss is tensor(0.4648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5935/16548 [2:45:18<5:00:32,  1.70s/it]11/15/2022 19:51:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:39 - INFO - train.train_snli_ve - loss is tensor(1.0076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5936/16548 [2:45:20<5:01:53,  1.71s/it]11/15/2022 19:51:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2708e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:41 - INFO - train.train_snli_ve - loss is tensor(0.5404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5937/16548 [2:45:22<4:58:31,  1.69s/it]11/15/2022 19:51:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.6737e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:43 - INFO - train.train_snli_ve - loss is tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5938/16548 [2:45:23<4:59:46,  1.70s/it]11/15/2022 19:51:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.3196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:44 - INFO - train.train_snli_ve - loss is tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5939/16548 [2:45:25<4:59:23,  1.69s/it]11/15/2022 19:51:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.8905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:46 - INFO - train.train_snli_ve - loss is tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5940/16548 [2:45:27<4:58:29,  1.69s/it]11/15/2022 19:51:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:48 - INFO - train.train_snli_ve - loss is tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5941/16548 [2:45:28<4:56:33,  1.68s/it]11/15/2022 19:51:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.4218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:49 - INFO - train.train_snli_ve - loss is tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5942/16548 [2:45:30<4:58:40,  1.69s/it]11/15/2022 19:51:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:51 - INFO - train.train_snli_ve - loss is tensor(0.9127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5943/16548 [2:45:32<4:57:21,  1.68s/it]11/15/2022 19:51:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:53 - INFO - train.train_snli_ve - loss is tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5944/16548 [2:45:33<4:58:11,  1.69s/it]11/15/2022 19:51:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.4314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:54 - INFO - train.train_snli_ve - loss is tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5945/16548 [2:45:35<4:58:57,  1.69s/it]11/15/2022 19:51:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.1937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:56 - INFO - train.train_snli_ve - loss is tensor(0.9689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5946/16548 [2:45:37<4:58:33,  1.69s/it]11/15/2022 19:51:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:58 - INFO - train.train_snli_ve - loss is tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5947/16548 [2:45:39<4:59:29,  1.70s/it]11/15/2022 19:51:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:51:59 - INFO - train.train_snli_ve - loss is tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5948/16548 [2:45:40<4:56:47,  1.68s/it]11/15/2022 19:52:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1415e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:01 - INFO - train.train_snli_ve - loss is tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5949/16548 [2:45:42<4:55:30,  1.67s/it]11/15/2022 19:52:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1728e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:03 - INFO - train.train_snli_ve - loss is tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5950/16548 [2:45:44<4:56:37,  1.68s/it]11/15/2022 19:52:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:05 - INFO - train.train_snli_ve - loss is tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5951/16548 [2:45:45<4:57:18,  1.68s/it]11/15/2022 19:52:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:06 - INFO - train.train_snli_ve - loss is tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5952/16548 [2:45:47<4:56:23,  1.68s/it]11/15/2022 19:52:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.0244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:08 - INFO - train.train_snli_ve - loss is tensor(0.8349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5953/16548 [2:45:49<4:56:10,  1.68s/it]11/15/2022 19:52:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.5320e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:10 - INFO - train.train_snli_ve - loss is tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5954/16548 [2:45:50<4:56:49,  1.68s/it]11/15/2022 19:52:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.2005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:11 - INFO - train.train_snli_ve - loss is tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5955/16548 [2:45:52<4:54:41,  1.67s/it]11/15/2022 19:52:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:13 - INFO - train.train_snli_ve - loss is tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5956/16548 [2:45:54<4:56:44,  1.68s/it]11/15/2022 19:52:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:15 - INFO - train.train_snli_ve - loss is tensor(0.5051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5957/16548 [2:45:55<4:59:05,  1.69s/it]11/15/2022 19:52:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.2032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:16 - INFO - train.train_snli_ve - loss is tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5958/16548 [2:45:57<4:56:36,  1.68s/it]11/15/2022 19:52:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.0760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:18 - INFO - train.train_snli_ve - loss is tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5959/16548 [2:45:59<4:53:46,  1.66s/it]11/15/2022 19:52:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:20 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5960/16548 [2:46:00<4:56:13,  1.68s/it]11/15/2022 19:52:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.2507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:21 - INFO - train.train_snli_ve - loss is tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5961/16548 [2:46:02<4:54:55,  1.67s/it]11/15/2022 19:52:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.4323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:23 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5962/16548 [2:46:04<4:54:54,  1.67s/it]11/15/2022 19:52:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.2085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:25 - INFO - train.train_snli_ve - loss is tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5963/16548 [2:46:05<4:55:39,  1.68s/it]11/15/2022 19:52:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:26 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5964/16548 [2:46:07<4:57:24,  1.69s/it]11/15/2022 19:52:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.6041e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:28 - INFO - train.train_snli_ve - loss is tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5965/16548 [2:46:09<4:55:39,  1.68s/it]11/15/2022 19:52:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:30 - INFO - train.train_snli_ve - loss is tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5966/16548 [2:46:10<4:56:18,  1.68s/it]11/15/2022 19:52:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.9414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:31 - INFO - train.train_snli_ve - loss is tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5967/16548 [2:46:12<4:55:32,  1.68s/it]11/15/2022 19:52:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:33 - INFO - train.train_snli_ve - loss is tensor(0.7922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5968/16548 [2:46:14<4:56:46,  1.68s/it]11/15/2022 19:52:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:35 - INFO - train.train_snli_ve - loss is tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5969/16548 [2:46:15<4:56:23,  1.68s/it]11/15/2022 19:52:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:36 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5970/16548 [2:46:17<4:53:54,  1.67s/it]11/15/2022 19:52:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.1151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:38 - INFO - train.train_snli_ve - loss is tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5971/16548 [2:46:19<4:59:20,  1.70s/it]11/15/2022 19:52:40 - INFO - train.train_snli_ve - kd_loss is tensor(9.9795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:40 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5972/16548 [2:46:21<4:59:35,  1.70s/it]11/15/2022 19:52:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.3714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:42 - INFO - train.train_snli_ve - loss is tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5973/16548 [2:46:22<4:57:32,  1.69s/it]11/15/2022 19:52:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:43 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5974/16548 [2:46:24<4:59:21,  1.70s/it]11/15/2022 19:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:45 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5975/16548 [2:46:26<4:57:08,  1.69s/it]11/15/2022 19:52:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:47 - INFO - train.train_snli_ve - loss is tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5976/16548 [2:46:27<4:56:50,  1.68s/it]11/15/2022 19:52:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:48 - INFO - train.train_snli_ve - loss is tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5977/16548 [2:46:29<4:55:50,  1.68s/it]11/15/2022 19:52:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:50 - INFO - train.train_snli_ve - loss is tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5978/16548 [2:46:31<4:55:01,  1.67s/it]11/15/2022 19:52:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:52 - INFO - train.train_snli_ve - loss is tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5979/16548 [2:46:32<4:54:16,  1.67s/it]11/15/2022 19:52:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:53 - INFO - train.train_snli_ve - loss is tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5980/16548 [2:46:34<4:57:00,  1.69s/it]11/15/2022 19:52:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.4378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:55 - INFO - train.train_snli_ve - loss is tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5981/16548 [2:46:36<4:54:57,  1.67s/it]11/15/2022 19:52:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:57 - INFO - train.train_snli_ve - loss is tensor(0.8950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5982/16548 [2:46:37<4:56:55,  1.69s/it]11/15/2022 19:52:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:52:58 - INFO - train.train_snli_ve - loss is tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5983/16548 [2:46:39<4:59:22,  1.70s/it]11/15/2022 19:53:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:00 - INFO - train.train_snli_ve - loss is tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5984/16548 [2:46:41<4:58:45,  1.70s/it]11/15/2022 19:53:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.4255e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:02 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5985/16548 [2:46:42<4:57:00,  1.69s/it]11/15/2022 19:53:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:03 - INFO - train.train_snli_ve - loss is tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5986/16548 [2:46:44<4:56:19,  1.68s/it]11/15/2022 19:53:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.3466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:05 - INFO - train.train_snli_ve - loss is tensor(0.9038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5987/16548 [2:46:46<4:56:38,  1.69s/it]11/15/2022 19:53:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.4012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:07 - INFO - train.train_snli_ve - loss is tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5988/16548 [2:46:48<4:58:22,  1.70s/it]11/15/2022 19:53:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:08 - INFO - train.train_snli_ve - loss is tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5989/16548 [2:46:49<4:56:35,  1.69s/it]11/15/2022 19:53:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5204e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:10 - INFO - train.train_snli_ve - loss is tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5990/16548 [2:46:51<4:56:26,  1.68s/it]11/15/2022 19:53:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.8704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:12 - INFO - train.train_snli_ve - loss is tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5991/16548 [2:46:53<4:55:35,  1.68s/it]11/15/2022 19:53:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:13 - INFO - train.train_snli_ve - loss is tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5992/16548 [2:46:54<4:54:37,  1.67s/it]11/15/2022 19:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:15 - INFO - train.train_snli_ve - loss is tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5993/16548 [2:46:56<4:55:27,  1.68s/it]11/15/2022 19:53:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:17 - INFO - train.train_snli_ve - loss is tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5994/16548 [2:46:58<4:54:41,  1.68s/it]11/15/2022 19:53:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:18 - INFO - train.train_snli_ve - loss is tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5995/16548 [2:46:59<4:53:29,  1.67s/it]11/15/2022 19:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8555e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:20 - INFO - train.train_snli_ve - loss is tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5996/16548 [2:47:01<4:51:54,  1.66s/it]11/15/2022 19:53:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:22 - INFO - train.train_snli_ve - loss is tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5997/16548 [2:47:03<4:53:23,  1.67s/it]11/15/2022 19:53:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:24 - INFO - train.train_snli_ve - loss is tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5998/16548 [2:47:04<4:54:37,  1.68s/it]11/15/2022 19:53:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.7034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:25 - INFO - train.train_snli_ve - loss is tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 5999/16548 [2:47:06<4:54:27,  1.67s/it]11/15/2022 19:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.0591e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:27 - INFO - train.train_snli_ve - loss is tensor(0.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6000/16548 [2:47:08<5:01:10,  1.71s/it]11/15/2022 19:53:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:29 - INFO - train.train_snli_ve - loss is tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6001/16548 [2:47:09<4:57:34,  1.69s/it]11/15/2022 19:53:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.1710e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:30 - INFO - train.train_snli_ve - loss is tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6002/16548 [2:47:11<4:57:00,  1.69s/it]11/15/2022 19:53:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4036e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:32 - INFO - train.train_snli_ve - loss is tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6003/16548 [2:47:13<4:55:56,  1.68s/it]11/15/2022 19:53:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:34 - INFO - train.train_snli_ve - loss is tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6004/16548 [2:47:14<4:53:19,  1.67s/it]11/15/2022 19:53:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:35 - INFO - train.train_snli_ve - loss is tensor(0.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6005/16548 [2:47:16<4:53:07,  1.67s/it]11/15/2022 19:53:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:37 - INFO - train.train_snli_ve - loss is tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6006/16548 [2:47:18<4:53:57,  1.67s/it]11/15/2022 19:53:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.8859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:39 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6007/16548 [2:47:19<4:52:11,  1.66s/it]11/15/2022 19:53:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:40 - INFO - train.train_snli_ve - loss is tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6008/16548 [2:47:21<4:52:27,  1.66s/it]11/15/2022 19:53:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:42 - INFO - train.train_snli_ve - loss is tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6009/16548 [2:47:23<4:51:14,  1.66s/it]11/15/2022 19:53:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.5055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:44 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6010/16548 [2:47:24<4:51:14,  1.66s/it]11/15/2022 19:53:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:45 - INFO - train.train_snli_ve - loss is tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6011/16548 [2:47:26<4:49:19,  1.65s/it]11/15/2022 19:53:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.2277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:47 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6012/16548 [2:47:28<4:50:32,  1.65s/it]11/15/2022 19:53:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:49 - INFO - train.train_snli_ve - loss is tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6013/16548 [2:47:29<4:52:26,  1.67s/it]11/15/2022 19:53:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:50 - INFO - train.train_snli_ve - loss is tensor(0.8959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6014/16548 [2:47:31<4:53:23,  1.67s/it]11/15/2022 19:53:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.2460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:52 - INFO - train.train_snli_ve - loss is tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6015/16548 [2:47:33<4:53:29,  1.67s/it]11/15/2022 19:53:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:54 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6016/16548 [2:47:34<4:53:12,  1.67s/it]11/15/2022 19:53:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.5692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:55 - INFO - train.train_snli_ve - loss is tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6017/16548 [2:47:36<4:53:41,  1.67s/it]11/15/2022 19:53:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:57 - INFO - train.train_snli_ve - loss is tensor(0.4412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6018/16548 [2:47:38<4:53:54,  1.67s/it]11/15/2022 19:53:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:53:59 - INFO - train.train_snli_ve - loss is tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6019/16548 [2:47:39<4:56:49,  1.69s/it]11/15/2022 19:54:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.7347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:00 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6020/16548 [2:47:41<4:54:54,  1.68s/it]11/15/2022 19:54:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:02 - INFO - train.train_snli_ve - loss is tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6021/16548 [2:47:43<4:53:02,  1.67s/it]11/15/2022 19:54:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.8951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:04 - INFO - train.train_snli_ve - loss is tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6022/16548 [2:47:44<4:52:42,  1.67s/it]11/15/2022 19:54:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:05 - INFO - train.train_snli_ve - loss is tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6023/16548 [2:47:46<4:52:50,  1.67s/it]11/15/2022 19:54:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6342e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:07 - INFO - train.train_snli_ve - loss is tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6024/16548 [2:47:48<4:51:23,  1.66s/it]11/15/2022 19:54:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5874e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:09 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6025/16548 [2:47:49<4:51:43,  1.66s/it]11/15/2022 19:54:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:10 - INFO - train.train_snli_ve - loss is tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6026/16548 [2:47:51<4:50:25,  1.66s/it]11/15/2022 19:54:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:12 - INFO - train.train_snli_ve - loss is tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6027/16548 [2:47:53<4:49:57,  1.65s/it]11/15/2022 19:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:14 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6028/16548 [2:47:54<4:50:06,  1.65s/it]11/15/2022 19:54:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:15 - INFO - train.train_snli_ve - loss is tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6029/16548 [2:47:56<4:49:50,  1.65s/it]11/15/2022 19:54:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:17 - INFO - train.train_snli_ve - loss is tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6030/16548 [2:47:58<4:49:28,  1.65s/it]11/15/2022 19:54:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:19 - INFO - train.train_snli_ve - loss is tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6031/16548 [2:47:59<4:49:37,  1.65s/it]11/15/2022 19:54:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.5180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:20 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6032/16548 [2:48:01<4:51:30,  1.66s/it]11/15/2022 19:54:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.9133e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:22 - INFO - train.train_snli_ve - loss is tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6033/16548 [2:48:03<4:54:19,  1.68s/it]11/15/2022 19:54:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2383e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:24 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6034/16548 [2:48:04<4:52:42,  1.67s/it]11/15/2022 19:54:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.3430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:25 - INFO - train.train_snli_ve - loss is tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6035/16548 [2:48:06<4:51:29,  1.66s/it]11/15/2022 19:54:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.1187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:27 - INFO - train.train_snli_ve - loss is tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6036/16548 [2:48:08<4:51:14,  1.66s/it]11/15/2022 19:54:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2288e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:29 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6037/16548 [2:48:09<4:54:09,  1.68s/it]11/15/2022 19:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:30 - INFO - train.train_snli_ve - loss is tensor(0.5930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6038/16548 [2:48:11<4:53:13,  1.67s/it]11/15/2022 19:54:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:32 - INFO - train.train_snli_ve - loss is tensor(0.4543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6039/16548 [2:48:13<4:51:49,  1.67s/it]11/15/2022 19:54:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:34 - INFO - train.train_snli_ve - loss is tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36% 6040/16548 [2:48:14<4:51:35,  1.66s/it]11/15/2022 19:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:35 - INFO - train.train_snli_ve - loss is tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6041/16548 [2:48:16<4:53:59,  1.68s/it]11/15/2022 19:54:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:37 - INFO - train.train_snli_ve - loss is tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6042/16548 [2:48:18<4:52:26,  1.67s/it]11/15/2022 19:54:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9268e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:39 - INFO - train.train_snli_ve - loss is tensor(0.5131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6043/16548 [2:48:19<4:51:46,  1.67s/it]11/15/2022 19:54:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.3636e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:40 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6044/16548 [2:48:21<4:53:14,  1.68s/it]11/15/2022 19:54:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.2361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:42 - INFO - train.train_snli_ve - loss is tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6045/16548 [2:48:23<4:54:23,  1.68s/it]11/15/2022 19:54:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:44 - INFO - train.train_snli_ve - loss is tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6046/16548 [2:48:24<4:53:37,  1.68s/it]11/15/2022 19:54:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:45 - INFO - train.train_snli_ve - loss is tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6047/16548 [2:48:26<4:52:08,  1.67s/it]11/15/2022 19:54:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.9929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:47 - INFO - train.train_snli_ve - loss is tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6048/16548 [2:48:28<4:54:37,  1.68s/it]11/15/2022 19:54:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:49 - INFO - train.train_snli_ve - loss is tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6049/16548 [2:48:29<4:54:15,  1.68s/it]11/15/2022 19:54:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:50 - INFO - train.train_snli_ve - loss is tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6050/16548 [2:48:31<4:56:55,  1.70s/it]11/15/2022 19:54:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:52 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6051/16548 [2:48:33<4:56:44,  1.70s/it]11/15/2022 19:54:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3026e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:54 - INFO - train.train_snli_ve - loss is tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6052/16548 [2:48:35<4:55:42,  1.69s/it]11/15/2022 19:54:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:55 - INFO - train.train_snli_ve - loss is tensor(0.4315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6053/16548 [2:48:36<4:53:48,  1.68s/it]11/15/2022 19:54:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.2867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:57 - INFO - train.train_snli_ve - loss is tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6054/16548 [2:48:38<4:53:47,  1.68s/it]11/15/2022 19:54:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:54:59 - INFO - train.train_snli_ve - loss is tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6055/16548 [2:48:40<4:52:55,  1.67s/it]11/15/2022 19:55:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:00 - INFO - train.train_snli_ve - loss is tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6056/16548 [2:48:41<4:52:55,  1.68s/it]11/15/2022 19:55:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.1715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:02 - INFO - train.train_snli_ve - loss is tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6057/16548 [2:48:43<4:51:46,  1.67s/it]11/15/2022 19:55:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.2530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:04 - INFO - train.train_snli_ve - loss is tensor(0.8671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6058/16548 [2:48:45<4:50:51,  1.66s/it]11/15/2022 19:55:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:05 - INFO - train.train_snli_ve - loss is tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6059/16548 [2:48:46<4:49:00,  1.65s/it]11/15/2022 19:55:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:07 - INFO - train.train_snli_ve - loss is tensor(0.7613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6060/16548 [2:48:48<4:52:01,  1.67s/it]11/15/2022 19:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:09 - INFO - train.train_snli_ve - loss is tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6061/16548 [2:48:50<4:51:24,  1.67s/it]11/15/2022 19:55:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:10 - INFO - train.train_snli_ve - loss is tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6062/16548 [2:48:51<4:50:21,  1.66s/it]11/15/2022 19:55:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:12 - INFO - train.train_snli_ve - loss is tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6063/16548 [2:48:53<4:50:30,  1.66s/it]11/15/2022 19:55:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1327e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:14 - INFO - train.train_snli_ve - loss is tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6064/16548 [2:48:55<4:50:44,  1.66s/it]11/15/2022 19:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.2042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:15 - INFO - train.train_snli_ve - loss is tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6065/16548 [2:48:56<4:52:44,  1.68s/it]11/15/2022 19:55:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:17 - INFO - train.train_snli_ve - loss is tensor(0.4436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6066/16548 [2:48:58<4:52:34,  1.67s/it]11/15/2022 19:55:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.9914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:19 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6067/16548 [2:49:00<4:50:51,  1.67s/it]11/15/2022 19:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.5150e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:20 - INFO - train.train_snli_ve - loss is tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6068/16548 [2:49:01<4:51:47,  1.67s/it]11/15/2022 19:55:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:22 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6069/16548 [2:49:03<4:49:19,  1.66s/it]11/15/2022 19:55:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:24 - INFO - train.train_snli_ve - loss is tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6070/16548 [2:49:05<4:50:41,  1.66s/it]11/15/2022 19:55:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:25 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6071/16548 [2:49:06<4:51:22,  1.67s/it]11/15/2022 19:55:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.1245e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:27 - INFO - train.train_snli_ve - loss is tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6072/16548 [2:49:08<4:54:07,  1.68s/it]11/15/2022 19:55:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.7343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:29 - INFO - train.train_snli_ve - loss is tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6073/16548 [2:49:10<4:52:32,  1.68s/it]11/15/2022 19:55:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:31 - INFO - train.train_snli_ve - loss is tensor(0.5904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6074/16548 [2:49:11<4:54:36,  1.69s/it]11/15/2022 19:55:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:32 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6075/16548 [2:49:13<4:54:56,  1.69s/it]11/15/2022 19:55:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4988e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:34 - INFO - train.train_snli_ve - loss is tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6076/16548 [2:49:15<4:53:00,  1.68s/it]11/15/2022 19:55:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:36 - INFO - train.train_snli_ve - loss is tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6077/16548 [2:49:16<4:52:37,  1.68s/it]11/15/2022 19:55:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:37 - INFO - train.train_snli_ve - loss is tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6078/16548 [2:49:18<4:53:22,  1.68s/it]11/15/2022 19:55:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:39 - INFO - train.train_snli_ve - loss is tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6079/16548 [2:49:20<4:52:34,  1.68s/it]11/15/2022 19:55:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.1898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:41 - INFO - train.train_snli_ve - loss is tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6080/16548 [2:49:21<4:51:05,  1.67s/it]11/15/2022 19:55:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.4472e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:42 - INFO - train.train_snli_ve - loss is tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6081/16548 [2:49:23<4:51:33,  1.67s/it]11/15/2022 19:55:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:44 - INFO - train.train_snli_ve - loss is tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6082/16548 [2:49:25<4:52:39,  1.68s/it]11/15/2022 19:55:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:46 - INFO - train.train_snli_ve - loss is tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6083/16548 [2:49:26<4:53:16,  1.68s/it]11/15/2022 19:55:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4685e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:47 - INFO - train.train_snli_ve - loss is tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6084/16548 [2:49:28<4:51:58,  1.67s/it]11/15/2022 19:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:49 - INFO - train.train_snli_ve - loss is tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6085/16548 [2:49:30<4:53:58,  1.69s/it]11/15/2022 19:55:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3201e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:51 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6086/16548 [2:49:31<4:52:21,  1.68s/it]11/15/2022 19:55:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.3414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:52 - INFO - train.train_snli_ve - loss is tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6087/16548 [2:49:33<4:52:29,  1.68s/it]11/15/2022 19:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:54 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6088/16548 [2:49:35<4:55:53,  1.70s/it]11/15/2022 19:55:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:56 - INFO - train.train_snli_ve - loss is tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6089/16548 [2:49:36<4:54:12,  1.69s/it]11/15/2022 19:55:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:57 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6090/16548 [2:49:38<4:52:02,  1.68s/it]11/15/2022 19:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2523e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:55:59 - INFO - train.train_snli_ve - loss is tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6091/16548 [2:49:40<4:55:54,  1.70s/it]11/15/2022 19:56:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:01 - INFO - train.train_snli_ve - loss is tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6092/16548 [2:49:42<4:54:16,  1.69s/it]11/15/2022 19:56:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.2989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:02 - INFO - train.train_snli_ve - loss is tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6093/16548 [2:49:43<4:51:35,  1.67s/it]11/15/2022 19:56:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:04 - INFO - train.train_snli_ve - loss is tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6094/16548 [2:49:45<4:55:36,  1.70s/it]11/15/2022 19:56:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:06 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6095/16548 [2:49:47<4:55:13,  1.69s/it]11/15/2022 19:56:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:08 - INFO - train.train_snli_ve - loss is tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6096/16548 [2:49:48<4:53:08,  1.68s/it]11/15/2022 19:56:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:09 - INFO - train.train_snli_ve - loss is tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6097/16548 [2:49:50<4:54:33,  1.69s/it]11/15/2022 19:56:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.1946e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:11 - INFO - train.train_snli_ve - loss is tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6098/16548 [2:49:52<4:54:41,  1.69s/it]11/15/2022 19:56:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:13 - INFO - train.train_snli_ve - loss is tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6099/16548 [2:49:53<4:57:47,  1.71s/it]11/15/2022 19:56:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:14 - INFO - train.train_snli_ve - loss is tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6100/16548 [2:49:55<5:00:58,  1.73s/it]11/15/2022 19:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.7377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:16 - INFO - train.train_snli_ve - loss is tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6101/16548 [2:49:57<4:58:41,  1.72s/it]11/15/2022 19:56:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:18 - INFO - train.train_snli_ve - loss is tensor(0.5788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6102/16548 [2:49:59<4:59:45,  1.72s/it]11/15/2022 19:56:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:20 - INFO - train.train_snli_ve - loss is tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6103/16548 [2:50:00<4:56:47,  1.70s/it]11/15/2022 19:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:21 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6104/16548 [2:50:02<4:54:56,  1.69s/it]11/15/2022 19:56:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4020e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:23 - INFO - train.train_snli_ve - loss is tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6105/16548 [2:50:04<4:52:18,  1.68s/it]11/15/2022 19:56:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:25 - INFO - train.train_snli_ve - loss is tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6106/16548 [2:50:05<4:51:03,  1.67s/it]11/15/2022 19:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:26 - INFO - train.train_snli_ve - loss is tensor(0.4745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6107/16548 [2:50:07<4:50:03,  1.67s/it]11/15/2022 19:56:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:28 - INFO - train.train_snli_ve - loss is tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6108/16548 [2:50:09<4:50:56,  1.67s/it]11/15/2022 19:56:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:30 - INFO - train.train_snli_ve - loss is tensor(0.9079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6109/16548 [2:50:10<4:50:16,  1.67s/it]11/15/2022 19:56:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.7290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:31 - INFO - train.train_snli_ve - loss is tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6110/16548 [2:50:12<4:50:33,  1.67s/it]11/15/2022 19:56:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2155e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:33 - INFO - train.train_snli_ve - loss is tensor(0.8919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6111/16548 [2:50:14<4:51:26,  1.68s/it]11/15/2022 19:56:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:35 - INFO - train.train_snli_ve - loss is tensor(0.8229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6112/16548 [2:50:15<4:54:42,  1.69s/it]11/15/2022 19:56:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:36 - INFO - train.train_snli_ve - loss is tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6113/16548 [2:50:17<4:52:06,  1.68s/it]11/15/2022 19:56:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.0447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:38 - INFO - train.train_snli_ve - loss is tensor(0.5155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6114/16548 [2:50:19<4:52:04,  1.68s/it]11/15/2022 19:56:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5280e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:40 - INFO - train.train_snli_ve - loss is tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6115/16548 [2:50:20<4:53:57,  1.69s/it]11/15/2022 19:56:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5645e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:41 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6116/16548 [2:50:22<4:52:12,  1.68s/it]11/15/2022 19:56:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.4828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:43 - INFO - train.train_snli_ve - loss is tensor(0.9379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6117/16548 [2:50:24<4:50:17,  1.67s/it]11/15/2022 19:56:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:45 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6118/16548 [2:50:25<4:49:39,  1.67s/it]11/15/2022 19:56:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3339e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:46 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6119/16548 [2:50:27<4:49:22,  1.66s/it]11/15/2022 19:56:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:48 - INFO - train.train_snli_ve - loss is tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6120/16548 [2:50:29<4:51:38,  1.68s/it]11/15/2022 19:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:50 - INFO - train.train_snli_ve - loss is tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6121/16548 [2:50:30<4:51:08,  1.68s/it]11/15/2022 19:56:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:51 - INFO - train.train_snli_ve - loss is tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6122/16548 [2:50:32<4:52:12,  1.68s/it]11/15/2022 19:56:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1628e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:53 - INFO - train.train_snli_ve - loss is tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6123/16548 [2:50:34<4:55:16,  1.70s/it]11/15/2022 19:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3704e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:55 - INFO - train.train_snli_ve - loss is tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6124/16548 [2:50:36<4:53:59,  1.69s/it]11/15/2022 19:56:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:56 - INFO - train.train_snli_ve - loss is tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6125/16548 [2:50:37<4:51:19,  1.68s/it]11/15/2022 19:56:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:56:58 - INFO - train.train_snli_ve - loss is tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6126/16548 [2:50:39<4:50:12,  1.67s/it]11/15/2022 19:57:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:00 - INFO - train.train_snli_ve - loss is tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6127/16548 [2:50:40<4:48:36,  1.66s/it]11/15/2022 19:57:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6115e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:01 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6128/16548 [2:50:42<4:49:55,  1.67s/it]11/15/2022 19:57:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.2257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:03 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6129/16548 [2:50:44<4:49:10,  1.67s/it]11/15/2022 19:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.4239e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:05 - INFO - train.train_snli_ve - loss is tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6130/16548 [2:50:46<4:51:31,  1.68s/it]11/15/2022 19:57:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.3569e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:06 - INFO - train.train_snli_ve - loss is tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6131/16548 [2:50:47<4:50:04,  1.67s/it]11/15/2022 19:57:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.6380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:08 - INFO - train.train_snli_ve - loss is tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6132/16548 [2:50:49<4:53:04,  1.69s/it]11/15/2022 19:57:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:10 - INFO - train.train_snli_ve - loss is tensor(0.5807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6133/16548 [2:50:51<4:51:07,  1.68s/it]11/15/2022 19:57:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3933e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:11 - INFO - train.train_snli_ve - loss is tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6134/16548 [2:50:52<4:50:35,  1.67s/it]11/15/2022 19:57:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.8101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:13 - INFO - train.train_snli_ve - loss is tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6135/16548 [2:50:54<4:49:22,  1.67s/it]11/15/2022 19:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0969e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:15 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6136/16548 [2:50:56<4:48:07,  1.66s/it]11/15/2022 19:57:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3364e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:16 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6137/16548 [2:50:57<4:47:59,  1.66s/it]11/15/2022 19:57:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:18 - INFO - train.train_snli_ve - loss is tensor(0.9258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6138/16548 [2:50:59<4:48:09,  1.66s/it]11/15/2022 19:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:20 - INFO - train.train_snli_ve - loss is tensor(0.9415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6139/16548 [2:51:01<4:48:55,  1.67s/it]11/15/2022 19:57:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.4847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:21 - INFO - train.train_snli_ve - loss is tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6140/16548 [2:51:02<4:49:35,  1.67s/it]11/15/2022 19:57:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:23 - INFO - train.train_snli_ve - loss is tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6141/16548 [2:51:04<4:50:58,  1.68s/it]11/15/2022 19:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:25 - INFO - train.train_snli_ve - loss is tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6142/16548 [2:51:06<4:51:38,  1.68s/it]11/15/2022 19:57:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0866e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:26 - INFO - train.train_snli_ve - loss is tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6143/16548 [2:51:07<4:50:19,  1.67s/it]11/15/2022 19:57:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2634e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:28 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6144/16548 [2:51:09<4:50:00,  1.67s/it]11/15/2022 19:57:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.2570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:30 - INFO - train.train_snli_ve - loss is tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6145/16548 [2:51:11<4:49:58,  1.67s/it]11/15/2022 19:57:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.3105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:31 - INFO - train.train_snli_ve - loss is tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6146/16548 [2:51:12<4:49:28,  1.67s/it]11/15/2022 19:57:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.0032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:33 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6147/16548 [2:51:14<4:49:15,  1.67s/it]11/15/2022 19:57:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:35 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6148/16548 [2:51:16<4:50:40,  1.68s/it]11/15/2022 19:57:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.0328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:37 - INFO - train.train_snli_ve - loss is tensor(0.7931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6149/16548 [2:51:17<4:50:15,  1.67s/it]11/15/2022 19:57:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.2667e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:38 - INFO - train.train_snli_ve - loss is tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6150/16548 [2:51:19<4:51:00,  1.68s/it]11/15/2022 19:57:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.4751e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:40 - INFO - train.train_snli_ve - loss is tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6151/16548 [2:51:21<4:51:05,  1.68s/it]11/15/2022 19:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(8.1350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:42 - INFO - train.train_snli_ve - loss is tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6152/16548 [2:51:22<4:52:17,  1.69s/it]11/15/2022 19:57:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.2774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:43 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6153/16548 [2:51:24<4:53:03,  1.69s/it]11/15/2022 19:57:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:45 - INFO - train.train_snli_ve - loss is tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6154/16548 [2:51:26<4:51:24,  1.68s/it]11/15/2022 19:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(9.8859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:47 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6155/16548 [2:51:27<4:49:31,  1.67s/it]11/15/2022 19:57:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.0942e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:48 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6156/16548 [2:51:29<4:50:19,  1.68s/it]11/15/2022 19:57:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:50 - INFO - train.train_snli_ve - loss is tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6157/16548 [2:51:31<4:50:11,  1.68s/it]11/15/2022 19:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.4994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:52 - INFO - train.train_snli_ve - loss is tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6158/16548 [2:51:32<4:52:02,  1.69s/it]11/15/2022 19:57:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:53 - INFO - train.train_snli_ve - loss is tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6159/16548 [2:51:34<4:52:54,  1.69s/it]11/15/2022 19:57:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:55 - INFO - train.train_snli_ve - loss is tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6160/16548 [2:51:36<4:53:43,  1.70s/it]11/15/2022 19:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3544e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:57 - INFO - train.train_snli_ve - loss is tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6161/16548 [2:51:37<4:51:41,  1.68s/it]11/15/2022 19:57:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:57:58 - INFO - train.train_snli_ve - loss is tensor(0.8558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6162/16548 [2:51:39<4:52:16,  1.69s/it]11/15/2022 19:58:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:00 - INFO - train.train_snli_ve - loss is tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6163/16548 [2:51:41<4:50:14,  1.68s/it]11/15/2022 19:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(9.9559e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:02 - INFO - train.train_snli_ve - loss is tensor(0.7447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6164/16548 [2:51:42<4:47:43,  1.66s/it]11/15/2022 19:58:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:03 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6165/16548 [2:51:44<4:47:45,  1.66s/it]11/15/2022 19:58:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5649e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:05 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6166/16548 [2:51:46<4:49:29,  1.67s/it]11/15/2022 19:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.3020e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:07 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6167/16548 [2:51:48<4:50:23,  1.68s/it]11/15/2022 19:58:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:08 - INFO - train.train_snli_ve - loss is tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6168/16548 [2:51:49<4:51:58,  1.69s/it]11/15/2022 19:58:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0173e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:10 - INFO - train.train_snli_ve - loss is tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6169/16548 [2:51:51<4:53:10,  1.69s/it]11/15/2022 19:58:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.0802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:12 - INFO - train.train_snli_ve - loss is tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6170/16548 [2:51:53<4:51:36,  1.69s/it]11/15/2022 19:58:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.8481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:14 - INFO - train.train_snli_ve - loss is tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6171/16548 [2:51:54<4:49:01,  1.67s/it]11/15/2022 19:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:15 - INFO - train.train_snli_ve - loss is tensor(0.7770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6172/16548 [2:51:56<4:49:31,  1.67s/it]11/15/2022 19:58:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:17 - INFO - train.train_snli_ve - loss is tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6173/16548 [2:51:58<4:47:19,  1.66s/it]11/15/2022 19:58:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.4925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:19 - INFO - train.train_snli_ve - loss is tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6174/16548 [2:51:59<4:50:48,  1.68s/it]11/15/2022 19:58:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0125e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:20 - INFO - train.train_snli_ve - loss is tensor(0.9806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6175/16548 [2:52:01<4:50:34,  1.68s/it]11/15/2022 19:58:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:22 - INFO - train.train_snli_ve - loss is tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6176/16548 [2:52:03<4:51:40,  1.69s/it]11/15/2022 19:58:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:24 - INFO - train.train_snli_ve - loss is tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6177/16548 [2:52:04<4:53:01,  1.70s/it]11/15/2022 19:58:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:25 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6178/16548 [2:52:06<4:51:19,  1.69s/it]11/15/2022 19:58:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:27 - INFO - train.train_snli_ve - loss is tensor(0.5105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6179/16548 [2:52:08<4:49:55,  1.68s/it]11/15/2022 19:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1022e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:29 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6180/16548 [2:52:09<4:52:16,  1.69s/it]11/15/2022 19:58:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:30 - INFO - train.train_snli_ve - loss is tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6181/16548 [2:52:11<4:50:32,  1.68s/it]11/15/2022 19:58:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:32 - INFO - train.train_snli_ve - loss is tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6182/16548 [2:52:13<4:49:36,  1.68s/it]11/15/2022 19:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1053e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:34 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6183/16548 [2:52:14<4:50:31,  1.68s/it]11/15/2022 19:58:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:35 - INFO - train.train_snli_ve - loss is tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6184/16548 [2:52:16<4:50:32,  1.68s/it]11/15/2022 19:58:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:37 - INFO - train.train_snli_ve - loss is tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6185/16548 [2:52:18<4:48:07,  1.67s/it]11/15/2022 19:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:39 - INFO - train.train_snli_ve - loss is tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6186/16548 [2:52:19<4:50:02,  1.68s/it]11/15/2022 19:58:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:40 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6187/16548 [2:52:21<4:49:34,  1.68s/it]11/15/2022 19:58:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:42 - INFO - train.train_snli_ve - loss is tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6188/16548 [2:52:23<4:48:40,  1.67s/it]11/15/2022 19:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1338e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:44 - INFO - train.train_snli_ve - loss is tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6189/16548 [2:52:24<4:49:20,  1.68s/it]11/15/2022 19:58:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:45 - INFO - train.train_snli_ve - loss is tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6190/16548 [2:52:26<4:49:25,  1.68s/it]11/15/2022 19:58:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7895e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:47 - INFO - train.train_snli_ve - loss is tensor(0.4293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6191/16548 [2:52:28<4:50:08,  1.68s/it]11/15/2022 19:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:49 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6192/16548 [2:52:30<4:52:37,  1.70s/it]11/15/2022 19:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4077e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:51 - INFO - train.train_snli_ve - loss is tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6193/16548 [2:52:31<4:51:41,  1.69s/it]11/15/2022 19:58:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:52 - INFO - train.train_snli_ve - loss is tensor(0.4804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6194/16548 [2:52:33<4:49:02,  1.67s/it]11/15/2022 19:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5089e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:54 - INFO - train.train_snli_ve - loss is tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6195/16548 [2:52:35<4:49:45,  1.68s/it]11/15/2022 19:58:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:56 - INFO - train.train_snli_ve - loss is tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6196/16548 [2:52:36<4:48:13,  1.67s/it]11/15/2022 19:58:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:57 - INFO - train.train_snli_ve - loss is tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6197/16548 [2:52:38<4:48:46,  1.67s/it]11/15/2022 19:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:58:59 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6198/16548 [2:52:40<4:48:24,  1.67s/it]11/15/2022 19:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.0225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:01 - INFO - train.train_snli_ve - loss is tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6199/16548 [2:52:41<4:49:14,  1.68s/it]11/15/2022 19:59:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:02 - INFO - train.train_snli_ve - loss is tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6200/16548 [2:52:43<4:53:34,  1.70s/it]11/15/2022 19:59:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:04 - INFO - train.train_snli_ve - loss is tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6201/16548 [2:52:45<4:51:00,  1.69s/it]11/15/2022 19:59:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:06 - INFO - train.train_snli_ve - loss is tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6202/16548 [2:52:46<4:49:54,  1.68s/it]11/15/2022 19:59:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6928e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:07 - INFO - train.train_snli_ve - loss is tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6203/16548 [2:52:48<4:51:06,  1.69s/it]11/15/2022 19:59:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:09 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6204/16548 [2:52:50<4:51:53,  1.69s/it]11/15/2022 19:59:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:11 - INFO - train.train_snli_ve - loss is tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37% 6205/16548 [2:52:51<4:49:33,  1.68s/it]11/15/2022 19:59:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:12 - INFO - train.train_snli_ve - loss is tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6206/16548 [2:52:53<4:49:10,  1.68s/it]11/15/2022 19:59:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.2389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:14 - INFO - train.train_snli_ve - loss is tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6207/16548 [2:52:55<4:51:06,  1.69s/it]11/15/2022 19:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3949e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:16 - INFO - train.train_snli_ve - loss is tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6208/16548 [2:52:56<4:48:48,  1.68s/it]11/15/2022 19:59:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:17 - INFO - train.train_snli_ve - loss is tensor(0.5371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6209/16548 [2:52:58<4:46:39,  1.66s/it]11/15/2022 19:59:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.9792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:19 - INFO - train.train_snli_ve - loss is tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6210/16548 [2:53:00<4:47:54,  1.67s/it]11/15/2022 19:59:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.0463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:21 - INFO - train.train_snli_ve - loss is tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6211/16548 [2:53:01<4:49:01,  1.68s/it]11/15/2022 19:59:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:22 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6212/16548 [2:53:03<4:48:44,  1.68s/it]11/15/2022 19:59:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:24 - INFO - train.train_snli_ve - loss is tensor(0.4599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6213/16548 [2:53:05<4:52:19,  1.70s/it]11/15/2022 19:59:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:26 - INFO - train.train_snli_ve - loss is tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6214/16548 [2:53:07<4:51:56,  1.70s/it]11/15/2022 19:59:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.4087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:27 - INFO - train.train_snli_ve - loss is tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6215/16548 [2:53:08<4:50:31,  1.69s/it]11/15/2022 19:59:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.8351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:29 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6216/16548 [2:53:10<4:49:55,  1.68s/it]11/15/2022 19:59:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.7219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:31 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6217/16548 [2:53:12<4:49:36,  1.68s/it]11/15/2022 19:59:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.0496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:33 - INFO - train.train_snli_ve - loss is tensor(0.4602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6218/16548 [2:53:13<4:49:10,  1.68s/it]11/15/2022 19:59:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:34 - INFO - train.train_snli_ve - loss is tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6219/16548 [2:53:15<4:50:00,  1.68s/it]11/15/2022 19:59:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.4780e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:36 - INFO - train.train_snli_ve - loss is tensor(0.8095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6220/16548 [2:53:17<4:51:18,  1.69s/it]11/15/2022 19:59:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6540e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:38 - INFO - train.train_snli_ve - loss is tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6221/16548 [2:53:18<4:49:38,  1.68s/it]11/15/2022 19:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:39 - INFO - train.train_snli_ve - loss is tensor(0.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6222/16548 [2:53:20<4:50:27,  1.69s/it]11/15/2022 19:59:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:41 - INFO - train.train_snli_ve - loss is tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6223/16548 [2:53:22<4:48:27,  1.68s/it]11/15/2022 19:59:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:43 - INFO - train.train_snli_ve - loss is tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6224/16548 [2:53:23<4:47:08,  1.67s/it]11/15/2022 19:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.3961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:44 - INFO - train.train_snli_ve - loss is tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6225/16548 [2:53:25<4:48:09,  1.67s/it]11/15/2022 19:59:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:46 - INFO - train.train_snli_ve - loss is tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6226/16548 [2:53:27<4:47:58,  1.67s/it]11/15/2022 19:59:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1470e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:48 - INFO - train.train_snli_ve - loss is tensor(0.8689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6227/16548 [2:53:28<4:48:22,  1.68s/it]11/15/2022 19:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:49 - INFO - train.train_snli_ve - loss is tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6228/16548 [2:53:30<4:47:06,  1.67s/it]11/15/2022 19:59:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:51 - INFO - train.train_snli_ve - loss is tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6229/16548 [2:53:32<4:47:20,  1.67s/it]11/15/2022 19:59:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:53 - INFO - train.train_snli_ve - loss is tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6230/16548 [2:53:33<4:49:06,  1.68s/it]11/15/2022 19:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.1027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:54 - INFO - train.train_snli_ve - loss is tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6231/16548 [2:53:35<4:47:50,  1.67s/it]11/15/2022 19:59:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.0013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:56 - INFO - train.train_snli_ve - loss is tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6232/16548 [2:53:37<4:47:51,  1.67s/it]11/15/2022 19:59:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:58 - INFO - train.train_snli_ve - loss is tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6233/16548 [2:53:38<4:47:15,  1.67s/it]11/15/2022 19:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 19:59:59 - INFO - train.train_snli_ve - loss is tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6234/16548 [2:53:40<4:47:21,  1.67s/it]11/15/2022 20:00:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:01 - INFO - train.train_snli_ve - loss is tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6235/16548 [2:53:42<4:46:47,  1.67s/it]11/15/2022 20:00:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:03 - INFO - train.train_snli_ve - loss is tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6236/16548 [2:53:43<4:47:14,  1.67s/it]11/15/2022 20:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.5802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:04 - INFO - train.train_snli_ve - loss is tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6237/16548 [2:53:45<4:47:36,  1.67s/it]11/15/2022 20:00:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:06 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6238/16548 [2:53:47<4:48:32,  1.68s/it]11/15/2022 20:00:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.4431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:08 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6239/16548 [2:53:48<4:47:47,  1.68s/it]11/15/2022 20:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:09 - INFO - train.train_snli_ve - loss is tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6240/16548 [2:53:50<4:48:20,  1.68s/it]11/15/2022 20:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:11 - INFO - train.train_snli_ve - loss is tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6241/16548 [2:53:52<4:47:34,  1.67s/it]11/15/2022 20:00:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:13 - INFO - train.train_snli_ve - loss is tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6242/16548 [2:53:53<4:46:49,  1.67s/it]11/15/2022 20:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:14 - INFO - train.train_snli_ve - loss is tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6243/16548 [2:53:55<4:46:45,  1.67s/it]11/15/2022 20:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6168e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:16 - INFO - train.train_snli_ve - loss is tensor(0.9130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6244/16548 [2:53:57<4:44:52,  1.66s/it]11/15/2022 20:00:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4375e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:18 - INFO - train.train_snli_ve - loss is tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6245/16548 [2:53:58<4:43:24,  1.65s/it]11/15/2022 20:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.0851e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:19 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6246/16548 [2:54:00<4:44:56,  1.66s/it]11/15/2022 20:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.5298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:21 - INFO - train.train_snli_ve - loss is tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6247/16548 [2:54:02<4:44:47,  1.66s/it]11/15/2022 20:00:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:23 - INFO - train.train_snli_ve - loss is tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6248/16548 [2:54:03<4:43:10,  1.65s/it]11/15/2022 20:00:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.5586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:24 - INFO - train.train_snli_ve - loss is tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6249/16548 [2:54:05<4:43:35,  1.65s/it]11/15/2022 20:00:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:26 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6250/16548 [2:54:07<4:43:40,  1.65s/it]11/15/2022 20:00:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4522e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:28 - INFO - train.train_snli_ve - loss is tensor(1.1646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6251/16548 [2:54:08<4:44:59,  1.66s/it]11/15/2022 20:00:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:29 - INFO - train.train_snli_ve - loss is tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6252/16548 [2:54:10<4:47:08,  1.67s/it]11/15/2022 20:00:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3965e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:31 - INFO - train.train_snli_ve - loss is tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6253/16548 [2:54:12<4:47:46,  1.68s/it]11/15/2022 20:00:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:33 - INFO - train.train_snli_ve - loss is tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6254/16548 [2:54:13<4:46:29,  1.67s/it]11/15/2022 20:00:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.5109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:34 - INFO - train.train_snli_ve - loss is tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6255/16548 [2:54:15<4:47:23,  1.68s/it]11/15/2022 20:00:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.8066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:36 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6256/16548 [2:54:17<4:46:02,  1.67s/it]11/15/2022 20:00:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6830e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:38 - INFO - train.train_snli_ve - loss is tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6257/16548 [2:54:18<4:44:23,  1.66s/it]11/15/2022 20:00:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:39 - INFO - train.train_snli_ve - loss is tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6258/16548 [2:54:20<4:45:55,  1.67s/it]11/15/2022 20:00:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:41 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6259/16548 [2:54:22<4:50:24,  1.69s/it]11/15/2022 20:00:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:43 - INFO - train.train_snli_ve - loss is tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6260/16548 [2:54:23<4:48:06,  1.68s/it]11/15/2022 20:00:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:44 - INFO - train.train_snli_ve - loss is tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6261/16548 [2:54:25<4:46:45,  1.67s/it]11/15/2022 20:00:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.0468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:46 - INFO - train.train_snli_ve - loss is tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6262/16548 [2:54:27<4:46:01,  1.67s/it]11/15/2022 20:00:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:48 - INFO - train.train_snli_ve - loss is tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6263/16548 [2:54:28<4:46:20,  1.67s/it]11/15/2022 20:00:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:49 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6264/16548 [2:54:30<4:46:47,  1.67s/it]11/15/2022 20:00:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.3632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:51 - INFO - train.train_snli_ve - loss is tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6265/16548 [2:54:32<4:47:03,  1.67s/it]11/15/2022 20:00:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:53 - INFO - train.train_snli_ve - loss is tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6266/16548 [2:54:33<4:45:47,  1.67s/it]11/15/2022 20:00:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:54 - INFO - train.train_snli_ve - loss is tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6267/16548 [2:54:35<4:45:07,  1.66s/it]11/15/2022 20:00:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.9547e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:56 - INFO - train.train_snli_ve - loss is tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6268/16548 [2:54:37<4:44:26,  1.66s/it]11/15/2022 20:00:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:58 - INFO - train.train_snli_ve - loss is tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6269/16548 [2:54:38<4:45:08,  1.66s/it]11/15/2022 20:00:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.3394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:00:59 - INFO - train.train_snli_ve - loss is tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6270/16548 [2:54:40<4:46:21,  1.67s/it]11/15/2022 20:01:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:01 - INFO - train.train_snli_ve - loss is tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6271/16548 [2:54:42<4:46:53,  1.67s/it]11/15/2022 20:01:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:03 - INFO - train.train_snli_ve - loss is tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6272/16548 [2:54:43<4:47:05,  1.68s/it]11/15/2022 20:01:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.5178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:04 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6273/16548 [2:54:45<4:46:25,  1.67s/it]11/15/2022 20:01:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.6222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:06 - INFO - train.train_snli_ve - loss is tensor(0.8838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6274/16548 [2:54:47<4:45:32,  1.67s/it]11/15/2022 20:01:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.2796e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:08 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6275/16548 [2:54:48<4:45:57,  1.67s/it]11/15/2022 20:01:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:09 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6276/16548 [2:54:50<4:44:44,  1.66s/it]11/15/2022 20:01:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:11 - INFO - train.train_snli_ve - loss is tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6277/16548 [2:54:52<4:44:19,  1.66s/it]11/15/2022 20:01:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:13 - INFO - train.train_snli_ve - loss is tensor(0.7418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6278/16548 [2:54:53<4:43:30,  1.66s/it]11/15/2022 20:01:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:14 - INFO - train.train_snli_ve - loss is tensor(0.4712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6279/16548 [2:54:55<4:46:35,  1.67s/it]11/15/2022 20:01:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2242e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:16 - INFO - train.train_snli_ve - loss is tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6280/16548 [2:54:57<4:47:20,  1.68s/it]11/15/2022 20:01:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5103e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:18 - INFO - train.train_snli_ve - loss is tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6281/16548 [2:54:58<4:45:38,  1.67s/it]11/15/2022 20:01:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:19 - INFO - train.train_snli_ve - loss is tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6282/16548 [2:55:00<4:45:27,  1.67s/it]11/15/2022 20:01:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:21 - INFO - train.train_snli_ve - loss is tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6283/16548 [2:55:02<4:46:58,  1.68s/it]11/15/2022 20:01:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6965e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:23 - INFO - train.train_snli_ve - loss is tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6284/16548 [2:55:04<4:47:05,  1.68s/it]11/15/2022 20:01:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6764e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:24 - INFO - train.train_snli_ve - loss is tensor(0.8500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6285/16548 [2:55:05<4:45:19,  1.67s/it]11/15/2022 20:01:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.3628e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:26 - INFO - train.train_snli_ve - loss is tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6286/16548 [2:55:07<4:46:46,  1.68s/it]11/15/2022 20:01:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:28 - INFO - train.train_snli_ve - loss is tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6287/16548 [2:55:09<4:45:29,  1.67s/it]11/15/2022 20:01:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2292e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:29 - INFO - train.train_snli_ve - loss is tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6288/16548 [2:55:10<4:44:39,  1.66s/it]11/15/2022 20:01:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:31 - INFO - train.train_snli_ve - loss is tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6289/16548 [2:55:12<4:44:54,  1.67s/it]11/15/2022 20:01:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:33 - INFO - train.train_snli_ve - loss is tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6290/16548 [2:55:14<4:45:25,  1.67s/it]11/15/2022 20:01:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:34 - INFO - train.train_snli_ve - loss is tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6291/16548 [2:55:15<4:44:49,  1.67s/it]11/15/2022 20:01:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:36 - INFO - train.train_snli_ve - loss is tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6292/16548 [2:55:17<4:46:36,  1.68s/it]11/15/2022 20:01:38 - INFO - train.train_snli_ve - kd_loss is tensor(9.1155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:38 - INFO - train.train_snli_ve - loss is tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6293/16548 [2:55:19<4:45:43,  1.67s/it]11/15/2022 20:01:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.8901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:40 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6294/16548 [2:55:20<4:47:40,  1.68s/it]11/15/2022 20:01:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:41 - INFO - train.train_snli_ve - loss is tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6295/16548 [2:55:22<4:45:05,  1.67s/it]11/15/2022 20:01:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:43 - INFO - train.train_snli_ve - loss is tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6296/16548 [2:55:24<4:46:15,  1.68s/it]11/15/2022 20:01:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:45 - INFO - train.train_snli_ve - loss is tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6297/16548 [2:55:25<4:46:00,  1.67s/it]11/15/2022 20:01:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.5842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:46 - INFO - train.train_snli_ve - loss is tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6298/16548 [2:55:27<4:46:17,  1.68s/it]11/15/2022 20:01:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.3759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:48 - INFO - train.train_snli_ve - loss is tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6299/16548 [2:55:29<4:45:13,  1.67s/it]11/15/2022 20:01:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.3951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:50 - INFO - train.train_snli_ve - loss is tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6300/16548 [2:55:30<4:47:36,  1.68s/it]11/15/2022 20:01:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.6167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:51 - INFO - train.train_snli_ve - loss is tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6301/16548 [2:55:32<4:44:32,  1.67s/it]11/15/2022 20:01:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:53 - INFO - train.train_snli_ve - loss is tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6302/16548 [2:55:34<4:43:49,  1.66s/it]11/15/2022 20:01:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.0767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:55 - INFO - train.train_snli_ve - loss is tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6303/16548 [2:55:35<4:44:23,  1.67s/it]11/15/2022 20:01:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.4964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:56 - INFO - train.train_snli_ve - loss is tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6304/16548 [2:55:37<4:42:46,  1.66s/it]11/15/2022 20:01:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.5201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:01:58 - INFO - train.train_snli_ve - loss is tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6305/16548 [2:55:39<4:44:34,  1.67s/it]11/15/2022 20:02:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0969e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:00 - INFO - train.train_snli_ve - loss is tensor(0.9752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6306/16548 [2:55:40<4:47:57,  1.69s/it]11/15/2022 20:02:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:01 - INFO - train.train_snli_ve - loss is tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6307/16548 [2:55:42<4:44:58,  1.67s/it]11/15/2022 20:02:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:03 - INFO - train.train_snli_ve - loss is tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6308/16548 [2:55:44<4:43:43,  1.66s/it]11/15/2022 20:02:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:04 - INFO - train.train_snli_ve - loss is tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6309/16548 [2:55:45<4:42:27,  1.66s/it]11/15/2022 20:02:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.8465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:06 - INFO - train.train_snli_ve - loss is tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6310/16548 [2:55:47<4:42:09,  1.65s/it]11/15/2022 20:02:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.1293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:08 - INFO - train.train_snli_ve - loss is tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6311/16548 [2:55:49<4:42:16,  1.65s/it]11/15/2022 20:02:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:09 - INFO - train.train_snli_ve - loss is tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6312/16548 [2:55:50<4:44:02,  1.66s/it]11/15/2022 20:02:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:11 - INFO - train.train_snli_ve - loss is tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6313/16548 [2:55:52<4:43:27,  1.66s/it]11/15/2022 20:02:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2050e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:13 - INFO - train.train_snli_ve - loss is tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6314/16548 [2:55:54<4:44:10,  1.67s/it]11/15/2022 20:02:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.3252e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:14 - INFO - train.train_snli_ve - loss is tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6315/16548 [2:55:55<4:42:20,  1.66s/it]11/15/2022 20:02:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:16 - INFO - train.train_snli_ve - loss is tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6316/16548 [2:55:57<4:41:41,  1.65s/it]11/15/2022 20:02:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.3108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:18 - INFO - train.train_snli_ve - loss is tensor(1.0186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6317/16548 [2:55:59<4:43:45,  1.66s/it]11/15/2022 20:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.6833e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:19 - INFO - train.train_snli_ve - loss is tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6318/16548 [2:56:00<4:42:25,  1.66s/it]11/15/2022 20:02:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.5711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:21 - INFO - train.train_snli_ve - loss is tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6319/16548 [2:56:02<4:41:12,  1.65s/it]11/15/2022 20:02:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:23 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6320/16548 [2:56:03<4:43:52,  1.67s/it]11/15/2022 20:02:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2607e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:25 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6321/16548 [2:56:05<4:48:17,  1.69s/it]11/15/2022 20:02:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:26 - INFO - train.train_snli_ve - loss is tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6322/16548 [2:56:07<4:50:08,  1.70s/it]11/15/2022 20:02:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:28 - INFO - train.train_snli_ve - loss is tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6323/16548 [2:56:09<4:49:03,  1.70s/it]11/15/2022 20:02:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:30 - INFO - train.train_snli_ve - loss is tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6324/16548 [2:56:10<4:45:47,  1.68s/it]11/15/2022 20:02:31 - INFO - train.train_snli_ve - kd_loss is tensor(9.2720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:31 - INFO - train.train_snli_ve - loss is tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6325/16548 [2:56:12<4:43:54,  1.67s/it]11/15/2022 20:02:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:33 - INFO - train.train_snli_ve - loss is tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6326/16548 [2:56:14<4:42:21,  1.66s/it]11/15/2022 20:02:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.6805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:34 - INFO - train.train_snli_ve - loss is tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6327/16548 [2:56:15<4:42:28,  1.66s/it]11/15/2022 20:02:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:36 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6328/16548 [2:56:17<4:41:53,  1.65s/it]11/15/2022 20:02:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:38 - INFO - train.train_snli_ve - loss is tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6329/16548 [2:56:19<4:41:30,  1.65s/it]11/15/2022 20:02:40 - INFO - train.train_snli_ve - kd_loss is tensor(9.8113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:40 - INFO - train.train_snli_ve - loss is tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6330/16548 [2:56:20<4:47:26,  1.69s/it]11/15/2022 20:02:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.0798e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:41 - INFO - train.train_snli_ve - loss is tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6331/16548 [2:56:22<4:47:23,  1.69s/it]11/15/2022 20:02:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6357e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:43 - INFO - train.train_snli_ve - loss is tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6332/16548 [2:56:24<4:46:47,  1.68s/it]11/15/2022 20:02:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:45 - INFO - train.train_snli_ve - loss is tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6333/16548 [2:56:25<4:48:09,  1.69s/it]11/15/2022 20:02:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:46 - INFO - train.train_snli_ve - loss is tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6334/16548 [2:56:27<4:47:04,  1.69s/it]11/15/2022 20:02:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:48 - INFO - train.train_snli_ve - loss is tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6335/16548 [2:56:29<4:45:32,  1.68s/it]11/15/2022 20:02:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1823e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:50 - INFO - train.train_snli_ve - loss is tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6336/16548 [2:56:30<4:45:34,  1.68s/it]11/15/2022 20:02:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2833e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:51 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6337/16548 [2:56:32<4:46:20,  1.68s/it]11/15/2022 20:02:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2374e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:53 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6338/16548 [2:56:34<4:44:53,  1.67s/it]11/15/2022 20:02:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:55 - INFO - train.train_snli_ve - loss is tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6339/16548 [2:56:35<4:44:28,  1.67s/it]11/15/2022 20:02:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.6814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:56 - INFO - train.train_snli_ve - loss is tensor(0.9066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6340/16548 [2:56:37<4:46:36,  1.68s/it]11/15/2022 20:02:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:02:58 - INFO - train.train_snli_ve - loss is tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6341/16548 [2:56:39<4:46:39,  1.69s/it]11/15/2022 20:03:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:00 - INFO - train.train_snli_ve - loss is tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6342/16548 [2:56:40<4:45:46,  1.68s/it]11/15/2022 20:03:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6569e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:01 - INFO - train.train_snli_ve - loss is tensor(0.4859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6343/16548 [2:56:42<4:47:47,  1.69s/it]11/15/2022 20:03:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3357e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:03 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6344/16548 [2:56:44<4:47:02,  1.69s/it]11/15/2022 20:03:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.3002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:05 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6345/16548 [2:56:46<4:45:32,  1.68s/it]11/15/2022 20:03:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:06 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6346/16548 [2:56:47<4:45:01,  1.68s/it]11/15/2022 20:03:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:08 - INFO - train.train_snli_ve - loss is tensor(0.9028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6347/16548 [2:56:49<4:46:11,  1.68s/it]11/15/2022 20:03:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.2408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:10 - INFO - train.train_snli_ve - loss is tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6348/16548 [2:56:51<4:45:21,  1.68s/it]11/15/2022 20:03:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:12 - INFO - train.train_snli_ve - loss is tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6349/16548 [2:56:52<4:46:19,  1.68s/it]11/15/2022 20:03:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:13 - INFO - train.train_snli_ve - loss is tensor(0.8005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6350/16548 [2:56:54<4:47:16,  1.69s/it]11/15/2022 20:03:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.0303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:15 - INFO - train.train_snli_ve - loss is tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6351/16548 [2:56:56<4:46:15,  1.68s/it]11/15/2022 20:03:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:17 - INFO - train.train_snli_ve - loss is tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6352/16548 [2:56:57<4:44:18,  1.67s/it]11/15/2022 20:03:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:18 - INFO - train.train_snli_ve - loss is tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6353/16548 [2:56:59<4:43:00,  1.67s/it]11/15/2022 20:03:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.1505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:20 - INFO - train.train_snli_ve - loss is tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6354/16548 [2:57:01<4:44:14,  1.67s/it]11/15/2022 20:03:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.8702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:22 - INFO - train.train_snli_ve - loss is tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6355/16548 [2:57:02<4:44:04,  1.67s/it]11/15/2022 20:03:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:23 - INFO - train.train_snli_ve - loss is tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6356/16548 [2:57:04<4:43:02,  1.67s/it]11/15/2022 20:03:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:25 - INFO - train.train_snli_ve - loss is tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6357/16548 [2:57:06<4:43:33,  1.67s/it]11/15/2022 20:03:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.0558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:27 - INFO - train.train_snli_ve - loss is tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6358/16548 [2:57:07<4:44:35,  1.68s/it]11/15/2022 20:03:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.0446e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:28 - INFO - train.train_snli_ve - loss is tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6359/16548 [2:57:09<4:44:33,  1.68s/it]11/15/2022 20:03:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4103e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:30 - INFO - train.train_snli_ve - loss is tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6360/16548 [2:57:11<4:44:50,  1.68s/it]11/15/2022 20:03:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5555e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:32 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6361/16548 [2:57:12<4:44:06,  1.67s/it]11/15/2022 20:03:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:33 - INFO - train.train_snli_ve - loss is tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6362/16548 [2:57:14<4:45:02,  1.68s/it]11/15/2022 20:03:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5365e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:35 - INFO - train.train_snli_ve - loss is tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6363/16548 [2:57:16<4:44:55,  1.68s/it]11/15/2022 20:03:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:37 - INFO - train.train_snli_ve - loss is tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6364/16548 [2:57:17<4:44:28,  1.68s/it]11/15/2022 20:03:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:38 - INFO - train.train_snli_ve - loss is tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6365/16548 [2:57:19<4:46:41,  1.69s/it]11/15/2022 20:03:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:40 - INFO - train.train_snli_ve - loss is tensor(0.9866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6366/16548 [2:57:21<4:46:11,  1.69s/it]11/15/2022 20:03:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:42 - INFO - train.train_snli_ve - loss is tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6367/16548 [2:57:22<4:44:44,  1.68s/it]11/15/2022 20:03:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:43 - INFO - train.train_snli_ve - loss is tensor(0.4370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6368/16548 [2:57:24<4:45:46,  1.68s/it]11/15/2022 20:03:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:45 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6369/16548 [2:57:26<4:45:49,  1.68s/it]11/15/2022 20:03:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:47 - INFO - train.train_snli_ve - loss is tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38% 6370/16548 [2:57:27<4:43:49,  1.67s/it]11/15/2022 20:03:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.5394e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:48 - INFO - train.train_snli_ve - loss is tensor(0.4648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6371/16548 [2:57:29<4:42:53,  1.67s/it]11/15/2022 20:03:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:50 - INFO - train.train_snli_ve - loss is tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6372/16548 [2:57:31<4:43:30,  1.67s/it]11/15/2022 20:03:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:52 - INFO - train.train_snli_ve - loss is tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6373/16548 [2:57:32<4:43:37,  1.67s/it]11/15/2022 20:03:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:53 - INFO - train.train_snli_ve - loss is tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6374/16548 [2:57:34<4:44:40,  1.68s/it]11/15/2022 20:03:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:55 - INFO - train.train_snli_ve - loss is tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6375/16548 [2:57:36<4:44:31,  1.68s/it]11/15/2022 20:03:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:57 - INFO - train.train_snli_ve - loss is tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6376/16548 [2:57:37<4:43:21,  1.67s/it]11/15/2022 20:03:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:03:58 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6377/16548 [2:57:39<4:43:12,  1.67s/it]11/15/2022 20:04:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.1272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:00 - INFO - train.train_snli_ve - loss is tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6378/16548 [2:57:41<4:41:36,  1.66s/it]11/15/2022 20:04:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:02 - INFO - train.train_snli_ve - loss is tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6379/16548 [2:57:42<4:41:56,  1.66s/it]11/15/2022 20:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.7900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:03 - INFO - train.train_snli_ve - loss is tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6380/16548 [2:57:44<4:41:05,  1.66s/it]11/15/2022 20:04:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:05 - INFO - train.train_snli_ve - loss is tensor(0.9912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6381/16548 [2:57:46<4:42:06,  1.66s/it]11/15/2022 20:04:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:07 - INFO - train.train_snli_ve - loss is tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6382/16548 [2:57:47<4:42:34,  1.67s/it]11/15/2022 20:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:08 - INFO - train.train_snli_ve - loss is tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6383/16548 [2:57:49<4:41:37,  1.66s/it]11/15/2022 20:04:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:10 - INFO - train.train_snli_ve - loss is tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6384/16548 [2:57:51<4:40:53,  1.66s/it]11/15/2022 20:04:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.0040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:12 - INFO - train.train_snli_ve - loss is tensor(0.8885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6385/16548 [2:57:52<4:41:35,  1.66s/it]11/15/2022 20:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:13 - INFO - train.train_snli_ve - loss is tensor(0.4712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6386/16548 [2:57:54<4:43:37,  1.67s/it]11/15/2022 20:04:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:15 - INFO - train.train_snli_ve - loss is tensor(0.5596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6387/16548 [2:57:56<4:45:23,  1.69s/it]11/15/2022 20:04:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:17 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6388/16548 [2:57:58<4:44:25,  1.68s/it]11/15/2022 20:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:18 - INFO - train.train_snli_ve - loss is tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6389/16548 [2:57:59<4:43:12,  1.67s/it]11/15/2022 20:04:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:20 - INFO - train.train_snli_ve - loss is tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6390/16548 [2:58:01<4:44:24,  1.68s/it]11/15/2022 20:04:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.8262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:22 - INFO - train.train_snli_ve - loss is tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6391/16548 [2:58:03<4:46:17,  1.69s/it]11/15/2022 20:04:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:24 - INFO - train.train_snli_ve - loss is tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6392/16548 [2:58:04<4:45:30,  1.69s/it]11/15/2022 20:04:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:25 - INFO - train.train_snli_ve - loss is tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6393/16548 [2:58:06<4:44:39,  1.68s/it]11/15/2022 20:04:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4059e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:27 - INFO - train.train_snli_ve - loss is tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6394/16548 [2:58:08<4:43:10,  1.67s/it]11/15/2022 20:04:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:29 - INFO - train.train_snli_ve - loss is tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6395/16548 [2:58:09<4:44:44,  1.68s/it]11/15/2022 20:04:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:30 - INFO - train.train_snli_ve - loss is tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6396/16548 [2:58:11<4:44:47,  1.68s/it]11/15/2022 20:04:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:32 - INFO - train.train_snli_ve - loss is tensor(0.9242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6397/16548 [2:58:13<4:43:16,  1.67s/it]11/15/2022 20:04:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:34 - INFO - train.train_snli_ve - loss is tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6398/16548 [2:58:14<4:42:10,  1.67s/it]11/15/2022 20:04:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0071e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:35 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6399/16548 [2:58:16<4:44:35,  1.68s/it]11/15/2022 20:04:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:37 - INFO - train.train_snli_ve - loss is tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6400/16548 [2:58:18<4:46:51,  1.70s/it]11/15/2022 20:04:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:39 - INFO - train.train_snli_ve - loss is tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6401/16548 [2:58:19<4:45:28,  1.69s/it]11/15/2022 20:04:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.3279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:40 - INFO - train.train_snli_ve - loss is tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6402/16548 [2:58:21<4:45:46,  1.69s/it]11/15/2022 20:04:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:42 - INFO - train.train_snli_ve - loss is tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6403/16548 [2:58:23<4:42:36,  1.67s/it]11/15/2022 20:04:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.6809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:44 - INFO - train.train_snli_ve - loss is tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6404/16548 [2:58:24<4:42:44,  1.67s/it]11/15/2022 20:04:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:45 - INFO - train.train_snli_ve - loss is tensor(0.7389, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6405/16548 [2:58:26<4:43:42,  1.68s/it]11/15/2022 20:04:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.5867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:47 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6406/16548 [2:58:28<4:42:38,  1.67s/it]11/15/2022 20:04:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.4296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:49 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6407/16548 [2:58:29<4:44:30,  1.68s/it]11/15/2022 20:04:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:50 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6408/16548 [2:58:31<4:42:55,  1.67s/it]11/15/2022 20:04:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:52 - INFO - train.train_snli_ve - loss is tensor(0.9042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6409/16548 [2:58:33<4:39:57,  1.66s/it]11/15/2022 20:04:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.7207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:54 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6410/16548 [2:58:34<4:42:19,  1.67s/it]11/15/2022 20:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.9468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:55 - INFO - train.train_snli_ve - loss is tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6411/16548 [2:58:36<4:42:56,  1.67s/it]11/15/2022 20:04:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:57 - INFO - train.train_snli_ve - loss is tensor(0.7906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6412/16548 [2:58:38<4:44:53,  1.69s/it]11/15/2022 20:04:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:04:59 - INFO - train.train_snli_ve - loss is tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6413/16548 [2:58:40<4:45:17,  1.69s/it]11/15/2022 20:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:00 - INFO - train.train_snli_ve - loss is tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6414/16548 [2:58:41<4:44:01,  1.68s/it]11/15/2022 20:05:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.4932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:02 - INFO - train.train_snli_ve - loss is tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6415/16548 [2:58:43<4:44:09,  1.68s/it]11/15/2022 20:05:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:04 - INFO - train.train_snli_ve - loss is tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6416/16548 [2:58:45<4:45:50,  1.69s/it]11/15/2022 20:05:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:06 - INFO - train.train_snli_ve - loss is tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6417/16548 [2:58:46<4:44:54,  1.69s/it]11/15/2022 20:05:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:07 - INFO - train.train_snli_ve - loss is tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6418/16548 [2:58:48<4:44:01,  1.68s/it]11/15/2022 20:05:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:09 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6419/16548 [2:58:50<4:44:29,  1.69s/it]11/15/2022 20:05:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.2574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:11 - INFO - train.train_snli_ve - loss is tensor(0.4991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6420/16548 [2:58:51<4:43:37,  1.68s/it]11/15/2022 20:05:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1461e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:12 - INFO - train.train_snli_ve - loss is tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6421/16548 [2:58:53<4:42:05,  1.67s/it]11/15/2022 20:05:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7464e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:14 - INFO - train.train_snli_ve - loss is tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6422/16548 [2:58:55<4:42:18,  1.67s/it]11/15/2022 20:05:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:16 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6423/16548 [2:58:56<4:43:23,  1.68s/it]11/15/2022 20:05:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:17 - INFO - train.train_snli_ve - loss is tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6424/16548 [2:58:58<4:41:35,  1.67s/it]11/15/2022 20:05:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:19 - INFO - train.train_snli_ve - loss is tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6425/16548 [2:59:00<4:42:31,  1.67s/it]11/15/2022 20:05:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:21 - INFO - train.train_snli_ve - loss is tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6426/16548 [2:59:01<4:43:18,  1.68s/it]11/15/2022 20:05:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:22 - INFO - train.train_snli_ve - loss is tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6427/16548 [2:59:03<4:43:48,  1.68s/it]11/15/2022 20:05:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:24 - INFO - train.train_snli_ve - loss is tensor(0.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6428/16548 [2:59:05<4:42:51,  1.68s/it]11/15/2022 20:05:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:26 - INFO - train.train_snli_ve - loss is tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6429/16548 [2:59:06<4:41:40,  1.67s/it]11/15/2022 20:05:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:27 - INFO - train.train_snli_ve - loss is tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6430/16548 [2:59:08<4:41:54,  1.67s/it]11/15/2022 20:05:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:29 - INFO - train.train_snli_ve - loss is tensor(0.8170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6431/16548 [2:59:10<4:41:47,  1.67s/it]11/15/2022 20:05:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:31 - INFO - train.train_snli_ve - loss is tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6432/16548 [2:59:11<4:44:09,  1.69s/it]11/15/2022 20:05:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.0043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:32 - INFO - train.train_snli_ve - loss is tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6433/16548 [2:59:13<4:44:14,  1.69s/it]11/15/2022 20:05:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:34 - INFO - train.train_snli_ve - loss is tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6434/16548 [2:59:15<4:43:26,  1.68s/it]11/15/2022 20:05:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:36 - INFO - train.train_snli_ve - loss is tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6435/16548 [2:59:16<4:44:57,  1.69s/it]11/15/2022 20:05:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:37 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6436/16548 [2:59:18<4:45:15,  1.69s/it]11/15/2022 20:05:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:39 - INFO - train.train_snli_ve - loss is tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6437/16548 [2:59:20<4:45:02,  1.69s/it]11/15/2022 20:05:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.7911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:41 - INFO - train.train_snli_ve - loss is tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6438/16548 [2:59:22<4:45:06,  1.69s/it]11/15/2022 20:05:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:43 - INFO - train.train_snli_ve - loss is tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6439/16548 [2:59:23<4:45:43,  1.70s/it]11/15/2022 20:05:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.6218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:44 - INFO - train.train_snli_ve - loss is tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6440/16548 [2:59:25<4:43:39,  1.68s/it]11/15/2022 20:05:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.9617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:46 - INFO - train.train_snli_ve - loss is tensor(0.5433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6441/16548 [2:59:27<4:42:19,  1.68s/it]11/15/2022 20:05:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:47 - INFO - train.train_snli_ve - loss is tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6442/16548 [2:59:28<4:41:19,  1.67s/it]11/15/2022 20:05:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.3658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:49 - INFO - train.train_snli_ve - loss is tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6443/16548 [2:59:30<4:42:27,  1.68s/it]11/15/2022 20:05:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:51 - INFO - train.train_snli_ve - loss is tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6444/16548 [2:59:32<4:41:21,  1.67s/it]11/15/2022 20:05:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1778e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:53 - INFO - train.train_snli_ve - loss is tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6445/16548 [2:59:33<4:40:48,  1.67s/it]11/15/2022 20:05:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.9530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:54 - INFO - train.train_snli_ve - loss is tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6446/16548 [2:59:35<4:40:07,  1.66s/it]11/15/2022 20:05:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:56 - INFO - train.train_snli_ve - loss is tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6447/16548 [2:59:37<4:40:20,  1.67s/it]11/15/2022 20:05:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0230e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:57 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6448/16548 [2:59:38<4:40:05,  1.66s/it]11/15/2022 20:05:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.3824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:05:59 - INFO - train.train_snli_ve - loss is tensor(0.9291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6449/16548 [2:59:40<4:39:52,  1.66s/it]11/15/2022 20:06:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:01 - INFO - train.train_snli_ve - loss is tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6450/16548 [2:59:42<4:40:53,  1.67s/it]11/15/2022 20:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.1871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:03 - INFO - train.train_snli_ve - loss is tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6451/16548 [2:59:43<4:41:57,  1.68s/it]11/15/2022 20:06:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:04 - INFO - train.train_snli_ve - loss is tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6452/16548 [2:59:45<4:42:40,  1.68s/it]11/15/2022 20:06:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.4056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:06 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6453/16548 [2:59:47<4:43:02,  1.68s/it]11/15/2022 20:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7269e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:08 - INFO - train.train_snli_ve - loss is tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6454/16548 [2:59:48<4:44:23,  1.69s/it]11/15/2022 20:06:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8677e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:09 - INFO - train.train_snli_ve - loss is tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6455/16548 [2:59:50<4:44:53,  1.69s/it]11/15/2022 20:06:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:11 - INFO - train.train_snli_ve - loss is tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6456/16548 [2:59:52<4:45:09,  1.70s/it]11/15/2022 20:06:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:13 - INFO - train.train_snli_ve - loss is tensor(0.4390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6457/16548 [2:59:53<4:45:43,  1.70s/it]11/15/2022 20:06:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:14 - INFO - train.train_snli_ve - loss is tensor(0.9176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6458/16548 [2:59:55<4:43:20,  1.68s/it]11/15/2022 20:06:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:16 - INFO - train.train_snli_ve - loss is tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6459/16548 [2:59:57<4:45:15,  1.70s/it]11/15/2022 20:06:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:18 - INFO - train.train_snli_ve - loss is tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6460/16548 [2:59:59<4:44:43,  1.69s/it]11/15/2022 20:06:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:19 - INFO - train.train_snli_ve - loss is tensor(0.9293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6461/16548 [3:00:00<4:44:32,  1.69s/it]11/15/2022 20:06:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:21 - INFO - train.train_snli_ve - loss is tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6462/16548 [3:00:02<4:42:39,  1.68s/it]11/15/2022 20:06:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:23 - INFO - train.train_snli_ve - loss is tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6463/16548 [3:00:04<4:42:14,  1.68s/it]11/15/2022 20:06:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:25 - INFO - train.train_snli_ve - loss is tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6464/16548 [3:00:05<4:44:12,  1.69s/it]11/15/2022 20:06:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:26 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6465/16548 [3:00:07<4:43:15,  1.69s/it]11/15/2022 20:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.4604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:28 - INFO - train.train_snli_ve - loss is tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6466/16548 [3:00:09<4:45:28,  1.70s/it]11/15/2022 20:06:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.1229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:30 - INFO - train.train_snli_ve - loss is tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6467/16548 [3:00:10<4:46:21,  1.70s/it]11/15/2022 20:06:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:31 - INFO - train.train_snli_ve - loss is tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6468/16548 [3:00:12<4:43:25,  1.69s/it]11/15/2022 20:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.3040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:33 - INFO - train.train_snli_ve - loss is tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6469/16548 [3:00:14<4:42:06,  1.68s/it]11/15/2022 20:06:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:35 - INFO - train.train_snli_ve - loss is tensor(0.4677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6470/16548 [3:00:15<4:43:31,  1.69s/it]11/15/2022 20:06:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1565e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:36 - INFO - train.train_snli_ve - loss is tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6471/16548 [3:00:17<4:44:13,  1.69s/it]11/15/2022 20:06:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:38 - INFO - train.train_snli_ve - loss is tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6472/16548 [3:00:19<4:45:08,  1.70s/it]11/15/2022 20:06:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:40 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6473/16548 [3:00:20<4:44:04,  1.69s/it]11/15/2022 20:06:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.4557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:41 - INFO - train.train_snli_ve - loss is tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6474/16548 [3:00:22<4:41:36,  1.68s/it]11/15/2022 20:06:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:43 - INFO - train.train_snli_ve - loss is tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6475/16548 [3:00:24<4:40:25,  1.67s/it]11/15/2022 20:06:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.0785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:45 - INFO - train.train_snli_ve - loss is tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6476/16548 [3:00:25<4:40:00,  1.67s/it]11/15/2022 20:06:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:46 - INFO - train.train_snli_ve - loss is tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6477/16548 [3:00:27<4:38:45,  1.66s/it]11/15/2022 20:06:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:48 - INFO - train.train_snli_ve - loss is tensor(0.8402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6478/16548 [3:00:29<4:38:41,  1.66s/it]11/15/2022 20:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:50 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6479/16548 [3:00:30<4:40:52,  1.67s/it]11/15/2022 20:06:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:51 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6480/16548 [3:00:32<4:42:36,  1.68s/it]11/15/2022 20:06:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.7134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:53 - INFO - train.train_snli_ve - loss is tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6481/16548 [3:00:34<4:42:31,  1.68s/it]11/15/2022 20:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:55 - INFO - train.train_snli_ve - loss is tensor(0.4362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6482/16548 [3:00:36<4:41:57,  1.68s/it]11/15/2022 20:06:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:56 - INFO - train.train_snli_ve - loss is tensor(0.8227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6483/16548 [3:00:37<4:40:45,  1.67s/it]11/15/2022 20:06:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:06:58 - INFO - train.train_snli_ve - loss is tensor(0.5150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6484/16548 [3:00:39<4:39:40,  1.67s/it]11/15/2022 20:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6230e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:00 - INFO - train.train_snli_ve - loss is tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6485/16548 [3:00:40<4:39:15,  1.67s/it]11/15/2022 20:07:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:01 - INFO - train.train_snli_ve - loss is tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6486/16548 [3:00:42<4:40:12,  1.67s/it]11/15/2022 20:07:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:03 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6487/16548 [3:00:44<4:38:52,  1.66s/it]11/15/2022 20:07:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.8974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:05 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6488/16548 [3:00:46<4:43:11,  1.69s/it]11/15/2022 20:07:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:06 - INFO - train.train_snli_ve - loss is tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6489/16548 [3:00:47<4:41:00,  1.68s/it]11/15/2022 20:07:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:08 - INFO - train.train_snli_ve - loss is tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6490/16548 [3:00:49<4:40:40,  1.67s/it]11/15/2022 20:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:10 - INFO - train.train_snli_ve - loss is tensor(0.4440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6491/16548 [3:00:51<4:39:40,  1.67s/it]11/15/2022 20:07:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5327e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:11 - INFO - train.train_snli_ve - loss is tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6492/16548 [3:00:52<4:41:13,  1.68s/it]11/15/2022 20:07:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.1857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:13 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6493/16548 [3:00:54<4:41:25,  1.68s/it]11/15/2022 20:07:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:15 - INFO - train.train_snli_ve - loss is tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6494/16548 [3:00:56<4:42:38,  1.69s/it]11/15/2022 20:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:17 - INFO - train.train_snli_ve - loss is tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6495/16548 [3:00:57<4:39:51,  1.67s/it]11/15/2022 20:07:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:18 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6496/16548 [3:00:59<4:40:25,  1.67s/it]11/15/2022 20:07:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:20 - INFO - train.train_snli_ve - loss is tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6497/16548 [3:01:01<4:39:16,  1.67s/it]11/15/2022 20:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.0388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:22 - INFO - train.train_snli_ve - loss is tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6498/16548 [3:01:02<4:40:08,  1.67s/it]11/15/2022 20:07:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:23 - INFO - train.train_snli_ve - loss is tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6499/16548 [3:01:04<4:42:50,  1.69s/it]11/15/2022 20:07:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:25 - INFO - train.train_snli_ve - loss is tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6500/16548 [3:01:06<4:55:56,  1.77s/it]11/15/2022 20:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:27 - INFO - train.train_snli_ve - loss is tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6501/16548 [3:01:08<4:53:31,  1.75s/it]11/15/2022 20:07:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:29 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6502/16548 [3:01:09<4:47:57,  1.72s/it]11/15/2022 20:07:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3115e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:30 - INFO - train.train_snli_ve - loss is tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6503/16548 [3:01:11<4:45:28,  1.71s/it]11/15/2022 20:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:32 - INFO - train.train_snli_ve - loss is tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6504/16548 [3:01:13<4:42:38,  1.69s/it]11/15/2022 20:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0260e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:34 - INFO - train.train_snli_ve - loss is tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6505/16548 [3:01:14<4:41:04,  1.68s/it]11/15/2022 20:07:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:35 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6506/16548 [3:01:16<4:40:29,  1.68s/it]11/15/2022 20:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:37 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6507/16548 [3:01:18<4:39:05,  1.67s/it]11/15/2022 20:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:39 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6508/16548 [3:01:19<4:40:24,  1.68s/it]11/15/2022 20:07:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:40 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6509/16548 [3:01:21<4:42:56,  1.69s/it]11/15/2022 20:07:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.2619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:42 - INFO - train.train_snli_ve - loss is tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6510/16548 [3:01:23<4:43:48,  1.70s/it]11/15/2022 20:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.3957e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:44 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6511/16548 [3:01:24<4:42:11,  1.69s/it]11/15/2022 20:07:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:45 - INFO - train.train_snli_ve - loss is tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6512/16548 [3:01:26<4:43:27,  1.69s/it]11/15/2022 20:07:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7525e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:47 - INFO - train.train_snli_ve - loss is tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6513/16548 [3:01:28<4:42:26,  1.69s/it]11/15/2022 20:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.6461e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:49 - INFO - train.train_snli_ve - loss is tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6514/16548 [3:01:30<4:43:32,  1.70s/it]11/15/2022 20:07:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:50 - INFO - train.train_snli_ve - loss is tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6515/16548 [3:01:31<4:40:22,  1.68s/it]11/15/2022 20:07:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:52 - INFO - train.train_snli_ve - loss is tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6516/16548 [3:01:33<4:39:07,  1.67s/it]11/15/2022 20:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6704e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:54 - INFO - train.train_snli_ve - loss is tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6517/16548 [3:01:34<4:38:33,  1.67s/it]11/15/2022 20:07:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.4986e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:55 - INFO - train.train_snli_ve - loss is tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6518/16548 [3:01:36<4:36:48,  1.66s/it]11/15/2022 20:07:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:57 - INFO - train.train_snli_ve - loss is tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6519/16548 [3:01:38<4:39:08,  1.67s/it]11/15/2022 20:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:07:59 - INFO - train.train_snli_ve - loss is tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6520/16548 [3:01:39<4:39:37,  1.67s/it]11/15/2022 20:08:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.1514e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:00 - INFO - train.train_snli_ve - loss is tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6521/16548 [3:01:41<4:37:43,  1.66s/it]11/15/2022 20:08:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.7371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:02 - INFO - train.train_snli_ve - loss is tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6522/16548 [3:01:43<4:39:39,  1.67s/it]11/15/2022 20:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:04 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6523/16548 [3:01:44<4:38:48,  1.67s/it]11/15/2022 20:08:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.5200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:05 - INFO - train.train_snli_ve - loss is tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6524/16548 [3:01:46<4:38:25,  1.67s/it]11/15/2022 20:08:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7710e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:07 - INFO - train.train_snli_ve - loss is tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6525/16548 [3:01:48<4:39:22,  1.67s/it]11/15/2022 20:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.9191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:09 - INFO - train.train_snli_ve - loss is tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6526/16548 [3:01:50<4:42:39,  1.69s/it]11/15/2022 20:08:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:10 - INFO - train.train_snli_ve - loss is tensor(0.5146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6527/16548 [3:01:51<4:42:44,  1.69s/it]11/15/2022 20:08:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:12 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6528/16548 [3:01:53<4:43:37,  1.70s/it]11/15/2022 20:08:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:14 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6529/16548 [3:01:55<4:41:41,  1.69s/it]11/15/2022 20:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.9238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:16 - INFO - train.train_snli_ve - loss is tensor(0.4353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6530/16548 [3:01:56<4:39:59,  1.68s/it]11/15/2022 20:08:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:17 - INFO - train.train_snli_ve - loss is tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6531/16548 [3:01:58<4:38:47,  1.67s/it]11/15/2022 20:08:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.6838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:19 - INFO - train.train_snli_ve - loss is tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6532/16548 [3:02:00<4:38:26,  1.67s/it]11/15/2022 20:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:21 - INFO - train.train_snli_ve - loss is tensor(0.4602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6533/16548 [3:02:01<4:40:32,  1.68s/it]11/15/2022 20:08:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.8659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:22 - INFO - train.train_snli_ve - loss is tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6534/16548 [3:02:03<4:40:54,  1.68s/it]11/15/2022 20:08:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:24 - INFO - train.train_snli_ve - loss is tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6535/16548 [3:02:05<4:41:14,  1.69s/it]11/15/2022 20:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:26 - INFO - train.train_snli_ve - loss is tensor(0.8749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39% 6536/16548 [3:02:06<4:39:26,  1.67s/it]11/15/2022 20:08:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.0694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:27 - INFO - train.train_snli_ve - loss is tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6537/16548 [3:02:08<4:40:09,  1.68s/it]11/15/2022 20:08:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:29 - INFO - train.train_snli_ve - loss is tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6538/16548 [3:02:10<4:40:26,  1.68s/it]11/15/2022 20:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.8493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:31 - INFO - train.train_snli_ve - loss is tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6539/16548 [3:02:11<4:39:15,  1.67s/it]11/15/2022 20:08:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:32 - INFO - train.train_snli_ve - loss is tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6540/16548 [3:02:13<4:41:17,  1.69s/it]11/15/2022 20:08:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.7560e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:34 - INFO - train.train_snli_ve - loss is tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6541/16548 [3:02:15<4:38:43,  1.67s/it]11/15/2022 20:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4004e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:36 - INFO - train.train_snli_ve - loss is tensor(1.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6542/16548 [3:02:16<4:38:59,  1.67s/it]11/15/2022 20:08:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:37 - INFO - train.train_snli_ve - loss is tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6543/16548 [3:02:18<4:37:37,  1.66s/it]11/15/2022 20:08:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.9763e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:39 - INFO - train.train_snli_ve - loss is tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6544/16548 [3:02:20<4:36:24,  1.66s/it]11/15/2022 20:08:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:41 - INFO - train.train_snli_ve - loss is tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6545/16548 [3:02:21<4:37:13,  1.66s/it]11/15/2022 20:08:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:42 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6546/16548 [3:02:23<4:38:20,  1.67s/it]11/15/2022 20:08:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:44 - INFO - train.train_snli_ve - loss is tensor(0.8439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6547/16548 [3:02:25<4:38:01,  1.67s/it]11/15/2022 20:08:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.3535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:46 - INFO - train.train_snli_ve - loss is tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6548/16548 [3:02:26<4:39:12,  1.68s/it]11/15/2022 20:08:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:47 - INFO - train.train_snli_ve - loss is tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6549/16548 [3:02:28<4:37:35,  1.67s/it]11/15/2022 20:08:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:49 - INFO - train.train_snli_ve - loss is tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6550/16548 [3:02:30<4:36:25,  1.66s/it]11/15/2022 20:08:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:51 - INFO - train.train_snli_ve - loss is tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6551/16548 [3:02:31<4:38:23,  1.67s/it]11/15/2022 20:08:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:52 - INFO - train.train_snli_ve - loss is tensor(0.9508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6552/16548 [3:02:33<4:38:09,  1.67s/it]11/15/2022 20:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.9994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:54 - INFO - train.train_snli_ve - loss is tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6553/16548 [3:02:35<4:37:40,  1.67s/it]11/15/2022 20:08:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5572e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:56 - INFO - train.train_snli_ve - loss is tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6554/16548 [3:02:36<4:39:56,  1.68s/it]11/15/2022 20:08:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:57 - INFO - train.train_snli_ve - loss is tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6555/16548 [3:02:38<4:40:37,  1.68s/it]11/15/2022 20:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:08:59 - INFO - train.train_snli_ve - loss is tensor(1.2580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6556/16548 [3:02:40<4:39:09,  1.68s/it]11/15/2022 20:09:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:01 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6557/16548 [3:02:41<4:39:27,  1.68s/it]11/15/2022 20:09:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:02 - INFO - train.train_snli_ve - loss is tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6558/16548 [3:02:43<4:39:21,  1.68s/it]11/15/2022 20:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:04 - INFO - train.train_snli_ve - loss is tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6559/16548 [3:02:45<4:40:27,  1.68s/it]11/15/2022 20:09:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:06 - INFO - train.train_snli_ve - loss is tensor(0.8754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6560/16548 [3:02:46<4:39:51,  1.68s/it]11/15/2022 20:09:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:07 - INFO - train.train_snli_ve - loss is tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6561/16548 [3:02:48<4:40:59,  1.69s/it]11/15/2022 20:09:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:09 - INFO - train.train_snli_ve - loss is tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6562/16548 [3:02:50<4:40:50,  1.69s/it]11/15/2022 20:09:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:11 - INFO - train.train_snli_ve - loss is tensor(0.7801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6563/16548 [3:02:52<4:41:09,  1.69s/it]11/15/2022 20:09:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:13 - INFO - train.train_snli_ve - loss is tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6564/16548 [3:02:53<4:40:44,  1.69s/it]11/15/2022 20:09:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:14 - INFO - train.train_snli_ve - loss is tensor(0.9002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6565/16548 [3:02:55<4:42:50,  1.70s/it]11/15/2022 20:09:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1532e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:16 - INFO - train.train_snli_ve - loss is tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6566/16548 [3:02:57<4:41:42,  1.69s/it]11/15/2022 20:09:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.3480e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:18 - INFO - train.train_snli_ve - loss is tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6567/16548 [3:02:58<4:40:50,  1.69s/it]11/15/2022 20:09:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:19 - INFO - train.train_snli_ve - loss is tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6568/16548 [3:03:00<4:43:07,  1.70s/it]11/15/2022 20:09:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.4944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:21 - INFO - train.train_snli_ve - loss is tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6569/16548 [3:03:02<4:41:10,  1.69s/it]11/15/2022 20:09:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:23 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6570/16548 [3:03:03<4:42:15,  1.70s/it]11/15/2022 20:09:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.9263e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:24 - INFO - train.train_snli_ve - loss is tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6571/16548 [3:03:05<4:39:10,  1.68s/it]11/15/2022 20:09:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:26 - INFO - train.train_snli_ve - loss is tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6572/16548 [3:03:07<4:39:29,  1.68s/it]11/15/2022 20:09:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.7334e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:28 - INFO - train.train_snli_ve - loss is tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6573/16548 [3:03:08<4:36:49,  1.67s/it]11/15/2022 20:09:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.0335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:29 - INFO - train.train_snli_ve - loss is tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6574/16548 [3:03:10<4:37:32,  1.67s/it]11/15/2022 20:09:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.7102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:31 - INFO - train.train_snli_ve - loss is tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6575/16548 [3:03:12<4:35:49,  1.66s/it]11/15/2022 20:09:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:33 - INFO - train.train_snli_ve - loss is tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6576/16548 [3:03:13<4:37:18,  1.67s/it]11/15/2022 20:09:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:34 - INFO - train.train_snli_ve - loss is tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6577/16548 [3:03:15<4:37:56,  1.67s/it]11/15/2022 20:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:36 - INFO - train.train_snli_ve - loss is tensor(0.8478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6578/16548 [3:03:17<4:36:23,  1.66s/it]11/15/2022 20:09:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0707e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:38 - INFO - train.train_snli_ve - loss is tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6579/16548 [3:03:18<4:35:41,  1.66s/it]11/15/2022 20:09:39 - INFO - train.train_snli_ve - kd_loss is tensor(9.6908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:39 - INFO - train.train_snli_ve - loss is tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6580/16548 [3:03:20<4:37:26,  1.67s/it]11/15/2022 20:09:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.2334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:41 - INFO - train.train_snli_ve - loss is tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6581/16548 [3:03:22<4:37:07,  1.67s/it]11/15/2022 20:09:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.1377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:43 - INFO - train.train_snli_ve - loss is tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6582/16548 [3:03:23<4:35:51,  1.66s/it]11/15/2022 20:09:44 - INFO - train.train_snli_ve - kd_loss is tensor(9.7394e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:44 - INFO - train.train_snli_ve - loss is tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6583/16548 [3:03:25<4:37:15,  1.67s/it]11/15/2022 20:09:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:46 - INFO - train.train_snli_ve - loss is tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6584/16548 [3:03:27<4:36:57,  1.67s/it]11/15/2022 20:09:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:48 - INFO - train.train_snli_ve - loss is tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6585/16548 [3:03:28<4:36:10,  1.66s/it]11/15/2022 20:09:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1571e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:49 - INFO - train.train_snli_ve - loss is tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6586/16548 [3:03:30<4:35:45,  1.66s/it]11/15/2022 20:09:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:51 - INFO - train.train_snli_ve - loss is tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6587/16548 [3:03:32<4:36:54,  1.67s/it]11/15/2022 20:09:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9503e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:53 - INFO - train.train_snli_ve - loss is tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6588/16548 [3:03:33<4:36:19,  1.66s/it]11/15/2022 20:09:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:54 - INFO - train.train_snli_ve - loss is tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6589/16548 [3:03:35<4:36:47,  1.67s/it]11/15/2022 20:09:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.8631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:56 - INFO - train.train_snli_ve - loss is tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6590/16548 [3:03:37<4:35:15,  1.66s/it]11/15/2022 20:09:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:58 - INFO - train.train_snli_ve - loss is tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6591/16548 [3:03:38<4:36:41,  1.67s/it]11/15/2022 20:09:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:09:59 - INFO - train.train_snli_ve - loss is tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6592/16548 [3:03:40<4:34:20,  1.65s/it]11/15/2022 20:10:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.3785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:01 - INFO - train.train_snli_ve - loss is tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6593/16548 [3:03:42<4:36:04,  1.66s/it]11/15/2022 20:10:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:03 - INFO - train.train_snli_ve - loss is tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6594/16548 [3:03:43<4:39:11,  1.68s/it]11/15/2022 20:10:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:04 - INFO - train.train_snli_ve - loss is tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6595/16548 [3:03:45<4:39:23,  1.68s/it]11/15/2022 20:10:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2560e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:06 - INFO - train.train_snli_ve - loss is tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6596/16548 [3:03:47<4:38:21,  1.68s/it]11/15/2022 20:10:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.1753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:08 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6597/16548 [3:03:48<4:37:38,  1.67s/it]11/15/2022 20:10:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:09 - INFO - train.train_snli_ve - loss is tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6598/16548 [3:03:50<4:38:55,  1.68s/it]11/15/2022 20:10:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.4348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:11 - INFO - train.train_snli_ve - loss is tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6599/16548 [3:03:52<4:37:27,  1.67s/it]11/15/2022 20:10:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.8864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:13 - INFO - train.train_snli_ve - loss is tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6600/16548 [3:03:54<4:45:52,  1.72s/it]11/15/2022 20:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.2752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:15 - INFO - train.train_snli_ve - loss is tensor(0.5382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6601/16548 [3:03:55<4:41:45,  1.70s/it]11/15/2022 20:10:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:16 - INFO - train.train_snli_ve - loss is tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6602/16548 [3:03:57<4:41:38,  1.70s/it]11/15/2022 20:10:18 - INFO - train.train_snli_ve - kd_loss is tensor(8.7883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:18 - INFO - train.train_snli_ve - loss is tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6603/16548 [3:03:59<4:42:38,  1.71s/it]11/15/2022 20:10:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:20 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6604/16548 [3:04:00<4:40:23,  1.69s/it]11/15/2022 20:10:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.3333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:21 - INFO - train.train_snli_ve - loss is tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6605/16548 [3:04:02<4:36:38,  1.67s/it]11/15/2022 20:10:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:23 - INFO - train.train_snli_ve - loss is tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6606/16548 [3:04:04<4:35:37,  1.66s/it]11/15/2022 20:10:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.3670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:25 - INFO - train.train_snli_ve - loss is tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6607/16548 [3:04:05<4:37:51,  1.68s/it]11/15/2022 20:10:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:26 - INFO - train.train_snli_ve - loss is tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6608/16548 [3:04:07<4:39:09,  1.69s/it]11/15/2022 20:10:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:28 - INFO - train.train_snli_ve - loss is tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6609/16548 [3:04:09<4:38:27,  1.68s/it]11/15/2022 20:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:30 - INFO - train.train_snli_ve - loss is tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6610/16548 [3:04:10<4:38:56,  1.68s/it]11/15/2022 20:10:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:31 - INFO - train.train_snli_ve - loss is tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6611/16548 [3:04:12<4:39:21,  1.69s/it]11/15/2022 20:10:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.8091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:33 - INFO - train.train_snli_ve - loss is tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6612/16548 [3:04:14<4:37:59,  1.68s/it]11/15/2022 20:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:35 - INFO - train.train_snli_ve - loss is tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6613/16548 [3:04:15<4:39:30,  1.69s/it]11/15/2022 20:10:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:36 - INFO - train.train_snli_ve - loss is tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6614/16548 [3:04:17<4:38:49,  1.68s/it]11/15/2022 20:10:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:38 - INFO - train.train_snli_ve - loss is tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6615/16548 [3:04:19<4:40:28,  1.69s/it]11/15/2022 20:10:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:40 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6616/16548 [3:04:21<4:40:54,  1.70s/it]11/15/2022 20:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.3590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:42 - INFO - train.train_snli_ve - loss is tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6617/16548 [3:04:22<4:41:24,  1.70s/it]11/15/2022 20:10:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:43 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6618/16548 [3:04:24<4:38:54,  1.69s/it]11/15/2022 20:10:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:45 - INFO - train.train_snli_ve - loss is tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6619/16548 [3:04:26<4:38:51,  1.69s/it]11/15/2022 20:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.2390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:47 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6620/16548 [3:04:27<4:39:43,  1.69s/it]11/15/2022 20:10:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.3733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:48 - INFO - train.train_snli_ve - loss is tensor(0.8393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6621/16548 [3:04:29<4:37:20,  1.68s/it]11/15/2022 20:10:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:50 - INFO - train.train_snli_ve - loss is tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6622/16548 [3:04:31<4:38:17,  1.68s/it]11/15/2022 20:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1519e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:52 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6623/16548 [3:04:32<4:39:18,  1.69s/it]11/15/2022 20:10:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:53 - INFO - train.train_snli_ve - loss is tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6624/16548 [3:04:34<4:39:00,  1.69s/it]11/15/2022 20:10:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.2243e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:55 - INFO - train.train_snli_ve - loss is tensor(0.5011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6625/16548 [3:04:36<4:37:47,  1.68s/it]11/15/2022 20:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.3334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:57 - INFO - train.train_snli_ve - loss is tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6626/16548 [3:04:37<4:38:46,  1.69s/it]11/15/2022 20:10:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:10:58 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6627/16548 [3:04:39<4:39:40,  1.69s/it]11/15/2022 20:11:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4650e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:11:00 - INFO - train.train_snli_ve - loss is tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6628/16548 [3:04:41<4:38:41,  1.69s/it]11/15/2022 20:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.5688e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 20:11:02 - INFO - train.train_snli_ve - loss is tensor(0.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40% 6629/16548 [3:04:42<4:37:54,  1.68s/it]