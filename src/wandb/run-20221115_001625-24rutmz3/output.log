11/15/2022 00:16:26 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 00:16:26 - INFO - __main__ - Training models on Vision-Language continual learning tasks...
11/15/2022 00:16:26 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 00:16:26 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/15/2022 00:16:26 - INFO - __main__ - Training vilt model on task #1: VQAv2
11/15/2022 00:16:27 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/15/2022 00:16:29 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 443757 examples
11/15/2022 00:16:29 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/15/2022 00:16:30 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 214354 examples
Creating DyTox!
Training epoch 1:   0% 0/13868 [00:00<?, ?it/s]Training epoch 1:   0% 1/13868 [00:02<8:07:37,  2.11s/it]Training epoch 1:   0% 2/13868 [00:02<4:09:50,  1.08s/it]Training epoch 1:   0% 3/13868 [00:02<3:03:26,  1.26it/s]Training epoch 1:   0% 4/13868 [00:03<2:27:01,  1.57it/s]Training epoch 1:   0% 5/13868 [00:03<2:06:32,  1.83it/s]Training epoch 1:   0% 6/13868 [00:04<1:55:39,  2.00it/s]Training epoch 1:   0% 7/13868 [00:04<1:49:46,  2.10it/s]Training epoch 1:   0% 8/13868 [00:05<1:49:06,  2.12it/s]Training epoch 1:   0% 9/13868 [00:05<1:45:24,  2.19it/s]Training epoch 1:   0% 10/13868 [00:05<1:43:00,  2.24it/s]Training epoch 1:   0% 11/13868 [00:06<1:39:49,  2.31it/s]Training epoch 1:   0% 12/13868 [00:06<1:38:20,  2.35it/s]Training epoch 1:   0% 13/13868 [00:07<1:35:44,  2.41it/s]Training epoch 1:   0% 14/13868 [00:07<1:36:27,  2.39it/s]Training epoch 1:   0% 15/13868 [00:07<1:35:56,  2.41it/s]Training epoch 1:   0% 16/13868 [00:08<1:34:19,  2.45it/s]Training epoch 1:   0% 17/13868 [00:08<1:33:45,  2.46it/s]Training epoch 1:   0% 18/13868 [00:09<1:34:12,  2.45it/s]Training epoch 1:   0% 19/13868 [00:09<1:34:10,  2.45it/s]Training epoch 1:   0% 20/13868 [00:09<1:34:41,  2.44it/s]Training epoch 1:   0% 21/13868 [00:10<1:34:34,  2.44it/s]Training epoch 1:   0% 22/13868 [00:10<1:34:00,  2.45it/s]Training epoch 1:   0% 23/13868 [00:11<1:31:21,  2.53it/s]Training epoch 1:   0% 24/13868 [00:11<1:32:40,  2.49it/s]Training epoch 1:   0% 25/13868 [00:11<1:32:48,  2.49it/s]Training epoch 1:   0% 26/13868 [00:12<1:32:31,  2.49it/s]Training epoch 1:   0% 27/13868 [00:12<1:31:45,  2.51it/s]Training epoch 1:   0% 28/13868 [00:13<1:31:39,  2.52it/s]Training epoch 1:   0% 29/13868 [00:13<1:31:55,  2.51it/s]Training epoch 1:   0% 30/13868 [00:13<1:33:22,  2.47it/s]Training epoch 1:   0% 31/13868 [00:14<1:32:23,  2.50it/s]Training epoch 1:   0% 32/13868 [00:14<1:31:45,  2.51it/s]Training epoch 1:   0% 33/13868 [00:15<1:33:04,  2.48it/s]Training epoch 1:   0% 34/13868 [00:15<1:31:59,  2.51it/s]Training epoch 1:   0% 35/13868 [00:15<1:33:35,  2.46it/s]Training epoch 1:   0% 36/13868 [00:16<1:35:40,  2.41it/s]Training epoch 1:   0% 37/13868 [00:16<1:34:06,  2.45it/s]Training epoch 1:   0% 38/13868 [00:17<1:30:57,  2.53it/s]Training epoch 1:   0% 39/13868 [00:17<1:32:17,  2.50it/s]Training epoch 1:   0% 40/13868 [00:17<1:33:57,  2.45it/s]Training epoch 1:   0% 41/13868 [00:18<1:33:51,  2.46it/s]Training epoch 1:   0% 42/13868 [00:18<1:33:11,  2.47it/s]Training epoch 1:   0% 43/13868 [00:19<1:33:18,  2.47it/s]Training epoch 1:   0% 44/13868 [00:19<1:34:14,  2.44it/s]Training epoch 1:   0% 45/13868 [00:19<1:32:31,  2.49it/s]Training epoch 1:   0% 46/13868 [00:20<1:33:17,  2.47it/s]Training epoch 1:   0% 47/13868 [00:20<1:32:34,  2.49it/s]Training epoch 1:   0% 48/13868 [00:21<1:32:46,  2.48it/s]Training epoch 1:   0% 49/13868 [00:21<1:31:26,  2.52it/s]Training epoch 1:   0% 50/13868 [00:22<1:34:04,  2.45it/s]Training epoch 1:   0% 51/13868 [00:22<1:34:45,  2.43it/s]Training epoch 1:   0% 52/13868 [00:22<1:33:42,  2.46it/s]Training epoch 1:   0% 53/13868 [00:23<1:35:22,  2.41it/s]Training epoch 1:   0% 54/13868 [00:23<1:35:05,  2.42it/s]Training epoch 1:   0% 55/13868 [00:24<1:35:36,  2.41it/s]Training epoch 1:   0% 56/13868 [00:24<1:34:16,  2.44it/s]Training epoch 1:   0% 57/13868 [00:24<1:33:16,  2.47it/s]Training epoch 1:   0% 58/13868 [00:25<1:33:26,  2.46it/s]Training epoch 1:   0% 59/13868 [00:25<1:32:30,  2.49it/s]Training epoch 1:   0% 60/13868 [00:26<1:33:00,  2.47it/s]Training epoch 1:   0% 61/13868 [00:26<1:33:40,  2.46it/s]Training epoch 1:   0% 62/13868 [00:26<1:31:32,  2.51it/s]Training epoch 1:   0% 63/13868 [00:27<1:32:05,  2.50it/s]Training epoch 1:   0% 64/13868 [00:27<1:32:32,  2.49it/s]Training epoch 1:   0% 65/13868 [00:28<1:32:45,  2.48it/s]Training epoch 1:   0% 66/13868 [00:28<1:31:23,  2.52it/s]Training epoch 1:   0% 67/13868 [00:28<1:31:08,  2.52it/s]Training epoch 1:   0% 68/13868 [00:29<1:32:03,  2.50it/s]Training epoch 1:   0% 69/13868 [00:29<1:31:56,  2.50it/s]Training epoch 1:   1% 70/13868 [00:30<1:30:49,  2.53it/s]Training epoch 1:   1% 71/13868 [00:30<1:29:39,  2.56it/s]Training epoch 1:   1% 72/13868 [00:30<1:30:27,  2.54it/s]Training epoch 1:   1% 73/13868 [00:31<1:30:11,  2.55it/s]Training epoch 1:   1% 74/13868 [00:31<1:30:06,  2.55it/s]Training epoch 1:   1% 75/13868 [00:32<1:32:54,  2.47it/s]Training epoch 1:   1% 76/13868 [00:32<1:35:22,  2.41it/s]Training epoch 1:   1% 77/13868 [00:32<1:34:04,  2.44it/s]Training epoch 1:   1% 78/13868 [00:33<1:35:52,  2.40it/s]Training epoch 1:   1% 79/13868 [00:33<1:35:07,  2.42it/s]Training epoch 1:   1% 80/13868 [00:34<1:33:42,  2.45it/s]Training epoch 1:   1% 81/13868 [00:34<1:33:06,  2.47it/s]Training epoch 1:   1% 82/13868 [00:34<1:33:47,  2.45it/s]Training epoch 1:   1% 83/13868 [00:35<1:33:42,  2.45it/s]Training epoch 1:   1% 84/13868 [00:35<1:32:44,  2.48it/s]Training epoch 1:   1% 85/13868 [00:36<1:32:04,  2.49it/s]Training epoch 1:   1% 86/13868 [00:36<1:32:37,  2.48it/s]Training epoch 1:   1% 87/13868 [00:36<1:31:57,  2.50it/s]Training epoch 1:   1% 88/13868 [00:37<1:31:47,  2.50it/s]Training epoch 1:   1% 89/13868 [00:37<1:30:21,  2.54it/s]Training epoch 1:   1% 90/13868 [00:38<1:30:12,  2.55it/s]Training epoch 1:   1% 91/13868 [00:38<1:30:04,  2.55it/s]Training epoch 1:   1% 92/13868 [00:38<1:31:12,  2.52it/s]Training epoch 1:   1% 93/13868 [00:39<1:30:29,  2.54it/s]Training epoch 1:   1% 94/13868 [00:39<1:29:25,  2.57it/s]Training epoch 1:   1% 95/13868 [00:40<1:30:00,  2.55it/s]Training epoch 1:   1% 96/13868 [00:40<1:29:18,  2.57it/s]Training epoch 1:   1% 97/13868 [00:40<1:28:21,  2.60it/s]Training epoch 1:   1% 98/13868 [00:41<1:29:46,  2.56it/s]Training epoch 1:   1% 99/13868 [00:41<1:30:32,  2.53it/s]Training epoch 1:   1% 100/13868 [00:42<1:38:09,  2.34it/s]Training epoch 1:   1% 100/13868 [00:42<1:37:45,  2.35it/s]
Evaluating on VQA val set:   0% 0/6699 [00:00<?, ?it/s]Evaluating on VQA val set:   0% 1/6699 [00:00<1:43:11,  1.08it/s]Evaluating on VQA val set:   0% 2/6699 [00:01<1:11:03,  1.57it/s]Evaluating on VQA val set:   0% 3/6699 [00:01<1:00:05,  1.86it/s]Evaluating on VQA val set:   0% 4/6699 [00:02<54:21,  2.05it/s]  Evaluating on VQA val set:   0% 5/6699 [00:02<50:32,  2.21it/s]Evaluating on VQA val set:   0% 6/6699 [00:02<47:01,  2.37it/s]Evaluating on VQA val set:   0% 7/6699 [00:03<48:17,  2.31it/s]Evaluating on VQA val set:   0% 8/6699 [00:03<46:32,  2.40it/s]Evaluating on VQA val set:   0% 9/6699 [00:04<46:18,  2.41it/s]Evaluating on VQA val set:   0% 10/6699 [00:04<43:43,  2.55it/s]Evaluating on VQA val set:   0% 11/6699 [00:04<43:56,  2.54it/s]Evaluating on VQA val set:   0% 12/6699 [00:05<42:38,  2.61it/s]Evaluating on VQA val set:   0% 13/6699 [00:05<43:47,  2.54it/s]Evaluating on VQA val set:   0% 14/6699 [00:06<44:34,  2.50it/s]Evaluating on VQA val set:   0% 15/6699 [00:06<43:38,  2.55it/s]Evaluating on VQA val set:   0% 16/6699 [00:06<43:57,  2.53it/s]Evaluating on VQA val set:   0% 17/6699 [00:07<42:51,  2.60it/s]Evaluating on VQA val set:   0% 18/6699 [00:07<42:22,  2.63it/s]Evaluating on VQA val set:   0% 19/6699 [00:08<42:27,  2.62it/s]Evaluating on VQA val set:   0% 20/6699 [00:08<44:52,  2.48it/s]Evaluating on VQA val set:   0% 21/6699 [00:08<45:01,  2.47it/s]Evaluating on VQA val set:   0% 22/6699 [00:09<44:31,  2.50it/s]Evaluating on VQA val set:   0% 23/6699 [00:09<43:41,  2.55it/s]Evaluating on VQA val set:   0% 24/6699 [00:10<43:34,  2.55it/s]Evaluating on VQA val set:   0% 25/6699 [00:10<44:29,  2.50it/s]Evaluating on VQA val set:   0% 26/6699 [00:10<45:19,  2.45it/s]Evaluating on VQA val set:   0% 27/6699 [00:11<45:55,  2.42it/s]Evaluating on VQA val set:   0% 28/6699 [00:11<45:18,  2.45it/s]Evaluating on VQA val set:   0% 29/6699 [00:12<45:12,  2.46it/s]Evaluating on VQA val set:   0% 30/6699 [00:12<43:38,  2.55it/s]Evaluating on VQA val set:   0% 31/6699 [00:12<41:37,  2.67it/s]Evaluating on VQA val set:   0% 32/6699 [00:13<41:32,  2.67it/s]Evaluating on VQA val set:   0% 33/6699 [00:13<43:03,  2.58it/s]Evaluating on VQA val set:   1% 34/6699 [00:13<42:24,  2.62it/s]Evaluating on VQA val set:   1% 35/6699 [00:14<41:50,  2.65it/s]Evaluating on VQA val set:   1% 36/6699 [00:14<41:49,  2.65it/s]Evaluating on VQA val set:   1% 37/6699 [00:15<43:12,  2.57it/s]Evaluating on VQA val set:   1% 38/6699 [00:15<43:52,  2.53it/s]Evaluating on VQA val set:   1% 39/6699 [00:15<43:54,  2.53it/s]Evaluating on VQA val set:   1% 40/6699 [00:16<45:01,  2.46it/s]Evaluating on VQA val set:   1% 41/6699 [00:16<43:48,  2.53it/s]Evaluating on VQA val set:   1% 42/6699 [00:17<43:16,  2.56it/s]Evaluating on VQA val set:   1% 43/6699 [00:17<43:57,  2.52it/s]Evaluating on VQA val set:   1% 44/6699 [00:17<44:37,  2.49it/s]Evaluating on VQA val set:   1% 45/6699 [00:18<44:23,  2.50it/s]Evaluating on VQA val set:   1% 46/6699 [00:18<43:37,  2.54it/s]Evaluating on VQA val set:   1% 47/6699 [00:19<44:04,  2.52it/s]Evaluating on VQA val set:   1% 48/6699 [00:19<43:22,  2.56it/s]Evaluating on VQA val set:   1% 49/6699 [00:19<43:55,  2.52it/s]Evaluating on VQA val set:   1% 50/6699 [00:20<42:17,  2.62it/s]Evaluating on VQA val set:   1% 51/6699 [00:20<43:20,  2.56it/s]Evaluating on VQA val set:   1% 52/6699 [00:21<43:09,  2.57it/s]Evaluating on VQA val set:   1% 53/6699 [00:21<44:03,  2.51it/s]Evaluating on VQA val set:   1% 54/6699 [00:21<44:31,  2.49it/s]Evaluating on VQA val set:   1% 55/6699 [00:22<46:07,  2.40it/s]Evaluating on VQA val set:   1% 56/6699 [00:22<46:02,  2.40it/s]Evaluating on VQA val set:   1% 57/6699 [00:23<45:07,  2.45it/s]Evaluating on VQA val set:   1% 58/6699 [00:23<45:04,  2.46it/s]Evaluating on VQA val set:   1% 59/6699 [00:23<44:43,  2.47it/s]Evaluating on VQA val set:   1% 60/6699 [00:24<43:32,  2.54it/s]Evaluating on VQA val set:   1% 61/6699 [00:24<45:27,  2.43it/s]Evaluating on VQA val set:   1% 62/6699 [00:25<45:10,  2.45it/s]Evaluating on VQA val set:   1% 63/6699 [00:25<44:46,  2.47it/s]Evaluating on VQA val set:   1% 64/6699 [00:25<46:01,  2.40it/s]Evaluating on VQA val set:   1% 65/6699 [00:26<46:15,  2.39it/s]Evaluating on VQA val set:   1% 66/6699 [00:26<47:10,  2.34it/s]Evaluating on VQA val set:   1% 67/6699 [00:27<44:59,  2.46it/s]Evaluating on VQA val set:   1% 68/6699 [00:27<44:13,  2.50it/s]Evaluating on VQA val set:   1% 69/6699 [00:28<44:42,  2.47it/s]Evaluating on VQA val set:   1% 70/6699 [00:28<43:36,  2.53it/s]Evaluating on VQA val set:   1% 71/6699 [00:28<44:14,  2.50it/s]Evaluating on VQA val set:   1% 72/6699 [00:29<44:03,  2.51it/s]Evaluating on VQA val set:   1% 73/6699 [00:29<43:11,  2.56it/s]Evaluating on VQA val set:   1% 74/6699 [00:29<44:21,  2.49it/s]Evaluating on VQA val set:   1% 75/6699 [00:30<44:34,  2.48it/s]Evaluating on VQA val set:   1% 76/6699 [00:30<45:34,  2.42it/s]Evaluating on VQA val set:   1% 77/6699 [00:31<46:23,  2.38it/s]Evaluating on VQA val set:   1% 78/6699 [00:31<44:24,  2.48it/s]Evaluating on VQA val set:   1% 79/6699 [00:31<42:30,  2.60it/s]Evaluating on VQA val set:   1% 80/6699 [00:32<43:12,  2.55it/s]Evaluating on VQA val set:   1% 81/6699 [00:32<43:03,  2.56it/s]Evaluating on VQA val set:   1% 82/6699 [00:33<43:22,  2.54it/s]Evaluating on VQA val set:   1% 83/6699 [00:33<42:23,  2.60it/s]Evaluating on VQA val set:   1% 84/6699 [00:33<43:53,  2.51it/s]Evaluating on VQA val set:   1% 85/6699 [00:34<42:29,  2.59it/s]Evaluating on VQA val set:   1% 86/6699 [00:34<42:23,  2.60it/s]Evaluating on VQA val set:   1% 87/6699 [00:35<39:39,  2.78it/s]Evaluating on VQA val set:   1% 88/6699 [00:35<38:00,  2.90it/s]Evaluating on VQA val set:   1% 89/6699 [00:35<39:55,  2.76it/s]Evaluating on VQA val set:   1% 90/6699 [00:36<41:28,  2.66it/s]Evaluating on VQA val set:   1% 91/6699 [00:36<41:31,  2.65it/s]Evaluating on VQA val set:   1% 92/6699 [00:36<42:55,  2.57it/s]Evaluating on VQA val set:   1% 93/6699 [00:37<43:25,  2.54it/s]Evaluating on VQA val set:   1% 94/6699 [00:37<44:02,  2.50it/s]Evaluating on VQA val set:   1% 95/6699 [00:38<43:39,  2.52it/s]Evaluating on VQA val set:   1% 96/6699 [00:38<44:43,  2.46it/s]Evaluating on VQA val set:   1% 97/6699 [00:38<43:26,  2.53it/s]Evaluating on VQA val set:   1% 98/6699 [00:39<44:01,  2.50it/s]Evaluating on VQA val set:   1% 99/6699 [00:39<44:02,  2.50it/s]Evaluating on VQA val set:   1% 100/6699 [00:40<45:00,  2.44it/s]Evaluating on VQA val set:   1% 100/6699 [00:40<44:33,  2.47it/s]
11/15/2022 00:18:13 - INFO - train.train_vqa - Evaluation after epoch 1: 0.00
11/15/2022 00:18:13 - INFO - __main__ - Best VQAv2 evaluation score = 0.00, after epoch 1
11/15/2022 00:18:13 - INFO - __main__ - Saving best model and encoder checkpoint after VQAv2 training
11/15/2022 00:18:15 - INFO - __main__ - Saved checkpoint!
11/15/2022 00:18:15 - INFO - __main__ - Saved continual learning results so far!
11/15/2022 00:18:15 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/15/2022 00:18:15 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/15/2022 00:18:15 - INFO - __main__ - Training vilt model on task #2: SNLI-VE
11/15/2022 00:18:15 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE train dataloader with batch size of 32
11/15/2022 00:18:16 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE train dataset, with 529527 examples
11/15/2022 00:18:16 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE dev dataloader with batch size of 32
11/15/2022 00:18:16 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE dev dataset, with 17858 examples
Training epoch 1:   0% 0/16548 [00:00<?, ?it/s]/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/nn/functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
11/15/2022 00:18:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4525e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:19 - INFO - train.train_snli_ve - loss is tensor(8.0164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 1/16548 [00:02<12:43:00,  2.77s/it]11/15/2022 00:18:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.4768e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:20 - INFO - train.train_snli_ve - loss is tensor(7.8868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 2/16548 [00:03<7:40:16,  1.67s/it] 11/15/2022 00:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:21 - INFO - train.train_snli_ve - loss is tensor(7.9707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 3/16548 [00:04<6:06:40,  1.33s/it]11/15/2022 00:18:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:22 - INFO - train.train_snli_ve - loss is tensor(7.9370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 4/16548 [00:05<5:20:05,  1.16s/it]11/15/2022 00:18:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:23 - INFO - train.train_snli_ve - loss is tensor(7.9540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 5/16548 [00:06<4:56:16,  1.07s/it]11/15/2022 00:18:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4521e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:24 - INFO - train.train_snli_ve - loss is tensor(7.9820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 6/16548 [00:07<4:40:46,  1.02s/it]11/15/2022 00:18:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.4533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:25 - INFO - train.train_snli_ve - loss is tensor(7.9100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 7/16548 [00:08<4:34:52,  1.00it/s]11/15/2022 00:18:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.4454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:26 - INFO - train.train_snli_ve - loss is tensor(7.9971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 8/16548 [00:09<4:28:08,  1.03it/s]11/15/2022 00:18:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:27 - INFO - train.train_snli_ve - loss is tensor(7.9033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 9/16548 [00:10<4:22:34,  1.05it/s]11/15/2022 00:18:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:28 - INFO - train.train_snli_ve - loss is tensor(7.9212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 10/16548 [00:11<4:19:14,  1.06it/s]11/15/2022 00:18:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:28 - INFO - train.train_snli_ve - loss is tensor(7.9037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 11/16548 [00:11<4:15:17,  1.08it/s]11/15/2022 00:18:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4370e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:29 - INFO - train.train_snli_ve - loss is tensor(7.9348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 12/16548 [00:12<4:14:23,  1.08it/s]11/15/2022 00:18:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.4313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:30 - INFO - train.train_snli_ve - loss is tensor(7.9173, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 13/16548 [00:13<4:13:31,  1.09it/s]11/15/2022 00:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.4558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:31 - INFO - train.train_snli_ve - loss is tensor(7.9077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 14/16548 [00:14<4:12:43,  1.09it/s]11/15/2022 00:18:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4374e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:32 - INFO - train.train_snli_ve - loss is tensor(7.9032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 15/16548 [00:15<4:10:21,  1.10it/s]11/15/2022 00:18:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4427e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:33 - INFO - train.train_snli_ve - loss is tensor(7.8776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 16/16548 [00:16<4:10:00,  1.10it/s]11/15/2022 00:18:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.4288e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:34 - INFO - train.train_snli_ve - loss is tensor(7.8299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 17/16548 [00:17<4:09:35,  1.10it/s]11/15/2022 00:18:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:35 - INFO - train.train_snli_ve - loss is tensor(7.8506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 18/16548 [00:18<4:10:05,  1.10it/s]11/15/2022 00:18:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4324e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:36 - INFO - train.train_snli_ve - loss is tensor(7.7922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 19/16548 [00:19<4:10:15,  1.10it/s]11/15/2022 00:18:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:37 - INFO - train.train_snli_ve - loss is tensor(7.8249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 20/16548 [00:20<4:11:55,  1.09it/s]11/15/2022 00:18:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4132e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:38 - INFO - train.train_snli_ve - loss is tensor(7.7748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 21/16548 [00:21<4:12:23,  1.09it/s]11/15/2022 00:18:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:38 - INFO - train.train_snli_ve - loss is tensor(7.7999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 22/16548 [00:21<4:13:16,  1.09it/s]11/15/2022 00:18:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:39 - INFO - train.train_snli_ve - loss is tensor(7.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 23/16548 [00:22<4:14:11,  1.08it/s]11/15/2022 00:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:40 - INFO - train.train_snli_ve - loss is tensor(7.7695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 24/16548 [00:23<4:12:39,  1.09it/s]11/15/2022 00:18:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4007e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:41 - INFO - train.train_snli_ve - loss is tensor(7.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 25/16548 [00:24<4:13:35,  1.09it/s]11/15/2022 00:18:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:42 - INFO - train.train_snli_ve - loss is tensor(7.7524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 26/16548 [00:25<4:15:13,  1.08it/s]11/15/2022 00:18:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3761e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:43 - INFO - train.train_snli_ve - loss is tensor(7.7388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 27/16548 [00:26<4:14:22,  1.08it/s]11/15/2022 00:18:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.3931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:44 - INFO - train.train_snli_ve - loss is tensor(7.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 28/16548 [00:27<4:13:49,  1.08it/s]11/15/2022 00:18:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:45 - INFO - train.train_snli_ve - loss is tensor(7.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 29/16548 [00:28<4:14:14,  1.08it/s]11/15/2022 00:18:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.3633e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:46 - INFO - train.train_snli_ve - loss is tensor(7.6915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 30/16548 [00:29<4:12:58,  1.09it/s]11/15/2022 00:18:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3594e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:47 - INFO - train.train_snli_ve - loss is tensor(7.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 31/16548 [00:30<4:13:17,  1.09it/s]11/15/2022 00:18:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.3615e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:48 - INFO - train.train_snli_ve - loss is tensor(7.6453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 32/16548 [00:31<4:14:11,  1.08it/s]11/15/2022 00:18:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:49 - INFO - train.train_snli_ve - loss is tensor(7.5786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 33/16548 [00:32<4:12:01,  1.09it/s]11/15/2022 00:18:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:50 - INFO - train.train_snli_ve - loss is tensor(7.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 34/16548 [00:32<4:11:22,  1.09it/s]11/15/2022 00:18:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:50 - INFO - train.train_snli_ve - loss is tensor(7.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 35/16548 [00:33<4:10:38,  1.10it/s]11/15/2022 00:18:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.3333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:51 - INFO - train.train_snli_ve - loss is tensor(7.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 36/16548 [00:34<4:11:14,  1.10it/s]11/15/2022 00:18:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.3249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:52 - INFO - train.train_snli_ve - loss is tensor(7.5093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 37/16548 [00:35<4:11:28,  1.09it/s]11/15/2022 00:18:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.3160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:53 - INFO - train.train_snli_ve - loss is tensor(7.4774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 38/16548 [00:36<4:12:21,  1.09it/s]11/15/2022 00:18:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2993e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:54 - INFO - train.train_snli_ve - loss is tensor(7.4610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 39/16548 [00:37<4:13:10,  1.09it/s]11/15/2022 00:18:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.3069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:55 - INFO - train.train_snli_ve - loss is tensor(7.4752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 40/16548 [00:38<4:12:15,  1.09it/s]11/15/2022 00:18:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2852e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:56 - INFO - train.train_snli_ve - loss is tensor(7.3711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 41/16548 [00:39<4:11:32,  1.09it/s]11/15/2022 00:18:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:57 - INFO - train.train_snli_ve - loss is tensor(7.4121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 42/16548 [00:40<4:11:49,  1.09it/s]11/15/2022 00:18:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.2676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:58 - INFO - train.train_snli_ve - loss is tensor(7.3897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 43/16548 [00:41<4:10:26,  1.10it/s]11/15/2022 00:18:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:18:59 - INFO - train.train_snli_ve - loss is tensor(7.3345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 44/16548 [00:42<4:10:21,  1.10it/s]11/15/2022 00:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2522e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:00 - INFO - train.train_snli_ve - loss is tensor(7.2614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 45/16548 [00:43<4:09:05,  1.10it/s]11/15/2022 00:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:00 - INFO - train.train_snli_ve - loss is tensor(7.2117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 46/16548 [00:43<4:08:46,  1.11it/s]11/15/2022 00:19:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.2361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:01 - INFO - train.train_snli_ve - loss is tensor(7.2378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 47/16548 [00:44<4:09:36,  1.10it/s]11/15/2022 00:19:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:02 - INFO - train.train_snli_ve - loss is tensor(7.1619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 48/16548 [00:45<4:10:12,  1.10it/s]11/15/2022 00:19:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:03 - INFO - train.train_snli_ve - loss is tensor(7.1127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 49/16548 [00:46<4:11:11,  1.09it/s]11/15/2022 00:19:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:04 - INFO - train.train_snli_ve - loss is tensor(7.1511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 50/16548 [00:47<4:13:00,  1.09it/s]11/15/2022 00:19:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:05 - INFO - train.train_snli_ve - loss is tensor(7.1348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 51/16548 [00:48<4:14:20,  1.08it/s]11/15/2022 00:19:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1866e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:06 - INFO - train.train_snli_ve - loss is tensor(6.9996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 52/16548 [00:49<4:15:28,  1.08it/s]11/15/2022 00:19:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1764e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:07 - INFO - train.train_snli_ve - loss is tensor(7.0226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 53/16548 [00:50<4:15:19,  1.08it/s]11/15/2022 00:19:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.1635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:08 - INFO - train.train_snli_ve - loss is tensor(6.9988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 54/16548 [00:51<4:15:27,  1.08it/s]11/15/2022 00:19:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:09 - INFO - train.train_snli_ve - loss is tensor(6.9238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 55/16548 [00:52<4:14:13,  1.08it/s]11/15/2022 00:19:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:10 - INFO - train.train_snli_ve - loss is tensor(6.9577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 56/16548 [00:53<4:14:24,  1.08it/s]11/15/2022 00:19:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:11 - INFO - train.train_snli_ve - loss is tensor(6.9157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 57/16548 [00:54<4:13:31,  1.08it/s]11/15/2022 00:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1338e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:12 - INFO - train.train_snli_ve - loss is tensor(6.8366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 58/16548 [00:54<4:12:06,  1.09it/s]11/15/2022 00:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:12 - INFO - train.train_snli_ve - loss is tensor(6.7939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 59/16548 [00:55<4:12:30,  1.09it/s]11/15/2022 00:19:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.1141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:13 - INFO - train.train_snli_ve - loss is tensor(6.7773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 60/16548 [00:56<4:13:34,  1.08it/s]11/15/2022 00:19:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:14 - INFO - train.train_snli_ve - loss is tensor(6.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 61/16548 [00:57<4:12:53,  1.09it/s]11/15/2022 00:19:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.0845e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:15 - INFO - train.train_snli_ve - loss is tensor(6.7175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 62/16548 [00:58<4:12:04,  1.09it/s]11/15/2022 00:19:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.0842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:16 - INFO - train.train_snli_ve - loss is tensor(6.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 63/16548 [00:59<4:10:26,  1.10it/s]11/15/2022 00:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0695e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:17 - INFO - train.train_snli_ve - loss is tensor(6.6423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 64/16548 [01:00<4:11:25,  1.09it/s]11/15/2022 00:19:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.0608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:18 - INFO - train.train_snli_ve - loss is tensor(6.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 65/16548 [01:01<4:10:11,  1.10it/s]11/15/2022 00:19:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:19 - INFO - train.train_snli_ve - loss is tensor(6.4850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 66/16548 [01:02<4:08:22,  1.11it/s]11/15/2022 00:19:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.0407e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:20 - INFO - train.train_snli_ve - loss is tensor(6.4834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 67/16548 [01:03<4:10:03,  1.10it/s]11/15/2022 00:19:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:21 - INFO - train.train_snli_ve - loss is tensor(6.4498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 68/16548 [01:04<4:10:58,  1.09it/s]11/15/2022 00:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:22 - INFO - train.train_snli_ve - loss is tensor(6.4042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 69/16548 [01:05<4:10:11,  1.10it/s]11/15/2022 00:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:22 - INFO - train.train_snli_ve - loss is tensor(6.3241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 70/16548 [01:05<4:11:48,  1.09it/s]11/15/2022 00:19:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.9910e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:23 - INFO - train.train_snli_ve - loss is tensor(6.3268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 71/16548 [01:06<4:11:28,  1.09it/s]11/15/2022 00:19:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.9998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:24 - INFO - train.train_snli_ve - loss is tensor(6.2763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 72/16548 [01:07<4:10:28,  1.10it/s]11/15/2022 00:19:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:25 - INFO - train.train_snli_ve - loss is tensor(6.1735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 73/16548 [01:08<4:12:12,  1.09it/s]11/15/2022 00:19:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.9667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:26 - INFO - train.train_snli_ve - loss is tensor(6.0976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 74/16548 [01:09<4:12:34,  1.09it/s]11/15/2022 00:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:27 - INFO - train.train_snli_ve - loss is tensor(6.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 75/16548 [01:10<4:10:42,  1.10it/s]11/15/2022 00:19:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:28 - INFO - train.train_snli_ve - loss is tensor(6.0403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 76/16548 [01:11<4:10:06,  1.10it/s]11/15/2022 00:19:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:29 - INFO - train.train_snli_ve - loss is tensor(6.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 77/16548 [01:12<4:09:10,  1.10it/s]11/15/2022 00:19:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:30 - INFO - train.train_snli_ve - loss is tensor(6.0325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 78/16548 [01:13<4:09:13,  1.10it/s]11/15/2022 00:19:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.9181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:31 - INFO - train.train_snli_ve - loss is tensor(5.9274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 79/16548 [01:14<4:10:25,  1.10it/s]11/15/2022 00:19:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:32 - INFO - train.train_snli_ve - loss is tensor(5.8731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 80/16548 [01:15<4:11:23,  1.09it/s]11/15/2022 00:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:33 - INFO - train.train_snli_ve - loss is tensor(5.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 81/16548 [01:16<4:11:02,  1.09it/s]11/15/2022 00:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/15/2022 00:19:33 - INFO - train.train_snli_ve - loss is tensor(5.7558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0% 82/16548 [01:16<4:11:27,  1.09it/s]