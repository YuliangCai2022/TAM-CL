0. To prepare the dataset:
NLVRv2: train: wget https://lil.nlp.cornell.edu/resources/NLVR2/train_img.zip
        dev: wget https://lil.nlp.cornell.edu/resources/NLVR2/dev_img.zip
        test: wget https://lil.nlp.cornell.edu/resources/NLVR2/test1_img.zip 
COCOQA: https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/
SNLI-VE: https://github.com/necla-ml/SNLI-VE
PathVqa: https://drive.google.com/file/d/1utnisF_HJ8Yk9Qe9dBe9mxuruuGe7DgW/view
COCO image dataset: https://visualqa.org/download.html

After downloading every dataset, put it into a folder with structure below:

-data
    -cocoqa/
        -test/
        -train/
        -ans2label.pkl
    -flickr30k
        -flickr30k_images/
    -nlvr2/
        -data/
            -balanced
            -dev.json
            -filter_data.py
            -test1.json
            -train.json
            -unbalanced
        -images
            -dev
            -train
            -test1
    -snli-ve
        snli_ve_dev.jsonl
        snli_ve_test.jsonl
        snli_ve_train.jsonl
    -ms-coco/
        -images
    -pathvqa/
        -split/


1.To run the experiment with conda, create a conda environment yml file with:

- _libgcc_mutex=0.1=main
  - _openmp_mutex=5.1=1_gnu
  - ca-certificates=2022.07.19=h06a4308_0
  - certifi=2021.5.30=py36h06a4308_0
  - ld_impl_linux-64=2.38=h1181459_1
  - libffi=3.3=he6710b0_2
  - libgcc-ng=11.2.0=h1234567_1
  - libgomp=11.2.0=h1234567_1
  - libstdcxx-ng=11.2.0=h1234567_1
  - ncurses=6.3=h5eee18b_3
  - openssl=1.1.1q=h7f8727e_0
  - pip=21.2.2=py36h06a4308_0
  - python=3.6.13=h12debd9_1
  - readline=8.1.2=h7f8727e_1
  - setuptools=58.0.4=py36h06a4308_0
  - sqlite=3.39.2=h5082296_0
  - tk=8.6.12=h1ccaba5_0
  - wheel=0.37.1=pyhd3eb1b0_0
  - xz=5.2.5=h7f8727e_1
  - zlib=1.2.12=h7f8727e_2
  - pip:
    - aiohttp==3.8.1
    - aiosignal==1.2.0
    - argon2-cffi==21.3.0
    - argon2-cffi-bindings==21.2.0
    - async-generator==1.10
    - async-timeout==4.0.2
    - asynctest==0.13.0
    - attrs==22.1.0
    - backcall==0.2.0
    - beautifulsoup4==4.11.1
    - bleach==4.1.0
    - cffi==1.15.1
    - charset-normalizer==2.0.12
    - click==8.0.4
    - configparser==5.2.0
    - cycler==0.11.0
    - dataclasses==0.8
    - datasets==2.4.0
    - decorator==5.1.1
    - defusedxml==0.7.1
    - dill==0.3.4
    - docker-pycreds==0.4.0
    - entrypoints==0.4
    - filelock==3.4.1
    - frozenlist==1.2.0
    - fsspec==2022.1.0
    - gdown==4.6.0
    - gitdb==4.0.9
    - gitpython==3.1.18
    - gql==0.2.0
    - graphql-core==1.1
    - huggingface-hub==0.4.0
    - idna==3.3
    - idna-ssl==1.1.0
    - importlib-metadata==4.8.3
    - importlib-resources==5.4.0
    - ipykernel==5.5.6
    - ipython==7.16.3
    - ipython-genutils==0.2.0
    - jedi==0.17.2
    - jinja2==3.0.3
    - joblib==1.1.0
    - jsonlines==3.1.0
    - jsonschema==3.2.0
    - jupyter-client==7.1.2
    - jupyter-core==4.9.2
    - jupyterlab-pygments==0.1.2
    - kiwisolver==1.3.1
    - markupsafe==2.0.1
    - matplotlib==3.3.4
    - mistune==0.8.4
    - multidict==5.2.0
    - multiprocess==0.70.12.2
    - nbclient==0.5.9
    - nbconvert==6.0.7
    - nbformat==5.1.3
    - nest-asyncio==1.5.5
    - notebook==6.4.10
    - numpy==1.19.5
    - nvidia-ml-py3==7.352.0
    - packaging==21.3
    - pandas==1.1.5
    - pandocfilters==1.5.0
    - parso==0.7.1
    - pathtools==0.1.2
    - pexpect==4.8.0
    - pickleshare==0.7.5
    - pillow==8.4.0
    - prometheus-client==0.14.1
    - promise==2.3
    - prompt-toolkit==3.0.30
    - protobuf==3.19.4
    - psutil==5.9.1
    - ptyprocess==0.7.0
    - pyarrow==6.0.1
    - pycparser==2.21
    - pygments==2.13.0
    - pyparsing==3.0.9
    - pyrsistent==0.18.0
    - pysocks==1.7.1
    - python-dateutil==2.8.2
    - pytz==2022.2.1
    - pyyaml==6.0
    - pyzmq==23.2.1
    - regex==2022.8.17
    - requests==2.27.1
    - responses==0.17.0
    - sacremoses==0.0.53
    - scikit-learn==0.24.2
    - scipy==1.5.4
    - send2trash==1.8.0
    - sentry-sdk==1.9.5
    - setproctitle==1.2.3
    - shortuuid==1.0.9
    - six==1.16.0
    - smmap==5.0.0
    - soupsieve==2.3.2.post1
    - subprocess32==3.5.4
    - tensorboardx==2.5.1
    - terminado==0.12.1
    - testpath==0.6.0
    - threadpoolctl==3.1.0
    - timm==0.6.9
    - tokenizers==0.11.6
    - torch==1.10.2+cu113
    - torchvision==0.11.3+cu113
    - tornado==6.1
    - tqdm==4.64.0
    - traitlets==4.3.3
    - transformers==4.16.2
    - typing-extensions==4.1.1
    - urllib3==1.26.12
    - wandb==0.9.7
    - watchdog==2.1.9
    - wcwidth==0.2.5
    - webencodings==0.5.1
    - xxhash==3.0.0
    - yarl==1.7.2
    - zipp==3.6.0

2. Within the conda environment, run:
    python -m run --encoder_name vilt \
                        --pretrained_model_name dandelin/vilt-b32-mlm \
                        --ordered_cl_tasks pathvqa,vqa,nlvr2,snli-ve\
                        --cl_algorithm sequential_ft \
                        --climb_data_dir * \
            		    --do_train \
                        --do_eval \
                        --output_dir * \
                        --batch_size 8 \
                        --task_attention 1 \
                        --dytox 1 \
                        --ewc 0 \
                        --parallel 0 \
                        --replay 0
Where --climb_data_dir is the folder where all the dataset are stored, --output_dir is the folder where all output files are stored,
to change task order, simply change the task name orders from --ordered_cl_tasks (no space betwen diferent tasks)