==========================================
SLURM_JOB_ID = 12346247
SLURM_JOB_NODELIST = e23-01
TMPDIR = /tmp/SLURM_12346247
==========================================
no change     /spack/conda/miniconda3/4.12.0/condabin/conda
no change     /spack/conda/miniconda3/4.12.0/bin/conda
no change     /spack/conda/miniconda3/4.12.0/bin/conda-env
no change     /spack/conda/miniconda3/4.12.0/bin/activate
no change     /spack/conda/miniconda3/4.12.0/bin/deactivate
no change     /spack/conda/miniconda3/4.12.0/etc/profile.d/conda.sh
no change     /spack/conda/miniconda3/4.12.0/etc/fish/conf.d/conda.fish
no change     /spack/conda/miniconda3/4.12.0/shell/condabin/Conda.psm1
no change     /spack/conda/miniconda3/4.12.0/shell/condabin/conda-hook.ps1
no change     /spack/conda/miniconda3/4.12.0/lib/python3.9/site-packages/xontrib/conda.xsh
no change     /spack/conda/miniconda3/4.12.0/etc/profile.d/conda.csh
modified      /home1/caiyulia/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

Added mamba to /home1/caiyulia/.bashrc

==> For changes to take effect, close and re-open your current shell. <==

11/16/2022 21:12:06 - INFO - vilt - ----------------------------------------------------------------------------------------------------
11/16/2022 21:12:06 - INFO - vilt - Loading ViLT encoder model: dandelin/vilt-b32-mlm
11/16/2022 21:12:12 - INFO - vilt - Successfully loaded pretrained ViLT encoder
11/16/2022 21:12:12 - INFO - vilt - Successfully created and initialized ViLT Continual Leaner model
11/16/2022 21:12:12 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 21:12:12 - INFO - __main__ - Training models on Vision-Language continual learning tasks...
11/16/2022 21:12:12 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 21:12:12 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 21:12:12 - INFO - __main__ - Training vilt model on task #1: VQAv2
11/16/2022 21:12:12 - INFO - data.image_datasets.cocoimages_dataset - in the MSCOCOImagesDatset
11/16/2022 21:12:13 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 train dataloader with batch size of 32
11/16/2022 21:12:25 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 train dataset, with 177786 examples
11/16/2022 21:12:25 - INFO - data.visionlanguage_datasets.vqa_dataset - Creating VQAv2 val dataloader with batch size of 32
11/16/2022 21:12:30 - INFO - data.visionlanguage_datasets.vqa_dataset - Loaded VQAv2 val dataset, with 85462 examples
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.cls_token
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.position_embeddings
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.word_embeddings.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.position_embeddings.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.token_type_embeddings.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.text_embeddings.LayerNorm.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.patch_embeddings.projection.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.embeddings.token_type_embeddings.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.0.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.1.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.2.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.3.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.4.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.5.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.6.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.7.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.8.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.9.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.10.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.query.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.key.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.attention.value.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.attention.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.intermediate.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.output.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_before.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.encoder.layer.11.layernorm_after.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.layernorm.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.vilt_encoder.vilt.pooler.dense.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.0.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.0.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.1.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.1.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.3.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.vqa.3.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.0.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.1.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.task_layer.snli-ve.3.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.norm1.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.norm1.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.attn.q.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.attn.k.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.attn.v.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.attn.proj.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.attn.proj.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.norm2.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.norm2.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.mlp.fc1.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.weight
11/16/2022 21:12:33 - INFO - train.train_vqa - transformer.TAB.mlp.fc2.bias
11/16/2022 21:12:33 - INFO - train.train_vqa - task_tokens.0
******************************creating vilt moel with task attention block!*****************************
Creating DyTox!
/project/rostamim_919/caiyulia/data/vqav2/v2_mscoco_train2014_annotations.json
/project/rostamim_919/caiyulia/data/vqav2/v2_mscoco_val2014_annotations.json
Training epoch 1:   0%|          | 0/5556 [00:00<?, ?it/s]Training epoch 1:   0%|          | 1/5556 [00:01<2:41:29,  1.74s/it]Training epoch 1:   0%|          | 2/5556 [00:02<1:46:55,  1.16s/it]Training epoch 1:   0%|          | 3/5556 [00:03<1:30:48,  1.02it/s]Training epoch 1:   0%|          | 4/5556 [00:04<1:23:05,  1.11it/s]Training epoch 1:   0%|          | 5/5556 [00:04<1:20:38,  1.15it/s]Training epoch 1:   0%|          | 6/5556 [00:05<1:15:34,  1.22it/s]Training epoch 1:   0%|          | 7/5556 [00:06<1:14:18,  1.24it/s]Training epoch 1:   0%|          | 8/5556 [00:07<1:11:18,  1.30it/s]Training epoch 1:   0%|          | 9/5556 [00:07<1:12:31,  1.27it/s]Training epoch 1:   0%|          | 10/5556 [00:08<1:10:14,  1.32it/s]Training epoch 1:   0%|          | 11/5556 [00:09<1:10:20,  1.31it/s]Training epoch 1:   0%|          | 12/5556 [00:10<1:10:05,  1.32it/s]Training epoch 1:   0%|          | 13/5556 [00:10<1:10:34,  1.31it/s]Training epoch 1:   0%|          | 14/5556 [00:11<1:10:02,  1.32it/s]Training epoch 1:   0%|          | 15/5556 [00:12<1:09:48,  1.32it/s]Training epoch 1:   0%|          | 16/5556 [00:13<1:10:28,  1.31it/s]Training epoch 1:   0%|          | 17/5556 [00:13<1:09:49,  1.32it/s]Training epoch 1:   0%|          | 18/5556 [00:14<1:09:58,  1.32it/s]Training epoch 1:   0%|          | 19/5556 [00:15<1:10:34,  1.31it/s]Training epoch 1:   0%|          | 20/5556 [00:16<1:11:25,  1.29it/s]Training epoch 1:   0%|          | 21/5556 [00:16<1:11:24,  1.29it/s]Training epoch 1:   0%|          | 22/5556 [00:17<1:10:37,  1.31it/s]Training epoch 1:   0%|          | 23/5556 [00:18<1:10:28,  1.31it/s]Training epoch 1:   0%|          | 24/5556 [00:19<1:09:39,  1.32it/s]Training epoch 1:   0%|          | 25/5556 [00:19<1:08:15,  1.35it/s]Training epoch 1:   0%|          | 26/5556 [00:20<1:08:56,  1.34it/s]Training epoch 1:   0%|          | 27/5556 [00:21<1:08:22,  1.35it/s]Training epoch 1:   1%|          | 28/5556 [00:22<1:08:22,  1.35it/s]Training epoch 1:   1%|          | 29/5556 [00:22<1:08:56,  1.34it/s]Training epoch 1:   1%|          | 30/5556 [00:23<1:09:21,  1.33it/s]Training epoch 1:   1%|          | 31/5556 [00:24<1:09:10,  1.33it/s]Training epoch 1:   1%|          | 32/5556 [00:25<1:09:07,  1.33it/s]Training epoch 1:   1%|          | 33/5556 [00:25<1:09:36,  1.32it/s]Training epoch 1:   1%|          | 34/5556 [00:26<1:09:52,  1.32it/s]Training epoch 1:   1%|          | 35/5556 [00:27<1:10:26,  1.31it/s]Training epoch 1:   1%|          | 36/5556 [00:28<1:10:02,  1.31it/s]Training epoch 1:   1%|          | 37/5556 [00:29<1:10:20,  1.31it/s]Training epoch 1:   1%|          | 38/5556 [00:29<1:09:43,  1.32it/s]Training epoch 1:   1%|          | 39/5556 [00:30<1:09:01,  1.33it/s]Training epoch 1:   1%|          | 40/5556 [00:31<1:08:23,  1.34it/s]Training epoch 1:   1%|          | 41/5556 [00:32<1:09:15,  1.33it/s]Training epoch 1:   1%|          | 42/5556 [00:32<1:09:24,  1.32it/s]Training epoch 1:   1%|          | 43/5556 [00:33<1:10:16,  1.31it/s]Training epoch 1:   1%|          | 44/5556 [00:34<1:09:53,  1.31it/s]Training epoch 1:   1%|          | 45/5556 [00:35<1:09:55,  1.31it/s]Training epoch 1:   1%|          | 46/5556 [00:35<1:09:29,  1.32it/s]Training epoch 1:   1%|          | 47/5556 [00:36<1:10:03,  1.31it/s]Training epoch 1:   1%|          | 48/5556 [00:37<1:10:04,  1.31it/s]Training epoch 1:   1%|          | 49/5556 [00:38<1:09:10,  1.33it/s]Training epoch 1:   1%|          | 50/5556 [00:38<1:08:24,  1.34it/s]Training epoch 1:   1%|          | 51/5556 [00:39<1:09:14,  1.33it/s]Training epoch 1:   1%|          | 52/5556 [00:40<1:07:59,  1.35it/s]Training epoch 1:   1%|          | 53/5556 [00:41<1:07:50,  1.35it/s]Training epoch 1:   1%|          | 54/5556 [00:41<1:08:42,  1.33it/s]Training epoch 1:   1%|          | 55/5556 [00:42<1:08:24,  1.34it/s]Training epoch 1:   1%|1         | 56/5556 [00:43<1:08:26,  1.34it/s]Training epoch 1:   1%|1         | 57/5556 [00:44<1:08:12,  1.34it/s]Training epoch 1:   1%|1         | 58/5556 [00:44<1:07:54,  1.35it/s]Training epoch 1:   1%|1         | 59/5556 [00:45<1:07:18,  1.36it/s]Training epoch 1:   1%|1         | 60/5556 [00:46<1:07:35,  1.36it/s]Training epoch 1:   1%|1         | 61/5556 [00:47<1:08:37,  1.33it/s]Training epoch 1:   1%|1         | 62/5556 [00:47<1:09:04,  1.33it/s]Training epoch 1:   1%|1         | 63/5556 [00:48<1:08:55,  1.33it/s]Training epoch 1:   1%|1         | 64/5556 [00:49<1:08:32,  1.34it/s]Training epoch 1:   1%|1         | 65/5556 [00:50<1:08:30,  1.34it/s]Training epoch 1:   1%|1         | 66/5556 [00:50<1:08:40,  1.33it/s]Training epoch 1:   1%|1         | 67/5556 [00:51<1:08:25,  1.34it/s]Training epoch 1:   1%|1         | 68/5556 [00:52<1:08:07,  1.34it/s]Training epoch 1:   1%|1         | 69/5556 [00:52<1:07:52,  1.35it/s]Training epoch 1:   1%|1         | 70/5556 [00:53<1:07:49,  1.35it/s]Training epoch 1:   1%|1         | 71/5556 [00:54<1:07:27,  1.36it/s]Training epoch 1:   1%|1         | 72/5556 [00:55<1:07:17,  1.36it/s]Training epoch 1:   1%|1         | 73/5556 [00:55<1:07:33,  1.35it/s]Training epoch 1:   1%|1         | 74/5556 [00:56<1:07:51,  1.35it/s]Training epoch 1:   1%|1         | 75/5556 [00:57<1:07:49,  1.35it/s]Training epoch 1:   1%|1         | 76/5556 [00:58<1:08:04,  1.34it/s]Training epoch 1:   1%|1         | 77/5556 [00:58<1:07:32,  1.35it/s]Training epoch 1:   1%|1         | 78/5556 [00:59<1:07:46,  1.35it/s]Training epoch 1:   1%|1         | 79/5556 [01:00<1:08:02,  1.34it/s]Training epoch 1:   1%|1         | 80/5556 [01:01<1:08:08,  1.34it/s]Training epoch 1:   1%|1         | 81/5556 [01:01<1:08:03,  1.34it/s]Training epoch 1:   1%|1         | 82/5556 [01:02<1:07:58,  1.34it/s]Training epoch 1:   1%|1         | 83/5556 [01:03<1:07:49,  1.34it/s]Training epoch 1:   2%|1         | 84/5556 [01:04<1:07:44,  1.35it/s]Training epoch 1:   2%|1         | 85/5556 [01:04<1:06:51,  1.36it/s]Training epoch 1:   2%|1         | 86/5556 [01:05<1:08:11,  1.34it/s]Training epoch 1:   2%|1         | 87/5556 [01:06<1:08:28,  1.33it/s]Training epoch 1:   2%|1         | 88/5556 [01:07<1:08:58,  1.32it/s]Training epoch 1:   2%|1         | 89/5556 [01:07<1:08:16,  1.33it/s]Training epoch 1:   2%|1         | 90/5556 [01:08<1:08:31,  1.33it/s]Training epoch 1:   2%|1         | 91/5556 [01:09<1:07:21,  1.35it/s]Training epoch 1:   2%|1         | 92/5556 [01:10<1:07:48,  1.34it/s]Training epoch 1:   2%|1         | 93/5556 [01:10<1:06:57,  1.36it/s]Training epoch 1:   2%|1         | 94/5556 [01:11<1:06:35,  1.37it/s]Training epoch 1:   2%|1         | 95/5556 [01:12<1:06:16,  1.37it/s]Training epoch 1:   2%|1         | 96/5556 [01:13<1:07:34,  1.35it/s]Training epoch 1:   2%|1         | 97/5556 [01:13<1:07:42,  1.34it/s]Training epoch 1:   2%|1         | 98/5556 [01:14<1:07:57,  1.34it/s]Training epoch 1:   2%|1         | 99/5556 [01:15<1:07:45,  1.34it/s]Training epoch 1:   2%|1         | 100/5556 [01:16<1:10:10,  1.30it/s]Training epoch 1:   2%|1         | 101/5556 [01:16<1:09:52,  1.30it/s]Training epoch 1:   2%|1         | 102/5556 [01:17<1:09:29,  1.31it/s]Training epoch 1:   2%|1         | 103/5556 [01:18<1:09:33,  1.31it/s]Training epoch 1:   2%|1         | 104/5556 [01:19<1:09:51,  1.30it/s]Training epoch 1:   2%|1         | 105/5556 [01:19<1:09:53,  1.30it/s]Training epoch 1:   2%|1         | 106/5556 [01:20<1:10:02,  1.30it/s]Training epoch 1:   2%|1         | 107/5556 [01:21<1:09:30,  1.31it/s]Training epoch 1:   2%|1         | 108/5556 [01:22<1:08:58,  1.32it/s]Training epoch 1:   2%|1         | 109/5556 [01:22<1:08:52,  1.32it/s]Training epoch 1:   2%|1         | 110/5556 [01:23<1:08:36,  1.32it/s]Training epoch 1:   2%|1         | 111/5556 [01:24<1:07:52,  1.34it/s]Training epoch 1:   2%|2         | 112/5556 [01:25<1:07:27,  1.34it/s]Training epoch 1:   2%|2         | 113/5556 [01:25<1:08:39,  1.32it/s]Training epoch 1:   2%|2         | 114/5556 [01:26<1:08:28,  1.32it/s]Training epoch 1:   2%|2         | 115/5556 [01:27<1:08:44,  1.32it/s]Training epoch 1:   2%|2         | 116/5556 [01:28<1:07:45,  1.34it/s]Training epoch 1:   2%|2         | 117/5556 [01:28<1:08:00,  1.33it/s]Training epoch 1:   2%|2         | 118/5556 [01:29<1:07:53,  1.34it/s]Training epoch 1:   2%|2         | 119/5556 [01:30<1:08:35,  1.32it/s]Training epoch 1:   2%|2         | 120/5556 [01:31<1:08:12,  1.33it/s]Training epoch 1:   2%|2         | 121/5556 [01:32<1:09:11,  1.31it/s]Training epoch 1:   2%|2         | 122/5556 [01:32<1:07:26,  1.34it/s]Training epoch 1:   2%|2         | 123/5556 [01:33<1:07:44,  1.34it/s]Training epoch 1:   2%|2         | 124/5556 [01:34<1:07:22,  1.34it/s]Training epoch 1:   2%|2         | 125/5556 [01:34<1:08:13,  1.33it/s]Training epoch 1:   2%|2         | 126/5556 [01:35<1:08:15,  1.33it/s]Training epoch 1:   2%|2         | 127/5556 [01:36<1:07:29,  1.34it/s]Training epoch 1:   2%|2         | 128/5556 [01:37<1:08:19,  1.32it/s]Training epoch 1:   2%|2         | 129/5556 [01:38<1:08:35,  1.32it/s]Training epoch 1:   2%|2         | 130/5556 [01:38<1:08:17,  1.32it/s]Training epoch 1:   2%|2         | 131/5556 [01:39<1:07:36,  1.34it/s]Training epoch 1:   2%|2         | 132/5556 [01:40<1:07:07,  1.35it/s]Training epoch 1:   2%|2         | 133/5556 [01:40<1:07:25,  1.34it/s]Training epoch 1:   2%|2         | 134/5556 [01:41<1:08:14,  1.32it/s]Training epoch 1:   2%|2         | 135/5556 [01:42<1:07:35,  1.34it/s]Training epoch 1:   2%|2         | 136/5556 [01:43<1:06:36,  1.36it/s]Training epoch 1:   2%|2         | 137/5556 [01:43<1:05:28,  1.38it/s]Training epoch 1:   2%|2         | 138/5556 [01:44<1:05:42,  1.37it/s]Training epoch 1:   3%|2         | 139/5556 [01:45<1:05:23,  1.38it/s]Training epoch 1:   3%|2         | 140/5556 [01:46<1:06:00,  1.37it/s]Training epoch 1:   3%|2         | 141/5556 [01:46<1:06:17,  1.36it/s]Training epoch 1:   3%|2         | 142/5556 [01:47<1:06:19,  1.36it/s]Training epoch 1:   3%|2         | 143/5556 [01:48<1:06:31,  1.36it/s]Training epoch 1:   3%|2         | 144/5556 [01:49<1:06:31,  1.36it/s]Training epoch 1:   3%|2         | 145/5556 [01:49<1:07:02,  1.35it/s]Training epoch 1:   3%|2         | 146/5556 [01:50<1:06:15,  1.36it/s]Training epoch 1:   3%|2         | 147/5556 [01:51<1:06:37,  1.35it/s]Training epoch 1:   3%|2         | 148/5556 [01:52<1:06:45,  1.35it/s]Training epoch 1:   3%|2         | 149/5556 [01:52<1:07:33,  1.33it/s]Training epoch 1:   3%|2         | 150/5556 [01:53<1:07:13,  1.34it/s]Training epoch 1:   3%|2         | 151/5556 [01:54<1:06:38,  1.35it/s]Training epoch 1:   3%|2         | 152/5556 [01:54<1:06:43,  1.35it/s]Training epoch 1:   3%|2         | 153/5556 [01:55<1:06:37,  1.35it/s]Training epoch 1:   3%|2         | 154/5556 [01:56<1:06:53,  1.35it/s]Training epoch 1:   3%|2         | 155/5556 [01:57<1:06:16,  1.36it/s]Training epoch 1:   3%|2         | 156/5556 [01:57<1:05:37,  1.37it/s]Training epoch 1:   3%|2         | 157/5556 [01:58<1:06:25,  1.35it/s]Training epoch 1:   3%|2         | 158/5556 [01:59<1:06:12,  1.36it/s]Training epoch 1:   3%|2         | 159/5556 [02:00<1:05:57,  1.36it/s]Training epoch 1:   3%|2         | 160/5556 [02:00<1:04:54,  1.39it/s]Training epoch 1:   3%|2         | 161/5556 [02:01<1:04:54,  1.39it/s]Training epoch 1:   3%|2         | 162/5556 [02:02<1:04:48,  1.39it/s]Training epoch 1:   3%|2         | 163/5556 [02:03<1:05:42,  1.37it/s]Training epoch 1:   3%|2         | 164/5556 [02:03<1:05:23,  1.37it/s]Training epoch 1:   3%|2         | 165/5556 [02:04<1:06:49,  1.34it/s]Training epoch 1:   3%|2         | 166/5556 [02:05<1:07:30,  1.33it/s]Training epoch 1:   3%|3         | 167/5556 [02:06<1:07:05,  1.34it/s]Training epoch 1:   3%|3         | 168/5556 [02:06<1:06:48,  1.34it/s]Training epoch 1:   3%|3         | 169/5556 [02:07<1:06:06,  1.36it/s]Training epoch 1:   3%|3         | 170/5556 [02:08<1:06:04,  1.36it/s]Training epoch 1:   3%|3         | 171/5556 [02:08<1:06:49,  1.34it/s]Training epoch 1:   3%|3         | 172/5556 [02:09<1:07:25,  1.33it/s]Training epoch 1:   3%|3         | 173/5556 [02:10<1:07:59,  1.32it/s]Training epoch 1:   3%|3         | 174/5556 [02:11<1:07:44,  1.32it/s]Training epoch 1:   3%|3         | 175/5556 [02:12<1:08:38,  1.31it/s]Training epoch 1:   3%|3         | 176/5556 [02:12<1:08:32,  1.31it/s]Training epoch 1:   3%|3         | 177/5556 [02:13<1:08:30,  1.31it/s]Training epoch 1:   3%|3         | 178/5556 [02:14<1:07:45,  1.32it/s]Training epoch 1:   3%|3         | 179/5556 [02:15<1:08:04,  1.32it/s]Training epoch 1:   3%|3         | 180/5556 [02:15<1:07:23,  1.33it/s]Training epoch 1:   3%|3         | 181/5556 [02:16<1:08:30,  1.31it/s]Training epoch 1:   3%|3         | 182/5556 [02:17<1:09:00,  1.30it/s]Training epoch 1:   3%|3         | 183/5556 [02:18<1:08:31,  1.31it/s]Training epoch 1:   3%|3         | 184/5556 [02:18<1:07:28,  1.33it/s]Training epoch 1:   3%|3         | 185/5556 [02:19<1:07:09,  1.33it/s]Training epoch 1:   3%|3         | 186/5556 [02:20<1:07:25,  1.33it/s]Training epoch 1:   3%|3         | 187/5556 [02:21<1:07:04,  1.33it/s]Training epoch 1:   3%|3         | 188/5556 [02:21<1:06:20,  1.35it/s]Training epoch 1:   3%|3         | 189/5556 [02:22<1:06:35,  1.34it/s]Training epoch 1:   3%|3         | 190/5556 [02:23<1:06:52,  1.34it/s]Training epoch 1:   3%|3         | 191/5556 [02:24<1:07:11,  1.33it/s]Training epoch 1:   3%|3         | 192/5556 [02:24<1:07:07,  1.33it/s]Training epoch 1:   3%|3         | 193/5556 [02:25<1:07:12,  1.33it/s]Training epoch 1:   3%|3         | 194/5556 [02:26<1:07:28,  1.32it/s]Training epoch 1:   4%|3         | 195/5556 [02:27<1:06:30,  1.34it/s]Training epoch 1:   4%|3         | 196/5556 [02:27<1:06:41,  1.34it/s]Training epoch 1:   4%|3         | 197/5556 [02:28<1:06:09,  1.35it/s]Training epoch 1:   4%|3         | 198/5556 [02:29<1:06:43,  1.34it/s]Training epoch 1:   4%|3         | 199/5556 [02:30<1:06:34,  1.34it/s]Training epoch 1:   4%|3         | 200/5556 [02:30<1:08:54,  1.30it/s]Training epoch 1:   4%|3         | 201/5556 [02:31<1:07:55,  1.31it/s]Training epoch 1:   4%|3         | 202/5556 [02:32<1:07:31,  1.32it/s]Training epoch 1:   4%|3         | 203/5556 [02:33<1:06:11,  1.35it/s]Training epoch 1:   4%|3         | 204/5556 [02:33<1:05:23,  1.36it/s]Training epoch 1:   4%|3         | 205/5556 [02:34<1:05:56,  1.35it/s]Training epoch 1:   4%|3         | 206/5556 [02:35<1:07:21,  1.32it/s]Training epoch 1:   4%|3         | 207/5556 [02:36<1:08:22,  1.30it/s]Training epoch 1:   4%|3         | 208/5556 [02:36<1:07:52,  1.31it/s]Training epoch 1:   4%|3         | 209/5556 [02:37<1:08:10,  1.31it/s]Training epoch 1:   4%|3         | 210/5556 [02:38<1:07:28,  1.32it/s]Training epoch 1:   4%|3         | 211/5556 [02:39<1:07:39,  1.32it/s]Training epoch 1:   4%|3         | 212/5556 [02:39<1:07:36,  1.32it/s]Training epoch 1:   4%|3         | 213/5556 [02:40<1:08:15,  1.30it/s]Training epoch 1:   4%|3         | 214/5556 [02:41<1:08:57,  1.29it/s]Training epoch 1:   4%|3         | 215/5556 [02:42<1:08:18,  1.30it/s]Training epoch 1:   4%|3         | 216/5556 [02:42<1:06:48,  1.33it/s]Training epoch 1:   4%|3         | 217/5556 [02:43<1:06:03,  1.35it/s]Training epoch 1:   4%|3         | 218/5556 [02:44<1:06:08,  1.35it/s]Training epoch 1:   4%|3         | 219/5556 [02:45<1:06:14,  1.34it/s]Training epoch 1:   4%|3         | 220/5556 [02:46<1:07:57,  1.31it/s]Training epoch 1:   4%|3         | 221/5556 [02:46<1:07:50,  1.31it/s]Training epoch 1:   4%|3         | 222/5556 [02:47<1:07:31,  1.32it/s]Training epoch 1:   4%|4         | 223/5556 [02:48<1:08:11,  1.30it/s]Training epoch 1:   4%|4         | 224/5556 [02:49<1:07:30,  1.32it/s]Training epoch 1:   4%|4         | 225/5556 [02:49<1:07:54,  1.31it/s]Training epoch 1:   4%|4         | 226/5556 [02:50<1:07:40,  1.31it/s]Training epoch 1:   4%|4         | 227/5556 [02:51<1:06:26,  1.34it/s]Training epoch 1:   4%|4         | 228/5556 [02:52<1:06:33,  1.33it/s]Training epoch 1:   4%|4         | 229/5556 [02:52<1:06:54,  1.33it/s]Training epoch 1:   4%|4         | 230/5556 [02:53<1:07:08,  1.32it/s]Training epoch 1:   4%|4         | 231/5556 [02:54<1:06:26,  1.34it/s]Training epoch 1:   4%|4         | 232/5556 [02:55<1:07:02,  1.32it/s]Training epoch 1:   4%|4         | 233/5556 [02:55<1:06:53,  1.33it/s]Training epoch 1:   4%|4         | 234/5556 [02:56<1:07:28,  1.31it/s]Training epoch 1:   4%|4         | 235/5556 [02:57<1:07:29,  1.31it/s]Training epoch 1:   4%|4         | 236/5556 [02:58<1:07:05,  1.32it/s]Training epoch 1:   4%|4         | 237/5556 [02:58<1:06:45,  1.33it/s]Training epoch 1:   4%|4         | 238/5556 [02:59<1:06:31,  1.33it/s]Training epoch 1:   4%|4         | 239/5556 [03:00<1:06:38,  1.33it/s]Training epoch 1:   4%|4         | 240/5556 [03:01<1:07:26,  1.31it/s]Training epoch 1:   4%|4         | 241/5556 [03:01<1:06:37,  1.33it/s]Training epoch 1:   4%|4         | 242/5556 [03:02<1:07:13,  1.32it/s]Training epoch 1:   4%|4         | 243/5556 [03:03<1:07:46,  1.31it/s]Training epoch 1:   4%|4         | 244/5556 [03:04<1:08:14,  1.30it/s]Training epoch 1:   4%|4         | 245/5556 [03:04<1:06:51,  1.32it/s]Training epoch 1:   4%|4         | 246/5556 [03:05<1:06:34,  1.33it/s]Training epoch 1:   4%|4         | 247/5556 [03:06<1:06:23,  1.33it/s]Training epoch 1:   4%|4         | 248/5556 [03:07<1:05:27,  1.35it/s]Training epoch 1:   4%|4         | 249/5556 [03:07<1:05:43,  1.35it/s]Training epoch 1:   4%|4         | 250/5556 [03:08<1:06:30,  1.33it/s]Training epoch 1:   5%|4         | 251/5556 [03:09<1:06:49,  1.32it/s]Training epoch 1:   5%|4         | 252/5556 [03:10<1:06:55,  1.32it/s]Training epoch 1:   5%|4         | 253/5556 [03:10<1:07:25,  1.31it/s]Training epoch 1:   5%|4         | 254/5556 [03:11<1:06:51,  1.32it/s]Training epoch 1:   5%|4         | 255/5556 [03:12<1:06:36,  1.33it/s]Training epoch 1:   5%|4         | 256/5556 [03:13<1:06:27,  1.33it/s]Training epoch 1:   5%|4         | 257/5556 [03:13<1:06:23,  1.33it/s]Training epoch 1:   5%|4         | 258/5556 [03:14<1:06:07,  1.34it/s]Training epoch 1:   5%|4         | 259/5556 [03:15<1:06:28,  1.33it/s]Training epoch 1:   5%|4         | 260/5556 [03:16<1:06:03,  1.34it/s]Training epoch 1:   5%|4         | 261/5556 [03:16<1:05:44,  1.34it/s]Training epoch 1:   5%|4         | 262/5556 [03:17<1:05:40,  1.34it/s]Training epoch 1:   5%|4         | 263/5556 [03:18<1:06:02,  1.34it/s]Training epoch 1:   5%|4         | 264/5556 [03:19<1:05:26,  1.35it/s]Training epoch 1:   5%|4         | 265/5556 [03:19<1:05:31,  1.35it/s]Training epoch 1:   5%|4         | 266/5556 [03:20<1:06:04,  1.33it/s]Training epoch 1:   5%|4         | 267/5556 [03:21<1:05:42,  1.34it/s]Training epoch 1:   5%|4         | 268/5556 [03:22<1:06:26,  1.33it/s]Training epoch 1:   5%|4         | 269/5556 [03:22<1:05:46,  1.34it/s]Training epoch 1:   5%|4         | 270/5556 [03:23<1:05:43,  1.34it/s]Training epoch 1:   5%|4         | 271/5556 [03:24<1:06:20,  1.33it/s]Training epoch 1:   5%|4         | 272/5556 [03:25<1:06:55,  1.32it/s]Training epoch 1:   5%|4         | 273/5556 [03:25<1:06:32,  1.32it/s]Training epoch 1:   5%|4         | 274/5556 [03:26<1:06:14,  1.33it/s]Training epoch 1:   5%|4         | 275/5556 [03:27<1:06:17,  1.33it/s]Training epoch 1:   5%|4         | 276/5556 [03:28<1:05:39,  1.34it/s]Training epoch 1:   5%|4         | 277/5556 [03:28<1:07:11,  1.31it/s]Training epoch 1:   5%|5         | 278/5556 [03:29<1:06:51,  1.32it/s]Training epoch 1:   5%|5         | 279/5556 [03:30<1:07:20,  1.31it/s]Training epoch 1:   5%|5         | 280/5556 [03:31<1:07:02,  1.31it/s]Training epoch 1:   5%|5         | 281/5556 [03:32<1:07:14,  1.31it/s]Training epoch 1:   5%|5         | 282/5556 [03:32<1:06:47,  1.32it/s]Training epoch 1:   5%|5         | 283/5556 [03:33<1:06:43,  1.32it/s]Training epoch 1:   5%|5         | 284/5556 [03:34<1:06:01,  1.33it/s]Training epoch 1:   5%|5         | 285/5556 [03:35<1:06:14,  1.33it/s]Training epoch 1:   5%|5         | 286/5556 [03:35<1:05:43,  1.34it/s]Training epoch 1:   5%|5         | 287/5556 [03:36<1:05:42,  1.34it/s]Training epoch 1:   5%|5         | 288/5556 [03:37<1:05:39,  1.34it/s]Training epoch 1:   5%|5         | 289/5556 [03:38<1:06:14,  1.33it/s]Training epoch 1:   5%|5         | 290/5556 [03:38<1:06:44,  1.32it/s]Training epoch 1:   5%|5         | 291/5556 [03:39<1:06:25,  1.32it/s]Training epoch 1:   5%|5         | 292/5556 [03:40<1:05:53,  1.33it/s]Training epoch 1:   5%|5         | 293/5556 [03:41<1:06:24,  1.32it/s]Training epoch 1:   5%|5         | 294/5556 [03:41<1:06:29,  1.32it/s]Training epoch 1:   5%|5         | 295/5556 [03:42<1:05:32,  1.34it/s]Training epoch 1:   5%|5         | 296/5556 [03:43<1:04:52,  1.35it/s]Training epoch 1:   5%|5         | 297/5556 [03:43<1:04:15,  1.36it/s]Training epoch 1:   5%|5         | 298/5556 [03:44<1:05:03,  1.35it/s]Training epoch 1:   5%|5         | 299/5556 [03:45<1:05:39,  1.33it/s]Training epoch 1:   5%|5         | 300/5556 [03:46<1:08:26,  1.28it/s]Training epoch 1:   5%|5         | 301/5556 [03:47<1:05:48,  1.33it/s]Training epoch 1:   5%|5         | 302/5556 [03:47<1:06:52,  1.31it/s]Training epoch 1:   5%|5         | 303/5556 [03:48<1:06:38,  1.31it/s]Training epoch 1:   5%|5         | 304/5556 [03:49<1:06:25,  1.32it/s]Training epoch 1:   5%|5         | 305/5556 [03:50<1:06:34,  1.31it/s]Training epoch 1:   6%|5         | 306/5556 [03:50<1:05:19,  1.34it/s]Training epoch 1:   6%|5         | 307/5556 [03:51<1:06:49,  1.31it/s]Training epoch 1:   6%|5         | 308/5556 [03:52<1:06:19,  1.32it/s]Training epoch 1:   6%|5         | 309/5556 [03:53<1:05:51,  1.33it/s]Training epoch 1:   6%|5         | 310/5556 [03:53<1:06:24,  1.32it/s]Training epoch 1:   6%|5         | 311/5556 [03:54<1:06:40,  1.31it/s]Training epoch 1:   6%|5         | 312/5556 [03:55<1:06:48,  1.31it/s]Training epoch 1:   6%|5         | 313/5556 [03:56<1:06:05,  1.32it/s]Training epoch 1:   6%|5         | 314/5556 [03:56<1:05:30,  1.33it/s]Training epoch 1:   6%|5         | 315/5556 [03:57<1:05:53,  1.33it/s]Training epoch 1:   6%|5         | 316/5556 [03:58<1:06:02,  1.32it/s]Training epoch 1:   6%|5         | 317/5556 [03:59<1:05:42,  1.33it/s]Training epoch 1:   6%|5         | 318/5556 [03:59<1:05:40,  1.33it/s]Training epoch 1:   6%|5         | 319/5556 [04:00<1:05:48,  1.33it/s]Training epoch 1:   6%|5         | 320/5556 [04:01<1:05:16,  1.34it/s]Training epoch 1:   6%|5         | 321/5556 [04:02<1:04:25,  1.35it/s]Training epoch 1:   6%|5         | 322/5556 [04:02<1:04:51,  1.34it/s]Training epoch 1:   6%|5         | 323/5556 [04:03<1:04:51,  1.34it/s]Training epoch 1:   6%|5         | 324/5556 [04:04<1:04:49,  1.35it/s]Training epoch 1:   6%|5         | 325/5556 [04:05<1:05:22,  1.33it/s]Training epoch 1:   6%|5         | 326/5556 [04:05<1:05:15,  1.34it/s]Training epoch 1:   6%|5         | 327/5556 [04:06<1:05:59,  1.32it/s]Training epoch 1:   6%|5         | 328/5556 [04:07<1:05:37,  1.33it/s]Training epoch 1:   6%|5         | 329/5556 [04:08<1:05:48,  1.32it/s]Training epoch 1:   6%|5         | 330/5556 [04:08<1:05:16,  1.33it/s]Training epoch 1:   6%|5         | 331/5556 [04:09<1:06:17,  1.31it/s]Training epoch 1:   6%|5         | 332/5556 [04:10<1:05:43,  1.32it/s]Training epoch 1:   6%|5         | 333/5556 [04:11<1:06:39,  1.31it/s]Training epoch 1:   6%|6         | 334/5556 [04:11<1:06:37,  1.31it/s]Training epoch 1:   6%|6         | 335/5556 [04:12<1:06:05,  1.32it/s]Training epoch 1:   6%|6         | 336/5556 [04:13<1:05:51,  1.32it/s]Training epoch 1:   6%|6         | 337/5556 [04:14<1:05:28,  1.33it/s]Training epoch 1:   6%|6         | 338/5556 [04:14<1:04:20,  1.35it/s]Training epoch 1:   6%|6         | 339/5556 [04:15<1:04:04,  1.36it/s]Training epoch 1:   6%|6         | 340/5556 [04:16<1:04:58,  1.34it/s]Training epoch 1:   6%|6         | 341/5556 [04:17<1:05:41,  1.32it/s]Training epoch 1:   6%|6         | 342/5556 [04:17<1:05:15,  1.33it/s]Training epoch 1:   6%|6         | 343/5556 [04:18<1:05:51,  1.32it/s]Training epoch 1:   6%|6         | 344/5556 [04:19<1:06:07,  1.31it/s]Training epoch 1:   6%|6         | 345/5556 [04:20<1:06:04,  1.31it/s]Training epoch 1:   6%|6         | 346/5556 [04:20<1:04:46,  1.34it/s]Training epoch 1:   6%|6         | 347/5556 [04:21<1:04:25,  1.35it/s]Training epoch 1:   6%|6         | 348/5556 [04:22<1:04:57,  1.34it/s]Training epoch 1:   6%|6         | 349/5556 [04:23<1:05:33,  1.32it/s]Training epoch 1:   6%|6         | 350/5556 [04:23<1:05:11,  1.33it/s]Training epoch 1:   6%|6         | 351/5556 [04:24<1:04:01,  1.35it/s]Training epoch 1:   6%|6         | 352/5556 [04:25<1:03:37,  1.36it/s]Training epoch 1:   6%|6         | 353/5556 [04:26<1:04:05,  1.35it/s]Training epoch 1:   6%|6         | 354/5556 [04:26<1:04:06,  1.35it/s]Training epoch 1:   6%|6         | 355/5556 [04:27<1:03:28,  1.37it/s]Training epoch 1:   6%|6         | 356/5556 [04:28<1:04:23,  1.35it/s]Training epoch 1:   6%|6         | 357/5556 [04:29<1:04:24,  1.35it/s]Training epoch 1:   6%|6         | 358/5556 [04:29<1:04:14,  1.35it/s]Training epoch 1:   6%|6         | 359/5556 [04:30<1:04:19,  1.35it/s]Training epoch 1:   6%|6         | 360/5556 [04:31<1:04:44,  1.34it/s]Training epoch 1:   6%|6         | 361/5556 [04:32<1:04:34,  1.34it/s]Training epoch 1:   7%|6         | 362/5556 [04:32<1:04:28,  1.34it/s]Training epoch 1:   7%|6         | 363/5556 [04:33<1:05:15,  1.33it/s]Training epoch 1:   7%|6         | 364/5556 [04:34<1:05:15,  1.33it/s]Training epoch 1:   7%|6         | 365/5556 [04:35<1:04:56,  1.33it/s]Training epoch 1:   7%|6         | 366/5556 [04:35<1:04:58,  1.33it/s]Training epoch 1:   7%|6         | 367/5556 [04:36<1:04:53,  1.33it/s]Training epoch 1:   7%|6         | 368/5556 [04:37<1:05:05,  1.33it/s]Training epoch 1:   7%|6         | 369/5556 [04:38<1:05:41,  1.32it/s]Training epoch 1:   7%|6         | 370/5556 [04:38<1:05:44,  1.31it/s]Training epoch 1:   7%|6         | 371/5556 [04:39<1:06:35,  1.30it/s]Training epoch 1:   7%|6         | 372/5556 [04:40<1:06:47,  1.29it/s]Training epoch 1:   7%|6         | 373/5556 [04:41<1:05:21,  1.32it/s]Training epoch 1:   7%|6         | 374/5556 [04:41<1:04:30,  1.34it/s]Training epoch 1:   7%|6         | 375/5556 [04:42<1:04:25,  1.34it/s]Training epoch 1:   7%|6         | 376/5556 [04:43<1:04:02,  1.35it/s]Training epoch 1:   7%|6         | 377/5556 [04:44<1:04:52,  1.33it/s]Training epoch 1:   7%|6         | 378/5556 [04:44<1:04:17,  1.34it/s]Training epoch 1:   7%|6         | 379/5556 [04:45<1:04:53,  1.33it/s]Training epoch 1:   7%|6         | 380/5556 [04:46<1:05:34,  1.32it/s]Training epoch 1:   7%|6         | 381/5556 [04:47<1:05:49,  1.31it/s]Training epoch 1:   7%|6         | 382/5556 [04:47<1:05:12,  1.32it/s]Training epoch 1:   7%|6         | 383/5556 [04:48<1:05:51,  1.31it/s]Training epoch 1:   7%|6         | 384/5556 [04:49<1:06:26,  1.30it/s]Training epoch 1:   7%|6         | 385/5556 [04:50<1:05:06,  1.32it/s]Training epoch 1:   7%|6         | 386/5556 [04:51<1:04:37,  1.33it/s]Training epoch 1:   7%|6         | 387/5556 [04:51<1:03:48,  1.35it/s]Training epoch 1:   7%|6         | 388/5556 [04:52<1:03:43,  1.35it/s]Training epoch 1:   7%|7         | 389/5556 [04:53<1:04:05,  1.34it/s]Training epoch 1:   7%|7         | 390/5556 [04:53<1:04:06,  1.34it/s]Training epoch 1:   7%|7         | 391/5556 [04:54<1:03:15,  1.36it/s]Training epoch 1:   7%|7         | 392/5556 [04:55<1:03:37,  1.35it/s]Training epoch 1:   7%|7         | 393/5556 [04:56<1:03:18,  1.36it/s]Training epoch 1:   7%|7         | 394/5556 [04:56<1:03:52,  1.35it/s]Training epoch 1:   7%|7         | 395/5556 [04:57<1:03:46,  1.35it/s]Training epoch 1:   7%|7         | 396/5556 [04:58<1:04:29,  1.33it/s]Training epoch 1:   7%|7         | 397/5556 [04:59<1:04:47,  1.33it/s]Training epoch 1:   7%|7         | 398/5556 [04:59<1:04:43,  1.33it/s]Training epoch 1:   7%|7         | 399/5556 [05:00<1:05:32,  1.31it/s]Training epoch 1:   7%|7         | 400/5556 [05:01<1:07:31,  1.27it/s]Training epoch 1:   7%|7         | 401/5556 [05:02<1:06:41,  1.29it/s]Training epoch 1:   7%|7         | 402/5556 [05:03<1:06:20,  1.29it/s]Training epoch 1:   7%|7         | 403/5556 [05:03<1:05:18,  1.31it/s]Training epoch 1:   7%|7         | 404/5556 [05:04<1:05:46,  1.31it/s]Training epoch 1:   7%|7         | 405/5556 [05:05<1:05:48,  1.30it/s]Training epoch 1:   7%|7         | 406/5556 [05:06<1:06:20,  1.29it/s]Training epoch 1:   7%|7         | 407/5556 [05:06<1:06:36,  1.29it/s]Training epoch 1:   7%|7         | 408/5556 [05:07<1:06:01,  1.30it/s]Training epoch 1:   7%|7         | 409/5556 [05:08<1:05:51,  1.30it/s]Training epoch 1:   7%|7         | 410/5556 [05:09<1:04:53,  1.32it/s]Training epoch 1:   7%|7         | 411/5556 [05:09<1:04:12,  1.34it/s]Training epoch 1:   7%|7         | 412/5556 [05:10<1:03:21,  1.35it/s]Training epoch 1:   7%|7         | 413/5556 [05:11<1:03:27,  1.35it/s]Training epoch 1:   7%|7         | 414/5556 [05:12<1:03:19,  1.35it/s]Training epoch 1:   7%|7         | 415/5556 [05:12<1:03:19,  1.35it/s]Training epoch 1:   7%|7         | 416/5556 [05:13<1:03:47,  1.34it/s]Training epoch 1:   8%|7         | 417/5556 [05:14<1:04:45,  1.32it/s]Training epoch 1:   8%|7         | 418/5556 [05:15<1:04:34,  1.33it/s]Training epoch 1:   8%|7         | 419/5556 [05:15<1:04:00,  1.34it/s]Training epoch 1:   8%|7         | 420/5556 [05:16<1:04:03,  1.34it/s]Training epoch 1:   8%|7         | 421/5556 [05:17<1:03:04,  1.36it/s]Training epoch 1:   8%|7         | 422/5556 [05:18<1:02:45,  1.36it/s]Training epoch 1:   8%|7         | 423/5556 [05:18<1:04:19,  1.33it/s]Training epoch 1:   8%|7         | 424/5556 [05:19<1:04:29,  1.33it/s]Training epoch 1:   8%|7         | 425/5556 [05:20<1:05:07,  1.31it/s]Training epoch 1:   8%|7         | 426/5556 [05:21<1:04:56,  1.32it/s]Training epoch 1:   8%|7         | 427/5556 [05:21<1:04:21,  1.33it/s]Training epoch 1:   8%|7         | 428/5556 [05:22<1:04:43,  1.32it/s]Training epoch 1:   8%|7         | 429/5556 [05:23<1:04:56,  1.32it/s]Training epoch 1:   8%|7         | 430/5556 [05:24<1:04:20,  1.33it/s]Training epoch 1:   8%|7         | 431/5556 [05:24<1:04:11,  1.33it/s]Training epoch 1:   8%|7         | 432/5556 [05:25<1:04:43,  1.32it/s]Training epoch 1:   8%|7         | 433/5556 [05:26<1:03:33,  1.34it/s]Training epoch 1:   8%|7         | 434/5556 [05:27<1:03:30,  1.34it/s]Training epoch 1:   8%|7         | 435/5556 [05:27<1:03:16,  1.35it/s]Training epoch 1:   8%|7         | 436/5556 [05:28<1:02:50,  1.36it/s]Training epoch 1:   8%|7         | 437/5556 [05:29<1:03:41,  1.34it/s]Training epoch 1:   8%|7         | 438/5556 [05:30<1:03:27,  1.34it/s]Training epoch 1:   8%|7         | 439/5556 [05:30<1:02:31,  1.36it/s]Training epoch 1:   8%|7         | 440/5556 [05:31<1:02:41,  1.36it/s]Training epoch 1:   8%|7         | 441/5556 [05:32<1:03:28,  1.34it/s]Training epoch 1:   8%|7         | 442/5556 [05:33<1:03:09,  1.35it/s]Training epoch 1:   8%|7         | 443/5556 [05:33<1:03:58,  1.33it/s]Training epoch 1:   8%|7         | 444/5556 [05:34<1:04:56,  1.31it/s]Training epoch 1:   8%|8         | 445/5556 [05:35<1:04:57,  1.31it/s]Training epoch 1:   8%|8         | 446/5556 [05:36<1:05:02,  1.31it/s]Training epoch 1:   8%|8         | 447/5556 [05:36<1:05:50,  1.29it/s]Training epoch 1:   8%|8         | 448/5556 [05:37<1:04:49,  1.31it/s]Training epoch 1:   8%|8         | 449/5556 [05:38<1:04:07,  1.33it/s]Training epoch 1:   8%|8         | 450/5556 [05:39<1:05:00,  1.31it/s]Training epoch 1:   8%|8         | 451/5556 [05:39<1:04:29,  1.32it/s]Training epoch 1:   8%|8         | 452/5556 [05:40<1:04:49,  1.31it/s]Training epoch 1:   8%|8         | 453/5556 [05:41<1:05:15,  1.30it/s]Training epoch 1:   8%|8         | 454/5556 [05:42<1:04:56,  1.31it/s]Training epoch 1:   8%|8         | 455/5556 [05:42<1:03:36,  1.34it/s]Training epoch 1:   8%|8         | 456/5556 [05:43<1:03:52,  1.33it/s]Training epoch 1:   8%|8         | 457/5556 [05:44<1:03:47,  1.33it/s]Training epoch 1:   8%|8         | 458/5556 [05:45<1:04:33,  1.32it/s]Training epoch 1:   8%|8         | 459/5556 [05:46<1:04:50,  1.31it/s]Training epoch 1:   8%|8         | 460/5556 [05:46<1:04:39,  1.31it/s]Training epoch 1:   8%|8         | 461/5556 [05:47<1:04:52,  1.31it/s]Training epoch 1:   8%|8         | 462/5556 [05:48<1:05:14,  1.30it/s]Training epoch 1:   8%|8         | 463/5556 [05:49<1:04:34,  1.31it/s]Training epoch 1:   8%|8         | 464/5556 [05:49<1:03:18,  1.34it/s]Training epoch 1:   8%|8         | 465/5556 [05:50<1:03:43,  1.33it/s]Training epoch 1:   8%|8         | 466/5556 [05:51<1:03:29,  1.34it/s]Training epoch 1:   8%|8         | 467/5556 [05:52<1:03:28,  1.34it/s]Training epoch 1:   8%|8         | 468/5556 [05:52<1:03:18,  1.34it/s]Training epoch 1:   8%|8         | 469/5556 [05:53<1:02:39,  1.35it/s]Training epoch 1:   8%|8         | 470/5556 [05:54<1:02:35,  1.35it/s]Training epoch 1:   8%|8         | 471/5556 [05:54<1:02:38,  1.35it/s]Training epoch 1:   8%|8         | 472/5556 [05:55<1:02:53,  1.35it/s]Training epoch 1:   9%|8         | 473/5556 [05:56<1:03:21,  1.34it/s]Training epoch 1:   9%|8         | 474/5556 [05:57<1:03:04,  1.34it/s]Training epoch 1:   9%|8         | 475/5556 [05:57<1:02:37,  1.35it/s]Training epoch 1:   9%|8         | 476/5556 [05:58<1:02:48,  1.35it/s]Training epoch 1:   9%|8         | 477/5556 [05:59<1:02:52,  1.35it/s]Training epoch 1:   9%|8         | 478/5556 [06:00<1:02:12,  1.36it/s]Training epoch 1:   9%|8         | 479/5556 [06:00<1:02:38,  1.35it/s]Training epoch 1:   9%|8         | 480/5556 [06:01<1:02:37,  1.35it/s]Training epoch 1:   9%|8         | 481/5556 [06:02<1:02:10,  1.36it/s]Training epoch 1:   9%|8         | 482/5556 [06:03<1:01:40,  1.37it/s]Training epoch 1:   9%|8         | 483/5556 [06:03<1:02:22,  1.36it/s]Training epoch 1:   9%|8         | 484/5556 [06:04<1:02:33,  1.35it/s]Training epoch 1:   9%|8         | 485/5556 [06:05<1:02:10,  1.36it/s]Training epoch 1:   9%|8         | 486/5556 [06:06<1:02:20,  1.36it/s]Training epoch 1:   9%|8         | 487/5556 [06:06<1:02:31,  1.35it/s]Training epoch 1:   9%|8         | 488/5556 [06:07<1:03:00,  1.34it/s]Training epoch 1:   9%|8         | 489/5556 [06:08<1:02:11,  1.36it/s]Training epoch 1:   9%|8         | 490/5556 [06:09<1:02:39,  1.35it/s]Training epoch 1:   9%|8         | 491/5556 [06:09<1:03:01,  1.34it/s]Training epoch 1:   9%|8         | 492/5556 [06:10<1:03:47,  1.32it/s]Training epoch 1:   9%|8         | 493/5556 [06:11<1:03:41,  1.33it/s]Training epoch 1:   9%|8         | 494/5556 [06:12<1:03:53,  1.32it/s]Training epoch 1:   9%|8         | 495/5556 [06:12<1:04:22,  1.31it/s]Training epoch 1:   9%|8         | 496/5556 [06:13<1:04:05,  1.32it/s]Training epoch 1:   9%|8         | 497/5556 [06:14<1:03:49,  1.32it/s]Training epoch 1:   9%|8         | 498/5556 [06:15<1:03:41,  1.32it/s]Training epoch 1:   9%|8         | 499/5556 [06:15<1:03:04,  1.34it/s]Training epoch 1:   9%|8         | 500/5556 [06:16<1:05:56,  1.28it/s]Training epoch 1:   9%|9         | 501/5556 [06:17<1:05:35,  1.28it/s]Training epoch 1:   9%|9         | 502/5556 [06:18<1:04:45,  1.30it/s]Training epoch 1:   9%|9         | 503/5556 [06:18<1:04:35,  1.30it/s]Training epoch 1:   9%|9         | 504/5556 [06:19<1:04:14,  1.31it/s]Training epoch 1:   9%|9         | 505/5556 [06:20<1:03:59,  1.32it/s]Training epoch 1:   9%|9         | 506/5556 [06:21<1:04:04,  1.31it/s]Training epoch 1:   9%|9         | 507/5556 [06:22<1:04:29,  1.30it/s]Training epoch 1:   9%|9         | 508/5556 [06:22<1:03:40,  1.32it/s]Training epoch 1:   9%|9         | 509/5556 [06:23<1:03:13,  1.33it/s]Training epoch 1:   9%|9         | 510/5556 [06:24<1:02:42,  1.34it/s]Training epoch 1:   9%|9         | 511/5556 [06:24<1:02:59,  1.33it/s]Training epoch 1:   9%|9         | 512/5556 [06:25<1:02:55,  1.34it/s]Training epoch 1:   9%|9         | 513/5556 [06:26<1:03:01,  1.33it/s]Training epoch 1:   9%|9         | 514/5556 [06:27<1:03:29,  1.32it/s]Training epoch 1:   9%|9         | 515/5556 [06:28<1:03:30,  1.32it/s]Training epoch 1:   9%|9         | 516/5556 [06:28<1:03:13,  1.33it/s]Training epoch 1:   9%|9         | 517/5556 [06:29<1:03:05,  1.33it/s]Training epoch 1:   9%|9         | 518/5556 [06:30<1:02:21,  1.35it/s]Training epoch 1:   9%|9         | 519/5556 [06:31<1:03:13,  1.33it/s]Training epoch 1:   9%|9         | 520/5556 [06:31<1:02:46,  1.34it/s]Training epoch 1:   9%|9         | 521/5556 [06:32<1:02:15,  1.35it/s]Training epoch 1:   9%|9         | 522/5556 [06:33<1:02:49,  1.34it/s]Training epoch 1:   9%|9         | 523/5556 [06:34<1:03:19,  1.32it/s]Training epoch 1:   9%|9         | 524/5556 [06:34<1:03:16,  1.33it/s]Training epoch 1:   9%|9         | 525/5556 [06:35<1:03:00,  1.33it/s]Training epoch 1:   9%|9         | 526/5556 [06:36<1:02:32,  1.34it/s]Training epoch 1:   9%|9         | 527/5556 [06:37<1:03:46,  1.31it/s]Training epoch 1:  10%|9         | 528/5556 [06:37<1:03:49,  1.31it/s]Training epoch 1:  10%|9         | 529/5556 [06:38<1:03:44,  1.31it/s]Training epoch 1:  10%|9         | 530/5556 [06:39<1:03:22,  1.32it/s]Training epoch 1:  10%|9         | 531/5556 [06:40<1:03:51,  1.31it/s]Training epoch 1:  10%|9         | 532/5556 [06:40<1:03:49,  1.31it/s]Training epoch 1:  10%|9         | 533/5556 [06:41<1:02:57,  1.33it/s]Training epoch 1:  10%|9         | 534/5556 [06:42<1:03:19,  1.32it/s]Training epoch 1:  10%|9         | 535/5556 [06:43<1:03:58,  1.31it/s]Training epoch 1:  10%|9         | 536/5556 [06:43<1:03:30,  1.32it/s]Training epoch 1:  10%|9         | 537/5556 [06:44<1:03:09,  1.32it/s]Training epoch 1:  10%|9         | 538/5556 [06:45<1:03:21,  1.32it/s]Training epoch 1:  10%|9         | 539/5556 [06:46<1:02:53,  1.33it/s]Training epoch 1:  10%|9         | 540/5556 [06:46<1:02:48,  1.33it/s]Training epoch 1:  10%|9         | 541/5556 [06:47<1:02:17,  1.34it/s]Training epoch 1:  10%|9         | 542/5556 [06:48<1:01:28,  1.36it/s]Training epoch 1:  10%|9         | 543/5556 [06:49<1:01:42,  1.35it/s]Training epoch 1:  10%|9         | 544/5556 [06:49<1:01:39,  1.35it/s]Training epoch 1:  10%|9         | 545/5556 [06:50<1:01:42,  1.35it/s]Training epoch 1:  10%|9         | 546/5556 [06:51<1:01:57,  1.35it/s]Training epoch 1:  10%|9         | 547/5556 [06:52<1:02:45,  1.33it/s]Training epoch 1:  10%|9         | 548/5556 [06:52<1:01:45,  1.35it/s]Training epoch 1:  10%|9         | 549/5556 [06:53<1:01:18,  1.36it/s]Training epoch 1:  10%|9         | 550/5556 [06:54<1:01:36,  1.35it/s]Training epoch 1:  10%|9         | 551/5556 [06:54<1:01:52,  1.35it/s]Training epoch 1:  10%|9         | 552/5556 [06:55<1:01:56,  1.35it/s]Training epoch 1:  10%|9         | 553/5556 [06:56<1:02:43,  1.33it/s]Training epoch 1:  10%|9         | 554/5556 [06:57<1:02:20,  1.34it/s]Training epoch 1:  10%|9         | 555/5556 [06:58<1:03:39,  1.31it/s]Training epoch 1:  10%|#         | 556/5556 [06:58<1:03:25,  1.31it/s]Training epoch 1:  10%|#         | 557/5556 [06:59<1:03:37,  1.31it/s]Training epoch 1:  10%|#         | 558/5556 [07:00<1:02:28,  1.33it/s]Training epoch 1:  10%|#         | 559/5556 [07:01<1:03:02,  1.32it/s]Training epoch 1:  10%|#         | 560/5556 [07:01<1:02:40,  1.33it/s]Training epoch 1:  10%|#         | 561/5556 [07:02<1:02:46,  1.33it/s]Training epoch 1:  10%|#         | 562/5556 [07:03<1:02:34,  1.33it/s]Training epoch 1:  10%|#         | 563/5556 [07:04<1:02:27,  1.33it/s]Training epoch 1:  10%|#         | 564/5556 [07:04<1:03:11,  1.32it/s]Training epoch 1:  10%|#         | 565/5556 [07:05<1:03:17,  1.31it/s]Training epoch 1:  10%|#         | 566/5556 [07:06<1:03:00,  1.32it/s]Training epoch 1:  10%|#         | 567/5556 [07:07<1:02:47,  1.32it/s]Training epoch 1:  10%|#         | 568/5556 [07:07<1:01:38,  1.35it/s]Training epoch 1:  10%|#         | 569/5556 [07:08<1:01:45,  1.35it/s]Training epoch 1:  10%|#         | 570/5556 [07:09<1:02:01,  1.34it/s]Training epoch 1:  10%|#         | 571/5556 [07:10<1:01:47,  1.34it/s]Training epoch 1:  10%|#         | 572/5556 [07:10<1:01:38,  1.35it/s]Training epoch 1:  10%|#         | 573/5556 [07:11<1:01:27,  1.35it/s]Training epoch 1:  10%|#         | 574/5556 [07:12<1:02:23,  1.33it/s]Training epoch 1:  10%|#         | 575/5556 [07:13<1:01:38,  1.35it/s]Training epoch 1:  10%|#         | 576/5556 [07:13<1:01:24,  1.35it/s]Training epoch 1:  10%|#         | 577/5556 [07:14<1:01:56,  1.34it/s]Training epoch 1:  10%|#         | 578/5556 [07:15<1:02:28,  1.33it/s]Training epoch 1:  10%|#         | 579/5556 [07:15<1:01:24,  1.35it/s]Training epoch 1:  10%|#         | 580/5556 [07:16<1:01:25,  1.35it/s]Training epoch 1:  10%|#         | 581/5556 [07:17<1:01:13,  1.35it/s]Training epoch 1:  10%|#         | 582/5556 [07:18<1:01:38,  1.35it/s]Training epoch 1:  10%|#         | 583/5556 [07:18<1:02:28,  1.33it/s]Training epoch 1:  11%|#         | 584/5556 [07:19<1:01:59,  1.34it/s]Training epoch 1:  11%|#         | 585/5556 [07:20<1:01:42,  1.34it/s]Training epoch 1:  11%|#         | 586/5556 [07:21<1:02:18,  1.33it/s]Training epoch 1:  11%|#         | 587/5556 [07:21<1:01:49,  1.34it/s]Training epoch 1:  11%|#         | 588/5556 [07:22<1:01:50,  1.34it/s]Training epoch 1:  11%|#         | 589/5556 [07:23<1:01:43,  1.34it/s]Training epoch 1:  11%|#         | 590/5556 [07:24<1:01:51,  1.34it/s]Training epoch 1:  11%|#         | 591/5556 [07:24<1:01:20,  1.35it/s]Training epoch 1:  11%|#         | 592/5556 [07:25<1:02:01,  1.33it/s]Training epoch 1:  11%|#         | 593/5556 [07:26<1:01:55,  1.34it/s]Training epoch 1:  11%|#         | 594/5556 [07:27<1:02:07,  1.33it/s]Training epoch 1:  11%|#         | 595/5556 [07:27<1:02:56,  1.31it/s]Training epoch 1:  11%|#         | 596/5556 [07:28<1:02:09,  1.33it/s]Training epoch 1:  11%|#         | 597/5556 [07:29<1:01:46,  1.34it/s]Training epoch 1:  11%|#         | 598/5556 [07:30<1:02:08,  1.33it/s]Training epoch 1:  11%|#         | 599/5556 [07:30<1:02:24,  1.32it/s]Training epoch 1:  11%|#         | 600/5556 [07:31<1:05:16,  1.27it/s]Training epoch 1:  11%|#         | 601/5556 [07:32<1:04:47,  1.27it/s]Training epoch 1:  11%|#         | 602/5556 [07:33<1:02:51,  1.31it/s]Training epoch 1:  11%|#         | 603/5556 [07:34<1:02:03,  1.33it/s]Training epoch 1:  11%|#         | 604/5556 [07:34<1:02:02,  1.33it/s]Training epoch 1:  11%|#         | 605/5556 [07:35<1:02:38,  1.32it/s]Training epoch 1:  11%|#         | 606/5556 [07:36<1:02:43,  1.32it/s]Training epoch 1:  11%|#         | 607/5556 [07:37<1:03:19,  1.30it/s]Training epoch 1:  11%|#         | 608/5556 [07:37<1:02:51,  1.31it/s]Training epoch 1:  11%|#         | 609/5556 [07:38<1:03:19,  1.30it/s]Training epoch 1:  11%|#         | 610/5556 [07:39<1:02:23,  1.32it/s]Training epoch 1:  11%|#         | 611/5556 [07:40<1:02:41,  1.31it/s]Training epoch 1:  11%|#1        | 612/5556 [07:40<1:01:46,  1.33it/s]Training epoch 1:  11%|#1        | 613/5556 [07:41<1:01:52,  1.33it/s]Training epoch 1:  11%|#1        | 614/5556 [07:42<1:01:22,  1.34it/s]Training epoch 1:  11%|#1        | 615/5556 [07:43<1:01:19,  1.34it/s]Training epoch 1:  11%|#1        | 616/5556 [07:43<1:01:00,  1.35it/s]Training epoch 1:  11%|#1        | 617/5556 [07:44<1:01:38,  1.34it/s]Training epoch 1:  11%|#1        | 618/5556 [07:45<1:01:54,  1.33it/s]Training epoch 1:  11%|#1        | 619/5556 [07:46<1:02:17,  1.32it/s]Training epoch 1:  11%|#1        | 620/5556 [07:46<1:01:42,  1.33it/s]Training epoch 1:  11%|#1        | 621/5556 [07:47<1:00:22,  1.36it/s]Training epoch 1:  11%|#1        | 622/5556 [07:48<1:00:39,  1.36it/s]Training epoch 1:  11%|#1        | 623/5556 [07:49<1:00:39,  1.36it/s]Training epoch 1:  11%|#1        | 624/5556 [07:49<1:00:39,  1.36it/s]Training epoch 1:  11%|#1        | 625/5556 [07:50<1:00:39,  1.35it/s]Training epoch 1:  11%|#1        | 626/5556 [07:51<1:00:24,  1.36it/s]Training epoch 1:  11%|#1        | 627/5556 [07:52<1:00:39,  1.35it/s]Training epoch 1:  11%|#1        | 628/5556 [07:52<1:00:16,  1.36it/s]Training epoch 1:  11%|#1        | 629/5556 [07:53<1:00:03,  1.37it/s]Training epoch 1:  11%|#1        | 630/5556 [07:54<59:57,  1.37it/s]  Training epoch 1:  11%|#1        | 631/5556 [07:54<1:00:04,  1.37it/s]Training epoch 1:  11%|#1        | 632/5556 [07:55<1:00:19,  1.36it/s]Training epoch 1:  11%|#1        | 633/5556 [07:56<59:52,  1.37it/s]  Training epoch 1:  11%|#1        | 634/5556 [07:57<1:01:10,  1.34it/s]Training epoch 1:  11%|#1        | 635/5556 [07:57<1:01:52,  1.33it/s]Training epoch 1:  11%|#1        | 636/5556 [07:58<1:02:04,  1.32it/s]Training epoch 1:  11%|#1        | 637/5556 [07:59<1:02:12,  1.32it/s]Training epoch 1:  11%|#1        | 638/5556 [08:00<1:02:40,  1.31it/s]Training epoch 1:  12%|#1        | 639/5556 [08:00<1:01:40,  1.33it/s]Training epoch 1:  12%|#1        | 640/5556 [08:01<1:01:44,  1.33it/s]Training epoch 1:  12%|#1        | 641/5556 [08:02<1:01:39,  1.33it/s]Training epoch 1:  12%|#1        | 642/5556 [08:03<1:01:18,  1.34it/s]Training epoch 1:  12%|#1        | 643/5556 [08:03<1:01:44,  1.33it/s]Training epoch 1:  12%|#1        | 644/5556 [08:04<1:01:11,  1.34it/s]Training epoch 1:  12%|#1        | 645/5556 [08:05<1:02:22,  1.31it/s]Training epoch 1:  12%|#1        | 646/5556 [08:06<1:02:25,  1.31it/s]Training epoch 1:  12%|#1        | 647/5556 [08:07<1:02:26,  1.31it/s]Training epoch 1:  12%|#1        | 648/5556 [08:07<1:02:01,  1.32it/s]Training epoch 1:  12%|#1        | 649/5556 [08:08<1:01:30,  1.33it/s]Training epoch 1:  12%|#1        | 650/5556 [08:09<1:01:47,  1.32it/s]Training epoch 1:  12%|#1        | 651/5556 [08:10<1:01:43,  1.32it/s]Training epoch 1:  12%|#1        | 652/5556 [08:10<1:01:09,  1.34it/s]Training epoch 1:  12%|#1        | 653/5556 [08:11<1:00:14,  1.36it/s]Training epoch 1:  12%|#1        | 654/5556 [08:12<1:00:48,  1.34it/s]Training epoch 1:  12%|#1        | 655/5556 [08:12<1:00:31,  1.35it/s]Training epoch 1:  12%|#1        | 656/5556 [08:13<1:01:16,  1.33it/s]Training epoch 1:  12%|#1        | 657/5556 [08:14<1:00:57,  1.34it/s]Training epoch 1:  12%|#1        | 658/5556 [08:15<1:01:43,  1.32it/s]Training epoch 1:  12%|#1        | 659/5556 [08:16<1:01:35,  1.32it/s]Training epoch 1:  12%|#1        | 660/5556 [08:16<1:01:14,  1.33it/s]Training epoch 1:  12%|#1        | 661/5556 [08:17<1:00:45,  1.34it/s]Training epoch 1:  12%|#1        | 662/5556 [08:18<1:00:35,  1.35it/s]Training epoch 1:  12%|#1        | 663/5556 [08:19<1:01:30,  1.33it/s]Training epoch 1:  12%|#1        | 664/5556 [08:19<1:02:22,  1.31it/s]Training epoch 1:  12%|#1        | 665/5556 [08:20<1:02:13,  1.31it/s]Training epoch 1:  12%|#1        | 666/5556 [08:21<1:01:28,  1.33it/s]Training epoch 1:  12%|#2        | 667/5556 [08:22<1:01:59,  1.31it/s]Training epoch 1:  12%|#2        | 668/5556 [08:22<1:01:44,  1.32it/s]Training epoch 1:  12%|#2        | 669/5556 [08:23<1:01:38,  1.32it/s]Training epoch 1:  12%|#2        | 670/5556 [08:24<1:00:15,  1.35it/s]Training epoch 1:  12%|#2        | 671/5556 [08:25<1:00:49,  1.34it/s]Training epoch 1:  12%|#2        | 672/5556 [08:25<1:00:21,  1.35it/s]Training epoch 1:  12%|#2        | 673/5556 [08:26<1:00:36,  1.34it/s]Training epoch 1:  12%|#2        | 674/5556 [08:27<1:00:58,  1.33it/s]Training epoch 1:  12%|#2        | 675/5556 [08:28<1:00:09,  1.35it/s]Training epoch 1:  12%|#2        | 676/5556 [08:28<1:00:46,  1.34it/s]Training epoch 1:  12%|#2        | 677/5556 [08:29<1:00:43,  1.34it/s]Training epoch 1:  12%|#2        | 678/5556 [08:30<1:00:49,  1.34it/s]Training epoch 1:  12%|#2        | 679/5556 [08:31<1:01:14,  1.33it/s]Training epoch 1:  12%|#2        | 680/5556 [08:31<1:01:32,  1.32it/s]Training epoch 1:  12%|#2        | 681/5556 [08:32<1:01:25,  1.32it/s]Training epoch 1:  12%|#2        | 682/5556 [08:33<1:00:55,  1.33it/s]Training epoch 1:  12%|#2        | 683/5556 [08:34<1:00:56,  1.33it/s]Training epoch 1:  12%|#2        | 684/5556 [08:34<1:00:57,  1.33it/s]Training epoch 1:  12%|#2        | 685/5556 [08:35<1:01:19,  1.32it/s]Training epoch 1:  12%|#2        | 686/5556 [08:36<1:01:03,  1.33it/s]Training epoch 1:  12%|#2        | 687/5556 [08:37<1:01:10,  1.33it/s]Training epoch 1:  12%|#2        | 688/5556 [08:37<1:00:27,  1.34it/s]Training epoch 1:  12%|#2        | 689/5556 [08:38<1:00:16,  1.35it/s]Training epoch 1:  12%|#2        | 690/5556 [08:39<1:00:03,  1.35it/s]Training epoch 1:  12%|#2        | 691/5556 [08:40<1:00:59,  1.33it/s]Training epoch 1:  12%|#2        | 692/5556 [08:40<1:01:41,  1.31it/s]Training epoch 1:  12%|#2        | 693/5556 [08:41<1:01:39,  1.31it/s]Training epoch 1:  12%|#2        | 694/5556 [08:42<1:01:47,  1.31it/s]Training epoch 1:  13%|#2        | 695/5556 [08:43<1:01:23,  1.32it/s]Training epoch 1:  13%|#2        | 696/5556 [08:43<1:00:23,  1.34it/s]Training epoch 1:  13%|#2        | 697/5556 [08:44<1:00:51,  1.33it/s]Training epoch 1:  13%|#2        | 698/5556 [08:45<1:00:11,  1.35it/s]Training epoch 1:  13%|#2        | 699/5556 [08:46<1:00:11,  1.35it/s]Training epoch 1:  13%|#2        | 700/5556 [08:46<1:03:38,  1.27it/s]Training epoch 1:  13%|#2        | 701/5556 [08:47<1:02:21,  1.30it/s]Training epoch 1:  13%|#2        | 702/5556 [08:48<1:01:55,  1.31it/s]Training epoch 1:  13%|#2        | 703/5556 [08:49<1:01:27,  1.32it/s]Training epoch 1:  13%|#2        | 704/5556 [08:49<1:01:48,  1.31it/s]Training epoch 1:  13%|#2        | 705/5556 [08:50<1:00:32,  1.34it/s]Training epoch 1:  13%|#2        | 706/5556 [08:51<1:00:51,  1.33it/s]Training epoch 1:  13%|#2        | 707/5556 [08:52<1:00:37,  1.33it/s]Training epoch 1:  13%|#2        | 708/5556 [08:52<59:53,  1.35it/s]  Training epoch 1:  13%|#2        | 709/5556 [08:53<1:00:35,  1.33it/s]Training epoch 1:  13%|#2        | 710/5556 [08:54<1:00:45,  1.33it/s]Training epoch 1:  13%|#2        | 711/5556 [08:55<59:50,  1.35it/s]  Training epoch 1:  13%|#2        | 712/5556 [08:55<1:01:31,  1.31it/s]Training epoch 1:  13%|#2        | 713/5556 [08:56<1:01:34,  1.31it/s]Training epoch 1:  13%|#2        | 714/5556 [08:57<1:01:32,  1.31it/s]Training epoch 1:  13%|#2        | 715/5556 [08:58<1:01:33,  1.31it/s]Training epoch 1:  13%|#2        | 716/5556 [08:58<1:00:57,  1.32it/s]Training epoch 1:  13%|#2        | 717/5556 [08:59<1:00:50,  1.33it/s]Training epoch 1:  13%|#2        | 718/5556 [09:00<1:00:04,  1.34it/s]Training epoch 1:  13%|#2        | 719/5556 [09:01<1:00:31,  1.33it/s]Training epoch 1:  13%|#2        | 720/5556 [09:01<1:00:49,  1.33it/s]Training epoch 1:  13%|#2        | 721/5556 [09:02<1:00:10,  1.34it/s]Training epoch 1:  13%|#2        | 722/5556 [09:03<1:00:15,  1.34it/s]Training epoch 1:  13%|#3        | 723/5556 [09:04<59:35,  1.35it/s]  Training epoch 1:  13%|#3        | 724/5556 [09:04<1:00:00,  1.34it/s]Training epoch 1:  13%|#3        | 725/5556 [09:05<59:37,  1.35it/s]  Training epoch 1:  13%|#3        | 726/5556 [09:06<59:13,  1.36it/s]Training epoch 1:  13%|#3        | 727/5556 [09:07<59:01,  1.36it/s]Training epoch 1:  13%|#3        | 728/5556 [09:07<58:48,  1.37it/s]Training epoch 1:  13%|#3        | 729/5556 [09:08<58:56,  1.36it/s]Training epoch 1:  13%|#3        | 730/5556 [09:09<59:38,  1.35it/s]Training epoch 1:  13%|#3        | 731/5556 [09:10<1:00:02,  1.34it/s]Training epoch 1:  13%|#3        | 732/5556 [09:10<1:00:13,  1.34it/s]Training epoch 1:  13%|#3        | 733/5556 [09:11<59:47,  1.34it/s]  Training epoch 1:  13%|#3        | 734/5556 [09:12<59:41,  1.35it/s]Training epoch 1:  13%|#3        | 735/5556 [09:13<59:30,  1.35it/s]Training epoch 1:  13%|#3        | 736/5556 [09:13<59:15,  1.36it/s]Training epoch 1:  13%|#3        | 737/5556 [09:14<59:59,  1.34it/s]Training epoch 1:  13%|#3        | 738/5556 [09:15<1:00:27,  1.33it/s]Training epoch 1:  13%|#3        | 739/5556 [09:16<1:00:14,  1.33it/s]Training epoch 1:  13%|#3        | 740/5556 [09:16<1:00:32,  1.33it/s]Training epoch 1:  13%|#3        | 741/5556 [09:17<1:01:09,  1.31it/s]Training epoch 1:  13%|#3        | 742/5556 [09:18<1:00:40,  1.32it/s]Training epoch 1:  13%|#3        | 743/5556 [09:19<59:59,  1.34it/s]  Training epoch 1:  13%|#3        | 744/5556 [09:19<59:54,  1.34it/s]Training epoch 1:  13%|#3        | 745/5556 [09:20<59:45,  1.34it/s]Training epoch 1:  13%|#3        | 746/5556 [09:21<59:36,  1.35it/s]Training epoch 1:  13%|#3        | 747/5556 [09:22<1:00:32,  1.32it/s]Training epoch 1:  13%|#3        | 748/5556 [09:22<1:00:56,  1.31it/s]Training epoch 1:  13%|#3        | 749/5556 [09:23<1:00:59,  1.31it/s]Training epoch 1:  13%|#3        | 750/5556 [09:24<1:01:00,  1.31it/s]Training epoch 1:  14%|#3        | 751/5556 [09:25<1:01:08,  1.31it/s]Training epoch 1:  14%|#3        | 752/5556 [09:25<1:00:42,  1.32it/s]Training epoch 1:  14%|#3        | 753/5556 [09:26<59:46,  1.34it/s]  Training epoch 1:  14%|#3        | 754/5556 [09:27<59:31,  1.34it/s]Training epoch 1:  14%|#3        | 755/5556 [09:28<58:46,  1.36it/s]Training epoch 1:  14%|#3        | 756/5556 [09:28<58:34,  1.37it/s]Training epoch 1:  14%|#3        | 757/5556 [09:29<59:31,  1.34it/s]Training epoch 1:  14%|#3        | 758/5556 [09:30<58:55,  1.36it/s]Training epoch 1:  14%|#3        | 759/5556 [09:31<59:15,  1.35it/s]Training epoch 1:  14%|#3        | 760/5556 [09:31<59:28,  1.34it/s]Training epoch 1:  14%|#3        | 761/5556 [09:32<59:57,  1.33it/s]Training epoch 1:  14%|#3        | 762/5556 [09:33<59:56,  1.33it/s]Training epoch 1:  14%|#3        | 763/5556 [09:34<59:42,  1.34it/s]Training epoch 1:  14%|#3        | 764/5556 [09:34<59:14,  1.35it/s]Training epoch 1:  14%|#3        | 765/5556 [09:35<59:11,  1.35it/s]Training epoch 1:  14%|#3        | 766/5556 [09:36<58:34,  1.36it/s]Training epoch 1:  14%|#3        | 767/5556 [09:36<58:50,  1.36it/s]Training epoch 1:  14%|#3        | 768/5556 [09:37<59:56,  1.33it/s]Training epoch 1:  14%|#3        | 769/5556 [09:38<59:22,  1.34it/s]Training epoch 1:  14%|#3        | 770/5556 [09:39<1:00:09,  1.33it/s]Training epoch 1:  14%|#3        | 771/5556 [09:40<1:00:15,  1.32it/s]Training epoch 1:  14%|#3        | 772/5556 [09:40<1:00:12,  1.32it/s]Training epoch 1:  14%|#3        | 773/5556 [09:41<59:42,  1.34it/s]  Training epoch 1:  14%|#3        | 774/5556 [09:42<59:25,  1.34it/s]Training epoch 1:  14%|#3        | 775/5556 [09:42<59:04,  1.35it/s]Training epoch 1:  14%|#3        | 776/5556 [09:43<58:56,  1.35it/s]Training epoch 1:  14%|#3        | 777/5556 [09:44<59:04,  1.35it/s]Training epoch 1:  14%|#4        | 778/5556 [09:45<59:09,  1.35it/s]Training epoch 1:  14%|#4        | 779/5556 [09:45<58:42,  1.36it/s]Training epoch 1:  14%|#4        | 780/5556 [09:46<58:26,  1.36it/s]Training epoch 1:  14%|#4        | 781/5556 [09:47<58:24,  1.36it/s]Training epoch 1:  14%|#4        | 782/5556 [09:48<58:23,  1.36it/s]Training epoch 1:  14%|#4        | 783/5556 [09:48<58:33,  1.36it/s]Training epoch 1:  14%|#4        | 784/5556 [09:49<58:59,  1.35it/s]Training epoch 1:  14%|#4        | 785/5556 [09:50<59:10,  1.34it/s]Training epoch 1:  14%|#4        | 786/5556 [09:51<59:14,  1.34it/s]Training epoch 1:  14%|#4        | 787/5556 [09:51<59:36,  1.33it/s]Training epoch 1:  14%|#4        | 788/5556 [09:52<59:21,  1.34it/s]Training epoch 1:  14%|#4        | 789/5556 [09:53<59:33,  1.33it/s]Training epoch 1:  14%|#4        | 790/5556 [09:54<59:20,  1.34it/s]Training epoch 1:  14%|#4        | 791/5556 [09:54<59:41,  1.33it/s]Training epoch 1:  14%|#4        | 792/5556 [09:55<1:00:08,  1.32it/s]Training epoch 1:  14%|#4        | 793/5556 [09:56<59:07,  1.34it/s]  Training epoch 1:  14%|#4        | 794/5556 [09:57<1:00:00,  1.32it/s]Training epoch 1:  14%|#4        | 795/5556 [09:57<59:59,  1.32it/s]  Training epoch 1:  14%|#4        | 796/5556 [09:58<1:00:07,  1.32it/s]Training epoch 1:  14%|#4        | 797/5556 [09:59<59:45,  1.33it/s]  Training epoch 1:  14%|#4        | 798/5556 [10:00<1:00:19,  1.31it/s]Training epoch 1:  14%|#4        | 799/5556 [10:00<1:00:03,  1.32it/s]Training epoch 1:  14%|#4        | 800/5556 [10:01<1:02:11,  1.27it/s]Training epoch 1:  14%|#4        | 801/5556 [10:02<1:01:46,  1.28it/s]Training epoch 1:  14%|#4        | 802/5556 [10:03<1:02:17,  1.27it/s]Training epoch 1:  14%|#4        | 803/5556 [10:04<1:01:49,  1.28it/s]Training epoch 1:  14%|#4        | 804/5556 [10:04<1:01:02,  1.30it/s]Training epoch 1:  14%|#4        | 805/5556 [10:05<1:00:03,  1.32it/s]Training epoch 1:  15%|#4        | 806/5556 [10:06<59:59,  1.32it/s]  Training epoch 1:  15%|#4        | 807/5556 [10:07<59:35,  1.33it/s]Training epoch 1:  15%|#4        | 808/5556 [10:07<58:58,  1.34it/s]Training epoch 1:  15%|#4        | 809/5556 [10:08<58:58,  1.34it/s]Training epoch 1:  15%|#4        | 810/5556 [10:09<58:35,  1.35it/s]Training epoch 1:  15%|#4        | 811/5556 [10:10<58:52,  1.34it/s]Training epoch 1:  15%|#4        | 812/5556 [10:10<59:09,  1.34it/s]Training epoch 1:  15%|#4        | 813/5556 [10:11<59:04,  1.34it/s]Training epoch 1:  15%|#4        | 814/5556 [10:12<58:48,  1.34it/s]Training epoch 1:  15%|#4        | 815/5556 [10:13<59:28,  1.33it/s]Training epoch 1:  15%|#4        | 816/5556 [10:13<59:44,  1.32it/s]Training epoch 1:  15%|#4        | 817/5556 [10:14<58:40,  1.35it/s]Training epoch 1:  15%|#4        | 818/5556 [10:15<59:19,  1.33it/s]Training epoch 1:  15%|#4        | 819/5556 [10:16<58:30,  1.35it/s]Training epoch 1:  15%|#4        | 820/5556 [10:16<59:18,  1.33it/s]Training epoch 1:  15%|#4        | 821/5556 [10:17<59:20,  1.33it/s]Training epoch 1:  15%|#4        | 822/5556 [10:18<59:37,  1.32it/s]Training epoch 1:  15%|#4        | 823/5556 [10:19<59:18,  1.33it/s]Training epoch 1:  15%|#4        | 824/5556 [10:19<59:15,  1.33it/s]Training epoch 1:  15%|#4        | 825/5556 [10:20<59:10,  1.33it/s]Training epoch 1:  15%|#4        | 826/5556 [10:21<59:26,  1.33it/s]Training epoch 1:  15%|#4        | 827/5556 [10:22<59:36,  1.32it/s]Training epoch 1:  15%|#4        | 828/5556 [10:22<59:25,  1.33it/s]Training epoch 1:  15%|#4        | 829/5556 [10:23<58:58,  1.34it/s]Training epoch 1:  15%|#4        | 830/5556 [10:24<58:53,  1.34it/s]Training epoch 1:  15%|#4        | 831/5556 [10:25<58:58,  1.34it/s]Training epoch 1:  15%|#4        | 832/5556 [10:25<59:08,  1.33it/s]Training epoch 1:  15%|#4        | 833/5556 [10:26<58:40,  1.34it/s]Training epoch 1:  15%|#5        | 834/5556 [10:27<58:40,  1.34it/s]Training epoch 1:  15%|#5        | 835/5556 [10:28<58:09,  1.35it/s]Training epoch 1:  15%|#5        | 836/5556 [10:28<58:59,  1.33it/s]Training epoch 1:  15%|#5        | 837/5556 [10:29<58:50,  1.34it/s]Training epoch 1:  15%|#5        | 838/5556 [10:30<58:18,  1.35it/s]Training epoch 1:  15%|#5        | 839/5556 [10:31<58:29,  1.34it/s]Training epoch 1:  15%|#5        | 840/5556 [10:31<58:14,  1.35it/s]Training epoch 1:  15%|#5        | 841/5556 [10:32<58:33,  1.34it/s]Training epoch 1:  15%|#5        | 842/5556 [10:33<58:55,  1.33it/s]Training epoch 1:  15%|#5        | 843/5556 [10:34<59:12,  1.33it/s]Training epoch 1:  15%|#5        | 844/5556 [10:34<58:34,  1.34it/s]Training epoch 1:  15%|#5        | 845/5556 [10:35<58:42,  1.34it/s]Training epoch 1:  15%|#5        | 846/5556 [10:36<58:07,  1.35it/s]Training epoch 1:  15%|#5        | 847/5556 [10:37<58:45,  1.34it/s]Training epoch 1:  15%|#5        | 848/5556 [10:37<59:54,  1.31it/s]Training epoch 1:  15%|#5        | 849/5556 [10:38<59:19,  1.32it/s]Training epoch 1:  15%|#5        | 850/5556 [10:39<59:10,  1.33it/s]Training epoch 1:  15%|#5        | 851/5556 [10:40<59:56,  1.31it/s]Training epoch 1:  15%|#5        | 852/5556 [10:40<1:00:03,  1.31it/s]Training epoch 1:  15%|#5        | 853/5556 [10:41<59:57,  1.31it/s]  Training epoch 1:  15%|#5        | 854/5556 [10:42<59:27,  1.32it/s]Training epoch 1:  15%|#5        | 855/5556 [10:43<59:19,  1.32it/s]Training epoch 1:  15%|#5        | 856/5556 [10:43<58:48,  1.33it/s]Training epoch 1:  15%|#5        | 857/5556 [10:44<58:59,  1.33it/s]Training epoch 1:  15%|#5        | 858/5556 [10:45<57:51,  1.35it/s]Training epoch 1:  15%|#5        | 859/5556 [10:46<57:48,  1.35it/s]Training epoch 1:  15%|#5        | 860/5556 [10:46<57:20,  1.36it/s]Training epoch 1:  15%|#5        | 861/5556 [10:47<58:21,  1.34it/s]Training epoch 1:  16%|#5        | 862/5556 [10:48<58:06,  1.35it/s]Training epoch 1:  16%|#5        | 863/5556 [10:49<58:32,  1.34it/s]Training epoch 1:  16%|#5        | 864/5556 [10:49<57:33,  1.36it/s]Training epoch 1:  16%|#5        | 865/5556 [10:50<57:15,  1.37it/s]Training epoch 1:  16%|#5        | 866/5556 [10:51<57:31,  1.36it/s]Training epoch 1:  16%|#5        | 867/5556 [10:51<58:20,  1.34it/s]Training epoch 1:  16%|#5        | 868/5556 [10:52<58:18,  1.34it/s]Training epoch 1:  16%|#5        | 869/5556 [10:53<58:36,  1.33it/s]Training epoch 1:  16%|#5        | 870/5556 [10:54<58:17,  1.34it/s]Training epoch 1:  16%|#5        | 871/5556 [10:54<58:30,  1.33it/s]Training epoch 1:  16%|#5        | 872/5556 [10:55<58:26,  1.34it/s]Training epoch 1:  16%|#5        | 873/5556 [10:56<58:27,  1.34it/s]Training epoch 1:  16%|#5        | 874/5556 [10:57<58:03,  1.34it/s]Training epoch 1:  16%|#5        | 875/5556 [10:57<57:50,  1.35it/s]Training epoch 1:  16%|#5        | 876/5556 [10:58<57:38,  1.35it/s]Training epoch 1:  16%|#5        | 877/5556 [10:59<57:22,  1.36it/s]Training epoch 1:  16%|#5        | 878/5556 [11:00<57:11,  1.36it/s]Training epoch 1:  16%|#5        | 879/5556 [11:00<57:11,  1.36it/s]Training epoch 1:  16%|#5        | 880/5556 [11:01<57:40,  1.35it/s]Training epoch 1:  16%|#5        | 881/5556 [11:02<58:05,  1.34it/s]Training epoch 1:  16%|#5        | 882/5556 [11:03<57:54,  1.35it/s]Training epoch 1:  16%|#5        | 883/5556 [11:03<58:13,  1.34it/s]Training epoch 1:  16%|#5        | 884/5556 [11:04<57:18,  1.36it/s]Training epoch 1:  16%|#5        | 885/5556 [11:05<58:23,  1.33it/s]Training epoch 1:  16%|#5        | 886/5556 [11:06<58:08,  1.34it/s]Training epoch 1:  16%|#5        | 887/5556 [11:06<58:01,  1.34it/s]Training epoch 1:  16%|#5        | 888/5556 [11:07<58:12,  1.34it/s]Training epoch 1:  16%|#6        | 889/5556 [11:08<58:19,  1.33it/s]Training epoch 1:  16%|#6        | 890/5556 [11:09<58:32,  1.33it/s]Training epoch 1:  16%|#6        | 891/5556 [11:09<58:57,  1.32it/s]Training epoch 1:  16%|#6        | 892/5556 [11:10<59:09,  1.31it/s]Training epoch 1:  16%|#6        | 893/5556 [11:11<58:37,  1.33it/s]Training epoch 1:  16%|#6        | 894/5556 [11:12<58:20,  1.33it/s]Training epoch 1:  16%|#6        | 895/5556 [11:12<58:12,  1.33it/s]Training epoch 1:  16%|#6        | 896/5556 [11:13<58:19,  1.33it/s]Training epoch 1:  16%|#6        | 897/5556 [11:14<58:14,  1.33it/s]Training epoch 1:  16%|#6        | 898/5556 [11:15<57:59,  1.34it/s]Training epoch 1:  16%|#6        | 899/5556 [11:15<58:15,  1.33it/s]Training epoch 1:  16%|#6        | 900/5556 [11:16<1:01:07,  1.27it/s]Training epoch 1:  16%|#6        | 901/5556 [11:17<1:00:37,  1.28it/s]Training epoch 1:  16%|#6        | 902/5556 [11:18<59:52,  1.30it/s]  Training epoch 1:  16%|#6        | 903/5556 [11:19<1:00:02,  1.29it/s]Training epoch 1:  16%|#6        | 904/5556 [11:19<59:24,  1.31it/s]  Training epoch 1:  16%|#6        | 905/5556 [11:20<58:38,  1.32it/s]Training epoch 1:  16%|#6        | 906/5556 [11:21<58:34,  1.32it/s]Training epoch 1:  16%|#6        | 907/5556 [11:22<57:31,  1.35it/s]Training epoch 1:  16%|#6        | 908/5556 [11:22<58:04,  1.33it/s]Training epoch 1:  16%|#6        | 909/5556 [11:23<58:02,  1.33it/s]Training epoch 1:  16%|#6        | 910/5556 [11:24<57:45,  1.34it/s]Training epoch 1:  16%|#6        | 911/5556 [11:24<57:26,  1.35it/s]Training epoch 1:  16%|#6        | 912/5556 [11:25<56:59,  1.36it/s]Training epoch 1:  16%|#6        | 913/5556 [11:26<57:26,  1.35it/s]Training epoch 1:  16%|#6        | 914/5556 [11:27<57:51,  1.34it/s]Training epoch 1:  16%|#6        | 915/5556 [11:27<57:11,  1.35it/s]Training epoch 1:  16%|#6        | 916/5556 [11:28<58:04,  1.33it/s]Training epoch 1:  17%|#6        | 917/5556 [11:29<57:33,  1.34it/s]Training epoch 1:  17%|#6        | 918/5556 [11:30<57:33,  1.34it/s]Training epoch 1:  17%|#6        | 919/5556 [11:30<58:08,  1.33it/s]Training epoch 1:  17%|#6        | 920/5556 [11:31<57:44,  1.34it/s]Training epoch 1:  17%|#6        | 921/5556 [11:32<57:54,  1.33it/s]Training epoch 1:  17%|#6        | 922/5556 [11:33<58:34,  1.32it/s]Training epoch 1:  17%|#6        | 923/5556 [11:33<58:06,  1.33it/s]Training epoch 1:  17%|#6        | 924/5556 [11:34<57:42,  1.34it/s]Training epoch 1:  17%|#6        | 925/5556 [11:35<57:43,  1.34it/s]Training epoch 1:  17%|#6        | 926/5556 [11:36<57:47,  1.34it/s]Training epoch 1:  17%|#6        | 927/5556 [11:36<57:12,  1.35it/s]Training epoch 1:  17%|#6        | 928/5556 [11:37<57:28,  1.34it/s]Training epoch 1:  17%|#6        | 929/5556 [11:38<57:50,  1.33it/s]Training epoch 1:  17%|#6        | 930/5556 [11:39<57:41,  1.34it/s]Training epoch 1:  17%|#6        | 931/5556 [11:39<58:26,  1.32it/s]Training epoch 1:  17%|#6        | 932/5556 [11:40<58:49,  1.31it/s]Training epoch 1:  17%|#6        | 933/5556 [11:41<58:04,  1.33it/s]Training epoch 1:  17%|#6        | 934/5556 [11:42<57:44,  1.33it/s]Training epoch 1:  17%|#6        | 935/5556 [11:42<57:02,  1.35it/s]Training epoch 1:  17%|#6        | 936/5556 [11:43<56:36,  1.36it/s]Training epoch 1:  17%|#6        | 937/5556 [11:44<57:32,  1.34it/s]Training epoch 1:  17%|#6        | 938/5556 [11:45<57:13,  1.35it/s]Training epoch 1:  17%|#6        | 939/5556 [11:45<57:30,  1.34it/s]Training epoch 1:  17%|#6        | 940/5556 [11:46<58:02,  1.33it/s]Training epoch 1:  17%|#6        | 941/5556 [11:47<57:14,  1.34it/s]Training epoch 1:  17%|#6        | 942/5556 [11:48<57:50,  1.33it/s]Training epoch 1:  17%|#6        | 943/5556 [11:48<57:19,  1.34it/s]Training epoch 1:  17%|#6        | 944/5556 [11:49<57:19,  1.34it/s]Training epoch 1:  17%|#7        | 945/5556 [11:50<57:43,  1.33it/s]Training epoch 1:  17%|#7        | 946/5556 [11:51<57:23,  1.34it/s]Training epoch 1:  17%|#7        | 947/5556 [11:51<56:54,  1.35it/s]Training epoch 1:  17%|#7        | 948/5556 [11:52<57:31,  1.34it/s]Training epoch 1:  17%|#7        | 949/5556 [11:53<57:41,  1.33it/s]Training epoch 1:  17%|#7        | 950/5556 [11:54<58:09,  1.32it/s]Training epoch 1:  17%|#7        | 951/5556 [11:54<58:01,  1.32it/s]Training epoch 1:  17%|#7        | 952/5556 [11:55<57:52,  1.33it/s]Training epoch 1:  17%|#7        | 953/5556 [11:56<57:37,  1.33it/s]Training epoch 1:  17%|#7        | 954/5556 [11:57<57:22,  1.34it/s]Training epoch 1:  17%|#7        | 955/5556 [11:57<57:16,  1.34it/s]Training epoch 1:  17%|#7        | 956/5556 [11:58<57:19,  1.34it/s]Training epoch 1:  17%|#7        | 957/5556 [11:59<56:53,  1.35it/s]Training epoch 1:  17%|#7        | 958/5556 [12:00<57:35,  1.33it/s]Training epoch 1:  17%|#7        | 959/5556 [12:00<56:59,  1.34it/s]Training epoch 1:  17%|#7        | 960/5556 [12:01<57:10,  1.34it/s]Training epoch 1:  17%|#7        | 961/5556 [12:02<57:10,  1.34it/s]Training epoch 1:  17%|#7        | 962/5556 [12:03<56:50,  1.35it/s]Training epoch 1:  17%|#7        | 963/5556 [12:03<56:40,  1.35it/s]Training epoch 1:  17%|#7        | 964/5556 [12:04<56:54,  1.34it/s]Training epoch 1:  17%|#7        | 965/5556 [12:05<57:41,  1.33it/s]Training epoch 1:  17%|#7        | 966/5556 [12:06<57:20,  1.33it/s]Training epoch 1:  17%|#7        | 967/5556 [12:06<57:32,  1.33it/s]Training epoch 1:  17%|#7        | 968/5556 [12:07<57:12,  1.34it/s]Training epoch 1:  17%|#7        | 969/5556 [12:08<57:23,  1.33it/s]Training epoch 1:  17%|#7        | 970/5556 [12:09<58:24,  1.31it/s]Training epoch 1:  17%|#7        | 971/5556 [12:09<58:16,  1.31it/s]Training epoch 1:  17%|#7        | 972/5556 [12:10<57:53,  1.32it/s]Training epoch 1:  18%|#7        | 973/5556 [12:11<57:52,  1.32it/s]Training epoch 1:  18%|#7        | 974/5556 [12:12<57:35,  1.33it/s]Training epoch 1:  18%|#7        | 975/5556 [12:12<56:38,  1.35it/s]Training epoch 1:  18%|#7        | 976/5556 [12:13<56:50,  1.34it/s]Training epoch 1:  18%|#7        | 977/5556 [12:14<56:43,  1.35it/s]Training epoch 1:  18%|#7        | 978/5556 [12:15<56:55,  1.34it/s]Training epoch 1:  18%|#7        | 979/5556 [12:15<56:40,  1.35it/s]Training epoch 1:  18%|#7        | 980/5556 [12:16<56:51,  1.34it/s]Training epoch 1:  18%|#7        | 981/5556 [12:17<57:14,  1.33it/s]Training epoch 1:  18%|#7        | 982/5556 [12:18<57:35,  1.32it/s]Training epoch 1:  18%|#7        | 983/5556 [12:18<57:09,  1.33it/s]Training epoch 1:  18%|#7        | 984/5556 [12:19<57:05,  1.33it/s]Training epoch 1:  18%|#7        | 985/5556 [12:20<56:12,  1.36it/s]Training epoch 1:  18%|#7        | 986/5556 [12:21<56:04,  1.36it/s]Training epoch 1:  18%|#7        | 987/5556 [12:21<56:32,  1.35it/s]Training epoch 1:  18%|#7        | 988/5556 [12:22<56:25,  1.35it/s]Training epoch 1:  18%|#7        | 989/5556 [12:23<56:41,  1.34it/s]Training epoch 1:  18%|#7        | 990/5556 [12:24<57:18,  1.33it/s]Training epoch 1:  18%|#7        | 991/5556 [12:24<57:35,  1.32it/s]Training epoch 1:  18%|#7        | 992/5556 [12:25<58:12,  1.31it/s]Training epoch 1:  18%|#7        | 993/5556 [12:26<56:58,  1.33it/s]Training epoch 1:  18%|#7        | 994/5556 [12:27<56:52,  1.34it/s]Training epoch 1:  18%|#7        | 995/5556 [12:27<56:23,  1.35it/s]Training epoch 1:  18%|#7        | 996/5556 [12:28<56:47,  1.34it/s]Training epoch 1:  18%|#7        | 997/5556 [12:29<57:18,  1.33it/s]Training epoch 1:  18%|#7        | 998/5556 [12:30<57:25,  1.32it/s]Training epoch 1:  18%|#7        | 999/5556 [12:30<57:30,  1.32it/s]Training epoch 1:  18%|#7        | 1000/5556 [12:31<1:00:17,  1.26it/s]Training epoch 1:  18%|#8        | 1001/5556 [12:32<58:59,  1.29it/s]  Training epoch 1:  18%|#8        | 1002/5556 [12:33<58:11,  1.30it/s]Training epoch 1:  18%|#8        | 1003/5556 [12:34<57:35,  1.32it/s]Training epoch 1:  18%|#8        | 1004/5556 [12:34<57:51,  1.31it/s]Training epoch 1:  18%|#8        | 1005/5556 [12:35<57:18,  1.32it/s]Training epoch 1:  18%|#8        | 1006/5556 [12:36<56:34,  1.34it/s]Training epoch 1:  18%|#8        | 1007/5556 [12:36<56:05,  1.35it/s]Training epoch 1:  18%|#8        | 1008/5556 [12:37<56:00,  1.35it/s]Training epoch 1:  18%|#8        | 1009/5556 [12:38<55:56,  1.35it/s]Training epoch 1:  18%|#8        | 1010/5556 [12:39<55:52,  1.36it/s]Training epoch 1:  18%|#8        | 1011/5556 [12:39<56:47,  1.33it/s]Training epoch 1:  18%|#8        | 1012/5556 [12:40<56:31,  1.34it/s]Training epoch 1:  18%|#8        | 1013/5556 [12:41<56:35,  1.34it/s]Training epoch 1:  18%|#8        | 1014/5556 [12:42<56:25,  1.34it/s]Training epoch 1:  18%|#8        | 1015/5556 [12:42<55:57,  1.35it/s]Training epoch 1:  18%|#8        | 1016/5556 [12:43<56:31,  1.34it/s]Training epoch 1:  18%|#8        | 1017/5556 [12:44<56:12,  1.35it/s]Training epoch 1:  18%|#8        | 1018/5556 [12:45<56:28,  1.34it/s]Training epoch 1:  18%|#8        | 1019/5556 [12:45<56:46,  1.33it/s]Training epoch 1:  18%|#8        | 1020/5556 [12:46<57:13,  1.32it/s]Training epoch 1:  18%|#8        | 1021/5556 [12:47<57:01,  1.33it/s]Training epoch 1:  18%|#8        | 1022/5556 [12:48<56:46,  1.33it/s]Training epoch 1:  18%|#8        | 1023/5556 [12:48<55:55,  1.35it/s]Training epoch 1:  18%|#8        | 1024/5556 [12:49<55:43,  1.36it/s]Training epoch 1:  18%|#8        | 1025/5556 [12:50<56:35,  1.33it/s]Training epoch 1:  18%|#8        | 1026/5556 [12:51<56:35,  1.33it/s]Training epoch 1:  18%|#8        | 1027/5556 [12:51<56:44,  1.33it/s]Training epoch 1:  19%|#8        | 1028/5556 [12:52<57:15,  1.32it/s]Training epoch 1:  19%|#8        | 1029/5556 [12:53<56:20,  1.34it/s]Training epoch 1:  19%|#8        | 1030/5556 [12:54<56:22,  1.34it/s]Training epoch 1:  19%|#8        | 1031/5556 [12:54<56:56,  1.32it/s]Training epoch 1:  19%|#8        | 1032/5556 [12:55<57:24,  1.31it/s]Training epoch 1:  19%|#8        | 1033/5556 [12:56<57:37,  1.31it/s]Training epoch 1:  19%|#8        | 1034/5556 [12:57<57:04,  1.32it/s]Training epoch 1:  19%|#8        | 1035/5556 [12:57<56:31,  1.33it/s]Training epoch 1:  19%|#8        | 1036/5556 [12:58<55:55,  1.35it/s]Training epoch 1:  19%|#8        | 1037/5556 [12:59<56:50,  1.32it/s]Training epoch 1:  19%|#8        | 1038/5556 [13:00<56:20,  1.34it/s]Training epoch 1:  19%|#8        | 1039/5556 [13:00<56:28,  1.33it/s]Training epoch 1:  19%|#8        | 1040/5556 [13:01<57:27,  1.31it/s]Training epoch 1:  19%|#8        | 1041/5556 [13:02<57:24,  1.31it/s]Training epoch 1:  19%|#8        | 1042/5556 [13:03<57:05,  1.32it/s]Training epoch 1:  19%|#8        | 1043/5556 [13:04<56:44,  1.33it/s]Training epoch 1:  19%|#8        | 1044/5556 [13:04<56:18,  1.34it/s]Training epoch 1:  19%|#8        | 1045/5556 [13:05<56:13,  1.34it/s]Training epoch 1:  19%|#8        | 1046/5556 [13:06<56:05,  1.34it/s]Training epoch 1:  19%|#8        | 1047/5556 [13:06<55:48,  1.35it/s]Training epoch 1:  19%|#8        | 1048/5556 [13:07<55:56,  1.34it/s]Training epoch 1:  19%|#8        | 1049/5556 [13:08<55:24,  1.36it/s]Training epoch 1:  19%|#8        | 1050/5556 [13:09<55:37,  1.35it/s]Training epoch 1:  19%|#8        | 1051/5556 [13:09<55:57,  1.34it/s]Training epoch 1:  19%|#8        | 1052/5556 [13:10<56:10,  1.34it/s]Training epoch 1:  19%|#8        | 1053/5556 [13:11<55:52,  1.34it/s]Training epoch 1:  19%|#8        | 1054/5556 [13:12<55:38,  1.35it/s]Training epoch 1:  19%|#8        | 1055/5556 [13:12<55:46,  1.35it/s]Training epoch 1:  19%|#9        | 1056/5556 [13:13<55:47,  1.34it/s]Training epoch 1:  19%|#9        | 1057/5556 [13:14<55:50,  1.34it/s]Training epoch 1:  19%|#9        | 1058/5556 [13:15<55:54,  1.34it/s]Training epoch 1:  19%|#9        | 1059/5556 [13:15<55:41,  1.35it/s]Training epoch 1:  19%|#9        | 1060/5556 [13:16<55:53,  1.34it/s]Training epoch 1:  19%|#9        | 1061/5556 [13:17<55:30,  1.35it/s]Training epoch 1:  19%|#9        | 1062/5556 [13:18<55:00,  1.36it/s]Training epoch 1:  19%|#9        | 1063/5556 [13:18<54:49,  1.37it/s]Training epoch 1:  19%|#9        | 1064/5556 [13:19<55:15,  1.35it/s]Training epoch 1:  19%|#9        | 1065/5556 [13:20<55:44,  1.34it/s]Training epoch 1:  19%|#9        | 1066/5556 [13:21<56:21,  1.33it/s]Training epoch 1:  19%|#9        | 1067/5556 [13:21<56:52,  1.32it/s]Training epoch 1:  19%|#9        | 1068/5556 [13:22<56:31,  1.32it/s]Training epoch 1:  19%|#9        | 1069/5556 [13:23<56:11,  1.33it/s]Training epoch 1:  19%|#9        | 1070/5556 [13:24<56:33,  1.32it/s]Training epoch 1:  19%|#9        | 1071/5556 [13:24<55:24,  1.35it/s]Training epoch 1:  19%|#9        | 1072/5556 [13:25<56:14,  1.33it/s]Training epoch 1:  19%|#9        | 1073/5556 [13:26<57:06,  1.31it/s]Training epoch 1:  19%|#9        | 1074/5556 [13:27<56:08,  1.33it/s]Training epoch 1:  19%|#9        | 1075/5556 [13:27<56:06,  1.33it/s]Training epoch 1:  19%|#9        | 1076/5556 [13:28<55:44,  1.34it/s]Training epoch 1:  19%|#9        | 1077/5556 [13:29<54:55,  1.36it/s]Training epoch 1:  19%|#9        | 1078/5556 [13:30<55:01,  1.36it/s]Training epoch 1:  19%|#9        | 1079/5556 [13:30<55:23,  1.35it/s]Training epoch 1:  19%|#9        | 1080/5556 [13:31<55:14,  1.35it/s]Training epoch 1:  19%|#9        | 1081/5556 [13:32<55:31,  1.34it/s]Training epoch 1:  19%|#9        | 1082/5556 [13:33<55:27,  1.34it/s]Training epoch 1:  19%|#9        | 1083/5556 [13:33<55:20,  1.35it/s]Training epoch 1:  20%|#9        | 1084/5556 [13:34<55:27,  1.34it/s]Training epoch 1:  20%|#9        | 1085/5556 [13:35<55:15,  1.35it/s]Training epoch 1:  20%|#9        | 1086/5556 [13:36<55:37,  1.34it/s]Training epoch 1:  20%|#9        | 1087/5556 [13:36<56:43,  1.31it/s]Training epoch 1:  20%|#9        | 1088/5556 [13:37<56:07,  1.33it/s]Training epoch 1:  20%|#9        | 1089/5556 [13:38<55:44,  1.34it/s]Training epoch 1:  20%|#9        | 1090/5556 [13:39<55:43,  1.34it/s]Training epoch 1:  20%|#9        | 1091/5556 [13:39<55:52,  1.33it/s]Training epoch 1:  20%|#9        | 1092/5556 [13:40<55:15,  1.35it/s]Training epoch 1:  20%|#9        | 1093/5556 [13:41<55:24,  1.34it/s]Training epoch 1:  20%|#9        | 1094/5556 [13:42<55:57,  1.33it/s]Training epoch 1:  20%|#9        | 1095/5556 [13:42<56:31,  1.32it/s]Training epoch 1:  20%|#9        | 1096/5556 [13:43<56:46,  1.31it/s]Training epoch 1:  20%|#9        | 1097/5556 [13:44<57:38,  1.29it/s]Training epoch 1:  20%|#9        | 1098/5556 [13:45<56:47,  1.31it/s]Training epoch 1:  20%|#9        | 1099/5556 [13:45<56:39,  1.31it/s]Training epoch 1:  20%|#9        | 1100/5556 [13:46<58:57,  1.26it/s]Training epoch 1:  20%|#9        | 1101/5556 [13:47<57:52,  1.28it/s]Training epoch 1:  20%|#9        | 1102/5556 [13:48<57:14,  1.30it/s]Training epoch 1:  20%|#9        | 1103/5556 [13:48<55:40,  1.33it/s]Training epoch 1:  20%|#9        | 1104/5556 [13:49<55:13,  1.34it/s]Training epoch 1:  20%|#9        | 1105/5556 [13:50<55:41,  1.33it/s]Training epoch 1:  20%|#9        | 1106/5556 [13:51<55:40,  1.33it/s]Training epoch 1:  20%|#9        | 1107/5556 [13:51<55:49,  1.33it/s]Training epoch 1:  20%|#9        | 1108/5556 [13:52<55:35,  1.33it/s]Training epoch 1:  20%|#9        | 1109/5556 [13:53<55:23,  1.34it/s]Training epoch 1:  20%|#9        | 1110/5556 [13:54<54:47,  1.35it/s]Training epoch 1:  20%|#9        | 1111/5556 [13:54<54:30,  1.36it/s]Training epoch 1:  20%|##        | 1112/5556 [13:55<54:35,  1.36it/s]Training epoch 1:  20%|##        | 1113/5556 [13:56<55:47,  1.33it/s]Training epoch 1:  20%|##        | 1114/5556 [13:57<55:33,  1.33it/s]Training epoch 1:  20%|##        | 1115/5556 [13:57<55:21,  1.34it/s]Training epoch 1:  20%|##        | 1116/5556 [13:58<55:45,  1.33it/s]Training epoch 1:  20%|##        | 1117/5556 [13:59<55:24,  1.34it/s]Training epoch 1:  20%|##        | 1118/5556 [14:00<55:47,  1.33it/s]Training epoch 1:  20%|##        | 1119/5556 [14:00<55:34,  1.33it/s]Training epoch 1:  20%|##        | 1120/5556 [14:01<55:50,  1.32it/s]Training epoch 1:  20%|##        | 1121/5556 [14:02<55:37,  1.33it/s]Training epoch 1:  20%|##        | 1122/5556 [14:03<55:50,  1.32it/s]Training epoch 1:  20%|##        | 1123/5556 [14:03<55:24,  1.33it/s]Training epoch 1:  20%|##        | 1124/5556 [14:04<54:41,  1.35it/s]Training epoch 1:  20%|##        | 1125/5556 [14:05<54:57,  1.34it/s]Training epoch 1:  20%|##        | 1126/5556 [14:06<55:09,  1.34it/s]Training epoch 1:  20%|##        | 1127/5556 [14:06<55:16,  1.34it/s]Training epoch 1:  20%|##        | 1128/5556 [14:07<55:27,  1.33it/s]Training epoch 1:  20%|##        | 1129/5556 [14:08<55:27,  1.33it/s]Training epoch 1:  20%|##        | 1130/5556 [14:09<55:59,  1.32it/s]Training epoch 1:  20%|##        | 1131/5556 [14:09<56:01,  1.32it/s]Training epoch 1:  20%|##        | 1132/5556 [14:10<55:32,  1.33it/s]Training epoch 1:  20%|##        | 1133/5556 [14:11<55:02,  1.34it/s]Training epoch 1:  20%|##        | 1134/5556 [14:12<54:45,  1.35it/s]Training epoch 1:  20%|##        | 1135/5556 [14:12<54:51,  1.34it/s]Training epoch 1:  20%|##        | 1136/5556 [14:13<54:44,  1.35it/s]Training epoch 1:  20%|##        | 1137/5556 [14:14<54:51,  1.34it/s]Training epoch 1:  20%|##        | 1138/5556 [14:15<55:26,  1.33it/s]Training epoch 1:  21%|##        | 1139/5556 [14:15<54:42,  1.35it/s]Training epoch 1:  21%|##        | 1140/5556 [14:16<54:06,  1.36it/s]Training epoch 1:  21%|##        | 1141/5556 [14:17<53:59,  1.36it/s]Training epoch 1:  21%|##        | 1142/5556 [14:18<54:10,  1.36it/s]Training epoch 1:  21%|##        | 1143/5556 [14:18<54:16,  1.36it/s]Training epoch 1:  21%|##        | 1144/5556 [14:19<54:24,  1.35it/s]Training epoch 1:  21%|##        | 1145/5556 [14:20<55:02,  1.34it/s]Training epoch 1:  21%|##        | 1146/5556 [14:21<55:57,  1.31it/s]Training epoch 1:  21%|##        | 1147/5556 [14:21<55:22,  1.33it/s]Training epoch 1:  21%|##        | 1148/5556 [14:22<54:51,  1.34it/s]Training epoch 1:  21%|##        | 1149/5556 [14:23<55:09,  1.33it/s]Training epoch 1:  21%|##        | 1150/5556 [14:24<53:53,  1.36it/s]Training epoch 1:  21%|##        | 1151/5556 [14:24<54:14,  1.35it/s]Training epoch 1:  21%|##        | 1152/5556 [14:25<53:55,  1.36it/s]Training epoch 1:  21%|##        | 1153/5556 [14:26<55:16,  1.33it/s]Training epoch 1:  21%|##        | 1154/5556 [14:27<55:34,  1.32it/s]Training epoch 1:  21%|##        | 1155/5556 [14:27<55:39,  1.32it/s]Training epoch 1:  21%|##        | 1156/5556 [14:28<56:03,  1.31it/s]Training epoch 1:  21%|##        | 1157/5556 [14:29<55:50,  1.31it/s]Training epoch 1:  21%|##        | 1158/5556 [14:30<55:22,  1.32it/s]Training epoch 1:  21%|##        | 1159/5556 [14:30<55:20,  1.32it/s]Training epoch 1:  21%|##        | 1160/5556 [14:31<55:44,  1.31it/s]Training epoch 1:  21%|##        | 1161/5556 [14:32<55:30,  1.32it/s]Training epoch 1:  21%|##        | 1162/5556 [14:33<55:44,  1.31it/s]Training epoch 1:  21%|##        | 1163/5556 [14:33<54:28,  1.34it/s]Training epoch 1:  21%|##        | 1164/5556 [14:34<54:42,  1.34it/s]Training epoch 1:  21%|##        | 1165/5556 [14:35<54:56,  1.33it/s]Training epoch 1:  21%|##        | 1166/5556 [14:36<55:16,  1.32it/s]Training epoch 1:  21%|##1       | 1167/5556 [14:36<55:52,  1.31it/s]Training epoch 1:  21%|##1       | 1168/5556 [14:37<56:07,  1.30it/s]Training epoch 1:  21%|##1       | 1169/5556 [14:38<56:18,  1.30it/s]Training epoch 1:  21%|##1       | 1170/5556 [14:39<56:27,  1.29it/s]Training epoch 1:  21%|##1       | 1171/5556 [14:40<56:19,  1.30it/s]Training epoch 1:  21%|##1       | 1172/5556 [14:40<56:01,  1.30it/s]Training epoch 1:  21%|##1       | 1173/5556 [14:41<55:50,  1.31it/s]Training epoch 1:  21%|##1       | 1174/5556 [14:42<55:34,  1.31it/s]Training epoch 1:  21%|##1       | 1175/5556 [14:43<55:31,  1.31it/s]Training epoch 1:  21%|##1       | 1176/5556 [14:43<55:19,  1.32it/s]Training epoch 1:  21%|##1       | 1177/5556 [14:44<54:20,  1.34it/s]Training epoch 1:  21%|##1       | 1178/5556 [14:45<54:43,  1.33it/s]Training epoch 1:  21%|##1       | 1179/5556 [14:46<54:38,  1.34it/s]Training epoch 1:  21%|##1       | 1180/5556 [14:46<54:14,  1.34it/s]Training epoch 1:  21%|##1       | 1181/5556 [14:47<54:20,  1.34it/s]Training epoch 1:  21%|##1       | 1182/5556 [14:48<53:50,  1.35it/s]Training epoch 1:  21%|##1       | 1183/5556 [14:48<53:37,  1.36it/s]Training epoch 1:  21%|##1       | 1184/5556 [14:49<53:22,  1.37it/s]Training epoch 1:  21%|##1       | 1185/5556 [14:50<53:30,  1.36it/s]Training epoch 1:  21%|##1       | 1186/5556 [14:51<53:42,  1.36it/s]Training epoch 1:  21%|##1       | 1187/5556 [14:51<53:43,  1.36it/s]Training epoch 1:  21%|##1       | 1188/5556 [14:52<53:38,  1.36it/s]Training epoch 1:  21%|##1       | 1189/5556 [14:53<53:53,  1.35it/s]Training epoch 1:  21%|##1       | 1190/5556 [14:54<53:50,  1.35it/s]Training epoch 1:  21%|##1       | 1191/5556 [14:54<54:25,  1.34it/s]Training epoch 1:  21%|##1       | 1192/5556 [14:55<54:21,  1.34it/s]Training epoch 1:  21%|##1       | 1193/5556 [14:56<54:01,  1.35it/s]Training epoch 1:  21%|##1       | 1194/5556 [14:57<54:04,  1.34it/s]Training epoch 1:  22%|##1       | 1195/5556 [14:57<54:21,  1.34it/s]Training epoch 1:  22%|##1       | 1196/5556 [14:58<54:49,  1.33it/s]Training epoch 1:  22%|##1       | 1197/5556 [14:59<55:06,  1.32it/s]Training epoch 1:  22%|##1       | 1198/5556 [15:00<54:36,  1.33it/s]Training epoch 1:  22%|##1       | 1199/5556 [15:00<54:24,  1.33it/s]Training epoch 1:  22%|##1       | 1200/5556 [15:01<57:01,  1.27it/s]Training epoch 1:  22%|##1       | 1201/5556 [15:02<55:33,  1.31it/s]Training epoch 1:  22%|##1       | 1202/5556 [15:03<54:46,  1.32it/s]Training epoch 1:  22%|##1       | 1203/5556 [15:03<53:24,  1.36it/s]Training epoch 1:  22%|##1       | 1204/5556 [15:04<53:28,  1.36it/s]Training epoch 1:  22%|##1       | 1205/5556 [15:05<54:03,  1.34it/s]Training epoch 1:  22%|##1       | 1206/5556 [15:06<54:25,  1.33it/s]Training epoch 1:  22%|##1       | 1207/5556 [15:06<54:20,  1.33it/s]Training epoch 1:  22%|##1       | 1208/5556 [15:07<54:53,  1.32it/s]Training epoch 1:  22%|##1       | 1209/5556 [15:08<55:04,  1.32it/s]Training epoch 1:  22%|##1       | 1210/5556 [15:09<54:25,  1.33it/s]Training epoch 1:  22%|##1       | 1211/5556 [15:09<54:19,  1.33it/s]Training epoch 1:  22%|##1       | 1212/5556 [15:10<54:33,  1.33it/s]Training epoch 1:  22%|##1       | 1213/5556 [15:11<54:34,  1.33it/s]Training epoch 1:  22%|##1       | 1214/5556 [15:12<54:50,  1.32it/s]Training epoch 1:  22%|##1       | 1215/5556 [15:12<54:05,  1.34it/s]Training epoch 1:  22%|##1       | 1216/5556 [15:13<53:55,  1.34it/s]Training epoch 1:  22%|##1       | 1217/5556 [15:14<54:10,  1.33it/s]Training epoch 1:  22%|##1       | 1218/5556 [15:15<53:40,  1.35it/s]Training epoch 1:  22%|##1       | 1219/5556 [15:15<54:15,  1.33it/s]Training epoch 1:  22%|##1       | 1220/5556 [15:16<54:29,  1.33it/s]Training epoch 1:  22%|##1       | 1221/5556 [15:17<54:08,  1.33it/s]Training epoch 1:  22%|##1       | 1222/5556 [15:18<53:41,  1.35it/s]Training epoch 1:  22%|##2       | 1223/5556 [15:18<54:35,  1.32it/s]Training epoch 1:  22%|##2       | 1224/5556 [15:19<54:58,  1.31it/s]Training epoch 1:  22%|##2       | 1225/5556 [15:20<55:18,  1.31it/s]Training epoch 1:  22%|##2       | 1226/5556 [15:21<55:01,  1.31it/s]Training epoch 1:  22%|##2       | 1227/5556 [15:22<55:40,  1.30it/s]Training epoch 1:  22%|##2       | 1228/5556 [15:22<55:29,  1.30it/s]Training epoch 1:  22%|##2       | 1229/5556 [15:23<54:39,  1.32it/s]Training epoch 1:  22%|##2       | 1230/5556 [15:24<54:00,  1.33it/s]Training epoch 1:  22%|##2       | 1231/5556 [15:25<54:29,  1.32it/s]Training epoch 1:  22%|##2       | 1232/5556 [15:25<54:49,  1.31it/s]Training epoch 1:  22%|##2       | 1233/5556 [15:26<54:31,  1.32it/s]Training epoch 1:  22%|##2       | 1234/5556 [15:27<54:12,  1.33it/s]Training epoch 1:  22%|##2       | 1235/5556 [15:28<54:06,  1.33it/s]Training epoch 1:  22%|##2       | 1236/5556 [15:28<54:11,  1.33it/s]Training epoch 1:  22%|##2       | 1237/5556 [15:29<55:16,  1.30it/s]Training epoch 1:  22%|##2       | 1238/5556 [15:30<55:35,  1.29it/s]Training epoch 1:  22%|##2       | 1239/5556 [15:31<54:56,  1.31it/s]Training epoch 1:  22%|##2       | 1240/5556 [15:31<54:01,  1.33it/s]Training epoch 1:  22%|##2       | 1241/5556 [15:32<54:35,  1.32it/s]Training epoch 1:  22%|##2       | 1242/5556 [15:33<54:19,  1.32it/s]Training epoch 1:  22%|##2       | 1243/5556 [15:34<53:28,  1.34it/s]Training epoch 1:  22%|##2       | 1244/5556 [15:34<53:30,  1.34it/s]Training epoch 1:  22%|##2       | 1245/5556 [15:35<53:15,  1.35it/s]Training epoch 1:  22%|##2       | 1246/5556 [15:36<53:47,  1.34it/s]Training epoch 1:  22%|##2       | 1247/5556 [15:37<53:24,  1.34it/s]Training epoch 1:  22%|##2       | 1248/5556 [15:37<53:22,  1.35it/s]Training epoch 1:  22%|##2       | 1249/5556 [15:38<53:24,  1.34it/s]Training epoch 1:  22%|##2       | 1250/5556 [15:39<53:13,  1.35it/s]Training epoch 1:  23%|##2       | 1251/5556 [15:40<53:31,  1.34it/s]Training epoch 1:  23%|##2       | 1252/5556 [15:40<53:28,  1.34it/s]Training epoch 1:  23%|##2       | 1253/5556 [15:41<53:20,  1.34it/s]Training epoch 1:  23%|##2       | 1254/5556 [15:42<53:50,  1.33it/s]Training epoch 1:  23%|##2       | 1255/5556 [15:43<53:06,  1.35it/s]Training epoch 1:  23%|##2       | 1256/5556 [15:43<53:54,  1.33it/s]Training epoch 1:  23%|##2       | 1257/5556 [15:44<54:29,  1.31it/s]Training epoch 1:  23%|##2       | 1258/5556 [15:45<54:05,  1.32it/s]Training epoch 1:  23%|##2       | 1259/5556 [15:46<53:31,  1.34it/s]Training epoch 1:  23%|##2       | 1260/5556 [15:46<53:06,  1.35it/s]Training epoch 1:  23%|##2       | 1261/5556 [15:47<54:19,  1.32it/s]Training epoch 1:  23%|##2       | 1262/5556 [15:48<53:42,  1.33it/s]Training epoch 1:  23%|##2       | 1263/5556 [15:49<53:11,  1.35it/s]Training epoch 1:  23%|##2       | 1264/5556 [15:49<53:09,  1.35it/s]Training epoch 1:  23%|##2       | 1265/5556 [15:50<53:19,  1.34it/s]Training epoch 1:  23%|##2       | 1266/5556 [15:51<52:46,  1.35it/s]Training epoch 1:  23%|##2       | 1267/5556 [15:52<52:59,  1.35it/s]Training epoch 1:  23%|##2       | 1268/5556 [15:52<53:29,  1.34it/s]Training epoch 1:  23%|##2       | 1269/5556 [15:53<54:14,  1.32it/s]Training epoch 1:  23%|##2       | 1270/5556 [15:54<54:43,  1.31it/s]Training epoch 1:  23%|##2       | 1271/5556 [15:55<54:16,  1.32it/s]Training epoch 1:  23%|##2       | 1272/5556 [15:55<54:13,  1.32it/s]Training epoch 1:  23%|##2       | 1273/5556 [15:56<53:55,  1.32it/s]Training epoch 1:  23%|##2       | 1274/5556 [15:57<54:24,  1.31it/s]Training epoch 1:  23%|##2       | 1275/5556 [15:58<54:17,  1.31it/s]Training epoch 1:  23%|##2       | 1276/5556 [15:58<53:54,  1.32it/s]Training epoch 1:  23%|##2       | 1277/5556 [15:59<53:53,  1.32it/s]Training epoch 1:  23%|##3       | 1278/5556 [16:00<53:43,  1.33it/s]Training epoch 1:  23%|##3       | 1279/5556 [16:01<53:56,  1.32it/s]Training epoch 1:  23%|##3       | 1280/5556 [16:01<54:04,  1.32it/s]Training epoch 1:  23%|##3       | 1281/5556 [16:02<54:11,  1.31it/s]Training epoch 1:  23%|##3       | 1282/5556 [16:03<54:05,  1.32it/s]Training epoch 1:  23%|##3       | 1283/5556 [16:04<54:03,  1.32it/s]Training epoch 1:  23%|##3       | 1284/5556 [16:04<53:49,  1.32it/s]Training epoch 1:  23%|##3       | 1285/5556 [16:05<53:47,  1.32it/s]Training epoch 1:  23%|##3       | 1286/5556 [16:06<53:54,  1.32it/s]Training epoch 1:  23%|##3       | 1287/5556 [16:07<53:14,  1.34it/s]Training epoch 1:  23%|##3       | 1288/5556 [16:07<53:54,  1.32it/s]Training epoch 1:  23%|##3       | 1289/5556 [16:08<53:44,  1.32it/s]Training epoch 1:  23%|##3       | 1290/5556 [16:09<53:35,  1.33it/s]Training epoch 1:  23%|##3       | 1291/5556 [16:10<53:56,  1.32it/s]Training epoch 1:  23%|##3       | 1292/5556 [16:10<53:15,  1.33it/s]Training epoch 1:  23%|##3       | 1293/5556 [16:11<52:30,  1.35it/s]Training epoch 1:  23%|##3       | 1294/5556 [16:12<52:22,  1.36it/s]Training epoch 1:  23%|##3       | 1295/5556 [16:13<51:49,  1.37it/s]Training epoch 1:  23%|##3       | 1296/5556 [16:13<52:26,  1.35it/s]Training epoch 1:  23%|##3       | 1297/5556 [16:14<52:47,  1.34it/s]Training epoch 1:  23%|##3       | 1298/5556 [16:15<52:38,  1.35it/s]Training epoch 1:  23%|##3       | 1299/5556 [16:16<52:23,  1.35it/s]Training epoch 1:  23%|##3       | 1300/5556 [16:16<54:31,  1.30it/s]Training epoch 1:  23%|##3       | 1301/5556 [16:17<54:00,  1.31it/s]Training epoch 1:  23%|##3       | 1302/5556 [16:18<53:54,  1.32it/s]Training epoch 1:  23%|##3       | 1303/5556 [16:19<53:10,  1.33it/s]Training epoch 1:  23%|##3       | 1304/5556 [16:19<52:47,  1.34it/s]Training epoch 1:  23%|##3       | 1305/5556 [16:20<52:39,  1.35it/s]Training epoch 1:  24%|##3       | 1306/5556 [16:21<52:35,  1.35it/s]Training epoch 1:  24%|##3       | 1307/5556 [16:22<52:10,  1.36it/s]Training epoch 1:  24%|##3       | 1308/5556 [16:22<53:30,  1.32it/s]Training epoch 1:  24%|##3       | 1309/5556 [16:23<54:11,  1.31it/s]Training epoch 1:  24%|##3       | 1310/5556 [16:24<53:02,  1.33it/s]Training epoch 1:  24%|##3       | 1311/5556 [16:25<52:41,  1.34it/s]Training epoch 1:  24%|##3       | 1312/5556 [16:25<52:14,  1.35it/s]Training epoch 1:  24%|##3       | 1313/5556 [16:26<51:43,  1.37it/s]Training epoch 1:  24%|##3       | 1314/5556 [16:27<51:51,  1.36it/s]Training epoch 1:  24%|##3       | 1315/5556 [16:28<53:18,  1.33it/s]Training epoch 1:  24%|##3       | 1316/5556 [16:28<53:15,  1.33it/s]Training epoch 1:  24%|##3       | 1317/5556 [16:29<53:13,  1.33it/s]Training epoch 1:  24%|##3       | 1318/5556 [16:30<53:20,  1.32it/s]Training epoch 1:  24%|##3       | 1319/5556 [16:31<53:04,  1.33it/s]Training epoch 1:  24%|##3       | 1320/5556 [16:31<52:56,  1.33it/s]Training epoch 1:  24%|##3       | 1321/5556 [16:32<52:39,  1.34it/s]Training epoch 1:  24%|##3       | 1322/5556 [16:33<52:41,  1.34it/s]Training epoch 1:  24%|##3       | 1323/5556 [16:34<52:27,  1.34it/s]Training epoch 1:  24%|##3       | 1324/5556 [16:34<52:52,  1.33it/s]Training epoch 1:  24%|##3       | 1325/5556 [16:35<52:48,  1.34it/s]Training epoch 1:  24%|##3       | 1326/5556 [16:36<52:50,  1.33it/s]Training epoch 1:  24%|##3       | 1327/5556 [16:37<53:05,  1.33it/s]Training epoch 1:  24%|##3       | 1328/5556 [16:37<52:48,  1.33it/s]Training epoch 1:  24%|##3       | 1329/5556 [16:38<52:29,  1.34it/s]Training epoch 1:  24%|##3       | 1330/5556 [16:39<52:56,  1.33it/s]Training epoch 1:  24%|##3       | 1331/5556 [16:40<52:35,  1.34it/s]Training epoch 1:  24%|##3       | 1332/5556 [16:40<52:26,  1.34it/s]Training epoch 1:  24%|##3       | 1333/5556 [16:41<52:48,  1.33it/s]Training epoch 1:  24%|##4       | 1334/5556 [16:42<53:11,  1.32it/s]Training epoch 1:  24%|##4       | 1335/5556 [16:43<53:06,  1.32it/s]Training epoch 1:  24%|##4       | 1336/5556 [16:43<53:05,  1.32it/s]Training epoch 1:  24%|##4       | 1337/5556 [16:44<52:24,  1.34it/s]Training epoch 1:  24%|##4       | 1338/5556 [16:45<52:01,  1.35it/s]Training epoch 1:  24%|##4       | 1339/5556 [16:46<52:40,  1.33it/s]Training epoch 1:  24%|##4       | 1340/5556 [16:46<52:47,  1.33it/s]Training epoch 1:  24%|##4       | 1341/5556 [16:47<52:49,  1.33it/s]Training epoch 1:  24%|##4       | 1342/5556 [16:48<52:37,  1.33it/s]Training epoch 1:  24%|##4       | 1343/5556 [16:49<52:44,  1.33it/s]Training epoch 1:  24%|##4       | 1344/5556 [16:49<52:03,  1.35it/s]Training epoch 1:  24%|##4       | 1345/5556 [16:50<52:19,  1.34it/s]Training epoch 1:  24%|##4       | 1346/5556 [16:51<52:29,  1.34it/s]Training epoch 1:  24%|##4       | 1347/5556 [16:52<53:01,  1.32it/s]Training epoch 1:  24%|##4       | 1348/5556 [16:52<53:21,  1.31it/s]Training epoch 1:  24%|##4       | 1349/5556 [16:53<53:04,  1.32it/s]Training epoch 1:  24%|##4       | 1350/5556 [16:54<53:17,  1.32it/s]Training epoch 1:  24%|##4       | 1351/5556 [16:55<53:21,  1.31it/s]Training epoch 1:  24%|##4       | 1352/5556 [16:55<53:13,  1.32it/s]Training epoch 1:  24%|##4       | 1353/5556 [16:56<52:59,  1.32it/s]Training epoch 1:  24%|##4       | 1354/5556 [16:57<53:08,  1.32it/s]Training epoch 1:  24%|##4       | 1355/5556 [16:58<52:59,  1.32it/s]Training epoch 1:  24%|##4       | 1356/5556 [16:58<52:40,  1.33it/s]Training epoch 1:  24%|##4       | 1357/5556 [16:59<52:58,  1.32it/s]Training epoch 1:  24%|##4       | 1358/5556 [17:00<52:44,  1.33it/s]Training epoch 1:  24%|##4       | 1359/5556 [17:01<53:00,  1.32it/s]Training epoch 1:  24%|##4       | 1360/5556 [17:01<52:51,  1.32it/s]Training epoch 1:  24%|##4       | 1361/5556 [17:02<53:30,  1.31it/s]Training epoch 1:  25%|##4       | 1362/5556 [17:03<53:04,  1.32it/s]Training epoch 1:  25%|##4       | 1363/5556 [17:04<53:06,  1.32it/s]Training epoch 1:  25%|##4       | 1364/5556 [17:05<53:16,  1.31it/s]Training epoch 1:  25%|##4       | 1365/5556 [17:05<52:51,  1.32it/s]Training epoch 1:  25%|##4       | 1366/5556 [17:06<52:47,  1.32it/s]Training epoch 1:  25%|##4       | 1367/5556 [17:07<52:51,  1.32it/s]Training epoch 1:  25%|##4       | 1368/5556 [17:08<52:15,  1.34it/s]Training epoch 1:  25%|##4       | 1369/5556 [17:08<52:21,  1.33it/s]Training epoch 1:  25%|##4       | 1370/5556 [17:09<52:31,  1.33it/s]Training epoch 1:  25%|##4       | 1371/5556 [17:10<52:37,  1.33it/s]Training epoch 1:  25%|##4       | 1372/5556 [17:11<52:31,  1.33it/s]Training epoch 1:  25%|##4       | 1373/5556 [17:11<53:00,  1.31it/s]Training epoch 1:  25%|##4       | 1374/5556 [17:12<52:43,  1.32it/s]Training epoch 1:  25%|##4       | 1375/5556 [17:13<53:29,  1.30it/s]Training epoch 1:  25%|##4       | 1376/5556 [17:14<52:35,  1.32it/s]Training epoch 1:  25%|##4       | 1377/5556 [17:14<52:14,  1.33it/s]Training epoch 1:  25%|##4       | 1378/5556 [17:15<52:11,  1.33it/s]Training epoch 1:  25%|##4       | 1379/5556 [17:16<51:44,  1.35it/s]Training epoch 1:  25%|##4       | 1380/5556 [17:17<52:01,  1.34it/s]Training epoch 1:  25%|##4       | 1381/5556 [17:17<52:21,  1.33it/s]Training epoch 1:  25%|##4       | 1382/5556 [17:18<52:42,  1.32it/s]Training epoch 1:  25%|##4       | 1383/5556 [17:19<52:42,  1.32it/s]Training epoch 1:  25%|##4       | 1384/5556 [17:20<53:01,  1.31it/s]Training epoch 1:  25%|##4       | 1385/5556 [17:20<53:15,  1.31it/s]Training epoch 1:  25%|##4       | 1386/5556 [17:21<52:54,  1.31it/s]Training epoch 1:  25%|##4       | 1387/5556 [17:22<52:09,  1.33it/s]Training epoch 1:  25%|##4       | 1388/5556 [17:23<51:26,  1.35it/s]Training epoch 1:  25%|##5       | 1389/5556 [17:23<51:32,  1.35it/s]Training epoch 1:  25%|##5       | 1390/5556 [17:24<52:03,  1.33it/s]Training epoch 1:  25%|##5       | 1391/5556 [17:25<51:32,  1.35it/s]Training epoch 1:  25%|##5       | 1392/5556 [17:26<51:43,  1.34it/s]Training epoch 1:  25%|##5       | 1393/5556 [17:26<51:52,  1.34it/s]Training epoch 1:  25%|##5       | 1394/5556 [17:27<52:11,  1.33it/s]Training epoch 1:  25%|##5       | 1395/5556 [17:28<51:45,  1.34it/s]Training epoch 1:  25%|##5       | 1396/5556 [17:29<51:25,  1.35it/s]Training epoch 1:  25%|##5       | 1397/5556 [17:29<51:08,  1.36it/s]Training epoch 1:  25%|##5       | 1398/5556 [17:30<50:47,  1.36it/s]Training epoch 1:  25%|##5       | 1399/5556 [17:31<51:11,  1.35it/s]Training epoch 1:  25%|##5       | 1400/5556 [17:32<53:37,  1.29it/s]Training epoch 1:  25%|##5       | 1401/5556 [17:32<52:52,  1.31it/s]Training epoch 1:  25%|##5       | 1402/5556 [17:33<52:07,  1.33it/s]Training epoch 1:  25%|##5       | 1403/5556 [17:34<52:26,  1.32it/s]Training epoch 1:  25%|##5       | 1404/5556 [17:35<52:46,  1.31it/s]Training epoch 1:  25%|##5       | 1405/5556 [17:35<52:48,  1.31it/s]Training epoch 1:  25%|##5       | 1406/5556 [17:36<52:19,  1.32it/s]Training epoch 1:  25%|##5       | 1407/5556 [17:37<51:48,  1.33it/s]Training epoch 1:  25%|##5       | 1408/5556 [17:38<51:54,  1.33it/s]Training epoch 1:  25%|##5       | 1409/5556 [17:38<52:31,  1.32it/s]Training epoch 1:  25%|##5       | 1410/5556 [17:39<52:08,  1.33it/s]Training epoch 1:  25%|##5       | 1411/5556 [17:40<52:25,  1.32it/s]Training epoch 1:  25%|##5       | 1412/5556 [17:41<52:34,  1.31it/s]Training epoch 1:  25%|##5       | 1413/5556 [17:41<52:19,  1.32it/s]Training epoch 1:  25%|##5       | 1414/5556 [17:42<52:32,  1.31it/s]Training epoch 1:  25%|##5       | 1415/5556 [17:43<52:10,  1.32it/s]Training epoch 1:  25%|##5       | 1416/5556 [17:44<52:07,  1.32it/s]Training epoch 1:  26%|##5       | 1417/5556 [17:44<51:20,  1.34it/s]Training epoch 1:  26%|##5       | 1418/5556 [17:45<51:42,  1.33it/s]Training epoch 1:  26%|##5       | 1419/5556 [17:46<52:03,  1.32it/s]Training epoch 1:  26%|##5       | 1420/5556 [17:47<52:32,  1.31it/s]Training epoch 1:  26%|##5       | 1421/5556 [17:47<51:47,  1.33it/s]Training epoch 1:  26%|##5       | 1422/5556 [17:48<51:32,  1.34it/s]Training epoch 1:  26%|##5       | 1423/5556 [17:49<51:44,  1.33it/s]Training epoch 1:  26%|##5       | 1424/5556 [17:50<51:17,  1.34it/s]Training epoch 1:  26%|##5       | 1425/5556 [17:50<51:00,  1.35it/s]Training epoch 1:  26%|##5       | 1426/5556 [17:51<51:17,  1.34it/s]Training epoch 1:  26%|##5       | 1427/5556 [17:52<51:24,  1.34it/s]Training epoch 1:  26%|##5       | 1428/5556 [17:53<51:19,  1.34it/s]Training epoch 1:  26%|##5       | 1429/5556 [17:53<51:01,  1.35it/s]Training epoch 1:  26%|##5       | 1430/5556 [17:54<50:41,  1.36it/s]Training epoch 1:  26%|##5       | 1431/5556 [17:55<50:19,  1.37it/s]Training epoch 1:  26%|##5       | 1432/5556 [17:56<50:21,  1.36it/s]Training epoch 1:  26%|##5       | 1433/5556 [17:56<50:55,  1.35it/s]Training epoch 1:  26%|##5       | 1434/5556 [17:57<50:58,  1.35it/s]Training epoch 1:  26%|##5       | 1435/5556 [17:58<50:49,  1.35it/s]Training epoch 1:  26%|##5       | 1436/5556 [17:59<51:19,  1.34it/s]Training epoch 1:  26%|##5       | 1437/5556 [17:59<52:04,  1.32it/s]Training epoch 1:  26%|##5       | 1438/5556 [18:00<52:40,  1.30it/s]Training epoch 1:  26%|##5       | 1439/5556 [18:01<52:14,  1.31it/s]Training epoch 1:  26%|##5       | 1440/5556 [18:02<52:24,  1.31it/s]Training epoch 1:  26%|##5       | 1441/5556 [18:02<52:36,  1.30it/s]Training epoch 1:  26%|##5       | 1442/5556 [18:03<52:52,  1.30it/s]Training epoch 1:  26%|##5       | 1443/5556 [18:04<52:28,  1.31it/s]Training epoch 1:  26%|##5       | 1444/5556 [18:05<52:29,  1.31it/s]Training epoch 1:  26%|##6       | 1445/5556 [18:06<52:34,  1.30it/s]Training epoch 1:  26%|##6       | 1446/5556 [18:06<51:59,  1.32it/s]Training epoch 1:  26%|##6       | 1447/5556 [18:07<51:44,  1.32it/s]Training epoch 1:  26%|##6       | 1448/5556 [18:08<51:32,  1.33it/s]Training epoch 1:  26%|##6       | 1449/5556 [18:09<51:20,  1.33it/s]Training epoch 1:  26%|##6       | 1450/5556 [18:09<51:39,  1.32it/s]Training epoch 1:  26%|##6       | 1451/5556 [18:10<51:23,  1.33it/s]Training epoch 1:  26%|##6       | 1452/5556 [18:11<51:36,  1.33it/s]Training epoch 1:  26%|##6       | 1453/5556 [18:12<51:27,  1.33it/s]Training epoch 1:  26%|##6       | 1454/5556 [18:12<51:00,  1.34it/s]Training epoch 1:  26%|##6       | 1455/5556 [18:13<50:50,  1.34it/s]Training epoch 1:  26%|##6       | 1456/5556 [18:14<51:45,  1.32it/s]Training epoch 1:  26%|##6       | 1457/5556 [18:15<52:09,  1.31it/s]Training epoch 1:  26%|##6       | 1458/5556 [18:15<52:38,  1.30it/s]Training epoch 1:  26%|##6       | 1459/5556 [18:16<52:01,  1.31it/s]Training epoch 1:  26%|##6       | 1460/5556 [18:17<51:50,  1.32it/s]Training epoch 1:  26%|##6       | 1461/5556 [18:18<51:09,  1.33it/s]Training epoch 1:  26%|##6       | 1462/5556 [18:18<51:46,  1.32it/s]Training epoch 1:  26%|##6       | 1463/5556 [18:19<51:39,  1.32it/s]Training epoch 1:  26%|##6       | 1464/5556 [18:20<51:57,  1.31it/s]Training epoch 1:  26%|##6       | 1465/5556 [18:21<51:55,  1.31it/s]Training epoch 1:  26%|##6       | 1466/5556 [18:21<51:40,  1.32it/s]Training epoch 1:  26%|##6       | 1467/5556 [18:22<50:51,  1.34it/s]Training epoch 1:  26%|##6       | 1468/5556 [18:23<50:59,  1.34it/s]Training epoch 1:  26%|##6       | 1469/5556 [18:24<51:09,  1.33it/s]Training epoch 1:  26%|##6       | 1470/5556 [18:24<50:50,  1.34it/s]Training epoch 1:  26%|##6       | 1471/5556 [18:25<51:20,  1.33it/s]Training epoch 1:  26%|##6       | 1472/5556 [18:26<51:51,  1.31it/s]Training epoch 1:  27%|##6       | 1473/5556 [18:27<51:25,  1.32it/s]Training epoch 1:  27%|##6       | 1474/5556 [18:27<51:41,  1.32it/s]Training epoch 1:  27%|##6       | 1475/5556 [18:28<51:05,  1.33it/s]Training epoch 1:  27%|##6       | 1476/5556 [18:29<50:56,  1.33it/s]Training epoch 1:  27%|##6       | 1477/5556 [18:30<50:38,  1.34it/s]Training epoch 1:  27%|##6       | 1478/5556 [18:30<51:46,  1.31it/s]Training epoch 1:  27%|##6       | 1479/5556 [18:31<51:04,  1.33it/s]Training epoch 1:  27%|##6       | 1480/5556 [18:32<50:29,  1.35it/s]Training epoch 1:  27%|##6       | 1481/5556 [18:33<50:30,  1.34it/s]Training epoch 1:  27%|##6       | 1482/5556 [18:33<50:57,  1.33it/s]Training epoch 1:  27%|##6       | 1483/5556 [18:34<50:52,  1.33it/s]Training epoch 1:  27%|##6       | 1484/5556 [18:35<51:20,  1.32it/s]Training epoch 1:  27%|##6       | 1485/5556 [18:36<51:52,  1.31it/s]Training epoch 1:  27%|##6       | 1486/5556 [18:36<51:36,  1.31it/s]Training epoch 1:  27%|##6       | 1487/5556 [18:37<51:41,  1.31it/s]Training epoch 1:  27%|##6       | 1488/5556 [18:38<50:53,  1.33it/s]Training epoch 1:  27%|##6       | 1489/5556 [18:39<50:50,  1.33it/s]Training epoch 1:  27%|##6       | 1490/5556 [18:39<50:59,  1.33it/s]Training epoch 1:  27%|##6       | 1491/5556 [18:40<50:52,  1.33it/s]Training epoch 1:  27%|##6       | 1492/5556 [18:41<51:27,  1.32it/s]Training epoch 1:  27%|##6       | 1493/5556 [18:42<51:55,  1.30it/s]Training epoch 1:  27%|##6       | 1494/5556 [18:43<51:50,  1.31it/s]Training epoch 1:  27%|##6       | 1495/5556 [18:43<52:18,  1.29it/s]Training epoch 1:  27%|##6       | 1496/5556 [18:44<51:02,  1.33it/s]Training epoch 1:  27%|##6       | 1497/5556 [18:45<50:47,  1.33it/s]Training epoch 1:  27%|##6       | 1498/5556 [18:45<50:04,  1.35it/s]Training epoch 1:  27%|##6       | 1499/5556 [18:46<50:17,  1.34it/s]Training epoch 1:  27%|##6       | 1500/5556 [18:47<52:50,  1.28it/s]Training epoch 1:  27%|##7       | 1501/5556 [18:48<52:30,  1.29it/s]Training epoch 1:  27%|##7       | 1502/5556 [18:49<51:28,  1.31it/s]Training epoch 1:  27%|##7       | 1503/5556 [18:49<51:31,  1.31it/s]Training epoch 1:  27%|##7       | 1504/5556 [18:50<51:37,  1.31it/s]Training epoch 1:  27%|##7       | 1505/5556 [18:51<51:09,  1.32it/s]Training epoch 1:  27%|##7       | 1506/5556 [18:52<51:13,  1.32it/s]Training epoch 1:  27%|##7       | 1507/5556 [18:52<51:11,  1.32it/s]Training epoch 1:  27%|##7       | 1508/5556 [18:53<51:05,  1.32it/s]Training epoch 1:  27%|##7       | 1509/5556 [18:54<50:50,  1.33it/s]Training epoch 1:  27%|##7       | 1510/5556 [18:55<50:02,  1.35it/s]Training epoch 1:  27%|##7       | 1511/5556 [18:55<50:42,  1.33it/s]Training epoch 1:  27%|##7       | 1512/5556 [18:56<50:51,  1.33it/s]Training epoch 1:  27%|##7       | 1513/5556 [18:57<50:52,  1.32it/s]Training epoch 1:  27%|##7       | 1514/5556 [18:58<51:20,  1.31it/s]Training epoch 1:  27%|##7       | 1515/5556 [18:58<50:40,  1.33it/s]Training epoch 1:  27%|##7       | 1516/5556 [18:59<51:04,  1.32it/s]Training epoch 1:  27%|##7       | 1517/5556 [19:00<51:14,  1.31it/s]Training epoch 1:  27%|##7       | 1518/5556 [19:01<50:21,  1.34it/s]Training epoch 1:  27%|##7       | 1519/5556 [19:01<50:45,  1.33it/s]Training epoch 1:  27%|##7       | 1520/5556 [19:02<50:34,  1.33it/s]Training epoch 1:  27%|##7       | 1521/5556 [19:03<50:41,  1.33it/s]Training epoch 1:  27%|##7       | 1522/5556 [19:04<50:27,  1.33it/s]Training epoch 1:  27%|##7       | 1523/5556 [19:04<50:26,  1.33it/s]Training epoch 1:  27%|##7       | 1524/5556 [19:05<50:57,  1.32it/s]Training epoch 1:  27%|##7       | 1525/5556 [19:06<50:37,  1.33it/s]Training epoch 1:  27%|##7       | 1526/5556 [19:07<50:10,  1.34it/s]Training epoch 1:  27%|##7       | 1527/5556 [19:07<50:29,  1.33it/s]Training epoch 1:  28%|##7       | 1528/5556 [19:08<49:52,  1.35it/s]Training epoch 1:  28%|##7       | 1529/5556 [19:09<50:00,  1.34it/s]Training epoch 1:  28%|##7       | 1530/5556 [19:10<50:54,  1.32it/s]Training epoch 1:  28%|##7       | 1531/5556 [19:11<51:32,  1.30it/s]Training epoch 1:  28%|##7       | 1532/5556 [19:11<50:52,  1.32it/s]Training epoch 1:  28%|##7       | 1533/5556 [19:12<51:19,  1.31it/s]Training epoch 1:  28%|##7       | 1534/5556 [19:13<50:52,  1.32it/s]Training epoch 1:  28%|##7       | 1535/5556 [19:14<50:53,  1.32it/s]Training epoch 1:  28%|##7       | 1536/5556 [19:14<50:25,  1.33it/s]Training epoch 1:  28%|##7       | 1537/5556 [19:15<49:58,  1.34it/s]Training epoch 1:  28%|##7       | 1538/5556 [19:16<50:07,  1.34it/s]Training epoch 1:  28%|##7       | 1539/5556 [19:16<50:14,  1.33it/s]Training epoch 1:  28%|##7       | 1540/5556 [19:17<50:18,  1.33it/s]Training epoch 1:  28%|##7       | 1541/5556 [19:18<49:51,  1.34it/s]Training epoch 1:  28%|##7       | 1542/5556 [19:19<49:32,  1.35it/s]Training epoch 1:  28%|##7       | 1543/5556 [19:19<49:21,  1.35it/s]Training epoch 1:  28%|##7       | 1544/5556 [19:20<49:37,  1.35it/s]Training epoch 1:  28%|##7       | 1545/5556 [19:21<49:53,  1.34it/s]Training epoch 1:  28%|##7       | 1546/5556 [19:22<49:56,  1.34it/s]Training epoch 1:  28%|##7       | 1547/5556 [19:22<50:04,  1.33it/s]Training epoch 1:  28%|##7       | 1548/5556 [19:23<49:51,  1.34it/s]Training epoch 1:  28%|##7       | 1549/5556 [19:24<50:17,  1.33it/s]Training epoch 1:  28%|##7       | 1550/5556 [19:25<49:47,  1.34it/s]Training epoch 1:  28%|##7       | 1551/5556 [19:25<50:39,  1.32it/s]Training epoch 1:  28%|##7       | 1552/5556 [19:26<50:36,  1.32it/s]Training epoch 1:  28%|##7       | 1553/5556 [19:27<49:40,  1.34it/s]Training epoch 1:  28%|##7       | 1554/5556 [19:28<50:04,  1.33it/s]Training epoch 1:  28%|##7       | 1555/5556 [19:28<49:23,  1.35it/s]Training epoch 1:  28%|##8       | 1556/5556 [19:29<49:34,  1.34it/s]Training epoch 1:  28%|##8       | 1557/5556 [19:30<49:17,  1.35it/s]Training epoch 1:  28%|##8       | 1558/5556 [19:31<49:32,  1.35it/s]Training epoch 1:  28%|##8       | 1559/5556 [19:31<49:57,  1.33it/s]Training epoch 1:  28%|##8       | 1560/5556 [19:32<49:11,  1.35it/s]Training epoch 1:  28%|##8       | 1561/5556 [19:33<48:43,  1.37it/s]Training epoch 1:  28%|##8       | 1562/5556 [19:34<49:02,  1.36it/s]Training epoch 1:  28%|##8       | 1563/5556 [19:34<49:29,  1.34it/s]Training epoch 1:  28%|##8       | 1564/5556 [19:35<49:03,  1.36it/s]Training epoch 1:  28%|##8       | 1565/5556 [19:36<49:07,  1.35it/s]Training epoch 1:  28%|##8       | 1566/5556 [19:37<49:29,  1.34it/s]Training epoch 1:  28%|##8       | 1567/5556 [19:37<49:18,  1.35it/s]Training epoch 1:  28%|##8       | 1568/5556 [19:38<49:21,  1.35it/s]Training epoch 1:  28%|##8       | 1569/5556 [19:39<48:53,  1.36it/s]Training epoch 1:  28%|##8       | 1570/5556 [19:40<49:06,  1.35it/s]Training epoch 1:  28%|##8       | 1571/5556 [19:40<49:27,  1.34it/s]Training epoch 1:  28%|##8       | 1572/5556 [19:41<49:34,  1.34it/s]Training epoch 1:  28%|##8       | 1573/5556 [19:42<49:46,  1.33it/s]Training epoch 1:  28%|##8       | 1574/5556 [19:42<48:30,  1.37it/s]Training epoch 1:  28%|##8       | 1575/5556 [19:43<48:53,  1.36it/s]Training epoch 1:  28%|##8       | 1576/5556 [19:44<48:59,  1.35it/s]Training epoch 1:  28%|##8       | 1577/5556 [19:45<49:11,  1.35it/s]Training epoch 1:  28%|##8       | 1578/5556 [19:46<49:46,  1.33it/s]Training epoch 1:  28%|##8       | 1579/5556 [19:46<49:26,  1.34it/s]Training epoch 1:  28%|##8       | 1580/5556 [19:47<49:06,  1.35it/s]Training epoch 1:  28%|##8       | 1581/5556 [19:48<49:27,  1.34it/s]Training epoch 1:  28%|##8       | 1582/5556 [19:49<49:59,  1.32it/s]Training epoch 1:  28%|##8       | 1583/5556 [19:49<49:46,  1.33it/s]Training epoch 1:  29%|##8       | 1584/5556 [19:50<50:23,  1.31it/s]Training epoch 1:  29%|##8       | 1585/5556 [19:51<49:48,  1.33it/s]Training epoch 1:  29%|##8       | 1586/5556 [19:52<49:44,  1.33it/s]Training epoch 1:  29%|##8       | 1587/5556 [19:52<50:05,  1.32it/s]Training epoch 1:  29%|##8       | 1588/5556 [19:53<49:40,  1.33it/s]Training epoch 1:  29%|##8       | 1589/5556 [19:54<50:02,  1.32it/s]Training epoch 1:  29%|##8       | 1590/5556 [19:55<50:08,  1.32it/s]Training epoch 1:  29%|##8       | 1591/5556 [19:55<50:02,  1.32it/s]Training epoch 1:  29%|##8       | 1592/5556 [19:56<48:36,  1.36it/s]Training epoch 1:  29%|##8       | 1593/5556 [19:57<48:55,  1.35it/s]Training epoch 1:  29%|##8       | 1594/5556 [19:58<49:18,  1.34it/s]Training epoch 1:  29%|##8       | 1595/5556 [19:58<49:51,  1.32it/s]Training epoch 1:  29%|##8       | 1596/5556 [19:59<50:03,  1.32it/s]Training epoch 1:  29%|##8       | 1597/5556 [20:00<50:25,  1.31it/s]Training epoch 1:  29%|##8       | 1598/5556 [20:01<50:22,  1.31it/s]Training epoch 1:  29%|##8       | 1599/5556 [20:01<50:45,  1.30it/s]Training epoch 1:  29%|##8       | 1600/5556 [20:02<51:59,  1.27it/s]Training epoch 1:  29%|##8       | 1601/5556 [20:03<50:55,  1.29it/s]Training epoch 1:  29%|##8       | 1602/5556 [20:04<50:35,  1.30it/s]Training epoch 1:  29%|##8       | 1603/5556 [20:04<49:29,  1.33it/s]Training epoch 1:  29%|##8       | 1604/5556 [20:05<49:37,  1.33it/s]Training epoch 1:  29%|##8       | 1605/5556 [20:06<49:44,  1.32it/s]Training epoch 1:  29%|##8       | 1606/5556 [20:07<49:48,  1.32it/s]Training epoch 1:  29%|##8       | 1607/5556 [20:07<49:16,  1.34it/s]Training epoch 1:  29%|##8       | 1608/5556 [20:08<49:58,  1.32it/s]Training epoch 1:  29%|##8       | 1609/5556 [20:09<49:27,  1.33it/s]Training epoch 1:  29%|##8       | 1610/5556 [20:10<49:26,  1.33it/s]Training epoch 1:  29%|##8       | 1611/5556 [20:10<49:49,  1.32it/s]Training epoch 1:  29%|##9       | 1612/5556 [20:11<50:06,  1.31it/s]Training epoch 1:  29%|##9       | 1613/5556 [20:12<49:07,  1.34it/s]Training epoch 1:  29%|##9       | 1614/5556 [20:13<48:39,  1.35it/s]Training epoch 1:  29%|##9       | 1615/5556 [20:13<48:31,  1.35it/s]Training epoch 1:  29%|##9       | 1616/5556 [20:14<49:54,  1.32it/s]Training epoch 1:  29%|##9       | 1617/5556 [20:15<49:51,  1.32it/s]Training epoch 1:  29%|##9       | 1618/5556 [20:16<49:20,  1.33it/s]Training epoch 1:  29%|##9       | 1619/5556 [20:16<49:38,  1.32it/s]Training epoch 1:  29%|##9       | 1620/5556 [20:17<49:14,  1.33it/s]Training epoch 1:  29%|##9       | 1621/5556 [20:18<48:28,  1.35it/s]Training epoch 1:  29%|##9       | 1622/5556 [20:19<48:24,  1.35it/s]Training epoch 1:  29%|##9       | 1623/5556 [20:19<48:47,  1.34it/s]Training epoch 1:  29%|##9       | 1624/5556 [20:20<49:12,  1.33it/s]Training epoch 1:  29%|##9       | 1625/5556 [20:21<49:01,  1.34it/s]Training epoch 1:  29%|##9       | 1626/5556 [20:22<48:45,  1.34it/s]Training epoch 1:  29%|##9       | 1627/5556 [20:22<49:22,  1.33it/s]Training epoch 1:  29%|##9       | 1628/5556 [20:23<49:04,  1.33it/s]Training epoch 1:  29%|##9       | 1629/5556 [20:24<49:03,  1.33it/s]Training epoch 1:  29%|##9       | 1630/5556 [20:25<49:12,  1.33it/s]Training epoch 1:  29%|##9       | 1631/5556 [20:25<49:28,  1.32it/s]Training epoch 1:  29%|##9       | 1632/5556 [20:26<49:09,  1.33it/s]Training epoch 1:  29%|##9       | 1633/5556 [20:27<49:01,  1.33it/s]Training epoch 1:  29%|##9       | 1634/5556 [20:28<48:19,  1.35it/s]Training epoch 1:  29%|##9       | 1635/5556 [20:28<48:55,  1.34it/s]Training epoch 1:  29%|##9       | 1636/5556 [20:29<49:16,  1.33it/s]Training epoch 1:  29%|##9       | 1637/5556 [20:30<49:10,  1.33it/s]Training epoch 1:  29%|##9       | 1638/5556 [20:31<48:36,  1.34it/s]Training epoch 1:  29%|##9       | 1639/5556 [20:31<48:50,  1.34it/s]Training epoch 1:  30%|##9       | 1640/5556 [20:32<49:03,  1.33it/s]Training epoch 1:  30%|##9       | 1641/5556 [20:33<49:12,  1.33it/s]Training epoch 1:  30%|##9       | 1642/5556 [20:34<49:24,  1.32it/s]Training epoch 1:  30%|##9       | 1643/5556 [20:34<49:01,  1.33it/s]Training epoch 1:  30%|##9       | 1644/5556 [20:35<47:57,  1.36it/s]Training epoch 1:  30%|##9       | 1645/5556 [20:36<48:23,  1.35it/s]Training epoch 1:  30%|##9       | 1646/5556 [20:37<49:22,  1.32it/s]Training epoch 1:  30%|##9       | 1647/5556 [20:37<49:35,  1.31it/s]Training epoch 1:  30%|##9       | 1648/5556 [20:38<49:03,  1.33it/s]Training epoch 1:  30%|##9       | 1649/5556 [20:39<48:50,  1.33it/s]Training epoch 1:  30%|##9       | 1650/5556 [20:40<48:50,  1.33it/s]Training epoch 1:  30%|##9       | 1651/5556 [20:40<49:09,  1.32it/s]Training epoch 1:  30%|##9       | 1652/5556 [20:41<48:40,  1.34it/s]Training epoch 1:  30%|##9       | 1653/5556 [20:42<48:40,  1.34it/s]Training epoch 1:  30%|##9       | 1654/5556 [20:43<49:05,  1.32it/s]Training epoch 1:  30%|##9       | 1655/5556 [20:43<48:46,  1.33it/s]Training epoch 1:  30%|##9       | 1656/5556 [20:44<48:40,  1.34it/s]Training epoch 1:  30%|##9       | 1657/5556 [20:45<49:01,  1.33it/s]Training epoch 1:  30%|##9       | 1658/5556 [20:46<49:09,  1.32it/s]Training epoch 1:  30%|##9       | 1659/5556 [20:46<48:51,  1.33it/s]Training epoch 1:  30%|##9       | 1660/5556 [20:47<48:52,  1.33it/s]Training epoch 1:  30%|##9       | 1661/5556 [20:48<49:11,  1.32it/s]Training epoch 1:  30%|##9       | 1662/5556 [20:49<49:48,  1.30it/s]Training epoch 1:  30%|##9       | 1663/5556 [20:50<49:46,  1.30it/s]Training epoch 1:  30%|##9       | 1664/5556 [20:50<50:12,  1.29it/s]Training epoch 1:  30%|##9       | 1665/5556 [20:51<49:55,  1.30it/s]Training epoch 1:  30%|##9       | 1666/5556 [20:52<49:03,  1.32it/s]Training epoch 1:  30%|###       | 1667/5556 [20:53<49:25,  1.31it/s]Training epoch 1:  30%|###       | 1668/5556 [20:53<48:57,  1.32it/s]Training epoch 1:  30%|###       | 1669/5556 [20:54<48:55,  1.32it/s]Training epoch 1:  30%|###       | 1670/5556 [20:55<48:39,  1.33it/s]Training epoch 1:  30%|###       | 1671/5556 [20:56<47:53,  1.35it/s]Training epoch 1:  30%|###       | 1672/5556 [20:56<47:51,  1.35it/s]Training epoch 1:  30%|###       | 1673/5556 [20:57<47:44,  1.36it/s]Training epoch 1:  30%|###       | 1674/5556 [20:58<48:56,  1.32it/s]Training epoch 1:  30%|###       | 1675/5556 [20:59<49:18,  1.31it/s]Training epoch 1:  30%|###       | 1676/5556 [20:59<48:48,  1.32it/s]Training epoch 1:  30%|###       | 1677/5556 [21:00<48:35,  1.33it/s]Training epoch 1:  30%|###       | 1678/5556 [21:01<48:34,  1.33it/s]Training epoch 1:  30%|###       | 1679/5556 [21:02<49:08,  1.31it/s]Training epoch 1:  30%|###       | 1680/5556 [21:02<48:15,  1.34it/s]Training epoch 1:  30%|###       | 1681/5556 [21:03<48:18,  1.34it/s]Training epoch 1:  30%|###       | 1682/5556 [21:04<48:28,  1.33it/s]Training epoch 1:  30%|###       | 1683/5556 [21:05<48:21,  1.33it/s]Training epoch 1:  30%|###       | 1684/5556 [21:05<48:31,  1.33it/s]Training epoch 1:  30%|###       | 1685/5556 [21:06<49:00,  1.32it/s]Training epoch 1:  30%|###       | 1686/5556 [21:07<48:46,  1.32it/s]Training epoch 1:  30%|###       | 1687/5556 [21:08<47:38,  1.35it/s]Training epoch 1:  30%|###       | 1688/5556 [21:08<48:08,  1.34it/s]Training epoch 1:  30%|###       | 1689/5556 [21:09<48:23,  1.33it/s]Training epoch 1:  30%|###       | 1690/5556 [21:10<48:21,  1.33it/s]Training epoch 1:  30%|###       | 1691/5556 [21:11<48:15,  1.33it/s]Training epoch 1:  30%|###       | 1692/5556 [21:11<47:59,  1.34it/s]Training epoch 1:  30%|###       | 1693/5556 [21:12<48:17,  1.33it/s]Training epoch 1:  30%|###       | 1694/5556 [21:13<47:49,  1.35it/s]Training epoch 1:  31%|###       | 1695/5556 [21:14<47:44,  1.35it/s]Training epoch 1:  31%|###       | 1696/5556 [21:14<47:40,  1.35it/s]Training epoch 1:  31%|###       | 1697/5556 [21:15<47:51,  1.34it/s]Training epoch 1:  31%|###       | 1698/5556 [21:16<47:48,  1.35it/s]Training epoch 1:  31%|###       | 1699/5556 [21:17<48:28,  1.33it/s]Training epoch 1:  31%|###       | 1700/5556 [21:17<50:51,  1.26it/s]Training epoch 1:  31%|###       | 1701/5556 [21:18<49:35,  1.30it/s]Training epoch 1:  31%|###       | 1702/5556 [21:19<49:24,  1.30it/s]Training epoch 1:  31%|###       | 1703/5556 [21:20<48:38,  1.32it/s]Training epoch 1:  31%|###       | 1704/5556 [21:20<47:51,  1.34it/s]Training epoch 1:  31%|###       | 1705/5556 [21:21<48:02,  1.34it/s]Training epoch 1:  31%|###       | 1706/5556 [21:22<48:54,  1.31it/s]Training epoch 1:  31%|###       | 1707/5556 [21:23<48:08,  1.33it/s]Training epoch 1:  31%|###       | 1708/5556 [21:23<48:15,  1.33it/s]Training epoch 1:  31%|###       | 1709/5556 [21:24<47:50,  1.34it/s]Training epoch 1:  31%|###       | 1710/5556 [21:25<47:32,  1.35it/s]Training epoch 1:  31%|###       | 1711/5556 [21:26<48:33,  1.32it/s]Training epoch 1:  31%|###       | 1712/5556 [21:26<48:35,  1.32it/s]Training epoch 1:  31%|###       | 1713/5556 [21:27<47:59,  1.33it/s]Training epoch 1:  31%|###       | 1714/5556 [21:28<48:18,  1.33it/s]Training epoch 1:  31%|###       | 1715/5556 [21:29<47:49,  1.34it/s]Training epoch 1:  31%|###       | 1716/5556 [21:29<47:16,  1.35it/s]Training epoch 1:  31%|###       | 1717/5556 [21:30<46:33,  1.37it/s]Training epoch 1:  31%|###       | 1718/5556 [21:31<46:57,  1.36it/s]Training epoch 1:  31%|###       | 1719/5556 [21:32<46:53,  1.36it/s]Training epoch 1:  31%|###       | 1720/5556 [21:32<47:59,  1.33it/s]Training epoch 1:  31%|###       | 1721/5556 [21:33<47:59,  1.33it/s]Training epoch 1:  31%|###       | 1722/5556 [21:34<47:44,  1.34it/s]Training epoch 1:  31%|###1      | 1723/5556 [21:35<48:01,  1.33it/s]Training epoch 1:  31%|###1      | 1724/5556 [21:35<47:21,  1.35it/s]Training epoch 1:  31%|###1      | 1725/5556 [21:36<47:21,  1.35it/s]Training epoch 1:  31%|###1      | 1726/5556 [21:37<47:01,  1.36it/s]Training epoch 1:  31%|###1      | 1727/5556 [21:38<47:34,  1.34it/s]Training epoch 1:  31%|###1      | 1728/5556 [21:38<47:25,  1.35it/s]Training epoch 1:  31%|###1      | 1729/5556 [21:39<47:31,  1.34it/s]Training epoch 1:  31%|###1      | 1730/5556 [21:40<48:16,  1.32it/s]Training epoch 1:  31%|###1      | 1731/5556 [21:41<48:09,  1.32it/s]Training epoch 1:  31%|###1      | 1732/5556 [21:41<48:32,  1.31it/s]Training epoch 1:  31%|###1      | 1733/5556 [21:42<48:23,  1.32it/s]Training epoch 1:  31%|###1      | 1734/5556 [21:43<48:03,  1.33it/s]Training epoch 1:  31%|###1      | 1735/5556 [21:44<48:07,  1.32it/s]Training epoch 1:  31%|###1      | 1736/5556 [21:44<47:51,  1.33it/s]Training epoch 1:  31%|###1      | 1737/5556 [21:45<47:52,  1.33it/s]Training epoch 1:  31%|###1      | 1738/5556 [21:46<48:14,  1.32it/s]Training epoch 1:  31%|###1      | 1739/5556 [21:47<48:18,  1.32it/s]Training epoch 1:  31%|###1      | 1740/5556 [21:47<48:22,  1.31it/s]Training epoch 1:  31%|###1      | 1741/5556 [21:48<48:06,  1.32it/s]Training epoch 1:  31%|###1      | 1742/5556 [21:49<48:09,  1.32it/s]Training epoch 1:  31%|###1      | 1743/5556 [21:50<47:56,  1.33it/s]Training epoch 1:  31%|###1      | 1744/5556 [21:50<47:50,  1.33it/s]Training epoch 1:  31%|###1      | 1745/5556 [21:51<47:52,  1.33it/s]Training epoch 1:  31%|###1      | 1746/5556 [21:52<47:48,  1.33it/s]Training epoch 1:  31%|###1      | 1747/5556 [21:53<47:26,  1.34it/s]Training epoch 1:  31%|###1      | 1748/5556 [21:53<47:09,  1.35it/s]Training epoch 1:  31%|###1      | 1749/5556 [21:54<47:07,  1.35it/s]Training epoch 1:  31%|###1      | 1750/5556 [21:55<46:42,  1.36it/s]Training epoch 1:  32%|###1      | 1751/5556 [21:56<46:49,  1.35it/s]Training epoch 1:  32%|###1      | 1752/5556 [21:56<47:07,  1.35it/s]Training epoch 1:  32%|###1      | 1753/5556 [21:57<47:29,  1.33it/s]Training epoch 1:  32%|###1      | 1754/5556 [21:58<47:16,  1.34it/s]Training epoch 1:  32%|###1      | 1755/5556 [21:59<46:53,  1.35it/s]Training epoch 1:  32%|###1      | 1756/5556 [21:59<46:48,  1.35it/s]Training epoch 1:  32%|###1      | 1757/5556 [22:00<46:37,  1.36it/s]Training epoch 1:  32%|###1      | 1758/5556 [22:01<46:50,  1.35it/s]Training epoch 1:  32%|###1      | 1759/5556 [22:02<47:03,  1.34it/s]Training epoch 1:  32%|###1      | 1760/5556 [22:02<46:50,  1.35it/s]Training epoch 1:  32%|###1      | 1761/5556 [22:03<47:08,  1.34it/s]Training epoch 1:  32%|###1      | 1762/5556 [22:04<47:31,  1.33it/s]Training epoch 1:  32%|###1      | 1763/5556 [22:05<47:09,  1.34it/s]Training epoch 1:  32%|###1      | 1764/5556 [22:05<46:57,  1.35it/s]Training epoch 1:  32%|###1      | 1765/5556 [22:06<47:29,  1.33it/s]Training epoch 1:  32%|###1      | 1766/5556 [22:07<47:03,  1.34it/s]Training epoch 1:  32%|###1      | 1767/5556 [22:07<47:10,  1.34it/s]Training epoch 1:  32%|###1      | 1768/5556 [22:08<46:42,  1.35it/s]Training epoch 1:  32%|###1      | 1769/5556 [22:09<46:46,  1.35it/s]Training epoch 1:  32%|###1      | 1770/5556 [22:10<47:34,  1.33it/s]Training epoch 1:  32%|###1      | 1771/5556 [22:10<47:18,  1.33it/s]Training epoch 1:  32%|###1      | 1772/5556 [22:11<47:08,  1.34it/s]Training epoch 1:  32%|###1      | 1773/5556 [22:12<47:13,  1.33it/s]Training epoch 1:  32%|###1      | 1774/5556 [22:13<47:15,  1.33it/s]Training epoch 1:  32%|###1      | 1775/5556 [22:13<46:57,  1.34it/s]Training epoch 1:  32%|###1      | 1776/5556 [22:14<47:43,  1.32it/s]Training epoch 1:  32%|###1      | 1777/5556 [22:15<47:57,  1.31it/s]Training epoch 1:  32%|###2      | 1778/5556 [22:16<47:11,  1.33it/s]Training epoch 1:  32%|###2      | 1779/5556 [22:17<47:15,  1.33it/s]Training epoch 1:  32%|###2      | 1780/5556 [22:17<47:23,  1.33it/s]Training epoch 1:  32%|###2      | 1781/5556 [22:18<48:02,  1.31it/s]Training epoch 1:  32%|###2      | 1782/5556 [22:19<47:24,  1.33it/s]Training epoch 1:  32%|###2      | 1783/5556 [22:20<47:04,  1.34it/s]Training epoch 1:  32%|###2      | 1784/5556 [22:20<47:35,  1.32it/s]Training epoch 1:  32%|###2      | 1785/5556 [22:21<46:59,  1.34it/s]Training epoch 1:  32%|###2      | 1786/5556 [22:22<47:05,  1.33it/s]Training epoch 1:  32%|###2      | 1787/5556 [22:23<47:25,  1.32it/s]Training epoch 1:  32%|###2      | 1788/5556 [22:23<47:10,  1.33it/s]Training epoch 1:  32%|###2      | 1789/5556 [22:24<46:59,  1.34it/s]Training epoch 1:  32%|###2      | 1790/5556 [22:25<46:57,  1.34it/s]Training epoch 1:  32%|###2      | 1791/5556 [22:26<47:34,  1.32it/s]Training epoch 1:  32%|###2      | 1792/5556 [22:26<47:01,  1.33it/s]Training epoch 1:  32%|###2      | 1793/5556 [22:27<46:44,  1.34it/s]Training epoch 1:  32%|###2      | 1794/5556 [22:28<47:23,  1.32it/s]Training epoch 1:  32%|###2      | 1795/5556 [22:29<47:36,  1.32it/s]Training epoch 1:  32%|###2      | 1796/5556 [22:29<48:08,  1.30it/s]Training epoch 1:  32%|###2      | 1797/5556 [22:30<47:21,  1.32it/s]Training epoch 1:  32%|###2      | 1798/5556 [22:31<47:12,  1.33it/s]Training epoch 1:  32%|###2      | 1799/5556 [22:32<47:25,  1.32it/s]Training epoch 1:  32%|###2      | 1800/5556 [22:32<49:06,  1.27it/s]Training epoch 1:  32%|###2      | 1801/5556 [22:33<47:40,  1.31it/s]Training epoch 1:  32%|###2      | 1802/5556 [22:34<47:52,  1.31it/s]Training epoch 1:  32%|###2      | 1803/5556 [22:35<47:25,  1.32it/s]Training epoch 1:  32%|###2      | 1804/5556 [22:35<47:04,  1.33it/s]Training epoch 1:  32%|###2      | 1805/5556 [22:36<47:58,  1.30it/s]Training epoch 1:  33%|###2      | 1806/5556 [22:37<48:06,  1.30it/s]Training epoch 1:  33%|###2      | 1807/5556 [22:38<48:03,  1.30it/s]Training epoch 1:  33%|###2      | 1808/5556 [22:39<47:59,  1.30it/s]Training epoch 1:  33%|###2      | 1809/5556 [22:39<47:44,  1.31it/s]Training epoch 1:  33%|###2      | 1810/5556 [22:40<47:45,  1.31it/s]Training epoch 1:  33%|###2      | 1811/5556 [22:41<47:13,  1.32it/s]Training epoch 1:  33%|###2      | 1812/5556 [22:42<46:45,  1.33it/s]Training epoch 1:  33%|###2      | 1813/5556 [22:42<46:35,  1.34it/s]Training epoch 1:  33%|###2      | 1814/5556 [22:43<46:51,  1.33it/s]Training epoch 1:  33%|###2      | 1815/5556 [22:44<47:06,  1.32it/s]Training epoch 1:  33%|###2      | 1816/5556 [22:45<47:20,  1.32it/s]Training epoch 1:  33%|###2      | 1817/5556 [22:45<47:21,  1.32it/s]Training epoch 1:  33%|###2      | 1818/5556 [22:46<46:41,  1.33it/s]Training epoch 1:  33%|###2      | 1819/5556 [22:47<46:57,  1.33it/s]Training epoch 1:  33%|###2      | 1820/5556 [22:48<46:56,  1.33it/s]Training epoch 1:  33%|###2      | 1821/5556 [22:48<46:51,  1.33it/s]Training epoch 1:  33%|###2      | 1822/5556 [22:49<46:44,  1.33it/s]Training epoch 1:  33%|###2      | 1823/5556 [22:50<46:52,  1.33it/s]Training epoch 1:  33%|###2      | 1824/5556 [22:51<46:51,  1.33it/s]Training epoch 1:  33%|###2      | 1825/5556 [22:51<46:59,  1.32it/s]Training epoch 1:  33%|###2      | 1826/5556 [22:52<47:09,  1.32it/s]Training epoch 1:  33%|###2      | 1827/5556 [22:53<47:11,  1.32it/s]Training epoch 1:  33%|###2      | 1828/5556 [22:54<46:58,  1.32it/s]Training epoch 1:  33%|###2      | 1829/5556 [22:54<46:40,  1.33it/s]Training epoch 1:  33%|###2      | 1830/5556 [22:55<46:33,  1.33it/s]Training epoch 1:  33%|###2      | 1831/5556 [22:56<46:16,  1.34it/s]Training epoch 1:  33%|###2      | 1832/5556 [22:57<46:36,  1.33it/s]Training epoch 1:  33%|###2      | 1833/5556 [22:57<46:57,  1.32it/s]Training epoch 1:  33%|###3      | 1834/5556 [22:58<46:25,  1.34it/s]Training epoch 1:  33%|###3      | 1835/5556 [22:59<46:34,  1.33it/s]Training epoch 1:  33%|###3      | 1836/5556 [23:00<46:24,  1.34it/s]Training epoch 1:  33%|###3      | 1837/5556 [23:00<46:44,  1.33it/s]Training epoch 1:  33%|###3      | 1838/5556 [23:01<47:11,  1.31it/s]Training epoch 1:  33%|###3      | 1839/5556 [23:02<46:48,  1.32it/s]Training epoch 1:  33%|###3      | 1840/5556 [23:03<47:14,  1.31it/s]Training epoch 1:  33%|###3      | 1841/5556 [23:03<47:12,  1.31it/s]Training epoch 1:  33%|###3      | 1842/5556 [23:04<46:37,  1.33it/s]Training epoch 1:  33%|###3      | 1843/5556 [23:05<46:47,  1.32it/s]Training epoch 1:  33%|###3      | 1844/5556 [23:06<46:22,  1.33it/s]Training epoch 1:  33%|###3      | 1845/5556 [23:06<46:30,  1.33it/s]Training epoch 1:  33%|###3      | 1846/5556 [23:07<45:50,  1.35it/s]Training epoch 1:  33%|###3      | 1847/5556 [23:08<45:50,  1.35it/s]Training epoch 1:  33%|###3      | 1848/5556 [23:09<46:02,  1.34it/s]Training epoch 1:  33%|###3      | 1849/5556 [23:09<46:00,  1.34it/s]Training epoch 1:  33%|###3      | 1850/5556 [23:10<46:10,  1.34it/s]Training epoch 1:  33%|###3      | 1851/5556 [23:11<45:47,  1.35it/s]Training epoch 1:  33%|###3      | 1852/5556 [23:12<45:45,  1.35it/s]Training epoch 1:  33%|###3      | 1853/5556 [23:12<46:00,  1.34it/s]Training epoch 1:  33%|###3      | 1854/5556 [23:13<46:23,  1.33it/s]Training epoch 1:  33%|###3      | 1855/5556 [23:14<46:13,  1.33it/s]Training epoch 1:  33%|###3      | 1856/5556 [23:15<45:56,  1.34it/s]Training epoch 1:  33%|###3      | 1857/5556 [23:15<45:52,  1.34it/s]Training epoch 1:  33%|###3      | 1858/5556 [23:16<46:09,  1.34it/s]Training epoch 1:  33%|###3      | 1859/5556 [23:17<45:58,  1.34it/s]Training epoch 1:  33%|###3      | 1860/5556 [23:18<45:55,  1.34it/s]Training epoch 1:  33%|###3      | 1861/5556 [23:18<46:24,  1.33it/s]Training epoch 1:  34%|###3      | 1862/5556 [23:19<46:12,  1.33it/s]Training epoch 1:  34%|###3      | 1863/5556 [23:20<46:04,  1.34it/s]Training epoch 1:  34%|###3      | 1864/5556 [23:21<45:42,  1.35it/s]Training epoch 1:  34%|###3      | 1865/5556 [23:21<45:47,  1.34it/s]Training epoch 1:  34%|###3      | 1866/5556 [23:22<46:11,  1.33it/s]Training epoch 1:  34%|###3      | 1867/5556 [23:23<46:20,  1.33it/s]Training epoch 1:  34%|###3      | 1868/5556 [23:24<46:32,  1.32it/s]Training epoch 1:  34%|###3      | 1869/5556 [23:24<46:30,  1.32it/s]Training epoch 1:  34%|###3      | 1870/5556 [23:25<45:45,  1.34it/s]Training epoch 1:  34%|###3      | 1871/5556 [23:26<46:11,  1.33it/s]Training epoch 1:  34%|###3      | 1872/5556 [23:27<46:15,  1.33it/s]Training epoch 1:  34%|###3      | 1873/5556 [23:27<46:02,  1.33it/s]Training epoch 1:  34%|###3      | 1874/5556 [23:28<46:31,  1.32it/s]Training epoch 1:  34%|###3      | 1875/5556 [23:29<46:19,  1.32it/s]Training epoch 1:  34%|###3      | 1876/5556 [23:30<45:32,  1.35it/s]Training epoch 1:  34%|###3      | 1877/5556 [23:30<45:57,  1.33it/s]Training epoch 1:  34%|###3      | 1878/5556 [23:31<45:18,  1.35it/s]Training epoch 1:  34%|###3      | 1879/5556 [23:32<45:49,  1.34it/s]Training epoch 1:  34%|###3      | 1880/5556 [23:33<46:06,  1.33it/s]Training epoch 1:  34%|###3      | 1881/5556 [23:33<45:51,  1.34it/s]Training epoch 1:  34%|###3      | 1882/5556 [23:34<46:24,  1.32it/s]Training epoch 1:  34%|###3      | 1883/5556 [23:35<45:51,  1.33it/s]Training epoch 1:  34%|###3      | 1884/5556 [23:36<45:58,  1.33it/s]Training epoch 1:  34%|###3      | 1885/5556 [23:36<46:18,  1.32it/s]Training epoch 1:  34%|###3      | 1886/5556 [23:37<46:00,  1.33it/s]Training epoch 1:  34%|###3      | 1887/5556 [23:38<46:09,  1.33it/s]Training epoch 1:  34%|###3      | 1888/5556 [23:39<45:54,  1.33it/s]Training epoch 1:  34%|###3      | 1889/5556 [23:39<45:59,  1.33it/s]Training epoch 1:  34%|###4      | 1890/5556 [23:40<46:26,  1.32it/s]Training epoch 1:  34%|###4      | 1891/5556 [23:41<46:10,  1.32it/s]Training epoch 1:  34%|###4      | 1892/5556 [23:42<46:03,  1.33it/s]Training epoch 1:  34%|###4      | 1893/5556 [23:42<45:49,  1.33it/s]Training epoch 1:  34%|###4      | 1894/5556 [23:43<45:08,  1.35it/s]Training epoch 1:  34%|###4      | 1895/5556 [23:44<45:05,  1.35it/s]Training epoch 1:  34%|###4      | 1896/5556 [23:45<45:04,  1.35it/s]Training epoch 1:  34%|###4      | 1897/5556 [23:45<45:37,  1.34it/s]Training epoch 1:  34%|###4      | 1898/5556 [23:46<45:55,  1.33it/s]Training epoch 1:  34%|###4      | 1899/5556 [23:47<45:25,  1.34it/s]Training epoch 1:  34%|###4      | 1900/5556 [23:48<47:28,  1.28it/s]Training epoch 1:  34%|###4      | 1901/5556 [23:48<47:35,  1.28it/s]Training epoch 1:  34%|###4      | 1902/5556 [23:49<46:57,  1.30it/s]Training epoch 1:  34%|###4      | 1903/5556 [23:50<46:08,  1.32it/s]Training epoch 1:  34%|###4      | 1904/5556 [23:51<45:59,  1.32it/s]Training epoch 1:  34%|###4      | 1905/5556 [23:51<46:13,  1.32it/s]Training epoch 1:  34%|###4      | 1906/5556 [23:52<46:00,  1.32it/s]Training epoch 1:  34%|###4      | 1907/5556 [23:53<45:22,  1.34it/s]Training epoch 1:  34%|###4      | 1908/5556 [23:54<45:28,  1.34it/s]Training epoch 1:  34%|###4      | 1909/5556 [23:54<45:12,  1.34it/s]Training epoch 1:  34%|###4      | 1910/5556 [23:55<45:57,  1.32it/s]Training epoch 1:  34%|###4      | 1911/5556 [23:56<46:18,  1.31it/s]Training epoch 1:  34%|###4      | 1912/5556 [23:57<45:41,  1.33it/s]Training epoch 1:  34%|###4      | 1913/5556 [23:57<45:16,  1.34it/s]Training epoch 1:  34%|###4      | 1914/5556 [23:58<45:08,  1.34it/s]Training epoch 1:  34%|###4      | 1915/5556 [23:59<44:56,  1.35it/s]Training epoch 1:  34%|###4      | 1916/5556 [24:00<44:48,  1.35it/s]Training epoch 1:  35%|###4      | 1917/5556 [24:00<44:21,  1.37it/s]Training epoch 1:  35%|###4      | 1918/5556 [24:01<45:27,  1.33it/s]Training epoch 1:  35%|###4      | 1919/5556 [24:02<45:47,  1.32it/s]Training epoch 1:  35%|###4      | 1920/5556 [24:03<45:21,  1.34it/s]Training epoch 1:  35%|###4      | 1921/5556 [24:03<45:16,  1.34it/s]Training epoch 1:  35%|###4      | 1922/5556 [24:04<44:58,  1.35it/s]Training epoch 1:  35%|###4      | 1923/5556 [24:05<45:27,  1.33it/s]Training epoch 1:  35%|###4      | 1924/5556 [24:06<45:21,  1.33it/s]Training epoch 1:  35%|###4      | 1925/5556 [24:06<44:56,  1.35it/s]Training epoch 1:  35%|###4      | 1926/5556 [24:07<44:28,  1.36it/s]Training epoch 1:  35%|###4      | 1927/5556 [24:08<44:50,  1.35it/s]Training epoch 1:  35%|###4      | 1928/5556 [24:09<45:01,  1.34it/s]Training epoch 1:  35%|###4      | 1929/5556 [24:09<45:26,  1.33it/s]Training epoch 1:  35%|###4      | 1930/5556 [24:10<45:49,  1.32it/s]Training epoch 1:  35%|###4      | 1931/5556 [24:11<45:29,  1.33it/s]Training epoch 1:  35%|###4      | 1932/5556 [24:12<45:27,  1.33it/s]Training epoch 1:  35%|###4      | 1933/5556 [24:12<45:07,  1.34it/s]Training epoch 1:  35%|###4      | 1934/5556 [24:13<45:20,  1.33it/s]Training epoch 1:  35%|###4      | 1935/5556 [24:14<44:59,  1.34it/s]Training epoch 1:  35%|###4      | 1936/5556 [24:15<44:45,  1.35it/s]Training epoch 1:  35%|###4      | 1937/5556 [24:15<44:45,  1.35it/s]Training epoch 1:  35%|###4      | 1938/5556 [24:16<45:08,  1.34it/s]Training epoch 1:  35%|###4      | 1939/5556 [24:17<45:18,  1.33it/s]Training epoch 1:  35%|###4      | 1940/5556 [24:18<45:02,  1.34it/s]Training epoch 1:  35%|###4      | 1941/5556 [24:18<45:07,  1.34it/s]Training epoch 1:  35%|###4      | 1942/5556 [24:19<45:41,  1.32it/s]Training epoch 1:  35%|###4      | 1943/5556 [24:20<45:04,  1.34it/s]Training epoch 1:  35%|###4      | 1944/5556 [24:21<45:22,  1.33it/s]Training epoch 1:  35%|###5      | 1945/5556 [24:21<45:06,  1.33it/s]Training epoch 1:  35%|###5      | 1946/5556 [24:22<45:46,  1.31it/s]Training epoch 1:  35%|###5      | 1947/5556 [24:23<46:13,  1.30it/s]Training epoch 1:  35%|###5      | 1948/5556 [24:24<45:48,  1.31it/s]Training epoch 1:  35%|###5      | 1949/5556 [24:24<45:42,  1.32it/s]Training epoch 1:  35%|###5      | 1950/5556 [24:25<44:50,  1.34it/s]Training epoch 1:  35%|###5      | 1951/5556 [24:26<44:43,  1.34it/s]Training epoch 1:  35%|###5      | 1952/5556 [24:27<44:20,  1.35it/s]Training epoch 1:  35%|###5      | 1953/5556 [24:27<44:19,  1.35it/s]Training epoch 1:  35%|###5      | 1954/5556 [24:28<44:03,  1.36it/s]Training epoch 1:  35%|###5      | 1955/5556 [24:29<44:13,  1.36it/s]Training epoch 1:  35%|###5      | 1956/5556 [24:30<45:11,  1.33it/s]Training epoch 1:  35%|###5      | 1957/5556 [24:30<45:42,  1.31it/s]Training epoch 1:  35%|###5      | 1958/5556 [24:31<45:50,  1.31it/s]Training epoch 1:  35%|###5      | 1959/5556 [24:32<45:09,  1.33it/s]Training epoch 1:  35%|###5      | 1960/5556 [24:33<45:07,  1.33it/s]Training epoch 1:  35%|###5      | 1961/5556 [24:33<45:19,  1.32it/s]Training epoch 1:  35%|###5      | 1962/5556 [24:34<45:17,  1.32it/s]Training epoch 1:  35%|###5      | 1963/5556 [24:35<44:52,  1.33it/s]Training epoch 1:  35%|###5      | 1964/5556 [24:36<44:46,  1.34it/s]Training epoch 1:  35%|###5      | 1965/5556 [24:36<44:38,  1.34it/s]Training epoch 1:  35%|###5      | 1966/5556 [24:37<45:00,  1.33it/s]Training epoch 1:  35%|###5      | 1967/5556 [24:38<45:09,  1.32it/s]Training epoch 1:  35%|###5      | 1968/5556 [24:39<45:21,  1.32it/s]Training epoch 1:  35%|###5      | 1969/5556 [24:39<46:16,  1.29it/s]Training epoch 1:  35%|###5      | 1970/5556 [24:40<45:24,  1.32it/s]Training epoch 1:  35%|###5      | 1971/5556 [24:41<45:13,  1.32it/s]Training epoch 1:  35%|###5      | 1972/5556 [24:42<45:43,  1.31it/s]Training epoch 1:  36%|###5      | 1973/5556 [24:43<45:40,  1.31it/s]Training epoch 1:  36%|###5      | 1974/5556 [24:43<45:26,  1.31it/s]Training epoch 1:  36%|###5      | 1975/5556 [24:44<44:39,  1.34it/s]Training epoch 1:  36%|###5      | 1976/5556 [24:45<44:09,  1.35it/s]Training epoch 1:  36%|###5      | 1977/5556 [24:45<44:57,  1.33it/s]Training epoch 1:  36%|###5      | 1978/5556 [24:46<44:43,  1.33it/s]Training epoch 1:  36%|###5      | 1979/5556 [24:47<44:40,  1.33it/s]Training epoch 1:  36%|###5      | 1980/5556 [24:48<44:52,  1.33it/s]Training epoch 1:  36%|###5      | 1981/5556 [24:48<44:55,  1.33it/s]Training epoch 1:  36%|###5      | 1982/5556 [24:49<44:35,  1.34it/s]Training epoch 1:  36%|###5      | 1983/5556 [24:50<44:02,  1.35it/s]Training epoch 1:  36%|###5      | 1984/5556 [24:51<43:35,  1.37it/s]Training epoch 1:  36%|###5      | 1985/5556 [24:51<44:33,  1.34it/s]Training epoch 1:  36%|###5      | 1986/5556 [24:52<44:31,  1.34it/s]Training epoch 1:  36%|###5      | 1987/5556 [24:53<44:08,  1.35it/s]Training epoch 1:  36%|###5      | 1988/5556 [24:54<45:22,  1.31it/s]Training epoch 1:  36%|###5      | 1989/5556 [24:55<45:52,  1.30it/s]Training epoch 1:  36%|###5      | 1990/5556 [24:55<45:12,  1.31it/s]Training epoch 1:  36%|###5      | 1991/5556 [24:56<44:51,  1.32it/s]Training epoch 1:  36%|###5      | 1992/5556 [24:57<45:09,  1.32it/s]Training epoch 1:  36%|###5      | 1993/5556 [24:58<44:45,  1.33it/s]Training epoch 1:  36%|###5      | 1994/5556 [24:58<44:44,  1.33it/s]Training epoch 1:  36%|###5      | 1995/5556 [24:59<44:57,  1.32it/s]Training epoch 1:  36%|###5      | 1996/5556 [25:00<44:52,  1.32it/s]Training epoch 1:  36%|###5      | 1997/5556 [25:01<45:00,  1.32it/s]Training epoch 1:  36%|###5      | 1998/5556 [25:01<44:43,  1.33it/s]Training epoch 1:  36%|###5      | 1999/5556 [25:02<44:40,  1.33it/s]Training epoch 1:  36%|###5      | 2000/5556 [25:03<46:17,  1.28it/s]Training epoch 1:  36%|###6      | 2001/5556 [25:04<45:41,  1.30it/s]Training epoch 1:  36%|###6      | 2002/5556 [25:04<45:14,  1.31it/s]Training epoch 1:  36%|###6      | 2003/5556 [25:05<45:12,  1.31it/s]Training epoch 1:  36%|###6      | 2004/5556 [25:06<44:39,  1.33it/s]Training epoch 1:  36%|###6      | 2005/5556 [25:07<43:45,  1.35it/s]Training epoch 1:  36%|###6      | 2006/5556 [25:07<43:32,  1.36it/s]Training epoch 1:  36%|###6      | 2007/5556 [25:08<44:09,  1.34it/s]Training epoch 1:  36%|###6      | 2008/5556 [25:09<43:45,  1.35it/s]Training epoch 1:  36%|###6      | 2009/5556 [25:10<44:02,  1.34it/s]Training epoch 1:  36%|###6      | 2010/5556 [25:10<43:57,  1.34it/s]Training epoch 1:  36%|###6      | 2011/5556 [25:11<43:55,  1.34it/s]Training epoch 1:  36%|###6      | 2012/5556 [25:12<44:01,  1.34it/s]Training epoch 1:  36%|###6      | 2013/5556 [25:13<43:44,  1.35it/s]Training epoch 1:  36%|###6      | 2014/5556 [25:13<43:50,  1.35it/s]Training epoch 1:  36%|###6      | 2015/5556 [25:14<43:53,  1.34it/s]Training epoch 1:  36%|###6      | 2016/5556 [25:15<43:52,  1.34it/s]Training epoch 1:  36%|###6      | 2017/5556 [25:16<44:02,  1.34it/s]Training epoch 1:  36%|###6      | 2018/5556 [25:16<44:07,  1.34it/s]Training epoch 1:  36%|###6      | 2019/5556 [25:17<44:13,  1.33it/s]Training epoch 1:  36%|###6      | 2020/5556 [25:18<44:00,  1.34it/s]Training epoch 1:  36%|###6      | 2021/5556 [25:19<44:17,  1.33it/s]Training epoch 1:  36%|###6      | 2022/5556 [25:19<44:43,  1.32it/s]Training epoch 1:  36%|###6      | 2023/5556 [25:20<44:10,  1.33it/s]Training epoch 1:  36%|###6      | 2024/5556 [25:21<44:14,  1.33it/s]Training epoch 1:  36%|###6      | 2025/5556 [25:22<44:22,  1.33it/s]Training epoch 1:  36%|###6      | 2026/5556 [25:22<43:56,  1.34it/s]Training epoch 1:  36%|###6      | 2027/5556 [25:23<44:44,  1.31it/s]Training epoch 1:  37%|###6      | 2028/5556 [25:24<44:52,  1.31it/s]Training epoch 1:  37%|###6      | 2029/5556 [25:25<44:54,  1.31it/s]Training epoch 1:  37%|###6      | 2030/5556 [25:25<44:47,  1.31it/s]Training epoch 1:  37%|###6      | 2031/5556 [25:26<44:36,  1.32it/s]Training epoch 1:  37%|###6      | 2032/5556 [25:27<44:31,  1.32it/s]Training epoch 1:  37%|###6      | 2033/5556 [25:28<44:23,  1.32it/s]Training epoch 1:  37%|###6      | 2034/5556 [25:28<44:56,  1.31it/s]Training epoch 1:  37%|###6      | 2035/5556 [25:29<45:32,  1.29it/s]Training epoch 1:  37%|###6      | 2036/5556 [25:30<45:07,  1.30it/s]Training epoch 1:  37%|###6      | 2037/5556 [25:31<44:51,  1.31it/s]Training epoch 1:  37%|###6      | 2038/5556 [25:31<44:45,  1.31it/s]Training epoch 1:  37%|###6      | 2039/5556 [25:32<44:35,  1.31it/s]Training epoch 1:  37%|###6      | 2040/5556 [25:33<43:43,  1.34it/s]Training epoch 1:  37%|###6      | 2041/5556 [25:34<43:53,  1.33it/s]Training epoch 1:  37%|###6      | 2042/5556 [25:34<44:26,  1.32it/s]Training epoch 1:  37%|###6      | 2043/5556 [25:35<44:01,  1.33it/s]Training epoch 1:  37%|###6      | 2044/5556 [25:36<43:17,  1.35it/s]Training epoch 1:  37%|###6      | 2045/5556 [25:37<43:32,  1.34it/s]Training epoch 1:  37%|###6      | 2046/5556 [25:37<43:17,  1.35it/s]Training epoch 1:  37%|###6      | 2047/5556 [25:38<43:31,  1.34it/s]Training epoch 1:  37%|###6      | 2048/5556 [25:39<43:02,  1.36it/s]Training epoch 1:  37%|###6      | 2049/5556 [25:40<43:44,  1.34it/s]Training epoch 1:  37%|###6      | 2050/5556 [25:40<44:24,  1.32it/s]Training epoch 1:  37%|###6      | 2051/5556 [25:41<44:33,  1.31it/s]Training epoch 1:  37%|###6      | 2052/5556 [25:42<44:08,  1.32it/s]Training epoch 1:  37%|###6      | 2053/5556 [25:43<44:23,  1.32it/s]Training epoch 1:  37%|###6      | 2054/5556 [25:43<43:42,  1.34it/s]Training epoch 1:  37%|###6      | 2055/5556 [25:44<43:21,  1.35it/s]Training epoch 1:  37%|###7      | 2056/5556 [25:45<43:29,  1.34it/s]Training epoch 1:  37%|###7      | 2057/5556 [25:46<43:37,  1.34it/s]Training epoch 1:  37%|###7      | 2058/5556 [25:46<43:28,  1.34it/s]Training epoch 1:  37%|###7      | 2059/5556 [25:47<43:45,  1.33it/s]Training epoch 1:  37%|###7      | 2060/5556 [25:48<44:14,  1.32it/s]Training epoch 1:  37%|###7      | 2061/5556 [25:49<44:15,  1.32it/s]Training epoch 1:  37%|###7      | 2062/5556 [25:50<44:22,  1.31it/s]Training epoch 1:  37%|###7      | 2063/5556 [25:50<43:51,  1.33it/s]Training epoch 1:  37%|###7      | 2064/5556 [25:51<43:32,  1.34it/s]Training epoch 1:  37%|###7      | 2065/5556 [25:52<43:15,  1.34it/s]Training epoch 1:  37%|###7      | 2066/5556 [25:52<43:05,  1.35it/s]Training epoch 1:  37%|###7      | 2067/5556 [25:53<43:26,  1.34it/s]Training epoch 1:  37%|###7      | 2068/5556 [25:54<43:34,  1.33it/s]Training epoch 1:  37%|###7      | 2069/5556 [25:55<43:52,  1.32it/s]Training epoch 1:  37%|###7      | 2070/5556 [25:55<43:16,  1.34it/s]Training epoch 1:  37%|###7      | 2071/5556 [25:56<43:03,  1.35it/s]Training epoch 1:  37%|###7      | 2072/5556 [25:57<43:08,  1.35it/s]Training epoch 1:  37%|###7      | 2073/5556 [25:58<43:06,  1.35it/s]Training epoch 1:  37%|###7      | 2074/5556 [25:58<43:01,  1.35it/s]Training epoch 1:  37%|###7      | 2075/5556 [25:59<43:36,  1.33it/s]Training epoch 1:  37%|###7      | 2076/5556 [26:00<43:27,  1.33it/s]Training epoch 1:  37%|###7      | 2077/5556 [26:01<43:32,  1.33it/s]Training epoch 1:  37%|###7      | 2078/5556 [26:01<43:56,  1.32it/s]Training epoch 1:  37%|###7      | 2079/5556 [26:02<43:34,  1.33it/s]Training epoch 1:  37%|###7      | 2080/5556 [26:03<44:17,  1.31it/s]Training epoch 1:  37%|###7      | 2081/5556 [26:04<44:04,  1.31it/s]Training epoch 1:  37%|###7      | 2082/5556 [26:04<43:49,  1.32it/s]Training epoch 1:  37%|###7      | 2083/5556 [26:05<43:40,  1.33it/s]Training epoch 1:  38%|###7      | 2084/5556 [26:06<43:04,  1.34it/s]Training epoch 1:  38%|###7      | 2085/5556 [26:07<42:57,  1.35it/s]Training epoch 1:  38%|###7      | 2086/5556 [26:07<43:46,  1.32it/s]Training epoch 1:  38%|###7      | 2087/5556 [26:08<44:05,  1.31it/s]Training epoch 1:  38%|###7      | 2088/5556 [26:09<44:00,  1.31it/s]Training epoch 1:  38%|###7      | 2089/5556 [26:10<44:33,  1.30it/s]Training epoch 1:  38%|###7      | 2090/5556 [26:11<44:14,  1.31it/s]Training epoch 1:  38%|###7      | 2091/5556 [26:11<43:56,  1.31it/s]Training epoch 1:  38%|###7      | 2092/5556 [26:12<44:20,  1.30it/s]Training epoch 1:  38%|###7      | 2093/5556 [26:13<43:57,  1.31it/s]Training epoch 1:  38%|###7      | 2094/5556 [26:14<44:07,  1.31it/s]Training epoch 1:  38%|###7      | 2095/5556 [26:14<43:50,  1.32it/s]Training epoch 1:  38%|###7      | 2096/5556 [26:15<42:59,  1.34it/s]Training epoch 1:  38%|###7      | 2097/5556 [26:16<43:30,  1.32it/s]Training epoch 1:  38%|###7      | 2098/5556 [26:17<43:20,  1.33it/s]Training epoch 1:  38%|###7      | 2099/5556 [26:17<42:46,  1.35it/s]Training epoch 1:  38%|###7      | 2100/5556 [26:18<45:09,  1.28it/s]Training epoch 1:  38%|###7      | 2101/5556 [26:19<44:12,  1.30it/s]Training epoch 1:  38%|###7      | 2102/5556 [26:20<43:49,  1.31it/s]Training epoch 1:  38%|###7      | 2103/5556 [26:20<43:45,  1.31it/s]Training epoch 1:  38%|###7      | 2104/5556 [26:21<43:38,  1.32it/s]Training epoch 1:  38%|###7      | 2105/5556 [26:22<43:41,  1.32it/s]Training epoch 1:  38%|###7      | 2106/5556 [26:23<43:16,  1.33it/s]Training epoch 1:  38%|###7      | 2107/5556 [26:23<42:39,  1.35it/s]Training epoch 1:  38%|###7      | 2108/5556 [26:24<42:18,  1.36it/s]Training epoch 1:  38%|###7      | 2109/5556 [26:25<41:48,  1.37it/s]Training epoch 1:  38%|###7      | 2110/5556 [26:26<42:31,  1.35it/s]Training epoch 1:  38%|###7      | 2111/5556 [26:26<42:18,  1.36it/s]Training epoch 1:  38%|###8      | 2112/5556 [26:27<42:43,  1.34it/s]Training epoch 1:  38%|###8      | 2113/5556 [26:28<43:20,  1.32it/s]Training epoch 1:  38%|###8      | 2114/5556 [26:29<43:04,  1.33it/s]Training epoch 1:  38%|###8      | 2115/5556 [26:29<42:47,  1.34it/s]Training epoch 1:  38%|###8      | 2116/5556 [26:30<42:28,  1.35it/s]Training epoch 1:  38%|###8      | 2117/5556 [26:31<43:02,  1.33it/s]Training epoch 1:  38%|###8      | 2118/5556 [26:32<43:37,  1.31it/s]Training epoch 1:  38%|###8      | 2119/5556 [26:32<43:44,  1.31it/s]Training epoch 1:  38%|###8      | 2120/5556 [26:33<43:21,  1.32it/s]Training epoch 1:  38%|###8      | 2121/5556 [26:34<43:35,  1.31it/s]Training epoch 1:  38%|###8      | 2122/5556 [26:35<43:56,  1.30it/s]Training epoch 1:  38%|###8      | 2123/5556 [26:35<43:46,  1.31it/s]Training epoch 1:  38%|###8      | 2124/5556 [26:36<43:37,  1.31it/s]Training epoch 1:  38%|###8      | 2125/5556 [26:37<43:18,  1.32it/s]Training epoch 1:  38%|###8      | 2126/5556 [26:38<42:46,  1.34it/s]Training epoch 1:  38%|###8      | 2127/5556 [26:38<42:50,  1.33it/s]Training epoch 1:  38%|###8      | 2128/5556 [26:39<43:12,  1.32it/s]Training epoch 1:  38%|###8      | 2129/5556 [26:40<43:04,  1.33it/s]Training epoch 1:  38%|###8      | 2130/5556 [26:41<43:15,  1.32it/s]Training epoch 1:  38%|###8      | 2131/5556 [26:42<44:04,  1.30it/s]Training epoch 1:  38%|###8      | 2132/5556 [26:42<43:41,  1.31it/s]Training epoch 1:  38%|###8      | 2133/5556 [26:43<43:14,  1.32it/s]Training epoch 1:  38%|###8      | 2134/5556 [26:44<43:15,  1.32it/s]Training epoch 1:  38%|###8      | 2135/5556 [26:45<42:30,  1.34it/s]Training epoch 1:  38%|###8      | 2136/5556 [26:45<42:06,  1.35it/s]Training epoch 1:  38%|###8      | 2137/5556 [26:46<42:44,  1.33it/s]Training epoch 1:  38%|###8      | 2138/5556 [26:47<42:05,  1.35it/s]Training epoch 1:  38%|###8      | 2139/5556 [26:47<42:16,  1.35it/s]Training epoch 1:  39%|###8      | 2140/5556 [26:48<42:35,  1.34it/s]Training epoch 1:  39%|###8      | 2141/5556 [26:49<42:29,  1.34it/s]Training epoch 1:  39%|###8      | 2142/5556 [26:50<42:07,  1.35it/s]Training epoch 1:  39%|###8      | 2143/5556 [26:50<42:14,  1.35it/s]Training epoch 1:  39%|###8      | 2144/5556 [26:51<42:31,  1.34it/s]Training epoch 1:  39%|###8      | 2145/5556 [26:52<42:50,  1.33it/s]Training epoch 1:  39%|###8      | 2146/5556 [26:53<42:01,  1.35it/s]Training epoch 1:  39%|###8      | 2147/5556 [26:53<42:31,  1.34it/s]Training epoch 1:  39%|###8      | 2148/5556 [26:54<42:31,  1.34it/s]Training epoch 1:  39%|###8      | 2149/5556 [26:55<42:49,  1.33it/s]Training epoch 1:  39%|###8      | 2150/5556 [26:56<42:41,  1.33it/s]Training epoch 1:  39%|###8      | 2151/5556 [26:56<42:42,  1.33it/s]Training epoch 1:  39%|###8      | 2152/5556 [26:57<42:39,  1.33it/s]Training epoch 1:  39%|###8      | 2153/5556 [26:58<42:26,  1.34it/s]Training epoch 1:  39%|###8      | 2154/5556 [26:59<42:21,  1.34it/s]Training epoch 1:  39%|###8      | 2155/5556 [26:59<42:11,  1.34it/s]Training epoch 1:  39%|###8      | 2156/5556 [27:00<41:55,  1.35it/s]Training epoch 1:  39%|###8      | 2157/5556 [27:01<42:25,  1.34it/s]Training epoch 1:  39%|###8      | 2158/5556 [27:02<42:23,  1.34it/s]Training epoch 1:  39%|###8      | 2159/5556 [27:02<42:31,  1.33it/s]Training epoch 1:  39%|###8      | 2160/5556 [27:03<42:44,  1.32it/s]Training epoch 1:  39%|###8      | 2161/5556 [27:04<41:42,  1.36it/s]Training epoch 1:  39%|###8      | 2162/5556 [27:05<42:13,  1.34it/s]Training epoch 1:  39%|###8      | 2163/5556 [27:05<42:18,  1.34it/s]Training epoch 1:  39%|###8      | 2164/5556 [27:06<42:33,  1.33it/s]Training epoch 1:  39%|###8      | 2165/5556 [27:07<42:18,  1.34it/s]Training epoch 1:  39%|###8      | 2166/5556 [27:08<42:30,  1.33it/s]Training epoch 1:  39%|###9      | 2167/5556 [27:08<42:29,  1.33it/s]Training epoch 1:  39%|###9      | 2168/5556 [27:09<42:35,  1.33it/s]Training epoch 1:  39%|###9      | 2169/5556 [27:10<42:23,  1.33it/s]Training epoch 1:  39%|###9      | 2170/5556 [27:11<43:04,  1.31it/s]Training epoch 1:  39%|###9      | 2171/5556 [27:12<43:18,  1.30it/s]Training epoch 1:  39%|###9      | 2172/5556 [27:12<43:14,  1.30it/s]Training epoch 1:  39%|###9      | 2173/5556 [27:13<43:05,  1.31it/s]Training epoch 1:  39%|###9      | 2174/5556 [27:14<43:23,  1.30it/s]Training epoch 1:  39%|###9      | 2175/5556 [27:15<42:41,  1.32it/s]Training epoch 1:  39%|###9      | 2176/5556 [27:15<42:48,  1.32it/s]Training epoch 1:  39%|###9      | 2177/5556 [27:16<42:59,  1.31it/s]Training epoch 1:  39%|###9      | 2178/5556 [27:17<42:41,  1.32it/s]Training epoch 1:  39%|###9      | 2179/5556 [27:18<42:31,  1.32it/s]Training epoch 1:  39%|###9      | 2180/5556 [27:18<42:15,  1.33it/s]Training epoch 1:  39%|###9      | 2181/5556 [27:19<42:24,  1.33it/s]Training epoch 1:  39%|###9      | 2182/5556 [27:20<42:17,  1.33it/s]Training epoch 1:  39%|###9      | 2183/5556 [27:21<41:59,  1.34it/s]Training epoch 1:  39%|###9      | 2184/5556 [27:21<42:26,  1.32it/s]Training epoch 1:  39%|###9      | 2185/5556 [27:22<41:53,  1.34it/s]Training epoch 1:  39%|###9      | 2186/5556 [27:23<41:57,  1.34it/s]Training epoch 1:  39%|###9      | 2187/5556 [27:24<42:04,  1.33it/s]Training epoch 1:  39%|###9      | 2188/5556 [27:24<41:51,  1.34it/s]Training epoch 1:  39%|###9      | 2189/5556 [27:25<41:46,  1.34it/s]Training epoch 1:  39%|###9      | 2190/5556 [27:26<41:31,  1.35it/s]Training epoch 1:  39%|###9      | 2191/5556 [27:27<41:50,  1.34it/s]Training epoch 1:  39%|###9      | 2192/5556 [27:27<41:45,  1.34it/s]Training epoch 1:  39%|###9      | 2193/5556 [27:28<42:13,  1.33it/s]Training epoch 1:  39%|###9      | 2194/5556 [27:29<42:00,  1.33it/s]Training epoch 1:  40%|###9      | 2195/5556 [27:30<42:08,  1.33it/s]Training epoch 1:  40%|###9      | 2196/5556 [27:30<41:30,  1.35it/s]Training epoch 1:  40%|###9      | 2197/5556 [27:31<41:28,  1.35it/s]Training epoch 1:  40%|###9      | 2198/5556 [27:32<41:44,  1.34it/s]Training epoch 1:  40%|###9      | 2199/5556 [27:32<41:24,  1.35it/s]Training epoch 1:  40%|###9      | 2200/5556 [27:33<43:01,  1.30it/s]Training epoch 1:  40%|###9      | 2201/5556 [27:34<42:39,  1.31it/s]Training epoch 1:  40%|###9      | 2202/5556 [27:35<42:16,  1.32it/s]Training epoch 1:  40%|###9      | 2203/5556 [27:36<42:01,  1.33it/s]Training epoch 1:  40%|###9      | 2204/5556 [27:36<41:20,  1.35it/s]Training epoch 1:  40%|###9      | 2205/5556 [27:37<41:00,  1.36it/s]Training epoch 1:  40%|###9      | 2206/5556 [27:38<40:58,  1.36it/s]Training epoch 1:  40%|###9      | 2207/5556 [27:38<41:31,  1.34it/s]Training epoch 1:  40%|###9      | 2208/5556 [27:39<41:51,  1.33it/s]Training epoch 1:  40%|###9      | 2209/5556 [27:40<41:48,  1.33it/s]Training epoch 1:  40%|###9      | 2210/5556 [27:41<42:18,  1.32it/s]Training epoch 1:  40%|###9      | 2211/5556 [27:42<42:09,  1.32it/s]Training epoch 1:  40%|###9      | 2212/5556 [27:42<42:25,  1.31it/s]Training epoch 1:  40%|###9      | 2213/5556 [27:43<42:42,  1.30it/s]Training epoch 1:  40%|###9      | 2214/5556 [27:44<42:47,  1.30it/s]Training epoch 1:  40%|###9      | 2215/5556 [27:45<41:44,  1.33it/s]Training epoch 1:  40%|###9      | 2216/5556 [27:45<41:51,  1.33it/s]Training epoch 1:  40%|###9      | 2217/5556 [27:46<41:45,  1.33it/s]Training epoch 1:  40%|###9      | 2218/5556 [27:47<41:33,  1.34it/s]Training epoch 1:  40%|###9      | 2219/5556 [27:48<41:54,  1.33it/s]Training epoch 1:  40%|###9      | 2220/5556 [27:48<41:24,  1.34it/s]Training epoch 1:  40%|###9      | 2221/5556 [27:49<42:04,  1.32it/s]Training epoch 1:  40%|###9      | 2222/5556 [27:50<41:51,  1.33it/s]Training epoch 1:  40%|####      | 2223/5556 [27:51<41:37,  1.33it/s]Training epoch 1:  40%|####      | 2224/5556 [27:51<40:58,  1.36it/s]Training epoch 1:  40%|####      | 2225/5556 [27:52<40:56,  1.36it/s]Training epoch 1:  40%|####      | 2226/5556 [27:53<40:45,  1.36it/s]Training epoch 1:  40%|####      | 2227/5556 [27:54<41:20,  1.34it/s]Training epoch 1:  40%|####      | 2228/5556 [27:54<41:15,  1.34it/s]Training epoch 1:  40%|####      | 2229/5556 [27:55<41:33,  1.33it/s]Training epoch 1:  40%|####      | 2230/5556 [27:56<41:11,  1.35it/s]Training epoch 1:  40%|####      | 2231/5556 [27:57<41:34,  1.33it/s]Training epoch 1:  40%|####      | 2232/5556 [27:57<42:06,  1.32it/s]Training epoch 1:  40%|####      | 2233/5556 [27:58<41:51,  1.32it/s]Training epoch 1:  40%|####      | 2234/5556 [27:59<41:41,  1.33it/s]Training epoch 1:  40%|####      | 2235/5556 [28:00<41:15,  1.34it/s]Training epoch 1:  40%|####      | 2236/5556 [28:00<41:37,  1.33it/s]Training epoch 1:  40%|####      | 2237/5556 [28:01<41:13,  1.34it/s]Training epoch 1:  40%|####      | 2238/5556 [28:02<41:16,  1.34it/s]Training epoch 1:  40%|####      | 2239/5556 [28:03<40:59,  1.35it/s]Training epoch 1:  40%|####      | 2240/5556 [28:03<40:47,  1.35it/s]Training epoch 1:  40%|####      | 2241/5556 [28:04<41:11,  1.34it/s]Training epoch 1:  40%|####      | 2242/5556 [28:05<41:27,  1.33it/s]Training epoch 1:  40%|####      | 2243/5556 [28:05<40:46,  1.35it/s]Training epoch 1:  40%|####      | 2244/5556 [28:06<41:24,  1.33it/s]Training epoch 1:  40%|####      | 2245/5556 [28:07<41:29,  1.33it/s]Training epoch 1:  40%|####      | 2246/5556 [28:08<41:14,  1.34it/s]Training epoch 1:  40%|####      | 2247/5556 [28:08<41:03,  1.34it/s]Training epoch 1:  40%|####      | 2248/5556 [28:09<40:48,  1.35it/s]Training epoch 1:  40%|####      | 2249/5556 [28:10<40:52,  1.35it/s]Training epoch 1:  40%|####      | 2250/5556 [28:11<40:43,  1.35it/s]Training epoch 1:  41%|####      | 2251/5556 [28:11<40:10,  1.37it/s]Training epoch 1:  41%|####      | 2252/5556 [28:12<40:41,  1.35it/s]Training epoch 1:  41%|####      | 2253/5556 [28:13<40:59,  1.34it/s]Training epoch 1:  41%|####      | 2254/5556 [28:14<40:42,  1.35it/s]Training epoch 1:  41%|####      | 2255/5556 [28:14<40:45,  1.35it/s]Training epoch 1:  41%|####      | 2256/5556 [28:15<40:43,  1.35it/s]Training epoch 1:  41%|####      | 2257/5556 [28:16<40:35,  1.35it/s]Training epoch 1:  41%|####      | 2258/5556 [28:17<41:12,  1.33it/s]Training epoch 1:  41%|####      | 2259/5556 [28:17<41:34,  1.32it/s]Training epoch 1:  41%|####      | 2260/5556 [28:18<41:45,  1.32it/s]Training epoch 1:  41%|####      | 2261/5556 [28:19<41:47,  1.31it/s]Training epoch 1:  41%|####      | 2262/5556 [28:20<41:44,  1.32it/s]Training epoch 1:  41%|####      | 2263/5556 [28:20<41:39,  1.32it/s]Training epoch 1:  41%|####      | 2264/5556 [28:21<41:09,  1.33it/s]Training epoch 1:  41%|####      | 2265/5556 [28:22<41:12,  1.33it/s]Training epoch 1:  41%|####      | 2266/5556 [28:23<40:35,  1.35it/s]Training epoch 1:  41%|####      | 2267/5556 [28:23<40:23,  1.36it/s]Training epoch 1:  41%|####      | 2268/5556 [28:24<40:08,  1.36it/s]Training epoch 1:  41%|####      | 2269/5556 [28:25<40:23,  1.36it/s]Training epoch 1:  41%|####      | 2270/5556 [28:26<40:21,  1.36it/s]Training epoch 1:  41%|####      | 2271/5556 [28:26<40:54,  1.34it/s]Training epoch 1:  41%|####      | 2272/5556 [28:27<40:54,  1.34it/s]Training epoch 1:  41%|####      | 2273/5556 [28:28<41:00,  1.33it/s]Training epoch 1:  41%|####      | 2274/5556 [28:29<40:30,  1.35it/s]Training epoch 1:  41%|####      | 2275/5556 [28:29<41:19,  1.32it/s]Training epoch 1:  41%|####      | 2276/5556 [28:30<40:56,  1.34it/s]Training epoch 1:  41%|####      | 2277/5556 [28:31<41:03,  1.33it/s]Training epoch 1:  41%|####1     | 2278/5556 [28:32<40:43,  1.34it/s]Training epoch 1:  41%|####1     | 2279/5556 [28:32<40:45,  1.34it/s]Training epoch 1:  41%|####1     | 2280/5556 [28:33<40:15,  1.36it/s]Training epoch 1:  41%|####1     | 2281/5556 [28:34<41:00,  1.33it/s]Training epoch 1:  41%|####1     | 2282/5556 [28:35<41:27,  1.32it/s]Training epoch 1:  41%|####1     | 2283/5556 [28:35<41:11,  1.32it/s]Training epoch 1:  41%|####1     | 2284/5556 [28:36<41:06,  1.33it/s]Training epoch 1:  41%|####1     | 2285/5556 [28:37<41:28,  1.31it/s]Training epoch 1:  41%|####1     | 2286/5556 [28:38<41:42,  1.31it/s]Training epoch 1:  41%|####1     | 2287/5556 [28:38<41:20,  1.32it/s]Training epoch 1:  41%|####1     | 2288/5556 [28:39<41:32,  1.31it/s]Training epoch 1:  41%|####1     | 2289/5556 [28:40<41:00,  1.33it/s]Training epoch 1:  41%|####1     | 2290/5556 [28:41<40:35,  1.34it/s]Training epoch 1:  41%|####1     | 2291/5556 [28:41<40:29,  1.34it/s]Training epoch 1:  41%|####1     | 2292/5556 [28:42<40:44,  1.34it/s]Training epoch 1:  41%|####1     | 2293/5556 [28:43<41:11,  1.32it/s]Training epoch 1:  41%|####1     | 2294/5556 [28:44<41:13,  1.32it/s]Training epoch 1:  41%|####1     | 2295/5556 [28:44<41:42,  1.30it/s]Training epoch 1:  41%|####1     | 2296/5556 [28:45<41:30,  1.31it/s]Training epoch 1:  41%|####1     | 2297/5556 [28:46<40:54,  1.33it/s]Training epoch 1:  41%|####1     | 2298/5556 [28:47<40:51,  1.33it/s]Training epoch 1:  41%|####1     | 2299/5556 [28:47<40:56,  1.33it/s]Training epoch 1:  41%|####1     | 2300/5556 [28:48<42:20,  1.28it/s]Training epoch 1:  41%|####1     | 2301/5556 [28:49<41:23,  1.31it/s]Training epoch 1:  41%|####1     | 2302/5556 [28:50<40:44,  1.33it/s]Training epoch 1:  41%|####1     | 2303/5556 [28:51<41:04,  1.32it/s]Training epoch 1:  41%|####1     | 2304/5556 [28:51<41:16,  1.31it/s]Training epoch 1:  41%|####1     | 2305/5556 [28:52<40:54,  1.32it/s]Training epoch 1:  42%|####1     | 2306/5556 [28:53<40:43,  1.33it/s]Training epoch 1:  42%|####1     | 2307/5556 [28:54<40:33,  1.34it/s]Training epoch 1:  42%|####1     | 2308/5556 [28:54<40:45,  1.33it/s]Training epoch 1:  42%|####1     | 2309/5556 [28:55<40:31,  1.34it/s]Training epoch 1:  42%|####1     | 2310/5556 [28:56<40:38,  1.33it/s]Training epoch 1:  42%|####1     | 2311/5556 [28:57<40:48,  1.33it/s]Training epoch 1:  42%|####1     | 2312/5556 [28:57<40:46,  1.33it/s]Training epoch 1:  42%|####1     | 2313/5556 [28:58<40:09,  1.35it/s]Training epoch 1:  42%|####1     | 2314/5556 [28:59<40:16,  1.34it/s]Training epoch 1:  42%|####1     | 2315/5556 [28:59<40:10,  1.34it/s]Training epoch 1:  42%|####1     | 2316/5556 [29:00<40:15,  1.34it/s]Training epoch 1:  42%|####1     | 2317/5556 [29:01<40:25,  1.34it/s]Training epoch 1:  42%|####1     | 2318/5556 [29:02<40:27,  1.33it/s]Training epoch 1:  42%|####1     | 2319/5556 [29:02<40:16,  1.34it/s]Training epoch 1:  42%|####1     | 2320/5556 [29:03<40:20,  1.34it/s]Training epoch 1:  42%|####1     | 2321/5556 [29:04<40:23,  1.33it/s]Training epoch 1:  42%|####1     | 2322/5556 [29:05<40:49,  1.32it/s]Training epoch 1:  42%|####1     | 2323/5556 [29:06<40:34,  1.33it/s]Training epoch 1:  42%|####1     | 2324/5556 [29:06<40:54,  1.32it/s]Training epoch 1:  42%|####1     | 2325/5556 [29:07<40:50,  1.32it/s]Training epoch 1:  42%|####1     | 2326/5556 [29:08<39:53,  1.35it/s]Training epoch 1:  42%|####1     | 2327/5556 [29:09<40:11,  1.34it/s]Training epoch 1:  42%|####1     | 2328/5556 [29:09<40:11,  1.34it/s]Training epoch 1:  42%|####1     | 2329/5556 [29:10<39:46,  1.35it/s]Training epoch 1:  42%|####1     | 2330/5556 [29:11<39:44,  1.35it/s]Training epoch 1:  42%|####1     | 2331/5556 [29:12<40:36,  1.32it/s]Training epoch 1:  42%|####1     | 2332/5556 [29:12<40:46,  1.32it/s]Training epoch 1:  42%|####1     | 2333/5556 [29:13<41:06,  1.31it/s]Training epoch 1:  42%|####2     | 2334/5556 [29:14<41:20,  1.30it/s]Training epoch 1:  42%|####2     | 2335/5556 [29:15<41:00,  1.31it/s]Training epoch 1:  42%|####2     | 2336/5556 [29:15<40:29,  1.33it/s]Training epoch 1:  42%|####2     | 2337/5556 [29:16<40:29,  1.32it/s]Training epoch 1:  42%|####2     | 2338/5556 [29:17<40:49,  1.31it/s]Training epoch 1:  42%|####2     | 2339/5556 [29:18<39:57,  1.34it/s]Training epoch 1:  42%|####2     | 2340/5556 [29:18<39:37,  1.35it/s]Training epoch 1:  42%|####2     | 2341/5556 [29:19<39:22,  1.36it/s]Training epoch 1:  42%|####2     | 2342/5556 [29:20<39:56,  1.34it/s]Training epoch 1:  42%|####2     | 2343/5556 [29:21<39:54,  1.34it/s]Training epoch 1:  42%|####2     | 2344/5556 [29:21<39:37,  1.35it/s]Training epoch 1:  42%|####2     | 2345/5556 [29:22<39:46,  1.35it/s]Training epoch 1:  42%|####2     | 2346/5556 [29:23<39:33,  1.35it/s]Training epoch 1:  42%|####2     | 2347/5556 [29:23<39:19,  1.36it/s]Training epoch 1:  42%|####2     | 2348/5556 [29:24<38:55,  1.37it/s]Training epoch 1:  42%|####2     | 2349/5556 [29:25<39:11,  1.36it/s]Training epoch 1:  42%|####2     | 2350/5556 [29:26<38:55,  1.37it/s]Training epoch 1:  42%|####2     | 2351/5556 [29:26<39:03,  1.37it/s]Training epoch 1:  42%|####2     | 2352/5556 [29:27<39:06,  1.37it/s]Training epoch 1:  42%|####2     | 2353/5556 [29:28<39:33,  1.35it/s]Training epoch 1:  42%|####2     | 2354/5556 [29:29<39:34,  1.35it/s]Training epoch 1:  42%|####2     | 2355/5556 [29:29<39:55,  1.34it/s]Training epoch 1:  42%|####2     | 2356/5556 [29:30<39:43,  1.34it/s]Training epoch 1:  42%|####2     | 2357/5556 [29:31<39:21,  1.35it/s]Training epoch 1:  42%|####2     | 2358/5556 [29:32<40:02,  1.33it/s]Training epoch 1:  42%|####2     | 2359/5556 [29:32<39:47,  1.34it/s]Training epoch 1:  42%|####2     | 2360/5556 [29:33<39:37,  1.34it/s]Training epoch 1:  42%|####2     | 2361/5556 [29:34<39:36,  1.34it/s]Training epoch 1:  43%|####2     | 2362/5556 [29:35<39:58,  1.33it/s]Training epoch 1:  43%|####2     | 2363/5556 [29:35<40:00,  1.33it/s]Training epoch 1:  43%|####2     | 2364/5556 [29:36<40:11,  1.32it/s]Training epoch 1:  43%|####2     | 2365/5556 [29:37<39:39,  1.34it/s]Training epoch 1:  43%|####2     | 2366/5556 [29:38<39:56,  1.33it/s]Training epoch 1:  43%|####2     | 2367/5556 [29:38<40:17,  1.32it/s]Training epoch 1:  43%|####2     | 2368/5556 [29:39<40:39,  1.31it/s]Training epoch 1:  43%|####2     | 2369/5556 [29:40<40:25,  1.31it/s]Training epoch 1:  43%|####2     | 2370/5556 [29:41<39:59,  1.33it/s]Training epoch 1:  43%|####2     | 2371/5556 [29:41<39:48,  1.33it/s]Training epoch 1:  43%|####2     | 2372/5556 [29:42<40:05,  1.32it/s]Training epoch 1:  43%|####2     | 2373/5556 [29:43<39:49,  1.33it/s]Training epoch 1:  43%|####2     | 2374/5556 [29:44<39:38,  1.34it/s]Training epoch 1:  43%|####2     | 2375/5556 [29:44<39:43,  1.33it/s]Training epoch 1:  43%|####2     | 2376/5556 [29:45<39:34,  1.34it/s]Training epoch 1:  43%|####2     | 2377/5556 [29:46<39:23,  1.34it/s]Training epoch 1:  43%|####2     | 2378/5556 [29:47<39:17,  1.35it/s]Training epoch 1:  43%|####2     | 2379/5556 [29:47<39:27,  1.34it/s]Training epoch 1:  43%|####2     | 2380/5556 [29:48<38:59,  1.36it/s]Training epoch 1:  43%|####2     | 2381/5556 [29:49<39:03,  1.35it/s]Training epoch 1:  43%|####2     | 2382/5556 [29:50<38:24,  1.38it/s]Training epoch 1:  43%|####2     | 2383/5556 [29:50<39:13,  1.35it/s]Training epoch 1:  43%|####2     | 2384/5556 [29:51<39:20,  1.34it/s]Training epoch 1:  43%|####2     | 2385/5556 [29:52<39:52,  1.33it/s]Training epoch 1:  43%|####2     | 2386/5556 [29:53<39:34,  1.33it/s]Training epoch 1:  43%|####2     | 2387/5556 [29:53<39:16,  1.34it/s]Training epoch 1:  43%|####2     | 2388/5556 [29:54<39:22,  1.34it/s]Training epoch 1:  43%|####2     | 2389/5556 [29:55<39:05,  1.35it/s]Training epoch 1:  43%|####3     | 2390/5556 [29:56<39:23,  1.34it/s]Training epoch 1:  43%|####3     | 2391/5556 [29:56<39:25,  1.34it/s]Training epoch 1:  43%|####3     | 2392/5556 [29:57<39:16,  1.34it/s]Training epoch 1:  43%|####3     | 2393/5556 [29:58<39:27,  1.34it/s]Training epoch 1:  43%|####3     | 2394/5556 [29:59<39:10,  1.35it/s]Training epoch 1:  43%|####3     | 2395/5556 [29:59<39:08,  1.35it/s]Training epoch 1:  43%|####3     | 2396/5556 [30:00<39:34,  1.33it/s]Training epoch 1:  43%|####3     | 2397/5556 [30:01<39:02,  1.35it/s]Training epoch 1:  43%|####3     | 2398/5556 [30:01<39:17,  1.34it/s]Training epoch 1:  43%|####3     | 2399/5556 [30:02<39:33,  1.33it/s]Training epoch 1:  43%|####3     | 2400/5556 [30:03<41:05,  1.28it/s]Training epoch 1:  43%|####3     | 2401/5556 [30:04<40:21,  1.30it/s]Training epoch 1:  43%|####3     | 2402/5556 [30:05<40:01,  1.31it/s]Training epoch 1:  43%|####3     | 2403/5556 [30:05<39:47,  1.32it/s]Training epoch 1:  43%|####3     | 2404/5556 [30:06<39:21,  1.33it/s]Training epoch 1:  43%|####3     | 2405/5556 [30:07<39:08,  1.34it/s]Training epoch 1:  43%|####3     | 2406/5556 [30:08<38:42,  1.36it/s]Training epoch 1:  43%|####3     | 2407/5556 [30:08<38:58,  1.35it/s]Training epoch 1:  43%|####3     | 2408/5556 [30:09<39:03,  1.34it/s]Training epoch 1:  43%|####3     | 2409/5556 [30:10<39:29,  1.33it/s]Training epoch 1:  43%|####3     | 2410/5556 [30:11<39:10,  1.34it/s]Training epoch 1:  43%|####3     | 2411/5556 [30:11<39:34,  1.32it/s]Training epoch 1:  43%|####3     | 2412/5556 [30:12<39:49,  1.32it/s]Training epoch 1:  43%|####3     | 2413/5556 [30:13<39:47,  1.32it/s]Training epoch 1:  43%|####3     | 2414/5556 [30:14<39:54,  1.31it/s]Training epoch 1:  43%|####3     | 2415/5556 [30:14<39:43,  1.32it/s]Training epoch 1:  43%|####3     | 2416/5556 [30:15<39:57,  1.31it/s]Training epoch 1:  44%|####3     | 2417/5556 [30:16<40:04,  1.31it/s]Training epoch 1:  44%|####3     | 2418/5556 [30:17<39:32,  1.32it/s]Training epoch 1:  44%|####3     | 2419/5556 [30:17<39:21,  1.33it/s]Training epoch 1:  44%|####3     | 2420/5556 [30:18<39:11,  1.33it/s]Training epoch 1:  44%|####3     | 2421/5556 [30:19<39:07,  1.34it/s]Training epoch 1:  44%|####3     | 2422/5556 [30:20<39:13,  1.33it/s]Training epoch 1:  44%|####3     | 2423/5556 [30:20<38:35,  1.35it/s]Training epoch 1:  44%|####3     | 2424/5556 [30:21<39:13,  1.33it/s]Training epoch 1:  44%|####3     | 2425/5556 [30:22<39:32,  1.32it/s]Training epoch 1:  44%|####3     | 2426/5556 [30:23<39:31,  1.32it/s]Training epoch 1:  44%|####3     | 2427/5556 [30:23<39:32,  1.32it/s]Training epoch 1:  44%|####3     | 2428/5556 [30:24<39:39,  1.31it/s]Training epoch 1:  44%|####3     | 2429/5556 [30:25<39:22,  1.32it/s]Training epoch 1:  44%|####3     | 2430/5556 [30:26<39:13,  1.33it/s]Training epoch 1:  44%|####3     | 2431/5556 [30:26<39:19,  1.32it/s]Training epoch 1:  44%|####3     | 2432/5556 [30:27<39:16,  1.33it/s]Training epoch 1:  44%|####3     | 2433/5556 [30:28<39:18,  1.32it/s]Training epoch 1:  44%|####3     | 2434/5556 [30:29<39:30,  1.32it/s]Training epoch 1:  44%|####3     | 2435/5556 [30:29<39:22,  1.32it/s]Training epoch 1:  44%|####3     | 2436/5556 [30:30<39:18,  1.32it/s]Training epoch 1:  44%|####3     | 2437/5556 [30:31<39:27,  1.32it/s]Training epoch 1:  44%|####3     | 2438/5556 [30:32<39:31,  1.31it/s]Training epoch 1:  44%|####3     | 2439/5556 [30:32<39:08,  1.33it/s]Training epoch 1:  44%|####3     | 2440/5556 [30:33<39:19,  1.32it/s]Training epoch 1:  44%|####3     | 2441/5556 [30:34<38:53,  1.33it/s]Training epoch 1:  44%|####3     | 2442/5556 [30:35<39:04,  1.33it/s]Training epoch 1:  44%|####3     | 2443/5556 [30:35<39:05,  1.33it/s]Training epoch 1:  44%|####3     | 2444/5556 [30:36<38:49,  1.34it/s]Training epoch 1:  44%|####4     | 2445/5556 [30:37<38:48,  1.34it/s]Training epoch 1:  44%|####4     | 2446/5556 [30:38<39:08,  1.32it/s]Training epoch 1:  44%|####4     | 2447/5556 [30:39<39:07,  1.32it/s]Training epoch 1:  44%|####4     | 2448/5556 [30:39<39:17,  1.32it/s]Training epoch 1:  44%|####4     | 2449/5556 [30:40<39:05,  1.32it/s]Training epoch 1:  44%|####4     | 2450/5556 [30:41<38:51,  1.33it/s]Training epoch 1:  44%|####4     | 2451/5556 [30:42<38:55,  1.33it/s]Training epoch 1:  44%|####4     | 2452/5556 [30:42<39:07,  1.32it/s]Training epoch 1:  44%|####4     | 2453/5556 [30:43<39:01,  1.33it/s]Training epoch 1:  44%|####4     | 2454/5556 [30:44<38:43,  1.33it/s]Training epoch 1:  44%|####4     | 2455/5556 [30:45<39:16,  1.32it/s]Training epoch 1:  44%|####4     | 2456/5556 [30:45<38:51,  1.33it/s]Training epoch 1:  44%|####4     | 2457/5556 [30:46<38:20,  1.35it/s]Training epoch 1:  44%|####4     | 2458/5556 [30:47<38:49,  1.33it/s]Training epoch 1:  44%|####4     | 2459/5556 [30:48<38:44,  1.33it/s]Training epoch 1:  44%|####4     | 2460/5556 [30:48<38:47,  1.33it/s]Training epoch 1:  44%|####4     | 2461/5556 [30:49<38:26,  1.34it/s]Training epoch 1:  44%|####4     | 2462/5556 [30:50<38:15,  1.35it/s]Training epoch 1:  44%|####4     | 2463/5556 [30:50<37:56,  1.36it/s]Training epoch 1:  44%|####4     | 2464/5556 [30:51<38:03,  1.35it/s]Training epoch 1:  44%|####4     | 2465/5556 [30:52<37:48,  1.36it/s]Training epoch 1:  44%|####4     | 2466/5556 [30:53<37:59,  1.36it/s]Training epoch 1:  44%|####4     | 2467/5556 [30:53<38:06,  1.35it/s]Training epoch 1:  44%|####4     | 2468/5556 [30:54<38:48,  1.33it/s]Training epoch 1:  44%|####4     | 2469/5556 [30:55<38:36,  1.33it/s]Training epoch 1:  44%|####4     | 2470/5556 [30:56<38:21,  1.34it/s]Training epoch 1:  44%|####4     | 2471/5556 [30:56<37:49,  1.36it/s]Training epoch 1:  44%|####4     | 2472/5556 [30:57<37:49,  1.36it/s]Training epoch 1:  45%|####4     | 2473/5556 [30:58<37:24,  1.37it/s]Training epoch 1:  45%|####4     | 2474/5556 [30:59<37:16,  1.38it/s]Training epoch 1:  45%|####4     | 2475/5556 [30:59<37:03,  1.39it/s]Training epoch 1:  45%|####4     | 2476/5556 [31:00<37:40,  1.36it/s]Training epoch 1:  45%|####4     | 2477/5556 [31:01<38:02,  1.35it/s]Training epoch 1:  45%|####4     | 2478/5556 [31:02<38:32,  1.33it/s]Training epoch 1:  45%|####4     | 2479/5556 [31:02<39:10,  1.31it/s]Training epoch 1:  45%|####4     | 2480/5556 [31:03<38:51,  1.32it/s]Training epoch 1:  45%|####4     | 2481/5556 [31:04<38:23,  1.33it/s]Training epoch 1:  45%|####4     | 2482/5556 [31:05<38:38,  1.33it/s]Training epoch 1:  45%|####4     | 2483/5556 [31:05<38:34,  1.33it/s]Training epoch 1:  45%|####4     | 2484/5556 [31:06<38:43,  1.32it/s]Training epoch 1:  45%|####4     | 2485/5556 [31:07<38:52,  1.32it/s]Training epoch 1:  45%|####4     | 2486/5556 [31:08<38:26,  1.33it/s]Training epoch 1:  45%|####4     | 2487/5556 [31:08<38:25,  1.33it/s]Training epoch 1:  45%|####4     | 2488/5556 [31:09<38:29,  1.33it/s]Training epoch 1:  45%|####4     | 2489/5556 [31:10<38:38,  1.32it/s]Training epoch 1:  45%|####4     | 2490/5556 [31:11<38:50,  1.32it/s]Training epoch 1:  45%|####4     | 2491/5556 [31:11<38:22,  1.33it/s]Training epoch 1:  45%|####4     | 2492/5556 [31:12<37:51,  1.35it/s]Training epoch 1:  45%|####4     | 2493/5556 [31:13<38:11,  1.34it/s]Training epoch 1:  45%|####4     | 2494/5556 [31:14<38:35,  1.32it/s]Training epoch 1:  45%|####4     | 2495/5556 [31:14<39:15,  1.30it/s]Training epoch 1:  45%|####4     | 2496/5556 [31:15<39:09,  1.30it/s]Training epoch 1:  45%|####4     | 2497/5556 [31:16<38:40,  1.32it/s]Training epoch 1:  45%|####4     | 2498/5556 [31:17<39:01,  1.31it/s]Training epoch 1:  45%|####4     | 2499/5556 [31:17<38:56,  1.31it/s]Training epoch 1:  45%|####4     | 2500/5556 [31:18<40:38,  1.25it/s]Training epoch 1:  45%|####5     | 2501/5556 [31:19<39:52,  1.28it/s]Training epoch 1:  45%|####5     | 2502/5556 [31:20<39:11,  1.30it/s]Training epoch 1:  45%|####5     | 2503/5556 [31:21<38:52,  1.31it/s]Training epoch 1:  45%|####5     | 2504/5556 [31:21<37:44,  1.35it/s]Training epoch 1:  45%|####5     | 2505/5556 [31:22<38:05,  1.33it/s]Training epoch 1:  45%|####5     | 2506/5556 [31:23<37:59,  1.34it/s]Training epoch 1:  45%|####5     | 2507/5556 [31:24<38:05,  1.33it/s]Training epoch 1:  45%|####5     | 2508/5556 [31:24<38:35,  1.32it/s]Training epoch 1:  45%|####5     | 2509/5556 [31:25<38:25,  1.32it/s]Training epoch 1:  45%|####5     | 2510/5556 [31:26<38:09,  1.33it/s]Training epoch 1:  45%|####5     | 2511/5556 [31:27<38:12,  1.33it/s]Training epoch 1:  45%|####5     | 2512/5556 [31:27<38:29,  1.32it/s]Training epoch 1:  45%|####5     | 2513/5556 [31:28<37:49,  1.34it/s]Training epoch 1:  45%|####5     | 2514/5556 [31:29<37:53,  1.34it/s]Training epoch 1:  45%|####5     | 2515/5556 [31:30<38:14,  1.33it/s]Training epoch 1:  45%|####5     | 2516/5556 [31:30<38:30,  1.32it/s]Training epoch 1:  45%|####5     | 2517/5556 [31:31<37:55,  1.34it/s]Training epoch 1:  45%|####5     | 2518/5556 [31:32<37:25,  1.35it/s]Training epoch 1:  45%|####5     | 2519/5556 [31:33<37:34,  1.35it/s]Training epoch 1:  45%|####5     | 2520/5556 [31:33<37:53,  1.34it/s]Training epoch 1:  45%|####5     | 2521/5556 [31:34<38:07,  1.33it/s]Training epoch 1:  45%|####5     | 2522/5556 [31:35<37:59,  1.33it/s]Training epoch 1:  45%|####5     | 2523/5556 [31:36<38:06,  1.33it/s]Training epoch 1:  45%|####5     | 2524/5556 [31:36<37:38,  1.34it/s]Training epoch 1:  45%|####5     | 2525/5556 [31:37<37:29,  1.35it/s]Training epoch 1:  45%|####5     | 2526/5556 [31:38<37:50,  1.33it/s]Training epoch 1:  45%|####5     | 2527/5556 [31:39<37:40,  1.34it/s]Training epoch 1:  46%|####5     | 2528/5556 [31:39<37:39,  1.34it/s]Training epoch 1:  46%|####5     | 2529/5556 [31:40<37:25,  1.35it/s]Training epoch 1:  46%|####5     | 2530/5556 [31:41<37:48,  1.33it/s]Training epoch 1:  46%|####5     | 2531/5556 [31:42<38:06,  1.32it/s]Training epoch 1:  46%|####5     | 2532/5556 [31:42<37:35,  1.34it/s]Training epoch 1:  46%|####5     | 2533/5556 [31:43<37:42,  1.34it/s]Training epoch 1:  46%|####5     | 2534/5556 [31:44<38:01,  1.32it/s]Training epoch 1:  46%|####5     | 2535/5556 [31:45<38:07,  1.32it/s]Training epoch 1:  46%|####5     | 2536/5556 [31:45<38:33,  1.31it/s]Training epoch 1:  46%|####5     | 2537/5556 [31:46<39:05,  1.29it/s]Training epoch 1:  46%|####5     | 2538/5556 [31:47<38:33,  1.30it/s]Training epoch 1:  46%|####5     | 2539/5556 [31:48<38:19,  1.31it/s]Training epoch 1:  46%|####5     | 2540/5556 [31:48<38:18,  1.31it/s]Training epoch 1:  46%|####5     | 2541/5556 [31:49<38:15,  1.31it/s]Training epoch 1:  46%|####5     | 2542/5556 [31:50<38:05,  1.32it/s]Training epoch 1:  46%|####5     | 2543/5556 [31:51<38:14,  1.31it/s]Training epoch 1:  46%|####5     | 2544/5556 [31:51<38:02,  1.32it/s]Training epoch 1:  46%|####5     | 2545/5556 [31:52<37:47,  1.33it/s]Training epoch 1:  46%|####5     | 2546/5556 [31:53<38:21,  1.31it/s]Training epoch 1:  46%|####5     | 2547/5556 [31:54<38:26,  1.30it/s]Training epoch 1:  46%|####5     | 2548/5556 [31:55<38:20,  1.31it/s]Training epoch 1:  46%|####5     | 2549/5556 [31:55<37:56,  1.32it/s]Training epoch 1:  46%|####5     | 2550/5556 [31:56<37:36,  1.33it/s]Training epoch 1:  46%|####5     | 2551/5556 [31:57<37:37,  1.33it/s]Training epoch 1:  46%|####5     | 2552/5556 [31:58<37:39,  1.33it/s]Training epoch 1:  46%|####5     | 2553/5556 [31:58<37:40,  1.33it/s]Training epoch 1:  46%|####5     | 2554/5556 [31:59<37:55,  1.32it/s]Training epoch 1:  46%|####5     | 2555/5556 [32:00<37:34,  1.33it/s]Training epoch 1:  46%|####6     | 2556/5556 [32:00<37:14,  1.34it/s]Training epoch 1:  46%|####6     | 2557/5556 [32:01<36:53,  1.35it/s]Training epoch 1:  46%|####6     | 2558/5556 [32:02<36:27,  1.37it/s]Training epoch 1:  46%|####6     | 2559/5556 [32:03<36:48,  1.36it/s]Training epoch 1:  46%|####6     | 2560/5556 [32:03<36:40,  1.36it/s]Training epoch 1:  46%|####6     | 2561/5556 [32:04<36:50,  1.36it/s]Training epoch 1:  46%|####6     | 2562/5556 [32:05<37:04,  1.35it/s]Training epoch 1:  46%|####6     | 2563/5556 [32:06<37:03,  1.35it/s]Training epoch 1:  46%|####6     | 2564/5556 [32:06<37:06,  1.34it/s]Training epoch 1:  46%|####6     | 2565/5556 [32:07<36:39,  1.36it/s]Training epoch 1:  46%|####6     | 2566/5556 [32:08<37:06,  1.34it/s]Training epoch 1:  46%|####6     | 2567/5556 [32:09<37:37,  1.32it/s]Training epoch 1:  46%|####6     | 2568/5556 [32:09<37:26,  1.33it/s]Training epoch 1:  46%|####6     | 2569/5556 [32:10<36:52,  1.35it/s]Training epoch 1:  46%|####6     | 2570/5556 [32:11<36:54,  1.35it/s]Training epoch 1:  46%|####6     | 2571/5556 [32:12<36:50,  1.35it/s]Training epoch 1:  46%|####6     | 2572/5556 [32:12<37:01,  1.34it/s]Training epoch 1:  46%|####6     | 2573/5556 [32:13<37:27,  1.33it/s]Training epoch 1:  46%|####6     | 2574/5556 [32:14<36:55,  1.35it/s]Training epoch 1:  46%|####6     | 2575/5556 [32:15<37:15,  1.33it/s]Training epoch 1:  46%|####6     | 2576/5556 [32:15<38:02,  1.31it/s]Training epoch 1:  46%|####6     | 2577/5556 [32:16<37:42,  1.32it/s]Training epoch 1:  46%|####6     | 2578/5556 [32:17<37:13,  1.33it/s]Training epoch 1:  46%|####6     | 2579/5556 [32:18<36:53,  1.34it/s]Training epoch 1:  46%|####6     | 2580/5556 [32:18<36:56,  1.34it/s]Training epoch 1:  46%|####6     | 2581/5556 [32:19<37:27,  1.32it/s]Training epoch 1:  46%|####6     | 2582/5556 [32:20<37:30,  1.32it/s]Training epoch 1:  46%|####6     | 2583/5556 [32:21<37:00,  1.34it/s]Training epoch 1:  47%|####6     | 2584/5556 [32:21<37:07,  1.33it/s]Training epoch 1:  47%|####6     | 2585/5556 [32:22<36:47,  1.35it/s]Training epoch 1:  47%|####6     | 2586/5556 [32:23<36:37,  1.35it/s]Training epoch 1:  47%|####6     | 2587/5556 [32:24<37:04,  1.33it/s]Training epoch 1:  47%|####6     | 2588/5556 [32:24<37:31,  1.32it/s]Training epoch 1:  47%|####6     | 2589/5556 [32:25<37:35,  1.32it/s]Training epoch 1:  47%|####6     | 2590/5556 [32:26<36:44,  1.35it/s]Training epoch 1:  47%|####6     | 2591/5556 [32:27<36:55,  1.34it/s]Training epoch 1:  47%|####6     | 2592/5556 [32:27<36:47,  1.34it/s]Training epoch 1:  47%|####6     | 2593/5556 [32:28<36:45,  1.34it/s]Training epoch 1:  47%|####6     | 2594/5556 [32:29<36:53,  1.34it/s]Training epoch 1:  47%|####6     | 2595/5556 [32:30<36:43,  1.34it/s]Training epoch 1:  47%|####6     | 2596/5556 [32:30<36:38,  1.35it/s]Training epoch 1:  47%|####6     | 2597/5556 [32:31<36:48,  1.34it/s]Training epoch 1:  47%|####6     | 2598/5556 [32:32<36:34,  1.35it/s]Training epoch 1:  47%|####6     | 2599/5556 [32:33<36:12,  1.36it/s]Training epoch 1:  47%|####6     | 2600/5556 [32:33<38:18,  1.29it/s]Training epoch 1:  47%|####6     | 2601/5556 [32:34<38:17,  1.29it/s]Training epoch 1:  47%|####6     | 2602/5556 [32:35<37:41,  1.31it/s]Training epoch 1:  47%|####6     | 2603/5556 [32:36<37:25,  1.31it/s]Training epoch 1:  47%|####6     | 2604/5556 [32:36<37:13,  1.32it/s]Training epoch 1:  47%|####6     | 2605/5556 [32:37<36:44,  1.34it/s]Training epoch 1:  47%|####6     | 2606/5556 [32:38<36:58,  1.33it/s]Training epoch 1:  47%|####6     | 2607/5556 [32:39<37:19,  1.32it/s]Training epoch 1:  47%|####6     | 2608/5556 [32:39<37:19,  1.32it/s]Training epoch 1:  47%|####6     | 2609/5556 [32:40<37:19,  1.32it/s]Training epoch 1:  47%|####6     | 2610/5556 [32:41<37:19,  1.32it/s]Training epoch 1:  47%|####6     | 2611/5556 [32:42<36:56,  1.33it/s]Training epoch 1:  47%|####7     | 2612/5556 [32:42<36:49,  1.33it/s]Training epoch 1:  47%|####7     | 2613/5556 [32:43<36:29,  1.34it/s]Training epoch 1:  47%|####7     | 2614/5556 [32:44<36:21,  1.35it/s]Training epoch 1:  47%|####7     | 2615/5556 [32:45<36:38,  1.34it/s]Training epoch 1:  47%|####7     | 2616/5556 [32:45<36:17,  1.35it/s]Training epoch 1:  47%|####7     | 2617/5556 [32:46<36:33,  1.34it/s]Training epoch 1:  47%|####7     | 2618/5556 [32:47<36:54,  1.33it/s]Training epoch 1:  47%|####7     | 2619/5556 [32:48<36:35,  1.34it/s]Training epoch 1:  47%|####7     | 2620/5556 [32:48<36:33,  1.34it/s]Training epoch 1:  47%|####7     | 2621/5556 [32:49<36:59,  1.32it/s]Training epoch 1:  47%|####7     | 2622/5556 [32:50<36:46,  1.33it/s]Training epoch 1:  47%|####7     | 2623/5556 [32:51<36:17,  1.35it/s]Training epoch 1:  47%|####7     | 2624/5556 [32:51<36:23,  1.34it/s]Training epoch 1:  47%|####7     | 2625/5556 [32:52<36:02,  1.36it/s]Training epoch 1:  47%|####7     | 2626/5556 [32:53<36:13,  1.35it/s]Training epoch 1:  47%|####7     | 2627/5556 [32:54<36:32,  1.34it/s]Training epoch 1:  47%|####7     | 2628/5556 [32:54<36:06,  1.35it/s]Training epoch 1:  47%|####7     | 2629/5556 [32:55<36:10,  1.35it/s]Training epoch 1:  47%|####7     | 2630/5556 [32:56<36:09,  1.35it/s]Training epoch 1:  47%|####7     | 2631/5556 [32:57<35:50,  1.36it/s]Training epoch 1:  47%|####7     | 2632/5556 [32:57<36:20,  1.34it/s]Training epoch 1:  47%|####7     | 2633/5556 [32:58<36:31,  1.33it/s]Training epoch 1:  47%|####7     | 2634/5556 [32:59<35:54,  1.36it/s]Training epoch 1:  47%|####7     | 2635/5556 [33:00<36:18,  1.34it/s]Training epoch 1:  47%|####7     | 2636/5556 [33:00<36:33,  1.33it/s]Training epoch 1:  47%|####7     | 2637/5556 [33:01<36:05,  1.35it/s]Training epoch 1:  47%|####7     | 2638/5556 [33:02<36:28,  1.33it/s]Training epoch 1:  47%|####7     | 2639/5556 [33:03<36:00,  1.35it/s]Training epoch 1:  48%|####7     | 2640/5556 [33:03<36:03,  1.35it/s]Training epoch 1:  48%|####7     | 2641/5556 [33:04<36:03,  1.35it/s]Training epoch 1:  48%|####7     | 2642/5556 [33:05<36:12,  1.34it/s]Training epoch 1:  48%|####7     | 2643/5556 [33:06<35:58,  1.35it/s]Training epoch 1:  48%|####7     | 2644/5556 [33:06<35:54,  1.35it/s]Training epoch 1:  48%|####7     | 2645/5556 [33:07<36:16,  1.34it/s]Training epoch 1:  48%|####7     | 2646/5556 [33:08<36:21,  1.33it/s]Training epoch 1:  48%|####7     | 2647/5556 [33:09<36:03,  1.34it/s]Training epoch 1:  48%|####7     | 2648/5556 [33:09<36:37,  1.32it/s]Training epoch 1:  48%|####7     | 2649/5556 [33:10<36:43,  1.32it/s]Training epoch 1:  48%|####7     | 2650/5556 [33:11<36:26,  1.33it/s]Training epoch 1:  48%|####7     | 2651/5556 [33:12<35:52,  1.35it/s]Training epoch 1:  48%|####7     | 2652/5556 [33:12<36:05,  1.34it/s]Training epoch 1:  48%|####7     | 2653/5556 [33:13<36:05,  1.34it/s]Training epoch 1:  48%|####7     | 2654/5556 [33:14<36:04,  1.34it/s]Training epoch 1:  48%|####7     | 2655/5556 [33:14<35:55,  1.35it/s]Training epoch 1:  48%|####7     | 2656/5556 [33:15<36:04,  1.34it/s]Training epoch 1:  48%|####7     | 2657/5556 [33:16<36:18,  1.33it/s]Training epoch 1:  48%|####7     | 2658/5556 [33:17<36:17,  1.33it/s]Training epoch 1:  48%|####7     | 2659/5556 [33:18<36:23,  1.33it/s]Training epoch 1:  48%|####7     | 2660/5556 [33:18<36:42,  1.32it/s]Training epoch 1:  48%|####7     | 2661/5556 [33:19<36:49,  1.31it/s]Training epoch 1:  48%|####7     | 2662/5556 [33:20<36:44,  1.31it/s]Training epoch 1:  48%|####7     | 2663/5556 [33:21<36:45,  1.31it/s]Training epoch 1:  48%|####7     | 2664/5556 [33:21<36:28,  1.32it/s]Training epoch 1:  48%|####7     | 2665/5556 [33:22<36:37,  1.32it/s]Training epoch 1:  48%|####7     | 2666/5556 [33:23<36:32,  1.32it/s]Training epoch 1:  48%|####8     | 2667/5556 [33:24<36:37,  1.31it/s]Training epoch 1:  48%|####8     | 2668/5556 [33:24<36:29,  1.32it/s]Training epoch 1:  48%|####8     | 2669/5556 [33:25<36:05,  1.33it/s]Training epoch 1:  48%|####8     | 2670/5556 [33:26<36:19,  1.32it/s]Training epoch 1:  48%|####8     | 2671/5556 [33:27<35:52,  1.34it/s]Training epoch 1:  48%|####8     | 2672/5556 [33:27<36:08,  1.33it/s]Training epoch 1:  48%|####8     | 2673/5556 [33:28<36:01,  1.33it/s]Training epoch 1:  48%|####8     | 2674/5556 [33:29<35:50,  1.34it/s]Training epoch 1:  48%|####8     | 2675/5556 [33:30<36:00,  1.33it/s]Training epoch 1:  48%|####8     | 2676/5556 [33:30<36:00,  1.33it/s]Training epoch 1:  48%|####8     | 2677/5556 [33:31<35:36,  1.35it/s]Training epoch 1:  48%|####8     | 2678/5556 [33:32<35:27,  1.35it/s]Training epoch 1:  48%|####8     | 2679/5556 [33:33<36:05,  1.33it/s]Training epoch 1:  48%|####8     | 2680/5556 [33:33<36:00,  1.33it/s]Training epoch 1:  48%|####8     | 2681/5556 [33:34<36:03,  1.33it/s]Training epoch 1:  48%|####8     | 2682/5556 [33:35<35:54,  1.33it/s]Training epoch 1:  48%|####8     | 2683/5556 [33:36<35:52,  1.33it/s]Training epoch 1:  48%|####8     | 2684/5556 [33:36<35:38,  1.34it/s]Training epoch 1:  48%|####8     | 2685/5556 [33:37<35:23,  1.35it/s]Training epoch 1:  48%|####8     | 2686/5556 [33:38<35:24,  1.35it/s]Training epoch 1:  48%|####8     | 2687/5556 [33:39<34:58,  1.37it/s]Training epoch 1:  48%|####8     | 2688/5556 [33:39<35:07,  1.36it/s]Training epoch 1:  48%|####8     | 2689/5556 [33:40<35:48,  1.33it/s]Training epoch 1:  48%|####8     | 2690/5556 [33:41<35:59,  1.33it/s]Training epoch 1:  48%|####8     | 2691/5556 [33:42<36:01,  1.33it/s]Training epoch 1:  48%|####8     | 2692/5556 [33:42<36:12,  1.32it/s]Training epoch 1:  48%|####8     | 2693/5556 [33:43<36:10,  1.32it/s]Training epoch 1:  48%|####8     | 2694/5556 [33:44<35:59,  1.33it/s]Training epoch 1:  49%|####8     | 2695/5556 [33:45<36:15,  1.32it/s]Training epoch 1:  49%|####8     | 2696/5556 [33:45<36:13,  1.32it/s]Training epoch 1:  49%|####8     | 2697/5556 [33:46<36:08,  1.32it/s]Training epoch 1:  49%|####8     | 2698/5556 [33:47<35:35,  1.34it/s]Training epoch 1:  49%|####8     | 2699/5556 [33:48<35:48,  1.33it/s]Training epoch 1:  49%|####8     | 2700/5556 [33:48<36:56,  1.29it/s]Training epoch 1:  49%|####8     | 2701/5556 [33:49<36:40,  1.30it/s]Training epoch 1:  49%|####8     | 2702/5556 [33:50<36:48,  1.29it/s]Training epoch 1:  49%|####8     | 2703/5556 [33:51<36:20,  1.31it/s]Training epoch 1:  49%|####8     | 2704/5556 [33:51<36:07,  1.32it/s]Training epoch 1:  49%|####8     | 2705/5556 [33:52<35:45,  1.33it/s]Training epoch 1:  49%|####8     | 2706/5556 [33:53<35:31,  1.34it/s]Training epoch 1:  49%|####8     | 2707/5556 [33:54<35:46,  1.33it/s]Training epoch 1:  49%|####8     | 2708/5556 [33:54<35:48,  1.33it/s]Training epoch 1:  49%|####8     | 2709/5556 [33:55<35:40,  1.33it/s]Training epoch 1:  49%|####8     | 2710/5556 [33:56<35:15,  1.35it/s]Training epoch 1:  49%|####8     | 2711/5556 [33:57<35:20,  1.34it/s]Training epoch 1:  49%|####8     | 2712/5556 [33:57<35:48,  1.32it/s]Training epoch 1:  49%|####8     | 2713/5556 [33:58<35:39,  1.33it/s]Training epoch 1:  49%|####8     | 2714/5556 [33:59<36:08,  1.31it/s]Training epoch 1:  49%|####8     | 2715/5556 [34:00<36:16,  1.31it/s]Training epoch 1:  49%|####8     | 2716/5556 [34:01<36:10,  1.31it/s]Training epoch 1:  49%|####8     | 2717/5556 [34:01<36:14,  1.31it/s]Training epoch 1:  49%|####8     | 2718/5556 [34:02<35:41,  1.33it/s]Training epoch 1:  49%|####8     | 2719/5556 [34:03<35:45,  1.32it/s]Training epoch 1:  49%|####8     | 2720/5556 [34:04<35:19,  1.34it/s]Training epoch 1:  49%|####8     | 2721/5556 [34:04<35:23,  1.34it/s]Training epoch 1:  49%|####8     | 2722/5556 [34:05<35:43,  1.32it/s]Training epoch 1:  49%|####9     | 2723/5556 [34:06<35:10,  1.34it/s]Training epoch 1:  49%|####9     | 2724/5556 [34:07<35:35,  1.33it/s]Training epoch 1:  49%|####9     | 2725/5556 [34:07<35:24,  1.33it/s]Training epoch 1:  49%|####9     | 2726/5556 [34:08<35:36,  1.32it/s]Training epoch 1:  49%|####9     | 2727/5556 [34:09<35:47,  1.32it/s]Training epoch 1:  49%|####9     | 2728/5556 [34:10<35:34,  1.32it/s]Training epoch 1:  49%|####9     | 2729/5556 [34:10<35:41,  1.32it/s]Training epoch 1:  49%|####9     | 2730/5556 [34:11<35:27,  1.33it/s]Training epoch 1:  49%|####9     | 2731/5556 [34:12<35:21,  1.33it/s]Training epoch 1:  49%|####9     | 2732/5556 [34:13<35:23,  1.33it/s]Training epoch 1:  49%|####9     | 2733/5556 [34:13<35:44,  1.32it/s]Training epoch 1:  49%|####9     | 2734/5556 [34:14<35:29,  1.33it/s]Training epoch 1:  49%|####9     | 2735/5556 [34:15<35:15,  1.33it/s]Training epoch 1:  49%|####9     | 2736/5556 [34:16<35:36,  1.32it/s]Training epoch 1:  49%|####9     | 2737/5556 [34:16<35:22,  1.33it/s]Training epoch 1:  49%|####9     | 2738/5556 [34:17<35:35,  1.32it/s]Training epoch 1:  49%|####9     | 2739/5556 [34:18<35:28,  1.32it/s]Training epoch 1:  49%|####9     | 2740/5556 [34:19<35:15,  1.33it/s]Training epoch 1:  49%|####9     | 2741/5556 [34:19<34:59,  1.34it/s]Training epoch 1:  49%|####9     | 2742/5556 [34:20<35:11,  1.33it/s]Training epoch 1:  49%|####9     | 2743/5556 [34:21<35:28,  1.32it/s]Training epoch 1:  49%|####9     | 2744/5556 [34:22<35:25,  1.32it/s]Training epoch 1:  49%|####9     | 2745/5556 [34:22<35:20,  1.33it/s]Training epoch 1:  49%|####9     | 2746/5556 [34:23<34:56,  1.34it/s]Training epoch 1:  49%|####9     | 2747/5556 [34:24<34:59,  1.34it/s]Training epoch 1:  49%|####9     | 2748/5556 [34:25<34:32,  1.35it/s]Training epoch 1:  49%|####9     | 2749/5556 [34:25<34:38,  1.35it/s]Training epoch 1:  49%|####9     | 2750/5556 [34:26<34:21,  1.36it/s]Training epoch 1:  50%|####9     | 2751/5556 [34:27<34:34,  1.35it/s]Training epoch 1:  50%|####9     | 2752/5556 [34:28<34:32,  1.35it/s]Training epoch 1:  50%|####9     | 2753/5556 [34:28<34:35,  1.35it/s]Training epoch 1:  50%|####9     | 2754/5556 [34:29<35:04,  1.33it/s]Training epoch 1:  50%|####9     | 2755/5556 [34:30<35:06,  1.33it/s]Training epoch 1:  50%|####9     | 2756/5556 [34:31<35:25,  1.32it/s]Training epoch 1:  50%|####9     | 2757/5556 [34:31<34:41,  1.34it/s]Training epoch 1:  50%|####9     | 2758/5556 [34:32<34:39,  1.35it/s]Training epoch 1:  50%|####9     | 2759/5556 [34:33<34:21,  1.36it/s]Training epoch 1:  50%|####9     | 2760/5556 [34:33<34:01,  1.37it/s]Training epoch 1:  50%|####9     | 2761/5556 [34:34<33:54,  1.37it/s]Training epoch 1:  50%|####9     | 2762/5556 [34:35<34:08,  1.36it/s]Training epoch 1:  50%|####9     | 2763/5556 [34:36<34:37,  1.34it/s]Training epoch 1:  50%|####9     | 2764/5556 [34:36<34:41,  1.34it/s]Training epoch 1:  50%|####9     | 2765/5556 [34:37<35:05,  1.33it/s]Training epoch 1:  50%|####9     | 2766/5556 [34:38<35:22,  1.31it/s]Training epoch 1:  50%|####9     | 2767/5556 [34:39<34:02,  1.37it/s]Training epoch 1:  50%|####9     | 2768/5556 [34:39<34:23,  1.35it/s]Training epoch 1:  50%|####9     | 2769/5556 [34:40<34:54,  1.33it/s]Training epoch 1:  50%|####9     | 2770/5556 [34:41<34:34,  1.34it/s]Training epoch 1:  50%|####9     | 2771/5556 [34:42<35:14,  1.32it/s]Training epoch 1:  50%|####9     | 2772/5556 [34:42<35:01,  1.32it/s]Training epoch 1:  50%|####9     | 2773/5556 [34:43<35:25,  1.31it/s]Training epoch 1:  50%|####9     | 2774/5556 [34:44<35:30,  1.31it/s]Training epoch 1:  50%|####9     | 2775/5556 [34:45<35:34,  1.30it/s]Training epoch 1:  50%|####9     | 2776/5556 [34:46<35:17,  1.31it/s]Training epoch 1:  50%|####9     | 2777/5556 [34:46<34:48,  1.33it/s]Training epoch 1:  50%|#####     | 2778/5556 [34:47<34:24,  1.35it/s]Training epoch 1:  50%|#####     | 2779/5556 [34:48<34:14,  1.35it/s]Training epoch 1:  50%|#####     | 2780/5556 [34:48<34:28,  1.34it/s]Training epoch 1:  50%|#####     | 2781/5556 [34:49<34:39,  1.33it/s]Training epoch 1:  50%|#####     | 2782/5556 [34:50<34:19,  1.35it/s]Training epoch 1:  50%|#####     | 2783/5556 [34:51<34:39,  1.33it/s]Training epoch 1:  50%|#####     | 2784/5556 [34:51<34:29,  1.34it/s]Training epoch 1:  50%|#####     | 2785/5556 [34:52<33:58,  1.36it/s]Training epoch 1:  50%|#####     | 2786/5556 [34:53<34:12,  1.35it/s]Training epoch 1:  50%|#####     | 2787/5556 [34:54<34:25,  1.34it/s]Training epoch 1:  50%|#####     | 2788/5556 [34:54<34:44,  1.33it/s]Training epoch 1:  50%|#####     | 2789/5556 [34:55<34:37,  1.33it/s]Training epoch 1:  50%|#####     | 2790/5556 [34:56<34:26,  1.34it/s]Training epoch 1:  50%|#####     | 2791/5556 [34:57<34:15,  1.34it/s]Training epoch 1:  50%|#####     | 2792/5556 [34:57<34:11,  1.35it/s]Training epoch 1:  50%|#####     | 2793/5556 [34:58<34:46,  1.32it/s]Training epoch 1:  50%|#####     | 2794/5556 [34:59<34:50,  1.32it/s]Training epoch 1:  50%|#####     | 2795/5556 [35:00<34:49,  1.32it/s]Training epoch 1:  50%|#####     | 2796/5556 [35:00<34:44,  1.32it/s]Training epoch 1:  50%|#####     | 2797/5556 [35:01<34:27,  1.33it/s]Training epoch 1:  50%|#####     | 2798/5556 [35:02<34:11,  1.34it/s]Training epoch 1:  50%|#####     | 2799/5556 [35:03<34:15,  1.34it/s]Training epoch 1:  50%|#####     | 2800/5556 [35:04<35:55,  1.28it/s]Training epoch 1:  50%|#####     | 2801/5556 [35:04<35:42,  1.29it/s]Training epoch 1:  50%|#####     | 2802/5556 [35:05<35:19,  1.30it/s]Training epoch 1:  50%|#####     | 2803/5556 [35:06<35:11,  1.30it/s]Training epoch 1:  50%|#####     | 2804/5556 [35:07<35:03,  1.31it/s]Training epoch 1:  50%|#####     | 2805/5556 [35:07<34:54,  1.31it/s]Training epoch 1:  51%|#####     | 2806/5556 [35:08<34:38,  1.32it/s]Training epoch 1:  51%|#####     | 2807/5556 [35:09<34:30,  1.33it/s]Training epoch 1:  51%|#####     | 2808/5556 [35:10<34:26,  1.33it/s]Training epoch 1:  51%|#####     | 2809/5556 [35:10<34:38,  1.32it/s]Training epoch 1:  51%|#####     | 2810/5556 [35:11<34:14,  1.34it/s]Training epoch 1:  51%|#####     | 2811/5556 [35:12<34:24,  1.33it/s]Training epoch 1:  51%|#####     | 2812/5556 [35:13<34:16,  1.33it/s]Training epoch 1:  51%|#####     | 2813/5556 [35:13<34:16,  1.33it/s]Training epoch 1:  51%|#####     | 2814/5556 [35:14<34:24,  1.33it/s]Training epoch 1:  51%|#####     | 2815/5556 [35:15<34:30,  1.32it/s]Training epoch 1:  51%|#####     | 2816/5556 [35:16<34:11,  1.34it/s]Training epoch 1:  51%|#####     | 2817/5556 [35:16<34:32,  1.32it/s]Training epoch 1:  51%|#####     | 2818/5556 [35:17<34:32,  1.32it/s]Training epoch 1:  51%|#####     | 2819/5556 [35:18<34:24,  1.33it/s]Training epoch 1:  51%|#####     | 2820/5556 [35:19<34:20,  1.33it/s]Training epoch 1:  51%|#####     | 2821/5556 [35:19<34:32,  1.32it/s]Training epoch 1:  51%|#####     | 2822/5556 [35:20<34:22,  1.33it/s]Training epoch 1:  51%|#####     | 2823/5556 [35:21<34:41,  1.31it/s]Training epoch 1:  51%|#####     | 2824/5556 [35:22<34:43,  1.31it/s]Training epoch 1:  51%|#####     | 2825/5556 [35:22<34:45,  1.31it/s]Training epoch 1:  51%|#####     | 2826/5556 [35:23<35:08,  1.29it/s]Training epoch 1:  51%|#####     | 2827/5556 [35:24<35:13,  1.29it/s]Training epoch 1:  51%|#####     | 2828/5556 [35:25<34:47,  1.31it/s]Training epoch 1:  51%|#####     | 2829/5556 [35:26<34:56,  1.30it/s]Training epoch 1:  51%|#####     | 2830/5556 [35:26<34:57,  1.30it/s]Training epoch 1:  51%|#####     | 2831/5556 [35:27<34:31,  1.32it/s]Training epoch 1:  51%|#####     | 2832/5556 [35:28<34:31,  1.31it/s]Training epoch 1:  51%|#####     | 2833/5556 [35:29<34:34,  1.31it/s]Training epoch 1:  51%|#####1    | 2834/5556 [35:29<34:29,  1.32it/s]Training epoch 1:  51%|#####1    | 2835/5556 [35:30<34:40,  1.31it/s]Training epoch 1:  51%|#####1    | 2836/5556 [35:31<34:28,  1.32it/s]Training epoch 1:  51%|#####1    | 2837/5556 [35:32<34:31,  1.31it/s]Training epoch 1:  51%|#####1    | 2838/5556 [35:32<34:35,  1.31it/s]Training epoch 1:  51%|#####1    | 2839/5556 [35:33<34:31,  1.31it/s]Training epoch 1:  51%|#####1    | 2840/5556 [35:34<34:38,  1.31it/s]Training epoch 1:  51%|#####1    | 2841/5556 [35:35<34:20,  1.32it/s]Training epoch 1:  51%|#####1    | 2842/5556 [35:35<34:10,  1.32it/s]Training epoch 1:  51%|#####1    | 2843/5556 [35:36<33:37,  1.35it/s]Training epoch 1:  51%|#####1    | 2844/5556 [35:37<33:59,  1.33it/s]Training epoch 1:  51%|#####1    | 2845/5556 [35:38<34:17,  1.32it/s]Training epoch 1:  51%|#####1    | 2846/5556 [35:38<34:10,  1.32it/s]Training epoch 1:  51%|#####1    | 2847/5556 [35:39<34:05,  1.32it/s]Training epoch 1:  51%|#####1    | 2848/5556 [35:40<34:12,  1.32it/s]Training epoch 1:  51%|#####1    | 2849/5556 [35:41<34:04,  1.32it/s]Training epoch 1:  51%|#####1    | 2850/5556 [35:41<33:52,  1.33it/s]Training epoch 1:  51%|#####1    | 2851/5556 [35:42<33:44,  1.34it/s]Training epoch 1:  51%|#####1    | 2852/5556 [35:43<33:57,  1.33it/s]Training epoch 1:  51%|#####1    | 2853/5556 [35:44<34:05,  1.32it/s]Training epoch 1:  51%|#####1    | 2854/5556 [35:44<34:05,  1.32it/s]Training epoch 1:  51%|#####1    | 2855/5556 [35:45<34:01,  1.32it/s]Training epoch 1:  51%|#####1    | 2856/5556 [35:46<33:38,  1.34it/s]Training epoch 1:  51%|#####1    | 2857/5556 [35:47<33:46,  1.33it/s]Training epoch 1:  51%|#####1    | 2858/5556 [35:47<33:21,  1.35it/s]Training epoch 1:  51%|#####1    | 2859/5556 [35:48<33:31,  1.34it/s]Training epoch 1:  51%|#####1    | 2860/5556 [35:49<33:40,  1.33it/s]Training epoch 1:  51%|#####1    | 2861/5556 [35:50<33:48,  1.33it/s]Training epoch 1:  52%|#####1    | 2862/5556 [35:50<33:43,  1.33it/s]Training epoch 1:  52%|#####1    | 2863/5556 [35:51<33:36,  1.34it/s]Training epoch 1:  52%|#####1    | 2864/5556 [35:52<33:28,  1.34it/s]Training epoch 1:  52%|#####1    | 2865/5556 [35:53<33:28,  1.34it/s]Training epoch 1:  52%|#####1    | 2866/5556 [35:53<33:26,  1.34it/s]Training epoch 1:  52%|#####1    | 2867/5556 [35:54<33:33,  1.34it/s]Training epoch 1:  52%|#####1    | 2868/5556 [35:55<33:32,  1.34it/s]Training epoch 1:  52%|#####1    | 2869/5556 [35:56<33:39,  1.33it/s]Training epoch 1:  52%|#####1    | 2870/5556 [35:56<33:30,  1.34it/s]Training epoch 1:  52%|#####1    | 2871/5556 [35:57<33:24,  1.34it/s]Training epoch 1:  52%|#####1    | 2872/5556 [35:58<33:16,  1.34it/s]Training epoch 1:  52%|#####1    | 2873/5556 [35:59<33:03,  1.35it/s]Training epoch 1:  52%|#####1    | 2874/5556 [35:59<32:49,  1.36it/s]Training epoch 1:  52%|#####1    | 2875/5556 [36:00<32:48,  1.36it/s]Training epoch 1:  52%|#####1    | 2876/5556 [36:01<32:42,  1.37it/s]Training epoch 1:  52%|#####1    | 2877/5556 [36:02<32:35,  1.37it/s]Training epoch 1:  52%|#####1    | 2878/5556 [36:02<32:53,  1.36it/s]Training epoch 1:  52%|#####1    | 2879/5556 [36:03<32:48,  1.36it/s]Training epoch 1:  52%|#####1    | 2880/5556 [36:04<33:16,  1.34it/s]Training epoch 1:  52%|#####1    | 2881/5556 [36:05<33:45,  1.32it/s]Training epoch 1:  52%|#####1    | 2882/5556 [36:05<33:44,  1.32it/s]Training epoch 1:  52%|#####1    | 2883/5556 [36:06<33:50,  1.32it/s]Training epoch 1:  52%|#####1    | 2884/5556 [36:07<33:40,  1.32it/s]Training epoch 1:  52%|#####1    | 2885/5556 [36:08<33:35,  1.33it/s]Training epoch 1:  52%|#####1    | 2886/5556 [36:08<33:45,  1.32it/s]Training epoch 1:  52%|#####1    | 2887/5556 [36:09<33:54,  1.31it/s]Training epoch 1:  52%|#####1    | 2888/5556 [36:10<33:41,  1.32it/s]Training epoch 1:  52%|#####1    | 2889/5556 [36:11<33:52,  1.31it/s]Training epoch 1:  52%|#####2    | 2890/5556 [36:11<33:28,  1.33it/s]Training epoch 1:  52%|#####2    | 2891/5556 [36:12<33:29,  1.33it/s]Training epoch 1:  52%|#####2    | 2892/5556 [36:13<33:56,  1.31it/s]Training epoch 1:  52%|#####2    | 2893/5556 [36:14<33:35,  1.32it/s]Training epoch 1:  52%|#####2    | 2894/5556 [36:14<33:33,  1.32it/s]Training epoch 1:  52%|#####2    | 2895/5556 [36:15<33:41,  1.32it/s]Training epoch 1:  52%|#####2    | 2896/5556 [36:16<33:41,  1.32it/s]Training epoch 1:  52%|#####2    | 2897/5556 [36:17<32:51,  1.35it/s]Training epoch 1:  52%|#####2    | 2898/5556 [36:17<32:36,  1.36it/s]Training epoch 1:  52%|#####2    | 2899/5556 [36:18<32:51,  1.35it/s]Training epoch 1:  52%|#####2    | 2900/5556 [36:19<34:17,  1.29it/s]Training epoch 1:  52%|#####2    | 2901/5556 [36:20<33:48,  1.31it/s]Training epoch 1:  52%|#####2    | 2902/5556 [36:20<33:54,  1.30it/s]Training epoch 1:  52%|#####2    | 2903/5556 [36:21<34:07,  1.30it/s]Training epoch 1:  52%|#####2    | 2904/5556 [36:22<34:10,  1.29it/s]Training epoch 1:  52%|#####2    | 2905/5556 [36:23<33:40,  1.31it/s]Training epoch 1:  52%|#####2    | 2906/5556 [36:24<33:50,  1.31it/s]Training epoch 1:  52%|#####2    | 2907/5556 [36:24<33:44,  1.31it/s]Training epoch 1:  52%|#####2    | 2908/5556 [36:25<33:27,  1.32it/s]Training epoch 1:  52%|#####2    | 2909/5556 [36:26<33:27,  1.32it/s]Training epoch 1:  52%|#####2    | 2910/5556 [36:27<32:54,  1.34it/s]Training epoch 1:  52%|#####2    | 2911/5556 [36:27<33:02,  1.33it/s]Training epoch 1:  52%|#####2    | 2912/5556 [36:28<33:06,  1.33it/s]Training epoch 1:  52%|#####2    | 2913/5556 [36:29<33:26,  1.32it/s]Training epoch 1:  52%|#####2    | 2914/5556 [36:30<33:35,  1.31it/s]Training epoch 1:  52%|#####2    | 2915/5556 [36:30<33:54,  1.30it/s]Training epoch 1:  52%|#####2    | 2916/5556 [36:31<33:51,  1.30it/s]Training epoch 1:  53%|#####2    | 2917/5556 [36:32<33:23,  1.32it/s]Training epoch 1:  53%|#####2    | 2918/5556 [36:33<33:37,  1.31it/s]Training epoch 1:  53%|#####2    | 2919/5556 [36:33<33:30,  1.31it/s]Training epoch 1:  53%|#####2    | 2920/5556 [36:34<33:15,  1.32it/s]Training epoch 1:  53%|#####2    | 2921/5556 [36:35<33:27,  1.31it/s]Training epoch 1:  53%|#####2    | 2922/5556 [36:36<33:33,  1.31it/s]Training epoch 1:  53%|#####2    | 2923/5556 [36:36<33:06,  1.33it/s]Training epoch 1:  53%|#####2    | 2924/5556 [36:37<32:53,  1.33it/s]Training epoch 1:  53%|#####2    | 2925/5556 [36:38<32:57,  1.33it/s]Training epoch 1:  53%|#####2    | 2926/5556 [36:39<32:52,  1.33it/s]Training epoch 1:  53%|#####2    | 2927/5556 [36:39<33:20,  1.31it/s]Training epoch 1:  53%|#####2    | 2928/5556 [36:40<33:24,  1.31it/s]Training epoch 1:  53%|#####2    | 2929/5556 [36:41<32:57,  1.33it/s]Training epoch 1:  53%|#####2    | 2930/5556 [36:42<32:56,  1.33it/s]Training epoch 1:  53%|#####2    | 2931/5556 [36:42<32:46,  1.34it/s]Training epoch 1:  53%|#####2    | 2932/5556 [36:43<32:36,  1.34it/s]Training epoch 1:  53%|#####2    | 2933/5556 [36:44<32:39,  1.34it/s]Training epoch 1:  53%|#####2    | 2934/5556 [36:45<32:38,  1.34it/s]Training epoch 1:  53%|#####2    | 2935/5556 [36:45<32:21,  1.35it/s]Training epoch 1:  53%|#####2    | 2936/5556 [36:46<32:14,  1.35it/s]Training epoch 1:  53%|#####2    | 2937/5556 [36:47<32:10,  1.36it/s]Training epoch 1:  53%|#####2    | 2938/5556 [36:48<31:52,  1.37it/s]Training epoch 1:  53%|#####2    | 2939/5556 [36:48<31:50,  1.37it/s]Training epoch 1:  53%|#####2    | 2940/5556 [36:49<32:22,  1.35it/s]Training epoch 1:  53%|#####2    | 2941/5556 [36:50<32:16,  1.35it/s]Training epoch 1:  53%|#####2    | 2942/5556 [36:51<32:43,  1.33it/s]Training epoch 1:  53%|#####2    | 2943/5556 [36:51<32:26,  1.34it/s]Training epoch 1:  53%|#####2    | 2944/5556 [36:52<32:22,  1.34it/s]Training epoch 1:  53%|#####3    | 2945/5556 [36:53<32:36,  1.33it/s]Training epoch 1:  53%|#####3    | 2946/5556 [36:54<32:36,  1.33it/s]Training epoch 1:  53%|#####3    | 2947/5556 [36:54<32:21,  1.34it/s]Training epoch 1:  53%|#####3    | 2948/5556 [36:55<32:42,  1.33it/s]Training epoch 1:  53%|#####3    | 2949/5556 [36:56<32:22,  1.34it/s]Training epoch 1:  53%|#####3    | 2950/5556 [36:57<32:20,  1.34it/s]Training epoch 1:  53%|#####3    | 2951/5556 [36:57<32:14,  1.35it/s]Training epoch 1:  53%|#####3    | 2952/5556 [36:58<32:32,  1.33it/s]Training epoch 1:  53%|#####3    | 2953/5556 [36:59<32:43,  1.33it/s]Training epoch 1:  53%|#####3    | 2954/5556 [37:00<33:08,  1.31it/s]Training epoch 1:  53%|#####3    | 2955/5556 [37:00<33:15,  1.30it/s]Training epoch 1:  53%|#####3    | 2956/5556 [37:01<32:41,  1.33it/s]Training epoch 1:  53%|#####3    | 2957/5556 [37:02<32:24,  1.34it/s]Training epoch 1:  53%|#####3    | 2958/5556 [37:03<32:38,  1.33it/s]Training epoch 1:  53%|#####3    | 2959/5556 [37:03<32:32,  1.33it/s]Training epoch 1:  53%|#####3    | 2960/5556 [37:04<32:50,  1.32it/s]Training epoch 1:  53%|#####3    | 2961/5556 [37:05<32:34,  1.33it/s]Training epoch 1:  53%|#####3    | 2962/5556 [37:06<32:29,  1.33it/s]Training epoch 1:  53%|#####3    | 2963/5556 [37:06<32:41,  1.32it/s]Training epoch 1:  53%|#####3    | 2964/5556 [37:07<32:48,  1.32it/s]Training epoch 1:  53%|#####3    | 2965/5556 [37:08<32:50,  1.32it/s]Training epoch 1:  53%|#####3    | 2966/5556 [37:09<32:54,  1.31it/s]Training epoch 1:  53%|#####3    | 2967/5556 [37:09<32:39,  1.32it/s]Training epoch 1:  53%|#####3    | 2968/5556 [37:10<32:40,  1.32it/s]Training epoch 1:  53%|#####3    | 2969/5556 [37:11<32:28,  1.33it/s]Training epoch 1:  53%|#####3    | 2970/5556 [37:12<32:17,  1.33it/s]Training epoch 1:  53%|#####3    | 2971/5556 [37:12<32:18,  1.33it/s]Training epoch 1:  53%|#####3    | 2972/5556 [37:13<32:03,  1.34it/s]Training epoch 1:  54%|#####3    | 2973/5556 [37:14<31:57,  1.35it/s]Training epoch 1:  54%|#####3    | 2974/5556 [37:15<31:50,  1.35it/s]Training epoch 1:  54%|#####3    | 2975/5556 [37:15<32:09,  1.34it/s]Training epoch 1:  54%|#####3    | 2976/5556 [37:16<32:44,  1.31it/s]Training epoch 1:  54%|#####3    | 2977/5556 [37:17<32:33,  1.32it/s]Training epoch 1:  54%|#####3    | 2978/5556 [37:18<32:16,  1.33it/s]Training epoch 1:  54%|#####3    | 2979/5556 [37:18<32:02,  1.34it/s]Training epoch 1:  54%|#####3    | 2980/5556 [37:19<31:56,  1.34it/s]Training epoch 1:  54%|#####3    | 2981/5556 [37:20<32:16,  1.33it/s]Training epoch 1:  54%|#####3    | 2982/5556 [37:21<32:23,  1.32it/s]Training epoch 1:  54%|#####3    | 2983/5556 [37:21<32:24,  1.32it/s]Training epoch 1:  54%|#####3    | 2984/5556 [37:22<32:15,  1.33it/s]Training epoch 1:  54%|#####3    | 2985/5556 [37:23<32:08,  1.33it/s]Training epoch 1:  54%|#####3    | 2986/5556 [37:24<31:55,  1.34it/s]Training epoch 1:  54%|#####3    | 2987/5556 [37:24<32:06,  1.33it/s]Training epoch 1:  54%|#####3    | 2988/5556 [37:25<31:56,  1.34it/s]Training epoch 1:  54%|#####3    | 2989/5556 [37:26<32:17,  1.33it/s]Training epoch 1:  54%|#####3    | 2990/5556 [37:27<32:32,  1.31it/s]Training epoch 1:  54%|#####3    | 2991/5556 [37:27<32:00,  1.34it/s]Training epoch 1:  54%|#####3    | 2992/5556 [37:28<31:40,  1.35it/s]Training epoch 1:  54%|#####3    | 2993/5556 [37:29<32:06,  1.33it/s]Training epoch 1:  54%|#####3    | 2994/5556 [37:30<32:08,  1.33it/s]Training epoch 1:  54%|#####3    | 2995/5556 [37:30<32:27,  1.31it/s]Training epoch 1:  54%|#####3    | 2996/5556 [37:31<32:25,  1.32it/s]Training epoch 1:  54%|#####3    | 2997/5556 [37:32<32:20,  1.32it/s]Training epoch 1:  54%|#####3    | 2998/5556 [37:33<31:54,  1.34it/s]Training epoch 1:  54%|#####3    | 2999/5556 [37:33<32:05,  1.33it/s]Training epoch 1:  54%|#####3    | 3000/5556 [37:34<33:23,  1.28it/s]Training epoch 1:  54%|#####4    | 3001/5556 [37:35<32:46,  1.30it/s]Training epoch 1:  54%|#####4    | 3002/5556 [37:36<32:28,  1.31it/s]Training epoch 1:  54%|#####4    | 3003/5556 [37:37<31:46,  1.34it/s]Training epoch 1:  54%|#####4    | 3004/5556 [37:37<31:56,  1.33it/s]Training epoch 1:  54%|#####4    | 3005/5556 [37:38<32:16,  1.32it/s]Training epoch 1:  54%|#####4    | 3006/5556 [37:39<32:37,  1.30it/s]Training epoch 1:  54%|#####4    | 3007/5556 [37:40<32:12,  1.32it/s]Training epoch 1:  54%|#####4    | 3008/5556 [37:40<32:19,  1.31it/s]Training epoch 1:  54%|#####4    | 3009/5556 [37:41<32:00,  1.33it/s]Training epoch 1:  54%|#####4    | 3010/5556 [37:42<31:55,  1.33it/s]Training epoch 1:  54%|#####4    | 3011/5556 [37:43<31:36,  1.34it/s]Training epoch 1:  54%|#####4    | 3012/5556 [37:43<31:24,  1.35it/s]Training epoch 1:  54%|#####4    | 3013/5556 [37:44<31:45,  1.33it/s]Training epoch 1:  54%|#####4    | 3014/5556 [37:45<32:07,  1.32it/s]Training epoch 1:  54%|#####4    | 3015/5556 [37:46<31:40,  1.34it/s]Training epoch 1:  54%|#####4    | 3016/5556 [37:46<31:33,  1.34it/s]Training epoch 1:  54%|#####4    | 3017/5556 [37:47<31:47,  1.33it/s]Training epoch 1:  54%|#####4    | 3018/5556 [37:48<31:23,  1.35it/s]Training epoch 1:  54%|#####4    | 3019/5556 [37:49<31:19,  1.35it/s]Training epoch 1:  54%|#####4    | 3020/5556 [37:49<31:31,  1.34it/s]Training epoch 1:  54%|#####4    | 3021/5556 [37:50<31:50,  1.33it/s]Training epoch 1:  54%|#####4    | 3022/5556 [37:51<31:36,  1.34it/s]Training epoch 1:  54%|#####4    | 3023/5556 [37:52<31:29,  1.34it/s]Training epoch 1:  54%|#####4    | 3024/5556 [37:52<31:17,  1.35it/s]Training epoch 1:  54%|#####4    | 3025/5556 [37:53<31:44,  1.33it/s]Training epoch 1:  54%|#####4    | 3026/5556 [37:54<32:08,  1.31it/s]Training epoch 1:  54%|#####4    | 3027/5556 [37:55<31:34,  1.33it/s]Training epoch 1:  54%|#####4    | 3028/5556 [37:55<31:27,  1.34it/s]Training epoch 1:  55%|#####4    | 3029/5556 [37:56<31:39,  1.33it/s]Training epoch 1:  55%|#####4    | 3030/5556 [37:57<31:24,  1.34it/s]Training epoch 1:  55%|#####4    | 3031/5556 [37:58<31:10,  1.35it/s]Training epoch 1:  55%|#####4    | 3032/5556 [37:58<31:24,  1.34it/s]Training epoch 1:  55%|#####4    | 3033/5556 [37:59<31:23,  1.34it/s]Training epoch 1:  55%|#####4    | 3034/5556 [38:00<31:12,  1.35it/s]Training epoch 1:  55%|#####4    | 3035/5556 [38:01<31:46,  1.32it/s]Training epoch 1:  55%|#####4    | 3036/5556 [38:01<31:41,  1.33it/s]Training epoch 1:  55%|#####4    | 3037/5556 [38:02<31:42,  1.32it/s]Training epoch 1:  55%|#####4    | 3038/5556 [38:03<31:31,  1.33it/s]Training epoch 1:  55%|#####4    | 3039/5556 [38:04<31:48,  1.32it/s]Training epoch 1:  55%|#####4    | 3040/5556 [38:04<31:51,  1.32it/s]Training epoch 1:  55%|#####4    | 3041/5556 [38:05<31:52,  1.31it/s]Training epoch 1:  55%|#####4    | 3042/5556 [38:06<31:17,  1.34it/s]Training epoch 1:  55%|#####4    | 3043/5556 [38:07<31:14,  1.34it/s]Training epoch 1:  55%|#####4    | 3044/5556 [38:07<31:42,  1.32it/s]Training epoch 1:  55%|#####4    | 3045/5556 [38:08<31:39,  1.32it/s]Training epoch 1:  55%|#####4    | 3046/5556 [38:09<31:36,  1.32it/s]Training epoch 1:  55%|#####4    | 3047/5556 [38:10<31:28,  1.33it/s]Training epoch 1:  55%|#####4    | 3048/5556 [38:10<31:23,  1.33it/s]Training epoch 1:  55%|#####4    | 3049/5556 [38:11<31:23,  1.33it/s]Training epoch 1:  55%|#####4    | 3050/5556 [38:12<31:04,  1.34it/s]Training epoch 1:  55%|#####4    | 3051/5556 [38:13<31:24,  1.33it/s]Training epoch 1:  55%|#####4    | 3052/5556 [38:13<31:31,  1.32it/s]Training epoch 1:  55%|#####4    | 3053/5556 [38:14<31:41,  1.32it/s]Training epoch 1:  55%|#####4    | 3054/5556 [38:15<31:41,  1.32it/s]Training epoch 1:  55%|#####4    | 3055/5556 [38:16<31:33,  1.32it/s]Training epoch 1:  55%|#####5    | 3056/5556 [38:16<31:26,  1.33it/s]Training epoch 1:  55%|#####5    | 3057/5556 [38:17<31:26,  1.32it/s]Training epoch 1:  55%|#####5    | 3058/5556 [38:18<30:59,  1.34it/s]Training epoch 1:  55%|#####5    | 3059/5556 [38:19<30:59,  1.34it/s]Training epoch 1:  55%|#####5    | 3060/5556 [38:19<31:12,  1.33it/s]Training epoch 1:  55%|#####5    | 3061/5556 [38:20<31:36,  1.32it/s]Training epoch 1:  55%|#####5    | 3062/5556 [38:21<31:24,  1.32it/s]Training epoch 1:  55%|#####5    | 3063/5556 [38:22<31:25,  1.32it/s]Training epoch 1:  55%|#####5    | 3064/5556 [38:22<31:06,  1.34it/s]Training epoch 1:  55%|#####5    | 3065/5556 [38:23<31:17,  1.33it/s]Training epoch 1:  55%|#####5    | 3066/5556 [38:24<31:34,  1.31it/s]Training epoch 1:  55%|#####5    | 3067/5556 [38:25<31:36,  1.31it/s]Training epoch 1:  55%|#####5    | 3068/5556 [38:25<31:20,  1.32it/s]Training epoch 1:  55%|#####5    | 3069/5556 [38:26<31:29,  1.32it/s]Training epoch 1:  55%|#####5    | 3070/5556 [38:27<31:12,  1.33it/s]Training epoch 1:  55%|#####5    | 3071/5556 [38:28<31:12,  1.33it/s]Training epoch 1:  55%|#####5    | 3072/5556 [38:28<31:25,  1.32it/s]Training epoch 1:  55%|#####5    | 3073/5556 [38:29<31:09,  1.33it/s]Training epoch 1:  55%|#####5    | 3074/5556 [38:30<31:06,  1.33it/s]Training epoch 1:  55%|#####5    | 3075/5556 [38:31<31:07,  1.33it/s]Training epoch 1:  55%|#####5    | 3076/5556 [38:31<31:07,  1.33it/s]Training epoch 1:  55%|#####5    | 3077/5556 [38:32<31:00,  1.33it/s]Training epoch 1:  55%|#####5    | 3078/5556 [38:33<30:34,  1.35it/s]Training epoch 1:  55%|#####5    | 3079/5556 [38:34<30:34,  1.35it/s]Training epoch 1:  55%|#####5    | 3080/5556 [38:34<30:35,  1.35it/s]Training epoch 1:  55%|#####5    | 3081/5556 [38:35<31:11,  1.32it/s]Training epoch 1:  55%|#####5    | 3082/5556 [38:36<31:10,  1.32it/s]Training epoch 1:  55%|#####5    | 3083/5556 [38:37<30:47,  1.34it/s]Training epoch 1:  56%|#####5    | 3084/5556 [38:37<30:55,  1.33it/s]Training epoch 1:  56%|#####5    | 3085/5556 [38:38<30:36,  1.35it/s]Training epoch 1:  56%|#####5    | 3086/5556 [38:39<30:25,  1.35it/s]Training epoch 1:  56%|#####5    | 3087/5556 [38:40<30:16,  1.36it/s]Training epoch 1:  56%|#####5    | 3088/5556 [38:40<30:28,  1.35it/s]Training epoch 1:  56%|#####5    | 3089/5556 [38:41<30:37,  1.34it/s]Training epoch 1:  56%|#####5    | 3090/5556 [38:42<30:36,  1.34it/s]Training epoch 1:  56%|#####5    | 3091/5556 [38:43<30:31,  1.35it/s]Training epoch 1:  56%|#####5    | 3092/5556 [38:43<30:28,  1.35it/s]Training epoch 1:  56%|#####5    | 3093/5556 [38:44<30:41,  1.34it/s]Training epoch 1:  56%|#####5    | 3094/5556 [38:45<30:26,  1.35it/s]Training epoch 1:  56%|#####5    | 3095/5556 [38:46<30:44,  1.33it/s]Training epoch 1:  56%|#####5    | 3096/5556 [38:46<30:36,  1.34it/s]Training epoch 1:  56%|#####5    | 3097/5556 [38:47<30:28,  1.34it/s]Training epoch 1:  56%|#####5    | 3098/5556 [38:48<30:17,  1.35it/s]Training epoch 1:  56%|#####5    | 3099/5556 [38:49<30:30,  1.34it/s]Training epoch 1:  56%|#####5    | 3100/5556 [38:49<31:40,  1.29it/s]Training epoch 1:  56%|#####5    | 3101/5556 [38:50<31:20,  1.31it/s]Training epoch 1:  56%|#####5    | 3102/5556 [38:51<31:00,  1.32it/s]Training epoch 1:  56%|#####5    | 3103/5556 [38:52<31:03,  1.32it/s]Training epoch 1:  56%|#####5    | 3104/5556 [38:52<30:57,  1.32it/s]Training epoch 1:  56%|#####5    | 3105/5556 [38:53<30:54,  1.32it/s]Training epoch 1:  56%|#####5    | 3106/5556 [38:54<30:37,  1.33it/s]Training epoch 1:  56%|#####5    | 3107/5556 [38:55<30:42,  1.33it/s]Training epoch 1:  56%|#####5    | 3108/5556 [38:55<30:22,  1.34it/s]Training epoch 1:  56%|#####5    | 3109/5556 [38:56<30:48,  1.32it/s]Training epoch 1:  56%|#####5    | 3110/5556 [38:57<30:34,  1.33it/s]Training epoch 1:  56%|#####5    | 3111/5556 [38:58<30:54,  1.32it/s]Training epoch 1:  56%|#####6    | 3112/5556 [38:58<30:51,  1.32it/s]Training epoch 1:  56%|#####6    | 3113/5556 [38:59<30:48,  1.32it/s]Training epoch 1:  56%|#####6    | 3114/5556 [39:00<30:30,  1.33it/s]Training epoch 1:  56%|#####6    | 3115/5556 [39:01<30:22,  1.34it/s]Training epoch 1:  56%|#####6    | 3116/5556 [39:01<30:19,  1.34it/s]Training epoch 1:  56%|#####6    | 3117/5556 [39:02<30:21,  1.34it/s]Training epoch 1:  56%|#####6    | 3118/5556 [39:03<30:50,  1.32it/s]Training epoch 1:  56%|#####6    | 3119/5556 [39:04<30:16,  1.34it/s]Training epoch 1:  56%|#####6    | 3120/5556 [39:04<30:50,  1.32it/s]Training epoch 1:  56%|#####6    | 3121/5556 [39:05<30:48,  1.32it/s]Training epoch 1:  56%|#####6    | 3122/5556 [39:06<30:55,  1.31it/s]Training epoch 1:  56%|#####6    | 3123/5556 [39:07<31:16,  1.30it/s]Training epoch 1:  56%|#####6    | 3124/5556 [39:08<30:53,  1.31it/s]Training epoch 1:  56%|#####6    | 3125/5556 [39:08<30:40,  1.32it/s]Training epoch 1:  56%|#####6    | 3126/5556 [39:09<30:35,  1.32it/s]Training epoch 1:  56%|#####6    | 3127/5556 [39:10<30:51,  1.31it/s]Training epoch 1:  56%|#####6    | 3128/5556 [39:11<31:02,  1.30it/s]Training epoch 1:  56%|#####6    | 3129/5556 [39:11<30:59,  1.31it/s]Training epoch 1:  56%|#####6    | 3130/5556 [39:12<30:53,  1.31it/s]Training epoch 1:  56%|#####6    | 3131/5556 [39:13<30:22,  1.33it/s]Training epoch 1:  56%|#####6    | 3132/5556 [39:14<30:11,  1.34it/s]Training epoch 1:  56%|#####6    | 3133/5556 [39:14<30:36,  1.32it/s]Training epoch 1:  56%|#####6    | 3134/5556 [39:15<30:13,  1.34it/s]Training epoch 1:  56%|#####6    | 3135/5556 [39:16<30:23,  1.33it/s]Training epoch 1:  56%|#####6    | 3136/5556 [39:17<29:59,  1.34it/s]Training epoch 1:  56%|#####6    | 3137/5556 [39:17<29:34,  1.36it/s]Training epoch 1:  56%|#####6    | 3138/5556 [39:18<29:43,  1.36it/s]Training epoch 1:  56%|#####6    | 3139/5556 [39:19<29:31,  1.36it/s]Training epoch 1:  57%|#####6    | 3140/5556 [39:19<29:46,  1.35it/s]Training epoch 1:  57%|#####6    | 3141/5556 [39:20<30:02,  1.34it/s]Training epoch 1:  57%|#####6    | 3142/5556 [39:21<29:43,  1.35it/s]Training epoch 1:  57%|#####6    | 3143/5556 [39:22<29:34,  1.36it/s]Training epoch 1:  57%|#####6    | 3144/5556 [39:22<29:45,  1.35it/s]Training epoch 1:  57%|#####6    | 3145/5556 [39:23<29:52,  1.35it/s]Training epoch 1:  57%|#####6    | 3146/5556 [39:24<29:50,  1.35it/s]Training epoch 1:  57%|#####6    | 3147/5556 [39:25<30:20,  1.32it/s]Training epoch 1:  57%|#####6    | 3148/5556 [39:25<30:22,  1.32it/s]Training epoch 1:  57%|#####6    | 3149/5556 [39:26<30:18,  1.32it/s]Training epoch 1:  57%|#####6    | 3150/5556 [39:27<30:11,  1.33it/s]Training epoch 1:  57%|#####6    | 3151/5556 [39:28<29:55,  1.34it/s]Training epoch 1:  57%|#####6    | 3152/5556 [39:28<29:33,  1.36it/s]Training epoch 1:  57%|#####6    | 3153/5556 [39:29<29:48,  1.34it/s]Training epoch 1:  57%|#####6    | 3154/5556 [39:30<29:49,  1.34it/s]Training epoch 1:  57%|#####6    | 3155/5556 [39:31<29:40,  1.35it/s]Training epoch 1:  57%|#####6    | 3156/5556 [39:31<30:19,  1.32it/s]Training epoch 1:  57%|#####6    | 3157/5556 [39:32<30:02,  1.33it/s]Training epoch 1:  57%|#####6    | 3158/5556 [39:33<30:28,  1.31it/s]Training epoch 1:  57%|#####6    | 3159/5556 [39:34<30:18,  1.32it/s]Training epoch 1:  57%|#####6    | 3160/5556 [39:34<29:52,  1.34it/s]Training epoch 1:  57%|#####6    | 3161/5556 [39:35<29:47,  1.34it/s]Training epoch 1:  57%|#####6    | 3162/5556 [39:36<29:54,  1.33it/s]Training epoch 1:  57%|#####6    | 3163/5556 [39:37<30:08,  1.32it/s]Training epoch 1:  57%|#####6    | 3164/5556 [39:38<30:17,  1.32it/s]Training epoch 1:  57%|#####6    | 3165/5556 [39:38<30:01,  1.33it/s]Training epoch 1:  57%|#####6    | 3166/5556 [39:39<30:06,  1.32it/s]Training epoch 1:  57%|#####7    | 3167/5556 [39:40<30:01,  1.33it/s]Training epoch 1:  57%|#####7    | 3168/5556 [39:41<30:14,  1.32it/s]Training epoch 1:  57%|#####7    | 3169/5556 [39:41<30:10,  1.32it/s]Training epoch 1:  57%|#####7    | 3170/5556 [39:42<29:43,  1.34it/s]Training epoch 1:  57%|#####7    | 3171/5556 [39:43<29:45,  1.34it/s]Training epoch 1:  57%|#####7    | 3172/5556 [39:44<29:44,  1.34it/s]Training epoch 1:  57%|#####7    | 3173/5556 [39:44<29:54,  1.33it/s]Training epoch 1:  57%|#####7    | 3174/5556 [39:45<29:58,  1.32it/s]Training epoch 1:  57%|#####7    | 3175/5556 [39:46<29:49,  1.33it/s]Training epoch 1:  57%|#####7    | 3176/5556 [39:47<29:57,  1.32it/s]Training epoch 1:  57%|#####7    | 3177/5556 [39:47<30:31,  1.30it/s]Training epoch 1:  57%|#####7    | 3178/5556 [39:48<30:21,  1.31it/s]Training epoch 1:  57%|#####7    | 3179/5556 [39:49<30:08,  1.31it/s]Training epoch 1:  57%|#####7    | 3180/5556 [39:50<29:54,  1.32it/s]Training epoch 1:  57%|#####7    | 3181/5556 [39:50<29:35,  1.34it/s]Training epoch 1:  57%|#####7    | 3182/5556 [39:51<29:29,  1.34it/s]Training epoch 1:  57%|#####7    | 3183/5556 [39:52<29:23,  1.35it/s]Training epoch 1:  57%|#####7    | 3184/5556 [39:53<29:37,  1.33it/s]Training epoch 1:  57%|#####7    | 3185/5556 [39:53<29:50,  1.32it/s]Training epoch 1:  57%|#####7    | 3186/5556 [39:54<29:44,  1.33it/s]Training epoch 1:  57%|#####7    | 3187/5556 [39:55<29:43,  1.33it/s]Training epoch 1:  57%|#####7    | 3188/5556 [39:56<29:59,  1.32it/s]Training epoch 1:  57%|#####7    | 3189/5556 [39:56<29:34,  1.33it/s]Training epoch 1:  57%|#####7    | 3190/5556 [39:57<29:45,  1.33it/s]Training epoch 1:  57%|#####7    | 3191/5556 [39:58<29:25,  1.34it/s]Training epoch 1:  57%|#####7    | 3192/5556 [39:59<29:06,  1.35it/s]Training epoch 1:  57%|#####7    | 3193/5556 [39:59<29:20,  1.34it/s]Training epoch 1:  57%|#####7    | 3194/5556 [40:00<29:32,  1.33it/s]Training epoch 1:  58%|#####7    | 3195/5556 [40:01<29:29,  1.33it/s]Training epoch 1:  58%|#####7    | 3196/5556 [40:02<29:53,  1.32it/s]Training epoch 1:  58%|#####7    | 3197/5556 [40:02<29:37,  1.33it/s]Training epoch 1:  58%|#####7    | 3198/5556 [40:03<29:38,  1.33it/s]Training epoch 1:  58%|#####7    | 3199/5556 [40:04<29:40,  1.32it/s]Training epoch 1:  58%|#####7    | 3200/5556 [40:05<30:47,  1.28it/s]Training epoch 1:  58%|#####7    | 3201/5556 [40:05<30:29,  1.29it/s]Training epoch 1:  58%|#####7    | 3202/5556 [40:06<30:13,  1.30it/s]Training epoch 1:  58%|#####7    | 3203/5556 [40:07<30:17,  1.29it/s]Training epoch 1:  58%|#####7    | 3204/5556 [40:08<30:07,  1.30it/s]Training epoch 1:  58%|#####7    | 3205/5556 [40:09<29:47,  1.32it/s]Training epoch 1:  58%|#####7    | 3206/5556 [40:09<30:02,  1.30it/s]Training epoch 1:  58%|#####7    | 3207/5556 [40:10<29:53,  1.31it/s]Training epoch 1:  58%|#####7    | 3208/5556 [40:11<29:41,  1.32it/s]Training epoch 1:  58%|#####7    | 3209/5556 [40:12<29:07,  1.34it/s]Training epoch 1:  58%|#####7    | 3210/5556 [40:12<29:06,  1.34it/s]Training epoch 1:  58%|#####7    | 3211/5556 [40:13<29:03,  1.35it/s]Training epoch 1:  58%|#####7    | 3212/5556 [40:14<29:06,  1.34it/s]Training epoch 1:  58%|#####7    | 3213/5556 [40:15<29:41,  1.32it/s]Training epoch 1:  58%|#####7    | 3214/5556 [40:15<30:05,  1.30it/s]Training epoch 1:  58%|#####7    | 3215/5556 [40:16<29:18,  1.33it/s]Training epoch 1:  58%|#####7    | 3216/5556 [40:17<29:34,  1.32it/s]Training epoch 1:  58%|#####7    | 3217/5556 [40:18<29:23,  1.33it/s]Training epoch 1:  58%|#####7    | 3218/5556 [40:18<28:56,  1.35it/s]Training epoch 1:  58%|#####7    | 3219/5556 [40:19<29:10,  1.33it/s]Training epoch 1:  58%|#####7    | 3220/5556 [40:20<29:04,  1.34it/s]Training epoch 1:  58%|#####7    | 3221/5556 [40:21<29:09,  1.33it/s]Training epoch 1:  58%|#####7    | 3222/5556 [40:21<29:25,  1.32it/s]Training epoch 1:  58%|#####8    | 3223/5556 [40:22<29:38,  1.31it/s]Training epoch 1:  58%|#####8    | 3224/5556 [40:23<29:29,  1.32it/s]Training epoch 1:  58%|#####8    | 3225/5556 [40:24<29:24,  1.32it/s]Training epoch 1:  58%|#####8    | 3226/5556 [40:24<29:38,  1.31it/s]Training epoch 1:  58%|#####8    | 3227/5556 [40:25<29:32,  1.31it/s]Training epoch 1:  58%|#####8    | 3228/5556 [40:26<29:17,  1.32it/s]Training epoch 1:  58%|#####8    | 3229/5556 [40:27<29:32,  1.31it/s]Training epoch 1:  58%|#####8    | 3230/5556 [40:27<29:19,  1.32it/s]Training epoch 1:  58%|#####8    | 3231/5556 [40:28<29:22,  1.32it/s]Training epoch 1:  58%|#####8    | 3232/5556 [40:29<29:32,  1.31it/s]Training epoch 1:  58%|#####8    | 3233/5556 [40:30<29:11,  1.33it/s]Training epoch 1:  58%|#####8    | 3234/5556 [40:30<28:54,  1.34it/s]Training epoch 1:  58%|#####8    | 3235/5556 [40:31<28:20,  1.37it/s]Training epoch 1:  58%|#####8    | 3236/5556 [40:32<28:39,  1.35it/s]Training epoch 1:  58%|#####8    | 3237/5556 [40:33<28:53,  1.34it/s]Training epoch 1:  58%|#####8    | 3238/5556 [40:33<28:52,  1.34it/s]Training epoch 1:  58%|#####8    | 3239/5556 [40:34<28:51,  1.34it/s]Training epoch 1:  58%|#####8    | 3240/5556 [40:35<28:48,  1.34it/s]Training epoch 1:  58%|#####8    | 3241/5556 [40:36<28:44,  1.34it/s]Training epoch 1:  58%|#####8    | 3242/5556 [40:36<29:02,  1.33it/s]Training epoch 1:  58%|#####8    | 3243/5556 [40:37<29:07,  1.32it/s]Training epoch 1:  58%|#####8    | 3244/5556 [40:38<29:32,  1.30it/s]Training epoch 1:  58%|#####8    | 3245/5556 [40:39<29:34,  1.30it/s]Training epoch 1:  58%|#####8    | 3246/5556 [40:39<29:46,  1.29it/s]Training epoch 1:  58%|#####8    | 3247/5556 [40:40<29:44,  1.29it/s]Training epoch 1:  58%|#####8    | 3248/5556 [40:41<29:37,  1.30it/s]Training epoch 1:  58%|#####8    | 3249/5556 [40:42<29:31,  1.30it/s]Training epoch 1:  58%|#####8    | 3250/5556 [40:43<29:14,  1.31it/s]Training epoch 1:  59%|#####8    | 3251/5556 [40:43<29:09,  1.32it/s]Training epoch 1:  59%|#####8    | 3252/5556 [40:44<29:14,  1.31it/s]Training epoch 1:  59%|#####8    | 3253/5556 [40:45<29:20,  1.31it/s]Training epoch 1:  59%|#####8    | 3254/5556 [40:46<28:56,  1.33it/s]Training epoch 1:  59%|#####8    | 3255/5556 [40:46<28:43,  1.34it/s]Training epoch 1:  59%|#####8    | 3256/5556 [40:47<28:36,  1.34it/s]Training epoch 1:  59%|#####8    | 3257/5556 [40:48<29:05,  1.32it/s]Training epoch 1:  59%|#####8    | 3258/5556 [40:49<29:08,  1.31it/s]Training epoch 1:  59%|#####8    | 3259/5556 [40:49<29:21,  1.30it/s]Training epoch 1:  59%|#####8    | 3260/5556 [40:50<28:49,  1.33it/s]Training epoch 1:  59%|#####8    | 3261/5556 [40:51<28:29,  1.34it/s]Training epoch 1:  59%|#####8    | 3262/5556 [40:52<28:42,  1.33it/s]Training epoch 1:  59%|#####8    | 3263/5556 [40:52<28:59,  1.32it/s]Training epoch 1:  59%|#####8    | 3264/5556 [40:53<28:43,  1.33it/s]Training epoch 1:  59%|#####8    | 3265/5556 [40:54<29:06,  1.31it/s]Training epoch 1:  59%|#####8    | 3266/5556 [40:55<28:46,  1.33it/s]Training epoch 1:  59%|#####8    | 3267/5556 [40:55<28:41,  1.33it/s]Training epoch 1:  59%|#####8    | 3268/5556 [40:56<28:31,  1.34it/s]Training epoch 1:  59%|#####8    | 3269/5556 [40:57<28:20,  1.34it/s]Training epoch 1:  59%|#####8    | 3270/5556 [40:58<28:25,  1.34it/s]Training epoch 1:  59%|#####8    | 3271/5556 [40:58<28:33,  1.33it/s]Training epoch 1:  59%|#####8    | 3272/5556 [40:59<28:51,  1.32it/s]Training epoch 1:  59%|#####8    | 3273/5556 [41:00<28:38,  1.33it/s]Training epoch 1:  59%|#####8    | 3274/5556 [41:01<28:24,  1.34it/s]Training epoch 1:  59%|#####8    | 3275/5556 [41:01<28:49,  1.32it/s]Training epoch 1:  59%|#####8    | 3276/5556 [41:02<28:45,  1.32it/s]Training epoch 1:  59%|#####8    | 3277/5556 [41:03<28:44,  1.32it/s]Training epoch 1:  59%|#####8    | 3278/5556 [41:04<28:36,  1.33it/s]Training epoch 1:  59%|#####9    | 3279/5556 [41:04<28:43,  1.32it/s]Training epoch 1:  59%|#####9    | 3280/5556 [41:05<28:45,  1.32it/s]Training epoch 1:  59%|#####9    | 3281/5556 [41:06<28:30,  1.33it/s]Training epoch 1:  59%|#####9    | 3282/5556 [41:07<28:45,  1.32it/s]Training epoch 1:  59%|#####9    | 3283/5556 [41:07<28:55,  1.31it/s]Training epoch 1:  59%|#####9    | 3284/5556 [41:08<28:41,  1.32it/s]Training epoch 1:  59%|#####9    | 3285/5556 [41:09<28:35,  1.32it/s]Training epoch 1:  59%|#####9    | 3286/5556 [41:10<28:42,  1.32it/s]Training epoch 1:  59%|#####9    | 3287/5556 [41:10<28:47,  1.31it/s]Training epoch 1:  59%|#####9    | 3288/5556 [41:11<28:43,  1.32it/s]Training epoch 1:  59%|#####9    | 3289/5556 [41:12<28:42,  1.32it/s]Training epoch 1:  59%|#####9    | 3290/5556 [41:13<28:55,  1.31it/s]Training epoch 1:  59%|#####9    | 3291/5556 [41:13<28:36,  1.32it/s]Training epoch 1:  59%|#####9    | 3292/5556 [41:14<28:20,  1.33it/s]Training epoch 1:  59%|#####9    | 3293/5556 [41:15<28:05,  1.34it/s]Training epoch 1:  59%|#####9    | 3294/5556 [41:16<28:30,  1.32it/s]Training epoch 1:  59%|#####9    | 3295/5556 [41:16<28:33,  1.32it/s]Training epoch 1:  59%|#####9    | 3296/5556 [41:17<28:39,  1.31it/s]Training epoch 1:  59%|#####9    | 3297/5556 [41:18<28:29,  1.32it/s]Training epoch 1:  59%|#####9    | 3298/5556 [41:19<28:36,  1.32it/s]Training epoch 1:  59%|#####9    | 3299/5556 [41:20<28:23,  1.33it/s]Training epoch 1:  59%|#####9    | 3300/5556 [41:20<29:25,  1.28it/s]Training epoch 1:  59%|#####9    | 3301/5556 [41:21<28:59,  1.30it/s]Training epoch 1:  59%|#####9    | 3302/5556 [41:22<28:37,  1.31it/s]Training epoch 1:  59%|#####9    | 3303/5556 [41:23<28:21,  1.32it/s]Training epoch 1:  59%|#####9    | 3304/5556 [41:23<27:53,  1.35it/s]Training epoch 1:  59%|#####9    | 3305/5556 [41:24<27:51,  1.35it/s]Training epoch 1:  60%|#####9    | 3306/5556 [41:25<27:36,  1.36it/s]Training epoch 1:  60%|#####9    | 3307/5556 [41:26<27:52,  1.34it/s]Training epoch 1:  60%|#####9    | 3308/5556 [41:26<27:59,  1.34it/s]Training epoch 1:  60%|#####9    | 3309/5556 [41:27<28:15,  1.33it/s]Training epoch 1:  60%|#####9    | 3310/5556 [41:28<28:20,  1.32it/s]Training epoch 1:  60%|#####9    | 3311/5556 [41:29<28:51,  1.30it/s]Training epoch 1:  60%|#####9    | 3312/5556 [41:29<28:43,  1.30it/s]Training epoch 1:  60%|#####9    | 3313/5556 [41:30<28:33,  1.31it/s]Training epoch 1:  60%|#####9    | 3314/5556 [41:31<28:20,  1.32it/s]Training epoch 1:  60%|#####9    | 3315/5556 [41:32<28:13,  1.32it/s]Training epoch 1:  60%|#####9    | 3316/5556 [41:32<28:06,  1.33it/s]Training epoch 1:  60%|#####9    | 3317/5556 [41:33<27:46,  1.34it/s]Training epoch 1:  60%|#####9    | 3318/5556 [41:34<28:08,  1.33it/s]Training epoch 1:  60%|#####9    | 3319/5556 [41:35<28:17,  1.32it/s]Training epoch 1:  60%|#####9    | 3320/5556 [41:35<28:21,  1.31it/s]Training epoch 1:  60%|#####9    | 3321/5556 [41:36<28:10,  1.32it/s]Training epoch 1:  60%|#####9    | 3322/5556 [41:37<28:07,  1.32it/s]Training epoch 1:  60%|#####9    | 3323/5556 [41:38<28:11,  1.32it/s]Training epoch 1:  60%|#####9    | 3324/5556 [41:38<28:10,  1.32it/s]Training epoch 1:  60%|#####9    | 3325/5556 [41:39<28:01,  1.33it/s]Training epoch 1:  60%|#####9    | 3326/5556 [41:40<27:55,  1.33it/s]Training epoch 1:  60%|#####9    | 3327/5556 [41:41<27:31,  1.35it/s]Training epoch 1:  60%|#####9    | 3328/5556 [41:41<27:45,  1.34it/s]Training epoch 1:  60%|#####9    | 3329/5556 [41:42<27:56,  1.33it/s]Training epoch 1:  60%|#####9    | 3330/5556 [41:43<28:05,  1.32it/s]Training epoch 1:  60%|#####9    | 3331/5556 [41:44<28:10,  1.32it/s]Training epoch 1:  60%|#####9    | 3332/5556 [41:44<28:09,  1.32it/s]Training epoch 1:  60%|#####9    | 3333/5556 [41:45<27:34,  1.34it/s]Training epoch 1:  60%|######    | 3334/5556 [41:46<27:51,  1.33it/s]Training epoch 1:  60%|######    | 3335/5556 [41:47<27:54,  1.33it/s]Training epoch 1:  60%|######    | 3336/5556 [41:47<27:36,  1.34it/s]Training epoch 1:  60%|######    | 3337/5556 [41:48<27:34,  1.34it/s]Training epoch 1:  60%|######    | 3338/5556 [41:49<27:37,  1.34it/s]Training epoch 1:  60%|######    | 3339/5556 [41:50<27:55,  1.32it/s]Training epoch 1:  60%|######    | 3340/5556 [41:50<27:50,  1.33it/s]Training epoch 1:  60%|######    | 3341/5556 [41:51<27:51,  1.32it/s]Training epoch 1:  60%|######    | 3342/5556 [41:52<27:45,  1.33it/s]Training epoch 1:  60%|######    | 3343/5556 [41:53<28:03,  1.31it/s]Training epoch 1:  60%|######    | 3344/5556 [41:53<27:47,  1.33it/s]Training epoch 1:  60%|######    | 3345/5556 [41:54<27:21,  1.35it/s]Training epoch 1:  60%|######    | 3346/5556 [41:55<27:20,  1.35it/s]Training epoch 1:  60%|######    | 3347/5556 [41:56<27:47,  1.32it/s]Training epoch 1:  60%|######    | 3348/5556 [41:56<27:20,  1.35it/s]Training epoch 1:  60%|######    | 3349/5556 [41:57<27:31,  1.34it/s]Training epoch 1:  60%|######    | 3350/5556 [41:58<27:20,  1.34it/s]Training epoch 1:  60%|######    | 3351/5556 [41:59<27:21,  1.34it/s]Training epoch 1:  60%|######    | 3352/5556 [41:59<27:30,  1.34it/s]Training epoch 1:  60%|######    | 3353/5556 [42:00<27:22,  1.34it/s]Training epoch 1:  60%|######    | 3354/5556 [42:01<27:04,  1.36it/s]Training epoch 1:  60%|######    | 3355/5556 [42:02<27:18,  1.34it/s]Training epoch 1:  60%|######    | 3356/5556 [42:02<27:17,  1.34it/s]Training epoch 1:  60%|######    | 3357/5556 [42:03<27:20,  1.34it/s]Training epoch 1:  60%|######    | 3358/5556 [42:04<27:17,  1.34it/s]Training epoch 1:  60%|######    | 3359/5556 [42:05<27:18,  1.34it/s]Training epoch 1:  60%|######    | 3360/5556 [42:05<27:13,  1.34it/s]Training epoch 1:  60%|######    | 3361/5556 [42:06<27:24,  1.33it/s]Training epoch 1:  61%|######    | 3362/5556 [42:07<27:32,  1.33it/s]Training epoch 1:  61%|######    | 3363/5556 [42:08<26:55,  1.36it/s]Training epoch 1:  61%|######    | 3364/5556 [42:08<27:10,  1.34it/s]Training epoch 1:  61%|######    | 3365/5556 [42:09<26:58,  1.35it/s]Training epoch 1:  61%|######    | 3366/5556 [42:10<27:06,  1.35it/s]Training epoch 1:  61%|######    | 3367/5556 [42:11<27:07,  1.34it/s]Training epoch 1:  61%|######    | 3368/5556 [42:11<27:07,  1.34it/s]Training epoch 1:  61%|######    | 3369/5556 [42:12<27:18,  1.33it/s]Training epoch 1:  61%|######    | 3370/5556 [42:13<27:13,  1.34it/s]Training epoch 1:  61%|######    | 3371/5556 [42:14<27:17,  1.33it/s]Training epoch 1:  61%|######    | 3372/5556 [42:14<26:50,  1.36it/s]Training epoch 1:  61%|######    | 3373/5556 [42:15<26:44,  1.36it/s]Training epoch 1:  61%|######    | 3374/5556 [42:16<26:55,  1.35it/s]Training epoch 1:  61%|######    | 3375/5556 [42:17<26:57,  1.35it/s]Training epoch 1:  61%|######    | 3376/5556 [42:17<27:07,  1.34it/s]Training epoch 1:  61%|######    | 3377/5556 [42:18<27:11,  1.34it/s]Training epoch 1:  61%|######    | 3378/5556 [42:19<27:12,  1.33it/s]Training epoch 1:  61%|######    | 3379/5556 [42:20<27:13,  1.33it/s]Training epoch 1:  61%|######    | 3380/5556 [42:20<27:04,  1.34it/s]Training epoch 1:  61%|######    | 3381/5556 [42:21<26:59,  1.34it/s]Training epoch 1:  61%|######    | 3382/5556 [42:22<26:46,  1.35it/s]Training epoch 1:  61%|######    | 3383/5556 [42:23<27:09,  1.33it/s]Training epoch 1:  61%|######    | 3384/5556 [42:23<27:07,  1.33it/s]Training epoch 1:  61%|######    | 3385/5556 [42:24<27:07,  1.33it/s]Training epoch 1:  61%|######    | 3386/5556 [42:25<27:03,  1.34it/s]Training epoch 1:  61%|######    | 3387/5556 [42:25<26:49,  1.35it/s]Training epoch 1:  61%|######    | 3388/5556 [42:26<26:51,  1.35it/s]Training epoch 1:  61%|######    | 3389/5556 [42:27<26:51,  1.34it/s]Training epoch 1:  61%|######1   | 3390/5556 [42:28<26:37,  1.36it/s]Training epoch 1:  61%|######1   | 3391/5556 [42:28<26:35,  1.36it/s]Training epoch 1:  61%|######1   | 3392/5556 [42:29<26:49,  1.34it/s]Training epoch 1:  61%|######1   | 3393/5556 [42:30<26:30,  1.36it/s]Training epoch 1:  61%|######1   | 3394/5556 [42:31<26:19,  1.37it/s]Training epoch 1:  61%|######1   | 3395/5556 [42:31<26:22,  1.37it/s]Training epoch 1:  61%|######1   | 3396/5556 [42:32<26:28,  1.36it/s]Training epoch 1:  61%|######1   | 3397/5556 [42:33<26:54,  1.34it/s]Training epoch 1:  61%|######1   | 3398/5556 [42:34<26:57,  1.33it/s]Training epoch 1:  61%|######1   | 3399/5556 [42:34<27:15,  1.32it/s]Training epoch 1:  61%|######1   | 3400/5556 [42:35<27:52,  1.29it/s]Training epoch 1:  61%|######1   | 3401/5556 [42:36<27:42,  1.30it/s]Training epoch 1:  61%|######1   | 3402/5556 [42:37<27:40,  1.30it/s]Training epoch 1:  61%|######1   | 3403/5556 [42:37<27:07,  1.32it/s]Training epoch 1:  61%|######1   | 3404/5556 [42:38<26:43,  1.34it/s]Training epoch 1:  61%|######1   | 3405/5556 [42:39<27:07,  1.32it/s]Training epoch 1:  61%|######1   | 3406/5556 [42:40<26:50,  1.34it/s]Training epoch 1:  61%|######1   | 3407/5556 [42:40<26:38,  1.34it/s]Training epoch 1:  61%|######1   | 3408/5556 [42:41<26:52,  1.33it/s]Training epoch 1:  61%|######1   | 3409/5556 [42:42<27:12,  1.32it/s]Training epoch 1:  61%|######1   | 3410/5556 [42:43<27:24,  1.31it/s]Training epoch 1:  61%|######1   | 3411/5556 [42:44<27:16,  1.31it/s]Training epoch 1:  61%|######1   | 3412/5556 [42:44<27:30,  1.30it/s]Training epoch 1:  61%|######1   | 3413/5556 [42:45<27:22,  1.30it/s]Training epoch 1:  61%|######1   | 3414/5556 [42:46<27:17,  1.31it/s]Training epoch 1:  61%|######1   | 3415/5556 [42:47<26:50,  1.33it/s]Training epoch 1:  61%|######1   | 3416/5556 [42:47<26:38,  1.34it/s]Training epoch 1:  62%|######1   | 3417/5556 [42:48<26:40,  1.34it/s]Training epoch 1:  62%|######1   | 3418/5556 [42:49<26:53,  1.33it/s]Training epoch 1:  62%|######1   | 3419/5556 [42:50<26:21,  1.35it/s]Training epoch 1:  62%|######1   | 3420/5556 [42:50<26:28,  1.34it/s]Training epoch 1:  62%|######1   | 3421/5556 [42:51<26:38,  1.34it/s]Training epoch 1:  62%|######1   | 3422/5556 [42:52<26:07,  1.36it/s]Training epoch 1:  62%|######1   | 3423/5556 [42:53<26:39,  1.33it/s]Training epoch 1:  62%|######1   | 3424/5556 [42:53<26:48,  1.33it/s]Training epoch 1:  62%|######1   | 3425/5556 [42:54<26:46,  1.33it/s]Training epoch 1:  62%|######1   | 3426/5556 [42:55<26:51,  1.32it/s]Training epoch 1:  62%|######1   | 3427/5556 [42:56<27:04,  1.31it/s]Training epoch 1:  62%|######1   | 3428/5556 [42:56<27:00,  1.31it/s]Training epoch 1:  62%|######1   | 3429/5556 [42:57<26:55,  1.32it/s]Training epoch 1:  62%|######1   | 3430/5556 [42:58<26:52,  1.32it/s]Training epoch 1:  62%|######1   | 3431/5556 [42:59<26:44,  1.32it/s]Training epoch 1:  62%|######1   | 3432/5556 [42:59<26:54,  1.32it/s]Training epoch 1:  62%|######1   | 3433/5556 [43:00<26:29,  1.34it/s]Training epoch 1:  62%|######1   | 3434/5556 [43:01<26:19,  1.34it/s]Training epoch 1:  62%|######1   | 3435/5556 [43:02<26:09,  1.35it/s]Training epoch 1:  62%|######1   | 3436/5556 [43:02<25:50,  1.37it/s]Training epoch 1:  62%|######1   | 3437/5556 [43:03<26:18,  1.34it/s]Training epoch 1:  62%|######1   | 3438/5556 [43:04<26:06,  1.35it/s]Training epoch 1:  62%|######1   | 3439/5556 [43:05<26:22,  1.34it/s]Training epoch 1:  62%|######1   | 3440/5556 [43:05<26:06,  1.35it/s]Training epoch 1:  62%|######1   | 3441/5556 [43:06<26:24,  1.33it/s]Training epoch 1:  62%|######1   | 3442/5556 [43:07<26:26,  1.33it/s]Training epoch 1:  62%|######1   | 3443/5556 [43:08<26:24,  1.33it/s]Training epoch 1:  62%|######1   | 3444/5556 [43:08<26:29,  1.33it/s]Training epoch 1:  62%|######2   | 3445/5556 [43:09<26:04,  1.35it/s]Training epoch 1:  62%|######2   | 3446/5556 [43:10<26:16,  1.34it/s]Training epoch 1:  62%|######2   | 3447/5556 [43:11<26:24,  1.33it/s]Training epoch 1:  62%|######2   | 3448/5556 [43:11<26:17,  1.34it/s]Training epoch 1:  62%|######2   | 3449/5556 [43:12<26:23,  1.33it/s]Training epoch 1:  62%|######2   | 3450/5556 [43:13<26:38,  1.32it/s]Training epoch 1:  62%|######2   | 3451/5556 [43:14<26:28,  1.33it/s]Training epoch 1:  62%|######2   | 3452/5556 [43:14<26:11,  1.34it/s]Training epoch 1:  62%|######2   | 3453/5556 [43:15<26:21,  1.33it/s]Training epoch 1:  62%|######2   | 3454/5556 [43:16<25:44,  1.36it/s]Training epoch 1:  62%|######2   | 3455/5556 [43:16<26:00,  1.35it/s]Training epoch 1:  62%|######2   | 3456/5556 [43:17<26:13,  1.33it/s]Training epoch 1:  62%|######2   | 3457/5556 [43:18<26:07,  1.34it/s]Training epoch 1:  62%|######2   | 3458/5556 [43:19<26:12,  1.33it/s]Training epoch 1:  62%|######2   | 3459/5556 [43:20<26:26,  1.32it/s]Training epoch 1:  62%|######2   | 3460/5556 [43:20<26:37,  1.31it/s]Training epoch 1:  62%|######2   | 3461/5556 [43:21<26:31,  1.32it/s]Training epoch 1:  62%|######2   | 3462/5556 [43:22<26:15,  1.33it/s]Training epoch 1:  62%|######2   | 3463/5556 [43:23<26:20,  1.32it/s]Training epoch 1:  62%|######2   | 3464/5556 [43:23<26:12,  1.33it/s]Training epoch 1:  62%|######2   | 3465/5556 [43:24<26:21,  1.32it/s]Training epoch 1:  62%|######2   | 3466/5556 [43:25<26:22,  1.32it/s]Training epoch 1:  62%|######2   | 3467/5556 [43:26<26:11,  1.33it/s]Training epoch 1:  62%|######2   | 3468/5556 [43:26<26:13,  1.33it/s]Training epoch 1:  62%|######2   | 3469/5556 [43:27<26:17,  1.32it/s]Training epoch 1:  62%|######2   | 3470/5556 [43:28<26:18,  1.32it/s]Training epoch 1:  62%|######2   | 3471/5556 [43:29<26:13,  1.33it/s]Training epoch 1:  62%|######2   | 3472/5556 [43:29<26:10,  1.33it/s]Training epoch 1:  63%|######2   | 3473/5556 [43:30<25:54,  1.34it/s]Training epoch 1:  63%|######2   | 3474/5556 [43:31<25:53,  1.34it/s]Training epoch 1:  63%|######2   | 3475/5556 [43:32<26:26,  1.31it/s]Training epoch 1:  63%|######2   | 3476/5556 [43:32<26:11,  1.32it/s]Training epoch 1:  63%|######2   | 3477/5556 [43:33<26:08,  1.33it/s]Training epoch 1:  63%|######2   | 3478/5556 [43:34<25:52,  1.34it/s]Training epoch 1:  63%|######2   | 3479/5556 [43:35<25:53,  1.34it/s]Training epoch 1:  63%|######2   | 3480/5556 [43:35<25:43,  1.35it/s]Training epoch 1:  63%|######2   | 3481/5556 [43:36<25:54,  1.33it/s]Training epoch 1:  63%|######2   | 3482/5556 [43:37<26:04,  1.33it/s]Training epoch 1:  63%|######2   | 3483/5556 [43:38<26:00,  1.33it/s]Training epoch 1:  63%|######2   | 3484/5556 [43:38<26:02,  1.33it/s]Training epoch 1:  63%|######2   | 3485/5556 [43:39<26:11,  1.32it/s]Training epoch 1:  63%|######2   | 3486/5556 [43:40<26:03,  1.32it/s]Training epoch 1:  63%|######2   | 3487/5556 [43:41<25:41,  1.34it/s]Training epoch 1:  63%|######2   | 3488/5556 [43:41<25:56,  1.33it/s]Training epoch 1:  63%|######2   | 3489/5556 [43:42<26:15,  1.31it/s]Training epoch 1:  63%|######2   | 3490/5556 [43:43<26:11,  1.31it/s]Training epoch 1:  63%|######2   | 3491/5556 [43:44<26:22,  1.30it/s]Training epoch 1:  63%|######2   | 3492/5556 [43:44<26:38,  1.29it/s]Training epoch 1:  63%|######2   | 3493/5556 [43:45<26:37,  1.29it/s]Training epoch 1:  63%|######2   | 3494/5556 [43:46<26:02,  1.32it/s]Training epoch 1:  63%|######2   | 3495/5556 [43:47<26:02,  1.32it/s]Training epoch 1:  63%|######2   | 3496/5556 [43:47<25:43,  1.33it/s]Training epoch 1:  63%|######2   | 3497/5556 [43:48<26:13,  1.31it/s]Training epoch 1:  63%|######2   | 3498/5556 [43:49<25:58,  1.32it/s]Training epoch 1:  63%|######2   | 3499/5556 [43:50<25:58,  1.32it/s]Training epoch 1:  63%|######2   | 3500/5556 [43:51<26:43,  1.28it/s]Training epoch 1:  63%|######3   | 3501/5556 [43:51<26:18,  1.30it/s]Training epoch 1:  63%|######3   | 3502/5556 [43:52<26:16,  1.30it/s]Training epoch 1:  63%|######3   | 3503/5556 [43:53<26:26,  1.29it/s]Training epoch 1:  63%|######3   | 3504/5556 [43:54<26:11,  1.31it/s]Training epoch 1:  63%|######3   | 3505/5556 [43:54<25:52,  1.32it/s]Training epoch 1:  63%|######3   | 3506/5556 [43:55<25:48,  1.32it/s]Training epoch 1:  63%|######3   | 3507/5556 [43:56<25:55,  1.32it/s]Training epoch 1:  63%|######3   | 3508/5556 [43:57<25:41,  1.33it/s]Training epoch 1:  63%|######3   | 3509/5556 [43:57<25:49,  1.32it/s]Training epoch 1:  63%|######3   | 3510/5556 [43:58<25:59,  1.31it/s]Training epoch 1:  63%|######3   | 3511/5556 [43:59<25:53,  1.32it/s]Training epoch 1:  63%|######3   | 3512/5556 [44:00<26:04,  1.31it/s]Training epoch 1:  63%|######3   | 3513/5556 [44:00<25:53,  1.32it/s]Training epoch 1:  63%|######3   | 3514/5556 [44:01<25:52,  1.32it/s]Training epoch 1:  63%|######3   | 3515/5556 [44:02<25:47,  1.32it/s]Training epoch 1:  63%|######3   | 3516/5556 [44:03<26:01,  1.31it/s]Training epoch 1:  63%|######3   | 3517/5556 [44:04<25:57,  1.31it/s]Training epoch 1:  63%|######3   | 3518/5556 [44:04<25:42,  1.32it/s]Training epoch 1:  63%|######3   | 3519/5556 [44:05<25:42,  1.32it/s]Training epoch 1:  63%|######3   | 3520/5556 [44:06<25:34,  1.33it/s]Training epoch 1:  63%|######3   | 3521/5556 [44:06<25:26,  1.33it/s]Training epoch 1:  63%|######3   | 3522/5556 [44:07<25:30,  1.33it/s]Training epoch 1:  63%|######3   | 3523/5556 [44:08<25:00,  1.36it/s]Training epoch 1:  63%|######3   | 3524/5556 [44:09<24:58,  1.36it/s]Training epoch 1:  63%|######3   | 3525/5556 [44:09<25:20,  1.34it/s]Training epoch 1:  63%|######3   | 3526/5556 [44:10<25:19,  1.34it/s]Training epoch 1:  63%|######3   | 3527/5556 [44:11<25:24,  1.33it/s]Training epoch 1:  63%|######3   | 3528/5556 [44:12<25:24,  1.33it/s]Training epoch 1:  64%|######3   | 3529/5556 [44:12<25:22,  1.33it/s]Training epoch 1:  64%|######3   | 3530/5556 [44:13<25:34,  1.32it/s]Training epoch 1:  64%|######3   | 3531/5556 [44:14<25:40,  1.31it/s]Training epoch 1:  64%|######3   | 3532/5556 [44:15<25:38,  1.32it/s]Training epoch 1:  64%|######3   | 3533/5556 [44:16<25:41,  1.31it/s]Training epoch 1:  64%|######3   | 3534/5556 [44:16<25:20,  1.33it/s]Training epoch 1:  64%|######3   | 3535/5556 [44:17<25:16,  1.33it/s]Training epoch 1:  64%|######3   | 3536/5556 [44:18<25:26,  1.32it/s]Training epoch 1:  64%|######3   | 3537/5556 [44:19<25:26,  1.32it/s]Training epoch 1:  64%|######3   | 3538/5556 [44:19<25:22,  1.33it/s]Training epoch 1:  64%|######3   | 3539/5556 [44:20<25:36,  1.31it/s]Training epoch 1:  64%|######3   | 3540/5556 [44:21<25:43,  1.31it/s]Training epoch 1:  64%|######3   | 3541/5556 [44:22<25:13,  1.33it/s]Training epoch 1:  64%|######3   | 3542/5556 [44:22<25:13,  1.33it/s]Training epoch 1:  64%|######3   | 3543/5556 [44:23<25:16,  1.33it/s]Training epoch 1:  64%|######3   | 3544/5556 [44:24<25:11,  1.33it/s]Training epoch 1:  64%|######3   | 3545/5556 [44:25<24:48,  1.35it/s]Training epoch 1:  64%|######3   | 3546/5556 [44:25<24:43,  1.35it/s]Training epoch 1:  64%|######3   | 3547/5556 [44:26<25:02,  1.34it/s]Training epoch 1:  64%|######3   | 3548/5556 [44:27<25:06,  1.33it/s]Training epoch 1:  64%|######3   | 3549/5556 [44:28<25:04,  1.33it/s]Training epoch 1:  64%|######3   | 3550/5556 [44:28<24:57,  1.34it/s]Training epoch 1:  64%|######3   | 3551/5556 [44:29<24:52,  1.34it/s]Training epoch 1:  64%|######3   | 3552/5556 [44:30<24:58,  1.34it/s]Training epoch 1:  64%|######3   | 3553/5556 [44:31<24:58,  1.34it/s]Training epoch 1:  64%|######3   | 3554/5556 [44:31<24:44,  1.35it/s]Training epoch 1:  64%|######3   | 3555/5556 [44:32<25:02,  1.33it/s]Training epoch 1:  64%|######4   | 3556/5556 [44:33<24:54,  1.34it/s]Training epoch 1:  64%|######4   | 3557/5556 [44:34<25:26,  1.31it/s]Training epoch 1:  64%|######4   | 3558/5556 [44:34<25:15,  1.32it/s]Training epoch 1:  64%|######4   | 3559/5556 [44:35<24:54,  1.34it/s]Training epoch 1:  64%|######4   | 3560/5556 [44:36<24:41,  1.35it/s]Training epoch 1:  64%|######4   | 3561/5556 [44:37<24:59,  1.33it/s]Training epoch 1:  64%|######4   | 3562/5556 [44:37<25:06,  1.32it/s]Training epoch 1:  64%|######4   | 3563/5556 [44:38<25:00,  1.33it/s]Training epoch 1:  64%|######4   | 3564/5556 [44:39<25:05,  1.32it/s]Training epoch 1:  64%|######4   | 3565/5556 [44:40<25:01,  1.33it/s]Training epoch 1:  64%|######4   | 3566/5556 [44:40<24:56,  1.33it/s]Training epoch 1:  64%|######4   | 3567/5556 [44:41<25:03,  1.32it/s]Training epoch 1:  64%|######4   | 3568/5556 [44:42<25:04,  1.32it/s]Training epoch 1:  64%|######4   | 3569/5556 [44:43<25:05,  1.32it/s]Training epoch 1:  64%|######4   | 3570/5556 [44:43<25:22,  1.30it/s]Training epoch 1:  64%|######4   | 3571/5556 [44:44<25:27,  1.30it/s]Training epoch 1:  64%|######4   | 3572/5556 [44:45<25:07,  1.32it/s]Training epoch 1:  64%|######4   | 3573/5556 [44:46<25:11,  1.31it/s]Training epoch 1:  64%|######4   | 3574/5556 [44:46<25:09,  1.31it/s]Training epoch 1:  64%|######4   | 3575/5556 [44:47<25:06,  1.32it/s]Training epoch 1:  64%|######4   | 3576/5556 [44:48<25:20,  1.30it/s]Training epoch 1:  64%|######4   | 3577/5556 [44:49<25:04,  1.32it/s]Training epoch 1:  64%|######4   | 3578/5556 [44:49<24:36,  1.34it/s]Training epoch 1:  64%|######4   | 3579/5556 [44:50<24:47,  1.33it/s]Training epoch 1:  64%|######4   | 3580/5556 [44:51<24:39,  1.34it/s]Training epoch 1:  64%|######4   | 3581/5556 [44:52<24:56,  1.32it/s]Training epoch 1:  64%|######4   | 3582/5556 [44:52<24:58,  1.32it/s]Training epoch 1:  64%|######4   | 3583/5556 [44:53<25:00,  1.32it/s]Training epoch 1:  65%|######4   | 3584/5556 [44:54<24:44,  1.33it/s]Training epoch 1:  65%|######4   | 3585/5556 [44:55<24:35,  1.34it/s]Training epoch 1:  65%|######4   | 3586/5556 [44:55<24:49,  1.32it/s]Training epoch 1:  65%|######4   | 3587/5556 [44:56<24:33,  1.34it/s]Training epoch 1:  65%|######4   | 3588/5556 [44:57<24:51,  1.32it/s]Training epoch 1:  65%|######4   | 3589/5556 [44:58<24:40,  1.33it/s]Training epoch 1:  65%|######4   | 3590/5556 [44:58<24:42,  1.33it/s]Training epoch 1:  65%|######4   | 3591/5556 [44:59<25:12,  1.30it/s]Training epoch 1:  65%|######4   | 3592/5556 [45:00<25:15,  1.30it/s]Training epoch 1:  65%|######4   | 3593/5556 [45:01<24:51,  1.32it/s]Training epoch 1:  65%|######4   | 3594/5556 [45:02<24:31,  1.33it/s]Training epoch 1:  65%|######4   | 3595/5556 [45:02<24:21,  1.34it/s]Training epoch 1:  65%|######4   | 3596/5556 [45:03<24:20,  1.34it/s]Training epoch 1:  65%|######4   | 3597/5556 [45:04<24:35,  1.33it/s]Training epoch 1:  65%|######4   | 3598/5556 [45:05<24:33,  1.33it/s]Training epoch 1:  65%|######4   | 3599/5556 [45:05<24:19,  1.34it/s]Training epoch 1:  65%|######4   | 3600/5556 [45:06<25:23,  1.28it/s]Training epoch 1:  65%|######4   | 3601/5556 [45:07<25:28,  1.28it/s]Training epoch 1:  65%|######4   | 3602/5556 [45:08<24:59,  1.30it/s]Training epoch 1:  65%|######4   | 3603/5556 [45:08<25:04,  1.30it/s]Training epoch 1:  65%|######4   | 3604/5556 [45:09<24:50,  1.31it/s]Training epoch 1:  65%|######4   | 3605/5556 [45:10<24:41,  1.32it/s]Training epoch 1:  65%|######4   | 3606/5556 [45:11<24:43,  1.31it/s]Training epoch 1:  65%|######4   | 3607/5556 [45:11<24:28,  1.33it/s]Training epoch 1:  65%|######4   | 3608/5556 [45:12<24:53,  1.30it/s]Training epoch 1:  65%|######4   | 3609/5556 [45:13<24:29,  1.33it/s]Training epoch 1:  65%|######4   | 3610/5556 [45:14<24:25,  1.33it/s]Training epoch 1:  65%|######4   | 3611/5556 [45:14<24:29,  1.32it/s]Training epoch 1:  65%|######5   | 3612/5556 [45:15<24:21,  1.33it/s]Training epoch 1:  65%|######5   | 3613/5556 [45:16<24:12,  1.34it/s]Training epoch 1:  65%|######5   | 3614/5556 [45:17<24:41,  1.31it/s]Training epoch 1:  65%|######5   | 3615/5556 [45:17<24:27,  1.32it/s]Training epoch 1:  65%|######5   | 3616/5556 [45:18<24:28,  1.32it/s]Training epoch 1:  65%|######5   | 3617/5556 [45:19<24:28,  1.32it/s]Training epoch 1:  65%|######5   | 3618/5556 [45:20<24:07,  1.34it/s]Training epoch 1:  65%|######5   | 3619/5556 [45:20<24:11,  1.33it/s]Training epoch 1:  65%|######5   | 3620/5556 [45:21<24:03,  1.34it/s]Training epoch 1:  65%|######5   | 3621/5556 [45:22<24:08,  1.34it/s]Training epoch 1:  65%|######5   | 3622/5556 [45:23<24:13,  1.33it/s]Training epoch 1:  65%|######5   | 3623/5556 [45:23<24:17,  1.33it/s]Training epoch 1:  65%|######5   | 3624/5556 [45:24<24:17,  1.33it/s]Training epoch 1:  65%|######5   | 3625/5556 [45:25<24:09,  1.33it/s]Training epoch 1:  65%|######5   | 3626/5556 [45:26<24:04,  1.34it/s]Training epoch 1:  65%|######5   | 3627/5556 [45:26<23:54,  1.34it/s]Training epoch 1:  65%|######5   | 3628/5556 [45:27<23:43,  1.35it/s]Training epoch 1:  65%|######5   | 3629/5556 [45:28<24:06,  1.33it/s]Training epoch 1:  65%|######5   | 3630/5556 [45:29<24:17,  1.32it/s]Training epoch 1:  65%|######5   | 3631/5556 [45:29<24:23,  1.32it/s]Training epoch 1:  65%|######5   | 3632/5556 [45:30<24:17,  1.32it/s]Training epoch 1:  65%|######5   | 3633/5556 [45:31<24:03,  1.33it/s]Training epoch 1:  65%|######5   | 3634/5556 [45:32<23:57,  1.34it/s]Training epoch 1:  65%|######5   | 3635/5556 [45:32<23:58,  1.34it/s]Training epoch 1:  65%|######5   | 3636/5556 [45:33<23:52,  1.34it/s]Training epoch 1:  65%|######5   | 3637/5556 [45:34<23:58,  1.33it/s]Training epoch 1:  65%|######5   | 3638/5556 [45:35<24:02,  1.33it/s]Training epoch 1:  65%|######5   | 3639/5556 [45:35<23:58,  1.33it/s]Training epoch 1:  66%|######5   | 3640/5556 [45:36<23:56,  1.33it/s]Training epoch 1:  66%|######5   | 3641/5556 [45:37<23:55,  1.33it/s]Training epoch 1:  66%|######5   | 3642/5556 [45:38<24:31,  1.30it/s]Training epoch 1:  66%|######5   | 3643/5556 [45:39<24:25,  1.31it/s]Training epoch 1:  66%|######5   | 3644/5556 [45:39<24:34,  1.30it/s]Training epoch 1:  66%|######5   | 3645/5556 [45:40<24:22,  1.31it/s]Training epoch 1:  66%|######5   | 3646/5556 [45:41<24:19,  1.31it/s]Training epoch 1:  66%|######5   | 3647/5556 [45:42<24:05,  1.32it/s]Training epoch 1:  66%|######5   | 3648/5556 [45:42<24:01,  1.32it/s]Training epoch 1:  66%|######5   | 3649/5556 [45:43<23:54,  1.33it/s]Training epoch 1:  66%|######5   | 3650/5556 [45:44<24:00,  1.32it/s]Training epoch 1:  66%|######5   | 3651/5556 [45:45<24:03,  1.32it/s]Training epoch 1:  66%|######5   | 3652/5556 [45:45<23:34,  1.35it/s]Training epoch 1:  66%|######5   | 3653/5556 [45:46<23:26,  1.35it/s]Training epoch 1:  66%|######5   | 3654/5556 [45:47<23:33,  1.35it/s]Training epoch 1:  66%|######5   | 3655/5556 [45:48<23:37,  1.34it/s]Training epoch 1:  66%|######5   | 3656/5556 [45:48<23:28,  1.35it/s]Training epoch 1:  66%|######5   | 3657/5556 [45:49<23:33,  1.34it/s]Training epoch 1:  66%|######5   | 3658/5556 [45:50<23:26,  1.35it/s]Training epoch 1:  66%|######5   | 3659/5556 [45:51<23:34,  1.34it/s]Training epoch 1:  66%|######5   | 3660/5556 [45:51<23:43,  1.33it/s]Training epoch 1:  66%|######5   | 3661/5556 [45:52<23:37,  1.34it/s]Training epoch 1:  66%|######5   | 3662/5556 [45:53<23:22,  1.35it/s]Training epoch 1:  66%|######5   | 3663/5556 [45:54<23:44,  1.33it/s]Training epoch 1:  66%|######5   | 3664/5556 [45:54<23:45,  1.33it/s]Training epoch 1:  66%|######5   | 3665/5556 [45:55<24:01,  1.31it/s]Training epoch 1:  66%|######5   | 3666/5556 [45:56<23:48,  1.32it/s]Training epoch 1:  66%|######6   | 3667/5556 [45:57<23:42,  1.33it/s]Training epoch 1:  66%|######6   | 3668/5556 [45:57<23:27,  1.34it/s]Training epoch 1:  66%|######6   | 3669/5556 [45:58<23:36,  1.33it/s]Training epoch 1:  66%|######6   | 3670/5556 [45:59<23:57,  1.31it/s]Training epoch 1:  66%|######6   | 3671/5556 [46:00<24:03,  1.31it/s]Training epoch 1:  66%|######6   | 3672/5556 [46:00<24:04,  1.30it/s]Training epoch 1:  66%|######6   | 3673/5556 [46:01<24:21,  1.29it/s]Training epoch 1:  66%|######6   | 3674/5556 [46:02<24:09,  1.30it/s]Training epoch 1:  66%|######6   | 3675/5556 [46:03<23:58,  1.31it/s]Training epoch 1:  66%|######6   | 3676/5556 [46:03<24:07,  1.30it/s]Training epoch 1:  66%|######6   | 3677/5556 [46:04<23:50,  1.31it/s]Training epoch 1:  66%|######6   | 3678/5556 [46:05<23:57,  1.31it/s]Training epoch 1:  66%|######6   | 3679/5556 [46:06<24:04,  1.30it/s]Training epoch 1:  66%|######6   | 3680/5556 [46:06<23:43,  1.32it/s]Training epoch 1:  66%|######6   | 3681/5556 [46:07<23:45,  1.32it/s]Training epoch 1:  66%|######6   | 3682/5556 [46:08<23:30,  1.33it/s]Training epoch 1:  66%|######6   | 3683/5556 [46:09<23:35,  1.32it/s]Training epoch 1:  66%|######6   | 3684/5556 [46:10<23:28,  1.33it/s]Training epoch 1:  66%|######6   | 3685/5556 [46:10<23:30,  1.33it/s]Training epoch 1:  66%|######6   | 3686/5556 [46:11<23:29,  1.33it/s]Training epoch 1:  66%|######6   | 3687/5556 [46:12<23:21,  1.33it/s]Training epoch 1:  66%|######6   | 3688/5556 [46:13<23:26,  1.33it/s]Training epoch 1:  66%|######6   | 3689/5556 [46:13<23:19,  1.33it/s]Training epoch 1:  66%|######6   | 3690/5556 [46:14<23:16,  1.34it/s]Training epoch 1:  66%|######6   | 3691/5556 [46:15<23:26,  1.33it/s]Training epoch 1:  66%|######6   | 3692/5556 [46:16<23:21,  1.33it/s]Training epoch 1:  66%|######6   | 3693/5556 [46:16<23:03,  1.35it/s]Training epoch 1:  66%|######6   | 3694/5556 [46:17<22:52,  1.36it/s]Training epoch 1:  67%|######6   | 3695/5556 [46:18<22:57,  1.35it/s]Training epoch 1:  67%|######6   | 3696/5556 [46:18<23:07,  1.34it/s]Training epoch 1:  67%|######6   | 3697/5556 [46:19<23:12,  1.34it/s]Training epoch 1:  67%|######6   | 3698/5556 [46:20<23:26,  1.32it/s]Training epoch 1:  67%|######6   | 3699/5556 [46:21<23:09,  1.34it/s]Training epoch 1:  67%|######6   | 3700/5556 [46:22<24:14,  1.28it/s]Training epoch 1:  67%|######6   | 3701/5556 [46:22<23:51,  1.30it/s]Training epoch 1:  67%|######6   | 3702/5556 [46:23<23:23,  1.32it/s]Training epoch 1:  67%|######6   | 3703/5556 [46:24<23:28,  1.32it/s]Training epoch 1:  67%|######6   | 3704/5556 [46:25<23:15,  1.33it/s]Training epoch 1:  67%|######6   | 3705/5556 [46:25<23:25,  1.32it/s]Training epoch 1:  67%|######6   | 3706/5556 [46:26<23:35,  1.31it/s]Training epoch 1:  67%|######6   | 3707/5556 [46:27<23:18,  1.32it/s]Training epoch 1:  67%|######6   | 3708/5556 [46:28<23:11,  1.33it/s]Training epoch 1:  67%|######6   | 3709/5556 [46:28<22:55,  1.34it/s]Training epoch 1:  67%|######6   | 3710/5556 [46:29<22:55,  1.34it/s]Training epoch 1:  67%|######6   | 3711/5556 [46:30<22:53,  1.34it/s]Training epoch 1:  67%|######6   | 3712/5556 [46:31<23:15,  1.32it/s]Training epoch 1:  67%|######6   | 3713/5556 [46:31<23:06,  1.33it/s]Training epoch 1:  67%|######6   | 3714/5556 [46:32<23:13,  1.32it/s]Training epoch 1:  67%|######6   | 3715/5556 [46:33<23:18,  1.32it/s]Training epoch 1:  67%|######6   | 3716/5556 [46:34<23:29,  1.31it/s]Training epoch 1:  67%|######6   | 3717/5556 [46:34<23:30,  1.30it/s]Training epoch 1:  67%|######6   | 3718/5556 [46:35<23:15,  1.32it/s]Training epoch 1:  67%|######6   | 3719/5556 [46:36<23:08,  1.32it/s]Training epoch 1:  67%|######6   | 3720/5556 [46:37<22:57,  1.33it/s]Training epoch 1:  67%|######6   | 3721/5556 [46:37<23:03,  1.33it/s]Training epoch 1:  67%|######6   | 3722/5556 [46:38<23:20,  1.31it/s]Training epoch 1:  67%|######7   | 3723/5556 [46:39<22:52,  1.34it/s]Training epoch 1:  67%|######7   | 3724/5556 [46:40<22:59,  1.33it/s]Training epoch 1:  67%|######7   | 3725/5556 [46:40<23:16,  1.31it/s]Training epoch 1:  67%|######7   | 3726/5556 [46:41<23:10,  1.32it/s]Training epoch 1:  67%|######7   | 3727/5556 [46:42<23:30,  1.30it/s]Training epoch 1:  67%|######7   | 3728/5556 [46:43<23:07,  1.32it/s]Training epoch 1:  67%|######7   | 3729/5556 [46:43<22:46,  1.34it/s]Training epoch 1:  67%|######7   | 3730/5556 [46:44<22:36,  1.35it/s]Training epoch 1:  67%|######7   | 3731/5556 [46:45<22:40,  1.34it/s]Training epoch 1:  67%|######7   | 3732/5556 [46:46<22:38,  1.34it/s]Training epoch 1:  67%|######7   | 3733/5556 [46:46<22:34,  1.35it/s]Training epoch 1:  67%|######7   | 3734/5556 [46:47<22:31,  1.35it/s]Training epoch 1:  67%|######7   | 3735/5556 [46:48<22:38,  1.34it/s]Training epoch 1:  67%|######7   | 3736/5556 [46:49<22:42,  1.34it/s]Training epoch 1:  67%|######7   | 3737/5556 [46:49<22:57,  1.32it/s]Training epoch 1:  67%|######7   | 3738/5556 [46:50<22:56,  1.32it/s]Training epoch 1:  67%|######7   | 3739/5556 [46:51<22:32,  1.34it/s]Training epoch 1:  67%|######7   | 3740/5556 [46:52<22:22,  1.35it/s]Training epoch 1:  67%|######7   | 3741/5556 [46:52<22:07,  1.37it/s]Training epoch 1:  67%|######7   | 3742/5556 [46:53<22:26,  1.35it/s]Training epoch 1:  67%|######7   | 3743/5556 [46:54<22:36,  1.34it/s]Training epoch 1:  67%|######7   | 3744/5556 [46:55<22:38,  1.33it/s]Training epoch 1:  67%|######7   | 3745/5556 [46:55<22:35,  1.34it/s]Training epoch 1:  67%|######7   | 3746/5556 [46:56<23:00,  1.31it/s]Training epoch 1:  67%|######7   | 3747/5556 [46:57<22:52,  1.32it/s]Training epoch 1:  67%|######7   | 3748/5556 [46:58<22:58,  1.31it/s]Training epoch 1:  67%|######7   | 3749/5556 [46:58<22:50,  1.32it/s]Training epoch 1:  67%|######7   | 3750/5556 [46:59<22:40,  1.33it/s]Training epoch 1:  68%|######7   | 3751/5556 [47:00<22:33,  1.33it/s]Training epoch 1:  68%|######7   | 3752/5556 [47:01<22:37,  1.33it/s]Training epoch 1:  68%|######7   | 3753/5556 [47:01<22:37,  1.33it/s]Training epoch 1:  68%|######7   | 3754/5556 [47:02<22:41,  1.32it/s]Training epoch 1:  68%|######7   | 3755/5556 [47:03<22:39,  1.32it/s]Training epoch 1:  68%|######7   | 3756/5556 [47:04<22:24,  1.34it/s]Training epoch 1:  68%|######7   | 3757/5556 [47:04<22:18,  1.34it/s]Training epoch 1:  68%|######7   | 3758/5556 [47:05<22:38,  1.32it/s]Training epoch 1:  68%|######7   | 3759/5556 [47:06<22:36,  1.33it/s]Training epoch 1:  68%|######7   | 3760/5556 [47:07<22:36,  1.32it/s]Training epoch 1:  68%|######7   | 3761/5556 [47:07<22:32,  1.33it/s]Training epoch 1:  68%|######7   | 3762/5556 [47:08<22:25,  1.33it/s]Training epoch 1:  68%|######7   | 3763/5556 [47:09<22:14,  1.34it/s]Training epoch 1:  68%|######7   | 3764/5556 [47:10<22:26,  1.33it/s]Training epoch 1:  68%|######7   | 3765/5556 [47:10<22:24,  1.33it/s]Training epoch 1:  68%|######7   | 3766/5556 [47:11<22:17,  1.34it/s]Training epoch 1:  68%|######7   | 3767/5556 [47:12<22:22,  1.33it/s]Training epoch 1:  68%|######7   | 3768/5556 [47:13<22:25,  1.33it/s]Training epoch 1:  68%|######7   | 3769/5556 [47:13<22:22,  1.33it/s]Training epoch 1:  68%|######7   | 3770/5556 [47:14<22:30,  1.32it/s]Training epoch 1:  68%|######7   | 3771/5556 [47:15<22:51,  1.30it/s]Training epoch 1:  68%|######7   | 3772/5556 [47:16<22:30,  1.32it/s]Training epoch 1:  68%|######7   | 3773/5556 [47:17<22:29,  1.32it/s]Training epoch 1:  68%|######7   | 3774/5556 [47:17<22:41,  1.31it/s]Training epoch 1:  68%|######7   | 3775/5556 [47:18<22:32,  1.32it/s]Training epoch 1:  68%|######7   | 3776/5556 [47:19<22:33,  1.32it/s]Training epoch 1:  68%|######7   | 3777/5556 [47:20<22:27,  1.32it/s]Training epoch 1:  68%|######7   | 3778/5556 [47:20<22:13,  1.33it/s]Training epoch 1:  68%|######8   | 3779/5556 [47:21<22:12,  1.33it/s]Training epoch 1:  68%|######8   | 3780/5556 [47:22<22:22,  1.32it/s]Training epoch 1:  68%|######8   | 3781/5556 [47:23<22:18,  1.33it/s]Training epoch 1:  68%|######8   | 3782/5556 [47:23<22:17,  1.33it/s]Training epoch 1:  68%|######8   | 3783/5556 [47:24<22:02,  1.34it/s]Training epoch 1:  68%|######8   | 3784/5556 [47:25<22:25,  1.32it/s]Training epoch 1:  68%|######8   | 3785/5556 [47:26<22:23,  1.32it/s]Training epoch 1:  68%|######8   | 3786/5556 [47:26<22:13,  1.33it/s]Training epoch 1:  68%|######8   | 3787/5556 [47:27<22:04,  1.34it/s]Training epoch 1:  68%|######8   | 3788/5556 [47:28<21:59,  1.34it/s]Training epoch 1:  68%|######8   | 3789/5556 [47:29<22:05,  1.33it/s]Training epoch 1:  68%|######8   | 3790/5556 [47:29<22:07,  1.33it/s]Training epoch 1:  68%|######8   | 3791/5556 [47:30<22:19,  1.32it/s]Training epoch 1:  68%|######8   | 3792/5556 [47:31<22:22,  1.31it/s]Training epoch 1:  68%|######8   | 3793/5556 [47:32<22:36,  1.30it/s]Training epoch 1:  68%|######8   | 3794/5556 [47:32<22:41,  1.29it/s]Training epoch 1:  68%|######8   | 3795/5556 [47:33<22:42,  1.29it/s]Training epoch 1:  68%|######8   | 3796/5556 [47:34<22:29,  1.30it/s]Training epoch 1:  68%|######8   | 3797/5556 [47:35<22:09,  1.32it/s]Training epoch 1:  68%|######8   | 3798/5556 [47:35<22:03,  1.33it/s]Training epoch 1:  68%|######8   | 3799/5556 [47:36<21:41,  1.35it/s]Training epoch 1:  68%|######8   | 3800/5556 [47:37<22:48,  1.28it/s]Training epoch 1:  68%|######8   | 3801/5556 [47:38<22:23,  1.31it/s]Training epoch 1:  68%|######8   | 3802/5556 [47:39<22:13,  1.32it/s]Training epoch 1:  68%|######8   | 3803/5556 [47:39<22:22,  1.31it/s]Training epoch 1:  68%|######8   | 3804/5556 [47:40<22:09,  1.32it/s]Training epoch 1:  68%|######8   | 3805/5556 [47:41<21:54,  1.33it/s]Training epoch 1:  69%|######8   | 3806/5556 [47:41<21:41,  1.34it/s]Training epoch 1:  69%|######8   | 3807/5556 [47:42<21:34,  1.35it/s]Training epoch 1:  69%|######8   | 3808/5556 [47:43<21:32,  1.35it/s]Training epoch 1:  69%|######8   | 3809/5556 [47:44<21:44,  1.34it/s]Training epoch 1:  69%|######8   | 3810/5556 [47:44<21:31,  1.35it/s]Training epoch 1:  69%|######8   | 3811/5556 [47:45<21:34,  1.35it/s]Training epoch 1:  69%|######8   | 3812/5556 [47:46<21:16,  1.37it/s]Training epoch 1:  69%|######8   | 3813/5556 [47:47<21:36,  1.34it/s]Training epoch 1:  69%|######8   | 3814/5556 [47:47<21:34,  1.35it/s]Training epoch 1:  69%|######8   | 3815/5556 [47:48<21:33,  1.35it/s]Training epoch 1:  69%|######8   | 3816/5556 [47:49<21:44,  1.33it/s]Training epoch 1:  69%|######8   | 3817/5556 [47:50<21:36,  1.34it/s]Training epoch 1:  69%|######8   | 3818/5556 [47:50<21:32,  1.34it/s]Training epoch 1:  69%|######8   | 3819/5556 [47:51<21:24,  1.35it/s]Training epoch 1:  69%|######8   | 3820/5556 [47:52<21:42,  1.33it/s]Training epoch 1:  69%|######8   | 3821/5556 [47:53<21:50,  1.32it/s]Training epoch 1:  69%|######8   | 3822/5556 [47:53<22:01,  1.31it/s]Training epoch 1:  69%|######8   | 3823/5556 [47:54<21:40,  1.33it/s]Training epoch 1:  69%|######8   | 3824/5556 [47:55<21:47,  1.32it/s]Training epoch 1:  69%|######8   | 3825/5556 [47:56<21:55,  1.32it/s]Training epoch 1:  69%|######8   | 3826/5556 [47:56<21:49,  1.32it/s]Training epoch 1:  69%|######8   | 3827/5556 [47:57<21:33,  1.34it/s]Training epoch 1:  69%|######8   | 3828/5556 [47:58<21:29,  1.34it/s]Training epoch 1:  69%|######8   | 3829/5556 [47:59<21:27,  1.34it/s]Training epoch 1:  69%|######8   | 3830/5556 [47:59<21:16,  1.35it/s]Training epoch 1:  69%|######8   | 3831/5556 [48:00<21:04,  1.36it/s]Training epoch 1:  69%|######8   | 3832/5556 [48:01<21:21,  1.35it/s]Training epoch 1:  69%|######8   | 3833/5556 [48:02<21:32,  1.33it/s]Training epoch 1:  69%|######9   | 3834/5556 [48:02<21:31,  1.33it/s]Training epoch 1:  69%|######9   | 3835/5556 [48:03<21:37,  1.33it/s]Training epoch 1:  69%|######9   | 3836/5556 [48:04<21:37,  1.33it/s]Training epoch 1:  69%|######9   | 3837/5556 [48:05<21:30,  1.33it/s]Training epoch 1:  69%|######9   | 3838/5556 [48:05<21:30,  1.33it/s]Training epoch 1:  69%|######9   | 3839/5556 [48:06<21:51,  1.31it/s]Training epoch 1:  69%|######9   | 3840/5556 [48:07<21:47,  1.31it/s]Training epoch 1:  69%|######9   | 3841/5556 [48:08<21:37,  1.32it/s]Training epoch 1:  69%|######9   | 3842/5556 [48:08<21:36,  1.32it/s]Training epoch 1:  69%|######9   | 3843/5556 [48:09<21:26,  1.33it/s]Training epoch 1:  69%|######9   | 3844/5556 [48:10<21:21,  1.34it/s]Training epoch 1:  69%|######9   | 3845/5556 [48:11<21:23,  1.33it/s]Training epoch 1:  69%|######9   | 3846/5556 [48:11<21:36,  1.32it/s]Training epoch 1:  69%|######9   | 3847/5556 [48:12<21:19,  1.34it/s]Training epoch 1:  69%|######9   | 3848/5556 [48:13<21:09,  1.35it/s]Training epoch 1:  69%|######9   | 3849/5556 [48:14<21:10,  1.34it/s]Training epoch 1:  69%|######9   | 3850/5556 [48:14<21:16,  1.34it/s]Training epoch 1:  69%|######9   | 3851/5556 [48:15<21:28,  1.32it/s]Training epoch 1:  69%|######9   | 3852/5556 [48:16<21:19,  1.33it/s]Training epoch 1:  69%|######9   | 3853/5556 [48:17<21:27,  1.32it/s]Training epoch 1:  69%|######9   | 3854/5556 [48:17<21:19,  1.33it/s]Training epoch 1:  69%|######9   | 3855/5556 [48:18<21:22,  1.33it/s]Training epoch 1:  69%|######9   | 3856/5556 [48:19<21:15,  1.33it/s]Training epoch 1:  69%|######9   | 3857/5556 [48:20<21:08,  1.34it/s]Training epoch 1:  69%|######9   | 3858/5556 [48:20<21:18,  1.33it/s]Training epoch 1:  69%|######9   | 3859/5556 [48:21<21:14,  1.33it/s]Training epoch 1:  69%|######9   | 3860/5556 [48:22<20:51,  1.36it/s]Training epoch 1:  69%|######9   | 3861/5556 [48:23<20:57,  1.35it/s]Training epoch 1:  70%|######9   | 3862/5556 [48:23<21:07,  1.34it/s]Training epoch 1:  70%|######9   | 3863/5556 [48:24<21:16,  1.33it/s]Training epoch 1:  70%|######9   | 3864/5556 [48:25<21:26,  1.31it/s]Training epoch 1:  70%|######9   | 3865/5556 [48:26<21:28,  1.31it/s]Training epoch 1:  70%|######9   | 3866/5556 [48:26<21:26,  1.31it/s]Training epoch 1:  70%|######9   | 3867/5556 [48:27<21:23,  1.32it/s]Training epoch 1:  70%|######9   | 3868/5556 [48:28<21:18,  1.32it/s]Training epoch 1:  70%|######9   | 3869/5556 [48:29<21:06,  1.33it/s]Training epoch 1:  70%|######9   | 3870/5556 [48:29<21:08,  1.33it/s]Training epoch 1:  70%|######9   | 3871/5556 [48:30<20:57,  1.34it/s]Training epoch 1:  70%|######9   | 3872/5556 [48:31<21:04,  1.33it/s]Training epoch 1:  70%|######9   | 3873/5556 [48:32<20:54,  1.34it/s]Training epoch 1:  70%|######9   | 3874/5556 [48:32<20:38,  1.36it/s]Training epoch 1:  70%|######9   | 3875/5556 [48:33<20:57,  1.34it/s]Training epoch 1:  70%|######9   | 3876/5556 [48:34<21:10,  1.32it/s]Training epoch 1:  70%|######9   | 3877/5556 [48:35<21:11,  1.32it/s]Training epoch 1:  70%|######9   | 3878/5556 [48:36<21:16,  1.31it/s]Training epoch 1:  70%|######9   | 3879/5556 [48:36<21:15,  1.31it/s]Training epoch 1:  70%|######9   | 3880/5556 [48:37<21:05,  1.32it/s]Training epoch 1:  70%|######9   | 3881/5556 [48:38<21:02,  1.33it/s]Training epoch 1:  70%|######9   | 3882/5556 [48:38<20:47,  1.34it/s]Training epoch 1:  70%|######9   | 3883/5556 [48:39<20:49,  1.34it/s]Training epoch 1:  70%|######9   | 3884/5556 [48:40<20:46,  1.34it/s]Training epoch 1:  70%|######9   | 3885/5556 [48:41<20:58,  1.33it/s]Training epoch 1:  70%|######9   | 3886/5556 [48:42<21:02,  1.32it/s]Training epoch 1:  70%|######9   | 3887/5556 [48:42<21:00,  1.32it/s]Training epoch 1:  70%|######9   | 3888/5556 [48:43<21:07,  1.32it/s]Training epoch 1:  70%|######9   | 3889/5556 [48:44<20:57,  1.33it/s]Training epoch 1:  70%|#######   | 3890/5556 [48:45<21:01,  1.32it/s]Training epoch 1:  70%|#######   | 3891/5556 [48:45<20:52,  1.33it/s]Training epoch 1:  70%|#######   | 3892/5556 [48:46<21:05,  1.32it/s]Training epoch 1:  70%|#######   | 3893/5556 [48:47<21:03,  1.32it/s]Training epoch 1:  70%|#######   | 3894/5556 [48:48<21:08,  1.31it/s]Training epoch 1:  70%|#######   | 3895/5556 [48:48<21:05,  1.31it/s]Training epoch 1:  70%|#######   | 3896/5556 [48:49<20:53,  1.32it/s]Training epoch 1:  70%|#######   | 3897/5556 [48:50<20:44,  1.33it/s]Training epoch 1:  70%|#######   | 3898/5556 [48:51<20:36,  1.34it/s]Training epoch 1:  70%|#######   | 3899/5556 [48:51<20:28,  1.35it/s]Training epoch 1:  70%|#######   | 3900/5556 [48:52<21:38,  1.27it/s]Training epoch 1:  70%|#######   | 3901/5556 [48:53<21:28,  1.28it/s]Training epoch 1:  70%|#######   | 3902/5556 [48:54<21:13,  1.30it/s]Training epoch 1:  70%|#######   | 3903/5556 [48:54<21:04,  1.31it/s]Training epoch 1:  70%|#######   | 3904/5556 [48:55<20:37,  1.33it/s]Training epoch 1:  70%|#######   | 3905/5556 [48:56<21:00,  1.31it/s]Training epoch 1:  70%|#######   | 3906/5556 [48:57<20:58,  1.31it/s]Training epoch 1:  70%|#######   | 3907/5556 [48:57<20:49,  1.32it/s]Training epoch 1:  70%|#######   | 3908/5556 [48:58<20:55,  1.31it/s]Training epoch 1:  70%|#######   | 3909/5556 [48:59<20:45,  1.32it/s]Training epoch 1:  70%|#######   | 3910/5556 [49:00<20:29,  1.34it/s]Training epoch 1:  70%|#######   | 3911/5556 [49:01<20:48,  1.32it/s]Training epoch 1:  70%|#######   | 3912/5556 [49:01<20:35,  1.33it/s]Training epoch 1:  70%|#######   | 3913/5556 [49:02<20:31,  1.33it/s]Training epoch 1:  70%|#######   | 3914/5556 [49:03<20:24,  1.34it/s]Training epoch 1:  70%|#######   | 3915/5556 [49:03<20:39,  1.32it/s]Training epoch 1:  70%|#######   | 3916/5556 [49:04<20:38,  1.32it/s]Training epoch 1:  71%|#######   | 3917/5556 [49:05<20:33,  1.33it/s]Training epoch 1:  71%|#######   | 3918/5556 [49:06<20:31,  1.33it/s]Training epoch 1:  71%|#######   | 3919/5556 [49:07<20:34,  1.33it/s]Training epoch 1:  71%|#######   | 3920/5556 [49:07<20:32,  1.33it/s]Training epoch 1:  71%|#######   | 3921/5556 [49:08<20:29,  1.33it/s]Training epoch 1:  71%|#######   | 3922/5556 [49:09<20:36,  1.32it/s]Training epoch 1:  71%|#######   | 3923/5556 [49:10<20:35,  1.32it/s]Training epoch 1:  71%|#######   | 3924/5556 [49:10<20:45,  1.31it/s]Training epoch 1:  71%|#######   | 3925/5556 [49:11<20:25,  1.33it/s]Training epoch 1:  71%|#######   | 3926/5556 [49:12<20:47,  1.31it/s]Training epoch 1:  71%|#######   | 3927/5556 [49:13<20:58,  1.29it/s]Training epoch 1:  71%|#######   | 3928/5556 [49:13<20:47,  1.31it/s]Training epoch 1:  71%|#######   | 3929/5556 [49:14<20:50,  1.30it/s]Training epoch 1:  71%|#######   | 3930/5556 [49:15<20:39,  1.31it/s]Training epoch 1:  71%|#######   | 3931/5556 [49:16<20:40,  1.31it/s]Training epoch 1:  71%|#######   | 3932/5556 [49:16<20:33,  1.32it/s]Training epoch 1:  71%|#######   | 3933/5556 [49:17<20:32,  1.32it/s]Training epoch 1:  71%|#######   | 3934/5556 [49:18<20:30,  1.32it/s]Training epoch 1:  71%|#######   | 3935/5556 [49:19<20:35,  1.31it/s]Training epoch 1:  71%|#######   | 3936/5556 [49:19<20:41,  1.30it/s]Training epoch 1:  71%|#######   | 3937/5556 [49:20<20:48,  1.30it/s]Training epoch 1:  71%|#######   | 3938/5556 [49:21<20:36,  1.31it/s]Training epoch 1:  71%|#######   | 3939/5556 [49:22<20:21,  1.32it/s]Training epoch 1:  71%|#######   | 3940/5556 [49:22<20:13,  1.33it/s]Training epoch 1:  71%|#######   | 3941/5556 [49:23<19:59,  1.35it/s]Training epoch 1:  71%|#######   | 3942/5556 [49:24<19:51,  1.36it/s]Training epoch 1:  71%|#######   | 3943/5556 [49:25<20:01,  1.34it/s]Training epoch 1:  71%|#######   | 3944/5556 [49:25<20:01,  1.34it/s]Training epoch 1:  71%|#######1  | 3945/5556 [49:26<20:00,  1.34it/s]Training epoch 1:  71%|#######1  | 3946/5556 [49:27<20:08,  1.33it/s]Training epoch 1:  71%|#######1  | 3947/5556 [49:28<20:10,  1.33it/s]Training epoch 1:  71%|#######1  | 3948/5556 [49:28<20:16,  1.32it/s]Training epoch 1:  71%|#######1  | 3949/5556 [49:29<20:13,  1.32it/s]Training epoch 1:  71%|#######1  | 3950/5556 [49:30<20:06,  1.33it/s]Training epoch 1:  71%|#######1  | 3951/5556 [49:31<20:16,  1.32it/s]Training epoch 1:  71%|#######1  | 3952/5556 [49:31<20:03,  1.33it/s]Training epoch 1:  71%|#######1  | 3953/5556 [49:32<19:47,  1.35it/s]Training epoch 1:  71%|#######1  | 3954/5556 [49:33<19:48,  1.35it/s]Training epoch 1:  71%|#######1  | 3955/5556 [49:34<20:01,  1.33it/s]Training epoch 1:  71%|#######1  | 3956/5556 [49:34<19:57,  1.34it/s]Training epoch 1:  71%|#######1  | 3957/5556 [49:35<20:05,  1.33it/s]Training epoch 1:  71%|#######1  | 3958/5556 [49:36<20:09,  1.32it/s]Training epoch 1:  71%|#######1  | 3959/5556 [49:37<20:11,  1.32it/s]Training epoch 1:  71%|#######1  | 3960/5556 [49:37<20:09,  1.32it/s]Training epoch 1:  71%|#######1  | 3961/5556 [49:38<19:42,  1.35it/s]Training epoch 1:  71%|#######1  | 3962/5556 [49:39<19:51,  1.34it/s]Training epoch 1:  71%|#######1  | 3963/5556 [49:40<19:47,  1.34it/s]Training epoch 1:  71%|#######1  | 3964/5556 [49:40<20:07,  1.32it/s]Training epoch 1:  71%|#######1  | 3965/5556 [49:41<19:55,  1.33it/s]Training epoch 1:  71%|#######1  | 3966/5556 [49:42<20:03,  1.32it/s]Training epoch 1:  71%|#######1  | 3967/5556 [49:43<20:04,  1.32it/s]Training epoch 1:  71%|#######1  | 3968/5556 [49:43<19:52,  1.33it/s]Training epoch 1:  71%|#######1  | 3969/5556 [49:44<19:58,  1.32it/s]Training epoch 1:  71%|#######1  | 3970/5556 [49:45<19:44,  1.34it/s]Training epoch 1:  71%|#######1  | 3971/5556 [49:46<19:56,  1.32it/s]Training epoch 1:  71%|#######1  | 3972/5556 [49:47<19:58,  1.32it/s]Training epoch 1:  72%|#######1  | 3973/5556 [49:47<19:58,  1.32it/s]Training epoch 1:  72%|#######1  | 3974/5556 [49:48<19:46,  1.33it/s]Training epoch 1:  72%|#######1  | 3975/5556 [49:49<19:44,  1.33it/s]Training epoch 1:  72%|#######1  | 3976/5556 [49:50<19:48,  1.33it/s]Training epoch 1:  72%|#######1  | 3977/5556 [49:50<19:36,  1.34it/s]Training epoch 1:  72%|#######1  | 3978/5556 [49:51<19:39,  1.34it/s]Training epoch 1:  72%|#######1  | 3979/5556 [49:52<19:44,  1.33it/s]Training epoch 1:  72%|#######1  | 3980/5556 [49:52<19:36,  1.34it/s]Training epoch 1:  72%|#######1  | 3981/5556 [49:53<19:39,  1.34it/s]Training epoch 1:  72%|#######1  | 3982/5556 [49:54<19:46,  1.33it/s]Training epoch 1:  72%|#######1  | 3983/5556 [49:55<19:41,  1.33it/s]Training epoch 1:  72%|#######1  | 3984/5556 [49:56<19:38,  1.33it/s]Training epoch 1:  72%|#######1  | 3985/5556 [49:56<19:37,  1.33it/s]Training epoch 1:  72%|#######1  | 3986/5556 [49:57<19:35,  1.34it/s]Training epoch 1:  72%|#######1  | 3987/5556 [49:58<19:43,  1.33it/s]Training epoch 1:  72%|#######1  | 3988/5556 [49:59<19:36,  1.33it/s]Training epoch 1:  72%|#######1  | 3989/5556 [49:59<20:02,  1.30it/s]Training epoch 1:  72%|#######1  | 3990/5556 [50:00<19:50,  1.32it/s]Training epoch 1:  72%|#######1  | 3991/5556 [50:01<19:31,  1.34it/s]Training epoch 1:  72%|#######1  | 3992/5556 [50:02<19:30,  1.34it/s]Training epoch 1:  72%|#######1  | 3993/5556 [50:02<19:19,  1.35it/s]Training epoch 1:  72%|#######1  | 3994/5556 [50:03<19:14,  1.35it/s]Training epoch 1:  72%|#######1  | 3995/5556 [50:04<19:21,  1.34it/s]Training epoch 1:  72%|#######1  | 3996/5556 [50:05<19:37,  1.32it/s]Training epoch 1:  72%|#######1  | 3997/5556 [50:05<19:46,  1.31it/s]Training epoch 1:  72%|#######1  | 3998/5556 [50:06<19:58,  1.30it/s]Training epoch 1:  72%|#######1  | 3999/5556 [50:07<20:06,  1.29it/s]Training epoch 1:  72%|#######1  | 4000/5556 [50:08<20:37,  1.26it/s]Training epoch 1:  72%|#######2  | 4001/5556 [50:08<20:25,  1.27it/s]Training epoch 1:  72%|#######2  | 4002/5556 [50:09<20:19,  1.27it/s]Training epoch 1:  72%|#######2  | 4003/5556 [50:10<20:07,  1.29it/s]Training epoch 1:  72%|#######2  | 4004/5556 [50:11<19:49,  1.30it/s]Training epoch 1:  72%|#######2  | 4005/5556 [50:11<19:33,  1.32it/s]Training epoch 1:  72%|#######2  | 4006/5556 [50:12<19:16,  1.34it/s]Training epoch 1:  72%|#######2  | 4007/5556 [50:13<19:15,  1.34it/s]Training epoch 1:  72%|#######2  | 4008/5556 [50:14<19:11,  1.34it/s]Training epoch 1:  72%|#######2  | 4009/5556 [50:14<19:19,  1.33it/s]Training epoch 1:  72%|#######2  | 4010/5556 [50:15<19:20,  1.33it/s]Training epoch 1:  72%|#######2  | 4011/5556 [50:16<19:27,  1.32it/s]Training epoch 1:  72%|#######2  | 4012/5556 [50:17<19:22,  1.33it/s]Training epoch 1:  72%|#######2  | 4013/5556 [50:17<19:13,  1.34it/s]Training epoch 1:  72%|#######2  | 4014/5556 [50:18<19:00,  1.35it/s]Training epoch 1:  72%|#######2  | 4015/5556 [50:19<18:58,  1.35it/s]Training epoch 1:  72%|#######2  | 4016/5556 [50:20<19:01,  1.35it/s]Training epoch 1:  72%|#######2  | 4017/5556 [50:20<19:03,  1.35it/s]Training epoch 1:  72%|#######2  | 4018/5556 [50:21<19:08,  1.34it/s]Training epoch 1:  72%|#######2  | 4019/5556 [50:22<19:22,  1.32it/s]Training epoch 1:  72%|#######2  | 4020/5556 [50:23<19:17,  1.33it/s]Training epoch 1:  72%|#######2  | 4021/5556 [50:23<19:24,  1.32it/s]Training epoch 1:  72%|#######2  | 4022/5556 [50:24<19:13,  1.33it/s]Training epoch 1:  72%|#######2  | 4023/5556 [50:25<19:20,  1.32it/s]Training epoch 1:  72%|#######2  | 4024/5556 [50:26<19:16,  1.33it/s]Training epoch 1:  72%|#######2  | 4025/5556 [50:26<19:04,  1.34it/s]Training epoch 1:  72%|#######2  | 4026/5556 [50:27<19:14,  1.33it/s]Training epoch 1:  72%|#######2  | 4027/5556 [50:28<18:54,  1.35it/s]Training epoch 1:  72%|#######2  | 4028/5556 [50:29<18:51,  1.35it/s]Training epoch 1:  73%|#######2  | 4029/5556 [50:29<18:46,  1.36it/s]Training epoch 1:  73%|#######2  | 4030/5556 [50:30<19:04,  1.33it/s]Training epoch 1:  73%|#######2  | 4031/5556 [50:31<19:11,  1.32it/s]Training epoch 1:  73%|#######2  | 4032/5556 [50:32<19:08,  1.33it/s]Training epoch 1:  73%|#######2  | 4033/5556 [50:32<19:19,  1.31it/s]Training epoch 1:  73%|#######2  | 4034/5556 [50:33<19:19,  1.31it/s]Training epoch 1:  73%|#######2  | 4035/5556 [50:34<19:18,  1.31it/s]Training epoch 1:  73%|#######2  | 4036/5556 [50:35<19:18,  1.31it/s]Training epoch 1:  73%|#######2  | 4037/5556 [50:36<19:02,  1.33it/s]Training epoch 1:  73%|#######2  | 4038/5556 [50:36<19:10,  1.32it/s]Training epoch 1:  73%|#######2  | 4039/5556 [50:37<19:15,  1.31it/s]Training epoch 1:  73%|#######2  | 4040/5556 [50:38<19:02,  1.33it/s]Training epoch 1:  73%|#######2  | 4041/5556 [50:39<19:17,  1.31it/s]Training epoch 1:  73%|#######2  | 4042/5556 [50:39<18:56,  1.33it/s]Training epoch 1:  73%|#######2  | 4043/5556 [50:40<18:54,  1.33it/s]Training epoch 1:  73%|#######2  | 4044/5556 [50:41<18:46,  1.34it/s]Training epoch 1:  73%|#######2  | 4045/5556 [50:42<18:42,  1.35it/s]Training epoch 1:  73%|#######2  | 4046/5556 [50:42<18:31,  1.36it/s]Training epoch 1:  73%|#######2  | 4047/5556 [50:43<18:20,  1.37it/s]Training epoch 1:  73%|#######2  | 4048/5556 [50:44<18:33,  1.35it/s]Training epoch 1:  73%|#######2  | 4049/5556 [50:44<18:43,  1.34it/s]Training epoch 1:  73%|#######2  | 4050/5556 [50:45<18:47,  1.34it/s]Training epoch 1:  73%|#######2  | 4051/5556 [50:46<19:18,  1.30it/s]Training epoch 1:  73%|#######2  | 4052/5556 [50:47<19:13,  1.30it/s]Training epoch 1:  73%|#######2  | 4053/5556 [50:48<19:01,  1.32it/s]Training epoch 1:  73%|#######2  | 4054/5556 [50:48<18:48,  1.33it/s]Training epoch 1:  73%|#######2  | 4055/5556 [50:49<18:40,  1.34it/s]Training epoch 1:  73%|#######3  | 4056/5556 [50:50<18:40,  1.34it/s]Training epoch 1:  73%|#######3  | 4057/5556 [50:51<18:45,  1.33it/s]Training epoch 1:  73%|#######3  | 4058/5556 [50:51<18:46,  1.33it/s]Training epoch 1:  73%|#######3  | 4059/5556 [50:52<19:04,  1.31it/s]Training epoch 1:  73%|#######3  | 4060/5556 [50:53<19:06,  1.30it/s]Training epoch 1:  73%|#######3  | 4061/5556 [50:54<18:52,  1.32it/s]Training epoch 1:  73%|#######3  | 4062/5556 [50:54<18:43,  1.33it/s]Training epoch 1:  73%|#######3  | 4063/5556 [50:55<18:27,  1.35it/s]Training epoch 1:  73%|#######3  | 4064/5556 [50:56<18:36,  1.34it/s]Training epoch 1:  73%|#######3  | 4065/5556 [50:57<18:38,  1.33it/s]Training epoch 1:  73%|#######3  | 4066/5556 [50:57<18:50,  1.32it/s]Training epoch 1:  73%|#######3  | 4067/5556 [50:58<18:51,  1.32it/s]Training epoch 1:  73%|#######3  | 4068/5556 [50:59<18:45,  1.32it/s]Training epoch 1:  73%|#######3  | 4069/5556 [51:00<18:45,  1.32it/s]Training epoch 1:  73%|#######3  | 4070/5556 [51:00<18:39,  1.33it/s]Training epoch 1:  73%|#######3  | 4071/5556 [51:01<18:31,  1.34it/s]Training epoch 1:  73%|#######3  | 4072/5556 [51:02<18:32,  1.33it/s]Training epoch 1:  73%|#######3  | 4073/5556 [51:03<18:26,  1.34it/s]Training epoch 1:  73%|#######3  | 4074/5556 [51:03<18:18,  1.35it/s]Training epoch 1:  73%|#######3  | 4075/5556 [51:04<18:14,  1.35it/s]Training epoch 1:  73%|#######3  | 4076/5556 [51:05<18:19,  1.35it/s]Training epoch 1:  73%|#######3  | 4077/5556 [51:06<18:25,  1.34it/s]Training epoch 1:  73%|#######3  | 4078/5556 [51:06<18:15,  1.35it/s]Training epoch 1:  73%|#######3  | 4079/5556 [51:07<18:30,  1.33it/s]Training epoch 1:  73%|#######3  | 4080/5556 [51:08<18:50,  1.31it/s]Training epoch 1:  73%|#######3  | 4081/5556 [51:09<18:29,  1.33it/s]Training epoch 1:  73%|#######3  | 4082/5556 [51:09<18:26,  1.33it/s]Training epoch 1:  73%|#######3  | 4083/5556 [51:10<18:17,  1.34it/s]Training epoch 1:  74%|#######3  | 4084/5556 [51:11<18:16,  1.34it/s]Training epoch 1:  74%|#######3  | 4085/5556 [51:12<18:21,  1.34it/s]Training epoch 1:  74%|#######3  | 4086/5556 [51:12<18:29,  1.33it/s]Training epoch 1:  74%|#######3  | 4087/5556 [51:13<18:23,  1.33it/s]Training epoch 1:  74%|#######3  | 4088/5556 [51:14<18:15,  1.34it/s]Training epoch 1:  74%|#######3  | 4089/5556 [51:15<18:14,  1.34it/s]Training epoch 1:  74%|#######3  | 4090/5556 [51:15<18:25,  1.33it/s]Training epoch 1:  74%|#######3  | 4091/5556 [51:16<18:42,  1.31it/s]Training epoch 1:  74%|#######3  | 4092/5556 [51:17<18:41,  1.31it/s]Training epoch 1:  74%|#######3  | 4093/5556 [51:18<18:16,  1.33it/s]Training epoch 1:  74%|#######3  | 4094/5556 [51:18<18:20,  1.33it/s]Training epoch 1:  74%|#######3  | 4095/5556 [51:19<18:19,  1.33it/s]Training epoch 1:  74%|#######3  | 4096/5556 [51:20<18:21,  1.33it/s]Training epoch 1:  74%|#######3  | 4097/5556 [51:21<18:02,  1.35it/s]Training epoch 1:  74%|#######3  | 4098/5556 [51:21<18:07,  1.34it/s]Training epoch 1:  74%|#######3  | 4099/5556 [51:22<18:08,  1.34it/s]Training epoch 1:  74%|#######3  | 4100/5556 [51:23<18:42,  1.30it/s]Training epoch 1:  74%|#######3  | 4101/5556 [51:24<18:33,  1.31it/s]Training epoch 1:  74%|#######3  | 4102/5556 [51:24<18:40,  1.30it/s]Training epoch 1:  74%|#######3  | 4103/5556 [51:25<18:24,  1.32it/s]Training epoch 1:  74%|#######3  | 4104/5556 [51:26<18:25,  1.31it/s]Training epoch 1:  74%|#######3  | 4105/5556 [51:27<18:26,  1.31it/s]Training epoch 1:  74%|#######3  | 4106/5556 [51:28<18:44,  1.29it/s]Training epoch 1:  74%|#######3  | 4107/5556 [51:28<18:29,  1.31it/s]Training epoch 1:  74%|#######3  | 4108/5556 [51:29<18:20,  1.32it/s]Training epoch 1:  74%|#######3  | 4109/5556 [51:30<18:24,  1.31it/s]Training epoch 1:  74%|#######3  | 4110/5556 [51:30<18:08,  1.33it/s]Training epoch 1:  74%|#######3  | 4111/5556 [51:31<18:08,  1.33it/s]Training epoch 1:  74%|#######4  | 4112/5556 [51:32<18:16,  1.32it/s]Training epoch 1:  74%|#######4  | 4113/5556 [51:33<18:27,  1.30it/s]Training epoch 1:  74%|#######4  | 4114/5556 [51:34<18:17,  1.31it/s]Training epoch 1:  74%|#######4  | 4115/5556 [51:34<18:13,  1.32it/s]Training epoch 1:  74%|#######4  | 4116/5556 [51:35<17:49,  1.35it/s]Training epoch 1:  74%|#######4  | 4117/5556 [51:36<17:50,  1.34it/s]Training epoch 1:  74%|#######4  | 4118/5556 [51:36<17:46,  1.35it/s]Training epoch 1:  74%|#######4  | 4119/5556 [51:37<17:47,  1.35it/s]Training epoch 1:  74%|#######4  | 4120/5556 [51:38<17:43,  1.35it/s]Training epoch 1:  74%|#######4  | 4121/5556 [51:39<18:01,  1.33it/s]Training epoch 1:  74%|#######4  | 4122/5556 [51:39<17:50,  1.34it/s]Training epoch 1:  74%|#######4  | 4123/5556 [51:40<17:42,  1.35it/s]Training epoch 1:  74%|#######4  | 4124/5556 [51:41<17:52,  1.33it/s]Training epoch 1:  74%|#######4  | 4125/5556 [51:42<17:46,  1.34it/s]Training epoch 1:  74%|#######4  | 4126/5556 [51:42<17:31,  1.36it/s]Training epoch 1:  74%|#######4  | 4127/5556 [51:43<17:46,  1.34it/s]Training epoch 1:  74%|#######4  | 4128/5556 [51:44<17:54,  1.33it/s]Training epoch 1:  74%|#######4  | 4129/5556 [51:45<18:00,  1.32it/s]Training epoch 1:  74%|#######4  | 4130/5556 [51:46<18:06,  1.31it/s]Training epoch 1:  74%|#######4  | 4131/5556 [51:46<17:59,  1.32it/s]Training epoch 1:  74%|#######4  | 4132/5556 [51:47<17:59,  1.32it/s]Training epoch 1:  74%|#######4  | 4133/5556 [51:48<18:01,  1.32it/s]Training epoch 1:  74%|#######4  | 4134/5556 [51:49<17:56,  1.32it/s]Training epoch 1:  74%|#######4  | 4135/5556 [51:49<17:54,  1.32it/s]Training epoch 1:  74%|#######4  | 4136/5556 [51:50<17:57,  1.32it/s]Training epoch 1:  74%|#######4  | 4137/5556 [51:51<18:08,  1.30it/s]Training epoch 1:  74%|#######4  | 4138/5556 [51:52<18:04,  1.31it/s]Training epoch 1:  74%|#######4  | 4139/5556 [51:52<18:03,  1.31it/s]Training epoch 1:  75%|#######4  | 4140/5556 [51:53<18:10,  1.30it/s]Training epoch 1:  75%|#######4  | 4141/5556 [51:54<18:04,  1.31it/s]Training epoch 1:  75%|#######4  | 4142/5556 [51:55<18:04,  1.30it/s]Training epoch 1:  75%|#######4  | 4143/5556 [51:55<18:08,  1.30it/s]Training epoch 1:  75%|#######4  | 4144/5556 [51:56<17:59,  1.31it/s]Training epoch 1:  75%|#######4  | 4145/5556 [51:57<18:01,  1.30it/s]Training epoch 1:  75%|#######4  | 4146/5556 [51:58<17:48,  1.32it/s]Training epoch 1:  75%|#######4  | 4147/5556 [51:58<17:48,  1.32it/s]Training epoch 1:  75%|#######4  | 4148/5556 [51:59<17:29,  1.34it/s]Training epoch 1:  75%|#######4  | 4149/5556 [52:00<17:34,  1.33it/s]Training epoch 1:  75%|#######4  | 4150/5556 [52:01<17:36,  1.33it/s]Training epoch 1:  75%|#######4  | 4151/5556 [52:01<17:34,  1.33it/s]Training epoch 1:  75%|#######4  | 4152/5556 [52:02<17:26,  1.34it/s]Training epoch 1:  75%|#######4  | 4153/5556 [52:03<17:37,  1.33it/s]Training epoch 1:  75%|#######4  | 4154/5556 [52:04<17:37,  1.33it/s]Training epoch 1:  75%|#######4  | 4155/5556 [52:04<17:40,  1.32it/s]Training epoch 1:  75%|#######4  | 4156/5556 [52:05<17:30,  1.33it/s]Training epoch 1:  75%|#######4  | 4157/5556 [52:06<17:41,  1.32it/s]Training epoch 1:  75%|#######4  | 4158/5556 [52:07<17:30,  1.33it/s]Training epoch 1:  75%|#######4  | 4159/5556 [52:07<17:29,  1.33it/s]Training epoch 1:  75%|#######4  | 4160/5556 [52:08<17:43,  1.31it/s]Training epoch 1:  75%|#######4  | 4161/5556 [52:09<17:37,  1.32it/s]Training epoch 1:  75%|#######4  | 4162/5556 [52:10<17:43,  1.31it/s]Training epoch 1:  75%|#######4  | 4163/5556 [52:11<17:53,  1.30it/s]Training epoch 1:  75%|#######4  | 4164/5556 [52:11<17:45,  1.31it/s]Training epoch 1:  75%|#######4  | 4165/5556 [52:12<17:38,  1.31it/s]Training epoch 1:  75%|#######4  | 4166/5556 [52:13<17:35,  1.32it/s]Training epoch 1:  75%|#######5  | 4167/5556 [52:14<17:24,  1.33it/s]Training epoch 1:  75%|#######5  | 4168/5556 [52:14<17:25,  1.33it/s]Training epoch 1:  75%|#######5  | 4169/5556 [52:15<17:31,  1.32it/s]Training epoch 1:  75%|#######5  | 4170/5556 [52:16<17:43,  1.30it/s]Training epoch 1:  75%|#######5  | 4171/5556 [52:17<17:34,  1.31it/s]Training epoch 1:  75%|#######5  | 4172/5556 [52:17<17:30,  1.32it/s]Training epoch 1:  75%|#######5  | 4173/5556 [52:18<17:19,  1.33it/s]Training epoch 1:  75%|#######5  | 4174/5556 [52:19<17:12,  1.34it/s]Training epoch 1:  75%|#######5  | 4175/5556 [52:20<17:19,  1.33it/s]Training epoch 1:  75%|#######5  | 4176/5556 [52:20<17:17,  1.33it/s]Training epoch 1:  75%|#######5  | 4177/5556 [52:21<17:25,  1.32it/s]Training epoch 1:  75%|#######5  | 4178/5556 [52:22<17:36,  1.30it/s]Training epoch 1:  75%|#######5  | 4179/5556 [52:23<17:23,  1.32it/s]Training epoch 1:  75%|#######5  | 4180/5556 [52:23<17:21,  1.32it/s]Training epoch 1:  75%|#######5  | 4181/5556 [52:24<17:25,  1.32it/s]Training epoch 1:  75%|#######5  | 4182/5556 [52:25<17:17,  1.32it/s]Training epoch 1:  75%|#######5  | 4183/5556 [52:26<17:07,  1.34it/s]Training epoch 1:  75%|#######5  | 4184/5556 [52:26<17:04,  1.34it/s]Training epoch 1:  75%|#######5  | 4185/5556 [52:27<17:05,  1.34it/s]Training epoch 1:  75%|#######5  | 4186/5556 [52:28<17:20,  1.32it/s]Training epoch 1:  75%|#######5  | 4187/5556 [52:29<17:17,  1.32it/s]Training epoch 1:  75%|#######5  | 4188/5556 [52:29<17:22,  1.31it/s]Training epoch 1:  75%|#######5  | 4189/5556 [52:30<17:18,  1.32it/s]Training epoch 1:  75%|#######5  | 4190/5556 [52:31<17:25,  1.31it/s]Training epoch 1:  75%|#######5  | 4191/5556 [52:32<17:26,  1.30it/s]Training epoch 1:  75%|#######5  | 4192/5556 [52:33<17:15,  1.32it/s]Training epoch 1:  75%|#######5  | 4193/5556 [52:33<17:03,  1.33it/s]Training epoch 1:  75%|#######5  | 4194/5556 [52:34<17:20,  1.31it/s]Training epoch 1:  76%|#######5  | 4195/5556 [52:35<17:22,  1.31it/s]Training epoch 1:  76%|#######5  | 4196/5556 [52:36<17:28,  1.30it/s]Training epoch 1:  76%|#######5  | 4197/5556 [52:36<17:38,  1.28it/s]Training epoch 1:  76%|#######5  | 4198/5556 [52:37<17:29,  1.29it/s]Training epoch 1:  76%|#######5  | 4199/5556 [52:38<17:28,  1.29it/s]Training epoch 1:  76%|#######5  | 4200/5556 [52:39<18:19,  1.23it/s]Training epoch 1:  76%|#######5  | 4201/5556 [52:40<17:56,  1.26it/s]Training epoch 1:  76%|#######5  | 4202/5556 [52:40<17:39,  1.28it/s]Training epoch 1:  76%|#######5  | 4203/5556 [52:41<17:27,  1.29it/s]Training epoch 1:  76%|#######5  | 4204/5556 [52:42<17:15,  1.31it/s]Training epoch 1:  76%|#######5  | 4205/5556 [52:43<17:08,  1.31it/s]Training epoch 1:  76%|#######5  | 4206/5556 [52:43<17:03,  1.32it/s]Training epoch 1:  76%|#######5  | 4207/5556 [52:44<16:58,  1.32it/s]Training epoch 1:  76%|#######5  | 4208/5556 [52:45<16:49,  1.34it/s]Training epoch 1:  76%|#######5  | 4209/5556 [52:46<16:59,  1.32it/s]Training epoch 1:  76%|#######5  | 4210/5556 [52:46<17:01,  1.32it/s]Training epoch 1:  76%|#######5  | 4211/5556 [52:47<17:02,  1.32it/s]Training epoch 1:  76%|#######5  | 4212/5556 [52:48<17:04,  1.31it/s]Training epoch 1:  76%|#######5  | 4213/5556 [52:49<16:52,  1.33it/s]Training epoch 1:  76%|#######5  | 4214/5556 [52:49<16:57,  1.32it/s]Training epoch 1:  76%|#######5  | 4215/5556 [52:50<17:02,  1.31it/s]Training epoch 1:  76%|#######5  | 4216/5556 [52:51<16:50,  1.33it/s]Training epoch 1:  76%|#######5  | 4217/5556 [52:52<16:49,  1.33it/s]Training epoch 1:  76%|#######5  | 4218/5556 [52:52<16:52,  1.32it/s]Training epoch 1:  76%|#######5  | 4219/5556 [52:53<16:54,  1.32it/s]Training epoch 1:  76%|#######5  | 4220/5556 [52:54<16:51,  1.32it/s]Training epoch 1:  76%|#######5  | 4221/5556 [52:55<16:48,  1.32it/s]Training epoch 1:  76%|#######5  | 4222/5556 [52:55<16:42,  1.33it/s]Training epoch 1:  76%|#######6  | 4223/5556 [52:56<16:48,  1.32it/s]Training epoch 1:  76%|#######6  | 4224/5556 [52:57<16:41,  1.33it/s]Training epoch 1:  76%|#######6  | 4225/5556 [52:58<16:32,  1.34it/s]Training epoch 1:  76%|#######6  | 4226/5556 [52:58<16:44,  1.32it/s]Training epoch 1:  76%|#######6  | 4227/5556 [52:59<16:37,  1.33it/s]Training epoch 1:  76%|#######6  | 4228/5556 [53:00<16:44,  1.32it/s]Training epoch 1:  76%|#######6  | 4229/5556 [53:01<16:45,  1.32it/s]Training epoch 1:  76%|#######6  | 4230/5556 [53:01<16:29,  1.34it/s]Training epoch 1:  76%|#######6  | 4231/5556 [53:02<16:51,  1.31it/s]Training epoch 1:  76%|#######6  | 4232/5556 [53:03<16:38,  1.33it/s]Training epoch 1:  76%|#######6  | 4233/5556 [53:04<16:45,  1.32it/s]Training epoch 1:  76%|#######6  | 4234/5556 [53:04<16:29,  1.34it/s]Training epoch 1:  76%|#######6  | 4235/5556 [53:05<16:33,  1.33it/s]Training epoch 1:  76%|#######6  | 4236/5556 [53:06<16:32,  1.33it/s]Training epoch 1:  76%|#######6  | 4237/5556 [53:07<16:33,  1.33it/s]Training epoch 1:  76%|#######6  | 4238/5556 [53:07<16:27,  1.33it/s]Training epoch 1:  76%|#######6  | 4239/5556 [53:08<16:27,  1.33it/s]Training epoch 1:  76%|#######6  | 4240/5556 [53:09<16:37,  1.32it/s]Training epoch 1:  76%|#######6  | 4241/5556 [53:10<16:34,  1.32it/s]Training epoch 1:  76%|#######6  | 4242/5556 [53:11<16:33,  1.32it/s]Training epoch 1:  76%|#######6  | 4243/5556 [53:11<16:27,  1.33it/s]Training epoch 1:  76%|#######6  | 4244/5556 [53:12<16:23,  1.33it/s]Training epoch 1:  76%|#######6  | 4245/5556 [53:13<16:28,  1.33it/s]Training epoch 1:  76%|#######6  | 4246/5556 [53:13<16:23,  1.33it/s]Training epoch 1:  76%|#######6  | 4247/5556 [53:14<16:30,  1.32it/s]Training epoch 1:  76%|#######6  | 4248/5556 [53:15<16:32,  1.32it/s]Training epoch 1:  76%|#######6  | 4249/5556 [53:16<16:42,  1.30it/s]Training epoch 1:  76%|#######6  | 4250/5556 [53:17<16:37,  1.31it/s]Training epoch 1:  77%|#######6  | 4251/5556 [53:17<16:37,  1.31it/s]Training epoch 1:  77%|#######6  | 4252/5556 [53:18<16:32,  1.31it/s]Training epoch 1:  77%|#######6  | 4253/5556 [53:19<16:25,  1.32it/s]Training epoch 1:  77%|#######6  | 4254/5556 [53:20<16:20,  1.33it/s]Training epoch 1:  77%|#######6  | 4255/5556 [53:20<16:25,  1.32it/s]Training epoch 1:  77%|#######6  | 4256/5556 [53:21<16:09,  1.34it/s]Training epoch 1:  77%|#######6  | 4257/5556 [53:22<16:14,  1.33it/s]Training epoch 1:  77%|#######6  | 4258/5556 [53:23<16:06,  1.34it/s]Training epoch 1:  77%|#######6  | 4259/5556 [53:23<16:05,  1.34it/s]Training epoch 1:  77%|#######6  | 4260/5556 [53:24<16:17,  1.33it/s]Training epoch 1:  77%|#######6  | 4261/5556 [53:25<16:19,  1.32it/s]Training epoch 1:  77%|#######6  | 4262/5556 [53:26<16:26,  1.31it/s]Training epoch 1:  77%|#######6  | 4263/5556 [53:26<16:22,  1.32it/s]Training epoch 1:  77%|#######6  | 4264/5556 [53:27<16:12,  1.33it/s]Training epoch 1:  77%|#######6  | 4265/5556 [53:28<16:02,  1.34it/s]Training epoch 1:  77%|#######6  | 4266/5556 [53:29<16:01,  1.34it/s]Training epoch 1:  77%|#######6  | 4267/5556 [53:29<16:01,  1.34it/s]Training epoch 1:  77%|#######6  | 4268/5556 [53:30<16:02,  1.34it/s]Training epoch 1:  77%|#######6  | 4269/5556 [53:31<16:12,  1.32it/s]Training epoch 1:  77%|#######6  | 4270/5556 [53:32<16:08,  1.33it/s]Training epoch 1:  77%|#######6  | 4271/5556 [53:32<16:03,  1.33it/s]Training epoch 1:  77%|#######6  | 4272/5556 [53:33<16:13,  1.32it/s]Training epoch 1:  77%|#######6  | 4273/5556 [53:34<16:09,  1.32it/s]Training epoch 1:  77%|#######6  | 4274/5556 [53:35<16:06,  1.33it/s]Training epoch 1:  77%|#######6  | 4275/5556 [53:35<16:08,  1.32it/s]Training epoch 1:  77%|#######6  | 4276/5556 [53:36<16:05,  1.33it/s]Training epoch 1:  77%|#######6  | 4277/5556 [53:37<16:11,  1.32it/s]Training epoch 1:  77%|#######6  | 4278/5556 [53:38<16:04,  1.32it/s]Training epoch 1:  77%|#######7  | 4279/5556 [53:38<15:50,  1.34it/s]Training epoch 1:  77%|#######7  | 4280/5556 [53:39<15:54,  1.34it/s]Training epoch 1:  77%|#######7  | 4281/5556 [53:40<16:11,  1.31it/s]Training epoch 1:  77%|#######7  | 4282/5556 [53:41<16:13,  1.31it/s]Training epoch 1:  77%|#######7  | 4283/5556 [53:41<16:13,  1.31it/s]Training epoch 1:  77%|#######7  | 4284/5556 [53:42<16:12,  1.31it/s]Training epoch 1:  77%|#######7  | 4285/5556 [53:43<16:12,  1.31it/s]Training epoch 1:  77%|#######7  | 4286/5556 [53:44<16:01,  1.32it/s]Training epoch 1:  77%|#######7  | 4287/5556 [53:45<16:09,  1.31it/s]Training epoch 1:  77%|#######7  | 4288/5556 [53:45<15:46,  1.34it/s]Training epoch 1:  77%|#######7  | 4289/5556 [53:46<15:46,  1.34it/s]Training epoch 1:  77%|#######7  | 4290/5556 [53:47<15:50,  1.33it/s]Training epoch 1:  77%|#######7  | 4291/5556 [53:47<15:40,  1.35it/s]Training epoch 1:  77%|#######7  | 4292/5556 [53:48<15:44,  1.34it/s]Training epoch 1:  77%|#######7  | 4293/5556 [53:49<15:47,  1.33it/s]Training epoch 1:  77%|#######7  | 4294/5556 [53:50<15:47,  1.33it/s]Training epoch 1:  77%|#######7  | 4295/5556 [53:50<15:49,  1.33it/s]Training epoch 1:  77%|#######7  | 4296/5556 [53:51<15:43,  1.34it/s]Training epoch 1:  77%|#######7  | 4297/5556 [53:52<15:41,  1.34it/s]Training epoch 1:  77%|#######7  | 4298/5556 [53:53<15:35,  1.34it/s]Training epoch 1:  77%|#######7  | 4299/5556 [53:53<15:34,  1.35it/s]Training epoch 1:  77%|#######7  | 4300/5556 [53:54<16:19,  1.28it/s]Training epoch 1:  77%|#######7  | 4301/5556 [53:55<15:58,  1.31it/s]Training epoch 1:  77%|#######7  | 4302/5556 [53:56<15:58,  1.31it/s]Training epoch 1:  77%|#######7  | 4303/5556 [53:57<15:58,  1.31it/s]Training epoch 1:  77%|#######7  | 4304/5556 [53:57<15:51,  1.32it/s]Training epoch 1:  77%|#######7  | 4305/5556 [53:58<15:40,  1.33it/s]Training epoch 1:  78%|#######7  | 4306/5556 [53:59<15:47,  1.32it/s]Training epoch 1:  78%|#######7  | 4307/5556 [54:00<15:33,  1.34it/s]Training epoch 1:  78%|#######7  | 4308/5556 [54:00<15:35,  1.33it/s]Training epoch 1:  78%|#######7  | 4309/5556 [54:01<15:34,  1.33it/s]Training epoch 1:  78%|#######7  | 4310/5556 [54:02<15:34,  1.33it/s]Training epoch 1:  78%|#######7  | 4311/5556 [54:03<15:38,  1.33it/s]Training epoch 1:  78%|#######7  | 4312/5556 [54:03<15:39,  1.32it/s]Training epoch 1:  78%|#######7  | 4313/5556 [54:04<15:28,  1.34it/s]Training epoch 1:  78%|#######7  | 4314/5556 [54:05<15:28,  1.34it/s]Training epoch 1:  78%|#######7  | 4315/5556 [54:06<15:25,  1.34it/s]Training epoch 1:  78%|#######7  | 4316/5556 [54:06<15:29,  1.33it/s]Training epoch 1:  78%|#######7  | 4317/5556 [54:07<15:27,  1.34it/s]Training epoch 1:  78%|#######7  | 4318/5556 [54:08<15:39,  1.32it/s]Training epoch 1:  78%|#######7  | 4319/5556 [54:09<15:27,  1.33it/s]Training epoch 1:  78%|#######7  | 4320/5556 [54:09<15:37,  1.32it/s]Training epoch 1:  78%|#######7  | 4321/5556 [54:10<15:36,  1.32it/s]Training epoch 1:  78%|#######7  | 4322/5556 [54:11<15:23,  1.34it/s]Training epoch 1:  78%|#######7  | 4323/5556 [54:12<15:20,  1.34it/s]Training epoch 1:  78%|#######7  | 4324/5556 [54:12<15:17,  1.34it/s]Training epoch 1:  78%|#######7  | 4325/5556 [54:13<15:12,  1.35it/s]Training epoch 1:  78%|#######7  | 4326/5556 [54:14<15:14,  1.34it/s]Training epoch 1:  78%|#######7  | 4327/5556 [54:15<15:28,  1.32it/s]Training epoch 1:  78%|#######7  | 4328/5556 [54:15<15:13,  1.34it/s]Training epoch 1:  78%|#######7  | 4329/5556 [54:16<15:29,  1.32it/s]Training epoch 1:  78%|#######7  | 4330/5556 [54:17<15:27,  1.32it/s]Training epoch 1:  78%|#######7  | 4331/5556 [54:18<15:30,  1.32it/s]Training epoch 1:  78%|#######7  | 4332/5556 [54:18<15:12,  1.34it/s]Training epoch 1:  78%|#######7  | 4333/5556 [54:19<15:12,  1.34it/s]Training epoch 1:  78%|#######8  | 4334/5556 [54:20<15:02,  1.35it/s]Training epoch 1:  78%|#######8  | 4335/5556 [54:21<15:14,  1.33it/s]Training epoch 1:  78%|#######8  | 4336/5556 [54:21<15:11,  1.34it/s]Training epoch 1:  78%|#######8  | 4337/5556 [54:22<15:08,  1.34it/s]Training epoch 1:  78%|#######8  | 4338/5556 [54:23<15:22,  1.32it/s]Training epoch 1:  78%|#######8  | 4339/5556 [54:24<15:17,  1.33it/s]Training epoch 1:  78%|#######8  | 4340/5556 [54:24<15:08,  1.34it/s]Training epoch 1:  78%|#######8  | 4341/5556 [54:25<15:06,  1.34it/s]Training epoch 1:  78%|#######8  | 4342/5556 [54:26<15:02,  1.35it/s]Training epoch 1:  78%|#######8  | 4343/5556 [54:27<15:02,  1.34it/s]Training epoch 1:  78%|#######8  | 4344/5556 [54:27<14:53,  1.36it/s]Training epoch 1:  78%|#######8  | 4345/5556 [54:28<14:57,  1.35it/s]Training epoch 1:  78%|#######8  | 4346/5556 [54:29<14:58,  1.35it/s]Training epoch 1:  78%|#######8  | 4347/5556 [54:29<14:57,  1.35it/s]Training epoch 1:  78%|#######8  | 4348/5556 [54:30<15:04,  1.34it/s]Training epoch 1:  78%|#######8  | 4349/5556 [54:31<15:03,  1.34it/s]Training epoch 1:  78%|#######8  | 4350/5556 [54:32<14:54,  1.35it/s]Training epoch 1:  78%|#######8  | 4351/5556 [54:32<15:05,  1.33it/s]Training epoch 1:  78%|#######8  | 4352/5556 [54:33<14:57,  1.34it/s]Training epoch 1:  78%|#######8  | 4353/5556 [54:34<14:59,  1.34it/s]Training epoch 1:  78%|#######8  | 4354/5556 [54:35<14:59,  1.34it/s]Training epoch 1:  78%|#######8  | 4355/5556 [54:35<14:34,  1.37it/s]Training epoch 1:  78%|#######8  | 4356/5556 [54:36<14:42,  1.36it/s]Training epoch 1:  78%|#######8  | 4357/5556 [54:37<14:49,  1.35it/s]Training epoch 1:  78%|#######8  | 4358/5556 [54:38<15:02,  1.33it/s]Training epoch 1:  78%|#######8  | 4359/5556 [54:38<15:02,  1.33it/s]Training epoch 1:  78%|#######8  | 4360/5556 [54:39<14:48,  1.35it/s]Training epoch 1:  78%|#######8  | 4361/5556 [54:40<14:46,  1.35it/s]Training epoch 1:  79%|#######8  | 4362/5556 [54:41<14:49,  1.34it/s]Training epoch 1:  79%|#######8  | 4363/5556 [54:41<14:58,  1.33it/s]Training epoch 1:  79%|#######8  | 4364/5556 [54:42<15:01,  1.32it/s]Training epoch 1:  79%|#######8  | 4365/5556 [54:43<14:55,  1.33it/s]Training epoch 1:  79%|#######8  | 4366/5556 [54:44<14:58,  1.32it/s]Training epoch 1:  79%|#######8  | 4367/5556 [54:44<15:02,  1.32it/s]Training epoch 1:  79%|#######8  | 4368/5556 [54:45<14:59,  1.32it/s]Training epoch 1:  79%|#######8  | 4369/5556 [54:46<14:45,  1.34it/s]Training epoch 1:  79%|#######8  | 4370/5556 [54:47<14:54,  1.33it/s]Training epoch 1:  79%|#######8  | 4371/5556 [54:47<14:57,  1.32it/s]Training epoch 1:  79%|#######8  | 4372/5556 [54:48<14:48,  1.33it/s]Training epoch 1:  79%|#######8  | 4373/5556 [54:49<14:46,  1.33it/s]Training epoch 1:  79%|#######8  | 4374/5556 [54:50<14:52,  1.33it/s]Training epoch 1:  79%|#######8  | 4375/5556 [54:50<14:54,  1.32it/s]Training epoch 1:  79%|#######8  | 4376/5556 [54:51<15:02,  1.31it/s]Training epoch 1:  79%|#######8  | 4377/5556 [54:52<14:57,  1.31it/s]Training epoch 1:  79%|#######8  | 4378/5556 [54:53<15:03,  1.30it/s]Training epoch 1:  79%|#######8  | 4379/5556 [54:54<14:45,  1.33it/s]Training epoch 1:  79%|#######8  | 4380/5556 [54:54<14:52,  1.32it/s]Training epoch 1:  79%|#######8  | 4381/5556 [54:55<14:54,  1.31it/s]Training epoch 1:  79%|#######8  | 4382/5556 [54:56<14:39,  1.33it/s]Training epoch 1:  79%|#######8  | 4383/5556 [54:57<14:34,  1.34it/s]Training epoch 1:  79%|#######8  | 4384/5556 [54:57<14:40,  1.33it/s]Training epoch 1:  79%|#######8  | 4385/5556 [54:58<14:42,  1.33it/s]Training epoch 1:  79%|#######8  | 4386/5556 [54:59<14:38,  1.33it/s]Training epoch 1:  79%|#######8  | 4387/5556 [55:00<14:43,  1.32it/s]Training epoch 1:  79%|#######8  | 4388/5556 [55:00<14:43,  1.32it/s]Training epoch 1:  79%|#######8  | 4389/5556 [55:01<14:30,  1.34it/s]Training epoch 1:  79%|#######9  | 4390/5556 [55:02<14:33,  1.33it/s]Training epoch 1:  79%|#######9  | 4391/5556 [55:03<14:29,  1.34it/s]Training epoch 1:  79%|#######9  | 4392/5556 [55:03<14:36,  1.33it/s]Training epoch 1:  79%|#######9  | 4393/5556 [55:04<14:40,  1.32it/s]Training epoch 1:  79%|#######9  | 4394/5556 [55:05<14:42,  1.32it/s]Training epoch 1:  79%|#######9  | 4395/5556 [55:06<14:44,  1.31it/s]Training epoch 1:  79%|#######9  | 4396/5556 [55:06<14:51,  1.30it/s]Training epoch 1:  79%|#######9  | 4397/5556 [55:07<14:39,  1.32it/s]Training epoch 1:  79%|#######9  | 4398/5556 [55:08<14:43,  1.31it/s]Training epoch 1:  79%|#######9  | 4399/5556 [55:09<14:47,  1.30it/s]Training epoch 1:  79%|#######9  | 4400/5556 [55:10<15:15,  1.26it/s]Training epoch 1:  79%|#######9  | 4401/5556 [55:10<15:06,  1.27it/s]Training epoch 1:  79%|#######9  | 4402/5556 [55:11<14:54,  1.29it/s]Training epoch 1:  79%|#######9  | 4403/5556 [55:12<14:52,  1.29it/s]Training epoch 1:  79%|#######9  | 4404/5556 [55:13<14:39,  1.31it/s]Training epoch 1:  79%|#######9  | 4405/5556 [55:13<14:25,  1.33it/s]Training epoch 1:  79%|#######9  | 4406/5556 [55:14<14:18,  1.34it/s]Training epoch 1:  79%|#######9  | 4407/5556 [55:15<14:10,  1.35it/s]Training epoch 1:  79%|#######9  | 4408/5556 [55:15<14:13,  1.34it/s]Training epoch 1:  79%|#######9  | 4409/5556 [55:16<14:10,  1.35it/s]Training epoch 1:  79%|#######9  | 4410/5556 [55:17<14:13,  1.34it/s]Training epoch 1:  79%|#######9  | 4411/5556 [55:18<14:24,  1.32it/s]Training epoch 1:  79%|#######9  | 4412/5556 [55:18<14:18,  1.33it/s]Training epoch 1:  79%|#######9  | 4413/5556 [55:19<14:23,  1.32it/s]Training epoch 1:  79%|#######9  | 4414/5556 [55:20<14:09,  1.34it/s]Training epoch 1:  79%|#######9  | 4415/5556 [55:21<14:02,  1.35it/s]Training epoch 1:  79%|#######9  | 4416/5556 [55:21<14:11,  1.34it/s]Training epoch 1:  79%|#######9  | 4417/5556 [55:22<14:19,  1.33it/s]Training epoch 1:  80%|#######9  | 4418/5556 [55:23<14:12,  1.33it/s]Training epoch 1:  80%|#######9  | 4419/5556 [55:24<14:13,  1.33it/s]Training epoch 1:  80%|#######9  | 4420/5556 [55:24<14:15,  1.33it/s]Training epoch 1:  80%|#######9  | 4421/5556 [55:25<14:15,  1.33it/s]Training epoch 1:  80%|#######9  | 4422/5556 [55:26<14:13,  1.33it/s]Training epoch 1:  80%|#######9  | 4423/5556 [55:27<14:16,  1.32it/s]Training epoch 1:  80%|#######9  | 4424/5556 [55:27<14:07,  1.34it/s]Training epoch 1:  80%|#######9  | 4425/5556 [55:28<14:13,  1.32it/s]Training epoch 1:  80%|#######9  | 4426/5556 [55:29<14:16,  1.32it/s]Training epoch 1:  80%|#######9  | 4427/5556 [55:30<14:15,  1.32it/s]Training epoch 1:  80%|#######9  | 4428/5556 [55:30<14:02,  1.34it/s]Training epoch 1:  80%|#######9  | 4429/5556 [55:31<14:02,  1.34it/s]Training epoch 1:  80%|#######9  | 4430/5556 [55:32<14:08,  1.33it/s]Training epoch 1:  80%|#######9  | 4431/5556 [55:33<14:02,  1.34it/s]Training epoch 1:  80%|#######9  | 4432/5556 [55:34<14:11,  1.32it/s]Training epoch 1:  80%|#######9  | 4433/5556 [55:34<14:12,  1.32it/s]Training epoch 1:  80%|#######9  | 4434/5556 [55:35<14:13,  1.31it/s]Training epoch 1:  80%|#######9  | 4435/5556 [55:36<14:19,  1.30it/s]Training epoch 1:  80%|#######9  | 4436/5556 [55:37<14:14,  1.31it/s]Training epoch 1:  80%|#######9  | 4437/5556 [55:37<14:01,  1.33it/s]Training epoch 1:  80%|#######9  | 4438/5556 [55:38<13:55,  1.34it/s]Training epoch 1:  80%|#######9  | 4439/5556 [55:39<13:53,  1.34it/s]Training epoch 1:  80%|#######9  | 4440/5556 [55:40<14:03,  1.32it/s]Training epoch 1:  80%|#######9  | 4441/5556 [55:40<13:57,  1.33it/s]Training epoch 1:  80%|#######9  | 4442/5556 [55:41<13:42,  1.35it/s]Training epoch 1:  80%|#######9  | 4443/5556 [55:42<13:41,  1.35it/s]Training epoch 1:  80%|#######9  | 4444/5556 [55:43<13:40,  1.36it/s]Training epoch 1:  80%|########  | 4445/5556 [55:43<13:52,  1.33it/s]Training epoch 1:  80%|########  | 4446/5556 [55:44<13:53,  1.33it/s]Training epoch 1:  80%|########  | 4447/5556 [55:45<14:02,  1.32it/s]Training epoch 1:  80%|########  | 4448/5556 [55:46<14:05,  1.31it/s]Training epoch 1:  80%|########  | 4449/5556 [55:46<13:57,  1.32it/s]Training epoch 1:  80%|########  | 4450/5556 [55:47<13:54,  1.33it/s]Training epoch 1:  80%|########  | 4451/5556 [55:48<14:05,  1.31it/s]Training epoch 1:  80%|########  | 4452/5556 [55:49<14:16,  1.29it/s]Training epoch 1:  80%|########  | 4453/5556 [55:49<14:07,  1.30it/s]Training epoch 1:  80%|########  | 4454/5556 [55:50<14:01,  1.31it/s]Training epoch 1:  80%|########  | 4455/5556 [55:51<13:55,  1.32it/s]Training epoch 1:  80%|########  | 4456/5556 [55:52<14:05,  1.30it/s]Training epoch 1:  80%|########  | 4457/5556 [55:52<14:07,  1.30it/s]Training epoch 1:  80%|########  | 4458/5556 [55:53<13:48,  1.32it/s]Training epoch 1:  80%|########  | 4459/5556 [55:54<13:51,  1.32it/s]Training epoch 1:  80%|########  | 4460/5556 [55:55<13:56,  1.31it/s]Training epoch 1:  80%|########  | 4461/5556 [55:55<13:52,  1.32it/s]Training epoch 1:  80%|########  | 4462/5556 [55:56<13:46,  1.32it/s]Training epoch 1:  80%|########  | 4463/5556 [55:57<14:01,  1.30it/s]Training epoch 1:  80%|########  | 4464/5556 [55:58<14:04,  1.29it/s]Training epoch 1:  80%|########  | 4465/5556 [55:59<14:04,  1.29it/s]Training epoch 1:  80%|########  | 4466/5556 [55:59<13:59,  1.30it/s]Training epoch 1:  80%|########  | 4467/5556 [56:00<13:46,  1.32it/s]Training epoch 1:  80%|########  | 4468/5556 [56:01<13:44,  1.32it/s]Training epoch 1:  80%|########  | 4469/5556 [56:02<13:40,  1.33it/s]Training epoch 1:  80%|########  | 4470/5556 [56:02<13:29,  1.34it/s]Training epoch 1:  80%|########  | 4471/5556 [56:03<13:28,  1.34it/s]Training epoch 1:  80%|########  | 4472/5556 [56:04<13:25,  1.35it/s]Training epoch 1:  81%|########  | 4473/5556 [56:05<13:30,  1.34it/s]Training epoch 1:  81%|########  | 4474/5556 [56:05<13:22,  1.35it/s]Training epoch 1:  81%|########  | 4475/5556 [56:06<13:25,  1.34it/s]Training epoch 1:  81%|########  | 4476/5556 [56:07<13:26,  1.34it/s]Training epoch 1:  81%|########  | 4477/5556 [56:08<13:25,  1.34it/s]Training epoch 1:  81%|########  | 4478/5556 [56:08<13:23,  1.34it/s]Training epoch 1:  81%|########  | 4479/5556 [56:09<13:17,  1.35it/s]Training epoch 1:  81%|########  | 4480/5556 [56:10<13:16,  1.35it/s]Training epoch 1:  81%|########  | 4481/5556 [56:11<13:36,  1.32it/s]Training epoch 1:  81%|########  | 4482/5556 [56:11<13:36,  1.32it/s]Training epoch 1:  81%|########  | 4483/5556 [56:12<13:35,  1.32it/s]Training epoch 1:  81%|########  | 4484/5556 [56:13<13:46,  1.30it/s]Training epoch 1:  81%|########  | 4485/5556 [56:14<13:39,  1.31it/s]Training epoch 1:  81%|########  | 4486/5556 [56:14<13:24,  1.33it/s]Training epoch 1:  81%|########  | 4487/5556 [56:15<13:24,  1.33it/s]Training epoch 1:  81%|########  | 4488/5556 [56:16<13:17,  1.34it/s]Training epoch 1:  81%|########  | 4489/5556 [56:17<13:20,  1.33it/s]Training epoch 1:  81%|########  | 4490/5556 [56:17<13:24,  1.32it/s]Training epoch 1:  81%|########  | 4491/5556 [56:18<13:07,  1.35it/s]Training epoch 1:  81%|########  | 4492/5556 [56:19<13:14,  1.34it/s]Training epoch 1:  81%|########  | 4493/5556 [56:20<13:20,  1.33it/s]Training epoch 1:  81%|########  | 4494/5556 [56:20<13:19,  1.33it/s]Training epoch 1:  81%|########  | 4495/5556 [56:21<13:25,  1.32it/s]Training epoch 1:  81%|########  | 4496/5556 [56:22<13:16,  1.33it/s]Training epoch 1:  81%|########  | 4497/5556 [56:23<13:09,  1.34it/s]Training epoch 1:  81%|########  | 4498/5556 [56:23<13:03,  1.35it/s]Training epoch 1:  81%|########  | 4499/5556 [56:24<13:08,  1.34it/s]Training epoch 1:  81%|########  | 4500/5556 [56:25<13:34,  1.30it/s]Training epoch 1:  81%|########1 | 4501/5556 [56:26<13:36,  1.29it/s]Training epoch 1:  81%|########1 | 4502/5556 [56:26<13:22,  1.31it/s]Training epoch 1:  81%|########1 | 4503/5556 [56:27<13:08,  1.33it/s]Training epoch 1:  81%|########1 | 4504/5556 [56:28<13:10,  1.33it/s]Training epoch 1:  81%|########1 | 4505/5556 [56:29<13:18,  1.32it/s]Training epoch 1:  81%|########1 | 4506/5556 [56:29<13:04,  1.34it/s]Training epoch 1:  81%|########1 | 4507/5556 [56:30<13:04,  1.34it/s]Training epoch 1:  81%|########1 | 4508/5556 [56:31<13:12,  1.32it/s]Training epoch 1:  81%|########1 | 4509/5556 [56:32<13:18,  1.31it/s]Training epoch 1:  81%|########1 | 4510/5556 [56:32<13:10,  1.32it/s]Training epoch 1:  81%|########1 | 4511/5556 [56:33<13:12,  1.32it/s]Training epoch 1:  81%|########1 | 4512/5556 [56:34<13:04,  1.33it/s]Training epoch 1:  81%|########1 | 4513/5556 [56:35<13:13,  1.31it/s]Training epoch 1:  81%|########1 | 4514/5556 [56:36<13:23,  1.30it/s]Training epoch 1:  81%|########1 | 4515/5556 [56:36<13:14,  1.31it/s]Training epoch 1:  81%|########1 | 4516/5556 [56:37<13:11,  1.31it/s]Training epoch 1:  81%|########1 | 4517/5556 [56:38<13:13,  1.31it/s]Training epoch 1:  81%|########1 | 4518/5556 [56:39<13:08,  1.32it/s]Training epoch 1:  81%|########1 | 4519/5556 [56:39<13:00,  1.33it/s]Training epoch 1:  81%|########1 | 4520/5556 [56:40<13:00,  1.33it/s]Training epoch 1:  81%|########1 | 4521/5556 [56:41<12:47,  1.35it/s]Training epoch 1:  81%|########1 | 4522/5556 [56:41<12:53,  1.34it/s]Training epoch 1:  81%|########1 | 4523/5556 [56:42<12:55,  1.33it/s]Training epoch 1:  81%|########1 | 4524/5556 [56:43<13:00,  1.32it/s]Training epoch 1:  81%|########1 | 4525/5556 [56:44<12:59,  1.32it/s]Training epoch 1:  81%|########1 | 4526/5556 [56:45<13:14,  1.30it/s]Training epoch 1:  81%|########1 | 4527/5556 [56:45<12:58,  1.32it/s]Training epoch 1:  81%|########1 | 4528/5556 [56:46<13:00,  1.32it/s]Training epoch 1:  82%|########1 | 4529/5556 [56:47<13:01,  1.31it/s]Training epoch 1:  82%|########1 | 4530/5556 [56:48<12:56,  1.32it/s]Training epoch 1:  82%|########1 | 4531/5556 [56:48<12:57,  1.32it/s]Training epoch 1:  82%|########1 | 4532/5556 [56:49<12:53,  1.32it/s]Training epoch 1:  82%|########1 | 4533/5556 [56:50<12:49,  1.33it/s]Training epoch 1:  82%|########1 | 4534/5556 [56:51<12:46,  1.33it/s]Training epoch 1:  82%|########1 | 4535/5556 [56:51<12:42,  1.34it/s]Training epoch 1:  82%|########1 | 4536/5556 [56:52<12:46,  1.33it/s]Training epoch 1:  82%|########1 | 4537/5556 [56:53<12:41,  1.34it/s]Training epoch 1:  82%|########1 | 4538/5556 [56:54<12:50,  1.32it/s]Training epoch 1:  82%|########1 | 4539/5556 [56:54<12:38,  1.34it/s]Training epoch 1:  82%|########1 | 4540/5556 [56:55<12:45,  1.33it/s]Training epoch 1:  82%|########1 | 4541/5556 [56:56<12:39,  1.34it/s]Training epoch 1:  82%|########1 | 4542/5556 [56:57<12:32,  1.35it/s]Training epoch 1:  82%|########1 | 4543/5556 [56:57<12:30,  1.35it/s]Training epoch 1:  82%|########1 | 4544/5556 [56:58<12:23,  1.36it/s]Training epoch 1:  82%|########1 | 4545/5556 [56:59<12:23,  1.36it/s]Training epoch 1:  82%|########1 | 4546/5556 [57:00<12:39,  1.33it/s]Training epoch 1:  82%|########1 | 4547/5556 [57:00<12:46,  1.32it/s]Training epoch 1:  82%|########1 | 4548/5556 [57:01<12:33,  1.34it/s]Training epoch 1:  82%|########1 | 4549/5556 [57:02<12:32,  1.34it/s]Training epoch 1:  82%|########1 | 4550/5556 [57:03<12:31,  1.34it/s]Training epoch 1:  82%|########1 | 4551/5556 [57:03<12:30,  1.34it/s]Training epoch 1:  82%|########1 | 4552/5556 [57:04<12:30,  1.34it/s]Training epoch 1:  82%|########1 | 4553/5556 [57:05<12:36,  1.33it/s]Training epoch 1:  82%|########1 | 4554/5556 [57:06<12:32,  1.33it/s]Training epoch 1:  82%|########1 | 4555/5556 [57:06<12:29,  1.34it/s]Training epoch 1:  82%|########2 | 4556/5556 [57:07<12:26,  1.34it/s]Training epoch 1:  82%|########2 | 4557/5556 [57:08<12:29,  1.33it/s]Training epoch 1:  82%|########2 | 4558/5556 [57:09<12:25,  1.34it/s]Training epoch 1:  82%|########2 | 4559/5556 [57:09<12:31,  1.33it/s]Training epoch 1:  82%|########2 | 4560/5556 [57:10<12:29,  1.33it/s]Training epoch 1:  82%|########2 | 4561/5556 [57:11<12:25,  1.34it/s]Training epoch 1:  82%|########2 | 4562/5556 [57:12<12:29,  1.33it/s]Training epoch 1:  82%|########2 | 4563/5556 [57:12<12:25,  1.33it/s]Training epoch 1:  82%|########2 | 4564/5556 [57:13<12:25,  1.33it/s]Training epoch 1:  82%|########2 | 4565/5556 [57:14<12:28,  1.32it/s]Training epoch 1:  82%|########2 | 4566/5556 [57:15<12:38,  1.31it/s]Training epoch 1:  82%|########2 | 4567/5556 [57:15<12:29,  1.32it/s]Training epoch 1:  82%|########2 | 4568/5556 [57:16<12:34,  1.31it/s]Training epoch 1:  82%|########2 | 4569/5556 [57:17<12:35,  1.31it/s]Training epoch 1:  82%|########2 | 4570/5556 [57:18<12:30,  1.31it/s]Training epoch 1:  82%|########2 | 4571/5556 [57:18<12:19,  1.33it/s]Training epoch 1:  82%|########2 | 4572/5556 [57:19<12:11,  1.34it/s]Training epoch 1:  82%|########2 | 4573/5556 [57:20<12:18,  1.33it/s]Training epoch 1:  82%|########2 | 4574/5556 [57:21<12:10,  1.34it/s]Training epoch 1:  82%|########2 | 4575/5556 [57:21<12:10,  1.34it/s]Training epoch 1:  82%|########2 | 4576/5556 [57:22<12:12,  1.34it/s]Training epoch 1:  82%|########2 | 4577/5556 [57:23<12:10,  1.34it/s]Training epoch 1:  82%|########2 | 4578/5556 [57:24<12:09,  1.34it/s]Training epoch 1:  82%|########2 | 4579/5556 [57:24<12:03,  1.35it/s]Training epoch 1:  82%|########2 | 4580/5556 [57:25<12:12,  1.33it/s]Training epoch 1:  82%|########2 | 4581/5556 [57:26<12:19,  1.32it/s]Training epoch 1:  82%|########2 | 4582/5556 [57:27<12:14,  1.33it/s]Training epoch 1:  82%|########2 | 4583/5556 [57:27<12:11,  1.33it/s]Training epoch 1:  83%|########2 | 4584/5556 [57:28<12:12,  1.33it/s]Training epoch 1:  83%|########2 | 4585/5556 [57:29<12:27,  1.30it/s]Training epoch 1:  83%|########2 | 4586/5556 [57:30<12:06,  1.34it/s]Training epoch 1:  83%|########2 | 4587/5556 [57:30<12:13,  1.32it/s]Training epoch 1:  83%|########2 | 4588/5556 [57:31<12:05,  1.33it/s]Training epoch 1:  83%|########2 | 4589/5556 [57:32<12:05,  1.33it/s]Training epoch 1:  83%|########2 | 4590/5556 [57:33<11:59,  1.34it/s]Training epoch 1:  83%|########2 | 4591/5556 [57:33<12:06,  1.33it/s]Training epoch 1:  83%|########2 | 4592/5556 [57:34<12:04,  1.33it/s]Training epoch 1:  83%|########2 | 4593/5556 [57:35<12:13,  1.31it/s]Training epoch 1:  83%|########2 | 4594/5556 [57:36<12:15,  1.31it/s]Training epoch 1:  83%|########2 | 4595/5556 [57:36<12:05,  1.32it/s]Training epoch 1:  83%|########2 | 4596/5556 [57:37<12:10,  1.31it/s]Training epoch 1:  83%|########2 | 4597/5556 [57:38<12:03,  1.33it/s]Training epoch 1:  83%|########2 | 4598/5556 [57:39<12:00,  1.33it/s]Training epoch 1:  83%|########2 | 4599/5556 [57:39<11:52,  1.34it/s]Training epoch 1:  83%|########2 | 4600/5556 [57:40<12:13,  1.30it/s]Training epoch 1:  83%|########2 | 4601/5556 [57:41<12:10,  1.31it/s]Training epoch 1:  83%|########2 | 4602/5556 [57:42<11:59,  1.33it/s]Training epoch 1:  83%|########2 | 4603/5556 [57:42<12:03,  1.32it/s]Training epoch 1:  83%|########2 | 4604/5556 [57:43<12:09,  1.31it/s]Training epoch 1:  83%|########2 | 4605/5556 [57:44<11:56,  1.33it/s]Training epoch 1:  83%|########2 | 4606/5556 [57:45<12:03,  1.31it/s]Training epoch 1:  83%|########2 | 4607/5556 [57:46<12:00,  1.32it/s]Training epoch 1:  83%|########2 | 4608/5556 [57:46<12:01,  1.31it/s]Training epoch 1:  83%|########2 | 4609/5556 [57:47<12:04,  1.31it/s]Training epoch 1:  83%|########2 | 4610/5556 [57:48<12:01,  1.31it/s]Training epoch 1:  83%|########2 | 4611/5556 [57:49<12:01,  1.31it/s]Training epoch 1:  83%|########3 | 4612/5556 [57:49<12:00,  1.31it/s]Training epoch 1:  83%|########3 | 4613/5556 [57:50<11:55,  1.32it/s]Training epoch 1:  83%|########3 | 4614/5556 [57:51<11:45,  1.33it/s]Training epoch 1:  83%|########3 | 4615/5556 [57:52<11:40,  1.34it/s]Training epoch 1:  83%|########3 | 4616/5556 [57:52<11:41,  1.34it/s]Training epoch 1:  83%|########3 | 4617/5556 [57:53<11:47,  1.33it/s]Training epoch 1:  83%|########3 | 4618/5556 [57:54<11:49,  1.32it/s]Training epoch 1:  83%|########3 | 4619/5556 [57:55<11:51,  1.32it/s]Training epoch 1:  83%|########3 | 4620/5556 [57:55<11:45,  1.33it/s]Training epoch 1:  83%|########3 | 4621/5556 [57:56<11:47,  1.32it/s]Training epoch 1:  83%|########3 | 4622/5556 [57:57<11:36,  1.34it/s]Training epoch 1:  83%|########3 | 4623/5556 [57:58<11:39,  1.33it/s]Training epoch 1:  83%|########3 | 4624/5556 [57:58<11:47,  1.32it/s]Training epoch 1:  83%|########3 | 4625/5556 [57:59<11:45,  1.32it/s]Training epoch 1:  83%|########3 | 4626/5556 [58:00<11:45,  1.32it/s]Training epoch 1:  83%|########3 | 4627/5556 [58:01<11:42,  1.32it/s]Training epoch 1:  83%|########3 | 4628/5556 [58:01<11:37,  1.33it/s]Training epoch 1:  83%|########3 | 4629/5556 [58:02<11:42,  1.32it/s]Training epoch 1:  83%|########3 | 4630/5556 [58:03<11:43,  1.32it/s]Training epoch 1:  83%|########3 | 4631/5556 [58:04<11:36,  1.33it/s]Training epoch 1:  83%|########3 | 4632/5556 [58:04<11:32,  1.33it/s]Training epoch 1:  83%|########3 | 4633/5556 [58:05<11:39,  1.32it/s]Training epoch 1:  83%|########3 | 4634/5556 [58:06<11:38,  1.32it/s]Training epoch 1:  83%|########3 | 4635/5556 [58:07<11:35,  1.33it/s]Training epoch 1:  83%|########3 | 4636/5556 [58:07<11:25,  1.34it/s]Training epoch 1:  83%|########3 | 4637/5556 [58:08<11:31,  1.33it/s]Training epoch 1:  83%|########3 | 4638/5556 [58:09<11:40,  1.31it/s]Training epoch 1:  83%|########3 | 4639/5556 [58:10<11:47,  1.30it/s]Training epoch 1:  84%|########3 | 4640/5556 [58:11<11:41,  1.30it/s]Training epoch 1:  84%|########3 | 4641/5556 [58:11<11:43,  1.30it/s]Training epoch 1:  84%|########3 | 4642/5556 [58:12<11:47,  1.29it/s]Training epoch 1:  84%|########3 | 4643/5556 [58:13<11:42,  1.30it/s]Training epoch 1:  84%|########3 | 4644/5556 [58:14<11:39,  1.30it/s]Training epoch 1:  84%|########3 | 4645/5556 [58:14<11:32,  1.32it/s]Training epoch 1:  84%|########3 | 4646/5556 [58:15<11:24,  1.33it/s]Training epoch 1:  84%|########3 | 4647/5556 [58:16<11:24,  1.33it/s]Training epoch 1:  84%|########3 | 4648/5556 [58:17<11:16,  1.34it/s]Training epoch 1:  84%|########3 | 4649/5556 [58:17<11:22,  1.33it/s]Training epoch 1:  84%|########3 | 4650/5556 [58:18<11:23,  1.32it/s]Training epoch 1:  84%|########3 | 4651/5556 [58:19<11:25,  1.32it/s]Training epoch 1:  84%|########3 | 4652/5556 [58:20<11:35,  1.30it/s]Training epoch 1:  84%|########3 | 4653/5556 [58:20<11:32,  1.30it/s]Training epoch 1:  84%|########3 | 4654/5556 [58:21<11:25,  1.32it/s]Training epoch 1:  84%|########3 | 4655/5556 [58:22<11:25,  1.31it/s]Training epoch 1:  84%|########3 | 4656/5556 [58:23<11:24,  1.32it/s]Training epoch 1:  84%|########3 | 4657/5556 [58:23<11:11,  1.34it/s]Training epoch 1:  84%|########3 | 4658/5556 [58:24<11:08,  1.34it/s]Training epoch 1:  84%|########3 | 4659/5556 [58:25<11:20,  1.32it/s]Training epoch 1:  84%|########3 | 4660/5556 [58:26<11:14,  1.33it/s]Training epoch 1:  84%|########3 | 4661/5556 [58:26<11:13,  1.33it/s]Training epoch 1:  84%|########3 | 4662/5556 [58:27<11:18,  1.32it/s]Training epoch 1:  84%|########3 | 4663/5556 [58:28<11:13,  1.33it/s]Training epoch 1:  84%|########3 | 4664/5556 [58:29<11:14,  1.32it/s]Training epoch 1:  84%|########3 | 4665/5556 [58:29<11:03,  1.34it/s]Training epoch 1:  84%|########3 | 4666/5556 [58:30<11:08,  1.33it/s]Training epoch 1:  84%|########3 | 4667/5556 [58:31<11:03,  1.34it/s]Training epoch 1:  84%|########4 | 4668/5556 [58:32<11:12,  1.32it/s]Training epoch 1:  84%|########4 | 4669/5556 [58:32<11:05,  1.33it/s]Training epoch 1:  84%|########4 | 4670/5556 [58:33<11:17,  1.31it/s]Training epoch 1:  84%|########4 | 4671/5556 [58:34<11:17,  1.31it/s]Training epoch 1:  84%|########4 | 4672/5556 [58:35<11:02,  1.33it/s]Training epoch 1:  84%|########4 | 4673/5556 [58:35<11:07,  1.32it/s]Training epoch 1:  84%|########4 | 4674/5556 [58:36<11:08,  1.32it/s]Training epoch 1:  84%|########4 | 4675/5556 [58:37<11:06,  1.32it/s]Training epoch 1:  84%|########4 | 4676/5556 [58:38<10:54,  1.35it/s]Training epoch 1:  84%|########4 | 4677/5556 [58:38<10:57,  1.34it/s]Training epoch 1:  84%|########4 | 4678/5556 [58:39<11:12,  1.30it/s]Training epoch 1:  84%|########4 | 4679/5556 [58:40<11:20,  1.29it/s]Training epoch 1:  84%|########4 | 4680/5556 [58:41<11:14,  1.30it/s]Training epoch 1:  84%|########4 | 4681/5556 [58:42<11:12,  1.30it/s]Training epoch 1:  84%|########4 | 4682/5556 [58:42<11:01,  1.32it/s]Training epoch 1:  84%|########4 | 4683/5556 [58:43<11:03,  1.32it/s]Training epoch 1:  84%|########4 | 4684/5556 [58:44<11:07,  1.31it/s]Training epoch 1:  84%|########4 | 4685/5556 [58:45<10:56,  1.33it/s]Training epoch 1:  84%|########4 | 4686/5556 [58:45<10:44,  1.35it/s]Training epoch 1:  84%|########4 | 4687/5556 [58:46<10:47,  1.34it/s]Training epoch 1:  84%|########4 | 4688/5556 [58:47<10:45,  1.34it/s]Training epoch 1:  84%|########4 | 4689/5556 [58:48<10:42,  1.35it/s]Training epoch 1:  84%|########4 | 4690/5556 [58:48<10:43,  1.35it/s]Training epoch 1:  84%|########4 | 4691/5556 [58:49<10:42,  1.35it/s]Training epoch 1:  84%|########4 | 4692/5556 [58:50<10:49,  1.33it/s]Training epoch 1:  84%|########4 | 4693/5556 [58:51<10:45,  1.34it/s]Training epoch 1:  84%|########4 | 4694/5556 [58:51<10:39,  1.35it/s]Training epoch 1:  85%|########4 | 4695/5556 [58:52<10:49,  1.33it/s]Training epoch 1:  85%|########4 | 4696/5556 [58:53<10:36,  1.35it/s]Training epoch 1:  85%|########4 | 4697/5556 [58:53<10:38,  1.35it/s]Training epoch 1:  85%|########4 | 4698/5556 [58:54<10:40,  1.34it/s]Training epoch 1:  85%|########4 | 4699/5556 [58:55<10:38,  1.34it/s]Training epoch 1:  85%|########4 | 4700/5556 [58:56<11:01,  1.29it/s]Training epoch 1:  85%|########4 | 4701/5556 [58:57<10:54,  1.31it/s]Training epoch 1:  85%|########4 | 4702/5556 [58:57<10:54,  1.30it/s]Training epoch 1:  85%|########4 | 4703/5556 [58:58<10:51,  1.31it/s]Training epoch 1:  85%|########4 | 4704/5556 [58:59<10:53,  1.30it/s]Training epoch 1:  85%|########4 | 4705/5556 [59:00<10:46,  1.32it/s]Training epoch 1:  85%|########4 | 4706/5556 [59:00<10:35,  1.34it/s]Training epoch 1:  85%|########4 | 4707/5556 [59:01<10:38,  1.33it/s]Training epoch 1:  85%|########4 | 4708/5556 [59:02<10:48,  1.31it/s]Training epoch 1:  85%|########4 | 4709/5556 [59:03<10:55,  1.29it/s]Training epoch 1:  85%|########4 | 4710/5556 [59:03<11:01,  1.28it/s]Training epoch 1:  85%|########4 | 4711/5556 [59:04<10:49,  1.30it/s]Training epoch 1:  85%|########4 | 4712/5556 [59:05<10:49,  1.30it/s]Training epoch 1:  85%|########4 | 4713/5556 [59:06<10:47,  1.30it/s]Training epoch 1:  85%|########4 | 4714/5556 [59:06<10:37,  1.32it/s]Training epoch 1:  85%|########4 | 4715/5556 [59:07<10:35,  1.32it/s]Training epoch 1:  85%|########4 | 4716/5556 [59:08<10:35,  1.32it/s]Training epoch 1:  85%|########4 | 4717/5556 [59:09<10:27,  1.34it/s]Training epoch 1:  85%|########4 | 4718/5556 [59:09<10:25,  1.34it/s]Training epoch 1:  85%|########4 | 4719/5556 [59:10<10:31,  1.33it/s]Training epoch 1:  85%|########4 | 4720/5556 [59:11<10:34,  1.32it/s]Training epoch 1:  85%|########4 | 4721/5556 [59:12<10:35,  1.31it/s]Training epoch 1:  85%|########4 | 4722/5556 [59:13<10:31,  1.32it/s]Training epoch 1:  85%|########5 | 4723/5556 [59:13<10:35,  1.31it/s]Training epoch 1:  85%|########5 | 4724/5556 [59:14<10:29,  1.32it/s]Training epoch 1:  85%|########5 | 4725/5556 [59:15<10:22,  1.33it/s]Training epoch 1:  85%|########5 | 4726/5556 [59:16<10:15,  1.35it/s]Training epoch 1:  85%|########5 | 4727/5556 [59:16<10:19,  1.34it/s]Training epoch 1:  85%|########5 | 4728/5556 [59:17<10:22,  1.33it/s]Training epoch 1:  85%|########5 | 4729/5556 [59:18<10:21,  1.33it/s]Training epoch 1:  85%|########5 | 4730/5556 [59:19<10:24,  1.32it/s]Training epoch 1:  85%|########5 | 4731/5556 [59:19<10:23,  1.32it/s]Training epoch 1:  85%|########5 | 4732/5556 [59:20<10:23,  1.32it/s]Training epoch 1:  85%|########5 | 4733/5556 [59:21<10:21,  1.33it/s]Training epoch 1:  85%|########5 | 4734/5556 [59:22<10:27,  1.31it/s]Training epoch 1:  85%|########5 | 4735/5556 [59:22<10:25,  1.31it/s]Training epoch 1:  85%|########5 | 4736/5556 [59:23<10:26,  1.31it/s]Training epoch 1:  85%|########5 | 4737/5556 [59:24<10:26,  1.31it/s]Training epoch 1:  85%|########5 | 4738/5556 [59:25<10:18,  1.32it/s]Training epoch 1:  85%|########5 | 4739/5556 [59:25<10:10,  1.34it/s]Training epoch 1:  85%|########5 | 4740/5556 [59:26<10:09,  1.34it/s]Training epoch 1:  85%|########5 | 4741/5556 [59:27<10:09,  1.34it/s]Training epoch 1:  85%|########5 | 4742/5556 [59:28<10:15,  1.32it/s]Training epoch 1:  85%|########5 | 4743/5556 [59:28<10:09,  1.33it/s]Training epoch 1:  85%|########5 | 4744/5556 [59:29<10:11,  1.33it/s]Training epoch 1:  85%|########5 | 4745/5556 [59:30<10:06,  1.34it/s]Training epoch 1:  85%|########5 | 4746/5556 [59:31<10:12,  1.32it/s]Training epoch 1:  85%|########5 | 4747/5556 [59:31<10:09,  1.33it/s]Training epoch 1:  85%|########5 | 4748/5556 [59:32<09:58,  1.35it/s]Training epoch 1:  85%|########5 | 4749/5556 [59:33<09:54,  1.36it/s]Training epoch 1:  85%|########5 | 4750/5556 [59:34<09:52,  1.36it/s]Training epoch 1:  86%|########5 | 4751/5556 [59:34<09:57,  1.35it/s]Training epoch 1:  86%|########5 | 4752/5556 [59:35<09:53,  1.36it/s]Training epoch 1:  86%|########5 | 4753/5556 [59:36<09:52,  1.35it/s]Training epoch 1:  86%|########5 | 4754/5556 [59:37<09:55,  1.35it/s]Training epoch 1:  86%|########5 | 4755/5556 [59:37<09:53,  1.35it/s]Training epoch 1:  86%|########5 | 4756/5556 [59:38<09:52,  1.35it/s]Training epoch 1:  86%|########5 | 4757/5556 [59:39<09:55,  1.34it/s]Training epoch 1:  86%|########5 | 4758/5556 [59:40<09:57,  1.34it/s]Training epoch 1:  86%|########5 | 4759/5556 [59:40<09:59,  1.33it/s]Training epoch 1:  86%|########5 | 4760/5556 [59:41<10:03,  1.32it/s]Training epoch 1:  86%|########5 | 4761/5556 [59:42<10:01,  1.32it/s]Training epoch 1:  86%|########5 | 4762/5556 [59:43<09:57,  1.33it/s]Training epoch 1:  86%|########5 | 4763/5556 [59:43<09:52,  1.34it/s]Training epoch 1:  86%|########5 | 4764/5556 [59:44<09:56,  1.33it/s]Training epoch 1:  86%|########5 | 4765/5556 [59:45<09:52,  1.33it/s]Training epoch 1:  86%|########5 | 4766/5556 [59:46<09:50,  1.34it/s]Training epoch 1:  86%|########5 | 4767/5556 [59:46<09:59,  1.32it/s]Training epoch 1:  86%|########5 | 4768/5556 [59:47<09:56,  1.32it/s]Training epoch 1:  86%|########5 | 4769/5556 [59:48<09:53,  1.33it/s]Training epoch 1:  86%|########5 | 4770/5556 [59:49<09:52,  1.33it/s]Training epoch 1:  86%|########5 | 4771/5556 [59:49<09:51,  1.33it/s]Training epoch 1:  86%|########5 | 4772/5556 [59:50<09:48,  1.33it/s]Training epoch 1:  86%|########5 | 4773/5556 [59:51<09:52,  1.32it/s]Training epoch 1:  86%|########5 | 4774/5556 [59:52<09:57,  1.31it/s]Training epoch 1:  86%|########5 | 4775/5556 [59:52<09:57,  1.31it/s]Training epoch 1:  86%|########5 | 4776/5556 [59:53<09:54,  1.31it/s]Training epoch 1:  86%|########5 | 4777/5556 [59:54<09:50,  1.32it/s]Training epoch 1:  86%|########5 | 4778/5556 [59:55<09:38,  1.34it/s]Training epoch 1:  86%|########6 | 4779/5556 [59:55<09:42,  1.33it/s]Training epoch 1:  86%|########6 | 4780/5556 [59:56<09:44,  1.33it/s]Training epoch 1:  86%|########6 | 4781/5556 [59:57<09:41,  1.33it/s]Training epoch 1:  86%|########6 | 4782/5556 [59:58<09:44,  1.32it/s]Training epoch 1:  86%|########6 | 4783/5556 [59:58<09:53,  1.30it/s]Training epoch 1:  86%|########6 | 4784/5556 [59:59<09:40,  1.33it/s]Training epoch 1:  86%|########6 | 4785/5556 [1:00:00<09:43,  1.32it/s]Training epoch 1:  86%|########6 | 4786/5556 [1:00:01<09:45,  1.32it/s]Training epoch 1:  86%|########6 | 4787/5556 [1:00:01<09:45,  1.31it/s]Training epoch 1:  86%|########6 | 4788/5556 [1:00:02<09:43,  1.32it/s]Training epoch 1:  86%|########6 | 4789/5556 [1:00:03<09:48,  1.30it/s]Training epoch 1:  86%|########6 | 4790/5556 [1:00:04<09:45,  1.31it/s]Training epoch 1:  86%|########6 | 4791/5556 [1:00:05<09:49,  1.30it/s]Training epoch 1:  86%|########6 | 4792/5556 [1:00:05<09:43,  1.31it/s]Training epoch 1:  86%|########6 | 4793/5556 [1:00:06<09:29,  1.34it/s]Training epoch 1:  86%|########6 | 4794/5556 [1:00:07<09:36,  1.32it/s]Training epoch 1:  86%|########6 | 4795/5556 [1:00:08<09:40,  1.31it/s]Training epoch 1:  86%|########6 | 4796/5556 [1:00:08<09:31,  1.33it/s]Training epoch 1:  86%|########6 | 4797/5556 [1:00:09<09:37,  1.31it/s]Training epoch 1:  86%|########6 | 4798/5556 [1:00:10<09:44,  1.30it/s]Training epoch 1:  86%|########6 | 4799/5556 [1:00:11<09:41,  1.30it/s]Training epoch 1:  86%|########6 | 4800/5556 [1:00:11<10:01,  1.26it/s]Training epoch 1:  86%|########6 | 4801/5556 [1:00:12<09:54,  1.27it/s]Training epoch 1:  86%|########6 | 4802/5556 [1:00:13<09:40,  1.30it/s]Training epoch 1:  86%|########6 | 4803/5556 [1:00:14<09:37,  1.30it/s]Training epoch 1:  86%|########6 | 4804/5556 [1:00:15<09:40,  1.30it/s]Training epoch 1:  86%|########6 | 4805/5556 [1:00:15<09:39,  1.30it/s]Training epoch 1:  87%|########6 | 4806/5556 [1:00:16<09:28,  1.32it/s]Training epoch 1:  87%|########6 | 4807/5556 [1:00:17<09:26,  1.32it/s]Training epoch 1:  87%|########6 | 4808/5556 [1:00:18<09:30,  1.31it/s]Training epoch 1:  87%|########6 | 4809/5556 [1:00:18<09:28,  1.31it/s]Training epoch 1:  87%|########6 | 4810/5556 [1:00:19<09:27,  1.31it/s]Training epoch 1:  87%|########6 | 4811/5556 [1:00:20<09:33,  1.30it/s]Training epoch 1:  87%|########6 | 4812/5556 [1:00:21<09:31,  1.30it/s]Training epoch 1:  87%|########6 | 4813/5556 [1:00:21<09:28,  1.31it/s]Training epoch 1:  87%|########6 | 4814/5556 [1:00:22<09:22,  1.32it/s]Training epoch 1:  87%|########6 | 4815/5556 [1:00:23<09:20,  1.32it/s]Training epoch 1:  87%|########6 | 4816/5556 [1:00:24<09:14,  1.33it/s]Training epoch 1:  87%|########6 | 4817/5556 [1:00:24<09:20,  1.32it/s]Training epoch 1:  87%|########6 | 4818/5556 [1:00:25<09:19,  1.32it/s]Training epoch 1:  87%|########6 | 4819/5556 [1:00:26<09:26,  1.30it/s]Training epoch 1:  87%|########6 | 4820/5556 [1:00:27<09:22,  1.31it/s]Training epoch 1:  87%|########6 | 4821/5556 [1:00:27<09:23,  1.30it/s]Training epoch 1:  87%|########6 | 4822/5556 [1:00:28<09:13,  1.33it/s]Training epoch 1:  87%|########6 | 4823/5556 [1:00:29<09:14,  1.32it/s]Training epoch 1:  87%|########6 | 4824/5556 [1:00:30<09:09,  1.33it/s]Training epoch 1:  87%|########6 | 4825/5556 [1:00:30<09:13,  1.32it/s]Training epoch 1:  87%|########6 | 4826/5556 [1:00:31<09:07,  1.33it/s]Training epoch 1:  87%|########6 | 4827/5556 [1:00:32<09:02,  1.34it/s]Training epoch 1:  87%|########6 | 4828/5556 [1:00:33<09:07,  1.33it/s]Training epoch 1:  87%|########6 | 4829/5556 [1:00:33<09:08,  1.33it/s]Training epoch 1:  87%|########6 | 4830/5556 [1:00:34<09:07,  1.33it/s]Training epoch 1:  87%|########6 | 4831/5556 [1:00:35<09:01,  1.34it/s]Training epoch 1:  87%|########6 | 4832/5556 [1:00:36<08:57,  1.35it/s]Training epoch 1:  87%|########6 | 4833/5556 [1:00:36<08:47,  1.37it/s]Training epoch 1:  87%|########7 | 4834/5556 [1:00:37<08:52,  1.36it/s]Training epoch 1:  87%|########7 | 4835/5556 [1:00:38<08:53,  1.35it/s]Training epoch 1:  87%|########7 | 4836/5556 [1:00:39<08:54,  1.35it/s]Training epoch 1:  87%|########7 | 4837/5556 [1:00:39<08:59,  1.33it/s]Training epoch 1:  87%|########7 | 4838/5556 [1:00:40<09:01,  1.32it/s]Training epoch 1:  87%|########7 | 4839/5556 [1:00:41<08:58,  1.33it/s]Training epoch 1:  87%|########7 | 4840/5556 [1:00:42<09:01,  1.32it/s]Training epoch 1:  87%|########7 | 4841/5556 [1:00:42<09:05,  1.31it/s]Training epoch 1:  87%|########7 | 4842/5556 [1:00:43<08:58,  1.33it/s]Training epoch 1:  87%|########7 | 4843/5556 [1:00:44<08:52,  1.34it/s]Training epoch 1:  87%|########7 | 4844/5556 [1:00:45<08:43,  1.36it/s]Training epoch 1:  87%|########7 | 4845/5556 [1:00:45<08:32,  1.39it/s]Training epoch 1:  87%|########7 | 4846/5556 [1:00:46<08:45,  1.35it/s]Training epoch 1:  87%|########7 | 4847/5556 [1:00:47<08:52,  1.33it/s]Training epoch 1:  87%|########7 | 4848/5556 [1:00:48<08:50,  1.34it/s]Training epoch 1:  87%|########7 | 4849/5556 [1:00:48<08:50,  1.33it/s]Training epoch 1:  87%|########7 | 4850/5556 [1:00:49<08:50,  1.33it/s]Training epoch 1:  87%|########7 | 4851/5556 [1:00:50<08:43,  1.35it/s]Training epoch 1:  87%|########7 | 4852/5556 [1:00:51<08:42,  1.35it/s]Training epoch 1:  87%|########7 | 4853/5556 [1:00:51<08:44,  1.34it/s]Training epoch 1:  87%|########7 | 4854/5556 [1:00:52<08:39,  1.35it/s]Training epoch 1:  87%|########7 | 4855/5556 [1:00:53<08:43,  1.34it/s]Training epoch 1:  87%|########7 | 4856/5556 [1:00:54<08:35,  1.36it/s]Training epoch 1:  87%|########7 | 4857/5556 [1:00:54<08:38,  1.35it/s]Training epoch 1:  87%|########7 | 4858/5556 [1:00:55<08:39,  1.34it/s]Training epoch 1:  87%|########7 | 4859/5556 [1:00:56<08:45,  1.33it/s]Training epoch 1:  87%|########7 | 4860/5556 [1:00:57<08:38,  1.34it/s]Training epoch 1:  87%|########7 | 4861/5556 [1:00:57<08:38,  1.34it/s]Training epoch 1:  88%|########7 | 4862/5556 [1:00:58<08:36,  1.34it/s]Training epoch 1:  88%|########7 | 4863/5556 [1:00:59<08:32,  1.35it/s]Training epoch 1:  88%|########7 | 4864/5556 [1:00:59<08:28,  1.36it/s]Training epoch 1:  88%|########7 | 4865/5556 [1:01:00<08:29,  1.36it/s]Training epoch 1:  88%|########7 | 4866/5556 [1:01:01<08:31,  1.35it/s]Training epoch 1:  88%|########7 | 4867/5556 [1:01:02<08:35,  1.34it/s]Training epoch 1:  88%|########7 | 4868/5556 [1:01:02<08:29,  1.35it/s]Training epoch 1:  88%|########7 | 4869/5556 [1:01:03<08:26,  1.36it/s]Training epoch 1:  88%|########7 | 4870/5556 [1:01:04<08:34,  1.33it/s]Training epoch 1:  88%|########7 | 4871/5556 [1:01:05<08:34,  1.33it/s]Training epoch 1:  88%|########7 | 4872/5556 [1:01:05<08:30,  1.34it/s]Training epoch 1:  88%|########7 | 4873/5556 [1:01:06<08:33,  1.33it/s]Training epoch 1:  88%|########7 | 4874/5556 [1:01:07<08:33,  1.33it/s]Training epoch 1:  88%|########7 | 4875/5556 [1:01:08<08:33,  1.33it/s]Training epoch 1:  88%|########7 | 4876/5556 [1:01:08<08:33,  1.32it/s]Training epoch 1:  88%|########7 | 4877/5556 [1:01:09<08:33,  1.32it/s]Training epoch 1:  88%|########7 | 4878/5556 [1:01:10<08:38,  1.31it/s]Training epoch 1:  88%|########7 | 4879/5556 [1:01:11<08:36,  1.31it/s]Training epoch 1:  88%|########7 | 4880/5556 [1:01:12<08:38,  1.30it/s]Training epoch 1:  88%|########7 | 4881/5556 [1:01:12<08:39,  1.30it/s]Training epoch 1:  88%|########7 | 4882/5556 [1:01:13<08:40,  1.29it/s]Training epoch 1:  88%|########7 | 4883/5556 [1:01:14<08:35,  1.31it/s]Training epoch 1:  88%|########7 | 4884/5556 [1:01:15<08:40,  1.29it/s]Training epoch 1:  88%|########7 | 4885/5556 [1:01:15<08:34,  1.31it/s]Training epoch 1:  88%|########7 | 4886/5556 [1:01:16<08:32,  1.31it/s]Training epoch 1:  88%|########7 | 4887/5556 [1:01:17<08:22,  1.33it/s]Training epoch 1:  88%|########7 | 4888/5556 [1:01:18<08:15,  1.35it/s]Training epoch 1:  88%|########7 | 4889/5556 [1:01:18<08:13,  1.35it/s]Training epoch 1:  88%|########8 | 4890/5556 [1:01:19<08:05,  1.37it/s]Training epoch 1:  88%|########8 | 4891/5556 [1:01:20<08:10,  1.36it/s]Training epoch 1:  88%|########8 | 4892/5556 [1:01:21<08:16,  1.34it/s]Training epoch 1:  88%|########8 | 4893/5556 [1:01:21<08:15,  1.34it/s]Training epoch 1:  88%|########8 | 4894/5556 [1:01:22<08:13,  1.34it/s]Training epoch 1:  88%|########8 | 4895/5556 [1:01:23<08:09,  1.35it/s]Training epoch 1:  88%|########8 | 4896/5556 [1:01:24<08:15,  1.33it/s]Training epoch 1:  88%|########8 | 4897/5556 [1:01:24<08:14,  1.33it/s]Training epoch 1:  88%|########8 | 4898/5556 [1:01:25<08:12,  1.34it/s]Training epoch 1:  88%|########8 | 4899/5556 [1:01:26<08:18,  1.32it/s]Training epoch 1:  88%|########8 | 4900/5556 [1:01:27<08:38,  1.27it/s]Training epoch 1:  88%|########8 | 4901/5556 [1:01:27<08:35,  1.27it/s]Training epoch 1:  88%|########8 | 4902/5556 [1:01:28<08:29,  1.28it/s]Training epoch 1:  88%|########8 | 4903/5556 [1:01:29<08:25,  1.29it/s]Training epoch 1:  88%|########8 | 4904/5556 [1:01:30<08:22,  1.30it/s]Training epoch 1:  88%|########8 | 4905/5556 [1:01:31<08:22,  1.30it/s]Training epoch 1:  88%|########8 | 4906/5556 [1:01:31<08:19,  1.30it/s]Training epoch 1:  88%|########8 | 4907/5556 [1:01:32<08:14,  1.31it/s]Training epoch 1:  88%|########8 | 4908/5556 [1:01:33<08:13,  1.31it/s]Training epoch 1:  88%|########8 | 4909/5556 [1:01:34<08:07,  1.33it/s]Training epoch 1:  88%|########8 | 4910/5556 [1:01:34<08:00,  1.34it/s]Training epoch 1:  88%|########8 | 4911/5556 [1:01:35<07:59,  1.34it/s]Training epoch 1:  88%|########8 | 4912/5556 [1:01:36<07:57,  1.35it/s]Training epoch 1:  88%|########8 | 4913/5556 [1:01:37<08:05,  1.32it/s]Training epoch 1:  88%|########8 | 4914/5556 [1:01:37<08:08,  1.31it/s]Training epoch 1:  88%|########8 | 4915/5556 [1:01:38<08:06,  1.32it/s]Training epoch 1:  88%|########8 | 4916/5556 [1:01:39<08:06,  1.31it/s]Training epoch 1:  88%|########8 | 4917/5556 [1:01:40<08:03,  1.32it/s]Training epoch 1:  89%|########8 | 4918/5556 [1:01:40<08:08,  1.31it/s]Training epoch 1:  89%|########8 | 4919/5556 [1:01:41<08:02,  1.32it/s]Training epoch 1:  89%|########8 | 4920/5556 [1:01:42<08:05,  1.31it/s]Training epoch 1:  89%|########8 | 4921/5556 [1:01:43<08:04,  1.31it/s]Training epoch 1:  89%|########8 | 4922/5556 [1:01:43<08:02,  1.31it/s]Training epoch 1:  89%|########8 | 4923/5556 [1:01:44<07:54,  1.33it/s]Training epoch 1:  89%|########8 | 4924/5556 [1:01:45<07:56,  1.33it/s]Training epoch 1:  89%|########8 | 4925/5556 [1:01:46<07:54,  1.33it/s]Training epoch 1:  89%|########8 | 4926/5556 [1:01:46<07:46,  1.35it/s]Training epoch 1:  89%|########8 | 4927/5556 [1:01:47<07:44,  1.35it/s]Training epoch 1:  89%|########8 | 4928/5556 [1:01:48<07:44,  1.35it/s]Training epoch 1:  89%|########8 | 4929/5556 [1:01:49<07:43,  1.35it/s]Training epoch 1:  89%|########8 | 4930/5556 [1:01:49<07:45,  1.35it/s]Training epoch 1:  89%|########8 | 4931/5556 [1:01:50<07:47,  1.34it/s]Training epoch 1:  89%|########8 | 4932/5556 [1:01:51<07:50,  1.33it/s]Training epoch 1:  89%|########8 | 4933/5556 [1:01:52<07:52,  1.32it/s]Training epoch 1:  89%|########8 | 4934/5556 [1:01:52<07:56,  1.31it/s]Training epoch 1:  89%|########8 | 4935/5556 [1:01:53<07:55,  1.31it/s]Training epoch 1:  89%|########8 | 4936/5556 [1:01:54<07:52,  1.31it/s]Training epoch 1:  89%|########8 | 4937/5556 [1:01:55<07:54,  1.31it/s]Training epoch 1:  89%|########8 | 4938/5556 [1:01:55<07:52,  1.31it/s]Training epoch 1:  89%|########8 | 4939/5556 [1:01:56<07:50,  1.31it/s]Training epoch 1:  89%|########8 | 4940/5556 [1:01:57<07:40,  1.34it/s]Training epoch 1:  89%|########8 | 4941/5556 [1:01:58<07:39,  1.34it/s]Training epoch 1:  89%|########8 | 4942/5556 [1:01:58<07:38,  1.34it/s]Training epoch 1:  89%|########8 | 4943/5556 [1:01:59<07:41,  1.33it/s]Training epoch 1:  89%|########8 | 4944/5556 [1:02:00<07:42,  1.32it/s]Training epoch 1:  89%|########9 | 4945/5556 [1:02:01<07:38,  1.33it/s]Training epoch 1:  89%|########9 | 4946/5556 [1:02:01<07:42,  1.32it/s]Training epoch 1:  89%|########9 | 4947/5556 [1:02:02<07:40,  1.32it/s]Training epoch 1:  89%|########9 | 4948/5556 [1:02:03<07:39,  1.32it/s]Training epoch 1:  89%|########9 | 4949/5556 [1:02:04<07:42,  1.31it/s]Training epoch 1:  89%|########9 | 4950/5556 [1:02:04<07:38,  1.32it/s]Training epoch 1:  89%|########9 | 4951/5556 [1:02:05<07:41,  1.31it/s]Training epoch 1:  89%|########9 | 4952/5556 [1:02:06<07:41,  1.31it/s]Training epoch 1:  89%|########9 | 4953/5556 [1:02:07<07:38,  1.32it/s]Training epoch 1:  89%|########9 | 4954/5556 [1:02:08<07:40,  1.31it/s]Training epoch 1:  89%|########9 | 4955/5556 [1:02:08<07:37,  1.31it/s]Training epoch 1:  89%|########9 | 4956/5556 [1:02:09<07:34,  1.32it/s]Training epoch 1:  89%|########9 | 4957/5556 [1:02:10<07:31,  1.33it/s]Training epoch 1:  89%|########9 | 4958/5556 [1:02:10<07:22,  1.35it/s]Training epoch 1:  89%|########9 | 4959/5556 [1:02:11<07:21,  1.35it/s]Training epoch 1:  89%|########9 | 4960/5556 [1:02:12<07:21,  1.35it/s]Training epoch 1:  89%|########9 | 4961/5556 [1:02:13<07:24,  1.34it/s]Training epoch 1:  89%|########9 | 4962/5556 [1:02:14<07:28,  1.32it/s]Training epoch 1:  89%|########9 | 4963/5556 [1:02:14<07:38,  1.29it/s]Training epoch 1:  89%|########9 | 4964/5556 [1:02:15<07:37,  1.29it/s]Training epoch 1:  89%|########9 | 4965/5556 [1:02:16<07:33,  1.30it/s]Training epoch 1:  89%|########9 | 4966/5556 [1:02:17<07:29,  1.31it/s]Training epoch 1:  89%|########9 | 4967/5556 [1:02:17<07:24,  1.32it/s]Training epoch 1:  89%|########9 | 4968/5556 [1:02:18<07:24,  1.32it/s]Training epoch 1:  89%|########9 | 4969/5556 [1:02:19<07:28,  1.31it/s]Training epoch 1:  89%|########9 | 4970/5556 [1:02:20<07:26,  1.31it/s]Training epoch 1:  89%|########9 | 4971/5556 [1:02:20<07:31,  1.30it/s]Training epoch 1:  89%|########9 | 4972/5556 [1:02:21<07:25,  1.31it/s]Training epoch 1:  90%|########9 | 4973/5556 [1:02:22<07:17,  1.33it/s]Training epoch 1:  90%|########9 | 4974/5556 [1:02:23<07:14,  1.34it/s]Training epoch 1:  90%|########9 | 4975/5556 [1:02:23<07:13,  1.34it/s]Training epoch 1:  90%|########9 | 4976/5556 [1:02:24<07:16,  1.33it/s]Training epoch 1:  90%|########9 | 4977/5556 [1:02:25<07:12,  1.34it/s]Training epoch 1:  90%|########9 | 4978/5556 [1:02:26<07:06,  1.35it/s]Training epoch 1:  90%|########9 | 4979/5556 [1:02:26<07:10,  1.34it/s]Training epoch 1:  90%|########9 | 4980/5556 [1:02:27<07:13,  1.33it/s]Training epoch 1:  90%|########9 | 4981/5556 [1:02:28<07:10,  1.33it/s]Training epoch 1:  90%|########9 | 4982/5556 [1:02:29<07:12,  1.33it/s]Training epoch 1:  90%|########9 | 4983/5556 [1:02:29<07:12,  1.33it/s]Training epoch 1:  90%|########9 | 4984/5556 [1:02:30<07:12,  1.32it/s]Training epoch 1:  90%|########9 | 4985/5556 [1:02:31<07:11,  1.32it/s]Training epoch 1:  90%|########9 | 4986/5556 [1:02:32<07:06,  1.34it/s]Training epoch 1:  90%|########9 | 4987/5556 [1:02:32<07:09,  1.32it/s]Training epoch 1:  90%|########9 | 4988/5556 [1:02:33<07:03,  1.34it/s]Training epoch 1:  90%|########9 | 4989/5556 [1:02:34<07:07,  1.33it/s]Training epoch 1:  90%|########9 | 4990/5556 [1:02:35<07:05,  1.33it/s]Training epoch 1:  90%|########9 | 4991/5556 [1:02:35<07:01,  1.34it/s]Training epoch 1:  90%|########9 | 4992/5556 [1:02:36<07:01,  1.34it/s]Training epoch 1:  90%|########9 | 4993/5556 [1:02:37<07:03,  1.33it/s]Training epoch 1:  90%|########9 | 4994/5556 [1:02:38<07:09,  1.31it/s]Training epoch 1:  90%|########9 | 4995/5556 [1:02:38<07:04,  1.32it/s]Training epoch 1:  90%|########9 | 4996/5556 [1:02:39<07:05,  1.32it/s]Training epoch 1:  90%|########9 | 4997/5556 [1:02:40<07:03,  1.32it/s]Training epoch 1:  90%|########9 | 4998/5556 [1:02:41<07:02,  1.32it/s]Training epoch 1:  90%|########9 | 4999/5556 [1:02:41<06:57,  1.33it/s]Training epoch 1:  90%|########9 | 5000/5556 [1:02:42<07:09,  1.30it/s]Training epoch 1:  90%|######### | 5001/5556 [1:02:43<07:07,  1.30it/s]Training epoch 1:  90%|######### | 5002/5556 [1:02:44<07:10,  1.29it/s]Training epoch 1:  90%|######### | 5003/5556 [1:02:45<07:12,  1.28it/s]Training epoch 1:  90%|######### | 5004/5556 [1:02:45<07:10,  1.28it/s]Training epoch 1:  90%|######### | 5005/5556 [1:02:46<07:05,  1.30it/s]Training epoch 1:  90%|######### | 5006/5556 [1:02:47<06:53,  1.33it/s]Training epoch 1:  90%|######### | 5007/5556 [1:02:48<06:52,  1.33it/s]Training epoch 1:  90%|######### | 5008/5556 [1:02:48<06:53,  1.33it/s]Training epoch 1:  90%|######### | 5009/5556 [1:02:49<06:52,  1.33it/s]Training epoch 1:  90%|######### | 5010/5556 [1:02:50<06:52,  1.32it/s]Training epoch 1:  90%|######### | 5011/5556 [1:02:51<06:52,  1.32it/s]Training epoch 1:  90%|######### | 5012/5556 [1:02:51<06:48,  1.33it/s]Training epoch 1:  90%|######### | 5013/5556 [1:02:52<06:46,  1.34it/s]Training epoch 1:  90%|######### | 5014/5556 [1:02:53<06:49,  1.32it/s]Training epoch 1:  90%|######### | 5015/5556 [1:02:54<06:45,  1.34it/s]Training epoch 1:  90%|######### | 5016/5556 [1:02:54<06:44,  1.34it/s]Training epoch 1:  90%|######### | 5017/5556 [1:02:55<06:42,  1.34it/s]Training epoch 1:  90%|######### | 5018/5556 [1:02:56<06:35,  1.36it/s]Training epoch 1:  90%|######### | 5019/5556 [1:02:57<06:41,  1.34it/s]Training epoch 1:  90%|######### | 5020/5556 [1:02:57<06:39,  1.34it/s]Training epoch 1:  90%|######### | 5021/5556 [1:02:58<06:43,  1.33it/s]Training epoch 1:  90%|######### | 5022/5556 [1:02:59<06:38,  1.34it/s]Training epoch 1:  90%|######### | 5023/5556 [1:03:00<06:32,  1.36it/s]Training epoch 1:  90%|######### | 5024/5556 [1:03:00<06:40,  1.33it/s]Training epoch 1:  90%|######### | 5025/5556 [1:03:01<06:37,  1.34it/s]Training epoch 1:  90%|######### | 5026/5556 [1:03:02<06:37,  1.33it/s]Training epoch 1:  90%|######### | 5027/5556 [1:03:03<06:32,  1.35it/s]Training epoch 1:  90%|######### | 5028/5556 [1:03:03<06:35,  1.33it/s]Training epoch 1:  91%|######### | 5029/5556 [1:03:04<06:40,  1.32it/s]Training epoch 1:  91%|######### | 5030/5556 [1:03:05<06:36,  1.33it/s]Training epoch 1:  91%|######### | 5031/5556 [1:03:06<06:34,  1.33it/s]Training epoch 1:  91%|######### | 5032/5556 [1:03:06<06:31,  1.34it/s]Training epoch 1:  91%|######### | 5033/5556 [1:03:07<06:31,  1.34it/s]Training epoch 1:  91%|######### | 5034/5556 [1:03:08<06:32,  1.33it/s]Training epoch 1:  91%|######### | 5035/5556 [1:03:09<06:29,  1.34it/s]Training epoch 1:  91%|######### | 5036/5556 [1:03:09<06:31,  1.33it/s]Training epoch 1:  91%|######### | 5037/5556 [1:03:10<06:31,  1.32it/s]Training epoch 1:  91%|######### | 5038/5556 [1:03:11<06:32,  1.32it/s]Training epoch 1:  91%|######### | 5039/5556 [1:03:12<06:27,  1.33it/s]Training epoch 1:  91%|######### | 5040/5556 [1:03:12<06:29,  1.33it/s]Training epoch 1:  91%|######### | 5041/5556 [1:03:13<06:24,  1.34it/s]Training epoch 1:  91%|######### | 5042/5556 [1:03:14<06:21,  1.35it/s]Training epoch 1:  91%|######### | 5043/5556 [1:03:15<06:21,  1.34it/s]Training epoch 1:  91%|######### | 5044/5556 [1:03:15<06:22,  1.34it/s]Training epoch 1:  91%|######### | 5045/5556 [1:03:16<06:22,  1.33it/s]Training epoch 1:  91%|######### | 5046/5556 [1:03:17<06:24,  1.32it/s]Training epoch 1:  91%|######### | 5047/5556 [1:03:18<06:22,  1.33it/s]Training epoch 1:  91%|######### | 5048/5556 [1:03:18<06:21,  1.33it/s]Training epoch 1:  91%|######### | 5049/5556 [1:03:19<06:23,  1.32it/s]Training epoch 1:  91%|######### | 5050/5556 [1:03:20<06:21,  1.33it/s]Training epoch 1:  91%|######### | 5051/5556 [1:03:21<06:20,  1.33it/s]Training epoch 1:  91%|######### | 5052/5556 [1:03:21<06:21,  1.32it/s]Training epoch 1:  91%|######### | 5053/5556 [1:03:22<06:19,  1.32it/s]Training epoch 1:  91%|######### | 5054/5556 [1:03:23<06:21,  1.32it/s]Training epoch 1:  91%|######### | 5055/5556 [1:03:24<06:20,  1.32it/s]Training epoch 1:  91%|#########1| 5056/5556 [1:03:24<06:17,  1.33it/s]Training epoch 1:  91%|#########1| 5057/5556 [1:03:25<06:17,  1.32it/s]Training epoch 1:  91%|#########1| 5058/5556 [1:03:26<06:17,  1.32it/s]Training epoch 1:  91%|#########1| 5059/5556 [1:03:27<06:13,  1.33it/s]Training epoch 1:  91%|#########1| 5060/5556 [1:03:27<06:12,  1.33it/s]Training epoch 1:  91%|#########1| 5061/5556 [1:03:28<06:11,  1.33it/s]Training epoch 1:  91%|#########1| 5062/5556 [1:03:29<06:09,  1.34it/s]Training epoch 1:  91%|#########1| 5063/5556 [1:03:30<06:07,  1.34it/s]Training epoch 1:  91%|#########1| 5064/5556 [1:03:30<06:12,  1.32it/s]Training epoch 1:  91%|#########1| 5065/5556 [1:03:31<06:11,  1.32it/s]Training epoch 1:  91%|#########1| 5066/5556 [1:03:32<06:10,  1.32it/s]Training epoch 1:  91%|#########1| 5067/5556 [1:03:33<06:08,  1.33it/s]Training epoch 1:  91%|#########1| 5068/5556 [1:03:33<06:05,  1.33it/s]Training epoch 1:  91%|#########1| 5069/5556 [1:03:34<06:04,  1.34it/s]Training epoch 1:  91%|#########1| 5070/5556 [1:03:35<06:03,  1.34it/s]Training epoch 1:  91%|#########1| 5071/5556 [1:03:36<06:02,  1.34it/s]Training epoch 1:  91%|#########1| 5072/5556 [1:03:36<06:05,  1.32it/s]Training epoch 1:  91%|#########1| 5073/5556 [1:03:37<06:05,  1.32it/s]Training epoch 1:  91%|#########1| 5074/5556 [1:03:38<06:01,  1.33it/s]Training epoch 1:  91%|#########1| 5075/5556 [1:03:39<05:57,  1.35it/s]Training epoch 1:  91%|#########1| 5076/5556 [1:03:39<06:01,  1.33it/s]Training epoch 1:  91%|#########1| 5077/5556 [1:03:40<05:59,  1.33it/s]Training epoch 1:  91%|#########1| 5078/5556 [1:03:41<05:54,  1.35it/s]Training epoch 1:  91%|#########1| 5079/5556 [1:03:42<05:55,  1.34it/s]Training epoch 1:  91%|#########1| 5080/5556 [1:03:42<05:53,  1.35it/s]Training epoch 1:  91%|#########1| 5081/5556 [1:03:43<05:53,  1.35it/s]Training epoch 1:  91%|#########1| 5082/5556 [1:03:44<05:55,  1.33it/s]Training epoch 1:  91%|#########1| 5083/5556 [1:03:45<05:58,  1.32it/s]Training epoch 1:  92%|#########1| 5084/5556 [1:03:45<05:55,  1.33it/s]Training epoch 1:  92%|#########1| 5085/5556 [1:03:46<05:56,  1.32it/s]Training epoch 1:  92%|#########1| 5086/5556 [1:03:47<05:54,  1.33it/s]Training epoch 1:  92%|#########1| 5087/5556 [1:03:48<05:52,  1.33it/s]Training epoch 1:  92%|#########1| 5088/5556 [1:03:48<05:50,  1.34it/s]Training epoch 1:  92%|#########1| 5089/5556 [1:03:49<05:51,  1.33it/s]Training epoch 1:  92%|#########1| 5090/5556 [1:03:50<05:52,  1.32it/s]Training epoch 1:  92%|#########1| 5091/5556 [1:03:51<05:49,  1.33it/s]Training epoch 1:  92%|#########1| 5092/5556 [1:03:51<05:49,  1.33it/s]Training epoch 1:  92%|#########1| 5093/5556 [1:03:52<05:49,  1.32it/s]Training epoch 1:  92%|#########1| 5094/5556 [1:03:53<05:46,  1.33it/s]Training epoch 1:  92%|#########1| 5095/5556 [1:03:54<05:41,  1.35it/s]Training epoch 1:  92%|#########1| 5096/5556 [1:03:54<05:41,  1.35it/s]Training epoch 1:  92%|#########1| 5097/5556 [1:03:55<05:37,  1.36it/s]Training epoch 1:  92%|#########1| 5098/5556 [1:03:56<05:42,  1.34it/s]Training epoch 1:  92%|#########1| 5099/5556 [1:03:57<05:44,  1.32it/s]Training epoch 1:  92%|#########1| 5100/5556 [1:03:58<06:01,  1.26it/s]Training epoch 1:  92%|#########1| 5101/5556 [1:03:58<05:54,  1.28it/s]Training epoch 1:  92%|#########1| 5102/5556 [1:03:59<05:49,  1.30it/s]Training epoch 1:  92%|#########1| 5103/5556 [1:04:00<05:44,  1.32it/s]Training epoch 1:  92%|#########1| 5104/5556 [1:04:01<05:45,  1.31it/s]Training epoch 1:  92%|#########1| 5105/5556 [1:04:01<05:43,  1.31it/s]Training epoch 1:  92%|#########1| 5106/5556 [1:04:02<05:45,  1.30it/s]Training epoch 1:  92%|#########1| 5107/5556 [1:04:03<05:44,  1.30it/s]Training epoch 1:  92%|#########1| 5108/5556 [1:04:04<05:44,  1.30it/s]Training epoch 1:  92%|#########1| 5109/5556 [1:04:04<05:41,  1.31it/s]Training epoch 1:  92%|#########1| 5110/5556 [1:04:05<05:40,  1.31it/s]Training epoch 1:  92%|#########1| 5111/5556 [1:04:06<05:38,  1.31it/s]Training epoch 1:  92%|#########2| 5112/5556 [1:04:07<05:40,  1.30it/s]Training epoch 1:  92%|#########2| 5113/5556 [1:04:07<05:37,  1.31it/s]Training epoch 1:  92%|#########2| 5114/5556 [1:04:08<05:34,  1.32it/s]Training epoch 1:  92%|#########2| 5115/5556 [1:04:09<05:31,  1.33it/s]Training epoch 1:  92%|#########2| 5116/5556 [1:04:10<05:28,  1.34it/s]Training epoch 1:  92%|#########2| 5117/5556 [1:04:10<05:29,  1.33it/s]Training epoch 1:  92%|#########2| 5118/5556 [1:04:11<05:31,  1.32it/s]Training epoch 1:  92%|#########2| 5119/5556 [1:04:12<05:32,  1.31it/s]Training epoch 1:  92%|#########2| 5120/5556 [1:04:13<05:30,  1.32it/s]Training epoch 1:  92%|#########2| 5121/5556 [1:04:13<05:29,  1.32it/s]Training epoch 1:  92%|#########2| 5122/5556 [1:04:14<05:28,  1.32it/s]Training epoch 1:  92%|#########2| 5123/5556 [1:04:15<05:26,  1.33it/s]Training epoch 1:  92%|#########2| 5124/5556 [1:04:16<05:25,  1.33it/s]Training epoch 1:  92%|#########2| 5125/5556 [1:04:16<05:24,  1.33it/s]Training epoch 1:  92%|#########2| 5126/5556 [1:04:17<05:24,  1.32it/s]Training epoch 1:  92%|#########2| 5127/5556 [1:04:18<05:21,  1.33it/s]Training epoch 1:  92%|#########2| 5128/5556 [1:04:19<05:19,  1.34it/s]Training epoch 1:  92%|#########2| 5129/5556 [1:04:19<05:19,  1.34it/s]Training epoch 1:  92%|#########2| 5130/5556 [1:04:20<05:17,  1.34it/s]Training epoch 1:  92%|#########2| 5131/5556 [1:04:21<05:19,  1.33it/s]Training epoch 1:  92%|#########2| 5132/5556 [1:04:22<05:22,  1.32it/s]Training epoch 1:  92%|#########2| 5133/5556 [1:04:22<05:18,  1.33it/s]Training epoch 1:  92%|#########2| 5134/5556 [1:04:23<05:20,  1.32it/s]Training epoch 1:  92%|#########2| 5135/5556 [1:04:24<05:18,  1.32it/s]Training epoch 1:  92%|#########2| 5136/5556 [1:04:25<05:12,  1.35it/s]Training epoch 1:  92%|#########2| 5137/5556 [1:04:25<05:11,  1.35it/s]Training epoch 1:  92%|#########2| 5138/5556 [1:04:26<05:11,  1.34it/s]Training epoch 1:  92%|#########2| 5139/5556 [1:04:27<05:08,  1.35it/s]Training epoch 1:  93%|#########2| 5140/5556 [1:04:28<05:07,  1.35it/s]Training epoch 1:  93%|#########2| 5141/5556 [1:04:28<05:11,  1.33it/s]Training epoch 1:  93%|#########2| 5142/5556 [1:04:29<05:08,  1.34it/s]Training epoch 1:  93%|#########2| 5143/5556 [1:04:30<05:09,  1.33it/s]Training epoch 1:  93%|#########2| 5144/5556 [1:04:31<05:10,  1.33it/s]Training epoch 1:  93%|#########2| 5145/5556 [1:04:31<05:08,  1.33it/s]Training epoch 1:  93%|#########2| 5146/5556 [1:04:32<05:06,  1.34it/s]Training epoch 1:  93%|#########2| 5147/5556 [1:04:33<05:03,  1.35it/s]Training epoch 1:  93%|#########2| 5148/5556 [1:04:34<05:00,  1.36it/s]Training epoch 1:  93%|#########2| 5149/5556 [1:04:34<05:01,  1.35it/s]Training epoch 1:  93%|#########2| 5150/5556 [1:04:35<05:03,  1.34it/s]Training epoch 1:  93%|#########2| 5151/5556 [1:04:36<05:01,  1.34it/s]Training epoch 1:  93%|#########2| 5152/5556 [1:04:37<05:02,  1.34it/s]Training epoch 1:  93%|#########2| 5153/5556 [1:04:37<04:59,  1.35it/s]Training epoch 1:  93%|#########2| 5154/5556 [1:04:38<04:57,  1.35it/s]Training epoch 1:  93%|#########2| 5155/5556 [1:04:39<05:02,  1.32it/s]Training epoch 1:  93%|#########2| 5156/5556 [1:04:40<05:02,  1.32it/s]Training epoch 1:  93%|#########2| 5157/5556 [1:04:40<05:00,  1.33it/s]Training epoch 1:  93%|#########2| 5158/5556 [1:04:41<05:03,  1.31it/s]Training epoch 1:  93%|#########2| 5159/5556 [1:04:42<05:01,  1.32it/s]Training epoch 1:  93%|#########2| 5160/5556 [1:04:43<04:58,  1.33it/s]Training epoch 1:  93%|#########2| 5161/5556 [1:04:43<04:59,  1.32it/s]Training epoch 1:  93%|#########2| 5162/5556 [1:04:44<04:59,  1.32it/s]Training epoch 1:  93%|#########2| 5163/5556 [1:04:45<04:59,  1.31it/s]Training epoch 1:  93%|#########2| 5164/5556 [1:04:46<04:55,  1.33it/s]Training epoch 1:  93%|#########2| 5165/5556 [1:04:46<04:48,  1.35it/s]Training epoch 1:  93%|#########2| 5166/5556 [1:04:47<04:53,  1.33it/s]Training epoch 1:  93%|#########2| 5167/5556 [1:04:48<04:50,  1.34it/s]Training epoch 1:  93%|#########3| 5168/5556 [1:04:49<04:50,  1.34it/s]Training epoch 1:  93%|#########3| 5169/5556 [1:04:49<04:50,  1.33it/s]Training epoch 1:  93%|#########3| 5170/5556 [1:04:50<04:47,  1.34it/s]Training epoch 1:  93%|#########3| 5171/5556 [1:04:51<04:50,  1.32it/s]Training epoch 1:  93%|#########3| 5172/5556 [1:04:52<04:46,  1.34it/s]Training epoch 1:  93%|#########3| 5173/5556 [1:04:52<04:48,  1.33it/s]Training epoch 1:  93%|#########3| 5174/5556 [1:04:53<04:46,  1.33it/s]Training epoch 1:  93%|#########3| 5175/5556 [1:04:54<04:43,  1.34it/s]Training epoch 1:  93%|#########3| 5176/5556 [1:04:55<04:40,  1.36it/s]Training epoch 1:  93%|#########3| 5177/5556 [1:04:55<04:40,  1.35it/s]Training epoch 1:  93%|#########3| 5178/5556 [1:04:56<04:38,  1.36it/s]Training epoch 1:  93%|#########3| 5179/5556 [1:04:57<04:38,  1.36it/s]Training epoch 1:  93%|#########3| 5180/5556 [1:04:58<04:35,  1.37it/s]Training epoch 1:  93%|#########3| 5181/5556 [1:04:58<04:37,  1.35it/s]Training epoch 1:  93%|#########3| 5182/5556 [1:04:59<04:38,  1.34it/s]Training epoch 1:  93%|#########3| 5183/5556 [1:05:00<04:38,  1.34it/s]Training epoch 1:  93%|#########3| 5184/5556 [1:05:01<04:36,  1.35it/s]Training epoch 1:  93%|#########3| 5185/5556 [1:05:01<04:36,  1.34it/s]Training epoch 1:  93%|#########3| 5186/5556 [1:05:02<04:31,  1.36it/s]Training epoch 1:  93%|#########3| 5187/5556 [1:05:03<04:36,  1.34it/s]Training epoch 1:  93%|#########3| 5188/5556 [1:05:04<04:30,  1.36it/s]Training epoch 1:  93%|#########3| 5189/5556 [1:05:04<04:33,  1.34it/s]Training epoch 1:  93%|#########3| 5190/5556 [1:05:05<04:32,  1.34it/s]Training epoch 1:  93%|#########3| 5191/5556 [1:05:06<04:32,  1.34it/s]Training epoch 1:  93%|#########3| 5192/5556 [1:05:07<04:33,  1.33it/s]Training epoch 1:  93%|#########3| 5193/5556 [1:05:07<04:32,  1.33it/s]Training epoch 1:  93%|#########3| 5194/5556 [1:05:08<04:30,  1.34it/s]Training epoch 1:  94%|#########3| 5195/5556 [1:05:09<04:27,  1.35it/s]Training epoch 1:  94%|#########3| 5196/5556 [1:05:10<04:28,  1.34it/s]Training epoch 1:  94%|#########3| 5197/5556 [1:05:10<04:29,  1.33it/s]Training epoch 1:  94%|#########3| 5198/5556 [1:05:11<04:26,  1.34it/s]Training epoch 1:  94%|#########3| 5199/5556 [1:05:12<04:29,  1.32it/s]Training epoch 1:  94%|#########3| 5200/5556 [1:05:13<04:37,  1.28it/s]Training epoch 1:  94%|#########3| 5201/5556 [1:05:13<04:31,  1.31it/s]Training epoch 1:  94%|#########3| 5202/5556 [1:05:14<04:31,  1.30it/s]Training epoch 1:  94%|#########3| 5203/5556 [1:05:15<04:31,  1.30it/s]Training epoch 1:  94%|#########3| 5204/5556 [1:05:16<04:24,  1.33it/s]Training epoch 1:  94%|#########3| 5205/5556 [1:05:16<04:24,  1.33it/s]Training epoch 1:  94%|#########3| 5206/5556 [1:05:17<04:21,  1.34it/s]Training epoch 1:  94%|#########3| 5207/5556 [1:05:18<04:20,  1.34it/s]Training epoch 1:  94%|#########3| 5208/5556 [1:05:19<04:21,  1.33it/s]Training epoch 1:  94%|#########3| 5209/5556 [1:05:19<04:20,  1.33it/s]Training epoch 1:  94%|#########3| 5210/5556 [1:05:20<04:21,  1.32it/s]Training epoch 1:  94%|#########3| 5211/5556 [1:05:21<04:20,  1.33it/s]Training epoch 1:  94%|#########3| 5212/5556 [1:05:22<04:21,  1.32it/s]Training epoch 1:  94%|#########3| 5213/5556 [1:05:22<04:19,  1.32it/s]Training epoch 1:  94%|#########3| 5214/5556 [1:05:23<04:19,  1.32it/s]Training epoch 1:  94%|#########3| 5215/5556 [1:05:24<04:20,  1.31it/s]Training epoch 1:  94%|#########3| 5216/5556 [1:05:25<04:17,  1.32it/s]Training epoch 1:  94%|#########3| 5217/5556 [1:05:25<04:14,  1.33it/s]Training epoch 1:  94%|#########3| 5218/5556 [1:05:26<04:14,  1.33it/s]Training epoch 1:  94%|#########3| 5219/5556 [1:05:27<04:16,  1.31it/s]Training epoch 1:  94%|#########3| 5220/5556 [1:05:28<04:15,  1.31it/s]Training epoch 1:  94%|#########3| 5221/5556 [1:05:28<04:14,  1.32it/s]Training epoch 1:  94%|#########3| 5222/5556 [1:05:29<04:10,  1.33it/s]Training epoch 1:  94%|#########4| 5223/5556 [1:05:30<04:17,  1.30it/s]Training epoch 1:  94%|#########4| 5224/5556 [1:05:31<04:11,  1.32it/s]Training epoch 1:  94%|#########4| 5225/5556 [1:05:32<04:11,  1.31it/s]Training epoch 1:  94%|#########4| 5226/5556 [1:05:32<04:12,  1.31it/s]Training epoch 1:  94%|#########4| 5227/5556 [1:05:33<04:11,  1.31it/s]Training epoch 1:  94%|#########4| 5228/5556 [1:05:34<04:08,  1.32it/s]Training epoch 1:  94%|#########4| 5229/5556 [1:05:35<04:08,  1.32it/s]Training epoch 1:  94%|#########4| 5230/5556 [1:05:35<04:05,  1.33it/s]Training epoch 1:  94%|#########4| 5231/5556 [1:05:36<04:06,  1.32it/s]Training epoch 1:  94%|#########4| 5232/5556 [1:05:37<04:04,  1.32it/s]Training epoch 1:  94%|#########4| 5233/5556 [1:05:38<04:07,  1.31it/s]Training epoch 1:  94%|#########4| 5234/5556 [1:05:38<04:09,  1.29it/s]Training epoch 1:  94%|#########4| 5235/5556 [1:05:39<04:05,  1.31it/s]Training epoch 1:  94%|#########4| 5236/5556 [1:05:40<04:04,  1.31it/s]Training epoch 1:  94%|#########4| 5237/5556 [1:05:41<04:00,  1.33it/s]Training epoch 1:  94%|#########4| 5238/5556 [1:05:41<03:58,  1.33it/s]Training epoch 1:  94%|#########4| 5239/5556 [1:05:42<03:58,  1.33it/s]Training epoch 1:  94%|#########4| 5240/5556 [1:05:43<04:00,  1.32it/s]Training epoch 1:  94%|#########4| 5241/5556 [1:05:44<04:01,  1.30it/s]Training epoch 1:  94%|#########4| 5242/5556 [1:05:44<03:58,  1.32it/s]Training epoch 1:  94%|#########4| 5243/5556 [1:05:45<03:56,  1.32it/s]Training epoch 1:  94%|#########4| 5244/5556 [1:05:46<03:57,  1.31it/s]Training epoch 1:  94%|#########4| 5245/5556 [1:05:47<03:55,  1.32it/s]Training epoch 1:  94%|#########4| 5246/5556 [1:05:47<03:56,  1.31it/s]Training epoch 1:  94%|#########4| 5247/5556 [1:05:48<03:53,  1.32it/s]Training epoch 1:  94%|#########4| 5248/5556 [1:05:49<03:53,  1.32it/s]Training epoch 1:  94%|#########4| 5249/5556 [1:05:50<03:54,  1.31it/s]Training epoch 1:  94%|#########4| 5250/5556 [1:05:51<03:53,  1.31it/s]Training epoch 1:  95%|#########4| 5251/5556 [1:05:51<03:53,  1.31it/s]Training epoch 1:  95%|#########4| 5252/5556 [1:05:52<03:51,  1.31it/s]Training epoch 1:  95%|#########4| 5253/5556 [1:05:53<03:50,  1.32it/s]Training epoch 1:  95%|#########4| 5254/5556 [1:05:54<03:46,  1.33it/s]Training epoch 1:  95%|#########4| 5255/5556 [1:05:54<03:47,  1.32it/s]Training epoch 1:  95%|#########4| 5256/5556 [1:05:55<03:46,  1.32it/s]Training epoch 1:  95%|#########4| 5257/5556 [1:05:56<03:44,  1.33it/s]Training epoch 1:  95%|#########4| 5258/5556 [1:05:57<03:42,  1.34it/s]Training epoch 1:  95%|#########4| 5259/5556 [1:05:57<03:41,  1.34it/s]Training epoch 1:  95%|#########4| 5260/5556 [1:05:58<03:41,  1.34it/s]Training epoch 1:  95%|#########4| 5261/5556 [1:05:59<03:40,  1.34it/s]Training epoch 1:  95%|#########4| 5262/5556 [1:06:00<03:39,  1.34it/s]Training epoch 1:  95%|#########4| 5263/5556 [1:06:00<03:39,  1.33it/s]Training epoch 1:  95%|#########4| 5264/5556 [1:06:01<03:41,  1.32it/s]Training epoch 1:  95%|#########4| 5265/5556 [1:06:02<03:39,  1.33it/s]Training epoch 1:  95%|#########4| 5266/5556 [1:06:03<03:39,  1.32it/s]Training epoch 1:  95%|#########4| 5267/5556 [1:06:03<03:37,  1.33it/s]Training epoch 1:  95%|#########4| 5268/5556 [1:06:04<03:33,  1.35it/s]Training epoch 1:  95%|#########4| 5269/5556 [1:06:05<03:36,  1.33it/s]Training epoch 1:  95%|#########4| 5270/5556 [1:06:06<03:34,  1.33it/s]Training epoch 1:  95%|#########4| 5271/5556 [1:06:06<03:35,  1.32it/s]Training epoch 1:  95%|#########4| 5272/5556 [1:06:07<03:33,  1.33it/s]Training epoch 1:  95%|#########4| 5273/5556 [1:06:08<03:35,  1.31it/s]Training epoch 1:  95%|#########4| 5274/5556 [1:06:09<03:32,  1.33it/s]Training epoch 1:  95%|#########4| 5275/5556 [1:06:09<03:31,  1.33it/s]Training epoch 1:  95%|#########4| 5276/5556 [1:06:10<03:31,  1.33it/s]Training epoch 1:  95%|#########4| 5277/5556 [1:06:11<03:31,  1.32it/s]Training epoch 1:  95%|#########4| 5278/5556 [1:06:12<03:31,  1.31it/s]Training epoch 1:  95%|#########5| 5279/5556 [1:06:12<03:30,  1.31it/s]Training epoch 1:  95%|#########5| 5280/5556 [1:06:13<03:30,  1.31it/s]Training epoch 1:  95%|#########5| 5281/5556 [1:06:14<03:29,  1.31it/s]Training epoch 1:  95%|#########5| 5282/5556 [1:06:15<03:30,  1.30it/s]Training epoch 1:  95%|#########5| 5283/5556 [1:06:15<03:28,  1.31it/s]Training epoch 1:  95%|#########5| 5284/5556 [1:06:16<03:23,  1.34it/s]Training epoch 1:  95%|#########5| 5285/5556 [1:06:17<03:26,  1.31it/s]Training epoch 1:  95%|#########5| 5286/5556 [1:06:18<03:23,  1.33it/s]Training epoch 1:  95%|#########5| 5287/5556 [1:06:18<03:20,  1.34it/s]Training epoch 1:  95%|#########5| 5288/5556 [1:06:19<03:22,  1.32it/s]Training epoch 1:  95%|#########5| 5289/5556 [1:06:20<03:22,  1.32it/s]Training epoch 1:  95%|#########5| 5290/5556 [1:06:21<03:21,  1.32it/s]Training epoch 1:  95%|#########5| 5291/5556 [1:06:21<03:18,  1.34it/s]Training epoch 1:  95%|#########5| 5292/5556 [1:06:22<03:16,  1.34it/s]Training epoch 1:  95%|#########5| 5293/5556 [1:06:23<03:16,  1.34it/s]Training epoch 1:  95%|#########5| 5294/5556 [1:06:24<03:18,  1.32it/s]Training epoch 1:  95%|#########5| 5295/5556 [1:06:24<03:15,  1.34it/s]Training epoch 1:  95%|#########5| 5296/5556 [1:06:25<03:15,  1.33it/s]Training epoch 1:  95%|#########5| 5297/5556 [1:06:26<03:13,  1.34it/s]Training epoch 1:  95%|#########5| 5298/5556 [1:06:27<03:14,  1.33it/s]Training epoch 1:  95%|#########5| 5299/5556 [1:06:27<03:14,  1.32it/s]Training epoch 1:  95%|#########5| 5300/5556 [1:06:28<03:22,  1.26it/s]Training epoch 1:  95%|#########5| 5301/5556 [1:06:29<03:18,  1.28it/s]Training epoch 1:  95%|#########5| 5302/5556 [1:06:30<03:15,  1.30it/s]Training epoch 1:  95%|#########5| 5303/5556 [1:06:31<03:13,  1.31it/s]Training epoch 1:  95%|#########5| 5304/5556 [1:06:31<03:10,  1.32it/s]Training epoch 1:  95%|#########5| 5305/5556 [1:06:32<03:10,  1.32it/s]Training epoch 1:  96%|#########5| 5306/5556 [1:06:33<03:09,  1.32it/s]Training epoch 1:  96%|#########5| 5307/5556 [1:06:34<03:09,  1.31it/s]Training epoch 1:  96%|#########5| 5308/5556 [1:06:34<03:08,  1.32it/s]Training epoch 1:  96%|#########5| 5309/5556 [1:06:35<03:04,  1.34it/s]Training epoch 1:  96%|#########5| 5310/5556 [1:06:36<03:03,  1.34it/s]Training epoch 1:  96%|#########5| 5311/5556 [1:06:37<03:05,  1.32it/s]Training epoch 1:  96%|#########5| 5312/5556 [1:06:37<03:05,  1.31it/s]Training epoch 1:  96%|#########5| 5313/5556 [1:06:38<03:02,  1.33it/s]Training epoch 1:  96%|#########5| 5314/5556 [1:06:39<03:01,  1.33it/s]Training epoch 1:  96%|#########5| 5315/5556 [1:06:40<03:03,  1.31it/s]Training epoch 1:  96%|#########5| 5316/5556 [1:06:40<03:01,  1.32it/s]Training epoch 1:  96%|#########5| 5317/5556 [1:06:41<03:01,  1.31it/s]Training epoch 1:  96%|#########5| 5318/5556 [1:06:42<03:01,  1.31it/s]Training epoch 1:  96%|#########5| 5319/5556 [1:06:43<02:59,  1.32it/s]Training epoch 1:  96%|#########5| 5320/5556 [1:06:43<02:57,  1.33it/s]Training epoch 1:  96%|#########5| 5321/5556 [1:06:44<02:57,  1.32it/s]Training epoch 1:  96%|#########5| 5322/5556 [1:06:45<02:55,  1.33it/s]Training epoch 1:  96%|#########5| 5323/5556 [1:06:46<02:51,  1.36it/s]Training epoch 1:  96%|#########5| 5324/5556 [1:06:46<02:54,  1.33it/s]Training epoch 1:  96%|#########5| 5325/5556 [1:06:47<02:53,  1.33it/s]Training epoch 1:  96%|#########5| 5326/5556 [1:06:48<02:51,  1.34it/s]Training epoch 1:  96%|#########5| 5327/5556 [1:06:49<02:50,  1.34it/s]Training epoch 1:  96%|#########5| 5328/5556 [1:06:49<02:51,  1.33it/s]Training epoch 1:  96%|#########5| 5329/5556 [1:06:50<02:50,  1.33it/s]Training epoch 1:  96%|#########5| 5330/5556 [1:06:51<02:49,  1.33it/s]Training epoch 1:  96%|#########5| 5331/5556 [1:06:52<02:48,  1.34it/s]Training epoch 1:  96%|#########5| 5332/5556 [1:06:52<02:46,  1.34it/s]Training epoch 1:  96%|#########5| 5333/5556 [1:06:53<02:46,  1.34it/s]Training epoch 1:  96%|#########6| 5334/5556 [1:06:54<02:44,  1.35it/s]Training epoch 1:  96%|#########6| 5335/5556 [1:06:55<02:46,  1.33it/s]Training epoch 1:  96%|#########6| 5336/5556 [1:06:55<02:46,  1.32it/s]Training epoch 1:  96%|#########6| 5337/5556 [1:06:56<02:48,  1.30it/s]Training epoch 1:  96%|#########6| 5338/5556 [1:06:57<02:47,  1.30it/s]Training epoch 1:  96%|#########6| 5339/5556 [1:06:58<02:46,  1.30it/s]Training epoch 1:  96%|#########6| 5340/5556 [1:06:58<02:44,  1.32it/s]Training epoch 1:  96%|#########6| 5341/5556 [1:06:59<02:42,  1.32it/s]Training epoch 1:  96%|#########6| 5342/5556 [1:07:00<02:43,  1.31it/s]Training epoch 1:  96%|#########6| 5343/5556 [1:07:01<02:41,  1.32it/s]Training epoch 1:  96%|#########6| 5344/5556 [1:07:01<02:39,  1.33it/s]Training epoch 1:  96%|#########6| 5345/5556 [1:07:02<02:39,  1.32it/s]Training epoch 1:  96%|#########6| 5346/5556 [1:07:03<02:38,  1.32it/s]Training epoch 1:  96%|#########6| 5347/5556 [1:07:04<02:39,  1.31it/s]Training epoch 1:  96%|#########6| 5348/5556 [1:07:05<02:38,  1.32it/s]Training epoch 1:  96%|#########6| 5349/5556 [1:07:05<02:37,  1.32it/s]Training epoch 1:  96%|#########6| 5350/5556 [1:07:06<02:37,  1.31it/s]Training epoch 1:  96%|#########6| 5351/5556 [1:07:07<02:35,  1.32it/s]Training epoch 1:  96%|#########6| 5352/5556 [1:07:08<02:32,  1.34it/s]Training epoch 1:  96%|#########6| 5353/5556 [1:07:08<02:29,  1.36it/s]Training epoch 1:  96%|#########6| 5354/5556 [1:07:09<02:30,  1.34it/s]Training epoch 1:  96%|#########6| 5355/5556 [1:07:10<02:31,  1.33it/s]Training epoch 1:  96%|#########6| 5356/5556 [1:07:11<02:30,  1.33it/s]Training epoch 1:  96%|#########6| 5357/5556 [1:07:11<02:27,  1.35it/s]Training epoch 1:  96%|#########6| 5358/5556 [1:07:12<02:26,  1.35it/s]Training epoch 1:  96%|#########6| 5359/5556 [1:07:13<02:26,  1.35it/s]Training epoch 1:  96%|#########6| 5360/5556 [1:07:13<02:26,  1.34it/s]Training epoch 1:  96%|#########6| 5361/5556 [1:07:14<02:27,  1.33it/s]Training epoch 1:  97%|#########6| 5362/5556 [1:07:15<02:26,  1.32it/s]Training epoch 1:  97%|#########6| 5363/5556 [1:07:16<02:24,  1.34it/s]Training epoch 1:  97%|#########6| 5364/5556 [1:07:17<02:24,  1.33it/s]Training epoch 1:  97%|#########6| 5365/5556 [1:07:17<02:23,  1.33it/s]Training epoch 1:  97%|#########6| 5366/5556 [1:07:18<02:21,  1.34it/s]Training epoch 1:  97%|#########6| 5367/5556 [1:07:19<02:21,  1.33it/s]Training epoch 1:  97%|#########6| 5368/5556 [1:07:20<02:21,  1.33it/s]Training epoch 1:  97%|#########6| 5369/5556 [1:07:20<02:19,  1.34it/s]Training epoch 1:  97%|#########6| 5370/5556 [1:07:21<02:18,  1.34it/s]Training epoch 1:  97%|#########6| 5371/5556 [1:07:22<02:19,  1.33it/s]Training epoch 1:  97%|#########6| 5372/5556 [1:07:22<02:16,  1.35it/s]Training epoch 1:  97%|#########6| 5373/5556 [1:07:23<02:16,  1.34it/s]Training epoch 1:  97%|#########6| 5374/5556 [1:07:24<02:19,  1.30it/s]Training epoch 1:  97%|#########6| 5375/5556 [1:07:25<02:18,  1.31it/s]Training epoch 1:  97%|#########6| 5376/5556 [1:07:26<02:18,  1.30it/s]Training epoch 1:  97%|#########6| 5377/5556 [1:07:26<02:17,  1.30it/s]Training epoch 1:  97%|#########6| 5378/5556 [1:07:27<02:15,  1.31it/s]Training epoch 1:  97%|#########6| 5379/5556 [1:07:28<02:15,  1.31it/s]Training epoch 1:  97%|#########6| 5380/5556 [1:07:29<02:13,  1.31it/s]Training epoch 1:  97%|#########6| 5381/5556 [1:07:29<02:13,  1.31it/s]Training epoch 1:  97%|#########6| 5382/5556 [1:07:30<02:11,  1.32it/s]Training epoch 1:  97%|#########6| 5383/5556 [1:07:31<02:10,  1.33it/s]Training epoch 1:  97%|#########6| 5384/5556 [1:07:32<02:08,  1.33it/s]Training epoch 1:  97%|#########6| 5385/5556 [1:07:32<02:09,  1.32it/s]Training epoch 1:  97%|#########6| 5386/5556 [1:07:33<02:08,  1.32it/s]Training epoch 1:  97%|#########6| 5387/5556 [1:07:34<02:07,  1.32it/s]Training epoch 1:  97%|#########6| 5388/5556 [1:07:35<02:06,  1.33it/s]Training epoch 1:  97%|#########6| 5389/5556 [1:07:35<02:05,  1.33it/s]Training epoch 1:  97%|#########7| 5390/5556 [1:07:36<02:05,  1.32it/s]Training epoch 1:  97%|#########7| 5391/5556 [1:07:37<02:05,  1.31it/s]Training epoch 1:  97%|#########7| 5392/5556 [1:07:38<02:07,  1.29it/s]Training epoch 1:  97%|#########7| 5393/5556 [1:07:39<02:06,  1.29it/s]Training epoch 1:  97%|#########7| 5394/5556 [1:07:39<02:04,  1.30it/s]Training epoch 1:  97%|#########7| 5395/5556 [1:07:40<02:01,  1.32it/s]Training epoch 1:  97%|#########7| 5396/5556 [1:07:41<02:01,  1.32it/s]Training epoch 1:  97%|#########7| 5397/5556 [1:07:42<01:59,  1.33it/s]Training epoch 1:  97%|#########7| 5398/5556 [1:07:42<01:58,  1.33it/s]Training epoch 1:  97%|#########7| 5399/5556 [1:07:43<01:57,  1.34it/s]Training epoch 1:  97%|#########7| 5400/5556 [1:07:44<02:02,  1.28it/s]Training epoch 1:  97%|#########7| 5401/5556 [1:07:45<01:59,  1.30it/s]Training epoch 1:  97%|#########7| 5402/5556 [1:07:45<01:58,  1.30it/s]Training epoch 1:  97%|#########7| 5403/5556 [1:07:46<01:57,  1.31it/s]Training epoch 1:  97%|#########7| 5404/5556 [1:07:47<01:56,  1.31it/s]Training epoch 1:  97%|#########7| 5405/5556 [1:07:48<01:55,  1.31it/s]Training epoch 1:  97%|#########7| 5406/5556 [1:07:48<01:54,  1.31it/s]Training epoch 1:  97%|#########7| 5407/5556 [1:07:49<01:53,  1.32it/s]Training epoch 1:  97%|#########7| 5408/5556 [1:07:50<01:50,  1.33it/s]Training epoch 1:  97%|#########7| 5409/5556 [1:07:51<01:50,  1.33it/s]Training epoch 1:  97%|#########7| 5410/5556 [1:07:51<01:49,  1.33it/s]Training epoch 1:  97%|#########7| 5411/5556 [1:07:52<01:49,  1.33it/s]Training epoch 1:  97%|#########7| 5412/5556 [1:07:53<01:48,  1.33it/s]Training epoch 1:  97%|#########7| 5413/5556 [1:07:54<01:46,  1.34it/s]Training epoch 1:  97%|#########7| 5414/5556 [1:07:54<01:47,  1.33it/s]Training epoch 1:  97%|#########7| 5415/5556 [1:07:55<01:47,  1.31it/s]Training epoch 1:  97%|#########7| 5416/5556 [1:07:56<01:46,  1.31it/s]Training epoch 1:  97%|#########7| 5417/5556 [1:07:57<01:45,  1.32it/s]Training epoch 1:  98%|#########7| 5418/5556 [1:07:57<01:43,  1.33it/s]Training epoch 1:  98%|#########7| 5419/5556 [1:07:58<01:41,  1.35it/s]Training epoch 1:  98%|#########7| 5420/5556 [1:07:59<01:41,  1.34it/s]Training epoch 1:  98%|#########7| 5421/5556 [1:08:00<01:41,  1.33it/s]Training epoch 1:  98%|#########7| 5422/5556 [1:08:00<01:40,  1.34it/s]Training epoch 1:  98%|#########7| 5423/5556 [1:08:01<01:39,  1.33it/s]Training epoch 1:  98%|#########7| 5424/5556 [1:08:02<01:39,  1.32it/s]Training epoch 1:  98%|#########7| 5425/5556 [1:08:03<01:37,  1.34it/s]Training epoch 1:  98%|#########7| 5426/5556 [1:08:03<01:36,  1.34it/s]Training epoch 1:  98%|#########7| 5427/5556 [1:08:04<01:37,  1.33it/s]Training epoch 1:  98%|#########7| 5428/5556 [1:08:05<01:36,  1.32it/s]Training epoch 1:  98%|#########7| 5429/5556 [1:08:06<01:34,  1.34it/s]Training epoch 1:  98%|#########7| 5430/5556 [1:08:06<01:34,  1.34it/s]Training epoch 1:  98%|#########7| 5431/5556 [1:08:07<01:33,  1.34it/s]Training epoch 1:  98%|#########7| 5432/5556 [1:08:08<01:32,  1.34it/s]Training epoch 1:  98%|#########7| 5433/5556 [1:08:09<01:31,  1.34it/s]Training epoch 1:  98%|#########7| 5434/5556 [1:08:09<01:31,  1.33it/s]Training epoch 1:  98%|#########7| 5435/5556 [1:08:10<01:31,  1.32it/s]Training epoch 1:  98%|#########7| 5436/5556 [1:08:11<01:31,  1.31it/s]Training epoch 1:  98%|#########7| 5437/5556 [1:08:12<01:31,  1.31it/s]Training epoch 1:  98%|#########7| 5438/5556 [1:08:12<01:29,  1.32it/s]Training epoch 1:  98%|#########7| 5439/5556 [1:08:13<01:27,  1.33it/s]Training epoch 1:  98%|#########7| 5440/5556 [1:08:14<01:26,  1.35it/s]Training epoch 1:  98%|#########7| 5441/5556 [1:08:15<01:25,  1.35it/s]Training epoch 1:  98%|#########7| 5442/5556 [1:08:15<01:24,  1.36it/s]Training epoch 1:  98%|#########7| 5443/5556 [1:08:16<01:24,  1.33it/s]Training epoch 1:  98%|#########7| 5444/5556 [1:08:17<01:23,  1.34it/s]Training epoch 1:  98%|#########8| 5445/5556 [1:08:18<01:24,  1.32it/s]Training epoch 1:  98%|#########8| 5446/5556 [1:08:18<01:23,  1.33it/s]Training epoch 1:  98%|#########8| 5447/5556 [1:08:19<01:22,  1.32it/s]Training epoch 1:  98%|#########8| 5448/5556 [1:08:20<01:21,  1.32it/s]Training epoch 1:  98%|#########8| 5449/5556 [1:08:21<01:20,  1.33it/s]Training epoch 1:  98%|#########8| 5450/5556 [1:08:21<01:19,  1.34it/s]Training epoch 1:  98%|#########8| 5451/5556 [1:08:22<01:19,  1.33it/s]Training epoch 1:  98%|#########8| 5452/5556 [1:08:23<01:18,  1.33it/s]Training epoch 1:  98%|#########8| 5453/5556 [1:08:24<01:17,  1.33it/s]Training epoch 1:  98%|#########8| 5454/5556 [1:08:24<01:17,  1.32it/s]Training epoch 1:  98%|#########8| 5455/5556 [1:08:25<01:16,  1.32it/s]Training epoch 1:  98%|#########8| 5456/5556 [1:08:26<01:15,  1.32it/s]Training epoch 1:  98%|#########8| 5457/5556 [1:08:27<01:15,  1.31it/s]Training epoch 1:  98%|#########8| 5458/5556 [1:08:28<01:14,  1.31it/s]Training epoch 1:  98%|#########8| 5459/5556 [1:08:28<01:13,  1.32it/s]Training epoch 1:  98%|#########8| 5460/5556 [1:08:29<01:13,  1.31it/s]Training epoch 1:  98%|#########8| 5461/5556 [1:08:30<01:12,  1.32it/s]Training epoch 1:  98%|#########8| 5462/5556 [1:08:31<01:10,  1.33it/s]Training epoch 1:  98%|#########8| 5463/5556 [1:08:31<01:10,  1.33it/s]Training epoch 1:  98%|#########8| 5464/5556 [1:08:32<01:09,  1.32it/s]Training epoch 1:  98%|#########8| 5465/5556 [1:08:33<01:08,  1.33it/s]Training epoch 1:  98%|#########8| 5466/5556 [1:08:34<01:07,  1.33it/s]Training epoch 1:  98%|#########8| 5467/5556 [1:08:34<01:08,  1.30it/s]Training epoch 1:  98%|#########8| 5468/5556 [1:08:35<01:06,  1.31it/s]Training epoch 1:  98%|#########8| 5469/5556 [1:08:36<01:06,  1.31it/s]Training epoch 1:  98%|#########8| 5470/5556 [1:08:37<01:05,  1.31it/s]Training epoch 1:  98%|#########8| 5471/5556 [1:08:37<01:04,  1.31it/s]Training epoch 1:  98%|#########8| 5472/5556 [1:08:38<01:04,  1.30it/s]Training epoch 1:  99%|#########8| 5473/5556 [1:08:39<01:03,  1.32it/s]Training epoch 1:  99%|#########8| 5474/5556 [1:08:40<01:02,  1.32it/s]Training epoch 1:  99%|#########8| 5475/5556 [1:08:40<01:00,  1.33it/s]Training epoch 1:  99%|#########8| 5476/5556 [1:08:41<01:00,  1.33it/s]Training epoch 1:  99%|#########8| 5477/5556 [1:08:42<00:59,  1.32it/s]Training epoch 1:  99%|#########8| 5478/5556 [1:08:43<00:58,  1.33it/s]Training epoch 1:  99%|#########8| 5479/5556 [1:08:43<00:57,  1.34it/s]Training epoch 1:  99%|#########8| 5480/5556 [1:08:44<00:56,  1.34it/s]Training epoch 1:  99%|#########8| 5481/5556 [1:08:45<00:56,  1.34it/s]Training epoch 1:  99%|#########8| 5482/5556 [1:08:46<00:55,  1.34it/s]Training epoch 1:  99%|#########8| 5483/5556 [1:08:46<00:53,  1.35it/s]Training epoch 1:  99%|#########8| 5484/5556 [1:08:47<00:53,  1.33it/s]Training epoch 1:  99%|#########8| 5485/5556 [1:08:48<00:53,  1.33it/s]Training epoch 1:  99%|#########8| 5486/5556 [1:08:49<00:52,  1.32it/s]Training epoch 1:  99%|#########8| 5487/5556 [1:08:49<00:52,  1.32it/s]Training epoch 1:  99%|#########8| 5488/5556 [1:08:50<00:51,  1.31it/s]Training epoch 1:  99%|#########8| 5489/5556 [1:08:51<00:50,  1.33it/s]Training epoch 1:  99%|#########8| 5490/5556 [1:08:52<00:50,  1.32it/s]Training epoch 1:  99%|#########8| 5491/5556 [1:08:52<00:49,  1.31it/s]Training epoch 1:  99%|#########8| 5492/5556 [1:08:53<00:48,  1.32it/s]Training epoch 1:  99%|#########8| 5493/5556 [1:08:54<00:47,  1.31it/s]Training epoch 1:  99%|#########8| 5494/5556 [1:08:55<00:47,  1.31it/s]Training epoch 1:  99%|#########8| 5495/5556 [1:08:56<00:46,  1.30it/s]Training epoch 1:  99%|#########8| 5496/5556 [1:08:56<00:45,  1.33it/s]Training epoch 1:  99%|#########8| 5497/5556 [1:08:57<00:44,  1.33it/s]Training epoch 1:  99%|#########8| 5498/5556 [1:08:58<00:43,  1.33it/s]Training epoch 1:  99%|#########8| 5499/5556 [1:08:58<00:42,  1.34it/s]Training epoch 1:  99%|#########8| 5500/5556 [1:08:59<00:43,  1.29it/s]Training epoch 1:  99%|#########9| 5501/5556 [1:09:00<00:42,  1.29it/s]Training epoch 1:  99%|#########9| 5502/5556 [1:09:01<00:41,  1.29it/s]Training epoch 1:  99%|#########9| 5503/5556 [1:09:02<00:41,  1.29it/s]Training epoch 1:  99%|#########9| 5504/5556 [1:09:02<00:39,  1.31it/s]Training epoch 1:  99%|#########9| 5505/5556 [1:09:03<00:38,  1.33it/s]Training epoch 1:  99%|#########9| 5506/5556 [1:09:04<00:37,  1.33it/s]Training epoch 1:  99%|#########9| 5507/5556 [1:09:05<00:37,  1.32it/s]Training epoch 1:  99%|#########9| 5508/5556 [1:09:05<00:36,  1.31it/s]Training epoch 1:  99%|#########9| 5509/5556 [1:09:06<00:35,  1.32it/s]Training epoch 1:  99%|#########9| 5510/5556 [1:09:07<00:34,  1.32it/s]Training epoch 1:  99%|#########9| 5511/5556 [1:09:08<00:34,  1.31it/s]Training epoch 1:  99%|#########9| 5512/5556 [1:09:08<00:33,  1.31it/s]Training epoch 1:  99%|#########9| 5513/5556 [1:09:09<00:32,  1.31it/s]Training epoch 1:  99%|#########9| 5514/5556 [1:09:10<00:32,  1.31it/s]Training epoch 1:  99%|#########9| 5515/5556 [1:09:11<00:31,  1.31it/s]Training epoch 1:  99%|#########9| 5516/5556 [1:09:11<00:30,  1.33it/s]Training epoch 1:  99%|#########9| 5517/5556 [1:09:12<00:29,  1.32it/s]Training epoch 1:  99%|#########9| 5518/5556 [1:09:13<00:28,  1.33it/s]Training epoch 1:  99%|#########9| 5519/5556 [1:09:14<00:28,  1.32it/s]Training epoch 1:  99%|#########9| 5520/5556 [1:09:15<00:27,  1.32it/s]Training epoch 1:  99%|#########9| 5521/5556 [1:09:15<00:26,  1.33it/s]Training epoch 1:  99%|#########9| 5522/5556 [1:09:16<00:25,  1.35it/s]Training epoch 1:  99%|#########9| 5523/5556 [1:09:17<00:24,  1.34it/s]Training epoch 1:  99%|#########9| 5524/5556 [1:09:17<00:23,  1.35it/s]Training epoch 1:  99%|#########9| 5525/5556 [1:09:18<00:23,  1.33it/s]Training epoch 1:  99%|#########9| 5526/5556 [1:09:19<00:22,  1.33it/s]Training epoch 1:  99%|#########9| 5527/5556 [1:09:20<00:22,  1.31it/s]Training epoch 1:  99%|#########9| 5528/5556 [1:09:21<00:21,  1.30it/s]Training epoch 1: 100%|#########9| 5529/5556 [1:09:21<00:20,  1.32it/s]Training epoch 1: 100%|#########9| 5530/5556 [1:09:22<00:19,  1.31it/s]Training epoch 1: 100%|#########9| 5531/5556 [1:09:23<00:18,  1.33it/s]Training epoch 1: 100%|#########9| 5532/5556 [1:09:24<00:17,  1.34it/s]Training epoch 1: 100%|#########9| 5533/5556 [1:09:24<00:17,  1.31it/s]Training epoch 1: 100%|#########9| 5534/5556 [1:09:25<00:16,  1.33it/s]Training epoch 1: 100%|#########9| 5535/5556 [1:09:26<00:15,  1.33it/s]Training epoch 1: 100%|#########9| 5536/5556 [1:09:27<00:15,  1.33it/s]Training epoch 1: 100%|#########9| 5537/5556 [1:09:27<00:14,  1.32it/s]Training epoch 1: 100%|#########9| 5538/5556 [1:09:28<00:13,  1.33it/s]Training epoch 1: 100%|#########9| 5539/5556 [1:09:29<00:12,  1.34it/s]Training epoch 1: 100%|#########9| 5540/5556 [1:09:30<00:11,  1.34it/s]Training epoch 1: 100%|#########9| 5541/5556 [1:09:30<00:11,  1.34it/s]Training epoch 1: 100%|#########9| 5542/5556 [1:09:31<00:10,  1.35it/s]Training epoch 1: 100%|#########9| 5543/5556 [1:09:32<00:09,  1.37it/s]Training epoch 1: 100%|#########9| 5544/5556 [1:09:32<00:08,  1.36it/s]Training epoch 1: 100%|#########9| 5545/5556 [1:09:33<00:08,  1.35it/s]Training epoch 1: 100%|#########9| 5546/5556 [1:09:34<00:07,  1.35it/s]Training epoch 1: 100%|#########9| 5547/5556 [1:09:35<00:06,  1.34it/s]Training epoch 1: 100%|#########9| 5548/5556 [1:09:35<00:06,  1.32it/s]Training epoch 1: 100%|#########9| 5549/5556 [1:09:36<00:05,  1.31it/s]Training epoch 1: 100%|#########9| 5550/5556 [1:09:37<00:04,  1.33it/s]Training epoch 1: 100%|#########9| 5551/5556 [1:09:38<00:03,  1.35it/s]Training epoch 1: 100%|#########9| 5552/5556 [1:09:38<00:02,  1.36it/s]Training epoch 1: 100%|#########9| 5553/5556 [1:09:39<00:02,  1.34it/s]Training epoch 1: 100%|#########9| 5554/5556 [1:09:40<00:01,  1.35it/s]Training epoch 1: 100%|#########9| 5555/5556 [1:09:41<00:00,  1.37it/s]Training epoch 1: 100%|##########| 5556/5556 [1:09:41<00:00,  1.46it/s]Training epoch 1: 100%|##########| 5556/5556 [1:09:41<00:00,  1.33it/s]
Evaluating on VQA val set:   0%|          | 0/2671 [00:00<?, ?it/s]Evaluating on VQA val set:   0%|          | 1/2671 [00:01<1:06:35,  1.50s/it]Evaluating on VQA val set:   0%|          | 2/2671 [00:02<45:33,  1.02s/it]  Evaluating on VQA val set:   0%|          | 3/2671 [00:02<37:59,  1.17it/s]Evaluating on VQA val set:   0%|          | 4/2671 [00:03<35:28,  1.25it/s]Evaluating on VQA val set:   0%|          | 5/2671 [00:04<34:51,  1.27it/s]Evaluating on VQA val set:   0%|          | 6/2671 [00:05<33:26,  1.33it/s]Evaluating on VQA val set:   0%|          | 7/2671 [00:05<33:55,  1.31it/s]Evaluating on VQA val set:   0%|          | 8/2671 [00:06<32:29,  1.37it/s]Evaluating on VQA val set:   0%|          | 9/2671 [00:07<33:10,  1.34it/s]Evaluating on VQA val set:   0%|          | 10/2671 [00:07<31:24,  1.41it/s]Evaluating on VQA val set:   0%|          | 11/2671 [00:08<32:08,  1.38it/s]Evaluating on VQA val set:   0%|          | 12/2671 [00:09<31:20,  1.41it/s]Evaluating on VQA val set:   0%|          | 13/2671 [00:10<32:03,  1.38it/s]Evaluating on VQA val set:   1%|          | 14/2671 [00:10<31:01,  1.43it/s]Evaluating on VQA val set:   1%|          | 15/2671 [00:11<32:01,  1.38it/s]Evaluating on VQA val set:   1%|          | 16/2671 [00:12<31:03,  1.42it/s]Evaluating on VQA val set:   1%|          | 17/2671 [00:12<32:24,  1.37it/s]Evaluating on VQA val set:   1%|          | 18/2671 [00:13<31:53,  1.39it/s]Evaluating on VQA val set:   1%|          | 19/2671 [00:14<32:10,  1.37it/s]Evaluating on VQA val set:   1%|          | 20/2671 [00:14<30:33,  1.45it/s]Evaluating on VQA val set:   1%|          | 21/2671 [00:15<31:44,  1.39it/s]Evaluating on VQA val set:   1%|          | 22/2671 [00:16<32:02,  1.38it/s]Evaluating on VQA val set:   1%|          | 23/2671 [00:17<32:36,  1.35it/s]Evaluating on VQA val set:   1%|          | 24/2671 [00:17<31:01,  1.42it/s]Evaluating on VQA val set:   1%|          | 25/2671 [00:18<32:27,  1.36it/s]Evaluating on VQA val set:   1%|          | 26/2671 [00:19<32:04,  1.37it/s]Evaluating on VQA val set:   1%|1         | 27/2671 [00:20<32:29,  1.36it/s]Evaluating on VQA val set:   1%|1         | 28/2671 [00:20<32:38,  1.35it/s]Evaluating on VQA val set:   1%|1         | 29/2671 [00:21<33:14,  1.32it/s]Evaluating on VQA val set:   1%|1         | 30/2671 [00:22<32:40,  1.35it/s]Evaluating on VQA val set:   1%|1         | 31/2671 [00:23<33:22,  1.32it/s]Evaluating on VQA val set:   1%|1         | 32/2671 [00:23<31:58,  1.38it/s]Evaluating on VQA val set:   1%|1         | 33/2671 [00:24<31:33,  1.39it/s]Evaluating on VQA val set:   1%|1         | 34/2671 [00:25<31:33,  1.39it/s]Evaluating on VQA val set:   1%|1         | 35/2671 [00:26<32:47,  1.34it/s]Evaluating on VQA val set:   1%|1         | 36/2671 [00:26<32:27,  1.35it/s]Evaluating on VQA val set:   1%|1         | 37/2671 [00:27<32:50,  1.34it/s]Evaluating on VQA val set:   1%|1         | 38/2671 [00:28<32:25,  1.35it/s]Evaluating on VQA val set:   1%|1         | 39/2671 [00:28<31:09,  1.41it/s]Evaluating on VQA val set:   1%|1         | 40/2671 [00:29<30:34,  1.43it/s]Evaluating on VQA val set:   2%|1         | 41/2671 [00:30<30:46,  1.42it/s]Evaluating on VQA val set:   2%|1         | 42/2671 [00:30<28:28,  1.54it/s]Evaluating on VQA val set:   2%|1         | 43/2671 [00:31<28:07,  1.56it/s]Evaluating on VQA val set:   2%|1         | 44/2671 [00:32<28:37,  1.53it/s]Evaluating on VQA val set:   2%|1         | 45/2671 [00:32<29:31,  1.48it/s]Evaluating on VQA val set:   2%|1         | 46/2671 [00:33<30:08,  1.45it/s]Evaluating on VQA val set:   2%|1         | 47/2671 [00:34<30:38,  1.43it/s]Evaluating on VQA val set:   2%|1         | 48/2671 [00:35<30:47,  1.42it/s]Evaluating on VQA val set:   2%|1         | 49/2671 [00:35<30:58,  1.41it/s]Evaluating on VQA val set:   2%|1         | 50/2671 [00:36<30:55,  1.41it/s]Evaluating on VQA val set:   2%|1         | 51/2671 [00:37<31:22,  1.39it/s]Evaluating on VQA val set:   2%|1         | 52/2671 [00:37<30:56,  1.41it/s]Evaluating on VQA val set:   2%|1         | 53/2671 [00:38<30:30,  1.43it/s]Evaluating on VQA val set:   2%|2         | 54/2671 [00:39<30:46,  1.42it/s]Evaluating on VQA val set:   2%|2         | 55/2671 [00:39<28:44,  1.52it/s]Evaluating on VQA val set:   2%|2         | 56/2671 [00:40<30:08,  1.45it/s]Evaluating on VQA val set:   2%|2         | 57/2671 [00:41<30:59,  1.41it/s]Evaluating on VQA val set:   2%|2         | 58/2671 [00:42<31:13,  1.40it/s]Evaluating on VQA val set:   2%|2         | 59/2671 [00:42<30:34,  1.42it/s]Evaluating on VQA val set:   2%|2         | 60/2671 [00:43<31:39,  1.37it/s]Evaluating on VQA val set:   2%|2         | 61/2671 [00:44<31:50,  1.37it/s]Evaluating on VQA val set:   2%|2         | 62/2671 [00:45<31:25,  1.38it/s]Evaluating on VQA val set:   2%|2         | 63/2671 [00:45<31:51,  1.36it/s]Evaluating on VQA val set:   2%|2         | 64/2671 [00:46<32:20,  1.34it/s]Evaluating on VQA val set:   2%|2         | 65/2671 [00:47<31:53,  1.36it/s]Evaluating on VQA val set:   2%|2         | 66/2671 [00:47<31:00,  1.40it/s]Evaluating on VQA val set:   3%|2         | 67/2671 [00:48<30:53,  1.40it/s]Evaluating on VQA val set:   3%|2         | 68/2671 [00:49<30:57,  1.40it/s]Evaluating on VQA val set:   3%|2         | 69/2671 [00:50<31:06,  1.39it/s]Evaluating on VQA val set:   3%|2         | 70/2671 [00:50<31:30,  1.38it/s]Evaluating on VQA val set:   3%|2         | 71/2671 [00:51<30:57,  1.40it/s]Evaluating on VQA val set:   3%|2         | 72/2671 [00:52<31:01,  1.40it/s]Evaluating on VQA val set:   3%|2         | 73/2671 [00:52<31:45,  1.36it/s]Evaluating on VQA val set:   3%|2         | 74/2671 [00:53<31:35,  1.37it/s]Evaluating on VQA val set:   3%|2         | 75/2671 [00:54<31:46,  1.36it/s]Evaluating on VQA val set:   3%|2         | 76/2671 [00:55<31:05,  1.39it/s]Evaluating on VQA val set:   3%|2         | 77/2671 [00:55<30:52,  1.40it/s]Evaluating on VQA val set:   3%|2         | 78/2671 [00:56<31:08,  1.39it/s]Evaluating on VQA val set:   3%|2         | 79/2671 [00:57<31:03,  1.39it/s]Evaluating on VQA val set:   3%|2         | 80/2671 [00:57<30:07,  1.43it/s]Evaluating on VQA val set:   3%|3         | 81/2671 [00:58<30:11,  1.43it/s]Evaluating on VQA val set:   3%|3         | 82/2671 [00:59<30:28,  1.42it/s]Evaluating on VQA val set:   3%|3         | 83/2671 [01:00<30:30,  1.41it/s]Evaluating on VQA val set:   3%|3         | 84/2671 [01:00<30:00,  1.44it/s]Evaluating on VQA val set:   3%|3         | 85/2671 [01:01<30:31,  1.41it/s]Evaluating on VQA val set:   3%|3         | 86/2671 [01:02<30:38,  1.41it/s]Evaluating on VQA val set:   3%|3         | 87/2671 [01:02<30:41,  1.40it/s]Evaluating on VQA val set:   3%|3         | 88/2671 [01:03<30:57,  1.39it/s]Evaluating on VQA val set:   3%|3         | 89/2671 [01:04<31:05,  1.38it/s]Evaluating on VQA val set:   3%|3         | 90/2671 [01:05<31:29,  1.37it/s]Evaluating on VQA val set:   3%|3         | 91/2671 [01:05<30:34,  1.41it/s]Evaluating on VQA val set:   3%|3         | 92/2671 [01:06<29:46,  1.44it/s]Evaluating on VQA val set:   3%|3         | 93/2671 [01:07<29:31,  1.46it/s]Evaluating on VQA val set:   4%|3         | 94/2671 [01:07<29:24,  1.46it/s]Evaluating on VQA val set:   4%|3         | 95/2671 [01:08<29:05,  1.48it/s]Evaluating on VQA val set:   4%|3         | 96/2671 [01:09<29:14,  1.47it/s]Evaluating on VQA val set:   4%|3         | 97/2671 [01:09<28:29,  1.51it/s]Evaluating on VQA val set:   4%|3         | 98/2671 [01:10<27:51,  1.54it/s]Evaluating on VQA val set:   4%|3         | 99/2671 [01:10<26:56,  1.59it/s]Evaluating on VQA val set:   4%|3         | 100/2671 [01:11<28:43,  1.49it/s]Evaluating on VQA val set:   4%|3         | 101/2671 [01:12<28:47,  1.49it/s]Evaluating on VQA val set:   4%|3         | 102/2671 [01:13<29:10,  1.47it/s]Evaluating on VQA val set:   4%|3         | 103/2671 [01:13<30:01,  1.43it/s]Evaluating on VQA val set:   4%|3         | 104/2671 [01:14<30:19,  1.41it/s]Evaluating on VQA val set:   4%|3         | 105/2671 [01:15<30:45,  1.39it/s]Evaluating on VQA val set:   4%|3         | 106/2671 [01:16<30:58,  1.38it/s]Evaluating on VQA val set:   4%|4         | 107/2671 [01:16<31:18,  1.36it/s]Evaluating on VQA val set:   4%|4         | 108/2671 [01:17<31:15,  1.37it/s]Evaluating on VQA val set:   4%|4         | 109/2671 [01:18<31:24,  1.36it/s]Evaluating on VQA val set:   4%|4         | 110/2671 [01:18<30:40,  1.39it/s]Evaluating on VQA val set:   4%|4         | 111/2671 [01:19<30:37,  1.39it/s]Evaluating on VQA val set:   4%|4         | 112/2671 [01:20<30:26,  1.40it/s]Evaluating on VQA val set:   4%|4         | 113/2671 [01:21<30:38,  1.39it/s]Evaluating on VQA val set:   4%|4         | 114/2671 [01:21<31:30,  1.35it/s]Evaluating on VQA val set:   4%|4         | 115/2671 [01:22<31:50,  1.34it/s]Evaluating on VQA val set:   4%|4         | 116/2671 [01:23<31:19,  1.36it/s]Evaluating on VQA val set:   4%|4         | 117/2671 [01:24<31:45,  1.34it/s]Evaluating on VQA val set:   4%|4         | 118/2671 [01:24<30:51,  1.38it/s]Evaluating on VQA val set:   4%|4         | 119/2671 [01:25<30:40,  1.39it/s]Evaluating on VQA val set:   4%|4         | 120/2671 [01:26<31:10,  1.36it/s]Evaluating on VQA val set:   5%|4         | 121/2671 [01:27<30:56,  1.37it/s]Evaluating on VQA val set:   5%|4         | 122/2671 [01:27<31:18,  1.36it/s]Evaluating on VQA val set:   5%|4         | 123/2671 [01:28<31:15,  1.36it/s]Evaluating on VQA val set:   5%|4         | 124/2671 [01:29<31:18,  1.36it/s]Evaluating on VQA val set:   5%|4         | 125/2671 [01:29<30:50,  1.38it/s]Evaluating on VQA val set:   5%|4         | 126/2671 [01:30<30:01,  1.41it/s]Evaluating on VQA val set:   5%|4         | 127/2671 [01:31<30:26,  1.39it/s]Evaluating on VQA val set:   5%|4         | 128/2671 [01:32<30:59,  1.37it/s]Evaluating on VQA val set:   5%|4         | 129/2671 [01:32<30:05,  1.41it/s]Evaluating on VQA val set:   5%|4         | 130/2671 [01:33<30:28,  1.39it/s]Evaluating on VQA val set:   5%|4         | 131/2671 [01:34<29:37,  1.43it/s]Evaluating on VQA val set:   5%|4         | 132/2671 [01:34<29:04,  1.46it/s]Evaluating on VQA val set:   5%|4         | 133/2671 [01:35<30:02,  1.41it/s]Evaluating on VQA val set:   5%|5         | 134/2671 [01:36<30:00,  1.41it/s]Evaluating on VQA val set:   5%|5         | 135/2671 [01:37<30:37,  1.38it/s]Evaluating on VQA val set:   5%|5         | 136/2671 [01:37<30:52,  1.37it/s]Evaluating on VQA val set:   5%|5         | 137/2671 [01:38<30:05,  1.40it/s]Evaluating on VQA val set:   5%|5         | 138/2671 [01:39<30:21,  1.39it/s]Evaluating on VQA val set:   5%|5         | 139/2671 [01:39<31:01,  1.36it/s]Evaluating on VQA val set:   5%|5         | 140/2671 [01:40<31:06,  1.36it/s]Evaluating on VQA val set:   5%|5         | 141/2671 [01:41<31:46,  1.33it/s]Evaluating on VQA val set:   5%|5         | 142/2671 [01:42<31:30,  1.34it/s]Evaluating on VQA val set:   5%|5         | 143/2671 [01:43<31:45,  1.33it/s]Evaluating on VQA val set:   5%|5         | 144/2671 [01:43<31:25,  1.34it/s]Evaluating on VQA val set:   5%|5         | 145/2671 [01:44<30:12,  1.39it/s]Evaluating on VQA val set:   5%|5         | 146/2671 [01:45<28:41,  1.47it/s]Evaluating on VQA val set:   6%|5         | 147/2671 [01:45<28:31,  1.47it/s]Evaluating on VQA val set:   6%|5         | 148/2671 [01:46<28:08,  1.49it/s]Evaluating on VQA val set:   6%|5         | 149/2671 [01:47<29:40,  1.42it/s]Evaluating on VQA val set:   6%|5         | 150/2671 [01:47<29:24,  1.43it/s]Evaluating on VQA val set:   6%|5         | 151/2671 [01:48<28:58,  1.45it/s]Evaluating on VQA val set:   6%|5         | 152/2671 [01:49<29:27,  1.43it/s]Evaluating on VQA val set:   6%|5         | 153/2671 [01:49<30:01,  1.40it/s]Evaluating on VQA val set:   6%|5         | 154/2671 [01:50<29:49,  1.41it/s]Evaluating on VQA val set:   6%|5         | 155/2671 [01:51<29:21,  1.43it/s]Evaluating on VQA val set:   6%|5         | 156/2671 [01:52<29:35,  1.42it/s]Evaluating on VQA val set:   6%|5         | 157/2671 [01:52<28:47,  1.46it/s]Evaluating on VQA val set:   6%|5         | 158/2671 [01:53<29:31,  1.42it/s]Evaluating on VQA val set:   6%|5         | 159/2671 [01:54<30:54,  1.35it/s]Evaluating on VQA val set:   6%|5         | 160/2671 [01:55<31:34,  1.33it/s]Evaluating on VQA val set:   6%|6         | 161/2671 [01:55<31:56,  1.31it/s]Evaluating on VQA val set:   6%|6         | 162/2671 [01:56<31:12,  1.34it/s]Evaluating on VQA val set:   6%|6         | 163/2671 [01:57<31:21,  1.33it/s]Evaluating on VQA val set:   6%|6         | 164/2671 [01:58<31:44,  1.32it/s]Evaluating on VQA val set:   6%|6         | 165/2671 [01:58<31:44,  1.32it/s]Evaluating on VQA val set:   6%|6         | 166/2671 [01:59<29:24,  1.42it/s]Evaluating on VQA val set:   6%|6         | 167/2671 [02:00<30:19,  1.38it/s]Evaluating on VQA val set:   6%|6         | 168/2671 [02:00<30:54,  1.35it/s]Evaluating on VQA val set:   6%|6         | 169/2671 [02:01<31:06,  1.34it/s]Evaluating on VQA val set:   6%|6         | 170/2671 [02:02<31:07,  1.34it/s]Evaluating on VQA val set:   6%|6         | 171/2671 [02:03<31:18,  1.33it/s]Evaluating on VQA val set:   6%|6         | 172/2671 [02:03<30:38,  1.36it/s]Evaluating on VQA val set:   6%|6         | 173/2671 [02:04<30:55,  1.35it/s]Evaluating on VQA val set:   7%|6         | 174/2671 [02:05<29:49,  1.40it/s]Evaluating on VQA val set:   7%|6         | 175/2671 [02:05<28:33,  1.46it/s]Evaluating on VQA val set:   7%|6         | 176/2671 [02:06<29:10,  1.43it/s]Evaluating on VQA val set:   7%|6         | 177/2671 [02:07<28:31,  1.46it/s]Evaluating on VQA val set:   7%|6         | 178/2671 [02:08<29:34,  1.41it/s]Evaluating on VQA val set:   7%|6         | 179/2671 [02:08<30:09,  1.38it/s]Evaluating on VQA val set:   7%|6         | 180/2671 [02:09<31:00,  1.34it/s]Evaluating on VQA val set:   7%|6         | 181/2671 [02:10<30:36,  1.36it/s]Evaluating on VQA val set:   7%|6         | 182/2671 [02:11<30:47,  1.35it/s]Evaluating on VQA val set:   7%|6         | 183/2671 [02:11<30:27,  1.36it/s]Evaluating on VQA val set:   7%|6         | 184/2671 [02:12<31:00,  1.34it/s]Evaluating on VQA val set:   7%|6         | 185/2671 [02:13<29:37,  1.40it/s]Evaluating on VQA val set:   7%|6         | 186/2671 [02:14<29:47,  1.39it/s]Evaluating on VQA val set:   7%|7         | 187/2671 [02:14<29:49,  1.39it/s]Evaluating on VQA val set:   7%|7         | 188/2671 [02:15<28:54,  1.43it/s]Evaluating on VQA val set:   7%|7         | 189/2671 [02:16<29:52,  1.38it/s]Evaluating on VQA val set:   7%|7         | 190/2671 [02:16<30:49,  1.34it/s]Evaluating on VQA val set:   7%|7         | 191/2671 [02:17<30:56,  1.34it/s]Evaluating on VQA val set:   7%|7         | 192/2671 [02:18<30:36,  1.35it/s]Evaluating on VQA val set:   7%|7         | 193/2671 [02:19<30:10,  1.37it/s]Evaluating on VQA val set:   7%|7         | 194/2671 [02:19<29:10,  1.41it/s]Evaluating on VQA val set:   7%|7         | 195/2671 [02:20<29:15,  1.41it/s]Evaluating on VQA val set:   7%|7         | 196/2671 [02:21<29:24,  1.40it/s]Evaluating on VQA val set:   7%|7         | 197/2671 [02:21<29:30,  1.40it/s]Evaluating on VQA val set:   7%|7         | 198/2671 [02:22<28:26,  1.45it/s]Evaluating on VQA val set:   7%|7         | 199/2671 [02:23<28:38,  1.44it/s]Evaluating on VQA val set:   7%|7         | 200/2671 [02:24<29:01,  1.42it/s]Evaluating on VQA val set:   8%|7         | 201/2671 [02:24<28:12,  1.46it/s]Evaluating on VQA val set:   8%|7         | 202/2671 [02:25<28:08,  1.46it/s]Evaluating on VQA val set:   8%|7         | 203/2671 [02:25<27:52,  1.48it/s]Evaluating on VQA val set:   8%|7         | 204/2671 [02:26<28:39,  1.44it/s]Evaluating on VQA val set:   8%|7         | 205/2671 [02:27<29:26,  1.40it/s]Evaluating on VQA val set:   8%|7         | 206/2671 [02:28<29:11,  1.41it/s]Evaluating on VQA val set:   8%|7         | 207/2671 [02:28<28:25,  1.44it/s]Evaluating on VQA val set:   8%|7         | 208/2671 [02:29<27:39,  1.48it/s]Evaluating on VQA val set:   8%|7         | 209/2671 [02:30<27:21,  1.50it/s]Evaluating on VQA val set:   8%|7         | 210/2671 [02:30<27:56,  1.47it/s]Evaluating on VQA val set:   8%|7         | 211/2671 [02:31<28:40,  1.43it/s]Evaluating on VQA val set:   8%|7         | 212/2671 [02:32<29:13,  1.40it/s]Evaluating on VQA val set:   8%|7         | 213/2671 [02:33<29:22,  1.39it/s]Evaluating on VQA val set:   8%|8         | 214/2671 [02:33<29:54,  1.37it/s]Evaluating on VQA val set:   8%|8         | 215/2671 [02:34<29:40,  1.38it/s]Evaluating on VQA val set:   8%|8         | 216/2671 [02:35<29:00,  1.41it/s]Evaluating on VQA val set:   8%|8         | 217/2671 [02:35<29:27,  1.39it/s]Evaluating on VQA val set:   8%|8         | 218/2671 [02:36<29:50,  1.37it/s]Evaluating on VQA val set:   8%|8         | 219/2671 [02:37<28:48,  1.42it/s]Evaluating on VQA val set:   8%|8         | 220/2671 [02:38<29:19,  1.39it/s]Evaluating on VQA val set:   8%|8         | 221/2671 [02:38<29:54,  1.36it/s]Evaluating on VQA val set:   8%|8         | 222/2671 [02:39<29:11,  1.40it/s]Evaluating on VQA val set:   8%|8         | 223/2671 [02:40<29:23,  1.39it/s]Evaluating on VQA val set:   8%|8         | 224/2671 [02:41<29:57,  1.36it/s]Evaluating on VQA val set:   8%|8         | 225/2671 [02:41<29:06,  1.40it/s]Evaluating on VQA val set:   8%|8         | 226/2671 [02:42<29:14,  1.39it/s]Evaluating on VQA val set:   8%|8         | 227/2671 [02:43<29:12,  1.39it/s]Evaluating on VQA val set:   9%|8         | 228/2671 [02:43<28:22,  1.43it/s]Evaluating on VQA val set:   9%|8         | 229/2671 [02:44<29:34,  1.38it/s]Evaluating on VQA val set:   9%|8         | 230/2671 [02:45<30:30,  1.33it/s]Evaluating on VQA val set:   9%|8         | 231/2671 [02:46<30:28,  1.33it/s]Evaluating on VQA val set:   9%|8         | 232/2671 [02:46<30:35,  1.33it/s]Evaluating on VQA val set:   9%|8         | 233/2671 [02:47<30:16,  1.34it/s]Evaluating on VQA val set:   9%|8         | 234/2671 [02:48<28:48,  1.41it/s]Evaluating on VQA val set:   9%|8         | 235/2671 [02:48<28:41,  1.41it/s]Evaluating on VQA val set:   9%|8         | 236/2671 [02:49<29:12,  1.39it/s]Evaluating on VQA val set:   9%|8         | 237/2671 [02:50<28:36,  1.42it/s]Evaluating on VQA val set:   9%|8         | 238/2671 [02:51<27:52,  1.45it/s]Evaluating on VQA val set:   9%|8         | 239/2671 [02:51<28:50,  1.41it/s]Evaluating on VQA val set:   9%|8         | 240/2671 [02:52<28:38,  1.41it/s]Evaluating on VQA val set:   9%|9         | 241/2671 [02:53<28:04,  1.44it/s]Evaluating on VQA val set:   9%|9         | 242/2671 [02:53<28:14,  1.43it/s]Evaluating on VQA val set:   9%|9         | 243/2671 [02:54<28:55,  1.40it/s]Evaluating on VQA val set:   9%|9         | 244/2671 [02:55<28:12,  1.43it/s]Evaluating on VQA val set:   9%|9         | 245/2671 [02:55<28:35,  1.41it/s]Evaluating on VQA val set:   9%|9         | 246/2671 [02:56<28:24,  1.42it/s]Evaluating on VQA val set:   9%|9         | 247/2671 [02:57<28:14,  1.43it/s]Evaluating on VQA val set:   9%|9         | 248/2671 [02:58<28:50,  1.40it/s]Evaluating on VQA val set:   9%|9         | 249/2671 [02:58<28:27,  1.42it/s]Evaluating on VQA val set:   9%|9         | 250/2671 [02:59<29:01,  1.39it/s]Evaluating on VQA val set:   9%|9         | 251/2671 [03:00<29:45,  1.36it/s]Evaluating on VQA val set:   9%|9         | 252/2671 [03:01<29:22,  1.37it/s]Evaluating on VQA val set:   9%|9         | 253/2671 [03:01<29:34,  1.36it/s]Evaluating on VQA val set:  10%|9         | 254/2671 [03:02<29:03,  1.39it/s]Evaluating on VQA val set:  10%|9         | 255/2671 [03:03<27:41,  1.45it/s]Evaluating on VQA val set:  10%|9         | 256/2671 [03:03<28:17,  1.42it/s]Evaluating on VQA val set:  10%|9         | 257/2671 [03:04<28:10,  1.43it/s]Evaluating on VQA val set:  10%|9         | 258/2671 [03:05<27:56,  1.44it/s]Evaluating on VQA val set:  10%|9         | 259/2671 [03:05<27:53,  1.44it/s]Evaluating on VQA val set:  10%|9         | 260/2671 [03:06<28:04,  1.43it/s]Evaluating on VQA val set:  10%|9         | 261/2671 [03:07<27:43,  1.45it/s]Evaluating on VQA val set:  10%|9         | 262/2671 [03:07<26:47,  1.50it/s]Evaluating on VQA val set:  10%|9         | 263/2671 [03:08<28:08,  1.43it/s]Evaluating on VQA val set:  10%|9         | 264/2671 [03:09<28:13,  1.42it/s]Evaluating on VQA val set:  10%|9         | 265/2671 [03:10<28:51,  1.39it/s]Evaluating on VQA val set:  10%|9         | 266/2671 [03:10<28:49,  1.39it/s]Evaluating on VQA val set:  10%|9         | 267/2671 [03:11<29:06,  1.38it/s]Evaluating on VQA val set:  10%|#         | 268/2671 [03:12<28:28,  1.41it/s]Evaluating on VQA val set:  10%|#         | 269/2671 [03:13<28:45,  1.39it/s]Evaluating on VQA val set:  10%|#         | 270/2671 [03:13<28:23,  1.41it/s]Evaluating on VQA val set:  10%|#         | 271/2671 [03:14<28:28,  1.40it/s]Evaluating on VQA val set:  10%|#         | 272/2671 [03:15<27:28,  1.46it/s]Evaluating on VQA val set:  10%|#         | 273/2671 [03:15<27:35,  1.45it/s]Evaluating on VQA val set:  10%|#         | 274/2671 [03:16<28:27,  1.40it/s]Evaluating on VQA val set:  10%|#         | 275/2671 [03:17<28:44,  1.39it/s]Evaluating on VQA val set:  10%|#         | 276/2671 [03:17<28:45,  1.39it/s]Evaluating on VQA val set:  10%|#         | 277/2671 [03:18<29:51,  1.34it/s]Evaluating on VQA val set:  10%|#         | 278/2671 [03:19<29:35,  1.35it/s]Evaluating on VQA val set:  10%|#         | 279/2671 [03:20<29:49,  1.34it/s]Evaluating on VQA val set:  10%|#         | 280/2671 [03:21<30:18,  1.31it/s]Evaluating on VQA val set:  11%|#         | 281/2671 [03:21<29:52,  1.33it/s]Evaluating on VQA val set:  11%|#         | 282/2671 [03:22<29:50,  1.33it/s]Evaluating on VQA val set:  11%|#         | 283/2671 [03:23<29:31,  1.35it/s]Evaluating on VQA val set:  11%|#         | 284/2671 [03:23<29:09,  1.36it/s]Evaluating on VQA val set:  11%|#         | 285/2671 [03:24<28:39,  1.39it/s]Evaluating on VQA val set:  11%|#         | 286/2671 [03:25<27:49,  1.43it/s]Evaluating on VQA val set:  11%|#         | 287/2671 [03:26<27:57,  1.42it/s]Evaluating on VQA val set:  11%|#         | 288/2671 [03:26<27:42,  1.43it/s]Evaluating on VQA val set:  11%|#         | 289/2671 [03:27<28:28,  1.39it/s]Evaluating on VQA val set:  11%|#         | 290/2671 [03:28<28:17,  1.40it/s]Evaluating on VQA val set:  11%|#         | 291/2671 [03:28<27:46,  1.43it/s]Evaluating on VQA val set:  11%|#         | 292/2671 [03:29<26:37,  1.49it/s]Evaluating on VQA val set:  11%|#         | 293/2671 [03:30<27:16,  1.45it/s]Evaluating on VQA val set:  11%|#1        | 294/2671 [03:30<26:55,  1.47it/s]Evaluating on VQA val set:  11%|#1        | 295/2671 [03:31<27:44,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 296/2671 [03:32<27:39,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 297/2671 [03:33<28:04,  1.41it/s]Evaluating on VQA val set:  11%|#1        | 298/2671 [03:33<28:12,  1.40it/s]Evaluating on VQA val set:  11%|#1        | 299/2671 [03:34<28:10,  1.40it/s]Evaluating on VQA val set:  11%|#1        | 300/2671 [03:35<27:55,  1.41it/s]Evaluating on VQA val set:  11%|#1        | 301/2671 [03:35<27:51,  1.42it/s]Evaluating on VQA val set:  11%|#1        | 302/2671 [03:36<27:43,  1.42it/s]Evaluating on VQA val set:  11%|#1        | 303/2671 [03:37<28:01,  1.41it/s]Evaluating on VQA val set:  11%|#1        | 304/2671 [03:37<28:08,  1.40it/s]Evaluating on VQA val set:  11%|#1        | 305/2671 [03:38<28:18,  1.39it/s]Evaluating on VQA val set:  11%|#1        | 306/2671 [03:39<26:57,  1.46it/s]Evaluating on VQA val set:  11%|#1        | 307/2671 [03:40<27:26,  1.44it/s]Evaluating on VQA val set:  12%|#1        | 308/2671 [03:40<27:44,  1.42it/s]Evaluating on VQA val set:  12%|#1        | 309/2671 [03:41<27:57,  1.41it/s]Evaluating on VQA val set:  12%|#1        | 310/2671 [03:42<27:54,  1.41it/s]Evaluating on VQA val set:  12%|#1        | 311/2671 [03:42<27:51,  1.41it/s]Evaluating on VQA val set:  12%|#1        | 312/2671 [03:43<28:19,  1.39it/s]Evaluating on VQA val set:  12%|#1        | 313/2671 [03:44<28:42,  1.37it/s]Evaluating on VQA val set:  12%|#1        | 314/2671 [03:45<28:02,  1.40it/s]Evaluating on VQA val set:  12%|#1        | 315/2671 [03:45<27:35,  1.42it/s]Evaluating on VQA val set:  12%|#1        | 316/2671 [03:46<28:03,  1.40it/s]Evaluating on VQA val set:  12%|#1        | 317/2671 [03:47<28:20,  1.38it/s]Evaluating on VQA val set:  12%|#1        | 318/2671 [03:48<28:46,  1.36it/s]Evaluating on VQA val set:  12%|#1        | 319/2671 [03:48<29:14,  1.34it/s]Evaluating on VQA val set:  12%|#1        | 320/2671 [03:49<28:54,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 321/2671 [03:50<28:52,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 322/2671 [03:50<29:01,  1.35it/s]Evaluating on VQA val set:  12%|#2        | 323/2671 [03:51<28:54,  1.35it/s]Evaluating on VQA val set:  12%|#2        | 324/2671 [03:52<28:05,  1.39it/s]Evaluating on VQA val set:  12%|#2        | 325/2671 [03:52<25:58,  1.51it/s]Evaluating on VQA val set:  12%|#2        | 326/2671 [03:53<26:45,  1.46it/s]Evaluating on VQA val set:  12%|#2        | 327/2671 [03:54<27:52,  1.40it/s]Evaluating on VQA val set:  12%|#2        | 328/2671 [03:55<28:26,  1.37it/s]Evaluating on VQA val set:  12%|#2        | 329/2671 [03:55<28:16,  1.38it/s]Evaluating on VQA val set:  12%|#2        | 330/2671 [03:56<28:47,  1.35it/s]Evaluating on VQA val set:  12%|#2        | 331/2671 [03:57<28:36,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 332/2671 [03:58<28:15,  1.38it/s]Evaluating on VQA val set:  12%|#2        | 333/2671 [03:58<28:14,  1.38it/s]Evaluating on VQA val set:  13%|#2        | 334/2671 [03:59<27:55,  1.40it/s]Evaluating on VQA val set:  13%|#2        | 335/2671 [04:00<26:36,  1.46it/s]Evaluating on VQA val set:  13%|#2        | 336/2671 [04:00<26:51,  1.45it/s]Evaluating on VQA val set:  13%|#2        | 337/2671 [04:01<26:17,  1.48it/s]Evaluating on VQA val set:  13%|#2        | 338/2671 [04:02<27:03,  1.44it/s]Evaluating on VQA val set:  13%|#2        | 339/2671 [04:02<27:10,  1.43it/s]Evaluating on VQA val set:  13%|#2        | 340/2671 [04:03<26:14,  1.48it/s]Evaluating on VQA val set:  13%|#2        | 341/2671 [04:04<27:12,  1.43it/s]Evaluating on VQA val set:  13%|#2        | 342/2671 [04:05<27:56,  1.39it/s]Evaluating on VQA val set:  13%|#2        | 343/2671 [04:05<28:20,  1.37it/s]Evaluating on VQA val set:  13%|#2        | 344/2671 [04:06<27:44,  1.40it/s]Evaluating on VQA val set:  13%|#2        | 345/2671 [04:07<27:37,  1.40it/s]Evaluating on VQA val set:  13%|#2        | 346/2671 [04:07<27:26,  1.41it/s]Evaluating on VQA val set:  13%|#2        | 347/2671 [04:08<27:46,  1.39it/s]Evaluating on VQA val set:  13%|#3        | 348/2671 [04:09<27:11,  1.42it/s]Evaluating on VQA val set:  13%|#3        | 349/2671 [04:10<27:22,  1.41it/s]Evaluating on VQA val set:  13%|#3        | 350/2671 [04:10<26:55,  1.44it/s]Evaluating on VQA val set:  13%|#3        | 351/2671 [04:11<27:31,  1.40it/s]Evaluating on VQA val set:  13%|#3        | 352/2671 [04:12<28:19,  1.36it/s]Evaluating on VQA val set:  13%|#3        | 353/2671 [04:12<27:51,  1.39it/s]Evaluating on VQA val set:  13%|#3        | 354/2671 [04:13<27:44,  1.39it/s]Evaluating on VQA val set:  13%|#3        | 355/2671 [04:14<26:29,  1.46it/s]Evaluating on VQA val set:  13%|#3        | 356/2671 [04:15<27:11,  1.42it/s]Evaluating on VQA val set:  13%|#3        | 357/2671 [04:15<27:57,  1.38it/s]Evaluating on VQA val set:  13%|#3        | 358/2671 [04:16<28:00,  1.38it/s]Evaluating on VQA val set:  13%|#3        | 359/2671 [04:17<28:47,  1.34it/s]Evaluating on VQA val set:  13%|#3        | 360/2671 [04:17<27:28,  1.40it/s]Evaluating on VQA val set:  14%|#3        | 361/2671 [04:18<27:47,  1.39it/s]Evaluating on VQA val set:  14%|#3        | 362/2671 [04:19<27:30,  1.40it/s]Evaluating on VQA val set:  14%|#3        | 363/2671 [04:20<27:34,  1.39it/s]Evaluating on VQA val set:  14%|#3        | 364/2671 [04:20<27:28,  1.40it/s]Evaluating on VQA val set:  14%|#3        | 365/2671 [04:21<28:17,  1.36it/s]Evaluating on VQA val set:  14%|#3        | 366/2671 [04:22<29:07,  1.32it/s]Evaluating on VQA val set:  14%|#3        | 367/2671 [04:23<29:00,  1.32it/s]Evaluating on VQA val set:  14%|#3        | 368/2671 [04:23<28:35,  1.34it/s]Evaluating on VQA val set:  14%|#3        | 369/2671 [04:24<28:49,  1.33it/s]Evaluating on VQA val set:  14%|#3        | 370/2671 [04:25<29:01,  1.32it/s]Evaluating on VQA val set:  14%|#3        | 371/2671 [04:26<28:50,  1.33it/s]Evaluating on VQA val set:  14%|#3        | 372/2671 [04:26<28:17,  1.35it/s]Evaluating on VQA val set:  14%|#3        | 373/2671 [04:27<28:31,  1.34it/s]Evaluating on VQA val set:  14%|#4        | 374/2671 [04:28<28:31,  1.34it/s]Evaluating on VQA val set:  14%|#4        | 375/2671 [04:29<28:23,  1.35it/s]Evaluating on VQA val set:  14%|#4        | 376/2671 [04:29<28:12,  1.36it/s]Evaluating on VQA val set:  14%|#4        | 377/2671 [04:30<28:04,  1.36it/s]Evaluating on VQA val set:  14%|#4        | 378/2671 [04:31<27:57,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 379/2671 [04:32<27:53,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 380/2671 [04:32<28:05,  1.36it/s]Evaluating on VQA val set:  14%|#4        | 381/2671 [04:33<27:17,  1.40it/s]Evaluating on VQA val set:  14%|#4        | 382/2671 [04:34<27:50,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 383/2671 [04:34<28:25,  1.34it/s]Evaluating on VQA val set:  14%|#4        | 384/2671 [04:35<27:16,  1.40it/s]Evaluating on VQA val set:  14%|#4        | 385/2671 [04:36<27:21,  1.39it/s]Evaluating on VQA val set:  14%|#4        | 386/2671 [04:37<27:03,  1.41it/s]Evaluating on VQA val set:  14%|#4        | 387/2671 [04:37<26:55,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 388/2671 [04:38<27:03,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 389/2671 [04:39<27:01,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 390/2671 [04:39<27:06,  1.40it/s]Evaluating on VQA val set:  15%|#4        | 391/2671 [04:40<27:04,  1.40it/s]Evaluating on VQA val set:  15%|#4        | 392/2671 [04:41<27:25,  1.39it/s]Evaluating on VQA val set:  15%|#4        | 393/2671 [04:42<27:47,  1.37it/s]Evaluating on VQA val set:  15%|#4        | 394/2671 [04:42<27:13,  1.39it/s]Evaluating on VQA val set:  15%|#4        | 395/2671 [04:43<27:56,  1.36it/s]Evaluating on VQA val set:  15%|#4        | 396/2671 [04:44<26:15,  1.44it/s]Evaluating on VQA val set:  15%|#4        | 397/2671 [04:44<26:08,  1.45it/s]Evaluating on VQA val set:  15%|#4        | 398/2671 [04:45<26:17,  1.44it/s]Evaluating on VQA val set:  15%|#4        | 399/2671 [04:46<27:01,  1.40it/s]Evaluating on VQA val set:  15%|#4        | 400/2671 [04:47<27:39,  1.37it/s]Evaluating on VQA val set:  15%|#5        | 401/2671 [04:47<27:06,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 402/2671 [04:48<27:09,  1.39it/s]Evaluating on VQA val set:  15%|#5        | 403/2671 [04:49<27:49,  1.36it/s]Evaluating on VQA val set:  15%|#5        | 404/2671 [04:49<27:26,  1.38it/s]Evaluating on VQA val set:  15%|#5        | 405/2671 [04:50<26:12,  1.44it/s]Evaluating on VQA val set:  15%|#5        | 406/2671 [04:51<26:00,  1.45it/s]Evaluating on VQA val set:  15%|#5        | 407/2671 [04:52<26:43,  1.41it/s]Evaluating on VQA val set:  15%|#5        | 408/2671 [04:52<26:59,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 409/2671 [04:53<27:11,  1.39it/s]Evaluating on VQA val set:  15%|#5        | 410/2671 [04:54<26:55,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 411/2671 [04:54<26:28,  1.42it/s]Evaluating on VQA val set:  15%|#5        | 412/2671 [04:55<26:36,  1.42it/s]Evaluating on VQA val set:  15%|#5        | 413/2671 [04:56<26:06,  1.44it/s]Evaluating on VQA val set:  15%|#5        | 414/2671 [04:56<26:40,  1.41it/s]Evaluating on VQA val set:  16%|#5        | 415/2671 [04:57<25:38,  1.47it/s]Evaluating on VQA val set:  16%|#5        | 416/2671 [04:58<25:22,  1.48it/s]Evaluating on VQA val set:  16%|#5        | 417/2671 [04:58<25:14,  1.49it/s]Evaluating on VQA val set:  16%|#5        | 418/2671 [04:59<25:39,  1.46it/s]Evaluating on VQA val set:  16%|#5        | 419/2671 [05:00<26:26,  1.42it/s]Evaluating on VQA val set:  16%|#5        | 420/2671 [05:01<25:51,  1.45it/s]Evaluating on VQA val set:  16%|#5        | 421/2671 [05:01<26:00,  1.44it/s]Evaluating on VQA val set:  16%|#5        | 422/2671 [05:02<27:01,  1.39it/s]Evaluating on VQA val set:  16%|#5        | 423/2671 [05:03<27:23,  1.37it/s]Evaluating on VQA val set:  16%|#5        | 424/2671 [05:04<27:28,  1.36it/s]Evaluating on VQA val set:  16%|#5        | 425/2671 [05:04<27:12,  1.38it/s]Evaluating on VQA val set:  16%|#5        | 426/2671 [05:05<27:42,  1.35it/s]Evaluating on VQA val set:  16%|#5        | 427/2671 [05:06<28:08,  1.33it/s]Evaluating on VQA val set:  16%|#6        | 428/2671 [05:06<27:04,  1.38it/s]Evaluating on VQA val set:  16%|#6        | 429/2671 [05:07<26:45,  1.40it/s]Evaluating on VQA val set:  16%|#6        | 430/2671 [05:08<25:55,  1.44it/s]Evaluating on VQA val set:  16%|#6        | 431/2671 [05:09<26:13,  1.42it/s]Evaluating on VQA val set:  16%|#6        | 432/2671 [05:09<24:21,  1.53it/s]Evaluating on VQA val set:  16%|#6        | 433/2671 [05:10<23:47,  1.57it/s]Evaluating on VQA val set:  16%|#6        | 434/2671 [05:10<24:30,  1.52it/s]Evaluating on VQA val set:  16%|#6        | 435/2671 [05:11<25:28,  1.46it/s]Evaluating on VQA val set:  16%|#6        | 436/2671 [05:12<25:45,  1.45it/s]Evaluating on VQA val set:  16%|#6        | 437/2671 [05:13<25:54,  1.44it/s]Evaluating on VQA val set:  16%|#6        | 438/2671 [05:13<26:10,  1.42it/s]Evaluating on VQA val set:  16%|#6        | 439/2671 [05:14<26:38,  1.40it/s]Evaluating on VQA val set:  16%|#6        | 440/2671 [05:15<27:07,  1.37it/s]Evaluating on VQA val set:  17%|#6        | 441/2671 [05:15<26:59,  1.38it/s]Evaluating on VQA val set:  17%|#6        | 442/2671 [05:16<26:33,  1.40it/s]Evaluating on VQA val set:  17%|#6        | 443/2671 [05:17<26:39,  1.39it/s]Evaluating on VQA val set:  17%|#6        | 444/2671 [05:18<26:13,  1.41it/s]Evaluating on VQA val set:  17%|#6        | 445/2671 [05:18<26:15,  1.41it/s]Evaluating on VQA val set:  17%|#6        | 446/2671 [05:19<25:55,  1.43it/s]Evaluating on VQA val set:  17%|#6        | 447/2671 [05:20<25:48,  1.44it/s]Evaluating on VQA val set:  17%|#6        | 448/2671 [05:20<26:09,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 449/2671 [05:21<26:35,  1.39it/s]Evaluating on VQA val set:  17%|#6        | 450/2671 [05:22<26:53,  1.38it/s]Evaluating on VQA val set:  17%|#6        | 451/2671 [05:23<26:29,  1.40it/s]Evaluating on VQA val set:  17%|#6        | 452/2671 [05:23<26:26,  1.40it/s]Evaluating on VQA val set:  17%|#6        | 453/2671 [05:24<26:17,  1.41it/s]Evaluating on VQA val set:  17%|#6        | 454/2671 [05:25<26:10,  1.41it/s]Evaluating on VQA val set:  17%|#7        | 455/2671 [05:25<25:28,  1.45it/s]Evaluating on VQA val set:  17%|#7        | 456/2671 [05:26<24:44,  1.49it/s]Evaluating on VQA val set:  17%|#7        | 457/2671 [05:27<25:48,  1.43it/s]Evaluating on VQA val set:  17%|#7        | 458/2671 [05:27<25:33,  1.44it/s]Evaluating on VQA val set:  17%|#7        | 459/2671 [05:28<26:20,  1.40it/s]Evaluating on VQA val set:  17%|#7        | 460/2671 [05:29<27:18,  1.35it/s]Evaluating on VQA val set:  17%|#7        | 461/2671 [05:30<27:13,  1.35it/s]Evaluating on VQA val set:  17%|#7        | 462/2671 [05:30<27:11,  1.35it/s]Evaluating on VQA val set:  17%|#7        | 463/2671 [05:31<27:01,  1.36it/s]Evaluating on VQA val set:  17%|#7        | 464/2671 [05:32<26:44,  1.38it/s]Evaluating on VQA val set:  17%|#7        | 465/2671 [05:32<25:46,  1.43it/s]Evaluating on VQA val set:  17%|#7        | 466/2671 [05:33<25:40,  1.43it/s]Evaluating on VQA val set:  17%|#7        | 467/2671 [05:34<25:32,  1.44it/s]Evaluating on VQA val set:  18%|#7        | 468/2671 [05:35<26:39,  1.38it/s]Evaluating on VQA val set:  18%|#7        | 469/2671 [05:35<26:36,  1.38it/s]Evaluating on VQA val set:  18%|#7        | 470/2671 [05:36<27:11,  1.35it/s]Evaluating on VQA val set:  18%|#7        | 471/2671 [05:37<27:24,  1.34it/s]Evaluating on VQA val set:  18%|#7        | 472/2671 [05:38<26:52,  1.36it/s]Evaluating on VQA val set:  18%|#7        | 473/2671 [05:38<27:23,  1.34it/s]Evaluating on VQA val set:  18%|#7        | 474/2671 [05:39<26:53,  1.36it/s]Evaluating on VQA val set:  18%|#7        | 475/2671 [05:40<26:39,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 476/2671 [05:41<27:00,  1.35it/s]Evaluating on VQA val set:  18%|#7        | 477/2671 [05:41<27:15,  1.34it/s]Evaluating on VQA val set:  18%|#7        | 478/2671 [05:42<27:08,  1.35it/s]Evaluating on VQA val set:  18%|#7        | 479/2671 [05:43<27:10,  1.34it/s]Evaluating on VQA val set:  18%|#7        | 480/2671 [05:43<25:51,  1.41it/s]Evaluating on VQA val set:  18%|#8        | 481/2671 [05:44<25:26,  1.43it/s]Evaluating on VQA val set:  18%|#8        | 482/2671 [05:45<24:41,  1.48it/s]Evaluating on VQA val set:  18%|#8        | 483/2671 [05:45<24:09,  1.51it/s]Evaluating on VQA val set:  18%|#8        | 484/2671 [05:46<24:53,  1.46it/s]Evaluating on VQA val set:  18%|#8        | 485/2671 [05:47<24:22,  1.49it/s]Evaluating on VQA val set:  18%|#8        | 486/2671 [05:47<24:42,  1.47it/s]Evaluating on VQA val set:  18%|#8        | 487/2671 [05:48<25:36,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 488/2671 [05:49<25:36,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 489/2671 [05:50<25:37,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 490/2671 [05:50<25:39,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 491/2671 [05:51<26:39,  1.36it/s]Evaluating on VQA val set:  18%|#8        | 492/2671 [05:52<26:27,  1.37it/s]Evaluating on VQA val set:  18%|#8        | 493/2671 [05:53<26:36,  1.36it/s]Evaluating on VQA val set:  18%|#8        | 494/2671 [05:53<27:12,  1.33it/s]Evaluating on VQA val set:  19%|#8        | 495/2671 [05:54<26:45,  1.36it/s]Evaluating on VQA val set:  19%|#8        | 496/2671 [05:55<26:49,  1.35it/s]Evaluating on VQA val set:  19%|#8        | 497/2671 [05:56<26:37,  1.36it/s]Evaluating on VQA val set:  19%|#8        | 498/2671 [05:56<26:54,  1.35it/s]Evaluating on VQA val set:  19%|#8        | 499/2671 [05:57<27:02,  1.34it/s]Evaluating on VQA val set:  19%|#8        | 500/2671 [05:58<27:17,  1.33it/s]Evaluating on VQA val set:  19%|#8        | 501/2671 [05:59<27:07,  1.33it/s]Evaluating on VQA val set:  19%|#8        | 502/2671 [05:59<26:24,  1.37it/s]Evaluating on VQA val set:  19%|#8        | 503/2671 [06:00<26:54,  1.34it/s]Evaluating on VQA val set:  19%|#8        | 504/2671 [06:01<27:04,  1.33it/s]Evaluating on VQA val set:  19%|#8        | 505/2671 [06:02<27:26,  1.32it/s]Evaluating on VQA val set:  19%|#8        | 506/2671 [06:02<26:46,  1.35it/s]Evaluating on VQA val set:  19%|#8        | 507/2671 [06:03<25:13,  1.43it/s]Evaluating on VQA val set:  19%|#9        | 508/2671 [06:04<24:58,  1.44it/s]Evaluating on VQA val set:  19%|#9        | 509/2671 [06:04<25:33,  1.41it/s]Evaluating on VQA val set:  19%|#9        | 510/2671 [06:05<24:59,  1.44it/s]Evaluating on VQA val set:  19%|#9        | 511/2671 [06:06<25:25,  1.42it/s]Evaluating on VQA val set:  19%|#9        | 512/2671 [06:06<25:41,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 513/2671 [06:07<25:28,  1.41it/s]Evaluating on VQA val set:  19%|#9        | 514/2671 [06:08<25:58,  1.38it/s]Evaluating on VQA val set:  19%|#9        | 515/2671 [06:09<25:44,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 516/2671 [06:09<24:57,  1.44it/s]Evaluating on VQA val set:  19%|#9        | 517/2671 [06:10<25:35,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 518/2671 [06:11<26:08,  1.37it/s]Evaluating on VQA val set:  19%|#9        | 519/2671 [06:11<26:05,  1.37it/s]Evaluating on VQA val set:  19%|#9        | 520/2671 [06:12<26:11,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 521/2671 [06:13<26:08,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 522/2671 [06:14<25:08,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 523/2671 [06:14<25:16,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 524/2671 [06:15<25:10,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 525/2671 [06:16<25:20,  1.41it/s]Evaluating on VQA val set:  20%|#9        | 526/2671 [06:16<25:53,  1.38it/s]Evaluating on VQA val set:  20%|#9        | 527/2671 [06:17<26:06,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 528/2671 [06:18<26:23,  1.35it/s]Evaluating on VQA val set:  20%|#9        | 529/2671 [06:19<26:16,  1.36it/s]Evaluating on VQA val set:  20%|#9        | 530/2671 [06:19<25:44,  1.39it/s]Evaluating on VQA val set:  20%|#9        | 531/2671 [06:20<26:01,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 532/2671 [06:21<25:03,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 533/2671 [06:21<24:43,  1.44it/s]Evaluating on VQA val set:  20%|#9        | 534/2671 [06:22<24:13,  1.47it/s]Evaluating on VQA val set:  20%|##        | 535/2671 [06:23<23:43,  1.50it/s]Evaluating on VQA val set:  20%|##        | 536/2671 [06:23<24:31,  1.45it/s]Evaluating on VQA val set:  20%|##        | 537/2671 [06:24<24:51,  1.43it/s]Evaluating on VQA val set:  20%|##        | 538/2671 [06:25<25:00,  1.42it/s]Evaluating on VQA val set:  20%|##        | 539/2671 [06:26<25:22,  1.40it/s]Evaluating on VQA val set:  20%|##        | 540/2671 [06:26<25:58,  1.37it/s]Evaluating on VQA val set:  20%|##        | 541/2671 [06:27<25:48,  1.38it/s]Evaluating on VQA val set:  20%|##        | 542/2671 [06:28<24:42,  1.44it/s]Evaluating on VQA val set:  20%|##        | 543/2671 [06:29<25:13,  1.41it/s]Evaluating on VQA val set:  20%|##        | 544/2671 [06:29<25:22,  1.40it/s]Evaluating on VQA val set:  20%|##        | 545/2671 [06:30<25:02,  1.42it/s]Evaluating on VQA val set:  20%|##        | 546/2671 [06:31<25:39,  1.38it/s]Evaluating on VQA val set:  20%|##        | 547/2671 [06:31<25:44,  1.38it/s]Evaluating on VQA val set:  21%|##        | 548/2671 [06:32<25:44,  1.37it/s]Evaluating on VQA val set:  21%|##        | 549/2671 [06:33<25:15,  1.40it/s]Evaluating on VQA val set:  21%|##        | 550/2671 [06:34<26:03,  1.36it/s]Evaluating on VQA val set:  21%|##        | 551/2671 [06:34<25:23,  1.39it/s]Evaluating on VQA val set:  21%|##        | 552/2671 [06:35<24:57,  1.42it/s]Evaluating on VQA val set:  21%|##        | 553/2671 [06:36<24:03,  1.47it/s]Evaluating on VQA val set:  21%|##        | 554/2671 [06:36<24:42,  1.43it/s]Evaluating on VQA val set:  21%|##        | 555/2671 [06:37<25:05,  1.41it/s]Evaluating on VQA val set:  21%|##        | 556/2671 [06:38<25:24,  1.39it/s]Evaluating on VQA val set:  21%|##        | 557/2671 [06:39<26:33,  1.33it/s]Evaluating on VQA val set:  21%|##        | 558/2671 [06:39<25:58,  1.36it/s]Evaluating on VQA val set:  21%|##        | 559/2671 [06:40<24:48,  1.42it/s]Evaluating on VQA val set:  21%|##        | 560/2671 [06:41<25:28,  1.38it/s]Evaluating on VQA val set:  21%|##1       | 561/2671 [06:41<24:46,  1.42it/s]Evaluating on VQA val set:  21%|##1       | 562/2671 [06:42<25:08,  1.40it/s]Evaluating on VQA val set:  21%|##1       | 563/2671 [06:43<25:32,  1.38it/s]Evaluating on VQA val set:  21%|##1       | 564/2671 [06:44<26:16,  1.34it/s]Evaluating on VQA val set:  21%|##1       | 565/2671 [06:44<25:04,  1.40it/s]Evaluating on VQA val set:  21%|##1       | 566/2671 [06:45<25:15,  1.39it/s]Evaluating on VQA val set:  21%|##1       | 567/2671 [06:46<23:47,  1.47it/s]Evaluating on VQA val set:  21%|##1       | 568/2671 [06:46<24:30,  1.43it/s]Evaluating on VQA val set:  21%|##1       | 569/2671 [06:47<24:31,  1.43it/s]Evaluating on VQA val set:  21%|##1       | 570/2671 [06:48<25:09,  1.39it/s]Evaluating on VQA val set:  21%|##1       | 571/2671 [06:49<25:12,  1.39it/s]Evaluating on VQA val set:  21%|##1       | 572/2671 [06:49<25:21,  1.38it/s]Evaluating on VQA val set:  21%|##1       | 573/2671 [06:50<25:16,  1.38it/s]Evaluating on VQA val set:  21%|##1       | 574/2671 [06:51<23:50,  1.47it/s]Evaluating on VQA val set:  22%|##1       | 575/2671 [06:51<24:16,  1.44it/s]Evaluating on VQA val set:  22%|##1       | 576/2671 [06:52<25:18,  1.38it/s]Evaluating on VQA val set:  22%|##1       | 577/2671 [06:53<25:36,  1.36it/s]Evaluating on VQA val set:  22%|##1       | 578/2671 [06:54<25:53,  1.35it/s]Evaluating on VQA val set:  22%|##1       | 579/2671 [06:54<25:42,  1.36it/s]Evaluating on VQA val set:  22%|##1       | 580/2671 [06:55<25:20,  1.38it/s]Evaluating on VQA val set:  22%|##1       | 581/2671 [06:56<24:51,  1.40it/s]Evaluating on VQA val set:  22%|##1       | 582/2671 [06:56<24:44,  1.41it/s]Evaluating on VQA val set:  22%|##1       | 583/2671 [06:57<25:16,  1.38it/s]Evaluating on VQA val set:  22%|##1       | 584/2671 [06:58<25:02,  1.39it/s]Evaluating on VQA val set:  22%|##1       | 585/2671 [06:59<25:35,  1.36it/s]Evaluating on VQA val set:  22%|##1       | 586/2671 [06:59<25:19,  1.37it/s]Evaluating on VQA val set:  22%|##1       | 587/2671 [07:00<24:45,  1.40it/s]Evaluating on VQA val set:  22%|##2       | 588/2671 [07:01<24:54,  1.39it/s]Evaluating on VQA val set:  22%|##2       | 589/2671 [07:02<24:56,  1.39it/s]Evaluating on VQA val set:  22%|##2       | 590/2671 [07:02<25:15,  1.37it/s]Evaluating on VQA val set:  22%|##2       | 591/2671 [07:03<24:38,  1.41it/s]Evaluating on VQA val set:  22%|##2       | 592/2671 [07:04<24:02,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 593/2671 [07:04<24:09,  1.43it/s]Evaluating on VQA val set:  22%|##2       | 594/2671 [07:05<24:06,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 595/2671 [07:06<24:25,  1.42it/s]Evaluating on VQA val set:  22%|##2       | 596/2671 [07:06<24:28,  1.41it/s]Evaluating on VQA val set:  22%|##2       | 597/2671 [07:07<24:37,  1.40it/s]Evaluating on VQA val set:  22%|##2       | 598/2671 [07:08<24:58,  1.38it/s]Evaluating on VQA val set:  22%|##2       | 599/2671 [07:09<25:22,  1.36it/s]Evaluating on VQA val set:  22%|##2       | 600/2671 [07:09<24:23,  1.42it/s]Evaluating on VQA val set:  23%|##2       | 601/2671 [07:10<24:42,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 602/2671 [07:11<24:35,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 603/2671 [07:12<24:43,  1.39it/s]Evaluating on VQA val set:  23%|##2       | 604/2671 [07:12<24:10,  1.42it/s]Evaluating on VQA val set:  23%|##2       | 605/2671 [07:13<24:33,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 606/2671 [07:14<24:35,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 607/2671 [07:14<24:17,  1.42it/s]Evaluating on VQA val set:  23%|##2       | 608/2671 [07:15<23:55,  1.44it/s]Evaluating on VQA val set:  23%|##2       | 609/2671 [07:16<23:37,  1.45it/s]Evaluating on VQA val set:  23%|##2       | 610/2671 [07:16<24:21,  1.41it/s]Evaluating on VQA val set:  23%|##2       | 611/2671 [07:17<24:20,  1.41it/s]Evaluating on VQA val set:  23%|##2       | 612/2671 [07:18<25:04,  1.37it/s]Evaluating on VQA val set:  23%|##2       | 613/2671 [07:19<25:22,  1.35it/s]Evaluating on VQA val set:  23%|##2       | 614/2671 [07:19<25:31,  1.34it/s]Evaluating on VQA val set:  23%|##3       | 615/2671 [07:20<25:22,  1.35it/s]Evaluating on VQA val set:  23%|##3       | 616/2671 [07:21<24:49,  1.38it/s]Evaluating on VQA val set:  23%|##3       | 617/2671 [07:22<25:03,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 618/2671 [07:22<24:14,  1.41it/s]Evaluating on VQA val set:  23%|##3       | 619/2671 [07:23<24:05,  1.42it/s]Evaluating on VQA val set:  23%|##3       | 620/2671 [07:24<24:33,  1.39it/s]Evaluating on VQA val set:  23%|##3       | 621/2671 [07:24<25:05,  1.36it/s]Evaluating on VQA val set:  23%|##3       | 622/2671 [07:25<25:18,  1.35it/s]Evaluating on VQA val set:  23%|##3       | 623/2671 [07:26<25:04,  1.36it/s]Evaluating on VQA val set:  23%|##3       | 624/2671 [07:27<24:23,  1.40it/s]Evaluating on VQA val set:  23%|##3       | 625/2671 [07:27<25:00,  1.36it/s]Evaluating on VQA val set:  23%|##3       | 626/2671 [07:28<25:16,  1.35it/s]Evaluating on VQA val set:  23%|##3       | 627/2671 [07:29<25:14,  1.35it/s]Evaluating on VQA val set:  24%|##3       | 628/2671 [07:30<25:30,  1.33it/s]Evaluating on VQA val set:  24%|##3       | 629/2671 [07:30<24:48,  1.37it/s]Evaluating on VQA val set:  24%|##3       | 630/2671 [07:31<24:17,  1.40it/s]Evaluating on VQA val set:  24%|##3       | 631/2671 [07:32<24:26,  1.39it/s]Evaluating on VQA val set:  24%|##3       | 632/2671 [07:32<24:04,  1.41it/s]Evaluating on VQA val set:  24%|##3       | 633/2671 [07:33<23:14,  1.46it/s]Evaluating on VQA val set:  24%|##3       | 634/2671 [07:34<23:46,  1.43it/s]Evaluating on VQA val set:  24%|##3       | 635/2671 [07:35<24:37,  1.38it/s]Evaluating on VQA val set:  24%|##3       | 636/2671 [07:35<24:50,  1.37it/s]Evaluating on VQA val set:  24%|##3       | 637/2671 [07:36<24:27,  1.39it/s]Evaluating on VQA val set:  24%|##3       | 638/2671 [07:37<24:29,  1.38it/s]Evaluating on VQA val set:  24%|##3       | 639/2671 [07:38<24:35,  1.38it/s]Evaluating on VQA val set:  24%|##3       | 640/2671 [07:38<24:29,  1.38it/s]Evaluating on VQA val set:  24%|##3       | 641/2671 [07:39<24:49,  1.36it/s]Evaluating on VQA val set:  24%|##4       | 642/2671 [07:40<24:12,  1.40it/s]Evaluating on VQA val set:  24%|##4       | 643/2671 [07:40<23:43,  1.42it/s]Evaluating on VQA val set:  24%|##4       | 644/2671 [07:41<24:04,  1.40it/s]Evaluating on VQA val set:  24%|##4       | 645/2671 [07:42<23:14,  1.45it/s]Evaluating on VQA val set:  24%|##4       | 646/2671 [07:42<22:01,  1.53it/s]Evaluating on VQA val set:  24%|##4       | 647/2671 [07:43<23:12,  1.45it/s]Evaluating on VQA val set:  24%|##4       | 648/2671 [07:44<24:01,  1.40it/s]Evaluating on VQA val set:  24%|##4       | 649/2671 [07:44<23:43,  1.42it/s]Evaluating on VQA val set:  24%|##4       | 650/2671 [07:45<23:57,  1.41it/s]Evaluating on VQA val set:  24%|##4       | 651/2671 [07:46<24:32,  1.37it/s]Evaluating on VQA val set:  24%|##4       | 652/2671 [07:47<24:11,  1.39it/s]Evaluating on VQA val set:  24%|##4       | 653/2671 [07:47<24:27,  1.37it/s]Evaluating on VQA val set:  24%|##4       | 654/2671 [07:48<24:11,  1.39it/s]Evaluating on VQA val set:  25%|##4       | 655/2671 [07:49<24:52,  1.35it/s]Evaluating on VQA val set:  25%|##4       | 656/2671 [07:50<24:52,  1.35it/s]Evaluating on VQA val set:  25%|##4       | 657/2671 [07:50<24:51,  1.35it/s]Evaluating on VQA val set:  25%|##4       | 658/2671 [07:51<24:31,  1.37it/s]Evaluating on VQA val set:  25%|##4       | 659/2671 [07:52<24:25,  1.37it/s]Evaluating on VQA val set:  25%|##4       | 660/2671 [07:53<24:35,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 661/2671 [07:53<24:43,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 662/2671 [07:54<24:05,  1.39it/s]Evaluating on VQA val set:  25%|##4       | 663/2671 [07:55<23:51,  1.40it/s]Evaluating on VQA val set:  25%|##4       | 664/2671 [07:55<23:25,  1.43it/s]Evaluating on VQA val set:  25%|##4       | 665/2671 [07:56<23:17,  1.44it/s]Evaluating on VQA val set:  25%|##4       | 666/2671 [07:57<23:50,  1.40it/s]Evaluating on VQA val set:  25%|##4       | 667/2671 [07:58<24:05,  1.39it/s]Evaluating on VQA val set:  25%|##5       | 668/2671 [07:58<24:09,  1.38it/s]Evaluating on VQA val set:  25%|##5       | 669/2671 [07:59<24:19,  1.37it/s]Evaluating on VQA val set:  25%|##5       | 670/2671 [08:00<24:49,  1.34it/s]Evaluating on VQA val set:  25%|##5       | 671/2671 [08:01<24:33,  1.36it/s]Evaluating on VQA val set:  25%|##5       | 672/2671 [08:01<24:16,  1.37it/s]Evaluating on VQA val set:  25%|##5       | 673/2671 [08:02<23:28,  1.42it/s]Evaluating on VQA val set:  25%|##5       | 674/2671 [08:03<23:33,  1.41it/s]Evaluating on VQA val set:  25%|##5       | 675/2671 [08:03<23:36,  1.41it/s]Evaluating on VQA val set:  25%|##5       | 676/2671 [08:04<22:38,  1.47it/s]Evaluating on VQA val set:  25%|##5       | 677/2671 [08:05<22:59,  1.45it/s]Evaluating on VQA val set:  25%|##5       | 678/2671 [08:05<23:08,  1.44it/s]Evaluating on VQA val set:  25%|##5       | 679/2671 [08:06<22:43,  1.46it/s]Evaluating on VQA val set:  25%|##5       | 680/2671 [08:07<23:23,  1.42it/s]Evaluating on VQA val set:  25%|##5       | 681/2671 [08:07<23:03,  1.44it/s]Evaluating on VQA val set:  26%|##5       | 682/2671 [08:08<23:19,  1.42it/s]Evaluating on VQA val set:  26%|##5       | 683/2671 [08:09<24:17,  1.36it/s]Evaluating on VQA val set:  26%|##5       | 684/2671 [08:10<24:09,  1.37it/s]Evaluating on VQA val set:  26%|##5       | 685/2671 [08:10<23:25,  1.41it/s]Evaluating on VQA val set:  26%|##5       | 686/2671 [08:11<23:57,  1.38it/s]Evaluating on VQA val set:  26%|##5       | 687/2671 [08:12<23:35,  1.40it/s]Evaluating on VQA val set:  26%|##5       | 688/2671 [08:12<23:21,  1.42it/s]Evaluating on VQA val set:  26%|##5       | 689/2671 [08:13<24:03,  1.37it/s]Evaluating on VQA val set:  26%|##5       | 690/2671 [08:14<24:00,  1.38it/s]Evaluating on VQA val set:  26%|##5       | 691/2671 [08:15<24:21,  1.36it/s]Evaluating on VQA val set:  26%|##5       | 692/2671 [08:15<23:38,  1.39it/s]Evaluating on VQA val set:  26%|##5       | 693/2671 [08:16<22:48,  1.45it/s]Evaluating on VQA val set:  26%|##5       | 694/2671 [08:17<22:03,  1.49it/s]Evaluating on VQA val set:  26%|##6       | 695/2671 [08:17<22:43,  1.45it/s]Evaluating on VQA val set:  26%|##6       | 696/2671 [08:18<22:54,  1.44it/s]Evaluating on VQA val set:  26%|##6       | 697/2671 [08:19<23:10,  1.42it/s]Evaluating on VQA val set:  26%|##6       | 698/2671 [08:20<23:09,  1.42it/s]Evaluating on VQA val set:  26%|##6       | 699/2671 [08:20<22:44,  1.44it/s]Evaluating on VQA val set:  26%|##6       | 700/2671 [08:21<23:23,  1.40it/s]Evaluating on VQA val set:  26%|##6       | 701/2671 [08:22<23:56,  1.37it/s]Evaluating on VQA val set:  26%|##6       | 702/2671 [08:22<23:49,  1.38it/s]Evaluating on VQA val set:  26%|##6       | 703/2671 [08:23<23:07,  1.42it/s]Evaluating on VQA val set:  26%|##6       | 704/2671 [08:24<23:47,  1.38it/s]Evaluating on VQA val set:  26%|##6       | 705/2671 [08:25<23:18,  1.41it/s]Evaluating on VQA val set:  26%|##6       | 706/2671 [08:25<23:08,  1.41it/s]Evaluating on VQA val set:  26%|##6       | 707/2671 [08:26<23:33,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 708/2671 [08:27<23:33,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 709/2671 [08:27<23:23,  1.40it/s]Evaluating on VQA val set:  27%|##6       | 710/2671 [08:28<23:31,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 711/2671 [08:29<24:11,  1.35it/s]Evaluating on VQA val set:  27%|##6       | 712/2671 [08:30<23:33,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 713/2671 [08:30<23:38,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 714/2671 [08:31<23:59,  1.36it/s]Evaluating on VQA val set:  27%|##6       | 715/2671 [08:32<24:05,  1.35it/s]Evaluating on VQA val set:  27%|##6       | 716/2671 [08:33<24:04,  1.35it/s]Evaluating on VQA val set:  27%|##6       | 717/2671 [08:33<23:55,  1.36it/s]Evaluating on VQA val set:  27%|##6       | 718/2671 [08:34<23:28,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 719/2671 [08:35<23:35,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 720/2671 [08:36<23:46,  1.37it/s]Evaluating on VQA val set:  27%|##6       | 721/2671 [08:36<23:41,  1.37it/s]Evaluating on VQA val set:  27%|##7       | 722/2671 [08:37<23:34,  1.38it/s]Evaluating on VQA val set:  27%|##7       | 723/2671 [08:38<22:54,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 724/2671 [08:38<22:50,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 725/2671 [08:39<22:27,  1.44it/s]Evaluating on VQA val set:  27%|##7       | 726/2671 [08:40<22:56,  1.41it/s]Evaluating on VQA val set:  27%|##7       | 727/2671 [08:40<22:47,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 728/2671 [08:41<23:01,  1.41it/s]Evaluating on VQA val set:  27%|##7       | 729/2671 [08:42<22:28,  1.44it/s]Evaluating on VQA val set:  27%|##7       | 730/2671 [08:43<22:44,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 731/2671 [08:43<23:10,  1.40it/s]Evaluating on VQA val set:  27%|##7       | 732/2671 [08:44<23:07,  1.40it/s]Evaluating on VQA val set:  27%|##7       | 733/2671 [08:45<23:26,  1.38it/s]Evaluating on VQA val set:  27%|##7       | 734/2671 [08:45<23:28,  1.37it/s]Evaluating on VQA val set:  28%|##7       | 735/2671 [08:46<23:20,  1.38it/s]Evaluating on VQA val set:  28%|##7       | 736/2671 [08:47<22:43,  1.42it/s]Evaluating on VQA val set:  28%|##7       | 737/2671 [08:48<23:10,  1.39it/s]Evaluating on VQA val set:  28%|##7       | 738/2671 [08:48<22:23,  1.44it/s]Evaluating on VQA val set:  28%|##7       | 739/2671 [08:49<22:51,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 740/2671 [08:50<23:11,  1.39it/s]Evaluating on VQA val set:  28%|##7       | 741/2671 [08:50<22:53,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 742/2671 [08:51<23:22,  1.38it/s]Evaluating on VQA val set:  28%|##7       | 743/2671 [08:52<23:50,  1.35it/s]Evaluating on VQA val set:  28%|##7       | 744/2671 [08:53<22:48,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 745/2671 [08:53<22:49,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 746/2671 [08:54<22:14,  1.44it/s]Evaluating on VQA val set:  28%|##7       | 747/2671 [08:55<22:39,  1.42it/s]Evaluating on VQA val set:  28%|##8       | 748/2671 [08:55<22:29,  1.42it/s]Evaluating on VQA val set:  28%|##8       | 749/2671 [08:56<22:06,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 750/2671 [08:57<22:38,  1.41it/s]Evaluating on VQA val set:  28%|##8       | 751/2671 [08:58<22:55,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 752/2671 [08:58<22:52,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 753/2671 [08:59<22:47,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 754/2671 [09:00<22:07,  1.44it/s]Evaluating on VQA val set:  28%|##8       | 755/2671 [09:00<22:23,  1.43it/s]Evaluating on VQA val set:  28%|##8       | 756/2671 [09:01<22:20,  1.43it/s]Evaluating on VQA val set:  28%|##8       | 757/2671 [09:02<22:56,  1.39it/s]Evaluating on VQA val set:  28%|##8       | 758/2671 [09:03<23:31,  1.36it/s]Evaluating on VQA val set:  28%|##8       | 759/2671 [09:03<23:17,  1.37it/s]Evaluating on VQA val set:  28%|##8       | 760/2671 [09:04<22:55,  1.39it/s]Evaluating on VQA val set:  28%|##8       | 761/2671 [09:05<22:48,  1.40it/s]Evaluating on VQA val set:  29%|##8       | 762/2671 [09:05<23:21,  1.36it/s]Evaluating on VQA val set:  29%|##8       | 763/2671 [09:06<23:00,  1.38it/s]Evaluating on VQA val set:  29%|##8       | 764/2671 [09:07<23:21,  1.36it/s]Evaluating on VQA val set:  29%|##8       | 765/2671 [09:08<23:32,  1.35it/s]Evaluating on VQA val set:  29%|##8       | 766/2671 [09:08<23:07,  1.37it/s]Evaluating on VQA val set:  29%|##8       | 767/2671 [09:09<23:06,  1.37it/s]Evaluating on VQA val set:  29%|##8       | 768/2671 [09:10<23:47,  1.33it/s]Evaluating on VQA val set:  29%|##8       | 769/2671 [09:11<23:44,  1.34it/s]Evaluating on VQA val set:  29%|##8       | 770/2671 [09:11<23:58,  1.32it/s]Evaluating on VQA val set:  29%|##8       | 771/2671 [09:12<23:43,  1.33it/s]Evaluating on VQA val set:  29%|##8       | 772/2671 [09:13<23:21,  1.36it/s]Evaluating on VQA val set:  29%|##8       | 773/2671 [09:14<23:14,  1.36it/s]Evaluating on VQA val set:  29%|##8       | 774/2671 [09:14<23:14,  1.36it/s]Evaluating on VQA val set:  29%|##9       | 775/2671 [09:15<23:45,  1.33it/s]Evaluating on VQA val set:  29%|##9       | 776/2671 [09:16<23:14,  1.36it/s]Evaluating on VQA val set:  29%|##9       | 777/2671 [09:16<22:42,  1.39it/s]Evaluating on VQA val set:  29%|##9       | 778/2671 [09:17<22:56,  1.38it/s]Evaluating on VQA val set:  29%|##9       | 779/2671 [09:18<23:02,  1.37it/s]Evaluating on VQA val set:  29%|##9       | 780/2671 [09:19<22:50,  1.38it/s]Evaluating on VQA val set:  29%|##9       | 781/2671 [09:19<23:07,  1.36it/s]Evaluating on VQA val set:  29%|##9       | 782/2671 [09:20<22:59,  1.37it/s]Evaluating on VQA val set:  29%|##9       | 783/2671 [09:21<23:22,  1.35it/s]Evaluating on VQA val set:  29%|##9       | 784/2671 [09:22<23:08,  1.36it/s]Evaluating on VQA val set:  29%|##9       | 785/2671 [09:22<22:31,  1.40it/s]Evaluating on VQA val set:  29%|##9       | 786/2671 [09:23<22:30,  1.40it/s]Evaluating on VQA val set:  29%|##9       | 787/2671 [09:24<22:36,  1.39it/s]Evaluating on VQA val set:  30%|##9       | 788/2671 [09:24<22:33,  1.39it/s]Evaluating on VQA val set:  30%|##9       | 789/2671 [09:25<23:22,  1.34it/s]Evaluating on VQA val set:  30%|##9       | 790/2671 [09:26<23:35,  1.33it/s]Evaluating on VQA val set:  30%|##9       | 791/2671 [09:27<23:01,  1.36it/s]Evaluating on VQA val set:  30%|##9       | 792/2671 [09:27<22:47,  1.37it/s]Evaluating on VQA val set:  30%|##9       | 793/2671 [09:28<23:14,  1.35it/s]Evaluating on VQA val set:  30%|##9       | 794/2671 [09:29<21:59,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 795/2671 [09:30<22:03,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 796/2671 [09:30<21:48,  1.43it/s]Evaluating on VQA val set:  30%|##9       | 797/2671 [09:31<22:00,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 798/2671 [09:32<22:09,  1.41it/s]Evaluating on VQA val set:  30%|##9       | 799/2671 [09:32<22:23,  1.39it/s]Evaluating on VQA val set:  30%|##9       | 800/2671 [09:33<22:15,  1.40it/s]Evaluating on VQA val set:  30%|##9       | 801/2671 [09:34<22:37,  1.38it/s]Evaluating on VQA val set:  30%|###       | 802/2671 [09:35<22:55,  1.36it/s]Evaluating on VQA val set:  30%|###       | 803/2671 [09:35<22:40,  1.37it/s]Evaluating on VQA val set:  30%|###       | 804/2671 [09:36<22:59,  1.35it/s]Evaluating on VQA val set:  30%|###       | 805/2671 [09:37<22:39,  1.37it/s]Evaluating on VQA val set:  30%|###       | 806/2671 [09:38<23:03,  1.35it/s]Evaluating on VQA val set:  30%|###       | 807/2671 [09:38<23:11,  1.34it/s]Evaluating on VQA val set:  30%|###       | 808/2671 [09:39<22:25,  1.38it/s]Evaluating on VQA val set:  30%|###       | 809/2671 [09:40<22:44,  1.37it/s]Evaluating on VQA val set:  30%|###       | 810/2671 [09:41<22:53,  1.36it/s]Evaluating on VQA val set:  30%|###       | 811/2671 [09:41<23:12,  1.34it/s]Evaluating on VQA val set:  30%|###       | 812/2671 [09:42<22:14,  1.39it/s]Evaluating on VQA val set:  30%|###       | 813/2671 [09:43<21:55,  1.41it/s]Evaluating on VQA val set:  30%|###       | 814/2671 [09:43<21:01,  1.47it/s]Evaluating on VQA val set:  31%|###       | 815/2671 [09:44<21:53,  1.41it/s]Evaluating on VQA val set:  31%|###       | 816/2671 [09:45<22:03,  1.40it/s]Evaluating on VQA val set:  31%|###       | 817/2671 [09:45<22:12,  1.39it/s]Evaluating on VQA val set:  31%|###       | 818/2671 [09:46<22:58,  1.34it/s]Evaluating on VQA val set:  31%|###       | 819/2671 [09:47<22:23,  1.38it/s]Evaluating on VQA val set:  31%|###       | 820/2671 [09:48<22:23,  1.38it/s]Evaluating on VQA val set:  31%|###       | 821/2671 [09:48<21:59,  1.40it/s]Evaluating on VQA val set:  31%|###       | 822/2671 [09:49<22:10,  1.39it/s]Evaluating on VQA val set:  31%|###       | 823/2671 [09:50<21:55,  1.40it/s]Evaluating on VQA val set:  31%|###       | 824/2671 [09:51<21:59,  1.40it/s]Evaluating on VQA val set:  31%|###       | 825/2671 [09:51<21:48,  1.41it/s]Evaluating on VQA val set:  31%|###       | 826/2671 [09:52<22:24,  1.37it/s]Evaluating on VQA val set:  31%|###       | 827/2671 [09:53<22:36,  1.36it/s]Evaluating on VQA val set:  31%|###       | 828/2671 [09:53<22:27,  1.37it/s]Evaluating on VQA val set:  31%|###1      | 829/2671 [09:54<22:34,  1.36it/s]Evaluating on VQA val set:  31%|###1      | 830/2671 [09:55<22:27,  1.37it/s]Evaluating on VQA val set:  31%|###1      | 831/2671 [09:56<22:47,  1.35it/s]Evaluating on VQA val set:  31%|###1      | 832/2671 [09:56<22:32,  1.36it/s]Evaluating on VQA val set:  31%|###1      | 833/2671 [09:57<22:57,  1.33it/s]Evaluating on VQA val set:  31%|###1      | 834/2671 [09:58<22:56,  1.33it/s]Evaluating on VQA val set:  31%|###1      | 835/2671 [09:59<22:40,  1.35it/s]Evaluating on VQA val set:  31%|###1      | 836/2671 [09:59<22:28,  1.36it/s]Evaluating on VQA val set:  31%|###1      | 837/2671 [10:00<22:15,  1.37it/s]Evaluating on VQA val set:  31%|###1      | 838/2671 [10:01<22:22,  1.37it/s]Evaluating on VQA val set:  31%|###1      | 839/2671 [10:02<22:00,  1.39it/s]Evaluating on VQA val set:  31%|###1      | 840/2671 [10:02<21:26,  1.42it/s]Evaluating on VQA val set:  31%|###1      | 841/2671 [10:03<21:39,  1.41it/s]Evaluating on VQA val set:  32%|###1      | 842/2671 [10:04<21:57,  1.39it/s]Evaluating on VQA val set:  32%|###1      | 843/2671 [10:04<21:17,  1.43it/s]Evaluating on VQA val set:  32%|###1      | 844/2671 [10:05<20:01,  1.52it/s]Evaluating on VQA val set:  32%|###1      | 845/2671 [10:06<20:32,  1.48it/s]Evaluating on VQA val set:  32%|###1      | 846/2671 [10:06<21:01,  1.45it/s]Evaluating on VQA val set:  32%|###1      | 847/2671 [10:07<20:28,  1.48it/s]Evaluating on VQA val set:  32%|###1      | 848/2671 [10:08<20:21,  1.49it/s]Evaluating on VQA val set:  32%|###1      | 849/2671 [10:08<20:59,  1.45it/s]Evaluating on VQA val set:  32%|###1      | 850/2671 [10:09<21:27,  1.41it/s]Evaluating on VQA val set:  32%|###1      | 851/2671 [10:10<21:43,  1.40it/s]Evaluating on VQA val set:  32%|###1      | 852/2671 [10:11<21:51,  1.39it/s]Evaluating on VQA val set:  32%|###1      | 853/2671 [10:11<21:41,  1.40it/s]Evaluating on VQA val set:  32%|###1      | 854/2671 [10:12<21:32,  1.41it/s]Evaluating on VQA val set:  32%|###2      | 855/2671 [10:13<21:43,  1.39it/s]Evaluating on VQA val set:  32%|###2      | 856/2671 [10:13<21:32,  1.40it/s]Evaluating on VQA val set:  32%|###2      | 857/2671 [10:14<21:23,  1.41it/s]Evaluating on VQA val set:  32%|###2      | 858/2671 [10:15<22:08,  1.36it/s]Evaluating on VQA val set:  32%|###2      | 859/2671 [10:16<22:11,  1.36it/s]Evaluating on VQA val set:  32%|###2      | 860/2671 [10:16<22:00,  1.37it/s]Evaluating on VQA val set:  32%|###2      | 861/2671 [10:17<21:52,  1.38it/s]Evaluating on VQA val set:  32%|###2      | 862/2671 [10:18<21:01,  1.43it/s]Evaluating on VQA val set:  32%|###2      | 863/2671 [10:18<21:47,  1.38it/s]Evaluating on VQA val set:  32%|###2      | 864/2671 [10:19<21:41,  1.39it/s]Evaluating on VQA val set:  32%|###2      | 865/2671 [10:20<21:15,  1.42it/s]Evaluating on VQA val set:  32%|###2      | 866/2671 [10:21<21:16,  1.41it/s]Evaluating on VQA val set:  32%|###2      | 867/2671 [10:21<21:23,  1.41it/s]Evaluating on VQA val set:  32%|###2      | 868/2671 [10:22<20:48,  1.44it/s]Evaluating on VQA val set:  33%|###2      | 869/2671 [10:23<21:06,  1.42it/s]Evaluating on VQA val set:  33%|###2      | 870/2671 [10:23<20:38,  1.45it/s]Evaluating on VQA val set:  33%|###2      | 871/2671 [10:24<21:14,  1.41it/s]Evaluating on VQA val set:  33%|###2      | 872/2671 [10:25<21:10,  1.42it/s]Evaluating on VQA val set:  33%|###2      | 873/2671 [10:26<21:35,  1.39it/s]Evaluating on VQA val set:  33%|###2      | 874/2671 [10:26<21:02,  1.42it/s]Evaluating on VQA val set:  33%|###2      | 875/2671 [10:27<21:13,  1.41it/s]Evaluating on VQA val set:  33%|###2      | 876/2671 [10:28<21:58,  1.36it/s]Evaluating on VQA val set:  33%|###2      | 877/2671 [10:28<21:45,  1.37it/s]Evaluating on VQA val set:  33%|###2      | 878/2671 [10:29<21:59,  1.36it/s]Evaluating on VQA val set:  33%|###2      | 879/2671 [10:30<21:37,  1.38it/s]Evaluating on VQA val set:  33%|###2      | 880/2671 [10:31<21:48,  1.37it/s]Evaluating on VQA val set:  33%|###2      | 881/2671 [10:31<21:31,  1.39it/s]Evaluating on VQA val set:  33%|###3      | 882/2671 [10:32<21:41,  1.37it/s]Evaluating on VQA val set:  33%|###3      | 883/2671 [10:33<21:30,  1.39it/s]Evaluating on VQA val set:  33%|###3      | 884/2671 [10:33<20:27,  1.46it/s]Evaluating on VQA val set:  33%|###3      | 885/2671 [10:34<20:41,  1.44it/s]Evaluating on VQA val set:  33%|###3      | 886/2671 [10:35<20:57,  1.42it/s]Evaluating on VQA val set:  33%|###3      | 887/2671 [10:35<20:39,  1.44it/s]Evaluating on VQA val set:  33%|###3      | 888/2671 [10:36<21:04,  1.41it/s]Evaluating on VQA val set:  33%|###3      | 889/2671 [10:37<21:04,  1.41it/s]Evaluating on VQA val set:  33%|###3      | 890/2671 [10:38<21:20,  1.39it/s]Evaluating on VQA val set:  33%|###3      | 891/2671 [10:38<22:05,  1.34it/s]Evaluating on VQA val set:  33%|###3      | 892/2671 [10:39<21:12,  1.40it/s]Evaluating on VQA val set:  33%|###3      | 893/2671 [10:40<21:20,  1.39it/s]Evaluating on VQA val set:  33%|###3      | 894/2671 [10:40<19:46,  1.50it/s]Evaluating on VQA val set:  34%|###3      | 895/2671 [10:41<19:53,  1.49it/s]Evaluating on VQA val set:  34%|###3      | 896/2671 [10:42<20:22,  1.45it/s]Evaluating on VQA val set:  34%|###3      | 897/2671 [10:43<21:12,  1.39it/s]Evaluating on VQA val set:  34%|###3      | 898/2671 [10:43<21:22,  1.38it/s]Evaluating on VQA val set:  34%|###3      | 899/2671 [10:44<21:38,  1.36it/s]Evaluating on VQA val set:  34%|###3      | 900/2671 [10:45<20:46,  1.42it/s]Evaluating on VQA val set:  34%|###3      | 901/2671 [10:45<20:59,  1.41it/s]Evaluating on VQA val set:  34%|###3      | 902/2671 [10:46<21:22,  1.38it/s]Evaluating on VQA val set:  34%|###3      | 903/2671 [10:47<21:20,  1.38it/s]Evaluating on VQA val set:  34%|###3      | 904/2671 [10:48<21:12,  1.39it/s]Evaluating on VQA val set:  34%|###3      | 905/2671 [10:48<19:52,  1.48it/s]Evaluating on VQA val set:  34%|###3      | 906/2671 [10:49<19:53,  1.48it/s]Evaluating on VQA val set:  34%|###3      | 907/2671 [10:50<20:37,  1.43it/s]Evaluating on VQA val set:  34%|###3      | 908/2671 [10:50<20:35,  1.43it/s]Evaluating on VQA val set:  34%|###4      | 909/2671 [10:51<20:15,  1.45it/s]Evaluating on VQA val set:  34%|###4      | 910/2671 [10:52<20:02,  1.46it/s]Evaluating on VQA val set:  34%|###4      | 911/2671 [10:52<20:07,  1.46it/s]Evaluating on VQA val set:  34%|###4      | 912/2671 [10:53<20:28,  1.43it/s]Evaluating on VQA val set:  34%|###4      | 913/2671 [10:54<20:37,  1.42it/s]Evaluating on VQA val set:  34%|###4      | 914/2671 [10:55<20:50,  1.40it/s]Evaluating on VQA val set:  34%|###4      | 915/2671 [10:55<20:12,  1.45it/s]Evaluating on VQA val set:  34%|###4      | 916/2671 [10:56<20:21,  1.44it/s]Evaluating on VQA val set:  34%|###4      | 917/2671 [10:57<20:44,  1.41it/s]Evaluating on VQA val set:  34%|###4      | 918/2671 [10:57<21:26,  1.36it/s]Evaluating on VQA val set:  34%|###4      | 919/2671 [10:58<21:35,  1.35it/s]Evaluating on VQA val set:  34%|###4      | 920/2671 [10:59<21:25,  1.36it/s]Evaluating on VQA val set:  34%|###4      | 921/2671 [11:00<21:58,  1.33it/s]Evaluating on VQA val set:  35%|###4      | 922/2671 [11:01<22:20,  1.31it/s]Evaluating on VQA val set:  35%|###4      | 923/2671 [11:01<22:02,  1.32it/s]Evaluating on VQA val set:  35%|###4      | 924/2671 [11:02<21:43,  1.34it/s]Evaluating on VQA val set:  35%|###4      | 925/2671 [11:03<21:36,  1.35it/s]Evaluating on VQA val set:  35%|###4      | 926/2671 [11:03<21:23,  1.36it/s]Evaluating on VQA val set:  35%|###4      | 927/2671 [11:04<21:04,  1.38it/s]Evaluating on VQA val set:  35%|###4      | 928/2671 [11:05<20:26,  1.42it/s]Evaluating on VQA val set:  35%|###4      | 929/2671 [11:05<19:51,  1.46it/s]Evaluating on VQA val set:  35%|###4      | 930/2671 [11:06<19:47,  1.47it/s]Evaluating on VQA val set:  35%|###4      | 931/2671 [11:07<19:03,  1.52it/s]Evaluating on VQA val set:  35%|###4      | 932/2671 [11:07<19:42,  1.47it/s]Evaluating on VQA val set:  35%|###4      | 933/2671 [11:08<19:51,  1.46it/s]Evaluating on VQA val set:  35%|###4      | 934/2671 [11:09<19:51,  1.46it/s]Evaluating on VQA val set:  35%|###5      | 935/2671 [11:10<20:26,  1.41it/s]Evaluating on VQA val set:  35%|###5      | 936/2671 [11:10<20:38,  1.40it/s]Evaluating on VQA val set:  35%|###5      | 937/2671 [11:11<21:04,  1.37it/s]Evaluating on VQA val set:  35%|###5      | 938/2671 [11:12<20:06,  1.44it/s]Evaluating on VQA val set:  35%|###5      | 939/2671 [11:12<20:07,  1.43it/s]Evaluating on VQA val set:  35%|###5      | 940/2671 [11:13<19:43,  1.46it/s]Evaluating on VQA val set:  35%|###5      | 941/2671 [11:14<20:12,  1.43it/s]Evaluating on VQA val set:  35%|###5      | 942/2671 [11:14<20:10,  1.43it/s]Evaluating on VQA val set:  35%|###5      | 943/2671 [11:15<19:49,  1.45it/s]Evaluating on VQA val set:  35%|###5      | 944/2671 [11:16<19:27,  1.48it/s]Evaluating on VQA val set:  35%|###5      | 945/2671 [11:16<19:48,  1.45it/s]Evaluating on VQA val set:  35%|###5      | 946/2671 [11:17<19:46,  1.45it/s]Evaluating on VQA val set:  35%|###5      | 947/2671 [11:18<19:19,  1.49it/s]Evaluating on VQA val set:  35%|###5      | 948/2671 [11:18<19:22,  1.48it/s]Evaluating on VQA val set:  36%|###5      | 949/2671 [11:19<19:35,  1.46it/s]Evaluating on VQA val set:  36%|###5      | 950/2671 [11:20<20:06,  1.43it/s]Evaluating on VQA val set:  36%|###5      | 951/2671 [11:21<20:09,  1.42it/s]Evaluating on VQA val set:  36%|###5      | 952/2671 [11:21<19:56,  1.44it/s]Evaluating on VQA val set:  36%|###5      | 953/2671 [11:22<20:04,  1.43it/s]Evaluating on VQA val set:  36%|###5      | 954/2671 [11:23<19:52,  1.44it/s]Evaluating on VQA val set:  36%|###5      | 955/2671 [11:23<20:21,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 956/2671 [11:24<20:11,  1.42it/s]Evaluating on VQA val set:  36%|###5      | 957/2671 [11:25<20:08,  1.42it/s]Evaluating on VQA val set:  36%|###5      | 958/2671 [11:26<20:55,  1.36it/s]Evaluating on VQA val set:  36%|###5      | 959/2671 [11:26<20:06,  1.42it/s]Evaluating on VQA val set:  36%|###5      | 960/2671 [11:27<20:20,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 961/2671 [11:28<20:01,  1.42it/s]Evaluating on VQA val set:  36%|###6      | 962/2671 [11:28<20:28,  1.39it/s]Evaluating on VQA val set:  36%|###6      | 963/2671 [11:29<19:58,  1.43it/s]Evaluating on VQA val set:  36%|###6      | 964/2671 [11:30<18:49,  1.51it/s]Evaluating on VQA val set:  36%|###6      | 965/2671 [11:30<19:24,  1.47it/s]Evaluating on VQA val set:  36%|###6      | 966/2671 [11:31<19:55,  1.43it/s]Evaluating on VQA val set:  36%|###6      | 967/2671 [11:32<20:19,  1.40it/s]Evaluating on VQA val set:  36%|###6      | 968/2671 [11:33<20:16,  1.40it/s]Evaluating on VQA val set:  36%|###6      | 969/2671 [11:33<20:59,  1.35it/s]Evaluating on VQA val set:  36%|###6      | 970/2671 [11:34<20:59,  1.35it/s]Evaluating on VQA val set:  36%|###6      | 971/2671 [11:35<20:30,  1.38it/s]Evaluating on VQA val set:  36%|###6      | 972/2671 [11:36<20:19,  1.39it/s]Evaluating on VQA val set:  36%|###6      | 973/2671 [11:36<20:04,  1.41it/s]Evaluating on VQA val set:  36%|###6      | 974/2671 [11:37<20:02,  1.41it/s]Evaluating on VQA val set:  37%|###6      | 975/2671 [11:38<20:11,  1.40it/s]Evaluating on VQA val set:  37%|###6      | 976/2671 [11:38<19:51,  1.42it/s]Evaluating on VQA val set:  37%|###6      | 977/2671 [11:39<19:46,  1.43it/s]Evaluating on VQA val set:  37%|###6      | 978/2671 [11:40<19:48,  1.43it/s]Evaluating on VQA val set:  37%|###6      | 979/2671 [11:41<20:04,  1.40it/s]Evaluating on VQA val set:  37%|###6      | 980/2671 [11:41<20:08,  1.40it/s]Evaluating on VQA val set:  37%|###6      | 981/2671 [11:42<19:54,  1.41it/s]Evaluating on VQA val set:  37%|###6      | 982/2671 [11:42<18:36,  1.51it/s]Evaluating on VQA val set:  37%|###6      | 983/2671 [11:43<18:24,  1.53it/s]Evaluating on VQA val set:  37%|###6      | 984/2671 [11:44<19:05,  1.47it/s]Evaluating on VQA val set:  37%|###6      | 985/2671 [11:45<19:02,  1.48it/s]Evaluating on VQA val set:  37%|###6      | 986/2671 [11:45<18:57,  1.48it/s]Evaluating on VQA val set:  37%|###6      | 987/2671 [11:46<19:16,  1.46it/s]Evaluating on VQA val set:  37%|###6      | 988/2671 [11:47<19:39,  1.43it/s]Evaluating on VQA val set:  37%|###7      | 989/2671 [11:47<19:53,  1.41it/s]Evaluating on VQA val set:  37%|###7      | 990/2671 [11:48<19:43,  1.42it/s]Evaluating on VQA val set:  37%|###7      | 991/2671 [11:49<19:45,  1.42it/s]Evaluating on VQA val set:  37%|###7      | 992/2671 [11:49<19:51,  1.41it/s]Evaluating on VQA val set:  37%|###7      | 993/2671 [11:50<18:58,  1.47it/s]Evaluating on VQA val set:  37%|###7      | 994/2671 [11:51<19:13,  1.45it/s]Evaluating on VQA val set:  37%|###7      | 995/2671 [11:51<19:11,  1.46it/s]Evaluating on VQA val set:  37%|###7      | 996/2671 [11:52<18:35,  1.50it/s]Evaluating on VQA val set:  37%|###7      | 997/2671 [11:53<18:46,  1.49it/s]Evaluating on VQA val set:  37%|###7      | 998/2671 [11:54<19:03,  1.46it/s]Evaluating on VQA val set:  37%|###7      | 999/2671 [11:54<18:47,  1.48it/s]Evaluating on VQA val set:  37%|###7      | 1000/2671 [11:55<18:38,  1.49it/s]Evaluating on VQA val set:  37%|###7      | 1001/2671 [11:56<19:03,  1.46it/s]Evaluating on VQA val set:  38%|###7      | 1002/2671 [11:56<19:20,  1.44it/s]Evaluating on VQA val set:  38%|###7      | 1003/2671 [11:57<19:44,  1.41it/s]Evaluating on VQA val set:  38%|###7      | 1004/2671 [11:58<19:54,  1.39it/s]Evaluating on VQA val set:  38%|###7      | 1005/2671 [11:58<20:20,  1.37it/s]Evaluating on VQA val set:  38%|###7      | 1006/2671 [11:59<19:44,  1.41it/s]Evaluating on VQA val set:  38%|###7      | 1007/2671 [12:00<20:05,  1.38it/s]Evaluating on VQA val set:  38%|###7      | 1008/2671 [12:01<20:26,  1.36it/s]Evaluating on VQA val set:  38%|###7      | 1009/2671 [12:01<20:30,  1.35it/s]Evaluating on VQA val set:  38%|###7      | 1010/2671 [12:02<20:23,  1.36it/s]Evaluating on VQA val set:  38%|###7      | 1011/2671 [12:03<20:26,  1.35it/s]Evaluating on VQA val set:  38%|###7      | 1012/2671 [12:04<19:50,  1.39it/s]Evaluating on VQA val set:  38%|###7      | 1013/2671 [12:04<20:03,  1.38it/s]Evaluating on VQA val set:  38%|###7      | 1014/2671 [12:05<20:04,  1.38it/s]Evaluating on VQA val set:  38%|###8      | 1015/2671 [12:06<20:22,  1.35it/s]Evaluating on VQA val set:  38%|###8      | 1016/2671 [12:07<20:30,  1.35it/s]Evaluating on VQA val set:  38%|###8      | 1017/2671 [12:07<19:56,  1.38it/s]Evaluating on VQA val set:  38%|###8      | 1018/2671 [12:08<20:00,  1.38it/s]Evaluating on VQA val set:  38%|###8      | 1019/2671 [12:09<20:18,  1.36it/s]Evaluating on VQA val set:  38%|###8      | 1020/2671 [12:09<19:28,  1.41it/s]Evaluating on VQA val set:  38%|###8      | 1021/2671 [12:10<19:33,  1.41it/s]Evaluating on VQA val set:  38%|###8      | 1022/2671 [12:11<19:25,  1.42it/s]Evaluating on VQA val set:  38%|###8      | 1023/2671 [12:12<19:32,  1.41it/s]Evaluating on VQA val set:  38%|###8      | 1024/2671 [12:12<18:37,  1.47it/s]Evaluating on VQA val set:  38%|###8      | 1025/2671 [12:13<18:54,  1.45it/s]Evaluating on VQA val set:  38%|###8      | 1026/2671 [12:14<19:13,  1.43it/s]Evaluating on VQA val set:  38%|###8      | 1027/2671 [12:14<19:30,  1.40it/s]Evaluating on VQA val set:  38%|###8      | 1028/2671 [12:15<19:38,  1.39it/s]Evaluating on VQA val set:  39%|###8      | 1029/2671 [12:16<19:07,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1030/2671 [12:16<19:11,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1031/2671 [12:17<19:53,  1.37it/s]Evaluating on VQA val set:  39%|###8      | 1032/2671 [12:18<19:50,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1033/2671 [12:19<19:09,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1034/2671 [12:19<19:16,  1.42it/s]Evaluating on VQA val set:  39%|###8      | 1035/2671 [12:20<19:32,  1.40it/s]Evaluating on VQA val set:  39%|###8      | 1036/2671 [12:21<19:48,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1037/2671 [12:21<19:49,  1.37it/s]Evaluating on VQA val set:  39%|###8      | 1038/2671 [12:22<19:49,  1.37it/s]Evaluating on VQA val set:  39%|###8      | 1039/2671 [12:23<20:01,  1.36it/s]Evaluating on VQA val set:  39%|###8      | 1040/2671 [12:24<20:38,  1.32it/s]Evaluating on VQA val set:  39%|###8      | 1041/2671 [12:25<20:29,  1.33it/s]Evaluating on VQA val set:  39%|###9      | 1042/2671 [12:25<19:38,  1.38it/s]Evaluating on VQA val set:  39%|###9      | 1043/2671 [12:26<19:07,  1.42it/s]Evaluating on VQA val set:  39%|###9      | 1044/2671 [12:27<19:29,  1.39it/s]Evaluating on VQA val set:  39%|###9      | 1045/2671 [12:27<19:36,  1.38it/s]Evaluating on VQA val set:  39%|###9      | 1046/2671 [12:28<18:48,  1.44it/s]Evaluating on VQA val set:  39%|###9      | 1047/2671 [12:29<18:58,  1.43it/s]Evaluating on VQA val set:  39%|###9      | 1048/2671 [12:29<19:26,  1.39it/s]Evaluating on VQA val set:  39%|###9      | 1049/2671 [12:30<19:25,  1.39it/s]Evaluating on VQA val set:  39%|###9      | 1050/2671 [12:31<19:34,  1.38it/s]Evaluating on VQA val set:  39%|###9      | 1051/2671 [12:32<19:18,  1.40it/s]Evaluating on VQA val set:  39%|###9      | 1052/2671 [12:32<19:28,  1.39it/s]Evaluating on VQA val set:  39%|###9      | 1053/2671 [12:33<19:09,  1.41it/s]Evaluating on VQA val set:  39%|###9      | 1054/2671 [12:34<19:03,  1.41it/s]Evaluating on VQA val set:  39%|###9      | 1055/2671 [12:34<19:31,  1.38it/s]Evaluating on VQA val set:  40%|###9      | 1056/2671 [12:35<19:00,  1.42it/s]Evaluating on VQA val set:  40%|###9      | 1057/2671 [12:36<19:14,  1.40it/s]Evaluating on VQA val set:  40%|###9      | 1058/2671 [12:37<18:59,  1.42it/s]Evaluating on VQA val set:  40%|###9      | 1059/2671 [12:37<19:20,  1.39it/s]Evaluating on VQA val set:  40%|###9      | 1060/2671 [12:38<19:25,  1.38it/s]Evaluating on VQA val set:  40%|###9      | 1061/2671 [12:39<19:17,  1.39it/s]Evaluating on VQA val set:  40%|###9      | 1062/2671 [12:39<19:12,  1.40it/s]Evaluating on VQA val set:  40%|###9      | 1063/2671 [12:40<18:47,  1.43it/s]Evaluating on VQA val set:  40%|###9      | 1064/2671 [12:41<18:55,  1.41it/s]Evaluating on VQA val set:  40%|###9      | 1065/2671 [12:41<18:32,  1.44it/s]Evaluating on VQA val set:  40%|###9      | 1066/2671 [12:42<18:06,  1.48it/s]Evaluating on VQA val set:  40%|###9      | 1067/2671 [12:43<18:32,  1.44it/s]Evaluating on VQA val set:  40%|###9      | 1068/2671 [12:44<19:07,  1.40it/s]Evaluating on VQA val set:  40%|####      | 1069/2671 [12:44<18:59,  1.41it/s]Evaluating on VQA val set:  40%|####      | 1070/2671 [12:45<19:14,  1.39it/s]Evaluating on VQA val set:  40%|####      | 1071/2671 [12:46<19:15,  1.38it/s]Evaluating on VQA val set:  40%|####      | 1072/2671 [12:46<18:58,  1.40it/s]Evaluating on VQA val set:  40%|####      | 1073/2671 [12:47<19:16,  1.38it/s]Evaluating on VQA val set:  40%|####      | 1074/2671 [12:48<19:25,  1.37it/s]Evaluating on VQA val set:  40%|####      | 1075/2671 [12:49<19:25,  1.37it/s]Evaluating on VQA val set:  40%|####      | 1076/2671 [12:49<19:31,  1.36it/s]Evaluating on VQA val set:  40%|####      | 1077/2671 [12:50<18:53,  1.41it/s]Evaluating on VQA val set:  40%|####      | 1078/2671 [12:51<18:01,  1.47it/s]Evaluating on VQA val set:  40%|####      | 1079/2671 [12:51<18:15,  1.45it/s]Evaluating on VQA val set:  40%|####      | 1080/2671 [12:52<19:04,  1.39it/s]Evaluating on VQA val set:  40%|####      | 1081/2671 [12:53<18:39,  1.42it/s]Evaluating on VQA val set:  41%|####      | 1082/2671 [12:54<18:58,  1.40it/s]Evaluating on VQA val set:  41%|####      | 1083/2671 [12:54<19:22,  1.37it/s]Evaluating on VQA val set:  41%|####      | 1084/2671 [12:55<19:21,  1.37it/s]Evaluating on VQA val set:  41%|####      | 1085/2671 [12:56<18:35,  1.42it/s]Evaluating on VQA val set:  41%|####      | 1086/2671 [12:57<18:55,  1.40it/s]Evaluating on VQA val set:  41%|####      | 1087/2671 [12:57<18:11,  1.45it/s]Evaluating on VQA val set:  41%|####      | 1088/2671 [12:58<18:32,  1.42it/s]Evaluating on VQA val set:  41%|####      | 1089/2671 [12:59<18:31,  1.42it/s]Evaluating on VQA val set:  41%|####      | 1090/2671 [12:59<19:07,  1.38it/s]Evaluating on VQA val set:  41%|####      | 1091/2671 [13:00<18:48,  1.40it/s]Evaluating on VQA val set:  41%|####      | 1092/2671 [13:01<18:34,  1.42it/s]Evaluating on VQA val set:  41%|####      | 1093/2671 [13:01<18:38,  1.41it/s]Evaluating on VQA val set:  41%|####      | 1094/2671 [13:02<18:53,  1.39it/s]Evaluating on VQA val set:  41%|####      | 1095/2671 [13:03<17:49,  1.47it/s]Evaluating on VQA val set:  41%|####1     | 1096/2671 [13:03<17:57,  1.46it/s]Evaluating on VQA val set:  41%|####1     | 1097/2671 [13:04<18:13,  1.44it/s]Evaluating on VQA val set:  41%|####1     | 1098/2671 [13:05<18:35,  1.41it/s]Evaluating on VQA val set:  41%|####1     | 1099/2671 [13:06<18:05,  1.45it/s]Evaluating on VQA val set:  41%|####1     | 1100/2671 [13:06<18:27,  1.42it/s]Evaluating on VQA val set:  41%|####1     | 1101/2671 [13:07<18:37,  1.40it/s]Evaluating on VQA val set:  41%|####1     | 1102/2671 [13:08<18:19,  1.43it/s]Evaluating on VQA val set:  41%|####1     | 1103/2671 [13:08<18:48,  1.39it/s]Evaluating on VQA val set:  41%|####1     | 1104/2671 [13:09<18:46,  1.39it/s]Evaluating on VQA val set:  41%|####1     | 1105/2671 [13:10<18:41,  1.40it/s]Evaluating on VQA val set:  41%|####1     | 1106/2671 [13:11<19:04,  1.37it/s]Evaluating on VQA val set:  41%|####1     | 1107/2671 [13:11<18:42,  1.39it/s]Evaluating on VQA val set:  41%|####1     | 1108/2671 [13:12<18:30,  1.41it/s]Evaluating on VQA val set:  42%|####1     | 1109/2671 [13:13<18:40,  1.39it/s]Evaluating on VQA val set:  42%|####1     | 1110/2671 [13:14<18:35,  1.40it/s]Evaluating on VQA val set:  42%|####1     | 1111/2671 [13:14<18:34,  1.40it/s]Evaluating on VQA val set:  42%|####1     | 1112/2671 [13:15<19:02,  1.36it/s]Evaluating on VQA val set:  42%|####1     | 1113/2671 [13:16<18:45,  1.38it/s]Evaluating on VQA val set:  42%|####1     | 1114/2671 [13:16<18:23,  1.41it/s]Evaluating on VQA val set:  42%|####1     | 1115/2671 [13:17<18:47,  1.38it/s]Evaluating on VQA val set:  42%|####1     | 1116/2671 [13:18<18:43,  1.38it/s]Evaluating on VQA val set:  42%|####1     | 1117/2671 [13:19<18:14,  1.42it/s]Evaluating on VQA val set:  42%|####1     | 1118/2671 [13:19<18:27,  1.40it/s]Evaluating on VQA val set:  42%|####1     | 1119/2671 [13:20<18:33,  1.39it/s]Evaluating on VQA val set:  42%|####1     | 1120/2671 [13:21<18:40,  1.38it/s]Evaluating on VQA val set:  42%|####1     | 1121/2671 [13:21<18:34,  1.39it/s]Evaluating on VQA val set:  42%|####2     | 1122/2671 [13:22<18:43,  1.38it/s]Evaluating on VQA val set:  42%|####2     | 1123/2671 [13:23<18:00,  1.43it/s]Evaluating on VQA val set:  42%|####2     | 1124/2671 [13:23<17:39,  1.46it/s]Evaluating on VQA val set:  42%|####2     | 1125/2671 [13:24<17:36,  1.46it/s]Evaluating on VQA val set:  42%|####2     | 1126/2671 [13:25<17:29,  1.47it/s]Evaluating on VQA val set:  42%|####2     | 1127/2671 [13:25<17:28,  1.47it/s]Evaluating on VQA val set:  42%|####2     | 1128/2671 [13:26<17:45,  1.45it/s]Evaluating on VQA val set:  42%|####2     | 1129/2671 [13:27<17:41,  1.45it/s]Evaluating on VQA val set:  42%|####2     | 1130/2671 [13:28<17:49,  1.44it/s]Evaluating on VQA val set:  42%|####2     | 1131/2671 [13:28<18:14,  1.41it/s]Evaluating on VQA val set:  42%|####2     | 1132/2671 [13:29<18:16,  1.40it/s]Evaluating on VQA val set:  42%|####2     | 1133/2671 [13:30<18:20,  1.40it/s]Evaluating on VQA val set:  42%|####2     | 1134/2671 [13:30<17:54,  1.43it/s]Evaluating on VQA val set:  42%|####2     | 1135/2671 [13:31<18:34,  1.38it/s]Evaluating on VQA val set:  43%|####2     | 1136/2671 [13:32<18:23,  1.39it/s]Evaluating on VQA val set:  43%|####2     | 1137/2671 [13:33<18:36,  1.37it/s]Evaluating on VQA val set:  43%|####2     | 1138/2671 [13:33<18:19,  1.39it/s]Evaluating on VQA val set:  43%|####2     | 1139/2671 [13:34<18:16,  1.40it/s]Evaluating on VQA val set:  43%|####2     | 1140/2671 [13:35<18:21,  1.39it/s]Evaluating on VQA val set:  43%|####2     | 1141/2671 [13:36<18:29,  1.38it/s]Evaluating on VQA val set:  43%|####2     | 1142/2671 [13:36<17:59,  1.42it/s]Evaluating on VQA val set:  43%|####2     | 1143/2671 [13:37<17:40,  1.44it/s]Evaluating on VQA val set:  43%|####2     | 1144/2671 [13:38<17:54,  1.42it/s]Evaluating on VQA val set:  43%|####2     | 1145/2671 [13:38<17:41,  1.44it/s]Evaluating on VQA val set:  43%|####2     | 1146/2671 [13:39<17:34,  1.45it/s]Evaluating on VQA val set:  43%|####2     | 1147/2671 [13:40<17:30,  1.45it/s]Evaluating on VQA val set:  43%|####2     | 1148/2671 [13:40<17:47,  1.43it/s]Evaluating on VQA val set:  43%|####3     | 1149/2671 [13:41<18:00,  1.41it/s]Evaluating on VQA val set:  43%|####3     | 1150/2671 [13:42<17:57,  1.41it/s]Evaluating on VQA val set:  43%|####3     | 1151/2671 [13:43<18:05,  1.40it/s]Evaluating on VQA val set:  43%|####3     | 1152/2671 [13:43<17:39,  1.43it/s]Evaluating on VQA val set:  43%|####3     | 1153/2671 [13:44<17:21,  1.46it/s]Evaluating on VQA val set:  43%|####3     | 1154/2671 [13:45<18:13,  1.39it/s]Evaluating on VQA val set:  43%|####3     | 1155/2671 [13:45<18:31,  1.36it/s]Evaluating on VQA val set:  43%|####3     | 1156/2671 [13:46<18:37,  1.36it/s]Evaluating on VQA val set:  43%|####3     | 1157/2671 [13:47<18:19,  1.38it/s]Evaluating on VQA val set:  43%|####3     | 1158/2671 [13:48<18:10,  1.39it/s]Evaluating on VQA val set:  43%|####3     | 1159/2671 [13:48<18:10,  1.39it/s]Evaluating on VQA val set:  43%|####3     | 1160/2671 [13:49<18:10,  1.39it/s]Evaluating on VQA val set:  43%|####3     | 1161/2671 [13:50<18:05,  1.39it/s]Evaluating on VQA val set:  44%|####3     | 1162/2671 [13:51<18:30,  1.36it/s]Evaluating on VQA val set:  44%|####3     | 1163/2671 [13:51<18:47,  1.34it/s]Evaluating on VQA val set:  44%|####3     | 1164/2671 [13:52<18:42,  1.34it/s]Evaluating on VQA val set:  44%|####3     | 1165/2671 [13:53<18:49,  1.33it/s]Evaluating on VQA val set:  44%|####3     | 1166/2671 [13:53<18:22,  1.36it/s]Evaluating on VQA val set:  44%|####3     | 1167/2671 [13:54<18:21,  1.37it/s]Evaluating on VQA val set:  44%|####3     | 1168/2671 [13:55<17:38,  1.42it/s]Evaluating on VQA val set:  44%|####3     | 1169/2671 [13:55<17:12,  1.45it/s]Evaluating on VQA val set:  44%|####3     | 1170/2671 [13:56<17:56,  1.39it/s]Evaluating on VQA val set:  44%|####3     | 1171/2671 [13:57<18:09,  1.38it/s]Evaluating on VQA val set:  44%|####3     | 1172/2671 [13:58<18:21,  1.36it/s]Evaluating on VQA val set:  44%|####3     | 1173/2671 [13:59<18:21,  1.36it/s]Evaluating on VQA val set:  44%|####3     | 1174/2671 [13:59<18:13,  1.37it/s]Evaluating on VQA val set:  44%|####3     | 1175/2671 [14:00<18:10,  1.37it/s]Evaluating on VQA val set:  44%|####4     | 1176/2671 [14:01<18:03,  1.38it/s]Evaluating on VQA val set:  44%|####4     | 1177/2671 [14:01<17:33,  1.42it/s]Evaluating on VQA val set:  44%|####4     | 1178/2671 [14:02<17:30,  1.42it/s]Evaluating on VQA val set:  44%|####4     | 1179/2671 [14:03<17:12,  1.44it/s]Evaluating on VQA val set:  44%|####4     | 1180/2671 [14:03<17:13,  1.44it/s]Evaluating on VQA val set:  44%|####4     | 1181/2671 [14:04<17:26,  1.42it/s]Evaluating on VQA val set:  44%|####4     | 1182/2671 [14:05<17:10,  1.44it/s]Evaluating on VQA val set:  44%|####4     | 1183/2671 [14:05<16:55,  1.46it/s]Evaluating on VQA val set:  44%|####4     | 1184/2671 [14:06<17:30,  1.41it/s]Evaluating on VQA val set:  44%|####4     | 1185/2671 [14:07<17:52,  1.39it/s]Evaluating on VQA val set:  44%|####4     | 1186/2671 [14:08<17:55,  1.38it/s]Evaluating on VQA val set:  44%|####4     | 1187/2671 [14:08<17:17,  1.43it/s]Evaluating on VQA val set:  44%|####4     | 1188/2671 [14:09<17:32,  1.41it/s]Evaluating on VQA val set:  45%|####4     | 1189/2671 [14:10<17:15,  1.43it/s]Evaluating on VQA val set:  45%|####4     | 1190/2671 [14:10<17:29,  1.41it/s]Evaluating on VQA val set:  45%|####4     | 1191/2671 [14:11<17:38,  1.40it/s]Evaluating on VQA val set:  45%|####4     | 1192/2671 [14:12<17:10,  1.43it/s]Evaluating on VQA val set:  45%|####4     | 1193/2671 [14:13<17:14,  1.43it/s]Evaluating on VQA val set:  45%|####4     | 1194/2671 [14:13<16:49,  1.46it/s]Evaluating on VQA val set:  45%|####4     | 1195/2671 [14:14<16:40,  1.48it/s]Evaluating on VQA val set:  45%|####4     | 1196/2671 [14:15<16:52,  1.46it/s]Evaluating on VQA val set:  45%|####4     | 1197/2671 [14:15<17:10,  1.43it/s]Evaluating on VQA val set:  45%|####4     | 1198/2671 [14:16<17:34,  1.40it/s]Evaluating on VQA val set:  45%|####4     | 1199/2671 [14:17<17:30,  1.40it/s]Evaluating on VQA val set:  45%|####4     | 1200/2671 [14:18<17:46,  1.38it/s]Evaluating on VQA val set:  45%|####4     | 1201/2671 [14:18<17:53,  1.37it/s]Evaluating on VQA val set:  45%|####5     | 1202/2671 [14:19<18:03,  1.36it/s]Evaluating on VQA val set:  45%|####5     | 1203/2671 [14:20<18:07,  1.35it/s]Evaluating on VQA val set:  45%|####5     | 1204/2671 [14:20<17:42,  1.38it/s]Evaluating on VQA val set:  45%|####5     | 1205/2671 [14:21<17:25,  1.40it/s]Evaluating on VQA val set:  45%|####5     | 1206/2671 [14:22<17:20,  1.41it/s]Evaluating on VQA val set:  45%|####5     | 1207/2671 [14:23<17:55,  1.36it/s]Evaluating on VQA val set:  45%|####5     | 1208/2671 [14:23<18:07,  1.35it/s]Evaluating on VQA val set:  45%|####5     | 1209/2671 [14:24<18:14,  1.34it/s]Evaluating on VQA val set:  45%|####5     | 1210/2671 [14:25<17:40,  1.38it/s]Evaluating on VQA val set:  45%|####5     | 1211/2671 [14:26<17:17,  1.41it/s]Evaluating on VQA val set:  45%|####5     | 1212/2671 [14:26<16:52,  1.44it/s]Evaluating on VQA val set:  45%|####5     | 1213/2671 [14:27<17:16,  1.41it/s]Evaluating on VQA val set:  45%|####5     | 1214/2671 [14:28<17:19,  1.40it/s]Evaluating on VQA val set:  45%|####5     | 1215/2671 [14:28<17:32,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1216/2671 [14:29<17:23,  1.39it/s]Evaluating on VQA val set:  46%|####5     | 1217/2671 [14:30<17:31,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1218/2671 [14:31<17:43,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1219/2671 [14:31<17:35,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1220/2671 [14:32<17:32,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1221/2671 [14:33<17:36,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1222/2671 [14:33<17:38,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1223/2671 [14:34<17:36,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1224/2671 [14:35<17:47,  1.36it/s]Evaluating on VQA val set:  46%|####5     | 1225/2671 [14:36<17:35,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1226/2671 [14:36<17:32,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1227/2671 [14:37<17:36,  1.37it/s]Evaluating on VQA val set:  46%|####5     | 1228/2671 [14:38<17:42,  1.36it/s]Evaluating on VQA val set:  46%|####6     | 1229/2671 [14:39<17:20,  1.39it/s]Evaluating on VQA val set:  46%|####6     | 1230/2671 [14:39<17:23,  1.38it/s]Evaluating on VQA val set:  46%|####6     | 1231/2671 [14:40<17:43,  1.35it/s]Evaluating on VQA val set:  46%|####6     | 1232/2671 [14:41<17:02,  1.41it/s]Evaluating on VQA val set:  46%|####6     | 1233/2671 [14:41<16:57,  1.41it/s]Evaluating on VQA val set:  46%|####6     | 1234/2671 [14:42<17:28,  1.37it/s]Evaluating on VQA val set:  46%|####6     | 1235/2671 [14:43<16:58,  1.41it/s]Evaluating on VQA val set:  46%|####6     | 1236/2671 [14:44<17:23,  1.37it/s]Evaluating on VQA val set:  46%|####6     | 1237/2671 [14:44<16:57,  1.41it/s]Evaluating on VQA val set:  46%|####6     | 1238/2671 [14:45<17:03,  1.40it/s]Evaluating on VQA val set:  46%|####6     | 1239/2671 [14:46<17:29,  1.36it/s]Evaluating on VQA val set:  46%|####6     | 1240/2671 [14:47<17:18,  1.38it/s]Evaluating on VQA val set:  46%|####6     | 1241/2671 [14:47<17:22,  1.37it/s]Evaluating on VQA val set:  46%|####6     | 1242/2671 [14:48<16:36,  1.43it/s]Evaluating on VQA val set:  47%|####6     | 1243/2671 [14:49<16:49,  1.41it/s]Evaluating on VQA val set:  47%|####6     | 1244/2671 [14:49<17:12,  1.38it/s]Evaluating on VQA val set:  47%|####6     | 1245/2671 [14:50<17:00,  1.40it/s]Evaluating on VQA val set:  47%|####6     | 1246/2671 [14:51<17:21,  1.37it/s]Evaluating on VQA val set:  47%|####6     | 1247/2671 [14:52<17:05,  1.39it/s]Evaluating on VQA val set:  47%|####6     | 1248/2671 [14:52<17:17,  1.37it/s]Evaluating on VQA val set:  47%|####6     | 1249/2671 [14:53<16:53,  1.40it/s]Evaluating on VQA val set:  47%|####6     | 1250/2671 [14:54<16:33,  1.43it/s]Evaluating on VQA val set:  47%|####6     | 1251/2671 [14:54<16:40,  1.42it/s]Evaluating on VQA val set:  47%|####6     | 1252/2671 [14:55<17:08,  1.38it/s]Evaluating on VQA val set:  47%|####6     | 1253/2671 [14:56<17:22,  1.36it/s]Evaluating on VQA val set:  47%|####6     | 1254/2671 [14:57<17:11,  1.37it/s]Evaluating on VQA val set:  47%|####6     | 1255/2671 [14:57<16:52,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1256/2671 [14:58<16:46,  1.41it/s]Evaluating on VQA val set:  47%|####7     | 1257/2671 [14:59<16:32,  1.42it/s]Evaluating on VQA val set:  47%|####7     | 1258/2671 [14:59<16:37,  1.42it/s]Evaluating on VQA val set:  47%|####7     | 1259/2671 [15:00<16:48,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1260/2671 [15:01<16:58,  1.39it/s]Evaluating on VQA val set:  47%|####7     | 1261/2671 [15:02<17:09,  1.37it/s]Evaluating on VQA val set:  47%|####7     | 1262/2671 [15:02<17:06,  1.37it/s]Evaluating on VQA val set:  47%|####7     | 1263/2671 [15:03<16:43,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1264/2671 [15:04<16:50,  1.39it/s]Evaluating on VQA val set:  47%|####7     | 1265/2671 [15:04<16:45,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1266/2671 [15:05<16:48,  1.39it/s]Evaluating on VQA val set:  47%|####7     | 1267/2671 [15:06<16:38,  1.41it/s]Evaluating on VQA val set:  47%|####7     | 1268/2671 [15:06<15:51,  1.47it/s]Evaluating on VQA val set:  48%|####7     | 1269/2671 [15:07<15:24,  1.52it/s]Evaluating on VQA val set:  48%|####7     | 1270/2671 [15:08<15:56,  1.46it/s]Evaluating on VQA val set:  48%|####7     | 1271/2671 [15:08<15:42,  1.48it/s]Evaluating on VQA val set:  48%|####7     | 1272/2671 [15:09<16:42,  1.40it/s]Evaluating on VQA val set:  48%|####7     | 1273/2671 [15:10<16:28,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1274/2671 [15:11<16:17,  1.43it/s]Evaluating on VQA val set:  48%|####7     | 1275/2671 [15:11<16:26,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1276/2671 [15:12<16:43,  1.39it/s]Evaluating on VQA val set:  48%|####7     | 1277/2671 [15:13<17:15,  1.35it/s]Evaluating on VQA val set:  48%|####7     | 1278/2671 [15:14<16:40,  1.39it/s]Evaluating on VQA val set:  48%|####7     | 1279/2671 [15:14<16:20,  1.42it/s]Evaluating on VQA val set:  48%|####7     | 1280/2671 [15:15<16:26,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1281/2671 [15:16<16:37,  1.39it/s]Evaluating on VQA val set:  48%|####7     | 1282/2671 [15:16<16:48,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1283/2671 [15:17<17:01,  1.36it/s]Evaluating on VQA val set:  48%|####8     | 1284/2671 [15:18<16:46,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1285/2671 [15:19<16:46,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1286/2671 [15:19<16:22,  1.41it/s]Evaluating on VQA val set:  48%|####8     | 1287/2671 [15:20<16:29,  1.40it/s]Evaluating on VQA val set:  48%|####8     | 1288/2671 [15:21<16:29,  1.40it/s]Evaluating on VQA val set:  48%|####8     | 1289/2671 [15:21<16:10,  1.42it/s]Evaluating on VQA val set:  48%|####8     | 1290/2671 [15:22<16:30,  1.39it/s]Evaluating on VQA val set:  48%|####8     | 1291/2671 [15:23<16:49,  1.37it/s]Evaluating on VQA val set:  48%|####8     | 1292/2671 [15:24<17:12,  1.34it/s]Evaluating on VQA val set:  48%|####8     | 1293/2671 [15:24<16:40,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1294/2671 [15:25<16:33,  1.39it/s]Evaluating on VQA val set:  48%|####8     | 1295/2671 [15:26<16:29,  1.39it/s]Evaluating on VQA val set:  49%|####8     | 1296/2671 [15:26<16:02,  1.43it/s]Evaluating on VQA val set:  49%|####8     | 1297/2671 [15:27<15:44,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1298/2671 [15:28<15:37,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1299/2671 [15:28<15:37,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1300/2671 [15:29<15:40,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1301/2671 [15:30<16:28,  1.39it/s]Evaluating on VQA val set:  49%|####8     | 1302/2671 [15:31<16:31,  1.38it/s]Evaluating on VQA val set:  49%|####8     | 1303/2671 [15:31<15:58,  1.43it/s]Evaluating on VQA val set:  49%|####8     | 1304/2671 [15:32<16:34,  1.37it/s]Evaluating on VQA val set:  49%|####8     | 1305/2671 [15:33<15:38,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1306/2671 [15:33<15:05,  1.51it/s]Evaluating on VQA val set:  49%|####8     | 1307/2671 [15:34<16:02,  1.42it/s]Evaluating on VQA val set:  49%|####8     | 1308/2671 [15:35<16:12,  1.40it/s]Evaluating on VQA val set:  49%|####9     | 1309/2671 [15:36<15:59,  1.42it/s]Evaluating on VQA val set:  49%|####9     | 1310/2671 [15:36<15:13,  1.49it/s]Evaluating on VQA val set:  49%|####9     | 1311/2671 [15:37<15:07,  1.50it/s]Evaluating on VQA val set:  49%|####9     | 1312/2671 [15:37<15:06,  1.50it/s]Evaluating on VQA val set:  49%|####9     | 1313/2671 [15:38<16:05,  1.41it/s]Evaluating on VQA val set:  49%|####9     | 1314/2671 [15:39<16:32,  1.37it/s]Evaluating on VQA val set:  49%|####9     | 1315/2671 [15:40<16:12,  1.39it/s]Evaluating on VQA val set:  49%|####9     | 1316/2671 [15:41<16:50,  1.34it/s]Evaluating on VQA val set:  49%|####9     | 1317/2671 [15:41<16:30,  1.37it/s]Evaluating on VQA val set:  49%|####9     | 1318/2671 [15:42<16:01,  1.41it/s]Evaluating on VQA val set:  49%|####9     | 1319/2671 [15:43<16:08,  1.40it/s]Evaluating on VQA val set:  49%|####9     | 1320/2671 [15:43<16:03,  1.40it/s]Evaluating on VQA val set:  49%|####9     | 1321/2671 [15:44<15:53,  1.42it/s]Evaluating on VQA val set:  49%|####9     | 1322/2671 [15:45<15:27,  1.46it/s]Evaluating on VQA val set:  50%|####9     | 1323/2671 [15:45<15:29,  1.45it/s]Evaluating on VQA val set:  50%|####9     | 1324/2671 [15:46<15:13,  1.47it/s]Evaluating on VQA val set:  50%|####9     | 1325/2671 [15:47<15:32,  1.44it/s]Evaluating on VQA val set:  50%|####9     | 1326/2671 [15:47<15:16,  1.47it/s]Evaluating on VQA val set:  50%|####9     | 1327/2671 [15:48<15:29,  1.45it/s]Evaluating on VQA val set:  50%|####9     | 1328/2671 [15:49<15:43,  1.42it/s]Evaluating on VQA val set:  50%|####9     | 1329/2671 [15:50<15:47,  1.42it/s]Evaluating on VQA val set:  50%|####9     | 1330/2671 [15:50<15:56,  1.40it/s]Evaluating on VQA val set:  50%|####9     | 1331/2671 [15:51<15:50,  1.41it/s]Evaluating on VQA val set:  50%|####9     | 1332/2671 [15:52<16:01,  1.39it/s]Evaluating on VQA val set:  50%|####9     | 1333/2671 [15:52<16:00,  1.39it/s]Evaluating on VQA val set:  50%|####9     | 1334/2671 [15:53<16:12,  1.37it/s]Evaluating on VQA val set:  50%|####9     | 1335/2671 [15:54<16:21,  1.36it/s]Evaluating on VQA val set:  50%|#####     | 1336/2671 [15:55<15:52,  1.40it/s]Evaluating on VQA val set:  50%|#####     | 1337/2671 [15:55<16:07,  1.38it/s]Evaluating on VQA val set:  50%|#####     | 1338/2671 [15:56<15:55,  1.40it/s]Evaluating on VQA val set:  50%|#####     | 1339/2671 [15:57<15:30,  1.43it/s]Evaluating on VQA val set:  50%|#####     | 1340/2671 [15:57<15:00,  1.48it/s]Evaluating on VQA val set:  50%|#####     | 1341/2671 [15:58<15:11,  1.46it/s]Evaluating on VQA val set:  50%|#####     | 1342/2671 [15:59<15:41,  1.41it/s]Evaluating on VQA val set:  50%|#####     | 1343/2671 [16:00<15:33,  1.42it/s]Evaluating on VQA val set:  50%|#####     | 1344/2671 [16:00<15:29,  1.43it/s]Evaluating on VQA val set:  50%|#####     | 1345/2671 [16:01<15:28,  1.43it/s]Evaluating on VQA val set:  50%|#####     | 1346/2671 [16:02<15:09,  1.46it/s]Evaluating on VQA val set:  50%|#####     | 1347/2671 [16:02<14:54,  1.48it/s]Evaluating on VQA val set:  50%|#####     | 1348/2671 [16:03<15:02,  1.47it/s]Evaluating on VQA val set:  51%|#####     | 1349/2671 [16:04<15:24,  1.43it/s]Evaluating on VQA val set:  51%|#####     | 1350/2671 [16:04<15:30,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1351/2671 [16:05<15:17,  1.44it/s]Evaluating on VQA val set:  51%|#####     | 1352/2671 [16:06<15:29,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1353/2671 [16:06<15:13,  1.44it/s]Evaluating on VQA val set:  51%|#####     | 1354/2671 [16:07<14:59,  1.46it/s]Evaluating on VQA val set:  51%|#####     | 1355/2671 [16:08<15:22,  1.43it/s]Evaluating on VQA val set:  51%|#####     | 1356/2671 [16:09<15:23,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1357/2671 [16:09<15:22,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1358/2671 [16:10<15:27,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1359/2671 [16:11<15:27,  1.41it/s]Evaluating on VQA val set:  51%|#####     | 1360/2671 [16:11<15:17,  1.43it/s]Evaluating on VQA val set:  51%|#####     | 1361/2671 [16:12<15:26,  1.41it/s]Evaluating on VQA val set:  51%|#####     | 1362/2671 [16:13<14:54,  1.46it/s]Evaluating on VQA val set:  51%|#####1    | 1363/2671 [16:13<14:21,  1.52it/s]Evaluating on VQA val set:  51%|#####1    | 1364/2671 [16:14<14:54,  1.46it/s]Evaluating on VQA val set:  51%|#####1    | 1365/2671 [16:15<15:12,  1.43it/s]Evaluating on VQA val set:  51%|#####1    | 1366/2671 [16:16<15:23,  1.41it/s]Evaluating on VQA val set:  51%|#####1    | 1367/2671 [16:16<15:24,  1.41it/s]Evaluating on VQA val set:  51%|#####1    | 1368/2671 [16:17<15:35,  1.39it/s]Evaluating on VQA val set:  51%|#####1    | 1369/2671 [16:18<15:51,  1.37it/s]Evaluating on VQA val set:  51%|#####1    | 1370/2671 [16:18<15:21,  1.41it/s]Evaluating on VQA val set:  51%|#####1    | 1371/2671 [16:19<15:53,  1.36it/s]Evaluating on VQA val set:  51%|#####1    | 1372/2671 [16:20<15:48,  1.37it/s]Evaluating on VQA val set:  51%|#####1    | 1373/2671 [16:21<15:15,  1.42it/s]Evaluating on VQA val set:  51%|#####1    | 1374/2671 [16:21<14:33,  1.48it/s]Evaluating on VQA val set:  51%|#####1    | 1375/2671 [16:22<14:38,  1.47it/s]Evaluating on VQA val set:  52%|#####1    | 1376/2671 [16:23<14:46,  1.46it/s]Evaluating on VQA val set:  52%|#####1    | 1377/2671 [16:23<14:51,  1.45it/s]Evaluating on VQA val set:  52%|#####1    | 1378/2671 [16:24<15:13,  1.42it/s]Evaluating on VQA val set:  52%|#####1    | 1379/2671 [16:25<15:19,  1.41it/s]Evaluating on VQA val set:  52%|#####1    | 1380/2671 [16:25<15:25,  1.39it/s]Evaluating on VQA val set:  52%|#####1    | 1381/2671 [16:26<15:41,  1.37it/s]Evaluating on VQA val set:  52%|#####1    | 1382/2671 [16:27<15:24,  1.39it/s]Evaluating on VQA val set:  52%|#####1    | 1383/2671 [16:28<15:00,  1.43it/s]Evaluating on VQA val set:  52%|#####1    | 1384/2671 [16:28<15:34,  1.38it/s]Evaluating on VQA val set:  52%|#####1    | 1385/2671 [16:29<15:47,  1.36it/s]Evaluating on VQA val set:  52%|#####1    | 1386/2671 [16:30<15:41,  1.37it/s]Evaluating on VQA val set:  52%|#####1    | 1387/2671 [16:31<16:00,  1.34it/s]Evaluating on VQA val set:  52%|#####1    | 1388/2671 [16:31<15:29,  1.38it/s]Evaluating on VQA val set:  52%|#####2    | 1389/2671 [16:32<15:43,  1.36it/s]Evaluating on VQA val set:  52%|#####2    | 1390/2671 [16:33<15:21,  1.39it/s]Evaluating on VQA val set:  52%|#####2    | 1391/2671 [16:33<15:16,  1.40it/s]Evaluating on VQA val set:  52%|#####2    | 1392/2671 [16:34<15:01,  1.42it/s]Evaluating on VQA val set:  52%|#####2    | 1393/2671 [16:35<14:56,  1.43it/s]Evaluating on VQA val set:  52%|#####2    | 1394/2671 [16:36<15:10,  1.40it/s]Evaluating on VQA val set:  52%|#####2    | 1395/2671 [16:36<15:16,  1.39it/s]Evaluating on VQA val set:  52%|#####2    | 1396/2671 [16:37<14:53,  1.43it/s]Evaluating on VQA val set:  52%|#####2    | 1397/2671 [16:38<14:46,  1.44it/s]Evaluating on VQA val set:  52%|#####2    | 1398/2671 [16:38<14:38,  1.45it/s]Evaluating on VQA val set:  52%|#####2    | 1399/2671 [16:39<14:28,  1.46it/s]Evaluating on VQA val set:  52%|#####2    | 1400/2671 [16:40<14:08,  1.50it/s]Evaluating on VQA val set:  52%|#####2    | 1401/2671 [16:40<14:31,  1.46it/s]Evaluating on VQA val set:  52%|#####2    | 1402/2671 [16:41<15:09,  1.39it/s]Evaluating on VQA val set:  53%|#####2    | 1403/2671 [16:42<15:17,  1.38it/s]Evaluating on VQA val set:  53%|#####2    | 1404/2671 [16:43<15:17,  1.38it/s]Evaluating on VQA val set:  53%|#####2    | 1405/2671 [16:43<14:48,  1.43it/s]Evaluating on VQA val set:  53%|#####2    | 1406/2671 [16:44<15:01,  1.40it/s]Evaluating on VQA val set:  53%|#####2    | 1407/2671 [16:45<14:31,  1.45it/s]Evaluating on VQA val set:  53%|#####2    | 1408/2671 [16:45<14:57,  1.41it/s]Evaluating on VQA val set:  53%|#####2    | 1409/2671 [16:46<15:02,  1.40it/s]Evaluating on VQA val set:  53%|#####2    | 1410/2671 [16:47<14:49,  1.42it/s]Evaluating on VQA val set:  53%|#####2    | 1411/2671 [16:47<14:52,  1.41it/s]Evaluating on VQA val set:  53%|#####2    | 1412/2671 [16:48<15:06,  1.39it/s]Evaluating on VQA val set:  53%|#####2    | 1413/2671 [16:49<14:59,  1.40it/s]Evaluating on VQA val set:  53%|#####2    | 1414/2671 [16:50<15:23,  1.36it/s]Evaluating on VQA val set:  53%|#####2    | 1415/2671 [16:50<15:00,  1.40it/s]Evaluating on VQA val set:  53%|#####3    | 1416/2671 [16:51<14:51,  1.41it/s]Evaluating on VQA val set:  53%|#####3    | 1417/2671 [16:52<15:00,  1.39it/s]Evaluating on VQA val set:  53%|#####3    | 1418/2671 [16:53<15:08,  1.38it/s]Evaluating on VQA val set:  53%|#####3    | 1419/2671 [16:53<14:56,  1.40it/s]Evaluating on VQA val set:  53%|#####3    | 1420/2671 [16:54<14:49,  1.41it/s]Evaluating on VQA val set:  53%|#####3    | 1421/2671 [16:55<14:51,  1.40it/s]Evaluating on VQA val set:  53%|#####3    | 1422/2671 [16:55<14:56,  1.39it/s]Evaluating on VQA val set:  53%|#####3    | 1423/2671 [16:56<14:43,  1.41it/s]Evaluating on VQA val set:  53%|#####3    | 1424/2671 [16:57<14:55,  1.39it/s]Evaluating on VQA val set:  53%|#####3    | 1425/2671 [16:58<14:40,  1.42it/s]Evaluating on VQA val set:  53%|#####3    | 1426/2671 [16:58<14:41,  1.41it/s]Evaluating on VQA val set:  53%|#####3    | 1427/2671 [16:59<14:24,  1.44it/s]Evaluating on VQA val set:  53%|#####3    | 1428/2671 [17:00<14:28,  1.43it/s]Evaluating on VQA val set:  54%|#####3    | 1429/2671 [17:00<14:57,  1.38it/s]Evaluating on VQA val set:  54%|#####3    | 1430/2671 [17:01<14:40,  1.41it/s]Evaluating on VQA val set:  54%|#####3    | 1431/2671 [17:02<14:09,  1.46it/s]Evaluating on VQA val set:  54%|#####3    | 1432/2671 [17:02<13:33,  1.52it/s]Evaluating on VQA val set:  54%|#####3    | 1433/2671 [17:03<14:14,  1.45it/s]Evaluating on VQA val set:  54%|#####3    | 1434/2671 [17:04<14:11,  1.45it/s]Evaluating on VQA val set:  54%|#####3    | 1435/2671 [17:04<14:29,  1.42it/s]Evaluating on VQA val set:  54%|#####3    | 1436/2671 [17:05<14:40,  1.40it/s]Evaluating on VQA val set:  54%|#####3    | 1437/2671 [17:06<14:57,  1.38it/s]Evaluating on VQA val set:  54%|#####3    | 1438/2671 [17:07<14:52,  1.38it/s]Evaluating on VQA val set:  54%|#####3    | 1439/2671 [17:07<15:07,  1.36it/s]Evaluating on VQA val set:  54%|#####3    | 1440/2671 [17:08<14:56,  1.37it/s]Evaluating on VQA val set:  54%|#####3    | 1441/2671 [17:09<15:16,  1.34it/s]Evaluating on VQA val set:  54%|#####3    | 1442/2671 [17:10<14:55,  1.37it/s]Evaluating on VQA val set:  54%|#####4    | 1443/2671 [17:10<14:53,  1.37it/s]Evaluating on VQA val set:  54%|#####4    | 1444/2671 [17:11<14:49,  1.38it/s]Evaluating on VQA val set:  54%|#####4    | 1445/2671 [17:12<14:46,  1.38it/s]Evaluating on VQA val set:  54%|#####4    | 1446/2671 [17:13<14:54,  1.37it/s]Evaluating on VQA val set:  54%|#####4    | 1447/2671 [17:13<14:39,  1.39it/s]Evaluating on VQA val set:  54%|#####4    | 1448/2671 [17:14<15:04,  1.35it/s]Evaluating on VQA val set:  54%|#####4    | 1449/2671 [17:15<14:56,  1.36it/s]Evaluating on VQA val set:  54%|#####4    | 1450/2671 [17:15<14:22,  1.41it/s]Evaluating on VQA val set:  54%|#####4    | 1451/2671 [17:16<14:16,  1.43it/s]Evaluating on VQA val set:  54%|#####4    | 1452/2671 [17:17<14:29,  1.40it/s]Evaluating on VQA val set:  54%|#####4    | 1453/2671 [17:17<14:19,  1.42it/s]Evaluating on VQA val set:  54%|#####4    | 1454/2671 [17:18<14:27,  1.40it/s]Evaluating on VQA val set:  54%|#####4    | 1455/2671 [17:19<14:51,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1456/2671 [17:20<14:53,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1457/2671 [17:20<14:48,  1.37it/s]Evaluating on VQA val set:  55%|#####4    | 1458/2671 [17:21<14:52,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1459/2671 [17:22<14:03,  1.44it/s]Evaluating on VQA val set:  55%|#####4    | 1460/2671 [17:23<14:24,  1.40it/s]Evaluating on VQA val set:  55%|#####4    | 1461/2671 [17:23<14:37,  1.38it/s]Evaluating on VQA val set:  55%|#####4    | 1462/2671 [17:24<14:42,  1.37it/s]Evaluating on VQA val set:  55%|#####4    | 1463/2671 [17:25<14:45,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1464/2671 [17:25<14:05,  1.43it/s]Evaluating on VQA val set:  55%|#####4    | 1465/2671 [17:26<14:03,  1.43it/s]Evaluating on VQA val set:  55%|#####4    | 1466/2671 [17:27<14:15,  1.41it/s]Evaluating on VQA val set:  55%|#####4    | 1467/2671 [17:28<14:19,  1.40it/s]Evaluating on VQA val set:  55%|#####4    | 1468/2671 [17:28<14:21,  1.40it/s]Evaluating on VQA val set:  55%|#####4    | 1469/2671 [17:29<14:16,  1.40it/s]Evaluating on VQA val set:  55%|#####5    | 1470/2671 [17:30<14:33,  1.37it/s]Evaluating on VQA val set:  55%|#####5    | 1471/2671 [17:30<14:19,  1.40it/s]Evaluating on VQA val set:  55%|#####5    | 1472/2671 [17:31<13:39,  1.46it/s]Evaluating on VQA val set:  55%|#####5    | 1473/2671 [17:32<13:13,  1.51it/s]Evaluating on VQA val set:  55%|#####5    | 1474/2671 [17:32<12:56,  1.54it/s]Evaluating on VQA val set:  55%|#####5    | 1475/2671 [17:33<13:18,  1.50it/s]Evaluating on VQA val set:  55%|#####5    | 1476/2671 [17:34<13:23,  1.49it/s]Evaluating on VQA val set:  55%|#####5    | 1477/2671 [17:34<13:09,  1.51it/s]Evaluating on VQA val set:  55%|#####5    | 1478/2671 [17:35<13:16,  1.50it/s]Evaluating on VQA val set:  55%|#####5    | 1479/2671 [17:36<12:45,  1.56it/s]Evaluating on VQA val set:  55%|#####5    | 1480/2671 [17:36<12:49,  1.55it/s]Evaluating on VQA val set:  55%|#####5    | 1481/2671 [17:37<13:12,  1.50it/s]Evaluating on VQA val set:  55%|#####5    | 1482/2671 [17:38<13:25,  1.48it/s]Evaluating on VQA val set:  56%|#####5    | 1483/2671 [17:38<13:48,  1.43it/s]Evaluating on VQA val set:  56%|#####5    | 1484/2671 [17:39<13:43,  1.44it/s]Evaluating on VQA val set:  56%|#####5    | 1485/2671 [17:40<14:05,  1.40it/s]Evaluating on VQA val set:  56%|#####5    | 1486/2671 [17:41<13:55,  1.42it/s]Evaluating on VQA val set:  56%|#####5    | 1487/2671 [17:41<14:10,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1488/2671 [17:42<13:23,  1.47it/s]Evaluating on VQA val set:  56%|#####5    | 1489/2671 [17:43<13:28,  1.46it/s]Evaluating on VQA val set:  56%|#####5    | 1490/2671 [17:43<13:46,  1.43it/s]Evaluating on VQA val set:  56%|#####5    | 1491/2671 [17:44<14:06,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1492/2671 [17:45<14:11,  1.38it/s]Evaluating on VQA val set:  56%|#####5    | 1493/2671 [17:46<14:09,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1494/2671 [17:46<14:08,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1495/2671 [17:47<13:56,  1.41it/s]Evaluating on VQA val set:  56%|#####6    | 1496/2671 [17:48<13:24,  1.46it/s]Evaluating on VQA val set:  56%|#####6    | 1497/2671 [17:48<13:16,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1498/2671 [17:49<13:11,  1.48it/s]Evaluating on VQA val set:  56%|#####6    | 1499/2671 [17:50<13:30,  1.45it/s]Evaluating on VQA val set:  56%|#####6    | 1500/2671 [17:50<13:20,  1.46it/s]Evaluating on VQA val set:  56%|#####6    | 1501/2671 [17:51<13:04,  1.49it/s]Evaluating on VQA val set:  56%|#####6    | 1502/2671 [17:52<13:16,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1503/2671 [17:52<13:12,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1504/2671 [17:53<12:59,  1.50it/s]Evaluating on VQA val set:  56%|#####6    | 1505/2671 [17:54<13:17,  1.46it/s]Evaluating on VQA val set:  56%|#####6    | 1506/2671 [17:54<13:33,  1.43it/s]Evaluating on VQA val set:  56%|#####6    | 1507/2671 [17:55<13:40,  1.42it/s]Evaluating on VQA val set:  56%|#####6    | 1508/2671 [17:56<13:40,  1.42it/s]Evaluating on VQA val set:  56%|#####6    | 1509/2671 [17:57<13:46,  1.41it/s]Evaluating on VQA val set:  57%|#####6    | 1510/2671 [17:57<14:00,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1511/2671 [17:58<14:05,  1.37it/s]Evaluating on VQA val set:  57%|#####6    | 1512/2671 [17:59<13:39,  1.41it/s]Evaluating on VQA val set:  57%|#####6    | 1513/2671 [17:59<13:42,  1.41it/s]Evaluating on VQA val set:  57%|#####6    | 1514/2671 [18:00<13:52,  1.39it/s]Evaluating on VQA val set:  57%|#####6    | 1515/2671 [18:01<13:46,  1.40it/s]Evaluating on VQA val set:  57%|#####6    | 1516/2671 [18:02<14:05,  1.37it/s]Evaluating on VQA val set:  57%|#####6    | 1517/2671 [18:02<13:53,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1518/2671 [18:03<13:24,  1.43it/s]Evaluating on VQA val set:  57%|#####6    | 1519/2671 [18:04<13:32,  1.42it/s]Evaluating on VQA val set:  57%|#####6    | 1520/2671 [18:04<13:43,  1.40it/s]Evaluating on VQA val set:  57%|#####6    | 1521/2671 [18:05<13:51,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1522/2671 [18:06<13:41,  1.40it/s]Evaluating on VQA val set:  57%|#####7    | 1523/2671 [18:07<13:45,  1.39it/s]Evaluating on VQA val set:  57%|#####7    | 1524/2671 [18:07<13:40,  1.40it/s]Evaluating on VQA val set:  57%|#####7    | 1525/2671 [18:08<13:21,  1.43it/s]Evaluating on VQA val set:  57%|#####7    | 1526/2671 [18:09<13:22,  1.43it/s]Evaluating on VQA val set:  57%|#####7    | 1527/2671 [18:09<13:30,  1.41it/s]Evaluating on VQA val set:  57%|#####7    | 1528/2671 [18:10<13:10,  1.45it/s]Evaluating on VQA val set:  57%|#####7    | 1529/2671 [18:11<12:56,  1.47it/s]Evaluating on VQA val set:  57%|#####7    | 1530/2671 [18:11<13:05,  1.45it/s]Evaluating on VQA val set:  57%|#####7    | 1531/2671 [18:12<13:19,  1.43it/s]Evaluating on VQA val set:  57%|#####7    | 1532/2671 [18:13<13:28,  1.41it/s]Evaluating on VQA val set:  57%|#####7    | 1533/2671 [18:14<13:37,  1.39it/s]Evaluating on VQA val set:  57%|#####7    | 1534/2671 [18:14<13:22,  1.42it/s]Evaluating on VQA val set:  57%|#####7    | 1535/2671 [18:15<13:13,  1.43it/s]Evaluating on VQA val set:  58%|#####7    | 1536/2671 [18:16<13:18,  1.42it/s]Evaluating on VQA val set:  58%|#####7    | 1537/2671 [18:16<13:18,  1.42it/s]Evaluating on VQA val set:  58%|#####7    | 1538/2671 [18:17<13:51,  1.36it/s]Evaluating on VQA val set:  58%|#####7    | 1539/2671 [18:18<13:53,  1.36it/s]Evaluating on VQA val set:  58%|#####7    | 1540/2671 [18:19<13:15,  1.42it/s]Evaluating on VQA val set:  58%|#####7    | 1541/2671 [18:19<12:59,  1.45it/s]Evaluating on VQA val set:  58%|#####7    | 1542/2671 [18:20<13:17,  1.42it/s]Evaluating on VQA val set:  58%|#####7    | 1543/2671 [18:21<13:19,  1.41it/s]Evaluating on VQA val set:  58%|#####7    | 1544/2671 [18:21<13:31,  1.39it/s]Evaluating on VQA val set:  58%|#####7    | 1545/2671 [18:22<13:48,  1.36it/s]Evaluating on VQA val set:  58%|#####7    | 1546/2671 [18:23<13:51,  1.35it/s]Evaluating on VQA val set:  58%|#####7    | 1547/2671 [18:24<13:42,  1.37it/s]Evaluating on VQA val set:  58%|#####7    | 1548/2671 [18:24<13:30,  1.39it/s]Evaluating on VQA val set:  58%|#####7    | 1549/2671 [18:25<13:31,  1.38it/s]Evaluating on VQA val set:  58%|#####8    | 1550/2671 [18:26<13:45,  1.36it/s]Evaluating on VQA val set:  58%|#####8    | 1551/2671 [18:27<13:50,  1.35it/s]Evaluating on VQA val set:  58%|#####8    | 1552/2671 [18:27<13:33,  1.38it/s]Evaluating on VQA val set:  58%|#####8    | 1553/2671 [18:28<13:25,  1.39it/s]Evaluating on VQA val set:  58%|#####8    | 1554/2671 [18:29<13:19,  1.40it/s]Evaluating on VQA val set:  58%|#####8    | 1555/2671 [18:29<13:10,  1.41it/s]Evaluating on VQA val set:  58%|#####8    | 1556/2671 [18:30<13:07,  1.42it/s]Evaluating on VQA val set:  58%|#####8    | 1557/2671 [18:31<13:18,  1.39it/s]Evaluating on VQA val set:  58%|#####8    | 1558/2671 [18:32<13:32,  1.37it/s]Evaluating on VQA val set:  58%|#####8    | 1559/2671 [18:32<13:36,  1.36it/s]Evaluating on VQA val set:  58%|#####8    | 1560/2671 [18:33<13:45,  1.35it/s]Evaluating on VQA val set:  58%|#####8    | 1561/2671 [18:34<13:42,  1.35it/s]Evaluating on VQA val set:  58%|#####8    | 1562/2671 [18:35<13:31,  1.37it/s]Evaluating on VQA val set:  59%|#####8    | 1563/2671 [18:35<13:12,  1.40it/s]Evaluating on VQA val set:  59%|#####8    | 1564/2671 [18:36<13:06,  1.41it/s]Evaluating on VQA val set:  59%|#####8    | 1565/2671 [18:37<13:30,  1.37it/s]Evaluating on VQA val set:  59%|#####8    | 1566/2671 [18:37<13:17,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1567/2671 [18:38<13:04,  1.41it/s]Evaluating on VQA val set:  59%|#####8    | 1568/2671 [18:39<13:34,  1.35it/s]Evaluating on VQA val set:  59%|#####8    | 1569/2671 [18:40<13:13,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1570/2671 [18:40<13:08,  1.40it/s]Evaluating on VQA val set:  59%|#####8    | 1571/2671 [18:41<13:09,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1572/2671 [18:42<12:48,  1.43it/s]Evaluating on VQA val set:  59%|#####8    | 1573/2671 [18:42<12:46,  1.43it/s]Evaluating on VQA val set:  59%|#####8    | 1574/2671 [18:43<13:06,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1575/2671 [18:44<13:02,  1.40it/s]Evaluating on VQA val set:  59%|#####9    | 1576/2671 [18:45<13:26,  1.36it/s]Evaluating on VQA val set:  59%|#####9    | 1577/2671 [18:45<13:24,  1.36it/s]Evaluating on VQA val set:  59%|#####9    | 1578/2671 [18:46<12:59,  1.40it/s]Evaluating on VQA val set:  59%|#####9    | 1579/2671 [18:47<13:03,  1.39it/s]Evaluating on VQA val set:  59%|#####9    | 1580/2671 [18:47<13:09,  1.38it/s]Evaluating on VQA val set:  59%|#####9    | 1581/2671 [18:48<13:23,  1.36it/s]Evaluating on VQA val set:  59%|#####9    | 1582/2671 [18:49<13:28,  1.35it/s]Evaluating on VQA val set:  59%|#####9    | 1583/2671 [18:50<13:29,  1.34it/s]Evaluating on VQA val set:  59%|#####9    | 1584/2671 [18:50<13:07,  1.38it/s]Evaluating on VQA val set:  59%|#####9    | 1585/2671 [18:51<12:43,  1.42it/s]Evaluating on VQA val set:  59%|#####9    | 1586/2671 [18:52<12:40,  1.43it/s]Evaluating on VQA val set:  59%|#####9    | 1587/2671 [18:52<12:46,  1.41it/s]Evaluating on VQA val set:  59%|#####9    | 1588/2671 [18:53<12:47,  1.41it/s]Evaluating on VQA val set:  59%|#####9    | 1589/2671 [18:54<12:24,  1.45it/s]Evaluating on VQA val set:  60%|#####9    | 1590/2671 [18:55<12:24,  1.45it/s]Evaluating on VQA val set:  60%|#####9    | 1591/2671 [18:55<12:23,  1.45it/s]Evaluating on VQA val set:  60%|#####9    | 1592/2671 [18:56<12:38,  1.42it/s]Evaluating on VQA val set:  60%|#####9    | 1593/2671 [18:57<12:52,  1.40it/s]Evaluating on VQA val set:  60%|#####9    | 1594/2671 [18:57<12:36,  1.42it/s]Evaluating on VQA val set:  60%|#####9    | 1595/2671 [18:58<12:12,  1.47it/s]Evaluating on VQA val set:  60%|#####9    | 1596/2671 [18:59<11:56,  1.50it/s]Evaluating on VQA val set:  60%|#####9    | 1597/2671 [18:59<12:07,  1.48it/s]Evaluating on VQA val set:  60%|#####9    | 1598/2671 [19:00<12:23,  1.44it/s]Evaluating on VQA val set:  60%|#####9    | 1599/2671 [19:01<12:45,  1.40it/s]Evaluating on VQA val set:  60%|#####9    | 1600/2671 [19:02<13:15,  1.35it/s]Evaluating on VQA val set:  60%|#####9    | 1601/2671 [19:02<12:59,  1.37it/s]Evaluating on VQA val set:  60%|#####9    | 1602/2671 [19:03<12:03,  1.48it/s]Evaluating on VQA val set:  60%|######    | 1603/2671 [19:04<12:25,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1604/2671 [19:04<12:25,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1605/2671 [19:05<12:23,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1606/2671 [19:06<12:22,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1607/2671 [19:06<12:27,  1.42it/s]Evaluating on VQA val set:  60%|######    | 1608/2671 [19:07<12:24,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1609/2671 [19:08<12:18,  1.44it/s]Evaluating on VQA val set:  60%|######    | 1610/2671 [19:09<12:51,  1.38it/s]Evaluating on VQA val set:  60%|######    | 1611/2671 [19:09<13:08,  1.34it/s]Evaluating on VQA val set:  60%|######    | 1612/2671 [19:10<12:47,  1.38it/s]Evaluating on VQA val set:  60%|######    | 1613/2671 [19:11<12:55,  1.36it/s]Evaluating on VQA val set:  60%|######    | 1614/2671 [19:12<12:39,  1.39it/s]Evaluating on VQA val set:  60%|######    | 1615/2671 [19:12<12:42,  1.38it/s]Evaluating on VQA val set:  61%|######    | 1616/2671 [19:13<13:01,  1.35it/s]Evaluating on VQA val set:  61%|######    | 1617/2671 [19:14<12:49,  1.37it/s]Evaluating on VQA val set:  61%|######    | 1618/2671 [19:14<12:32,  1.40it/s]Evaluating on VQA val set:  61%|######    | 1619/2671 [19:15<11:49,  1.48it/s]Evaluating on VQA val set:  61%|######    | 1620/2671 [19:16<12:16,  1.43it/s]Evaluating on VQA val set:  61%|######    | 1621/2671 [19:17<12:27,  1.41it/s]Evaluating on VQA val set:  61%|######    | 1622/2671 [19:17<12:41,  1.38it/s]Evaluating on VQA val set:  61%|######    | 1623/2671 [19:18<12:45,  1.37it/s]Evaluating on VQA val set:  61%|######    | 1624/2671 [19:19<13:03,  1.34it/s]Evaluating on VQA val set:  61%|######    | 1625/2671 [19:20<12:53,  1.35it/s]Evaluating on VQA val set:  61%|######    | 1626/2671 [19:20<12:48,  1.36it/s]Evaluating on VQA val set:  61%|######    | 1627/2671 [19:21<12:26,  1.40it/s]Evaluating on VQA val set:  61%|######    | 1628/2671 [19:22<12:11,  1.43it/s]Evaluating on VQA val set:  61%|######    | 1629/2671 [19:22<12:39,  1.37it/s]Evaluating on VQA val set:  61%|######1   | 1630/2671 [19:23<12:31,  1.39it/s]Evaluating on VQA val set:  61%|######1   | 1631/2671 [19:24<12:15,  1.41it/s]Evaluating on VQA val set:  61%|######1   | 1632/2671 [19:25<12:27,  1.39it/s]Evaluating on VQA val set:  61%|######1   | 1633/2671 [19:25<12:32,  1.38it/s]Evaluating on VQA val set:  61%|######1   | 1634/2671 [19:26<12:31,  1.38it/s]Evaluating on VQA val set:  61%|######1   | 1635/2671 [19:27<12:05,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1636/2671 [19:27<12:02,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1637/2671 [19:28<11:48,  1.46it/s]Evaluating on VQA val set:  61%|######1   | 1638/2671 [19:29<12:01,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1639/2671 [19:29<12:08,  1.42it/s]Evaluating on VQA val set:  61%|######1   | 1640/2671 [19:30<12:02,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1641/2671 [19:31<12:12,  1.41it/s]Evaluating on VQA val set:  61%|######1   | 1642/2671 [19:31<11:33,  1.48it/s]Evaluating on VQA val set:  62%|######1   | 1643/2671 [19:32<11:48,  1.45it/s]Evaluating on VQA val set:  62%|######1   | 1644/2671 [19:33<11:23,  1.50it/s]Evaluating on VQA val set:  62%|######1   | 1645/2671 [19:33<11:20,  1.51it/s]Evaluating on VQA val set:  62%|######1   | 1646/2671 [19:34<11:29,  1.49it/s]Evaluating on VQA val set:  62%|######1   | 1647/2671 [19:35<10:56,  1.56it/s]Evaluating on VQA val set:  62%|######1   | 1648/2671 [19:35<11:24,  1.50it/s]Evaluating on VQA val set:  62%|######1   | 1649/2671 [19:36<11:33,  1.47it/s]Evaluating on VQA val set:  62%|######1   | 1650/2671 [19:37<11:39,  1.46it/s]Evaluating on VQA val set:  62%|######1   | 1651/2671 [19:38<11:53,  1.43it/s]Evaluating on VQA val set:  62%|######1   | 1652/2671 [19:38<11:44,  1.45it/s]Evaluating on VQA val set:  62%|######1   | 1653/2671 [19:39<11:38,  1.46it/s]Evaluating on VQA val set:  62%|######1   | 1654/2671 [19:40<11:40,  1.45it/s]Evaluating on VQA val set:  62%|######1   | 1655/2671 [19:40<11:50,  1.43it/s]Evaluating on VQA val set:  62%|######1   | 1656/2671 [19:41<11:57,  1.41it/s]Evaluating on VQA val set:  62%|######2   | 1657/2671 [19:42<11:55,  1.42it/s]Evaluating on VQA val set:  62%|######2   | 1658/2671 [19:42<11:43,  1.44it/s]Evaluating on VQA val set:  62%|######2   | 1659/2671 [19:43<10:46,  1.56it/s]Evaluating on VQA val set:  62%|######2   | 1660/2671 [19:44<11:17,  1.49it/s]Evaluating on VQA val set:  62%|######2   | 1661/2671 [19:44<11:17,  1.49it/s]Evaluating on VQA val set:  62%|######2   | 1662/2671 [19:45<11:05,  1.52it/s]Evaluating on VQA val set:  62%|######2   | 1663/2671 [19:46<11:27,  1.47it/s]Evaluating on VQA val set:  62%|######2   | 1664/2671 [19:46<11:41,  1.44it/s]Evaluating on VQA val set:  62%|######2   | 1665/2671 [19:47<11:43,  1.43it/s]Evaluating on VQA val set:  62%|######2   | 1666/2671 [19:48<12:01,  1.39it/s]Evaluating on VQA val set:  62%|######2   | 1667/2671 [19:49<11:56,  1.40it/s]Evaluating on VQA val set:  62%|######2   | 1668/2671 [19:49<11:53,  1.41it/s]Evaluating on VQA val set:  62%|######2   | 1669/2671 [19:50<11:57,  1.40it/s]Evaluating on VQA val set:  63%|######2   | 1670/2671 [19:51<11:58,  1.39it/s]Evaluating on VQA val set:  63%|######2   | 1671/2671 [19:51<11:44,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1672/2671 [19:52<11:35,  1.44it/s]Evaluating on VQA val set:  63%|######2   | 1673/2671 [19:53<11:38,  1.43it/s]Evaluating on VQA val set:  63%|######2   | 1674/2671 [19:54<11:55,  1.39it/s]Evaluating on VQA val set:  63%|######2   | 1675/2671 [19:54<11:53,  1.40it/s]Evaluating on VQA val set:  63%|######2   | 1676/2671 [19:55<11:42,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1677/2671 [19:56<11:44,  1.41it/s]Evaluating on VQA val set:  63%|######2   | 1678/2671 [19:56<11:48,  1.40it/s]Evaluating on VQA val set:  63%|######2   | 1679/2671 [19:57<11:40,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1680/2671 [19:58<11:36,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1681/2671 [19:58<11:15,  1.46it/s]Evaluating on VQA val set:  63%|######2   | 1682/2671 [19:59<11:30,  1.43it/s]Evaluating on VQA val set:  63%|######3   | 1683/2671 [20:00<11:23,  1.45it/s]Evaluating on VQA val set:  63%|######3   | 1684/2671 [20:01<11:36,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1685/2671 [20:01<11:35,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1686/2671 [20:02<11:37,  1.41it/s]Evaluating on VQA val set:  63%|######3   | 1687/2671 [20:03<11:17,  1.45it/s]Evaluating on VQA val set:  63%|######3   | 1688/2671 [20:03<11:24,  1.44it/s]Evaluating on VQA val set:  63%|######3   | 1689/2671 [20:04<11:44,  1.39it/s]Evaluating on VQA val set:  63%|######3   | 1690/2671 [20:05<11:40,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1691/2671 [20:06<11:45,  1.39it/s]Evaluating on VQA val set:  63%|######3   | 1692/2671 [20:06<11:37,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1693/2671 [20:07<11:40,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1694/2671 [20:08<11:54,  1.37it/s]Evaluating on VQA val set:  63%|######3   | 1695/2671 [20:08<11:38,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1696/2671 [20:09<11:42,  1.39it/s]Evaluating on VQA val set:  64%|######3   | 1697/2671 [20:10<11:28,  1.42it/s]Evaluating on VQA val set:  64%|######3   | 1698/2671 [20:11<11:34,  1.40it/s]Evaluating on VQA val set:  64%|######3   | 1699/2671 [20:11<11:44,  1.38it/s]Evaluating on VQA val set:  64%|######3   | 1700/2671 [20:12<11:55,  1.36it/s]Evaluating on VQA val set:  64%|######3   | 1701/2671 [20:13<11:41,  1.38it/s]Evaluating on VQA val set:  64%|######3   | 1702/2671 [20:14<11:44,  1.37it/s]Evaluating on VQA val set:  64%|######3   | 1703/2671 [20:14<11:34,  1.39it/s]Evaluating on VQA val set:  64%|######3   | 1704/2671 [20:15<11:34,  1.39it/s]Evaluating on VQA val set:  64%|######3   | 1705/2671 [20:16<11:16,  1.43it/s]Evaluating on VQA val set:  64%|######3   | 1706/2671 [20:16<11:33,  1.39it/s]Evaluating on VQA val set:  64%|######3   | 1707/2671 [20:17<11:26,  1.40it/s]Evaluating on VQA val set:  64%|######3   | 1708/2671 [20:18<11:23,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1709/2671 [20:18<11:36,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1710/2671 [20:19<11:45,  1.36it/s]Evaluating on VQA val set:  64%|######4   | 1711/2671 [20:20<11:54,  1.34it/s]Evaluating on VQA val set:  64%|######4   | 1712/2671 [20:21<11:36,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1713/2671 [20:21<11:34,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1714/2671 [20:22<11:42,  1.36it/s]Evaluating on VQA val set:  64%|######4   | 1715/2671 [20:23<11:46,  1.35it/s]Evaluating on VQA val set:  64%|######4   | 1716/2671 [20:24<11:34,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1717/2671 [20:24<11:27,  1.39it/s]Evaluating on VQA val set:  64%|######4   | 1718/2671 [20:25<11:26,  1.39it/s]Evaluating on VQA val set:  64%|######4   | 1719/2671 [20:26<11:33,  1.37it/s]Evaluating on VQA val set:  64%|######4   | 1720/2671 [20:26<11:02,  1.43it/s]Evaluating on VQA val set:  64%|######4   | 1721/2671 [20:27<11:26,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1722/2671 [20:28<11:36,  1.36it/s]Evaluating on VQA val set:  65%|######4   | 1723/2671 [20:29<10:48,  1.46it/s]Evaluating on VQA val set:  65%|######4   | 1724/2671 [20:29<10:29,  1.50it/s]Evaluating on VQA val set:  65%|######4   | 1725/2671 [20:30<10:46,  1.46it/s]Evaluating on VQA val set:  65%|######4   | 1726/2671 [20:31<10:59,  1.43it/s]Evaluating on VQA val set:  65%|######4   | 1727/2671 [20:31<10:58,  1.43it/s]Evaluating on VQA val set:  65%|######4   | 1728/2671 [20:32<10:58,  1.43it/s]Evaluating on VQA val set:  65%|######4   | 1729/2671 [20:33<11:01,  1.42it/s]Evaluating on VQA val set:  65%|######4   | 1730/2671 [20:33<10:53,  1.44it/s]Evaluating on VQA val set:  65%|######4   | 1731/2671 [20:34<10:54,  1.44it/s]Evaluating on VQA val set:  65%|######4   | 1732/2671 [20:35<10:57,  1.43it/s]Evaluating on VQA val set:  65%|######4   | 1733/2671 [20:35<10:49,  1.44it/s]Evaluating on VQA val set:  65%|######4   | 1734/2671 [20:36<11:01,  1.42it/s]Evaluating on VQA val set:  65%|######4   | 1735/2671 [20:37<11:19,  1.38it/s]Evaluating on VQA val set:  65%|######4   | 1736/2671 [20:38<11:08,  1.40it/s]Evaluating on VQA val set:  65%|######5   | 1737/2671 [20:38<10:59,  1.42it/s]Evaluating on VQA val set:  65%|######5   | 1738/2671 [20:39<11:02,  1.41it/s]Evaluating on VQA val set:  65%|######5   | 1739/2671 [20:40<11:01,  1.41it/s]Evaluating on VQA val set:  65%|######5   | 1740/2671 [20:40<10:56,  1.42it/s]Evaluating on VQA val set:  65%|######5   | 1741/2671 [20:41<10:50,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1742/2671 [20:42<11:05,  1.40it/s]Evaluating on VQA val set:  65%|######5   | 1743/2671 [20:43<10:46,  1.44it/s]Evaluating on VQA val set:  65%|######5   | 1744/2671 [20:43<09:42,  1.59it/s]Evaluating on VQA val set:  65%|######5   | 1745/2671 [20:44<09:56,  1.55it/s]Evaluating on VQA val set:  65%|######5   | 1746/2671 [20:45<10:31,  1.46it/s]Evaluating on VQA val set:  65%|######5   | 1747/2671 [20:45<10:38,  1.45it/s]Evaluating on VQA val set:  65%|######5   | 1748/2671 [20:46<10:46,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1749/2671 [20:47<10:28,  1.47it/s]Evaluating on VQA val set:  66%|######5   | 1750/2671 [20:47<10:22,  1.48it/s]Evaluating on VQA val set:  66%|######5   | 1751/2671 [20:48<10:25,  1.47it/s]Evaluating on VQA val set:  66%|######5   | 1752/2671 [20:49<10:02,  1.52it/s]Evaluating on VQA val set:  66%|######5   | 1753/2671 [20:49<10:16,  1.49it/s]Evaluating on VQA val set:  66%|######5   | 1754/2671 [20:50<10:19,  1.48it/s]Evaluating on VQA val set:  66%|######5   | 1755/2671 [20:51<10:28,  1.46it/s]Evaluating on VQA val set:  66%|######5   | 1756/2671 [20:51<10:31,  1.45it/s]Evaluating on VQA val set:  66%|######5   | 1757/2671 [20:52<10:53,  1.40it/s]Evaluating on VQA val set:  66%|######5   | 1758/2671 [20:53<10:47,  1.41it/s]Evaluating on VQA val set:  66%|######5   | 1759/2671 [20:54<10:46,  1.41it/s]Evaluating on VQA val set:  66%|######5   | 1760/2671 [20:54<10:56,  1.39it/s]Evaluating on VQA val set:  66%|######5   | 1761/2671 [20:55<11:01,  1.38it/s]Evaluating on VQA val set:  66%|######5   | 1762/2671 [20:56<11:04,  1.37it/s]Evaluating on VQA val set:  66%|######6   | 1763/2671 [20:56<10:21,  1.46it/s]Evaluating on VQA val set:  66%|######6   | 1764/2671 [20:57<10:17,  1.47it/s]Evaluating on VQA val set:  66%|######6   | 1765/2671 [20:58<10:26,  1.45it/s]Evaluating on VQA val set:  66%|######6   | 1766/2671 [20:58<10:25,  1.45it/s]Evaluating on VQA val set:  66%|######6   | 1767/2671 [20:59<10:33,  1.43it/s]Evaluating on VQA val set:  66%|######6   | 1768/2671 [21:00<10:41,  1.41it/s]Evaluating on VQA val set:  66%|######6   | 1769/2671 [21:01<10:47,  1.39it/s]Evaluating on VQA val set:  66%|######6   | 1770/2671 [21:01<10:44,  1.40it/s]Evaluating on VQA val set:  66%|######6   | 1771/2671 [21:02<10:54,  1.38it/s]Evaluating on VQA val set:  66%|######6   | 1772/2671 [21:03<11:06,  1.35it/s]Evaluating on VQA val set:  66%|######6   | 1773/2671 [21:04<10:58,  1.36it/s]Evaluating on VQA val set:  66%|######6   | 1774/2671 [21:04<10:50,  1.38it/s]Evaluating on VQA val set:  66%|######6   | 1775/2671 [21:05<10:39,  1.40it/s]Evaluating on VQA val set:  66%|######6   | 1776/2671 [21:06<10:38,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1777/2671 [21:06<10:30,  1.42it/s]Evaluating on VQA val set:  67%|######6   | 1778/2671 [21:07<10:27,  1.42it/s]Evaluating on VQA val set:  67%|######6   | 1779/2671 [21:08<10:40,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1780/2671 [21:09<10:41,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1781/2671 [21:09<10:44,  1.38it/s]Evaluating on VQA val set:  67%|######6   | 1782/2671 [21:10<10:26,  1.42it/s]Evaluating on VQA val set:  67%|######6   | 1783/2671 [21:11<10:39,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1784/2671 [21:11<10:38,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1785/2671 [21:12<10:34,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1786/2671 [21:13<10:44,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1787/2671 [21:14<10:44,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1788/2671 [21:14<10:45,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1789/2671 [21:15<10:58,  1.34it/s]Evaluating on VQA val set:  67%|######7   | 1790/2671 [21:16<10:38,  1.38it/s]Evaluating on VQA val set:  67%|######7   | 1791/2671 [21:16<10:25,  1.41it/s]Evaluating on VQA val set:  67%|######7   | 1792/2671 [21:17<10:26,  1.40it/s]Evaluating on VQA val set:  67%|######7   | 1793/2671 [21:18<10:28,  1.40it/s]Evaluating on VQA val set:  67%|######7   | 1794/2671 [21:19<10:06,  1.45it/s]Evaluating on VQA val set:  67%|######7   | 1795/2671 [21:19<10:17,  1.42it/s]Evaluating on VQA val set:  67%|######7   | 1796/2671 [21:20<10:11,  1.43it/s]Evaluating on VQA val set:  67%|######7   | 1797/2671 [21:21<10:17,  1.42it/s]Evaluating on VQA val set:  67%|######7   | 1798/2671 [21:21<10:33,  1.38it/s]Evaluating on VQA val set:  67%|######7   | 1799/2671 [21:22<10:36,  1.37it/s]Evaluating on VQA val set:  67%|######7   | 1800/2671 [21:23<10:32,  1.38it/s]Evaluating on VQA val set:  67%|######7   | 1801/2671 [21:24<10:18,  1.41it/s]Evaluating on VQA val set:  67%|######7   | 1802/2671 [21:24<09:49,  1.47it/s]Evaluating on VQA val set:  68%|######7   | 1803/2671 [21:25<09:45,  1.48it/s]Evaluating on VQA val set:  68%|######7   | 1804/2671 [21:25<09:31,  1.52it/s]Evaluating on VQA val set:  68%|######7   | 1805/2671 [21:26<09:44,  1.48it/s]Evaluating on VQA val set:  68%|######7   | 1806/2671 [21:27<09:59,  1.44it/s]Evaluating on VQA val set:  68%|######7   | 1807/2671 [21:28<10:13,  1.41it/s]Evaluating on VQA val set:  68%|######7   | 1808/2671 [21:28<10:08,  1.42it/s]Evaluating on VQA val set:  68%|######7   | 1809/2671 [21:29<10:15,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1810/2671 [21:30<10:22,  1.38it/s]Evaluating on VQA val set:  68%|######7   | 1811/2671 [21:31<10:15,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1812/2671 [21:31<10:21,  1.38it/s]Evaluating on VQA val set:  68%|######7   | 1813/2671 [21:32<10:13,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1814/2671 [21:33<10:13,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1815/2671 [21:33<10:11,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1816/2671 [21:34<10:01,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1817/2671 [21:35<10:00,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1818/2671 [21:35<10:04,  1.41it/s]Evaluating on VQA val set:  68%|######8   | 1819/2671 [21:36<10:08,  1.40it/s]Evaluating on VQA val set:  68%|######8   | 1820/2671 [21:37<10:08,  1.40it/s]Evaluating on VQA val set:  68%|######8   | 1821/2671 [21:38<09:52,  1.44it/s]Evaluating on VQA val set:  68%|######8   | 1822/2671 [21:38<09:56,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1823/2671 [21:39<09:56,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1824/2671 [21:40<09:58,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1825/2671 [21:40<09:55,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1826/2671 [21:41<09:59,  1.41it/s]Evaluating on VQA val set:  68%|######8   | 1827/2671 [21:42<09:55,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1828/2671 [21:43<10:07,  1.39it/s]Evaluating on VQA val set:  68%|######8   | 1829/2671 [21:43<09:56,  1.41it/s]Evaluating on VQA val set:  69%|######8   | 1830/2671 [21:44<09:53,  1.42it/s]Evaluating on VQA val set:  69%|######8   | 1831/2671 [21:45<09:47,  1.43it/s]Evaluating on VQA val set:  69%|######8   | 1832/2671 [21:45<09:53,  1.41it/s]Evaluating on VQA val set:  69%|######8   | 1833/2671 [21:46<09:59,  1.40it/s]Evaluating on VQA val set:  69%|######8   | 1834/2671 [21:47<10:22,  1.34it/s]Evaluating on VQA val set:  69%|######8   | 1835/2671 [21:48<10:18,  1.35it/s]Evaluating on VQA val set:  69%|######8   | 1836/2671 [21:48<10:17,  1.35it/s]Evaluating on VQA val set:  69%|######8   | 1837/2671 [21:49<10:18,  1.35it/s]Evaluating on VQA val set:  69%|######8   | 1838/2671 [21:50<10:20,  1.34it/s]Evaluating on VQA val set:  69%|######8   | 1839/2671 [21:51<10:20,  1.34it/s]Evaluating on VQA val set:  69%|######8   | 1840/2671 [21:51<10:07,  1.37it/s]Evaluating on VQA val set:  69%|######8   | 1841/2671 [21:52<10:09,  1.36it/s]Evaluating on VQA val set:  69%|######8   | 1842/2671 [21:53<09:47,  1.41it/s]Evaluating on VQA val set:  69%|######9   | 1843/2671 [21:54<10:03,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1844/2671 [21:54<10:15,  1.34it/s]Evaluating on VQA val set:  69%|######9   | 1845/2671 [21:55<09:33,  1.44it/s]Evaluating on VQA val set:  69%|######9   | 1846/2671 [21:55<09:05,  1.51it/s]Evaluating on VQA val set:  69%|######9   | 1847/2671 [21:56<09:17,  1.48it/s]Evaluating on VQA val set:  69%|######9   | 1848/2671 [21:57<09:35,  1.43it/s]Evaluating on VQA val set:  69%|######9   | 1849/2671 [21:58<09:26,  1.45it/s]Evaluating on VQA val set:  69%|######9   | 1850/2671 [21:58<09:31,  1.44it/s]Evaluating on VQA val set:  69%|######9   | 1851/2671 [21:59<09:33,  1.43it/s]Evaluating on VQA val set:  69%|######9   | 1852/2671 [22:00<09:51,  1.38it/s]Evaluating on VQA val set:  69%|######9   | 1853/2671 [22:01<09:56,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1854/2671 [22:01<09:57,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1855/2671 [22:02<09:58,  1.36it/s]Evaluating on VQA val set:  69%|######9   | 1856/2671 [22:03<10:01,  1.35it/s]Evaluating on VQA val set:  70%|######9   | 1857/2671 [22:03<09:49,  1.38it/s]Evaluating on VQA val set:  70%|######9   | 1858/2671 [22:04<09:31,  1.42it/s]Evaluating on VQA val set:  70%|######9   | 1859/2671 [22:05<09:31,  1.42it/s]Evaluating on VQA val set:  70%|######9   | 1860/2671 [22:05<09:00,  1.50it/s]Evaluating on VQA val set:  70%|######9   | 1861/2671 [22:06<09:13,  1.46it/s]Evaluating on VQA val set:  70%|######9   | 1862/2671 [22:07<09:07,  1.48it/s]Evaluating on VQA val set:  70%|######9   | 1863/2671 [22:07<09:14,  1.46it/s]Evaluating on VQA val set:  70%|######9   | 1864/2671 [22:08<09:09,  1.47it/s]Evaluating on VQA val set:  70%|######9   | 1865/2671 [22:09<09:25,  1.43it/s]Evaluating on VQA val set:  70%|######9   | 1866/2671 [22:10<09:34,  1.40it/s]Evaluating on VQA val set:  70%|######9   | 1867/2671 [22:10<09:33,  1.40it/s]Evaluating on VQA val set:  70%|######9   | 1868/2671 [22:11<09:36,  1.39it/s]Evaluating on VQA val set:  70%|######9   | 1869/2671 [22:12<09:33,  1.40it/s]Evaluating on VQA val set:  70%|#######   | 1870/2671 [22:12<09:31,  1.40it/s]Evaluating on VQA val set:  70%|#######   | 1871/2671 [22:13<09:28,  1.41it/s]Evaluating on VQA val set:  70%|#######   | 1872/2671 [22:14<09:22,  1.42it/s]Evaluating on VQA val set:  70%|#######   | 1873/2671 [22:15<09:19,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1874/2671 [22:15<09:39,  1.38it/s]Evaluating on VQA val set:  70%|#######   | 1875/2671 [22:16<09:07,  1.45it/s]Evaluating on VQA val set:  70%|#######   | 1876/2671 [22:17<08:55,  1.48it/s]Evaluating on VQA val set:  70%|#######   | 1877/2671 [22:17<08:54,  1.49it/s]Evaluating on VQA val set:  70%|#######   | 1878/2671 [22:18<08:54,  1.48it/s]Evaluating on VQA val set:  70%|#######   | 1879/2671 [22:19<09:08,  1.44it/s]Evaluating on VQA val set:  70%|#######   | 1880/2671 [22:19<09:11,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1881/2671 [22:20<09:11,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1882/2671 [22:21<09:28,  1.39it/s]Evaluating on VQA val set:  70%|#######   | 1883/2671 [22:22<09:28,  1.39it/s]Evaluating on VQA val set:  71%|#######   | 1884/2671 [22:22<09:07,  1.44it/s]Evaluating on VQA val set:  71%|#######   | 1885/2671 [22:23<09:22,  1.40it/s]Evaluating on VQA val set:  71%|#######   | 1886/2671 [22:24<09:38,  1.36it/s]Evaluating on VQA val set:  71%|#######   | 1887/2671 [22:25<09:37,  1.36it/s]Evaluating on VQA val set:  71%|#######   | 1888/2671 [22:25<09:38,  1.35it/s]Evaluating on VQA val set:  71%|#######   | 1889/2671 [22:26<09:18,  1.40it/s]Evaluating on VQA val set:  71%|#######   | 1890/2671 [22:27<09:04,  1.43it/s]Evaluating on VQA val set:  71%|#######   | 1891/2671 [22:27<09:11,  1.41it/s]Evaluating on VQA val set:  71%|#######   | 1892/2671 [22:28<09:14,  1.41it/s]Evaluating on VQA val set:  71%|#######   | 1893/2671 [22:29<09:26,  1.37it/s]Evaluating on VQA val set:  71%|#######   | 1894/2671 [22:30<09:32,  1.36it/s]Evaluating on VQA val set:  71%|#######   | 1895/2671 [22:30<09:25,  1.37it/s]Evaluating on VQA val set:  71%|#######   | 1896/2671 [22:31<09:19,  1.39it/s]Evaluating on VQA val set:  71%|#######1  | 1897/2671 [22:32<09:28,  1.36it/s]Evaluating on VQA val set:  71%|#######1  | 1898/2671 [22:32<09:31,  1.35it/s]Evaluating on VQA val set:  71%|#######1  | 1899/2671 [22:33<09:21,  1.37it/s]Evaluating on VQA val set:  71%|#######1  | 1900/2671 [22:34<09:35,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1901/2671 [22:35<09:39,  1.33it/s]Evaluating on VQA val set:  71%|#######1  | 1902/2671 [22:35<09:33,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1903/2671 [22:36<09:34,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1904/2671 [22:37<09:30,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1905/2671 [22:38<09:32,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1906/2671 [22:38<09:24,  1.36it/s]Evaluating on VQA val set:  71%|#######1  | 1907/2671 [22:39<09:04,  1.40it/s]Evaluating on VQA val set:  71%|#######1  | 1908/2671 [22:40<09:03,  1.40it/s]Evaluating on VQA val set:  71%|#######1  | 1909/2671 [22:40<08:57,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1910/2671 [22:41<09:01,  1.41it/s]Evaluating on VQA val set:  72%|#######1  | 1911/2671 [22:42<09:20,  1.36it/s]Evaluating on VQA val set:  72%|#######1  | 1912/2671 [22:43<08:58,  1.41it/s]Evaluating on VQA val set:  72%|#######1  | 1913/2671 [22:43<08:52,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1914/2671 [22:44<08:53,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1915/2671 [22:45<08:51,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1916/2671 [22:45<08:59,  1.40it/s]Evaluating on VQA val set:  72%|#######1  | 1917/2671 [22:46<09:13,  1.36it/s]Evaluating on VQA val set:  72%|#######1  | 1918/2671 [22:47<08:55,  1.41it/s]Evaluating on VQA val set:  72%|#######1  | 1919/2671 [22:48<08:49,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1920/2671 [22:48<08:41,  1.44it/s]Evaluating on VQA val set:  72%|#######1  | 1921/2671 [22:49<08:58,  1.39it/s]Evaluating on VQA val set:  72%|#######1  | 1922/2671 [22:50<09:11,  1.36it/s]Evaluating on VQA val set:  72%|#######1  | 1923/2671 [22:51<09:10,  1.36it/s]Evaluating on VQA val set:  72%|#######2  | 1924/2671 [22:51<08:52,  1.40it/s]Evaluating on VQA val set:  72%|#######2  | 1925/2671 [22:52<09:03,  1.37it/s]Evaluating on VQA val set:  72%|#######2  | 1926/2671 [22:53<09:09,  1.36it/s]Evaluating on VQA val set:  72%|#######2  | 1927/2671 [22:53<09:04,  1.37it/s]Evaluating on VQA val set:  72%|#######2  | 1928/2671 [22:54<08:59,  1.38it/s]Evaluating on VQA val set:  72%|#######2  | 1929/2671 [22:55<09:16,  1.33it/s]Evaluating on VQA val set:  72%|#######2  | 1930/2671 [22:56<09:12,  1.34it/s]Evaluating on VQA val set:  72%|#######2  | 1931/2671 [22:56<09:11,  1.34it/s]Evaluating on VQA val set:  72%|#######2  | 1932/2671 [22:57<09:19,  1.32it/s]Evaluating on VQA val set:  72%|#######2  | 1933/2671 [22:58<09:15,  1.33it/s]Evaluating on VQA val set:  72%|#######2  | 1934/2671 [22:59<09:19,  1.32it/s]Evaluating on VQA val set:  72%|#######2  | 1935/2671 [23:00<09:23,  1.31it/s]Evaluating on VQA val set:  72%|#######2  | 1936/2671 [23:00<09:06,  1.35it/s]Evaluating on VQA val set:  73%|#######2  | 1937/2671 [23:01<09:03,  1.35it/s]Evaluating on VQA val set:  73%|#######2  | 1938/2671 [23:02<08:58,  1.36it/s]Evaluating on VQA val set:  73%|#######2  | 1939/2671 [23:02<09:02,  1.35it/s]Evaluating on VQA val set:  73%|#######2  | 1940/2671 [23:03<09:07,  1.34it/s]Evaluating on VQA val set:  73%|#######2  | 1941/2671 [23:04<08:46,  1.39it/s]Evaluating on VQA val set:  73%|#######2  | 1942/2671 [23:05<08:49,  1.38it/s]Evaluating on VQA val set:  73%|#######2  | 1943/2671 [23:05<08:50,  1.37it/s]Evaluating on VQA val set:  73%|#######2  | 1944/2671 [23:06<08:54,  1.36it/s]Evaluating on VQA val set:  73%|#######2  | 1945/2671 [23:07<08:55,  1.36it/s]Evaluating on VQA val set:  73%|#######2  | 1946/2671 [23:08<08:48,  1.37it/s]Evaluating on VQA val set:  73%|#######2  | 1947/2671 [23:08<08:29,  1.42it/s]Evaluating on VQA val set:  73%|#######2  | 1948/2671 [23:09<08:37,  1.40it/s]Evaluating on VQA val set:  73%|#######2  | 1949/2671 [23:10<08:44,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1950/2671 [23:10<08:43,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1951/2671 [23:11<08:49,  1.36it/s]Evaluating on VQA val set:  73%|#######3  | 1952/2671 [23:12<08:48,  1.36it/s]Evaluating on VQA val set:  73%|#######3  | 1953/2671 [23:13<09:00,  1.33it/s]Evaluating on VQA val set:  73%|#######3  | 1954/2671 [23:13<09:03,  1.32it/s]Evaluating on VQA val set:  73%|#######3  | 1955/2671 [23:14<08:49,  1.35it/s]Evaluating on VQA val set:  73%|#######3  | 1956/2671 [23:15<08:41,  1.37it/s]Evaluating on VQA val set:  73%|#######3  | 1957/2671 [23:16<08:36,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1958/2671 [23:16<08:25,  1.41it/s]Evaluating on VQA val set:  73%|#######3  | 1959/2671 [23:17<08:32,  1.39it/s]Evaluating on VQA val set:  73%|#######3  | 1960/2671 [23:18<08:33,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1961/2671 [23:18<08:22,  1.41it/s]Evaluating on VQA val set:  73%|#######3  | 1962/2671 [23:19<08:26,  1.40it/s]Evaluating on VQA val set:  73%|#######3  | 1963/2671 [23:20<08:38,  1.37it/s]Evaluating on VQA val set:  74%|#######3  | 1964/2671 [23:21<08:22,  1.41it/s]Evaluating on VQA val set:  74%|#######3  | 1965/2671 [23:21<08:21,  1.41it/s]Evaluating on VQA val set:  74%|#######3  | 1966/2671 [23:22<08:19,  1.41it/s]Evaluating on VQA val set:  74%|#######3  | 1967/2671 [23:23<08:24,  1.40it/s]Evaluating on VQA val set:  74%|#######3  | 1968/2671 [23:23<08:33,  1.37it/s]Evaluating on VQA val set:  74%|#######3  | 1969/2671 [23:24<08:43,  1.34it/s]Evaluating on VQA val set:  74%|#######3  | 1970/2671 [23:25<08:30,  1.37it/s]Evaluating on VQA val set:  74%|#######3  | 1971/2671 [23:26<08:28,  1.38it/s]Evaluating on VQA val set:  74%|#######3  | 1972/2671 [23:26<08:29,  1.37it/s]Evaluating on VQA val set:  74%|#######3  | 1973/2671 [23:27<08:26,  1.38it/s]Evaluating on VQA val set:  74%|#######3  | 1974/2671 [23:28<08:19,  1.40it/s]Evaluating on VQA val set:  74%|#######3  | 1975/2671 [23:29<08:24,  1.38it/s]Evaluating on VQA val set:  74%|#######3  | 1976/2671 [23:29<08:23,  1.38it/s]Evaluating on VQA val set:  74%|#######4  | 1977/2671 [23:30<08:07,  1.42it/s]Evaluating on VQA val set:  74%|#######4  | 1978/2671 [23:31<08:08,  1.42it/s]Evaluating on VQA val set:  74%|#######4  | 1979/2671 [23:31<08:21,  1.38it/s]Evaluating on VQA val set:  74%|#######4  | 1980/2671 [23:32<08:22,  1.37it/s]Evaluating on VQA val set:  74%|#######4  | 1981/2671 [23:33<08:19,  1.38it/s]Evaluating on VQA val set:  74%|#######4  | 1982/2671 [23:34<08:13,  1.40it/s]Evaluating on VQA val set:  74%|#######4  | 1983/2671 [23:34<08:31,  1.35it/s]Evaluating on VQA val set:  74%|#######4  | 1984/2671 [23:35<08:16,  1.38it/s]Evaluating on VQA val set:  74%|#######4  | 1985/2671 [23:36<08:15,  1.38it/s]Evaluating on VQA val set:  74%|#######4  | 1986/2671 [23:36<08:05,  1.41it/s]Evaluating on VQA val set:  74%|#######4  | 1987/2671 [23:37<07:37,  1.49it/s]Evaluating on VQA val set:  74%|#######4  | 1988/2671 [23:38<07:32,  1.51it/s]Evaluating on VQA val set:  74%|#######4  | 1989/2671 [23:38<07:50,  1.45it/s]Evaluating on VQA val set:  75%|#######4  | 1990/2671 [23:39<07:47,  1.46it/s]Evaluating on VQA val set:  75%|#######4  | 1991/2671 [23:40<07:43,  1.47it/s]Evaluating on VQA val set:  75%|#######4  | 1992/2671 [23:41<08:01,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 1993/2671 [23:41<07:56,  1.42it/s]Evaluating on VQA val set:  75%|#######4  | 1994/2671 [23:42<07:53,  1.43it/s]Evaluating on VQA val set:  75%|#######4  | 1995/2671 [23:43<07:54,  1.42it/s]Evaluating on VQA val set:  75%|#######4  | 1996/2671 [23:43<07:57,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 1997/2671 [23:44<07:57,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 1998/2671 [23:45<08:18,  1.35it/s]Evaluating on VQA val set:  75%|#######4  | 1999/2671 [23:46<08:17,  1.35it/s]Evaluating on VQA val set:  75%|#######4  | 2000/2671 [23:46<08:10,  1.37it/s]Evaluating on VQA val set:  75%|#######4  | 2001/2671 [23:47<07:56,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 2002/2671 [23:48<07:52,  1.42it/s]Evaluating on VQA val set:  75%|#######4  | 2003/2671 [23:48<07:50,  1.42it/s]Evaluating on VQA val set:  75%|#######5  | 2004/2671 [23:49<08:02,  1.38it/s]Evaluating on VQA val set:  75%|#######5  | 2005/2671 [23:50<08:02,  1.38it/s]Evaluating on VQA val set:  75%|#######5  | 2006/2671 [23:51<08:13,  1.35it/s]Evaluating on VQA val set:  75%|#######5  | 2007/2671 [23:51<08:14,  1.34it/s]Evaluating on VQA val set:  75%|#######5  | 2008/2671 [23:52<08:10,  1.35it/s]Evaluating on VQA val set:  75%|#######5  | 2009/2671 [23:53<08:02,  1.37it/s]Evaluating on VQA val set:  75%|#######5  | 2010/2671 [23:54<07:51,  1.40it/s]Evaluating on VQA val set:  75%|#######5  | 2011/2671 [23:54<07:39,  1.44it/s]Evaluating on VQA val set:  75%|#######5  | 2012/2671 [23:55<07:52,  1.39it/s]Evaluating on VQA val set:  75%|#######5  | 2013/2671 [23:56<07:53,  1.39it/s]Evaluating on VQA val set:  75%|#######5  | 2014/2671 [23:56<07:37,  1.44it/s]Evaluating on VQA val set:  75%|#######5  | 2015/2671 [23:57<07:34,  1.44it/s]Evaluating on VQA val set:  75%|#######5  | 2016/2671 [23:58<07:46,  1.41it/s]Evaluating on VQA val set:  76%|#######5  | 2017/2671 [23:58<07:47,  1.40it/s]Evaluating on VQA val set:  76%|#######5  | 2018/2671 [23:59<07:39,  1.42it/s]Evaluating on VQA val set:  76%|#######5  | 2019/2671 [24:00<07:34,  1.43it/s]Evaluating on VQA val set:  76%|#######5  | 2020/2671 [24:01<07:37,  1.42it/s]Evaluating on VQA val set:  76%|#######5  | 2021/2671 [24:01<07:34,  1.43it/s]Evaluating on VQA val set:  76%|#######5  | 2022/2671 [24:02<07:20,  1.47it/s]Evaluating on VQA val set:  76%|#######5  | 2023/2671 [24:03<07:25,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2024/2671 [24:03<07:41,  1.40it/s]Evaluating on VQA val set:  76%|#######5  | 2025/2671 [24:04<07:43,  1.39it/s]Evaluating on VQA val set:  76%|#######5  | 2026/2671 [24:05<07:47,  1.38it/s]Evaluating on VQA val set:  76%|#######5  | 2027/2671 [24:05<07:32,  1.42it/s]Evaluating on VQA val set:  76%|#######5  | 2028/2671 [24:06<07:24,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2029/2671 [24:07<07:23,  1.45it/s]Evaluating on VQA val set:  76%|#######6  | 2030/2671 [24:08<07:18,  1.46it/s]Evaluating on VQA val set:  76%|#######6  | 2031/2671 [24:08<07:14,  1.47it/s]Evaluating on VQA val set:  76%|#######6  | 2032/2671 [24:09<07:21,  1.45it/s]Evaluating on VQA val set:  76%|#######6  | 2033/2671 [24:10<07:22,  1.44it/s]Evaluating on VQA val set:  76%|#######6  | 2034/2671 [24:10<07:19,  1.45it/s]Evaluating on VQA val set:  76%|#######6  | 2035/2671 [24:11<07:09,  1.48it/s]Evaluating on VQA val set:  76%|#######6  | 2036/2671 [24:12<07:21,  1.44it/s]Evaluating on VQA val set:  76%|#######6  | 2037/2671 [24:12<07:07,  1.48it/s]Evaluating on VQA val set:  76%|#######6  | 2038/2671 [24:13<07:09,  1.47it/s]Evaluating on VQA val set:  76%|#######6  | 2039/2671 [24:14<07:29,  1.40it/s]Evaluating on VQA val set:  76%|#######6  | 2040/2671 [24:15<07:38,  1.38it/s]Evaluating on VQA val set:  76%|#######6  | 2041/2671 [24:15<07:18,  1.44it/s]Evaluating on VQA val set:  76%|#######6  | 2042/2671 [24:16<07:08,  1.47it/s]Evaluating on VQA val set:  76%|#######6  | 2043/2671 [24:17<07:14,  1.45it/s]Evaluating on VQA val set:  77%|#######6  | 2044/2671 [24:17<07:18,  1.43it/s]Evaluating on VQA val set:  77%|#######6  | 2045/2671 [24:18<07:30,  1.39it/s]Evaluating on VQA val set:  77%|#######6  | 2046/2671 [24:19<07:27,  1.40it/s]Evaluating on VQA val set:  77%|#######6  | 2047/2671 [24:19<07:18,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2048/2671 [24:20<07:20,  1.41it/s]Evaluating on VQA val set:  77%|#######6  | 2049/2671 [24:21<07:13,  1.43it/s]Evaluating on VQA val set:  77%|#######6  | 2050/2671 [24:21<07:01,  1.47it/s]Evaluating on VQA val set:  77%|#######6  | 2051/2671 [24:22<07:07,  1.45it/s]Evaluating on VQA val set:  77%|#######6  | 2052/2671 [24:23<07:20,  1.40it/s]Evaluating on VQA val set:  77%|#######6  | 2053/2671 [24:24<07:10,  1.44it/s]Evaluating on VQA val set:  77%|#######6  | 2054/2671 [24:24<07:21,  1.40it/s]Evaluating on VQA val set:  77%|#######6  | 2055/2671 [24:25<07:19,  1.40it/s]Evaluating on VQA val set:  77%|#######6  | 2056/2671 [24:26<07:17,  1.40it/s]Evaluating on VQA val set:  77%|#######7  | 2057/2671 [24:26<07:09,  1.43it/s]Evaluating on VQA val set:  77%|#######7  | 2058/2671 [24:27<06:55,  1.47it/s]Evaluating on VQA val set:  77%|#######7  | 2059/2671 [24:28<07:10,  1.42it/s]Evaluating on VQA val set:  77%|#######7  | 2060/2671 [24:28<07:00,  1.45it/s]Evaluating on VQA val set:  77%|#######7  | 2061/2671 [24:29<06:59,  1.45it/s]Evaluating on VQA val set:  77%|#######7  | 2062/2671 [24:30<07:12,  1.41it/s]Evaluating on VQA val set:  77%|#######7  | 2063/2671 [24:31<07:19,  1.38it/s]Evaluating on VQA val set:  77%|#######7  | 2064/2671 [24:31<07:13,  1.40it/s]Evaluating on VQA val set:  77%|#######7  | 2065/2671 [24:32<07:16,  1.39it/s]Evaluating on VQA val set:  77%|#######7  | 2066/2671 [24:33<07:21,  1.37it/s]Evaluating on VQA val set:  77%|#######7  | 2067/2671 [24:34<07:18,  1.38it/s]Evaluating on VQA val set:  77%|#######7  | 2068/2671 [24:34<07:29,  1.34it/s]Evaluating on VQA val set:  77%|#######7  | 2069/2671 [24:35<07:26,  1.35it/s]Evaluating on VQA val set:  77%|#######7  | 2070/2671 [24:36<07:16,  1.38it/s]Evaluating on VQA val set:  78%|#######7  | 2071/2671 [24:36<07:17,  1.37it/s]Evaluating on VQA val set:  78%|#######7  | 2072/2671 [24:37<07:15,  1.38it/s]Evaluating on VQA val set:  78%|#######7  | 2073/2671 [24:38<07:06,  1.40it/s]Evaluating on VQA val set:  78%|#######7  | 2074/2671 [24:39<07:13,  1.38it/s]Evaluating on VQA val set:  78%|#######7  | 2075/2671 [24:39<07:17,  1.36it/s]Evaluating on VQA val set:  78%|#######7  | 2076/2671 [24:40<07:07,  1.39it/s]Evaluating on VQA val set:  78%|#######7  | 2077/2671 [24:41<06:54,  1.43it/s]Evaluating on VQA val set:  78%|#######7  | 2078/2671 [24:41<06:55,  1.43it/s]Evaluating on VQA val set:  78%|#######7  | 2079/2671 [24:42<07:03,  1.40it/s]Evaluating on VQA val set:  78%|#######7  | 2080/2671 [24:43<07:12,  1.37it/s]Evaluating on VQA val set:  78%|#######7  | 2081/2671 [24:44<07:14,  1.36it/s]Evaluating on VQA val set:  78%|#######7  | 2082/2671 [24:44<07:06,  1.38it/s]Evaluating on VQA val set:  78%|#######7  | 2083/2671 [24:45<07:11,  1.36it/s]Evaluating on VQA val set:  78%|#######8  | 2084/2671 [24:46<07:08,  1.37it/s]Evaluating on VQA val set:  78%|#######8  | 2085/2671 [24:47<07:10,  1.36it/s]Evaluating on VQA val set:  78%|#######8  | 2086/2671 [24:47<07:00,  1.39it/s]Evaluating on VQA val set:  78%|#######8  | 2087/2671 [24:48<07:11,  1.35it/s]Evaluating on VQA val set:  78%|#######8  | 2088/2671 [24:49<07:21,  1.32it/s]Evaluating on VQA val set:  78%|#######8  | 2089/2671 [24:50<07:13,  1.34it/s]Evaluating on VQA val set:  78%|#######8  | 2090/2671 [24:50<07:12,  1.34it/s]Evaluating on VQA val set:  78%|#######8  | 2091/2671 [24:51<06:54,  1.40it/s]Evaluating on VQA val set:  78%|#######8  | 2092/2671 [24:52<06:55,  1.39it/s]Evaluating on VQA val set:  78%|#######8  | 2093/2671 [24:52<06:54,  1.39it/s]Evaluating on VQA val set:  78%|#######8  | 2094/2671 [24:53<06:55,  1.39it/s]Evaluating on VQA val set:  78%|#######8  | 2095/2671 [24:54<06:48,  1.41it/s]Evaluating on VQA val set:  78%|#######8  | 2096/2671 [24:54<06:32,  1.47it/s]Evaluating on VQA val set:  79%|#######8  | 2097/2671 [24:55<06:38,  1.44it/s]Evaluating on VQA val set:  79%|#######8  | 2098/2671 [24:56<06:30,  1.47it/s]Evaluating on VQA val set:  79%|#######8  | 2099/2671 [24:57<06:33,  1.45it/s]Evaluating on VQA val set:  79%|#######8  | 2100/2671 [24:57<06:39,  1.43it/s]Evaluating on VQA val set:  79%|#######8  | 2101/2671 [24:58<06:44,  1.41it/s]Evaluating on VQA val set:  79%|#######8  | 2102/2671 [24:59<06:49,  1.39it/s]Evaluating on VQA val set:  79%|#######8  | 2103/2671 [24:59<06:50,  1.38it/s]Evaluating on VQA val set:  79%|#######8  | 2104/2671 [25:00<06:45,  1.40it/s]Evaluating on VQA val set:  79%|#######8  | 2105/2671 [25:01<07:01,  1.34it/s]Evaluating on VQA val set:  79%|#######8  | 2106/2671 [25:02<07:02,  1.34it/s]Evaluating on VQA val set:  79%|#######8  | 2107/2671 [25:02<06:51,  1.37it/s]Evaluating on VQA val set:  79%|#######8  | 2108/2671 [25:03<06:56,  1.35it/s]Evaluating on VQA val set:  79%|#######8  | 2109/2671 [25:04<06:53,  1.36it/s]Evaluating on VQA val set:  79%|#######8  | 2110/2671 [25:05<06:36,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2111/2671 [25:05<06:34,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2112/2671 [25:06<06:29,  1.44it/s]Evaluating on VQA val set:  79%|#######9  | 2113/2671 [25:07<06:32,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2114/2671 [25:07<06:42,  1.38it/s]Evaluating on VQA val set:  79%|#######9  | 2115/2671 [25:08<06:39,  1.39it/s]Evaluating on VQA val set:  79%|#######9  | 2116/2671 [25:09<06:45,  1.37it/s]Evaluating on VQA val set:  79%|#######9  | 2117/2671 [25:10<06:33,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2118/2671 [25:10<06:34,  1.40it/s]Evaluating on VQA val set:  79%|#######9  | 2119/2671 [25:11<06:32,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2120/2671 [25:12<06:42,  1.37it/s]Evaluating on VQA val set:  79%|#######9  | 2121/2671 [25:13<06:49,  1.34it/s]Evaluating on VQA val set:  79%|#######9  | 2122/2671 [25:13<06:45,  1.35it/s]Evaluating on VQA val set:  79%|#######9  | 2123/2671 [25:14<06:50,  1.33it/s]Evaluating on VQA val set:  80%|#######9  | 2124/2671 [25:15<06:52,  1.33it/s]Evaluating on VQA val set:  80%|#######9  | 2125/2671 [25:15<06:33,  1.39it/s]Evaluating on VQA val set:  80%|#######9  | 2126/2671 [25:16<06:24,  1.42it/s]Evaluating on VQA val set:  80%|#######9  | 2127/2671 [25:17<06:29,  1.40it/s]Evaluating on VQA val set:  80%|#######9  | 2128/2671 [25:18<06:28,  1.40it/s]Evaluating on VQA val set:  80%|#######9  | 2129/2671 [25:18<06:38,  1.36it/s]Evaluating on VQA val set:  80%|#######9  | 2130/2671 [25:19<06:29,  1.39it/s]Evaluating on VQA val set:  80%|#######9  | 2131/2671 [25:20<06:31,  1.38it/s]Evaluating on VQA val set:  80%|#######9  | 2132/2671 [25:20<06:31,  1.38it/s]Evaluating on VQA val set:  80%|#######9  | 2133/2671 [25:21<06:23,  1.40it/s]Evaluating on VQA val set:  80%|#######9  | 2134/2671 [25:22<06:11,  1.45it/s]Evaluating on VQA val set:  80%|#######9  | 2135/2671 [25:23<06:13,  1.43it/s]Evaluating on VQA val set:  80%|#######9  | 2136/2671 [25:23<06:02,  1.48it/s]Evaluating on VQA val set:  80%|########  | 2137/2671 [25:24<06:15,  1.42it/s]Evaluating on VQA val set:  80%|########  | 2138/2671 [25:25<06:15,  1.42it/s]Evaluating on VQA val set:  80%|########  | 2139/2671 [25:25<06:17,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2140/2671 [25:26<06:15,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2141/2671 [25:27<06:04,  1.45it/s]Evaluating on VQA val set:  80%|########  | 2142/2671 [25:27<06:14,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2143/2671 [25:28<06:12,  1.42it/s]Evaluating on VQA val set:  80%|########  | 2144/2671 [25:29<06:11,  1.42it/s]Evaluating on VQA val set:  80%|########  | 2145/2671 [25:30<06:12,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2146/2671 [25:30<06:19,  1.38it/s]Evaluating on VQA val set:  80%|########  | 2147/2671 [25:31<06:22,  1.37it/s]Evaluating on VQA val set:  80%|########  | 2148/2671 [25:32<06:24,  1.36it/s]Evaluating on VQA val set:  80%|########  | 2149/2671 [25:32<06:14,  1.39it/s]Evaluating on VQA val set:  80%|########  | 2150/2671 [25:33<06:17,  1.38it/s]Evaluating on VQA val set:  81%|########  | 2151/2671 [25:34<06:12,  1.39it/s]Evaluating on VQA val set:  81%|########  | 2152/2671 [25:35<06:11,  1.40it/s]Evaluating on VQA val set:  81%|########  | 2153/2671 [25:35<06:01,  1.43it/s]Evaluating on VQA val set:  81%|########  | 2154/2671 [25:36<05:41,  1.52it/s]Evaluating on VQA val set:  81%|########  | 2155/2671 [25:37<05:38,  1.52it/s]Evaluating on VQA val set:  81%|########  | 2156/2671 [25:37<05:47,  1.48it/s]Evaluating on VQA val set:  81%|########  | 2157/2671 [25:38<05:54,  1.45it/s]Evaluating on VQA val set:  81%|########  | 2158/2671 [25:39<05:50,  1.46it/s]Evaluating on VQA val set:  81%|########  | 2159/2671 [25:39<05:46,  1.48it/s]Evaluating on VQA val set:  81%|########  | 2160/2671 [25:40<06:03,  1.40it/s]Evaluating on VQA val set:  81%|########  | 2161/2671 [25:41<06:04,  1.40it/s]Evaluating on VQA val set:  81%|########  | 2162/2671 [25:41<05:57,  1.42it/s]Evaluating on VQA val set:  81%|########  | 2163/2671 [25:42<06:00,  1.41it/s]Evaluating on VQA val set:  81%|########1 | 2164/2671 [25:43<05:57,  1.42it/s]Evaluating on VQA val set:  81%|########1 | 2165/2671 [25:44<05:52,  1.44it/s]Evaluating on VQA val set:  81%|########1 | 2166/2671 [25:44<06:03,  1.39it/s]Evaluating on VQA val set:  81%|########1 | 2167/2671 [25:45<05:58,  1.41it/s]Evaluating on VQA val set:  81%|########1 | 2168/2671 [25:46<06:02,  1.39it/s]Evaluating on VQA val set:  81%|########1 | 2169/2671 [25:46<05:59,  1.40it/s]Evaluating on VQA val set:  81%|########1 | 2170/2671 [25:47<06:04,  1.37it/s]Evaluating on VQA val set:  81%|########1 | 2171/2671 [25:48<06:04,  1.37it/s]Evaluating on VQA val set:  81%|########1 | 2172/2671 [25:49<06:08,  1.36it/s]Evaluating on VQA val set:  81%|########1 | 2173/2671 [25:49<06:01,  1.38it/s]Evaluating on VQA val set:  81%|########1 | 2174/2671 [25:50<05:57,  1.39it/s]Evaluating on VQA val set:  81%|########1 | 2175/2671 [25:51<05:54,  1.40it/s]Evaluating on VQA val set:  81%|########1 | 2176/2671 [25:52<05:46,  1.43it/s]Evaluating on VQA val set:  82%|########1 | 2177/2671 [25:52<05:45,  1.43it/s]Evaluating on VQA val set:  82%|########1 | 2178/2671 [25:53<05:45,  1.43it/s]Evaluating on VQA val set:  82%|########1 | 2179/2671 [25:54<05:33,  1.48it/s]Evaluating on VQA val set:  82%|########1 | 2180/2671 [25:54<05:28,  1.50it/s]Evaluating on VQA val set:  82%|########1 | 2181/2671 [25:55<05:36,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2182/2671 [25:56<05:40,  1.44it/s]Evaluating on VQA val set:  82%|########1 | 2183/2671 [25:56<05:48,  1.40it/s]Evaluating on VQA val set:  82%|########1 | 2184/2671 [25:57<05:42,  1.42it/s]Evaluating on VQA val set:  82%|########1 | 2185/2671 [25:58<05:29,  1.48it/s]Evaluating on VQA val set:  82%|########1 | 2186/2671 [25:58<05:24,  1.49it/s]Evaluating on VQA val set:  82%|########1 | 2187/2671 [25:59<05:36,  1.44it/s]Evaluating on VQA val set:  82%|########1 | 2188/2671 [26:00<05:39,  1.42it/s]Evaluating on VQA val set:  82%|########1 | 2189/2671 [26:00<05:29,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2190/2671 [26:01<05:26,  1.47it/s]Evaluating on VQA val set:  82%|########2 | 2191/2671 [26:02<05:29,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2192/2671 [26:03<05:40,  1.41it/s]Evaluating on VQA val set:  82%|########2 | 2193/2671 [26:03<05:38,  1.41it/s]Evaluating on VQA val set:  82%|########2 | 2194/2671 [26:04<05:28,  1.45it/s]Evaluating on VQA val set:  82%|########2 | 2195/2671 [26:05<05:27,  1.45it/s]Evaluating on VQA val set:  82%|########2 | 2196/2671 [26:05<05:30,  1.44it/s]Evaluating on VQA val set:  82%|########2 | 2197/2671 [26:06<05:24,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2198/2671 [26:07<05:30,  1.43it/s]Evaluating on VQA val set:  82%|########2 | 2199/2671 [26:07<05:29,  1.43it/s]Evaluating on VQA val set:  82%|########2 | 2200/2671 [26:08<05:38,  1.39it/s]Evaluating on VQA val set:  82%|########2 | 2201/2671 [26:09<05:47,  1.35it/s]Evaluating on VQA val set:  82%|########2 | 2202/2671 [26:10<05:22,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2203/2671 [26:10<05:27,  1.43it/s]Evaluating on VQA val set:  83%|########2 | 2204/2671 [26:11<05:34,  1.40it/s]Evaluating on VQA val set:  83%|########2 | 2205/2671 [26:12<05:34,  1.39it/s]Evaluating on VQA val set:  83%|########2 | 2206/2671 [26:12<05:33,  1.39it/s]Evaluating on VQA val set:  83%|########2 | 2207/2671 [26:13<05:39,  1.37it/s]Evaluating on VQA val set:  83%|########2 | 2208/2671 [26:14<05:36,  1.38it/s]Evaluating on VQA val set:  83%|########2 | 2209/2671 [26:15<05:37,  1.37it/s]Evaluating on VQA val set:  83%|########2 | 2210/2671 [26:15<05:37,  1.37it/s]Evaluating on VQA val set:  83%|########2 | 2211/2671 [26:16<05:33,  1.38it/s]Evaluating on VQA val set:  83%|########2 | 2212/2671 [26:17<05:26,  1.41it/s]Evaluating on VQA val set:  83%|########2 | 2213/2671 [26:18<05:27,  1.40it/s]Evaluating on VQA val set:  83%|########2 | 2214/2671 [26:18<05:20,  1.43it/s]Evaluating on VQA val set:  83%|########2 | 2215/2671 [26:19<05:24,  1.41it/s]Evaluating on VQA val set:  83%|########2 | 2216/2671 [26:20<05:18,  1.43it/s]Evaluating on VQA val set:  83%|########3 | 2217/2671 [26:20<05:26,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2218/2671 [26:21<05:22,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2219/2671 [26:22<05:18,  1.42it/s]Evaluating on VQA val set:  83%|########3 | 2220/2671 [26:22<05:22,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2221/2671 [26:23<05:29,  1.36it/s]Evaluating on VQA val set:  83%|########3 | 2222/2671 [26:24<05:17,  1.41it/s]Evaluating on VQA val set:  83%|########3 | 2223/2671 [26:25<05:16,  1.41it/s]Evaluating on VQA val set:  83%|########3 | 2224/2671 [26:25<05:18,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2225/2671 [26:26<05:13,  1.42it/s]Evaluating on VQA val set:  83%|########3 | 2226/2671 [26:27<05:24,  1.37it/s]Evaluating on VQA val set:  83%|########3 | 2227/2671 [26:28<05:20,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2228/2671 [26:28<05:15,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2229/2671 [26:29<05:16,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2230/2671 [26:30<05:17,  1.39it/s]Evaluating on VQA val set:  84%|########3 | 2231/2671 [26:30<05:22,  1.37it/s]Evaluating on VQA val set:  84%|########3 | 2232/2671 [26:31<05:22,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2233/2671 [26:32<05:23,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2234/2671 [26:33<05:14,  1.39it/s]Evaluating on VQA val set:  84%|########3 | 2235/2671 [26:33<05:16,  1.38it/s]Evaluating on VQA val set:  84%|########3 | 2236/2671 [26:34<05:15,  1.38it/s]Evaluating on VQA val set:  84%|########3 | 2237/2671 [26:35<05:19,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2238/2671 [26:35<05:04,  1.42it/s]Evaluating on VQA val set:  84%|########3 | 2239/2671 [26:36<05:05,  1.41it/s]Evaluating on VQA val set:  84%|########3 | 2240/2671 [26:37<05:06,  1.40it/s]Evaluating on VQA val set:  84%|########3 | 2241/2671 [26:38<05:15,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2242/2671 [26:38<05:19,  1.34it/s]Evaluating on VQA val set:  84%|########3 | 2243/2671 [26:39<05:13,  1.37it/s]Evaluating on VQA val set:  84%|########4 | 2244/2671 [26:40<05:15,  1.35it/s]Evaluating on VQA val set:  84%|########4 | 2245/2671 [26:41<05:19,  1.33it/s]Evaluating on VQA val set:  84%|########4 | 2246/2671 [26:41<05:12,  1.36it/s]Evaluating on VQA val set:  84%|########4 | 2247/2671 [26:42<05:10,  1.36it/s]Evaluating on VQA val set:  84%|########4 | 2248/2671 [26:43<05:03,  1.39it/s]Evaluating on VQA val set:  84%|########4 | 2249/2671 [26:43<04:57,  1.42it/s]Evaluating on VQA val set:  84%|########4 | 2250/2671 [26:44<04:59,  1.41it/s]Evaluating on VQA val set:  84%|########4 | 2251/2671 [26:45<04:52,  1.43it/s]Evaluating on VQA val set:  84%|########4 | 2252/2671 [26:46<05:03,  1.38it/s]Evaluating on VQA val set:  84%|########4 | 2253/2671 [26:46<05:02,  1.38it/s]Evaluating on VQA val set:  84%|########4 | 2254/2671 [26:47<05:07,  1.36it/s]Evaluating on VQA val set:  84%|########4 | 2255/2671 [26:48<05:03,  1.37it/s]Evaluating on VQA val set:  84%|########4 | 2256/2671 [26:48<04:49,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2257/2671 [26:49<04:49,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2258/2671 [26:50<04:48,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2259/2671 [26:51<04:49,  1.42it/s]Evaluating on VQA val set:  85%|########4 | 2260/2671 [26:51<04:48,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2261/2671 [26:52<04:49,  1.42it/s]Evaluating on VQA val set:  85%|########4 | 2262/2671 [26:53<04:49,  1.41it/s]Evaluating on VQA val set:  85%|########4 | 2263/2671 [26:53<04:44,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2264/2671 [26:54<04:49,  1.41it/s]Evaluating on VQA val set:  85%|########4 | 2265/2671 [26:55<04:52,  1.39it/s]Evaluating on VQA val set:  85%|########4 | 2266/2671 [26:56<04:51,  1.39it/s]Evaluating on VQA val set:  85%|########4 | 2267/2671 [26:56<04:52,  1.38it/s]Evaluating on VQA val set:  85%|########4 | 2268/2671 [26:57<04:55,  1.36it/s]Evaluating on VQA val set:  85%|########4 | 2269/2671 [26:58<04:50,  1.38it/s]Evaluating on VQA val set:  85%|########4 | 2270/2671 [26:58<04:48,  1.39it/s]Evaluating on VQA val set:  85%|########5 | 2271/2671 [26:59<04:51,  1.37it/s]Evaluating on VQA val set:  85%|########5 | 2272/2671 [27:00<04:55,  1.35it/s]Evaluating on VQA val set:  85%|########5 | 2273/2671 [27:01<04:48,  1.38it/s]Evaluating on VQA val set:  85%|########5 | 2274/2671 [27:01<04:48,  1.38it/s]Evaluating on VQA val set:  85%|########5 | 2275/2671 [27:02<04:45,  1.39it/s]Evaluating on VQA val set:  85%|########5 | 2276/2671 [27:03<04:44,  1.39it/s]Evaluating on VQA val set:  85%|########5 | 2277/2671 [27:04<04:48,  1.37it/s]Evaluating on VQA val set:  85%|########5 | 2278/2671 [27:04<04:40,  1.40it/s]Evaluating on VQA val set:  85%|########5 | 2279/2671 [27:05<04:23,  1.49it/s]Evaluating on VQA val set:  85%|########5 | 2280/2671 [27:06<04:26,  1.47it/s]Evaluating on VQA val set:  85%|########5 | 2281/2671 [27:06<04:32,  1.43it/s]Evaluating on VQA val set:  85%|########5 | 2282/2671 [27:07<04:31,  1.43it/s]Evaluating on VQA val set:  85%|########5 | 2283/2671 [27:08<04:30,  1.43it/s]Evaluating on VQA val set:  86%|########5 | 2284/2671 [27:08<04:34,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2285/2671 [27:09<04:33,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2286/2671 [27:10<04:28,  1.43it/s]Evaluating on VQA val set:  86%|########5 | 2287/2671 [27:10<04:28,  1.43it/s]Evaluating on VQA val set:  86%|########5 | 2288/2671 [27:11<04:23,  1.45it/s]Evaluating on VQA val set:  86%|########5 | 2289/2671 [27:12<04:29,  1.42it/s]Evaluating on VQA val set:  86%|########5 | 2290/2671 [27:13<04:29,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2291/2671 [27:13<04:19,  1.46it/s]Evaluating on VQA val set:  86%|########5 | 2292/2671 [27:14<04:25,  1.43it/s]Evaluating on VQA val set:  86%|########5 | 2293/2671 [27:15<04:30,  1.40it/s]Evaluating on VQA val set:  86%|########5 | 2294/2671 [27:15<04:33,  1.38it/s]Evaluating on VQA val set:  86%|########5 | 2295/2671 [27:16<04:35,  1.37it/s]Evaluating on VQA val set:  86%|########5 | 2296/2671 [27:17<04:29,  1.39it/s]Evaluating on VQA val set:  86%|########5 | 2297/2671 [27:18<04:35,  1.36it/s]Evaluating on VQA val set:  86%|########6 | 2298/2671 [27:18<04:38,  1.34it/s]Evaluating on VQA val set:  86%|########6 | 2299/2671 [27:19<04:27,  1.39it/s]Evaluating on VQA val set:  86%|########6 | 2300/2671 [27:20<04:29,  1.38it/s]Evaluating on VQA val set:  86%|########6 | 2301/2671 [27:21<04:29,  1.38it/s]Evaluating on VQA val set:  86%|########6 | 2302/2671 [27:21<04:20,  1.41it/s]Evaluating on VQA val set:  86%|########6 | 2303/2671 [27:22<04:26,  1.38it/s]Evaluating on VQA val set:  86%|########6 | 2304/2671 [27:23<04:20,  1.41it/s]Evaluating on VQA val set:  86%|########6 | 2305/2671 [27:23<04:16,  1.43it/s]Evaluating on VQA val set:  86%|########6 | 2306/2671 [27:24<04:17,  1.42it/s]Evaluating on VQA val set:  86%|########6 | 2307/2671 [27:25<04:14,  1.43it/s]Evaluating on VQA val set:  86%|########6 | 2308/2671 [27:25<04:15,  1.42it/s]Evaluating on VQA val set:  86%|########6 | 2309/2671 [27:26<04:19,  1.40it/s]Evaluating on VQA val set:  86%|########6 | 2310/2671 [27:27<04:18,  1.40it/s]Evaluating on VQA val set:  87%|########6 | 2311/2671 [27:28<04:16,  1.40it/s]Evaluating on VQA val set:  87%|########6 | 2312/2671 [27:28<04:15,  1.41it/s]Evaluating on VQA val set:  87%|########6 | 2313/2671 [27:29<04:11,  1.43it/s]Evaluating on VQA val set:  87%|########6 | 2314/2671 [27:30<04:10,  1.42it/s]Evaluating on VQA val set:  87%|########6 | 2315/2671 [27:30<04:06,  1.45it/s]Evaluating on VQA val set:  87%|########6 | 2316/2671 [27:31<04:12,  1.40it/s]Evaluating on VQA val set:  87%|########6 | 2317/2671 [27:32<04:13,  1.40it/s]Evaluating on VQA val set:  87%|########6 | 2318/2671 [27:33<04:14,  1.38it/s]Evaluating on VQA val set:  87%|########6 | 2319/2671 [27:33<04:24,  1.33it/s]Evaluating on VQA val set:  87%|########6 | 2320/2671 [27:34<04:20,  1.35it/s]Evaluating on VQA val set:  87%|########6 | 2321/2671 [27:35<04:24,  1.32it/s]Evaluating on VQA val set:  87%|########6 | 2322/2671 [27:36<04:10,  1.39it/s]Evaluating on VQA val set:  87%|########6 | 2323/2671 [27:36<04:12,  1.38it/s]Evaluating on VQA val set:  87%|########7 | 2324/2671 [27:37<04:08,  1.40it/s]Evaluating on VQA val set:  87%|########7 | 2325/2671 [27:38<04:02,  1.43it/s]Evaluating on VQA val set:  87%|########7 | 2326/2671 [27:38<04:03,  1.41it/s]Evaluating on VQA val set:  87%|########7 | 2327/2671 [27:39<04:06,  1.39it/s]Evaluating on VQA val set:  87%|########7 | 2328/2671 [27:40<04:11,  1.36it/s]Evaluating on VQA val set:  87%|########7 | 2329/2671 [27:41<04:07,  1.38it/s]Evaluating on VQA val set:  87%|########7 | 2330/2671 [27:41<04:07,  1.38it/s]Evaluating on VQA val set:  87%|########7 | 2331/2671 [27:42<04:07,  1.37it/s]Evaluating on VQA val set:  87%|########7 | 2332/2671 [27:43<04:07,  1.37it/s]Evaluating on VQA val set:  87%|########7 | 2333/2671 [27:44<04:10,  1.35it/s]Evaluating on VQA val set:  87%|########7 | 2334/2671 [27:44<04:11,  1.34it/s]Evaluating on VQA val set:  87%|########7 | 2335/2671 [27:45<04:05,  1.37it/s]Evaluating on VQA val set:  87%|########7 | 2336/2671 [27:46<03:58,  1.40it/s]Evaluating on VQA val set:  87%|########7 | 2337/2671 [27:46<03:58,  1.40it/s]Evaluating on VQA val set:  88%|########7 | 2338/2671 [27:47<03:57,  1.40it/s]Evaluating on VQA val set:  88%|########7 | 2339/2671 [27:48<04:01,  1.37it/s]Evaluating on VQA val set:  88%|########7 | 2340/2671 [27:49<04:06,  1.34it/s]Evaluating on VQA val set:  88%|########7 | 2341/2671 [27:50<04:13,  1.30it/s]Evaluating on VQA val set:  88%|########7 | 2342/2671 [27:50<04:02,  1.36it/s]Evaluating on VQA val set:  88%|########7 | 2343/2671 [27:51<03:55,  1.39it/s]Evaluating on VQA val set:  88%|########7 | 2344/2671 [27:51<03:46,  1.44it/s]Evaluating on VQA val set:  88%|########7 | 2345/2671 [27:52<03:46,  1.44it/s]Evaluating on VQA val set:  88%|########7 | 2346/2671 [27:53<03:42,  1.46it/s]Evaluating on VQA val set:  88%|########7 | 2347/2671 [27:54<03:48,  1.42it/s]Evaluating on VQA val set:  88%|########7 | 2348/2671 [27:54<03:54,  1.37it/s]Evaluating on VQA val set:  88%|########7 | 2349/2671 [27:55<04:02,  1.33it/s]Evaluating on VQA val set:  88%|########7 | 2350/2671 [27:56<04:03,  1.32it/s]Evaluating on VQA val set:  88%|########8 | 2351/2671 [27:57<03:54,  1.37it/s]Evaluating on VQA val set:  88%|########8 | 2352/2671 [27:57<03:46,  1.41it/s]Evaluating on VQA val set:  88%|########8 | 2353/2671 [27:58<03:50,  1.38it/s]Evaluating on VQA val set:  88%|########8 | 2354/2671 [27:59<03:53,  1.36it/s]Evaluating on VQA val set:  88%|########8 | 2355/2671 [28:00<03:53,  1.36it/s]Evaluating on VQA val set:  88%|########8 | 2356/2671 [28:00<03:50,  1.37it/s]Evaluating on VQA val set:  88%|########8 | 2357/2671 [28:01<03:49,  1.37it/s]Evaluating on VQA val set:  88%|########8 | 2358/2671 [28:02<03:42,  1.41it/s]Evaluating on VQA val set:  88%|########8 | 2359/2671 [28:02<03:32,  1.47it/s]Evaluating on VQA val set:  88%|########8 | 2360/2671 [28:03<03:32,  1.47it/s]Evaluating on VQA val set:  88%|########8 | 2361/2671 [28:04<03:35,  1.44it/s]Evaluating on VQA val set:  88%|########8 | 2362/2671 [28:04<03:41,  1.40it/s]Evaluating on VQA val set:  88%|########8 | 2363/2671 [28:05<03:40,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2364/2671 [28:06<03:35,  1.42it/s]Evaluating on VQA val set:  89%|########8 | 2365/2671 [28:07<03:37,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2366/2671 [28:07<03:38,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2367/2671 [28:08<03:33,  1.42it/s]Evaluating on VQA val set:  89%|########8 | 2368/2671 [28:09<03:33,  1.42it/s]Evaluating on VQA val set:  89%|########8 | 2369/2671 [28:09<03:24,  1.48it/s]Evaluating on VQA val set:  89%|########8 | 2370/2671 [28:10<03:27,  1.45it/s]Evaluating on VQA val set:  89%|########8 | 2371/2671 [28:11<03:32,  1.41it/s]Evaluating on VQA val set:  89%|########8 | 2372/2671 [28:11<03:32,  1.41it/s]Evaluating on VQA val set:  89%|########8 | 2373/2671 [28:12<03:36,  1.38it/s]Evaluating on VQA val set:  89%|########8 | 2374/2671 [28:13<03:32,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2375/2671 [28:14<03:33,  1.39it/s]Evaluating on VQA val set:  89%|########8 | 2376/2671 [28:14<03:28,  1.41it/s]Evaluating on VQA val set:  89%|########8 | 2377/2671 [28:15<03:29,  1.41it/s]Evaluating on VQA val set:  89%|########9 | 2378/2671 [28:16<03:32,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2379/2671 [28:16<03:28,  1.40it/s]Evaluating on VQA val set:  89%|########9 | 2380/2671 [28:17<03:28,  1.39it/s]Evaluating on VQA val set:  89%|########9 | 2381/2671 [28:18<03:33,  1.36it/s]Evaluating on VQA val set:  89%|########9 | 2382/2671 [28:19<03:33,  1.35it/s]Evaluating on VQA val set:  89%|########9 | 2383/2671 [28:19<03:33,  1.35it/s]Evaluating on VQA val set:  89%|########9 | 2384/2671 [28:20<03:28,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2385/2671 [28:21<03:27,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2386/2671 [28:22<03:30,  1.36it/s]Evaluating on VQA val set:  89%|########9 | 2387/2671 [28:22<03:25,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2388/2671 [28:23<03:29,  1.35it/s]Evaluating on VQA val set:  89%|########9 | 2389/2671 [28:24<03:33,  1.32it/s]Evaluating on VQA val set:  89%|########9 | 2390/2671 [28:25<03:32,  1.32it/s]Evaluating on VQA val set:  90%|########9 | 2391/2671 [28:25<03:29,  1.33it/s]Evaluating on VQA val set:  90%|########9 | 2392/2671 [28:26<03:30,  1.33it/s]Evaluating on VQA val set:  90%|########9 | 2393/2671 [28:27<03:24,  1.36it/s]Evaluating on VQA val set:  90%|########9 | 2394/2671 [28:28<03:21,  1.38it/s]Evaluating on VQA val set:  90%|########9 | 2395/2671 [28:28<03:22,  1.36it/s]Evaluating on VQA val set:  90%|########9 | 2396/2671 [28:29<03:20,  1.37it/s]Evaluating on VQA val set:  90%|########9 | 2397/2671 [28:30<03:18,  1.38it/s]Evaluating on VQA val set:  90%|########9 | 2398/2671 [28:31<03:19,  1.37it/s]Evaluating on VQA val set:  90%|########9 | 2399/2671 [28:31<03:21,  1.35it/s]Evaluating on VQA val set:  90%|########9 | 2400/2671 [28:32<03:15,  1.38it/s]Evaluating on VQA val set:  90%|########9 | 2401/2671 [28:32<02:51,  1.57it/s]Evaluating on VQA val set:  90%|########9 | 2402/2671 [28:33<02:36,  1.72it/s]Evaluating on VQA val set:  90%|########9 | 2403/2671 [28:34<02:42,  1.65it/s]Evaluating on VQA val set:  90%|######### | 2404/2671 [28:34<02:52,  1.55it/s]Evaluating on VQA val set:  90%|######### | 2405/2671 [28:35<02:59,  1.48it/s]Evaluating on VQA val set:  90%|######### | 2406/2671 [28:36<02:58,  1.49it/s]Evaluating on VQA val set:  90%|######### | 2407/2671 [28:36<03:01,  1.46it/s]Evaluating on VQA val set:  90%|######### | 2408/2671 [28:37<03:03,  1.43it/s]Evaluating on VQA val set:  90%|######### | 2409/2671 [28:38<02:57,  1.48it/s]Evaluating on VQA val set:  90%|######### | 2410/2671 [28:38<02:48,  1.55it/s]Evaluating on VQA val set:  90%|######### | 2411/2671 [28:39<02:55,  1.48it/s]Evaluating on VQA val set:  90%|######### | 2412/2671 [28:40<02:56,  1.47it/s]Evaluating on VQA val set:  90%|######### | 2413/2671 [28:41<03:06,  1.38it/s]Evaluating on VQA val set:  90%|######### | 2414/2671 [28:41<03:11,  1.34it/s]Evaluating on VQA val set:  90%|######### | 2415/2671 [28:42<03:11,  1.34it/s]Evaluating on VQA val set:  90%|######### | 2416/2671 [28:43<03:01,  1.40it/s]Evaluating on VQA val set:  90%|######### | 2417/2671 [28:43<03:04,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2418/2671 [28:44<03:03,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2419/2671 [28:45<03:00,  1.39it/s]Evaluating on VQA val set:  91%|######### | 2420/2671 [28:46<02:58,  1.41it/s]Evaluating on VQA val set:  91%|######### | 2421/2671 [28:46<02:57,  1.41it/s]Evaluating on VQA val set:  91%|######### | 2422/2671 [28:47<02:59,  1.39it/s]Evaluating on VQA val set:  91%|######### | 2423/2671 [28:48<03:00,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2424/2671 [28:49<02:57,  1.39it/s]Evaluating on VQA val set:  91%|######### | 2425/2671 [28:49<02:54,  1.41it/s]Evaluating on VQA val set:  91%|######### | 2426/2671 [28:50<02:51,  1.42it/s]Evaluating on VQA val set:  91%|######### | 2427/2671 [28:51<02:56,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2428/2671 [28:51<02:56,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2429/2671 [28:52<02:54,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2430/2671 [28:53<02:53,  1.39it/s]Evaluating on VQA val set:  91%|#########1| 2431/2671 [28:54<02:53,  1.38it/s]Evaluating on VQA val set:  91%|#########1| 2432/2671 [28:54<02:52,  1.38it/s]Evaluating on VQA val set:  91%|#########1| 2433/2671 [28:55<02:51,  1.39it/s]Evaluating on VQA val set:  91%|#########1| 2434/2671 [28:56<02:53,  1.37it/s]Evaluating on VQA val set:  91%|#########1| 2435/2671 [28:56<02:48,  1.40it/s]Evaluating on VQA val set:  91%|#########1| 2436/2671 [28:57<02:48,  1.40it/s]Evaluating on VQA val set:  91%|#########1| 2437/2671 [28:58<02:43,  1.43it/s]Evaluating on VQA val set:  91%|#########1| 2438/2671 [28:59<02:45,  1.41it/s]Evaluating on VQA val set:  91%|#########1| 2439/2671 [28:59<02:45,  1.40it/s]Evaluating on VQA val set:  91%|#########1| 2440/2671 [29:00<02:48,  1.37it/s]Evaluating on VQA val set:  91%|#########1| 2441/2671 [29:01<02:50,  1.35it/s]Evaluating on VQA val set:  91%|#########1| 2442/2671 [29:01<02:47,  1.37it/s]Evaluating on VQA val set:  91%|#########1| 2443/2671 [29:02<02:48,  1.35it/s]Evaluating on VQA val set:  92%|#########1| 2444/2671 [29:03<02:43,  1.39it/s]Evaluating on VQA val set:  92%|#########1| 2445/2671 [29:04<02:39,  1.42it/s]Evaluating on VQA val set:  92%|#########1| 2446/2671 [29:04<02:37,  1.42it/s]Evaluating on VQA val set:  92%|#########1| 2447/2671 [29:05<02:36,  1.43it/s]Evaluating on VQA val set:  92%|#########1| 2448/2671 [29:06<02:35,  1.43it/s]Evaluating on VQA val set:  92%|#########1| 2449/2671 [29:06<02:30,  1.48it/s]Evaluating on VQA val set:  92%|#########1| 2450/2671 [29:07<02:25,  1.52it/s]Evaluating on VQA val set:  92%|#########1| 2451/2671 [29:08<02:31,  1.45it/s]Evaluating on VQA val set:  92%|#########1| 2452/2671 [29:08<02:31,  1.45it/s]Evaluating on VQA val set:  92%|#########1| 2453/2671 [29:09<02:29,  1.46it/s]Evaluating on VQA val set:  92%|#########1| 2454/2671 [29:10<02:30,  1.44it/s]Evaluating on VQA val set:  92%|#########1| 2455/2671 [29:10<02:25,  1.49it/s]Evaluating on VQA val set:  92%|#########1| 2456/2671 [29:11<02:24,  1.49it/s]Evaluating on VQA val set:  92%|#########1| 2457/2671 [29:12<02:26,  1.46it/s]Evaluating on VQA val set:  92%|#########2| 2458/2671 [29:13<02:30,  1.42it/s]Evaluating on VQA val set:  92%|#########2| 2459/2671 [29:13<02:28,  1.43it/s]Evaluating on VQA val set:  92%|#########2| 2460/2671 [29:14<02:30,  1.40it/s]Evaluating on VQA val set:  92%|#########2| 2461/2671 [29:15<02:34,  1.36it/s]Evaluating on VQA val set:  92%|#########2| 2462/2671 [29:15<02:34,  1.35it/s]Evaluating on VQA val set:  92%|#########2| 2463/2671 [29:16<02:35,  1.33it/s]Evaluating on VQA val set:  92%|#########2| 2464/2671 [29:17<02:33,  1.35it/s]Evaluating on VQA val set:  92%|#########2| 2465/2671 [29:18<02:34,  1.34it/s]Evaluating on VQA val set:  92%|#########2| 2466/2671 [29:18<02:30,  1.36it/s]Evaluating on VQA val set:  92%|#########2| 2467/2671 [29:19<02:33,  1.33it/s]Evaluating on VQA val set:  92%|#########2| 2468/2671 [29:20<02:25,  1.39it/s]Evaluating on VQA val set:  92%|#########2| 2469/2671 [29:21<02:20,  1.44it/s]Evaluating on VQA val set:  92%|#########2| 2470/2671 [29:21<02:20,  1.44it/s]Evaluating on VQA val set:  93%|#########2| 2471/2671 [29:22<02:24,  1.38it/s]Evaluating on VQA val set:  93%|#########2| 2472/2671 [29:23<02:23,  1.39it/s]Evaluating on VQA val set:  93%|#########2| 2473/2671 [29:23<02:19,  1.42it/s]Evaluating on VQA val set:  93%|#########2| 2474/2671 [29:24<02:17,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2475/2671 [29:25<02:17,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2476/2671 [29:25<02:11,  1.49it/s]Evaluating on VQA val set:  93%|#########2| 2477/2671 [29:26<02:13,  1.45it/s]Evaluating on VQA val set:  93%|#########2| 2478/2671 [29:27<02:13,  1.45it/s]Evaluating on VQA val set:  93%|#########2| 2479/2671 [29:28<02:15,  1.42it/s]Evaluating on VQA val set:  93%|#########2| 2480/2671 [29:28<02:10,  1.46it/s]Evaluating on VQA val set:  93%|#########2| 2481/2671 [29:29<02:11,  1.45it/s]Evaluating on VQA val set:  93%|#########2| 2482/2671 [29:30<02:13,  1.42it/s]Evaluating on VQA val set:  93%|#########2| 2483/2671 [29:30<02:11,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2484/2671 [29:31<02:12,  1.42it/s]Evaluating on VQA val set:  93%|#########3| 2485/2671 [29:32<02:15,  1.37it/s]Evaluating on VQA val set:  93%|#########3| 2486/2671 [29:33<02:14,  1.38it/s]Evaluating on VQA val set:  93%|#########3| 2487/2671 [29:33<02:09,  1.42it/s]Evaluating on VQA val set:  93%|#########3| 2488/2671 [29:34<02:05,  1.45it/s]Evaluating on VQA val set:  93%|#########3| 2489/2671 [29:34<02:02,  1.49it/s]Evaluating on VQA val set:  93%|#########3| 2490/2671 [29:35<01:56,  1.56it/s]Evaluating on VQA val set:  93%|#########3| 2491/2671 [29:36<01:50,  1.62it/s]Evaluating on VQA val set:  93%|#########3| 2492/2671 [29:36<01:51,  1.61it/s]Evaluating on VQA val set:  93%|#########3| 2493/2671 [29:37<01:55,  1.54it/s]Evaluating on VQA val set:  93%|#########3| 2494/2671 [29:38<02:01,  1.46it/s]Evaluating on VQA val set:  93%|#########3| 2495/2671 [29:38<01:58,  1.48it/s]Evaluating on VQA val set:  93%|#########3| 2496/2671 [29:39<02:00,  1.45it/s]Evaluating on VQA val set:  93%|#########3| 2497/2671 [29:40<01:56,  1.50it/s]Evaluating on VQA val set:  94%|#########3| 2498/2671 [29:40<01:55,  1.49it/s]Evaluating on VQA val set:  94%|#########3| 2499/2671 [29:41<01:56,  1.48it/s]Evaluating on VQA val set:  94%|#########3| 2500/2671 [29:42<01:54,  1.50it/s]Evaluating on VQA val set:  94%|#########3| 2501/2671 [29:42<01:56,  1.46it/s]Evaluating on VQA val set:  94%|#########3| 2502/2671 [29:43<01:57,  1.44it/s]Evaluating on VQA val set:  94%|#########3| 2503/2671 [29:44<01:59,  1.41it/s]Evaluating on VQA val set:  94%|#########3| 2504/2671 [29:45<01:59,  1.39it/s]Evaluating on VQA val set:  94%|#########3| 2505/2671 [29:45<01:56,  1.43it/s]Evaluating on VQA val set:  94%|#########3| 2506/2671 [29:46<01:52,  1.47it/s]Evaluating on VQA val set:  94%|#########3| 2507/2671 [29:47<01:55,  1.42it/s]Evaluating on VQA val set:  94%|#########3| 2508/2671 [29:47<01:51,  1.47it/s]Evaluating on VQA val set:  94%|#########3| 2509/2671 [29:48<01:41,  1.59it/s]Evaluating on VQA val set:  94%|#########3| 2510/2671 [29:48<01:42,  1.57it/s]Evaluating on VQA val set:  94%|#########4| 2511/2671 [29:49<01:43,  1.54it/s]Evaluating on VQA val set:  94%|#########4| 2512/2671 [29:50<01:46,  1.49it/s]Evaluating on VQA val set:  94%|#########4| 2513/2671 [29:51<01:47,  1.47it/s]Evaluating on VQA val set:  94%|#########4| 2514/2671 [29:51<01:47,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2515/2671 [29:52<01:47,  1.45it/s]Evaluating on VQA val set:  94%|#########4| 2516/2671 [29:53<01:48,  1.43it/s]Evaluating on VQA val set:  94%|#########4| 2517/2671 [29:53<01:47,  1.43it/s]Evaluating on VQA val set:  94%|#########4| 2518/2671 [29:54<01:47,  1.42it/s]Evaluating on VQA val set:  94%|#########4| 2519/2671 [29:55<01:47,  1.41it/s]Evaluating on VQA val set:  94%|#########4| 2520/2671 [29:56<01:46,  1.42it/s]Evaluating on VQA val set:  94%|#########4| 2521/2671 [29:56<01:43,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2522/2671 [29:57<01:42,  1.45it/s]Evaluating on VQA val set:  94%|#########4| 2523/2671 [29:58<01:42,  1.45it/s]Evaluating on VQA val set:  94%|#########4| 2524/2671 [29:58<01:42,  1.44it/s]Evaluating on VQA val set:  95%|#########4| 2525/2671 [29:59<01:42,  1.42it/s]Evaluating on VQA val set:  95%|#########4| 2526/2671 [30:00<01:37,  1.48it/s]Evaluating on VQA val set:  95%|#########4| 2527/2671 [30:00<01:38,  1.46it/s]Evaluating on VQA val set:  95%|#########4| 2528/2671 [30:01<01:40,  1.42it/s]Evaluating on VQA val set:  95%|#########4| 2529/2671 [30:02<01:39,  1.42it/s]Evaluating on VQA val set:  95%|#########4| 2530/2671 [30:02<01:37,  1.45it/s]Evaluating on VQA val set:  95%|#########4| 2531/2671 [30:03<01:34,  1.48it/s]Evaluating on VQA val set:  95%|#########4| 2532/2671 [30:04<01:35,  1.45it/s]Evaluating on VQA val set:  95%|#########4| 2533/2671 [30:04<01:35,  1.45it/s]Evaluating on VQA val set:  95%|#########4| 2534/2671 [30:05<01:33,  1.46it/s]Evaluating on VQA val set:  95%|#########4| 2535/2671 [30:06<01:31,  1.49it/s]Evaluating on VQA val set:  95%|#########4| 2536/2671 [30:07<01:34,  1.43it/s]Evaluating on VQA val set:  95%|#########4| 2537/2671 [30:07<01:34,  1.42it/s]Evaluating on VQA val set:  95%|#########5| 2538/2671 [30:08<01:31,  1.45it/s]Evaluating on VQA val set:  95%|#########5| 2539/2671 [30:09<01:29,  1.48it/s]Evaluating on VQA val set:  95%|#########5| 2540/2671 [30:09<01:26,  1.52it/s]Evaluating on VQA val set:  95%|#########5| 2541/2671 [30:10<01:28,  1.47it/s]Evaluating on VQA val set:  95%|#########5| 2542/2671 [30:11<01:31,  1.41it/s]Evaluating on VQA val set:  95%|#########5| 2543/2671 [30:12<01:34,  1.35it/s]Evaluating on VQA val set:  95%|#########5| 2544/2671 [30:12<01:30,  1.40it/s]Evaluating on VQA val set:  95%|#########5| 2545/2671 [30:13<01:28,  1.42it/s]Evaluating on VQA val set:  95%|#########5| 2546/2671 [30:14<01:26,  1.44it/s]Evaluating on VQA val set:  95%|#########5| 2547/2671 [30:14<01:26,  1.43it/s]Evaluating on VQA val set:  95%|#########5| 2548/2671 [30:15<01:26,  1.43it/s]Evaluating on VQA val set:  95%|#########5| 2549/2671 [30:16<01:27,  1.39it/s]Evaluating on VQA val set:  95%|#########5| 2550/2671 [30:16<01:27,  1.38it/s]Evaluating on VQA val set:  96%|#########5| 2551/2671 [30:17<01:29,  1.34it/s]Evaluating on VQA val set:  96%|#########5| 2552/2671 [30:18<01:28,  1.34it/s]Evaluating on VQA val set:  96%|#########5| 2553/2671 [30:19<01:26,  1.36it/s]Evaluating on VQA val set:  96%|#########5| 2554/2671 [30:19<01:24,  1.39it/s]Evaluating on VQA val set:  96%|#########5| 2555/2671 [30:20<01:21,  1.42it/s]Evaluating on VQA val set:  96%|#########5| 2556/2671 [30:21<01:19,  1.45it/s]Evaluating on VQA val set:  96%|#########5| 2557/2671 [30:21<01:19,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2558/2671 [30:22<01:20,  1.41it/s]Evaluating on VQA val set:  96%|#########5| 2559/2671 [30:23<01:18,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2560/2671 [30:23<01:16,  1.45it/s]Evaluating on VQA val set:  96%|#########5| 2561/2671 [30:24<01:17,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2562/2671 [30:25<01:15,  1.44it/s]Evaluating on VQA val set:  96%|#########5| 2563/2671 [30:26<01:14,  1.44it/s]Evaluating on VQA val set:  96%|#########5| 2564/2671 [30:26<01:14,  1.43it/s]Evaluating on VQA val set:  96%|#########6| 2565/2671 [30:27<01:12,  1.46it/s]Evaluating on VQA val set:  96%|#########6| 2566/2671 [30:28<01:10,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2567/2671 [30:28<01:12,  1.44it/s]Evaluating on VQA val set:  96%|#########6| 2568/2671 [30:29<01:09,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2569/2671 [30:30<01:08,  1.50it/s]Evaluating on VQA val set:  96%|#########6| 2570/2671 [30:30<01:09,  1.46it/s]Evaluating on VQA val set:  96%|#########6| 2571/2671 [30:31<01:06,  1.49it/s]Evaluating on VQA val set:  96%|#########6| 2572/2671 [30:32<01:07,  1.47it/s]Evaluating on VQA val set:  96%|#########6| 2573/2671 [30:32<01:06,  1.46it/s]Evaluating on VQA val set:  96%|#########6| 2574/2671 [30:33<01:04,  1.50it/s]Evaluating on VQA val set:  96%|#########6| 2575/2671 [30:34<01:06,  1.44it/s]Evaluating on VQA val set:  96%|#########6| 2576/2671 [30:34<01:06,  1.44it/s]Evaluating on VQA val set:  96%|#########6| 2577/2671 [30:35<01:05,  1.43it/s]Evaluating on VQA val set:  97%|#########6| 2578/2671 [30:36<01:05,  1.41it/s]Evaluating on VQA val set:  97%|#########6| 2579/2671 [30:37<01:05,  1.40it/s]Evaluating on VQA val set:  97%|#########6| 2580/2671 [30:37<01:04,  1.42it/s]Evaluating on VQA val set:  97%|#########6| 2581/2671 [30:38<01:04,  1.39it/s]Evaluating on VQA val set:  97%|#########6| 2582/2671 [30:39<01:03,  1.41it/s]Evaluating on VQA val set:  97%|#########6| 2583/2671 [30:40<01:04,  1.37it/s]Evaluating on VQA val set:  97%|#########6| 2584/2671 [30:40<01:02,  1.40it/s]Evaluating on VQA val set:  97%|#########6| 2585/2671 [30:41<01:02,  1.38it/s]Evaluating on VQA val set:  97%|#########6| 2586/2671 [30:42<01:02,  1.36it/s]Evaluating on VQA val set:  97%|#########6| 2587/2671 [30:42<01:00,  1.38it/s]Evaluating on VQA val set:  97%|#########6| 2588/2671 [30:43<01:01,  1.35it/s]Evaluating on VQA val set:  97%|#########6| 2589/2671 [30:44<01:00,  1.35it/s]Evaluating on VQA val set:  97%|#########6| 2590/2671 [30:45<00:59,  1.36it/s]Evaluating on VQA val set:  97%|#########7| 2591/2671 [30:45<00:58,  1.36it/s]Evaluating on VQA val set:  97%|#########7| 2592/2671 [30:46<00:57,  1.37it/s]Evaluating on VQA val set:  97%|#########7| 2593/2671 [30:47<00:55,  1.40it/s]Evaluating on VQA val set:  97%|#########7| 2594/2671 [30:47<00:54,  1.40it/s]Evaluating on VQA val set:  97%|#########7| 2595/2671 [30:48<00:53,  1.43it/s]Evaluating on VQA val set:  97%|#########7| 2596/2671 [30:49<00:51,  1.44it/s]Evaluating on VQA val set:  97%|#########7| 2597/2671 [30:50<00:52,  1.42it/s]Evaluating on VQA val set:  97%|#########7| 2598/2671 [30:50<00:51,  1.41it/s]Evaluating on VQA val set:  97%|#########7| 2599/2671 [30:51<00:50,  1.41it/s]Evaluating on VQA val set:  97%|#########7| 2600/2671 [30:52<00:51,  1.39it/s]Evaluating on VQA val set:  97%|#########7| 2601/2671 [30:52<00:50,  1.39it/s]Evaluating on VQA val set:  97%|#########7| 2602/2671 [30:53<00:49,  1.39it/s]Evaluating on VQA val set:  97%|#########7| 2603/2671 [30:54<00:47,  1.42it/s]Evaluating on VQA val set:  97%|#########7| 2604/2671 [30:55<00:46,  1.44it/s]Evaluating on VQA val set:  98%|#########7| 2605/2671 [30:55<00:43,  1.53it/s]Evaluating on VQA val set:  98%|#########7| 2606/2671 [30:56<00:44,  1.46it/s]Evaluating on VQA val set:  98%|#########7| 2607/2671 [30:57<00:44,  1.43it/s]Evaluating on VQA val set:  98%|#########7| 2608/2671 [30:57<00:44,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2609/2671 [30:58<00:44,  1.38it/s]Evaluating on VQA val set:  98%|#########7| 2610/2671 [30:59<00:43,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2611/2671 [30:59<00:42,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2612/2671 [31:00<00:42,  1.39it/s]Evaluating on VQA val set:  98%|#########7| 2613/2671 [31:01<00:40,  1.42it/s]Evaluating on VQA val set:  98%|#########7| 2614/2671 [31:02<00:41,  1.39it/s]Evaluating on VQA val set:  98%|#########7| 2615/2671 [31:02<00:39,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2616/2671 [31:03<00:39,  1.39it/s]Evaluating on VQA val set:  98%|#########7| 2617/2671 [31:04<00:39,  1.38it/s]Evaluating on VQA val set:  98%|#########8| 2618/2671 [31:04<00:37,  1.41it/s]Evaluating on VQA val set:  98%|#########8| 2619/2671 [31:05<00:36,  1.43it/s]Evaluating on VQA val set:  98%|#########8| 2620/2671 [31:06<00:34,  1.46it/s]Evaluating on VQA val set:  98%|#########8| 2621/2671 [31:06<00:34,  1.45it/s]Evaluating on VQA val set:  98%|#########8| 2622/2671 [31:07<00:33,  1.47it/s]Evaluating on VQA val set:  98%|#########8| 2623/2671 [31:08<00:32,  1.49it/s]Evaluating on VQA val set:  98%|#########8| 2624/2671 [31:08<00:31,  1.50it/s]Evaluating on VQA val set:  98%|#########8| 2625/2671 [31:09<00:31,  1.46it/s]Evaluating on VQA val set:  98%|#########8| 2626/2671 [31:10<00:31,  1.42it/s]Evaluating on VQA val set:  98%|#########8| 2627/2671 [31:11<00:31,  1.39it/s]Evaluating on VQA val set:  98%|#########8| 2628/2671 [31:11<00:31,  1.35it/s]Evaluating on VQA val set:  98%|#########8| 2629/2671 [31:12<00:29,  1.40it/s]Evaluating on VQA val set:  98%|#########8| 2630/2671 [31:13<00:28,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2631/2671 [31:14<00:28,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2632/2671 [31:14<00:27,  1.40it/s]Evaluating on VQA val set:  99%|#########8| 2633/2671 [31:15<00:27,  1.38it/s]Evaluating on VQA val set:  99%|#########8| 2634/2671 [31:16<00:25,  1.43it/s]Evaluating on VQA val set:  99%|#########8| 2635/2671 [31:16<00:25,  1.42it/s]Evaluating on VQA val set:  99%|#########8| 2636/2671 [31:17<00:24,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2637/2671 [31:18<00:24,  1.40it/s]Evaluating on VQA val set:  99%|#########8| 2638/2671 [31:18<00:23,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2639/2671 [31:19<00:22,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2640/2671 [31:20<00:20,  1.48it/s]Evaluating on VQA val set:  99%|#########8| 2641/2671 [31:21<00:21,  1.43it/s]Evaluating on VQA val set:  99%|#########8| 2642/2671 [31:21<00:20,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2643/2671 [31:22<00:19,  1.44it/s]Evaluating on VQA val set:  99%|#########8| 2644/2671 [31:23<00:17,  1.50it/s]Evaluating on VQA val set:  99%|#########9| 2645/2671 [31:23<00:17,  1.46it/s]Evaluating on VQA val set:  99%|#########9| 2646/2671 [31:24<00:17,  1.46it/s]Evaluating on VQA val set:  99%|#########9| 2647/2671 [31:25<00:16,  1.44it/s]Evaluating on VQA val set:  99%|#########9| 2648/2671 [31:25<00:16,  1.42it/s]Evaluating on VQA val set:  99%|#########9| 2649/2671 [31:26<00:15,  1.39it/s]Evaluating on VQA val set:  99%|#########9| 2650/2671 [31:27<00:14,  1.42it/s]Evaluating on VQA val set:  99%|#########9| 2651/2671 [31:28<00:14,  1.35it/s]Evaluating on VQA val set:  99%|#########9| 2652/2671 [31:28<00:13,  1.38it/s]Evaluating on VQA val set:  99%|#########9| 2653/2671 [31:29<00:12,  1.40it/s]Evaluating on VQA val set:  99%|#########9| 2654/2671 [31:30<00:12,  1.37it/s]Evaluating on VQA val set:  99%|#########9| 2655/2671 [31:30<00:11,  1.39it/s]Evaluating on VQA val set:  99%|#########9| 2656/2671 [31:31<00:10,  1.39it/s]Evaluating on VQA val set:  99%|#########9| 2657/2671 [31:32<00:09,  1.43it/s]Evaluating on VQA val set: 100%|#########9| 2658/2671 [31:33<00:09,  1.41it/s]Evaluating on VQA val set: 100%|#########9| 2659/2671 [31:33<00:08,  1.40it/s]Evaluating on VQA val set: 100%|#########9| 2660/2671 [31:34<00:08,  1.37it/s]Evaluating on VQA val set: 100%|#########9| 2661/2671 [31:35<00:07,  1.41it/s]Evaluating on VQA val set: 100%|#########9| 2662/2671 [31:35<00:06,  1.44it/s]Evaluating on VQA val set: 100%|#########9| 2663/2671 [31:36<00:05,  1.43it/s]Evaluating on VQA val set: 100%|#########9| 2664/2671 [31:37<00:04,  1.45it/s]Evaluating on VQA val set: 100%|#########9| 2665/2671 [31:37<00:04,  1.46it/s]Evaluating on VQA val set: 100%|#########9| 2666/2671 [31:38<00:03,  1.42it/s]Evaluating on VQA val set: 100%|#########9| 2667/2671 [31:39<00:02,  1.41it/s]Evaluating on VQA val set: 100%|#########9| 2668/2671 [31:40<00:02,  1.41it/s]Evaluating on VQA val set: 100%|#########9| 2669/2671 [31:40<00:01,  1.42it/s]Evaluating on VQA val set: 100%|#########9| 2670/2671 [31:41<00:00,  1.42it/s]Evaluating on VQA val set: 100%|##########| 2671/2671 [31:41<00:00,  1.63it/s]Evaluating on VQA val set: 100%|##########| 2671/2671 [31:41<00:00,  1.40it/s]
11/16/2022 22:53:57 - INFO - train.train_vqa - Evaluation after epoch 1: 58.18
11/16/2022 22:53:57 - INFO - train.train_vqa - New best evaluation score: 58.18
11/16/2022 22:53:57 - INFO - __main__ - Best VQAv2 evaluation score = 58.18, after epoch 1
11/16/2022 22:53:57 - INFO - __main__ - Saving best model and encoder checkpoint after VQAv2 training
11/16/2022 22:53:59 - INFO - __main__ - Saved checkpoint!
11/16/2022 22:53:59 - INFO - __main__ - Saved continual learning results so far!
11/16/2022 22:53:59 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/16/2022 22:53:59 - INFO - __main__ - ********************** found the task token with same task key! *****************************
11/16/2022 22:53:59 - INFO - __main__ - Training vilt model on task #2: SNLI-VE
11/16/2022 22:53:59 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE train dataloader with batch size of 32
0it [00:00, ?it/s]830it [00:00, 8299.26it/s]2253it [00:00, 11439.76it/s]7015it [00:00, 27690.48it/s]14887it [00:00, 47384.60it/s]23311it [00:00, 60543.19it/s]31837it [00:00, 68891.38it/s]40304it [00:00, 74024.04it/s]48521it [00:00, 76607.84it/s]57015it [00:00, 79205.88it/s]65955it [00:01, 82346.63it/s]75008it [00:01, 84844.84it/s]83645it [00:01, 85307.32it/s]92241it [00:01, 85501.56it/s]101227it [00:01, 86813.27it/s]109911it [00:01, 85935.75it/s]118508it [00:01, 85420.70it/s]127181it [00:01, 85809.92it/s]136070it [00:01, 86726.44it/s]144745it [00:01, 85579.78it/s]153410it [00:02, 85894.17it/s]162187it [00:02, 86449.93it/s]170835it [00:02, 85756.39it/s]179414it [00:02, 85076.89it/s]188243it [00:02, 86027.95it/s]196977it [00:02, 86415.68it/s]205622it [00:02, 86195.62it/s]214382it [00:02, 86611.70it/s]223109it [00:02, 86807.12it/s]231808it [00:02, 86749.87it/s]240484it [00:03, 86197.04it/s]249195it [00:03, 86466.99it/s]258315it [00:03, 87879.54it/s]267105it [00:03, 87769.67it/s]275883it [00:03, 87445.91it/s]284629it [00:04, 33557.55it/s]293199it [00:04, 40890.55it/s]301990it [00:04, 48743.06it/s]310538it [00:04, 55827.19it/s]319106it [00:04, 62249.26it/s]327640it [00:04, 67691.16it/s]336447it [00:04, 72824.11it/s]345311it [00:04, 77008.12it/s]354210it [00:04, 80293.73it/s]362998it [00:04, 82390.24it/s]371721it [00:05, 83776.41it/s]380418it [00:05, 83986.66it/s]389206it [00:05, 85072.45it/s]398056it [00:05, 86078.18it/s]407123it [00:05, 87434.55it/s]415949it [00:05, 87457.96it/s]424752it [00:05, 86204.76it/s]433417it [00:05, 86091.12it/s]442057it [00:05, 85805.06it/s]450815it [00:05, 86327.96it/s]459679it [00:06, 87012.72it/s]468750it [00:06, 88112.96it/s]477598it [00:06, 88221.52it/s]486427it [00:06, 88236.24it/s]495265it [00:06, 88277.02it/s]504096it [00:06, 86713.69it/s]512776it [00:06, 86181.48it/s]521492it [00:06, 86468.55it/s]529527it [00:06, 76974.02it/s]
11/16/2022 22:54:06 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE train dataset, with 214371 examples
11/16/2022 22:54:06 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Creating SNLI-VE dev dataloader with batch size of 32
0it [00:00, ?it/s]1130it [00:00, 8786.60it/s]2851it [00:00, 13120.38it/s]7634it [00:00, 27950.75it/s]16325it [00:00, 50297.88it/s]17858it [00:00, 39829.90it/s]
11/16/2022 22:54:07 - INFO - data.visionlanguage_datasets.snli_ve_dataset - Loaded SNLI-VE dev dataset, with 6855 examples
Training epoch 1:   0%|          | 0/6700 [00:00<?, ?it/s]/home1/caiyulia/.conda/envs/climb/lib/python3.6/site-packages/torch/nn/functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  "reduction: 'mean' divides the total loss by both the batch size and the support size."
11/16/2022 22:54:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:09 - INFO - train.train_snli_ve - loss is tensor(8.3235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 1/6700 [00:02<4:28:32,  2.41s/it]11/16/2022 22:54:11 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:11 - INFO - train.train_snli_ve - loss is tensor(8.2891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 2/6700 [00:04<3:43:14,  2.00s/it]11/16/2022 22:54:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:13 - INFO - train.train_snli_ve - loss is tensor(8.2608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 3/6700 [00:05<3:29:54,  1.88s/it]11/16/2022 22:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:14 - INFO - train.train_snli_ve - loss is tensor(8.3143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 4/6700 [00:07<3:21:09,  1.80s/it]11/16/2022 22:54:16 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:16 - INFO - train.train_snli_ve - loss is tensor(8.2706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 5/6700 [00:09<3:18:44,  1.78s/it]11/16/2022 22:54:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:18 - INFO - train.train_snli_ve - loss is tensor(8.3158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 6/6700 [00:10<3:12:57,  1.73s/it]11/16/2022 22:54:19 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:19 - INFO - train.train_snli_ve - loss is tensor(8.1449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 7/6700 [00:12<3:13:22,  1.73s/it]11/16/2022 22:54:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:21 - INFO - train.train_snli_ve - loss is tensor(8.3342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 8/6700 [00:14<3:12:31,  1.73s/it]11/16/2022 22:54:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:23 - INFO - train.train_snli_ve - loss is tensor(8.1479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 9/6700 [00:16<3:13:26,  1.73s/it]11/16/2022 22:54:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:25 - INFO - train.train_snli_ve - loss is tensor(8.1612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 10/6700 [00:17<3:08:43,  1.69s/it]11/16/2022 22:54:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:26 - INFO - train.train_snli_ve - loss is tensor(8.1935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 11/6700 [00:19<3:11:44,  1.72s/it]11/16/2022 22:54:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:28 - INFO - train.train_snli_ve - loss is tensor(8.1809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 12/6700 [00:21<3:08:50,  1.69s/it]11/16/2022 22:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:30 - INFO - train.train_snli_ve - loss is tensor(8.2704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 13/6700 [00:22<3:08:49,  1.69s/it]11/16/2022 22:54:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:31 - INFO - train.train_snli_ve - loss is tensor(8.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 14/6700 [00:24<3:07:42,  1.68s/it]11/16/2022 22:54:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:33 - INFO - train.train_snli_ve - loss is tensor(8.1834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 15/6700 [00:26<3:11:47,  1.72s/it]11/16/2022 22:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:35 - INFO - train.train_snli_ve - loss is tensor(8.1375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 16/6700 [00:27<3:10:27,  1.71s/it]11/16/2022 22:54:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:37 - INFO - train.train_snli_ve - loss is tensor(8.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 17/6700 [00:29<3:10:27,  1.71s/it]11/16/2022 22:54:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:38 - INFO - train.train_snli_ve - loss is tensor(8.0772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 18/6700 [00:31<3:08:56,  1.70s/it]11/16/2022 22:54:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:40 - INFO - train.train_snli_ve - loss is tensor(8.1199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 19/6700 [00:33<3:10:22,  1.71s/it]11/16/2022 22:54:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:42 - INFO - train.train_snli_ve - loss is tensor(8.1566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 20/6700 [00:34<3:07:27,  1.68s/it]11/16/2022 22:54:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:43 - INFO - train.train_snli_ve - loss is tensor(8.0646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 21/6700 [00:36<3:11:12,  1.72s/it]11/16/2022 22:54:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:45 - INFO - train.train_snli_ve - loss is tensor(7.9342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 22/6700 [00:38<3:09:21,  1.70s/it]11/16/2022 22:54:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:47 - INFO - train.train_snli_ve - loss is tensor(8.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 23/6700 [00:39<3:09:49,  1.71s/it]11/16/2022 22:54:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:48 - INFO - train.train_snli_ve - loss is tensor(8.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 24/6700 [00:41<3:07:56,  1.69s/it]11/16/2022 22:54:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:50 - INFO - train.train_snli_ve - loss is tensor(7.8453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 25/6700 [00:43<3:08:38,  1.70s/it]11/16/2022 22:54:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:52 - INFO - train.train_snli_ve - loss is tensor(7.9179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 26/6700 [00:44<3:07:00,  1.68s/it]11/16/2022 22:54:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:53 - INFO - train.train_snli_ve - loss is tensor(7.8536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 27/6700 [00:46<3:07:16,  1.68s/it]11/16/2022 22:54:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:55 - INFO - train.train_snli_ve - loss is tensor(7.8332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 28/6700 [00:48<3:05:24,  1.67s/it]11/16/2022 22:54:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:57 - INFO - train.train_snli_ve - loss is tensor(7.8262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 29/6700 [00:49<3:07:39,  1.69s/it]11/16/2022 22:54:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:54:58 - INFO - train.train_snli_ve - loss is tensor(7.7828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 30/6700 [00:51<3:05:08,  1.67s/it]11/16/2022 22:55:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:00 - INFO - train.train_snli_ve - loss is tensor(7.7221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 31/6700 [00:53<3:06:41,  1.68s/it]11/16/2022 22:55:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:02 - INFO - train.train_snli_ve - loss is tensor(7.6717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 32/6700 [00:54<3:04:53,  1.66s/it]11/16/2022 22:55:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:03 - INFO - train.train_snli_ve - loss is tensor(7.5794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   0%|          | 33/6700 [00:56<3:08:31,  1.70s/it]11/16/2022 22:55:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:05 - INFO - train.train_snli_ve - loss is tensor(7.5699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 34/6700 [00:58<3:07:50,  1.69s/it]11/16/2022 22:55:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:07 - INFO - train.train_snli_ve - loss is tensor(7.4677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 35/6700 [01:00<3:09:37,  1.71s/it]11/16/2022 22:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:09 - INFO - train.train_snli_ve - loss is tensor(7.5430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 36/6700 [01:01<3:08:52,  1.70s/it]11/16/2022 22:55:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:10 - INFO - train.train_snli_ve - loss is tensor(7.4276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 37/6700 [01:03<3:08:52,  1.70s/it]11/16/2022 22:55:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:12 - INFO - train.train_snli_ve - loss is tensor(7.4094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 38/6700 [01:05<3:06:31,  1.68s/it]11/16/2022 22:55:14 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:14 - INFO - train.train_snli_ve - loss is tensor(7.4097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 39/6700 [01:06<3:08:18,  1.70s/it]11/16/2022 22:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:15 - INFO - train.train_snli_ve - loss is tensor(7.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 40/6700 [01:08<3:06:31,  1.68s/it]11/16/2022 22:55:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:17 - INFO - train.train_snli_ve - loss is tensor(7.2640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 41/6700 [01:10<3:09:58,  1.71s/it]11/16/2022 22:55:19 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:19 - INFO - train.train_snli_ve - loss is tensor(7.1979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 42/6700 [01:11<3:07:08,  1.69s/it]11/16/2022 22:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:20 - INFO - train.train_snli_ve - loss is tensor(7.0825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 43/6700 [01:13<3:08:46,  1.70s/it]11/16/2022 22:55:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:22 - INFO - train.train_snli_ve - loss is tensor(7.1197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 44/6700 [01:15<3:07:30,  1.69s/it]11/16/2022 22:55:24 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:24 - INFO - train.train_snli_ve - loss is tensor(6.9380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 45/6700 [01:17<3:10:39,  1.72s/it]11/16/2022 22:55:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:26 - INFO - train.train_snli_ve - loss is tensor(6.9858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 46/6700 [01:18<3:07:40,  1.69s/it]11/16/2022 22:55:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:27 - INFO - train.train_snli_ve - loss is tensor(6.8712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 47/6700 [01:20<3:08:40,  1.70s/it]11/16/2022 22:55:29 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:29 - INFO - train.train_snli_ve - loss is tensor(6.8288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 48/6700 [01:22<3:06:47,  1.68s/it]11/16/2022 22:55:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:31 - INFO - train.train_snli_ve - loss is tensor(6.8255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 49/6700 [01:23<3:09:11,  1.71s/it]11/16/2022 22:55:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:32 - INFO - train.train_snli_ve - loss is tensor(6.7270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 50/6700 [01:25<3:09:06,  1.71s/it]11/16/2022 22:55:34 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:34 - INFO - train.train_snli_ve - loss is tensor(6.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 51/6700 [01:27<3:11:21,  1.73s/it]11/16/2022 22:55:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:36 - INFO - train.train_snli_ve - loss is tensor(6.3929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 52/6700 [01:28<3:06:44,  1.69s/it]11/16/2022 22:55:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:37 - INFO - train.train_snli_ve - loss is tensor(6.5324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 53/6700 [01:30<3:07:59,  1.70s/it]11/16/2022 22:55:39 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:39 - INFO - train.train_snli_ve - loss is tensor(6.3049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 54/6700 [01:32<3:05:50,  1.68s/it]11/16/2022 22:55:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:41 - INFO - train.train_snli_ve - loss is tensor(6.3522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 55/6700 [01:34<3:07:16,  1.69s/it]11/16/2022 22:55:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:42 - INFO - train.train_snli_ve - loss is tensor(6.2283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 56/6700 [01:35<3:07:13,  1.69s/it]11/16/2022 22:55:44 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:44 - INFO - train.train_snli_ve - loss is tensor(6.2453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 57/6700 [01:37<3:08:23,  1.70s/it]11/16/2022 22:55:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:46 - INFO - train.train_snli_ve - loss is tensor(6.0303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 58/6700 [01:39<3:08:02,  1.70s/it]11/16/2022 22:55:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:48 - INFO - train.train_snli_ve - loss is tensor(6.2580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 59/6700 [01:40<3:08:08,  1.70s/it]11/16/2022 22:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:49 - INFO - train.train_snli_ve - loss is tensor(6.0876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 60/6700 [01:42<3:06:05,  1.68s/it]11/16/2022 22:55:51 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:51 - INFO - train.train_snli_ve - loss is tensor(5.9730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 61/6700 [01:44<3:07:35,  1.70s/it]11/16/2022 22:55:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:53 - INFO - train.train_snli_ve - loss is tensor(5.7672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 62/6700 [01:45<3:06:09,  1.68s/it]11/16/2022 22:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:54 - INFO - train.train_snli_ve - loss is tensor(5.5089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 63/6700 [01:47<3:08:10,  1.70s/it]11/16/2022 22:55:56 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:56 - INFO - train.train_snli_ve - loss is tensor(5.7086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 64/6700 [01:49<3:06:23,  1.69s/it]11/16/2022 22:55:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:58 - INFO - train.train_snli_ve - loss is tensor(5.1926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 65/6700 [01:50<3:06:58,  1.69s/it]11/16/2022 22:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:55:59 - INFO - train.train_snli_ve - loss is tensor(5.5926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|          | 66/6700 [01:52<3:05:25,  1.68s/it]11/16/2022 22:56:01 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:01 - INFO - train.train_snli_ve - loss is tensor(5.4391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 67/6700 [01:54<3:07:19,  1.69s/it]11/16/2022 22:56:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:03 - INFO - train.train_snli_ve - loss is tensor(5.1357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 68/6700 [01:55<3:05:41,  1.68s/it]11/16/2022 22:56:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:05 - INFO - train.train_snli_ve - loss is tensor(5.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 69/6700 [01:57<3:08:05,  1.70s/it]11/16/2022 22:56:06 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:06 - INFO - train.train_snli_ve - loss is tensor(5.1053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 70/6700 [01:59<3:05:20,  1.68s/it]11/16/2022 22:56:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:08 - INFO - train.train_snli_ve - loss is tensor(5.0891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 71/6700 [02:01<3:05:22,  1.68s/it]11/16/2022 22:56:09 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:09 - INFO - train.train_snli_ve - loss is tensor(5.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 72/6700 [02:02<3:04:44,  1.67s/it]11/16/2022 22:56:11 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:11 - INFO - train.train_snli_ve - loss is tensor(5.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 73/6700 [02:04<3:06:08,  1.69s/it]11/16/2022 22:56:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:13 - INFO - train.train_snli_ve - loss is tensor(4.9741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 74/6700 [02:06<3:05:21,  1.68s/it]11/16/2022 22:56:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:15 - INFO - train.train_snli_ve - loss is tensor(4.4585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 75/6700 [02:07<3:05:19,  1.68s/it]11/16/2022 22:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:16 - INFO - train.train_snli_ve - loss is tensor(4.7613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 76/6700 [02:09<3:03:38,  1.66s/it]11/16/2022 22:56:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:18 - INFO - train.train_snli_ve - loss is tensor(4.4627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 77/6700 [02:11<3:06:49,  1.69s/it]11/16/2022 22:56:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:20 - INFO - train.train_snli_ve - loss is tensor(4.4062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 78/6700 [02:12<3:04:55,  1.68s/it]11/16/2022 22:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:21 - INFO - train.train_snli_ve - loss is tensor(4.2494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 79/6700 [02:14<3:06:35,  1.69s/it]11/16/2022 22:56:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:23 - INFO - train.train_snli_ve - loss is tensor(4.3639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 80/6700 [02:16<3:04:34,  1.67s/it]11/16/2022 22:56:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:25 - INFO - train.train_snli_ve - loss is tensor(4.0327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 81/6700 [02:17<3:05:03,  1.68s/it]11/16/2022 22:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:26 - INFO - train.train_snli_ve - loss is tensor(4.1495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 82/6700 [02:19<3:03:36,  1.66s/it]11/16/2022 22:56:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:28 - INFO - train.train_snli_ve - loss is tensor(4.0614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 83/6700 [02:21<3:04:35,  1.67s/it]11/16/2022 22:56:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:30 - INFO - train.train_snli_ve - loss is tensor(4.1460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 84/6700 [02:22<3:02:37,  1.66s/it]11/16/2022 22:56:31 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:31 - INFO - train.train_snli_ve - loss is tensor(4.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 85/6700 [02:24<3:04:36,  1.67s/it]11/16/2022 22:56:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:33 - INFO - train.train_snli_ve - loss is tensor(3.7105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 86/6700 [02:26<3:04:12,  1.67s/it]11/16/2022 22:56:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:35 - INFO - train.train_snli_ve - loss is tensor(3.7097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 87/6700 [02:27<3:05:35,  1.68s/it]11/16/2022 22:56:36 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:36 - INFO - train.train_snli_ve - loss is tensor(3.4160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 88/6700 [02:29<3:04:39,  1.68s/it]11/16/2022 22:56:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:38 - INFO - train.train_snli_ve - loss is tensor(3.4191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 89/6700 [02:31<3:06:00,  1.69s/it]11/16/2022 22:56:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:40 - INFO - train.train_snli_ve - loss is tensor(3.5030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 90/6700 [02:32<3:03:36,  1.67s/it]11/16/2022 22:56:41 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:41 - INFO - train.train_snli_ve - loss is tensor(3.3746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 91/6700 [02:34<3:05:56,  1.69s/it]11/16/2022 22:56:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:43 - INFO - train.train_snli_ve - loss is tensor(3.3446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 92/6700 [02:36<3:04:32,  1.68s/it]11/16/2022 22:56:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:45 - INFO - train.train_snli_ve - loss is tensor(3.2815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 93/6700 [02:37<3:05:17,  1.68s/it]11/16/2022 22:56:46 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:46 - INFO - train.train_snli_ve - loss is tensor(3.1733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 94/6700 [02:39<3:02:56,  1.66s/it]11/16/2022 22:56:48 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:48 - INFO - train.train_snli_ve - loss is tensor(3.1911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 95/6700 [02:41<3:05:57,  1.69s/it]11/16/2022 22:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:50 - INFO - train.train_snli_ve - loss is tensor(3.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 96/6700 [02:42<3:04:50,  1.68s/it]11/16/2022 22:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:52 - INFO - train.train_snli_ve - loss is tensor(2.8817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 97/6700 [02:44<3:07:30,  1.70s/it]11/16/2022 22:56:53 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:53 - INFO - train.train_snli_ve - loss is tensor(3.0066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 98/6700 [02:46<3:05:12,  1.68s/it]11/16/2022 22:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:55 - INFO - train.train_snli_ve - loss is tensor(3.0766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 99/6700 [02:48<3:07:15,  1.70s/it]11/16/2022 22:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:57 - INFO - train.train_snli_ve - loss is tensor(2.8671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   1%|1         | 100/6700 [02:49<3:05:26,  1.69s/it]11/16/2022 22:56:58 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:56:58 - INFO - train.train_snli_ve - loss is tensor(2.7830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 101/6700 [02:51<3:06:02,  1.69s/it]11/16/2022 22:57:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:00 - INFO - train.train_snli_ve - loss is tensor(2.6056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 102/6700 [02:53<3:04:52,  1.68s/it]11/16/2022 22:57:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:02 - INFO - train.train_snli_ve - loss is tensor(2.7680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 103/6700 [02:54<3:05:56,  1.69s/it]11/16/2022 22:57:03 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:03 - INFO - train.train_snli_ve - loss is tensor(2.3367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 104/6700 [02:56<3:04:35,  1.68s/it]11/16/2022 22:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:05 - INFO - train.train_snli_ve - loss is tensor(2.4344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 105/6700 [02:58<3:04:58,  1.68s/it]11/16/2022 22:57:07 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:07 - INFO - train.train_snli_ve - loss is tensor(2.2300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 106/6700 [02:59<3:04:06,  1.68s/it]11/16/2022 22:57:08 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:08 - INFO - train.train_snli_ve - loss is tensor(2.5198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 107/6700 [03:01<3:05:39,  1.69s/it]11/16/2022 22:57:10 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:10 - INFO - train.train_snli_ve - loss is tensor(2.4168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 108/6700 [03:03<3:03:31,  1.67s/it]11/16/2022 22:57:12 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:12 - INFO - train.train_snli_ve - loss is tensor(2.4446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 109/6700 [03:04<3:04:57,  1.68s/it]11/16/2022 22:57:13 - INFO - train.train_snli_ve - kd_loss is tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:13 - INFO - train.train_snli_ve - loss is tensor(2.2936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 110/6700 [03:06<3:04:07,  1.68s/it]11/16/2022 22:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:15 - INFO - train.train_snli_ve - loss is tensor(2.0953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 111/6700 [03:08<3:05:21,  1.69s/it]11/16/2022 22:57:17 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:17 - INFO - train.train_snli_ve - loss is tensor(2.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 112/6700 [03:09<3:03:59,  1.68s/it]11/16/2022 22:57:18 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:18 - INFO - train.train_snli_ve - loss is tensor(2.3156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 113/6700 [03:11<3:04:03,  1.68s/it]11/16/2022 22:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:20 - INFO - train.train_snli_ve - loss is tensor(2.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 114/6700 [03:13<3:02:57,  1.67s/it]11/16/2022 22:57:22 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:22 - INFO - train.train_snli_ve - loss is tensor(2.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 115/6700 [03:14<3:03:52,  1.68s/it]11/16/2022 22:57:23 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:23 - INFO - train.train_snli_ve - loss is tensor(1.9271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 116/6700 [03:16<3:01:30,  1.65s/it]11/16/2022 22:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:25 - INFO - train.train_snli_ve - loss is tensor(1.8808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 117/6700 [03:18<3:03:26,  1.67s/it]11/16/2022 22:57:27 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:27 - INFO - train.train_snli_ve - loss is tensor(1.9272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 118/6700 [03:19<3:01:17,  1.65s/it]11/16/2022 22:57:28 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:28 - INFO - train.train_snli_ve - loss is tensor(1.8515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 119/6700 [03:21<3:04:14,  1.68s/it]11/16/2022 22:57:30 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:30 - INFO - train.train_snli_ve - loss is tensor(1.8918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 120/6700 [03:23<3:04:14,  1.68s/it]11/16/2022 22:57:32 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:32 - INFO - train.train_snli_ve - loss is tensor(1.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 121/6700 [03:24<3:05:42,  1.69s/it]11/16/2022 22:57:33 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:33 - INFO - train.train_snli_ve - loss is tensor(1.7302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 122/6700 [03:26<3:02:46,  1.67s/it]11/16/2022 22:57:35 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:35 - INFO - train.train_snli_ve - loss is tensor(1.9132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 123/6700 [03:28<3:04:09,  1.68s/it]11/16/2022 22:57:37 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:37 - INFO - train.train_snli_ve - loss is tensor(1.8485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 124/6700 [03:29<3:02:46,  1.67s/it]11/16/2022 22:57:38 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:38 - INFO - train.train_snli_ve - loss is tensor(1.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 125/6700 [03:31<3:04:42,  1.69s/it]11/16/2022 22:57:40 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:40 - INFO - train.train_snli_ve - loss is tensor(1.6696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 126/6700 [03:33<3:02:44,  1.67s/it]11/16/2022 22:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:42 - INFO - train.train_snli_ve - loss is tensor(1.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 127/6700 [03:35<3:04:31,  1.68s/it]11/16/2022 22:57:43 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:43 - INFO - train.train_snli_ve - loss is tensor(1.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 128/6700 [03:36<3:03:14,  1.67s/it]11/16/2022 22:57:45 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:45 - INFO - train.train_snli_ve - loss is tensor(1.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 129/6700 [03:38<3:05:49,  1.70s/it]11/16/2022 22:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:47 - INFO - train.train_snli_ve - loss is tensor(1.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 130/6700 [03:40<3:03:55,  1.68s/it]11/16/2022 22:57:49 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:49 - INFO - train.train_snli_ve - loss is tensor(1.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 131/6700 [03:41<3:05:44,  1.70s/it]11/16/2022 22:57:50 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:50 - INFO - train.train_snli_ve - loss is tensor(1.6189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 132/6700 [03:43<3:04:45,  1.69s/it]11/16/2022 22:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:52 - INFO - train.train_snli_ve - loss is tensor(1.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|1         | 133/6700 [03:45<3:04:14,  1.68s/it]11/16/2022 22:57:54 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:54 - INFO - train.train_snli_ve - loss is tensor(1.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 134/6700 [03:46<3:02:47,  1.67s/it]11/16/2022 22:57:55 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:55 - INFO - train.train_snli_ve - loss is tensor(1.5103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 135/6700 [03:48<3:04:02,  1.68s/it]11/16/2022 22:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:57 - INFO - train.train_snli_ve - loss is tensor(1.4508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 136/6700 [03:50<3:04:13,  1.68s/it]11/16/2022 22:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:57:59 - INFO - train.train_snli_ve - loss is tensor(1.4973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 137/6700 [03:51<3:04:24,  1.69s/it]11/16/2022 22:58:00 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:00 - INFO - train.train_snli_ve - loss is tensor(1.4703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 138/6700 [03:53<3:01:45,  1.66s/it]11/16/2022 22:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:02 - INFO - train.train_snli_ve - loss is tensor(1.5772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 139/6700 [03:55<3:04:44,  1.69s/it]11/16/2022 22:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.7136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:04 - INFO - train.train_snli_ve - loss is tensor(1.4364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 140/6700 [03:56<3:02:48,  1.67s/it]11/16/2022 22:58:05 - INFO - train.train_snli_ve - kd_loss is tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:05 - INFO - train.train_snli_ve - loss is tensor(1.4120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 141/6700 [03:58<3:03:40,  1.68s/it]11/16/2022 22:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.6454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:07 - INFO - train.train_snli_ve - loss is tensor(1.4793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 142/6700 [04:00<3:01:03,  1.66s/it]11/16/2022 22:58:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.7229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:09 - INFO - train.train_snli_ve - loss is tensor(1.3413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 143/6700 [04:01<3:03:16,  1.68s/it]11/16/2022 22:58:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.2350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:10 - INFO - train.train_snli_ve - loss is tensor(1.3379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 144/6700 [04:03<3:03:24,  1.68s/it]11/16/2022 22:58:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.8756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:12 - INFO - train.train_snli_ve - loss is tensor(1.2474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 145/6700 [04:05<3:04:21,  1.69s/it]11/16/2022 22:58:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.5610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:14 - INFO - train.train_snli_ve - loss is tensor(1.3917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 146/6700 [04:06<3:00:55,  1.66s/it]11/16/2022 22:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.8407e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:15 - INFO - train.train_snli_ve - loss is tensor(1.2434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 147/6700 [04:08<3:02:09,  1.67s/it]11/16/2022 22:58:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.4648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:17 - INFO - train.train_snli_ve - loss is tensor(1.2351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 148/6700 [04:10<3:02:38,  1.67s/it]11/16/2022 22:58:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.1640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:19 - INFO - train.train_snli_ve - loss is tensor(1.3059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 149/6700 [04:11<3:04:36,  1.69s/it]11/16/2022 22:58:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.3572e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:20 - INFO - train.train_snli_ve - loss is tensor(1.2552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 150/6700 [04:13<3:02:59,  1.68s/it]11/16/2022 22:58:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.2098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:22 - INFO - train.train_snli_ve - loss is tensor(0.9840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 151/6700 [04:15<3:03:58,  1.69s/it]11/16/2022 22:58:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.2849e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:24 - INFO - train.train_snli_ve - loss is tensor(1.1948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 152/6700 [04:16<3:02:46,  1.67s/it]11/16/2022 22:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.3104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:26 - INFO - train.train_snli_ve - loss is tensor(1.1541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 153/6700 [04:18<3:05:04,  1.70s/it]11/16/2022 22:58:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.8915e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:27 - INFO - train.train_snli_ve - loss is tensor(1.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 154/6700 [04:20<3:02:10,  1.67s/it]11/16/2022 22:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.2232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:29 - INFO - train.train_snli_ve - loss is tensor(1.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 155/6700 [04:21<3:02:08,  1.67s/it]11/16/2022 22:58:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.0281e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:30 - INFO - train.train_snli_ve - loss is tensor(1.2450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 156/6700 [04:23<3:01:38,  1.67s/it]11/16/2022 22:58:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.4398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:32 - INFO - train.train_snli_ve - loss is tensor(1.0977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 157/6700 [04:25<3:03:12,  1.68s/it]11/16/2022 22:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.6686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:34 - INFO - train.train_snli_ve - loss is tensor(1.3254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 158/6700 [04:26<2:59:59,  1.65s/it]11/16/2022 22:58:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.3355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:35 - INFO - train.train_snli_ve - loss is tensor(1.4510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 159/6700 [04:28<3:02:42,  1.68s/it]11/16/2022 22:58:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.9896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:37 - INFO - train.train_snli_ve - loss is tensor(1.2078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 160/6700 [04:30<3:03:16,  1.68s/it]11/16/2022 22:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.3275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:39 - INFO - train.train_snli_ve - loss is tensor(1.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 161/6700 [04:32<3:05:09,  1.70s/it]11/16/2022 22:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.2576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:41 - INFO - train.train_snli_ve - loss is tensor(1.2010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 162/6700 [04:33<3:02:57,  1.68s/it]11/16/2022 22:58:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.1976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:42 - INFO - train.train_snli_ve - loss is tensor(1.2053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 163/6700 [04:35<3:02:53,  1.68s/it]11/16/2022 22:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(6.9577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:44 - INFO - train.train_snli_ve - loss is tensor(1.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 164/6700 [04:37<3:00:11,  1.65s/it]11/16/2022 22:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.7286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:46 - INFO - train.train_snli_ve - loss is tensor(1.3231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 165/6700 [04:38<3:01:56,  1.67s/it]11/16/2022 22:58:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.3720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:47 - INFO - train.train_snli_ve - loss is tensor(1.0380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 166/6700 [04:40<3:01:54,  1.67s/it]11/16/2022 22:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.6811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:49 - INFO - train.train_snli_ve - loss is tensor(1.2693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   2%|2         | 167/6700 [04:42<3:03:26,  1.68s/it]11/16/2022 22:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.3929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:51 - INFO - train.train_snli_ve - loss is tensor(1.1054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 168/6700 [04:43<3:01:01,  1.66s/it]11/16/2022 22:58:52 - INFO - train.train_snli_ve - kd_loss is tensor(6.2665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:52 - INFO - train.train_snli_ve - loss is tensor(0.9714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 169/6700 [04:45<3:03:05,  1.68s/it]11/16/2022 22:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.3900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:54 - INFO - train.train_snli_ve - loss is tensor(1.0437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 170/6700 [04:47<3:01:12,  1.67s/it]11/16/2022 22:58:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.1216e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:56 - INFO - train.train_snli_ve - loss is tensor(1.0634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 171/6700 [04:48<3:01:18,  1.67s/it]11/16/2022 22:58:57 - INFO - train.train_snli_ve - kd_loss is tensor(6.0825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:57 - INFO - train.train_snli_ve - loss is tensor(1.0000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 172/6700 [04:50<3:00:24,  1.66s/it]11/16/2022 22:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.8334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:58:59 - INFO - train.train_snli_ve - loss is tensor(1.1451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 173/6700 [04:52<3:01:50,  1.67s/it]11/16/2022 22:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.6868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:01 - INFO - train.train_snli_ve - loss is tensor(1.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 174/6700 [04:53<3:02:21,  1.68s/it]11/16/2022 22:59:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.5610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:02 - INFO - train.train_snli_ve - loss is tensor(1.2528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 175/6700 [04:55<3:03:30,  1.69s/it]11/16/2022 22:59:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.7985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:04 - INFO - train.train_snli_ve - loss is tensor(1.1007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 176/6700 [04:57<3:00:22,  1.66s/it]11/16/2022 22:59:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.5031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:06 - INFO - train.train_snli_ve - loss is tensor(1.0523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 177/6700 [04:58<3:02:22,  1.68s/it]11/16/2022 22:59:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.9063e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:07 - INFO - train.train_snli_ve - loss is tensor(1.0460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 178/6700 [05:00<3:01:19,  1.67s/it]11/16/2022 22:59:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.5219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:09 - INFO - train.train_snli_ve - loss is tensor(1.1207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 179/6700 [05:02<3:03:09,  1.69s/it]11/16/2022 22:59:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.5838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:11 - INFO - train.train_snli_ve - loss is tensor(1.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 180/6700 [05:03<3:02:23,  1.68s/it]11/16/2022 22:59:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.1998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:12 - INFO - train.train_snli_ve - loss is tensor(1.0442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 181/6700 [05:05<3:03:12,  1.69s/it]11/16/2022 22:59:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.3075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:14 - INFO - train.train_snli_ve - loss is tensor(1.0150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 182/6700 [05:07<3:01:15,  1.67s/it]11/16/2022 22:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.3445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:16 - INFO - train.train_snli_ve - loss is tensor(1.2057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 183/6700 [05:08<3:02:13,  1.68s/it]11/16/2022 22:59:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.2970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:17 - INFO - train.train_snli_ve - loss is tensor(1.0727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 184/6700 [05:10<2:59:29,  1.65s/it]11/16/2022 22:59:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.9812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:19 - INFO - train.train_snli_ve - loss is tensor(1.0027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 185/6700 [05:12<3:00:10,  1.66s/it]11/16/2022 22:59:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.9152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:21 - INFO - train.train_snli_ve - loss is tensor(0.9193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 186/6700 [05:13<3:00:20,  1.66s/it]11/16/2022 22:59:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.8259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:22 - INFO - train.train_snli_ve - loss is tensor(1.1114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 187/6700 [05:15<3:02:30,  1.68s/it]11/16/2022 22:59:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.9279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:24 - INFO - train.train_snli_ve - loss is tensor(0.9027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 188/6700 [05:17<3:01:06,  1.67s/it]11/16/2022 22:59:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.6921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:26 - INFO - train.train_snli_ve - loss is tensor(1.2326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 189/6700 [05:18<3:03:07,  1.69s/it]11/16/2022 22:59:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.6395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:27 - INFO - train.train_snli_ve - loss is tensor(1.0879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 190/6700 [05:20<3:01:46,  1.68s/it]11/16/2022 22:59:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.9886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:29 - INFO - train.train_snli_ve - loss is tensor(0.9348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 191/6700 [05:22<3:03:28,  1.69s/it]11/16/2022 22:59:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.4460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:31 - INFO - train.train_snli_ve - loss is tensor(1.0362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 192/6700 [05:23<3:01:50,  1.68s/it]11/16/2022 22:59:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.5104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:32 - INFO - train.train_snli_ve - loss is tensor(1.0343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 193/6700 [05:25<3:02:51,  1.69s/it]11/16/2022 22:59:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.4492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:34 - INFO - train.train_snli_ve - loss is tensor(1.2125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 194/6700 [05:27<3:02:47,  1.69s/it]11/16/2022 22:59:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.4127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:36 - INFO - train.train_snli_ve - loss is tensor(1.0575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 195/6700 [05:29<3:03:32,  1.69s/it]11/16/2022 22:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.2389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:37 - INFO - train.train_snli_ve - loss is tensor(1.2540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 196/6700 [05:30<3:02:09,  1.68s/it]11/16/2022 22:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:39 - INFO - train.train_snli_ve - loss is tensor(1.0353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 197/6700 [05:32<3:02:42,  1.69s/it]11/16/2022 22:59:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.1368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:41 - INFO - train.train_snli_ve - loss is tensor(1.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 198/6700 [05:34<3:02:26,  1.68s/it]11/16/2022 22:59:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.1921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:43 - INFO - train.train_snli_ve - loss is tensor(1.1649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 199/6700 [05:35<3:03:19,  1.69s/it]11/16/2022 22:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.0912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:44 - INFO - train.train_snli_ve - loss is tensor(0.9649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|2         | 200/6700 [05:37<3:01:55,  1.68s/it]11/16/2022 22:59:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.0734e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:46 - INFO - train.train_snli_ve - loss is tensor(0.9216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 201/6700 [05:39<3:02:28,  1.68s/it]11/16/2022 22:59:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.7992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:48 - INFO - train.train_snli_ve - loss is tensor(1.0809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 202/6700 [05:40<3:00:50,  1.67s/it]11/16/2022 22:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.7795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:49 - INFO - train.train_snli_ve - loss is tensor(1.0034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 203/6700 [05:42<3:01:29,  1.68s/it]11/16/2022 22:59:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.8452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:51 - INFO - train.train_snli_ve - loss is tensor(1.0924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 204/6700 [05:44<2:59:49,  1.66s/it]11/16/2022 22:59:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.8236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:53 - INFO - train.train_snli_ve - loss is tensor(1.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 205/6700 [05:45<3:00:27,  1.67s/it]11/16/2022 22:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.7208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:54 - INFO - train.train_snli_ve - loss is tensor(0.9167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 206/6700 [05:47<2:59:26,  1.66s/it]11/16/2022 22:59:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.6297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:56 - INFO - train.train_snli_ve - loss is tensor(0.8881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 207/6700 [05:49<3:02:17,  1.68s/it]11/16/2022 22:59:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.6588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:58 - INFO - train.train_snli_ve - loss is tensor(1.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 208/6700 [05:50<3:00:39,  1.67s/it]11/16/2022 22:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.5618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 22:59:59 - INFO - train.train_snli_ve - loss is tensor(1.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 209/6700 [05:52<3:00:44,  1.67s/it]11/16/2022 23:00:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.7102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:01 - INFO - train.train_snli_ve - loss is tensor(0.8782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 210/6700 [05:54<2:59:54,  1.66s/it]11/16/2022 23:00:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.5907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:03 - INFO - train.train_snli_ve - loss is tensor(1.2742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 211/6700 [05:55<3:01:11,  1.68s/it]11/16/2022 23:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.3954e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:04 - INFO - train.train_snli_ve - loss is tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 212/6700 [05:57<3:00:54,  1.67s/it]11/16/2022 23:00:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.3995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:06 - INFO - train.train_snli_ve - loss is tensor(0.9718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 213/6700 [05:59<3:01:53,  1.68s/it]11/16/2022 23:00:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.3397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:08 - INFO - train.train_snli_ve - loss is tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 214/6700 [06:00<3:02:13,  1.69s/it]11/16/2022 23:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.3960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:09 - INFO - train.train_snli_ve - loss is tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 215/6700 [06:02<3:02:03,  1.68s/it]11/16/2022 23:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.2938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:11 - INFO - train.train_snli_ve - loss is tensor(1.1035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 216/6700 [06:04<2:59:39,  1.66s/it]11/16/2022 23:00:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.2289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:13 - INFO - train.train_snli_ve - loss is tensor(1.0807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 217/6700 [06:05<3:02:36,  1.69s/it]11/16/2022 23:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.1832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:14 - INFO - train.train_snli_ve - loss is tensor(0.8976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 218/6700 [06:07<3:01:04,  1.68s/it]11/16/2022 23:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.3223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:16 - INFO - train.train_snli_ve - loss is tensor(1.0774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 219/6700 [06:09<3:02:47,  1.69s/it]11/16/2022 23:00:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.1460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:18 - INFO - train.train_snli_ve - loss is tensor(0.9733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 220/6700 [06:10<3:02:15,  1.69s/it]11/16/2022 23:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.1496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:19 - INFO - train.train_snli_ve - loss is tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 221/6700 [06:12<3:02:52,  1.69s/it]11/16/2022 23:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.9653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:21 - INFO - train.train_snli_ve - loss is tensor(0.8412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 222/6700 [06:14<3:01:13,  1.68s/it]11/16/2022 23:00:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.0412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:23 - INFO - train.train_snli_ve - loss is tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 223/6700 [06:16<3:03:14,  1.70s/it]11/16/2022 23:00:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.9397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:25 - INFO - train.train_snli_ve - loss is tensor(0.8952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 224/6700 [06:17<3:04:45,  1.71s/it]11/16/2022 23:00:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:26 - INFO - train.train_snli_ve - loss is tensor(0.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 225/6700 [06:19<3:04:36,  1.71s/it]11/16/2022 23:00:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.7813e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:28 - INFO - train.train_snli_ve - loss is tensor(0.9718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 226/6700 [06:21<3:03:20,  1.70s/it]11/16/2022 23:00:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:30 - INFO - train.train_snli_ve - loss is tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 227/6700 [06:22<3:03:19,  1.70s/it]11/16/2022 23:00:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.8311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:31 - INFO - train.train_snli_ve - loss is tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 228/6700 [06:24<3:00:30,  1.67s/it]11/16/2022 23:00:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:33 - INFO - train.train_snli_ve - loss is tensor(1.1210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 229/6700 [06:26<3:02:34,  1.69s/it]11/16/2022 23:00:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.7310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:35 - INFO - train.train_snli_ve - loss is tensor(0.9292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 230/6700 [06:27<3:00:32,  1.67s/it]11/16/2022 23:00:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.7110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:36 - INFO - train.train_snli_ve - loss is tensor(1.0502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 231/6700 [06:29<3:01:19,  1.68s/it]11/16/2022 23:00:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.5577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:38 - INFO - train.train_snli_ve - loss is tensor(1.1761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 232/6700 [06:31<3:00:41,  1.68s/it]11/16/2022 23:00:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.7362e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:40 - INFO - train.train_snli_ve - loss is tensor(1.0221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 233/6700 [06:32<3:02:24,  1.69s/it]11/16/2022 23:00:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:41 - INFO - train.train_snli_ve - loss is tensor(0.9956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   3%|3         | 234/6700 [06:34<2:59:49,  1.67s/it]11/16/2022 23:00:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.5035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:43 - INFO - train.train_snli_ve - loss is tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 235/6700 [06:36<3:02:38,  1.70s/it]11/16/2022 23:00:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.4675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:45 - INFO - train.train_snli_ve - loss is tensor(1.0617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 236/6700 [06:37<3:00:43,  1.68s/it]11/16/2022 23:00:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.5874e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:46 - INFO - train.train_snli_ve - loss is tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 237/6700 [06:39<3:01:59,  1.69s/it]11/16/2022 23:00:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.4669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:48 - INFO - train.train_snli_ve - loss is tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 238/6700 [06:41<3:01:29,  1.69s/it]11/16/2022 23:00:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5025e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:50 - INFO - train.train_snli_ve - loss is tensor(0.8745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 239/6700 [06:43<3:02:32,  1.70s/it]11/16/2022 23:00:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.3292e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:52 - INFO - train.train_snli_ve - loss is tensor(1.0479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 240/6700 [06:44<3:01:39,  1.69s/it]11/16/2022 23:00:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.3474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:53 - INFO - train.train_snli_ve - loss is tensor(0.8421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 241/6700 [06:46<3:01:22,  1.68s/it]11/16/2022 23:00:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2893e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:55 - INFO - train.train_snli_ve - loss is tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 242/6700 [06:48<2:59:29,  1.67s/it]11/16/2022 23:00:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.4320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:57 - INFO - train.train_snli_ve - loss is tensor(1.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 243/6700 [06:49<3:00:44,  1.68s/it]11/16/2022 23:00:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3604e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:00:58 - INFO - train.train_snli_ve - loss is tensor(0.9455, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 244/6700 [06:51<3:00:17,  1.68s/it]11/16/2022 23:01:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2827e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:00 - INFO - train.train_snli_ve - loss is tensor(0.9256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 245/6700 [06:53<3:00:05,  1.67s/it]11/16/2022 23:01:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:02 - INFO - train.train_snli_ve - loss is tensor(1.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 246/6700 [06:54<3:00:18,  1.68s/it]11/16/2022 23:01:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:03 - INFO - train.train_snli_ve - loss is tensor(0.9999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 247/6700 [06:56<3:00:53,  1.68s/it]11/16/2022 23:01:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:05 - INFO - train.train_snli_ve - loss is tensor(0.9769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 248/6700 [06:58<2:59:29,  1.67s/it]11/16/2022 23:01:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:07 - INFO - train.train_snli_ve - loss is tensor(1.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 249/6700 [06:59<3:00:39,  1.68s/it]11/16/2022 23:01:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:08 - INFO - train.train_snli_ve - loss is tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 250/6700 [07:01<3:00:03,  1.67s/it]11/16/2022 23:01:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:10 - INFO - train.train_snli_ve - loss is tensor(0.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 251/6700 [07:03<3:01:48,  1.69s/it]11/16/2022 23:01:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.0594e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:12 - INFO - train.train_snli_ve - loss is tensor(1.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 252/6700 [07:04<3:01:02,  1.68s/it]11/16/2022 23:01:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0175e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:13 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 253/6700 [07:06<3:01:44,  1.69s/it]11/16/2022 23:01:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.0353e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:15 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 254/6700 [07:08<2:59:18,  1.67s/it]11/16/2022 23:01:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.9527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:17 - INFO - train.train_snli_ve - loss is tensor(0.9554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 255/6700 [07:09<3:00:27,  1.68s/it]11/16/2022 23:01:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.0239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:18 - INFO - train.train_snli_ve - loss is tensor(1.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 256/6700 [07:11<2:59:53,  1.67s/it]11/16/2022 23:01:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:20 - INFO - train.train_snli_ve - loss is tensor(0.9895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 257/6700 [07:13<2:59:47,  1.67s/it]11/16/2022 23:01:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:22 - INFO - train.train_snli_ve - loss is tensor(1.0379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 258/6700 [07:14<2:57:25,  1.65s/it]11/16/2022 23:01:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.8910e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:23 - INFO - train.train_snli_ve - loss is tensor(1.1558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 259/6700 [07:16<3:00:56,  1.69s/it]11/16/2022 23:01:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:25 - INFO - train.train_snli_ve - loss is tensor(0.9351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 260/6700 [07:18<2:59:39,  1.67s/it]11/16/2022 23:01:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8190e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:27 - INFO - train.train_snli_ve - loss is tensor(0.9317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 261/6700 [07:19<3:00:11,  1.68s/it]11/16/2022 23:01:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9204e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:28 - INFO - train.train_snli_ve - loss is tensor(1.0322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 262/6700 [07:21<2:59:16,  1.67s/it]11/16/2022 23:01:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8258e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:30 - INFO - train.train_snli_ve - loss is tensor(0.9916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 263/6700 [07:23<3:00:07,  1.68s/it]11/16/2022 23:01:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:32 - INFO - train.train_snli_ve - loss is tensor(1.2297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 264/6700 [07:24<2:58:30,  1.66s/it]11/16/2022 23:01:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:33 - INFO - train.train_snli_ve - loss is tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 265/6700 [07:26<2:59:45,  1.68s/it]11/16/2022 23:01:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:35 - INFO - train.train_snli_ve - loss is tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 266/6700 [07:28<2:57:25,  1.65s/it]11/16/2022 23:01:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:37 - INFO - train.train_snli_ve - loss is tensor(0.8714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|3         | 267/6700 [07:29<3:00:50,  1.69s/it]11/16/2022 23:01:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.7202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:38 - INFO - train.train_snli_ve - loss is tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 268/6700 [07:31<2:59:03,  1.67s/it]11/16/2022 23:01:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:40 - INFO - train.train_snli_ve - loss is tensor(0.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 269/6700 [07:33<3:00:17,  1.68s/it]11/16/2022 23:01:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.6962e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:42 - INFO - train.train_snli_ve - loss is tensor(0.8995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 270/6700 [07:34<2:59:21,  1.67s/it]11/16/2022 23:01:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.6654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:43 - INFO - train.train_snli_ve - loss is tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 271/6700 [07:36<3:00:00,  1.68s/it]11/16/2022 23:01:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:45 - INFO - train.train_snli_ve - loss is tensor(1.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 272/6700 [07:38<2:58:05,  1.66s/it]11/16/2022 23:01:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.5794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:47 - INFO - train.train_snli_ve - loss is tensor(0.8595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 273/6700 [07:39<2:59:09,  1.67s/it]11/16/2022 23:01:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5874e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:48 - INFO - train.train_snli_ve - loss is tensor(0.8555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 274/6700 [07:41<2:58:06,  1.66s/it]11/16/2022 23:01:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:50 - INFO - train.train_snli_ve - loss is tensor(0.8932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 275/6700 [07:43<3:01:08,  1.69s/it]11/16/2022 23:01:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:52 - INFO - train.train_snli_ve - loss is tensor(0.8860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 276/6700 [07:44<2:58:49,  1.67s/it]11/16/2022 23:01:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:54 - INFO - train.train_snli_ve - loss is tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 277/6700 [07:46<2:59:53,  1.68s/it]11/16/2022 23:01:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.5704e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:55 - INFO - train.train_snli_ve - loss is tensor(0.9341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 278/6700 [07:48<2:58:59,  1.67s/it]11/16/2022 23:01:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.5263e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:57 - INFO - train.train_snli_ve - loss is tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 279/6700 [07:50<3:00:36,  1.69s/it]11/16/2022 23:01:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:01:59 - INFO - train.train_snli_ve - loss is tensor(1.0904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 280/6700 [07:51<2:59:09,  1.67s/it]11/16/2022 23:02:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:00 - INFO - train.train_snli_ve - loss is tensor(0.8515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 281/6700 [07:53<3:00:31,  1.69s/it]11/16/2022 23:02:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.4513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:02 - INFO - train.train_snli_ve - loss is tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 282/6700 [07:55<2:57:43,  1.66s/it]11/16/2022 23:02:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:04 - INFO - train.train_snli_ve - loss is tensor(1.0294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 283/6700 [07:56<2:58:13,  1.67s/it]11/16/2022 23:02:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4704e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:05 - INFO - train.train_snli_ve - loss is tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 284/6700 [07:58<2:57:39,  1.66s/it]11/16/2022 23:02:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.4195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:07 - INFO - train.train_snli_ve - loss is tensor(0.8332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 285/6700 [08:00<3:00:31,  1.69s/it]11/16/2022 23:02:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:09 - INFO - train.train_snli_ve - loss is tensor(0.8904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 286/6700 [08:01<2:59:04,  1.68s/it]11/16/2022 23:02:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:10 - INFO - train.train_snli_ve - loss is tensor(0.9605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 287/6700 [08:03<3:00:08,  1.69s/it]11/16/2022 23:02:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.3888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:12 - INFO - train.train_snli_ve - loss is tensor(0.9832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 288/6700 [08:05<2:59:13,  1.68s/it]11/16/2022 23:02:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.3762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:14 - INFO - train.train_snli_ve - loss is tensor(0.8546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 289/6700 [08:06<3:00:23,  1.69s/it]11/16/2022 23:02:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:15 - INFO - train.train_snli_ve - loss is tensor(0.9674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 290/6700 [08:08<2:58:22,  1.67s/it]11/16/2022 23:02:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:17 - INFO - train.train_snli_ve - loss is tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 291/6700 [08:10<2:59:39,  1.68s/it]11/16/2022 23:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:19 - INFO - train.train_snli_ve - loss is tensor(1.1644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 292/6700 [08:11<2:58:42,  1.67s/it]11/16/2022 23:02:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:20 - INFO - train.train_snli_ve - loss is tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 293/6700 [08:13<3:00:34,  1.69s/it]11/16/2022 23:02:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.3300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:22 - INFO - train.train_snli_ve - loss is tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 294/6700 [08:15<2:59:47,  1.68s/it]11/16/2022 23:02:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2807e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:24 - INFO - train.train_snli_ve - loss is tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 295/6700 [08:16<3:01:25,  1.70s/it]11/16/2022 23:02:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:25 - INFO - train.train_snli_ve - loss is tensor(0.9234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 296/6700 [08:18<2:59:02,  1.68s/it]11/16/2022 23:02:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:27 - INFO - train.train_snli_ve - loss is tensor(0.8155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 297/6700 [08:20<2:59:07,  1.68s/it]11/16/2022 23:02:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.2437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:29 - INFO - train.train_snli_ve - loss is tensor(1.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 298/6700 [08:21<3:00:06,  1.69s/it]11/16/2022 23:02:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:30 - INFO - train.train_snli_ve - loss is tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 299/6700 [08:23<3:00:12,  1.69s/it]11/16/2022 23:02:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:32 - INFO - train.train_snli_ve - loss is tensor(1.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 300/6700 [08:25<2:59:50,  1.69s/it]11/16/2022 23:02:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:34 - INFO - train.train_snli_ve - loss is tensor(0.9295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   4%|4         | 301/6700 [08:27<3:00:10,  1.69s/it]11/16/2022 23:02:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:35 - INFO - train.train_snli_ve - loss is tensor(0.9391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 302/6700 [08:28<2:57:41,  1.67s/it]11/16/2022 23:02:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:37 - INFO - train.train_snli_ve - loss is tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 303/6700 [08:30<2:58:32,  1.67s/it]11/16/2022 23:02:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:39 - INFO - train.train_snli_ve - loss is tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 304/6700 [08:32<2:58:11,  1.67s/it]11/16/2022 23:02:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:41 - INFO - train.train_snli_ve - loss is tensor(1.2089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 305/6700 [08:33<3:00:16,  1.69s/it]11/16/2022 23:02:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.1556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:42 - INFO - train.train_snli_ve - loss is tensor(0.8958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 306/6700 [08:35<2:57:05,  1.66s/it]11/16/2022 23:02:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:44 - INFO - train.train_snli_ve - loss is tensor(0.8896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 307/6700 [08:37<3:00:39,  1.70s/it]11/16/2022 23:02:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:46 - INFO - train.train_snli_ve - loss is tensor(0.9413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 308/6700 [08:38<2:59:32,  1.69s/it]11/16/2022 23:02:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:47 - INFO - train.train_snli_ve - loss is tensor(0.8997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 309/6700 [08:40<3:00:40,  1.70s/it]11/16/2022 23:02:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:49 - INFO - train.train_snli_ve - loss is tensor(0.8589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 310/6700 [08:42<2:58:56,  1.68s/it]11/16/2022 23:02:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1034e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:51 - INFO - train.train_snli_ve - loss is tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 311/6700 [08:43<3:00:52,  1.70s/it]11/16/2022 23:02:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.0821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:52 - INFO - train.train_snli_ve - loss is tensor(0.8362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 312/6700 [08:45<2:56:58,  1.66s/it]11/16/2022 23:02:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:54 - INFO - train.train_snli_ve - loss is tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 313/6700 [08:47<2:57:50,  1.67s/it]11/16/2022 23:02:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:56 - INFO - train.train_snli_ve - loss is tensor(0.8649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 314/6700 [08:48<2:55:50,  1.65s/it]11/16/2022 23:02:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:57 - INFO - train.train_snli_ve - loss is tensor(0.8501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 315/6700 [08:50<2:57:15,  1.67s/it]11/16/2022 23:02:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:02:59 - INFO - train.train_snli_ve - loss is tensor(0.8767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 316/6700 [08:52<2:56:05,  1.66s/it]11/16/2022 23:03:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:01 - INFO - train.train_snli_ve - loss is tensor(1.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 317/6700 [08:53<2:58:55,  1.68s/it]11/16/2022 23:03:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:02 - INFO - train.train_snli_ve - loss is tensor(1.0148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 318/6700 [08:55<2:56:39,  1.66s/it]11/16/2022 23:03:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0267e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:04 - INFO - train.train_snli_ve - loss is tensor(1.0203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 319/6700 [08:57<2:58:53,  1.68s/it]11/16/2022 23:03:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:06 - INFO - train.train_snli_ve - loss is tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 320/6700 [08:58<2:56:42,  1.66s/it]11/16/2022 23:03:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.6104e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:07 - INFO - train.train_snli_ve - loss is tensor(1.0177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 321/6700 [09:00<2:57:20,  1.67s/it]11/16/2022 23:03:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.7183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:09 - INFO - train.train_snli_ve - loss is tensor(0.9769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 322/6700 [09:02<2:56:50,  1.66s/it]11/16/2022 23:03:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.5695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:11 - INFO - train.train_snli_ve - loss is tensor(1.0048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 323/6700 [09:03<2:59:09,  1.69s/it]11/16/2022 23:03:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.4605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:12 - INFO - train.train_snli_ve - loss is tensor(1.0503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 324/6700 [09:05<2:57:04,  1.67s/it]11/16/2022 23:03:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.3415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:14 - INFO - train.train_snli_ve - loss is tensor(1.0768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 325/6700 [09:07<2:58:10,  1.68s/it]11/16/2022 23:03:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.2687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:16 - INFO - train.train_snli_ve - loss is tensor(0.8143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 326/6700 [09:08<2:57:08,  1.67s/it]11/16/2022 23:03:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.0090e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:17 - INFO - train.train_snli_ve - loss is tensor(1.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 327/6700 [09:10<2:58:31,  1.68s/it]11/16/2022 23:03:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.1490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:19 - INFO - train.train_snli_ve - loss is tensor(0.7869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 328/6700 [09:12<2:56:33,  1.66s/it]11/16/2022 23:03:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.8355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:21 - INFO - train.train_snli_ve - loss is tensor(0.9204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 329/6700 [09:13<2:57:07,  1.67s/it]11/16/2022 23:03:22 - INFO - train.train_snli_ve - kd_loss is tensor(8.9020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:22 - INFO - train.train_snli_ve - loss is tensor(0.8975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 330/6700 [09:15<2:55:17,  1.65s/it]11/16/2022 23:03:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.9411e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:24 - INFO - train.train_snli_ve - loss is tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 331/6700 [09:17<2:57:59,  1.68s/it]11/16/2022 23:03:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.7561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:26 - INFO - train.train_snli_ve - loss is tensor(0.9835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 332/6700 [09:18<2:56:00,  1.66s/it]11/16/2022 23:03:27 - INFO - train.train_snli_ve - kd_loss is tensor(8.7428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:27 - INFO - train.train_snli_ve - loss is tensor(0.8293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 333/6700 [09:20<2:57:33,  1.67s/it]11/16/2022 23:03:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.5919e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:29 - INFO - train.train_snli_ve - loss is tensor(0.8357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|4         | 334/6700 [09:22<2:57:06,  1.67s/it]11/16/2022 23:03:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.0702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:31 - INFO - train.train_snli_ve - loss is tensor(1.0295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 335/6700 [09:23<2:59:51,  1.70s/it]11/16/2022 23:03:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.1092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:32 - INFO - train.train_snli_ve - loss is tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 336/6700 [09:25<2:59:09,  1.69s/it]11/16/2022 23:03:34 - INFO - train.train_snli_ve - kd_loss is tensor(7.9158e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:34 - INFO - train.train_snli_ve - loss is tensor(0.9571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 337/6700 [09:27<2:58:21,  1.68s/it]11/16/2022 23:03:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.7697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:36 - INFO - train.train_snli_ve - loss is tensor(0.8101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 338/6700 [09:28<2:56:55,  1.67s/it]11/16/2022 23:03:37 - INFO - train.train_snli_ve - kd_loss is tensor(7.7996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:37 - INFO - train.train_snli_ve - loss is tensor(1.0550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 339/6700 [09:30<2:59:23,  1.69s/it]11/16/2022 23:03:39 - INFO - train.train_snli_ve - kd_loss is tensor(7.6311e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:39 - INFO - train.train_snli_ve - loss is tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 340/6700 [09:32<2:58:22,  1.68s/it]11/16/2022 23:03:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.6278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:41 - INFO - train.train_snli_ve - loss is tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 341/6700 [09:34<2:59:39,  1.70s/it]11/16/2022 23:03:43 - INFO - train.train_snli_ve - kd_loss is tensor(7.5415e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:43 - INFO - train.train_snli_ve - loss is tensor(0.9422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 342/6700 [09:35<2:58:57,  1.69s/it]11/16/2022 23:03:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.0521e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:44 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 343/6700 [09:37<2:58:33,  1.69s/it]11/16/2022 23:03:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.3036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:46 - INFO - train.train_snli_ve - loss is tensor(1.0106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 344/6700 [09:38<2:55:21,  1.66s/it]11/16/2022 23:03:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.8313e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:47 - INFO - train.train_snli_ve - loss is tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 345/6700 [09:40<2:55:53,  1.66s/it]11/16/2022 23:03:49 - INFO - train.train_snli_ve - kd_loss is tensor(6.9219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:49 - INFO - train.train_snli_ve - loss is tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 346/6700 [09:42<2:55:55,  1.66s/it]11/16/2022 23:03:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.8693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:51 - INFO - train.train_snli_ve - loss is tensor(0.8677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 347/6700 [09:44<3:00:01,  1.70s/it]11/16/2022 23:03:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.7811e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:53 - INFO - train.train_snli_ve - loss is tensor(1.0580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 348/6700 [09:45<2:58:35,  1.69s/it]11/16/2022 23:03:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.9409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:54 - INFO - train.train_snli_ve - loss is tensor(0.9077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 349/6700 [09:47<2:59:20,  1.69s/it]11/16/2022 23:03:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.6608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:56 - INFO - train.train_snli_ve - loss is tensor(0.9303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 350/6700 [09:49<2:56:49,  1.67s/it]11/16/2022 23:03:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.5319e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:58 - INFO - train.train_snli_ve - loss is tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 351/6700 [09:50<2:57:13,  1.67s/it]11/16/2022 23:03:59 - INFO - train.train_snli_ve - kd_loss is tensor(6.2473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:03:59 - INFO - train.train_snli_ve - loss is tensor(0.8599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 352/6700 [09:52<2:54:29,  1.65s/it]11/16/2022 23:04:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.3357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:01 - INFO - train.train_snli_ve - loss is tensor(1.0116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 353/6700 [09:54<2:56:37,  1.67s/it]11/16/2022 23:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(6.2537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:03 - INFO - train.train_snli_ve - loss is tensor(0.8610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 354/6700 [09:55<2:57:04,  1.67s/it]11/16/2022 23:04:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.9565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:04 - INFO - train.train_snli_ve - loss is tensor(0.9529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 355/6700 [09:57<2:57:25,  1.68s/it]11/16/2022 23:04:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.9814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:06 - INFO - train.train_snli_ve - loss is tensor(0.9350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 356/6700 [09:59<2:57:06,  1.68s/it]11/16/2022 23:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(6.1501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:08 - INFO - train.train_snli_ve - loss is tensor(1.0040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 357/6700 [10:00<2:59:32,  1.70s/it]11/16/2022 23:04:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.5772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:09 - INFO - train.train_snli_ve - loss is tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 358/6700 [10:02<2:57:08,  1.68s/it]11/16/2022 23:04:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.4204e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:11 - INFO - train.train_snli_ve - loss is tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 359/6700 [10:04<2:57:54,  1.68s/it]11/16/2022 23:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.5089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:13 - INFO - train.train_snli_ve - loss is tensor(0.8447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 360/6700 [10:05<2:55:58,  1.67s/it]11/16/2022 23:04:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.2879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:14 - INFO - train.train_snli_ve - loss is tensor(0.9417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 361/6700 [10:07<2:58:09,  1.69s/it]11/16/2022 23:04:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.2238e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:16 - INFO - train.train_snli_ve - loss is tensor(0.9332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 362/6700 [10:09<2:57:04,  1.68s/it]11/16/2022 23:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.1396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:18 - INFO - train.train_snli_ve - loss is tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 363/6700 [10:10<2:58:15,  1.69s/it]11/16/2022 23:04:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.1381e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:19 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 364/6700 [10:12<2:55:42,  1.66s/it]11/16/2022 23:04:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.9711e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:21 - INFO - train.train_snli_ve - loss is tensor(0.8290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 365/6700 [10:14<2:57:17,  1.68s/it]11/16/2022 23:04:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.8798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:23 - INFO - train.train_snli_ve - loss is tensor(0.9193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 366/6700 [10:15<2:55:46,  1.67s/it]11/16/2022 23:04:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.1592e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:24 - INFO - train.train_snli_ve - loss is tensor(0.9169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 367/6700 [10:17<2:58:21,  1.69s/it]11/16/2022 23:04:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.7030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:26 - INFO - train.train_snli_ve - loss is tensor(0.9677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   5%|5         | 368/6700 [10:19<2:57:27,  1.68s/it]11/16/2022 23:04:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.2128e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:28 - INFO - train.train_snli_ve - loss is tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 369/6700 [10:21<2:59:22,  1.70s/it]11/16/2022 23:04:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.1300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:29 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 370/6700 [10:22<2:57:04,  1.68s/it]11/16/2022 23:04:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.5878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:31 - INFO - train.train_snli_ve - loss is tensor(0.8099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 371/6700 [10:24<2:58:18,  1.69s/it]11/16/2022 23:04:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.5428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:33 - INFO - train.train_snli_ve - loss is tensor(0.8420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 372/6700 [10:26<2:56:00,  1.67s/it]11/16/2022 23:04:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.0565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:35 - INFO - train.train_snli_ve - loss is tensor(0.9346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 373/6700 [10:27<2:57:18,  1.68s/it]11/16/2022 23:04:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.1551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:36 - INFO - train.train_snli_ve - loss is tensor(0.9537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 374/6700 [10:29<2:55:35,  1.67s/it]11/16/2022 23:04:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.0357e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:38 - INFO - train.train_snli_ve - loss is tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 375/6700 [10:31<2:56:24,  1.67s/it]11/16/2022 23:04:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.3161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:39 - INFO - train.train_snli_ve - loss is tensor(0.8671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 376/6700 [10:32<2:55:19,  1.66s/it]11/16/2022 23:04:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.0051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:41 - INFO - train.train_snli_ve - loss is tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 377/6700 [10:34<2:57:53,  1.69s/it]11/16/2022 23:04:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.6668e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:43 - INFO - train.train_snli_ve - loss is tensor(0.8494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 378/6700 [10:36<2:55:27,  1.67s/it]11/16/2022 23:04:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.7662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:45 - INFO - train.train_snli_ve - loss is tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 379/6700 [10:37<2:56:35,  1.68s/it]11/16/2022 23:04:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.0272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:46 - INFO - train.train_snli_ve - loss is tensor(0.9450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 380/6700 [10:39<2:54:47,  1.66s/it]11/16/2022 23:04:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.3385e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:48 - INFO - train.train_snli_ve - loss is tensor(0.8550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 381/6700 [10:41<2:55:55,  1.67s/it]11/16/2022 23:04:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.1316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:49 - INFO - train.train_snli_ve - loss is tensor(0.8836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 382/6700 [10:42<2:54:11,  1.65s/it]11/16/2022 23:04:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.8072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:51 - INFO - train.train_snli_ve - loss is tensor(0.8999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 383/6700 [10:44<2:55:53,  1.67s/it]11/16/2022 23:04:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.3768e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:53 - INFO - train.train_snli_ve - loss is tensor(0.9428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 384/6700 [10:46<2:56:24,  1.68s/it]11/16/2022 23:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.4436e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:55 - INFO - train.train_snli_ve - loss is tensor(0.9406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 385/6700 [10:47<2:58:18,  1.69s/it]11/16/2022 23:04:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.3539e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:56 - INFO - train.train_snli_ve - loss is tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 386/6700 [10:49<2:57:48,  1.69s/it]11/16/2022 23:04:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.4708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:04:58 - INFO - train.train_snli_ve - loss is tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 387/6700 [10:51<2:57:38,  1.69s/it]11/16/2022 23:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.3792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:00 - INFO - train.train_snli_ve - loss is tensor(0.8453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 388/6700 [10:52<2:55:49,  1.67s/it]11/16/2022 23:05:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.3026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:01 - INFO - train.train_snli_ve - loss is tensor(0.8501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 389/6700 [10:54<2:57:11,  1.68s/it]11/16/2022 23:05:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.1163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:03 - INFO - train.train_snli_ve - loss is tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 390/6700 [10:56<2:55:01,  1.66s/it]11/16/2022 23:05:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.2826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:05 - INFO - train.train_snli_ve - loss is tensor(0.8622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 391/6700 [10:57<2:57:48,  1.69s/it]11/16/2022 23:05:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.0678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:06 - INFO - train.train_snli_ve - loss is tensor(0.8514, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 392/6700 [10:59<2:57:05,  1.68s/it]11/16/2022 23:05:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.6310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:08 - INFO - train.train_snli_ve - loss is tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 393/6700 [11:01<2:58:52,  1.70s/it]11/16/2022 23:05:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.7586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:10 - INFO - train.train_snli_ve - loss is tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 394/6700 [11:02<2:57:28,  1.69s/it]11/16/2022 23:05:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:11 - INFO - train.train_snli_ve - loss is tensor(0.8539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 395/6700 [11:04<2:57:46,  1.69s/it]11/16/2022 23:05:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3375e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:13 - INFO - train.train_snli_ve - loss is tensor(1.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 396/6700 [11:06<2:57:02,  1.69s/it]11/16/2022 23:05:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.0079e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:15 - INFO - train.train_snli_ve - loss is tensor(0.7814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 397/6700 [11:07<2:56:23,  1.68s/it]11/16/2022 23:05:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:16 - INFO - train.train_snli_ve - loss is tensor(1.0516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 398/6700 [11:09<2:56:18,  1.68s/it]11/16/2022 23:05:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.5537e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:18 - INFO - train.train_snli_ve - loss is tensor(0.9582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 399/6700 [11:11<2:57:51,  1.69s/it]11/16/2022 23:05:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.5081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:20 - INFO - train.train_snli_ve - loss is tensor(0.8746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 400/6700 [11:13<2:56:18,  1.68s/it]11/16/2022 23:05:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.5106e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:22 - INFO - train.train_snli_ve - loss is tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|5         | 401/6700 [11:14<2:57:08,  1.69s/it]11/16/2022 23:05:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4610e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:23 - INFO - train.train_snli_ve - loss is tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 402/6700 [11:16<2:56:03,  1.68s/it]11/16/2022 23:05:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.8844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:25 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 403/6700 [11:18<2:57:50,  1.69s/it]11/16/2022 23:05:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.7740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:27 - INFO - train.train_snli_ve - loss is tensor(1.1574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 404/6700 [11:19<2:55:15,  1.67s/it]11/16/2022 23:05:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:28 - INFO - train.train_snli_ve - loss is tensor(1.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 405/6700 [11:21<2:57:18,  1.69s/it]11/16/2022 23:05:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.4612e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:30 - INFO - train.train_snli_ve - loss is tensor(0.8541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 406/6700 [11:23<2:56:21,  1.68s/it]11/16/2022 23:05:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4168e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:32 - INFO - train.train_snli_ve - loss is tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 407/6700 [11:24<2:57:39,  1.69s/it]11/16/2022 23:05:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.0127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:33 - INFO - train.train_snli_ve - loss is tensor(0.8421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 408/6700 [11:26<2:55:54,  1.68s/it]11/16/2022 23:05:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:35 - INFO - train.train_snli_ve - loss is tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 409/6700 [11:28<2:56:11,  1.68s/it]11/16/2022 23:05:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9590e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:37 - INFO - train.train_snli_ve - loss is tensor(1.1190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 410/6700 [11:29<2:54:50,  1.67s/it]11/16/2022 23:05:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0562e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:38 - INFO - train.train_snli_ve - loss is tensor(0.8795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 411/6700 [11:31<2:56:33,  1.68s/it]11/16/2022 23:05:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9023e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:40 - INFO - train.train_snli_ve - loss is tensor(0.8316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 412/6700 [11:33<2:53:55,  1.66s/it]11/16/2022 23:05:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.2611e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:42 - INFO - train.train_snli_ve - loss is tensor(0.9611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 413/6700 [11:34<2:56:29,  1.68s/it]11/16/2022 23:05:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:43 - INFO - train.train_snli_ve - loss is tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 414/6700 [11:36<2:54:43,  1.67s/it]11/16/2022 23:05:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:45 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 415/6700 [11:38<2:56:47,  1.69s/it]11/16/2022 23:05:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:47 - INFO - train.train_snli_ve - loss is tensor(1.1562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 416/6700 [11:39<2:53:49,  1.66s/it]11/16/2022 23:05:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:48 - INFO - train.train_snli_ve - loss is tensor(0.8976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 417/6700 [11:41<2:56:17,  1.68s/it]11/16/2022 23:05:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:50 - INFO - train.train_snli_ve - loss is tensor(1.0311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 418/6700 [11:43<2:54:56,  1.67s/it]11/16/2022 23:05:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:52 - INFO - train.train_snli_ve - loss is tensor(0.8747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 419/6700 [11:44<2:54:25,  1.67s/it]11/16/2022 23:05:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.3437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:53 - INFO - train.train_snli_ve - loss is tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 420/6700 [11:46<2:53:49,  1.66s/it]11/16/2022 23:05:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2835e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:55 - INFO - train.train_snli_ve - loss is tensor(1.0482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 421/6700 [11:48<2:55:33,  1.68s/it]11/16/2022 23:05:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:57 - INFO - train.train_snli_ve - loss is tensor(0.9382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 422/6700 [11:49<2:53:29,  1.66s/it]11/16/2022 23:05:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:05:58 - INFO - train.train_snli_ve - loss is tensor(1.0234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 423/6700 [11:51<2:54:19,  1.67s/it]11/16/2022 23:06:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.7789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:00 - INFO - train.train_snli_ve - loss is tensor(0.8684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 424/6700 [11:53<2:52:11,  1.65s/it]11/16/2022 23:06:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.4986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:02 - INFO - train.train_snli_ve - loss is tensor(1.0828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 425/6700 [11:54<2:55:21,  1.68s/it]11/16/2022 23:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8643e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:03 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 426/6700 [11:56<2:53:26,  1.66s/it]11/16/2022 23:06:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6421e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:05 - INFO - train.train_snli_ve - loss is tensor(0.9485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 427/6700 [11:58<2:54:48,  1.67s/it]11/16/2022 23:06:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.5388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:07 - INFO - train.train_snli_ve - loss is tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 428/6700 [11:59<2:54:29,  1.67s/it]11/16/2022 23:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:08 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 429/6700 [12:01<2:55:48,  1.68s/it]11/16/2022 23:06:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.7145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:10 - INFO - train.train_snli_ve - loss is tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 430/6700 [12:03<2:54:40,  1.67s/it]11/16/2022 23:06:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:12 - INFO - train.train_snli_ve - loss is tensor(1.2293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 431/6700 [12:04<2:54:44,  1.67s/it]11/16/2022 23:06:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.4809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:13 - INFO - train.train_snli_ve - loss is tensor(0.8807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 432/6700 [12:06<2:52:34,  1.65s/it]11/16/2022 23:06:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.5947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:15 - INFO - train.train_snli_ve - loss is tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 433/6700 [12:08<2:54:40,  1.67s/it]11/16/2022 23:06:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5137e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:17 - INFO - train.train_snli_ve - loss is tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 434/6700 [12:09<2:53:03,  1.66s/it]11/16/2022 23:06:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:18 - INFO - train.train_snli_ve - loss is tensor(0.9988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   6%|6         | 435/6700 [12:11<2:53:59,  1.67s/it]11/16/2022 23:06:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:20 - INFO - train.train_snli_ve - loss is tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 436/6700 [12:13<2:51:34,  1.64s/it]11/16/2022 23:06:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.5735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:22 - INFO - train.train_snli_ve - loss is tensor(1.1042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 437/6700 [12:14<2:53:55,  1.67s/it]11/16/2022 23:06:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.5295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:23 - INFO - train.train_snli_ve - loss is tensor(0.8145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 438/6700 [12:16<2:53:23,  1.66s/it]11/16/2022 23:06:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:25 - INFO - train.train_snli_ve - loss is tensor(0.9139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 439/6700 [12:18<2:55:26,  1.68s/it]11/16/2022 23:06:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7649e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:27 - INFO - train.train_snli_ve - loss is tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 440/6700 [12:19<2:55:25,  1.68s/it]11/16/2022 23:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:28 - INFO - train.train_snli_ve - loss is tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 441/6700 [12:21<2:56:14,  1.69s/it]11/16/2022 23:06:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5722e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:30 - INFO - train.train_snli_ve - loss is tensor(0.8605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 442/6700 [12:23<2:54:00,  1.67s/it]11/16/2022 23:06:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:32 - INFO - train.train_snli_ve - loss is tensor(1.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 443/6700 [12:24<2:54:52,  1.68s/it]11/16/2022 23:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.4145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:33 - INFO - train.train_snli_ve - loss is tensor(0.8646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 444/6700 [12:26<2:53:04,  1.66s/it]11/16/2022 23:06:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6586e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:35 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 445/6700 [12:28<2:56:08,  1.69s/it]11/16/2022 23:06:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:37 - INFO - train.train_snli_ve - loss is tensor(0.9228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 446/6700 [12:29<2:55:07,  1.68s/it]11/16/2022 23:06:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:39 - INFO - train.train_snli_ve - loss is tensor(0.9308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 447/6700 [12:31<2:57:28,  1.70s/it]11/16/2022 23:06:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.3679e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:40 - INFO - train.train_snli_ve - loss is tensor(0.8567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 448/6700 [12:33<2:56:16,  1.69s/it]11/16/2022 23:06:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.6881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:42 - INFO - train.train_snli_ve - loss is tensor(0.8800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 449/6700 [12:35<2:57:47,  1.71s/it]11/16/2022 23:06:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.4373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:44 - INFO - train.train_snli_ve - loss is tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 450/6700 [12:36<2:54:44,  1.68s/it]11/16/2022 23:06:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3623e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:45 - INFO - train.train_snli_ve - loss is tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 451/6700 [12:38<2:56:31,  1.69s/it]11/16/2022 23:06:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7212e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:47 - INFO - train.train_snli_ve - loss is tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 452/6700 [12:40<2:56:42,  1.70s/it]11/16/2022 23:06:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.6040e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:49 - INFO - train.train_snli_ve - loss is tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 453/6700 [12:41<2:56:29,  1.70s/it]11/16/2022 23:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:50 - INFO - train.train_snli_ve - loss is tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 454/6700 [12:43<2:54:43,  1.68s/it]11/16/2022 23:06:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.6536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:52 - INFO - train.train_snli_ve - loss is tensor(1.0377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 455/6700 [12:45<2:55:44,  1.69s/it]11/16/2022 23:06:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6896e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:54 - INFO - train.train_snli_ve - loss is tensor(0.9483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 456/6700 [12:46<2:55:13,  1.68s/it]11/16/2022 23:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0480e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:55 - INFO - train.train_snli_ve - loss is tensor(1.1082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 457/6700 [12:48<2:55:53,  1.69s/it]11/16/2022 23:06:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:57 - INFO - train.train_snli_ve - loss is tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 458/6700 [12:50<2:53:32,  1.67s/it]11/16/2022 23:06:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3150e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:06:59 - INFO - train.train_snli_ve - loss is tensor(0.8143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 459/6700 [12:51<2:56:13,  1.69s/it]11/16/2022 23:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.9166e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:00 - INFO - train.train_snli_ve - loss is tensor(0.9055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 460/6700 [12:53<2:54:11,  1.67s/it]11/16/2022 23:07:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:02 - INFO - train.train_snli_ve - loss is tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 461/6700 [12:55<2:54:55,  1.68s/it]11/16/2022 23:07:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.1968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:04 - INFO - train.train_snli_ve - loss is tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 462/6700 [12:56<2:54:33,  1.68s/it]11/16/2022 23:07:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:06 - INFO - train.train_snli_ve - loss is tensor(0.9795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 463/6700 [12:58<2:57:30,  1.71s/it]11/16/2022 23:07:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:07 - INFO - train.train_snli_ve - loss is tensor(0.8239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 464/6700 [13:00<2:54:32,  1.68s/it]11/16/2022 23:07:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5520e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:09 - INFO - train.train_snli_ve - loss is tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 465/6700 [13:02<2:55:26,  1.69s/it]11/16/2022 23:07:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:11 - INFO - train.train_snli_ve - loss is tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 466/6700 [13:03<2:54:06,  1.68s/it]11/16/2022 23:07:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:12 - INFO - train.train_snli_ve - loss is tensor(0.9147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 467/6700 [13:05<2:54:07,  1.68s/it]11/16/2022 23:07:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.2321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:14 - INFO - train.train_snli_ve - loss is tensor(0.8979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|6         | 468/6700 [13:07<2:52:52,  1.66s/it]11/16/2022 23:07:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:16 - INFO - train.train_snli_ve - loss is tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 469/6700 [13:08<2:54:10,  1.68s/it]11/16/2022 23:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.9155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:17 - INFO - train.train_snli_ve - loss is tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 470/6700 [13:10<2:52:49,  1.66s/it]11/16/2022 23:07:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:19 - INFO - train.train_snli_ve - loss is tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 471/6700 [13:12<2:53:37,  1.67s/it]11/16/2022 23:07:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.8879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:21 - INFO - train.train_snli_ve - loss is tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 472/6700 [13:13<2:52:06,  1.66s/it]11/16/2022 23:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:22 - INFO - train.train_snli_ve - loss is tensor(1.0571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 473/6700 [13:15<2:53:23,  1.67s/it]11/16/2022 23:07:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:24 - INFO - train.train_snli_ve - loss is tensor(0.7906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 474/6700 [13:17<2:53:08,  1.67s/it]11/16/2022 23:07:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.9120e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:26 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 475/6700 [13:18<2:53:27,  1.67s/it]11/16/2022 23:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5109e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:27 - INFO - train.train_snli_ve - loss is tensor(0.9690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 476/6700 [13:20<2:52:22,  1.66s/it]11/16/2022 23:07:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.8830e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:29 - INFO - train.train_snli_ve - loss is tensor(0.9773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 477/6700 [13:22<2:55:59,  1.70s/it]11/16/2022 23:07:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3457e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:31 - INFO - train.train_snli_ve - loss is tensor(0.9210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 478/6700 [13:23<2:54:27,  1.68s/it]11/16/2022 23:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:32 - INFO - train.train_snli_ve - loss is tensor(0.8994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 479/6700 [13:25<2:56:31,  1.70s/it]11/16/2022 23:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.0030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:34 - INFO - train.train_snli_ve - loss is tensor(0.9142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 480/6700 [13:27<2:53:52,  1.68s/it]11/16/2022 23:07:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.2501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:36 - INFO - train.train_snli_ve - loss is tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 481/6700 [13:28<2:54:50,  1.69s/it]11/16/2022 23:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:37 - INFO - train.train_snli_ve - loss is tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 482/6700 [13:30<2:52:14,  1.66s/it]11/16/2022 23:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6871e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:39 - INFO - train.train_snli_ve - loss is tensor(0.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 483/6700 [13:32<2:53:27,  1.67s/it]11/16/2022 23:07:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:41 - INFO - train.train_snli_ve - loss is tensor(0.9170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 484/6700 [13:33<2:52:22,  1.66s/it]11/16/2022 23:07:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:42 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 485/6700 [13:35<2:54:02,  1.68s/it]11/16/2022 23:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.4959e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:44 - INFO - train.train_snli_ve - loss is tensor(0.8844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 486/6700 [13:37<2:54:47,  1.69s/it]11/16/2022 23:07:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:46 - INFO - train.train_snli_ve - loss is tensor(0.9559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 487/6700 [13:38<2:54:26,  1.68s/it]11/16/2022 23:07:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3209e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:47 - INFO - train.train_snli_ve - loss is tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 488/6700 [13:40<2:51:55,  1.66s/it]11/16/2022 23:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.8914e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:49 - INFO - train.train_snli_ve - loss is tensor(0.8685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 489/6700 [13:42<2:52:52,  1.67s/it]11/16/2022 23:07:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.8084e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:51 - INFO - train.train_snli_ve - loss is tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 490/6700 [13:43<2:52:12,  1.66s/it]11/16/2022 23:07:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:52 - INFO - train.train_snli_ve - loss is tensor(1.0238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 491/6700 [13:45<2:52:38,  1.67s/it]11/16/2022 23:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:54 - INFO - train.train_snli_ve - loss is tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 492/6700 [13:47<2:52:24,  1.67s/it]11/16/2022 23:07:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:56 - INFO - train.train_snli_ve - loss is tensor(0.9825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 493/6700 [13:48<2:55:31,  1.70s/it]11/16/2022 23:07:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:57 - INFO - train.train_snli_ve - loss is tensor(0.9218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 494/6700 [13:50<2:53:49,  1.68s/it]11/16/2022 23:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0760e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:07:59 - INFO - train.train_snli_ve - loss is tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 495/6700 [13:52<2:54:07,  1.68s/it]11/16/2022 23:08:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.9505e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:01 - INFO - train.train_snli_ve - loss is tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 496/6700 [13:53<2:51:20,  1.66s/it]11/16/2022 23:08:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:02 - INFO - train.train_snli_ve - loss is tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 497/6700 [13:55<2:53:42,  1.68s/it]11/16/2022 23:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:04 - INFO - train.train_snli_ve - loss is tensor(0.9000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 498/6700 [13:57<2:53:41,  1.68s/it]11/16/2022 23:08:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.7937e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:06 - INFO - train.train_snli_ve - loss is tensor(0.9305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 499/6700 [13:59<2:56:02,  1.70s/it]11/16/2022 23:08:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.8913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:08 - INFO - train.train_snli_ve - loss is tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 500/6700 [14:00<2:54:24,  1.69s/it]11/16/2022 23:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3526e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:09 - INFO - train.train_snli_ve - loss is tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 501/6700 [14:02<2:55:59,  1.70s/it]11/16/2022 23:08:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:11 - INFO - train.train_snli_ve - loss is tensor(0.9273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   7%|7         | 502/6700 [14:04<2:53:34,  1.68s/it]11/16/2022 23:08:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3363e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:13 - INFO - train.train_snli_ve - loss is tensor(0.9698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 503/6700 [14:05<2:54:02,  1.69s/it]11/16/2022 23:08:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:14 - INFO - train.train_snli_ve - loss is tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 504/6700 [14:07<2:53:02,  1.68s/it]11/16/2022 23:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:16 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 505/6700 [14:09<2:54:21,  1.69s/it]11/16/2022 23:08:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:18 - INFO - train.train_snli_ve - loss is tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 506/6700 [14:10<2:51:41,  1.66s/it]11/16/2022 23:08:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:19 - INFO - train.train_snli_ve - loss is tensor(0.9089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 507/6700 [14:12<2:53:48,  1.68s/it]11/16/2022 23:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.3149e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:21 - INFO - train.train_snli_ve - loss is tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 508/6700 [14:14<2:51:18,  1.66s/it]11/16/2022 23:08:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:23 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 509/6700 [14:15<2:51:32,  1.66s/it]11/16/2022 23:08:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3010e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:24 - INFO - train.train_snli_ve - loss is tensor(0.9343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 510/6700 [14:17<2:51:13,  1.66s/it]11/16/2022 23:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.3461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:26 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 511/6700 [14:19<2:52:27,  1.67s/it]11/16/2022 23:08:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:28 - INFO - train.train_snli_ve - loss is tensor(0.9110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 512/6700 [14:20<2:50:28,  1.65s/it]11/16/2022 23:08:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.5556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:29 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 513/6700 [14:22<2:52:20,  1.67s/it]11/16/2022 23:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:31 - INFO - train.train_snli_ve - loss is tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 514/6700 [14:24<2:54:15,  1.69s/it]11/16/2022 23:08:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:33 - INFO - train.train_snli_ve - loss is tensor(1.1973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 515/6700 [14:25<2:55:37,  1.70s/it]11/16/2022 23:08:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.9658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:34 - INFO - train.train_snli_ve - loss is tensor(0.9450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 516/6700 [14:27<2:55:02,  1.70s/it]11/16/2022 23:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.6079e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:36 - INFO - train.train_snli_ve - loss is tensor(0.9949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 517/6700 [14:29<2:55:40,  1.70s/it]11/16/2022 23:08:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.7732e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:38 - INFO - train.train_snli_ve - loss is tensor(0.9079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 518/6700 [14:30<2:52:54,  1.68s/it]11/16/2022 23:08:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.0832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:40 - INFO - train.train_snli_ve - loss is tensor(0.9570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 519/6700 [14:32<2:55:39,  1.71s/it]11/16/2022 23:08:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1419e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:41 - INFO - train.train_snli_ve - loss is tensor(0.8654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 520/6700 [14:34<2:52:21,  1.67s/it]11/16/2022 23:08:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8714e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:43 - INFO - train.train_snli_ve - loss is tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 521/6700 [14:36<2:54:09,  1.69s/it]11/16/2022 23:08:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0897e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:44 - INFO - train.train_snli_ve - loss is tensor(0.8979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 522/6700 [14:37<2:52:14,  1.67s/it]11/16/2022 23:08:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.5577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:46 - INFO - train.train_snli_ve - loss is tensor(0.8951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 523/6700 [14:39<2:53:01,  1.68s/it]11/16/2022 23:08:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1545e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:48 - INFO - train.train_snli_ve - loss is tensor(0.8256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 524/6700 [14:41<2:54:11,  1.69s/it]11/16/2022 23:08:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5952e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:50 - INFO - train.train_snli_ve - loss is tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 525/6700 [14:42<2:53:49,  1.69s/it]11/16/2022 23:08:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.5435e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:51 - INFO - train.train_snli_ve - loss is tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 526/6700 [14:44<2:53:12,  1.68s/it]11/16/2022 23:08:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:53 - INFO - train.train_snli_ve - loss is tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 527/6700 [14:46<2:53:00,  1.68s/it]11/16/2022 23:08:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:55 - INFO - train.train_snli_ve - loss is tensor(0.9977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 528/6700 [14:47<2:49:42,  1.65s/it]11/16/2022 23:08:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:56 - INFO - train.train_snli_ve - loss is tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 529/6700 [14:49<2:50:53,  1.66s/it]11/16/2022 23:08:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:08:58 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 530/6700 [14:51<2:50:09,  1.65s/it]11/16/2022 23:09:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:00 - INFO - train.train_snli_ve - loss is tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 531/6700 [14:52<2:51:59,  1.67s/it]11/16/2022 23:09:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:01 - INFO - train.train_snli_ve - loss is tensor(0.9122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 532/6700 [14:54<2:50:47,  1.66s/it]11/16/2022 23:09:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:03 - INFO - train.train_snli_ve - loss is tensor(0.9625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 533/6700 [14:56<2:51:33,  1.67s/it]11/16/2022 23:09:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2359e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:05 - INFO - train.train_snli_ve - loss is tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 534/6700 [14:57<2:50:45,  1.66s/it]11/16/2022 23:09:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1461e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:06 - INFO - train.train_snli_ve - loss is tensor(0.7454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|7         | 535/6700 [14:59<2:54:36,  1.70s/it]11/16/2022 23:09:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:08 - INFO - train.train_snli_ve - loss is tensor(0.8288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 536/6700 [15:01<2:54:04,  1.69s/it]11/16/2022 23:09:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:10 - INFO - train.train_snli_ve - loss is tensor(0.8211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 537/6700 [15:02<2:55:47,  1.71s/it]11/16/2022 23:09:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:11 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 538/6700 [15:04<2:53:06,  1.69s/it]11/16/2022 23:09:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5204e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:13 - INFO - train.train_snli_ve - loss is tensor(0.9418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 539/6700 [15:06<2:54:32,  1.70s/it]11/16/2022 23:09:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:15 - INFO - train.train_snli_ve - loss is tensor(0.9940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 540/6700 [15:07<2:52:36,  1.68s/it]11/16/2022 23:09:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.7712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:16 - INFO - train.train_snli_ve - loss is tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 541/6700 [15:09<2:52:40,  1.68s/it]11/16/2022 23:09:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:18 - INFO - train.train_snli_ve - loss is tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 542/6700 [15:11<2:50:46,  1.66s/it]11/16/2022 23:09:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:20 - INFO - train.train_snli_ve - loss is tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 543/6700 [15:12<2:52:10,  1.68s/it]11/16/2022 23:09:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:21 - INFO - train.train_snli_ve - loss is tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 544/6700 [15:14<2:52:20,  1.68s/it]11/16/2022 23:09:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2848e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:23 - INFO - train.train_snli_ve - loss is tensor(0.9464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 545/6700 [15:16<2:52:33,  1.68s/it]11/16/2022 23:09:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.0207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:25 - INFO - train.train_snli_ve - loss is tensor(0.9216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 546/6700 [15:18<2:53:38,  1.69s/it]11/16/2022 23:09:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:27 - INFO - train.train_snli_ve - loss is tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 547/6700 [15:19<2:53:31,  1.69s/it]11/16/2022 23:09:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:28 - INFO - train.train_snli_ve - loss is tensor(1.0529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 548/6700 [15:21<2:51:54,  1.68s/it]11/16/2022 23:09:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9093e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:30 - INFO - train.train_snli_ve - loss is tensor(1.0428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 549/6700 [15:23<2:52:49,  1.69s/it]11/16/2022 23:09:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:32 - INFO - train.train_snli_ve - loss is tensor(0.9491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 550/6700 [15:24<2:50:47,  1.67s/it]11/16/2022 23:09:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:33 - INFO - train.train_snli_ve - loss is tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 551/6700 [15:26<2:52:31,  1.68s/it]11/16/2022 23:09:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4839e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:35 - INFO - train.train_snli_ve - loss is tensor(0.8448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 552/6700 [15:28<2:51:24,  1.67s/it]11/16/2022 23:09:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.8708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:37 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 553/6700 [15:29<2:53:06,  1.69s/it]11/16/2022 23:09:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4568e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:38 - INFO - train.train_snli_ve - loss is tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 554/6700 [15:31<2:51:46,  1.68s/it]11/16/2022 23:09:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4615e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:40 - INFO - train.train_snli_ve - loss is tensor(0.8628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 555/6700 [15:33<2:53:00,  1.69s/it]11/16/2022 23:09:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.3195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:42 - INFO - train.train_snli_ve - loss is tensor(0.9206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 556/6700 [15:34<2:50:25,  1.66s/it]11/16/2022 23:09:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:43 - INFO - train.train_snli_ve - loss is tensor(1.0247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 557/6700 [15:36<2:53:01,  1.69s/it]11/16/2022 23:09:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.8123e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:45 - INFO - train.train_snli_ve - loss is tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 558/6700 [15:38<2:51:22,  1.67s/it]11/16/2022 23:09:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.5544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:47 - INFO - train.train_snli_ve - loss is tensor(1.0341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 559/6700 [15:39<2:53:10,  1.69s/it]11/16/2022 23:09:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6600e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:48 - INFO - train.train_snli_ve - loss is tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 560/6700 [15:41<2:50:50,  1.67s/it]11/16/2022 23:09:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5402e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:50 - INFO - train.train_snli_ve - loss is tensor(0.9440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 561/6700 [15:43<2:52:49,  1.69s/it]11/16/2022 23:09:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.3751e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:52 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 562/6700 [15:44<2:50:41,  1.67s/it]11/16/2022 23:09:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:53 - INFO - train.train_snli_ve - loss is tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 563/6700 [15:46<2:51:40,  1.68s/it]11/16/2022 23:09:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:55 - INFO - train.train_snli_ve - loss is tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 564/6700 [15:48<2:50:10,  1.66s/it]11/16/2022 23:09:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:57 - INFO - train.train_snli_ve - loss is tensor(0.8872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 565/6700 [15:49<2:54:00,  1.70s/it]11/16/2022 23:09:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:09:58 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 566/6700 [15:51<2:53:29,  1.70s/it]11/16/2022 23:10:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:00 - INFO - train.train_snli_ve - loss is tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 567/6700 [15:53<2:54:52,  1.71s/it]11/16/2022 23:10:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3479e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:02 - INFO - train.train_snli_ve - loss is tensor(0.9521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 568/6700 [15:55<2:51:19,  1.68s/it]11/16/2022 23:10:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:04 - INFO - train.train_snli_ve - loss is tensor(0.8325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   8%|8         | 569/6700 [15:56<2:52:55,  1.69s/it]11/16/2022 23:10:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5069e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:05 - INFO - train.train_snli_ve - loss is tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 570/6700 [15:58<2:51:16,  1.68s/it]11/16/2022 23:10:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:07 - INFO - train.train_snli_ve - loss is tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 571/6700 [16:00<2:51:27,  1.68s/it]11/16/2022 23:10:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5755e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:09 - INFO - train.train_snli_ve - loss is tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 572/6700 [16:01<2:50:27,  1.67s/it]11/16/2022 23:10:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3922e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:10 - INFO - train.train_snli_ve - loss is tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 573/6700 [16:03<2:53:09,  1.70s/it]11/16/2022 23:10:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:12 - INFO - train.train_snli_ve - loss is tensor(0.9732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 574/6700 [16:05<2:51:38,  1.68s/it]11/16/2022 23:10:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.5659e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:14 - INFO - train.train_snli_ve - loss is tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 575/6700 [16:06<2:52:09,  1.69s/it]11/16/2022 23:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:15 - INFO - train.train_snli_ve - loss is tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 576/6700 [16:08<2:50:07,  1.67s/it]11/16/2022 23:10:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3010e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:17 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 577/6700 [16:10<2:53:03,  1.70s/it]11/16/2022 23:10:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.4056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:19 - INFO - train.train_snli_ve - loss is tensor(0.8713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 578/6700 [16:11<2:52:02,  1.69s/it]11/16/2022 23:10:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.6113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:20 - INFO - train.train_snli_ve - loss is tensor(1.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 579/6700 [16:13<2:52:39,  1.69s/it]11/16/2022 23:10:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.5074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:22 - INFO - train.train_snli_ve - loss is tensor(0.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 580/6700 [16:15<2:50:54,  1.68s/it]11/16/2022 23:10:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:24 - INFO - train.train_snli_ve - loss is tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 581/6700 [16:16<2:52:30,  1.69s/it]11/16/2022 23:10:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:25 - INFO - train.train_snli_ve - loss is tensor(0.8763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 582/6700 [16:18<2:52:23,  1.69s/it]11/16/2022 23:10:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:27 - INFO - train.train_snli_ve - loss is tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 583/6700 [16:20<2:52:58,  1.70s/it]11/16/2022 23:10:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:29 - INFO - train.train_snli_ve - loss is tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 584/6700 [16:21<2:51:26,  1.68s/it]11/16/2022 23:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.6026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:30 - INFO - train.train_snli_ve - loss is tensor(0.7251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 585/6700 [16:23<2:52:01,  1.69s/it]11/16/2022 23:10:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6021e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:32 - INFO - train.train_snli_ve - loss is tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 586/6700 [16:25<2:49:22,  1.66s/it]11/16/2022 23:10:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.8939e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:34 - INFO - train.train_snli_ve - loss is tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 587/6700 [16:27<2:51:20,  1.68s/it]11/16/2022 23:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8047e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:35 - INFO - train.train_snli_ve - loss is tensor(0.8316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 588/6700 [16:28<2:50:11,  1.67s/it]11/16/2022 23:10:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4651e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:37 - INFO - train.train_snli_ve - loss is tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 589/6700 [16:30<2:51:37,  1.69s/it]11/16/2022 23:10:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5420e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:39 - INFO - train.train_snli_ve - loss is tensor(0.8697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 590/6700 [16:32<2:51:00,  1.68s/it]11/16/2022 23:10:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9701e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:41 - INFO - train.train_snli_ve - loss is tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 591/6700 [16:33<2:52:08,  1.69s/it]11/16/2022 23:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:42 - INFO - train.train_snli_ve - loss is tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 592/6700 [16:35<2:48:23,  1.65s/it]11/16/2022 23:10:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:44 - INFO - train.train_snli_ve - loss is tensor(1.0110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 593/6700 [16:37<2:49:54,  1.67s/it]11/16/2022 23:10:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6398e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:46 - INFO - train.train_snli_ve - loss is tensor(1.0078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 594/6700 [16:38<2:51:39,  1.69s/it]11/16/2022 23:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:47 - INFO - train.train_snli_ve - loss is tensor(1.1363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 595/6700 [16:40<2:52:20,  1.69s/it]11/16/2022 23:10:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.4540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:49 - INFO - train.train_snli_ve - loss is tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 596/6700 [16:42<2:51:25,  1.69s/it]11/16/2022 23:10:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:51 - INFO - train.train_snli_ve - loss is tensor(0.9687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 597/6700 [16:43<2:50:53,  1.68s/it]11/16/2022 23:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.4729e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:52 - INFO - train.train_snli_ve - loss is tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 598/6700 [16:45<2:49:16,  1.66s/it]11/16/2022 23:10:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5849e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:54 - INFO - train.train_snli_ve - loss is tensor(0.9982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 599/6700 [16:47<2:52:02,  1.69s/it]11/16/2022 23:10:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:56 - INFO - train.train_snli_ve - loss is tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 600/6700 [16:48<2:51:49,  1.69s/it]11/16/2022 23:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8270e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:57 - INFO - train.train_snli_ve - loss is tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 601/6700 [16:50<2:52:29,  1.70s/it]11/16/2022 23:10:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7017e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:10:59 - INFO - train.train_snli_ve - loss is tensor(0.8953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|8         | 602/6700 [16:52<2:50:21,  1.68s/it]11/16/2022 23:11:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8114e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:01 - INFO - train.train_snli_ve - loss is tensor(0.9889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 603/6700 [16:53<2:52:32,  1.70s/it]11/16/2022 23:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.7001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:02 - INFO - train.train_snli_ve - loss is tensor(0.7816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 604/6700 [16:55<2:52:07,  1.69s/it]11/16/2022 23:11:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:04 - INFO - train.train_snli_ve - loss is tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 605/6700 [16:57<2:53:18,  1.71s/it]11/16/2022 23:11:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5203e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:06 - INFO - train.train_snli_ve - loss is tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 606/6700 [16:59<2:51:48,  1.69s/it]11/16/2022 23:11:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.6674e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:08 - INFO - train.train_snli_ve - loss is tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 607/6700 [17:00<2:53:26,  1.71s/it]11/16/2022 23:11:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5588e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:09 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 608/6700 [17:02<2:51:48,  1.69s/it]11/16/2022 23:11:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:11 - INFO - train.train_snli_ve - loss is tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 609/6700 [17:04<2:53:08,  1.71s/it]11/16/2022 23:11:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9029e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:13 - INFO - train.train_snli_ve - loss is tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 610/6700 [17:05<2:51:01,  1.68s/it]11/16/2022 23:11:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.4139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:14 - INFO - train.train_snli_ve - loss is tensor(0.8385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 611/6700 [17:07<2:54:04,  1.72s/it]11/16/2022 23:11:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.1814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:16 - INFO - train.train_snli_ve - loss is tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 612/6700 [17:09<2:53:03,  1.71s/it]11/16/2022 23:11:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.7885e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:18 - INFO - train.train_snli_ve - loss is tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 613/6700 [17:11<2:55:03,  1.73s/it]11/16/2022 23:11:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9098e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:20 - INFO - train.train_snli_ve - loss is tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 614/6700 [17:12<2:53:26,  1.71s/it]11/16/2022 23:11:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.7027e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:21 - INFO - train.train_snli_ve - loss is tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 615/6700 [17:14<2:53:38,  1.71s/it]11/16/2022 23:11:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.3935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:23 - INFO - train.train_snli_ve - loss is tensor(0.8125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 616/6700 [17:16<2:51:24,  1.69s/it]11/16/2022 23:11:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.7661e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:25 - INFO - train.train_snli_ve - loss is tensor(0.8817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 617/6700 [17:17<2:52:18,  1.70s/it]11/16/2022 23:11:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.3309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:26 - INFO - train.train_snli_ve - loss is tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 618/6700 [17:19<2:51:37,  1.69s/it]11/16/2022 23:11:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:28 - INFO - train.train_snli_ve - loss is tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 619/6700 [17:21<2:53:13,  1.71s/it]11/16/2022 23:11:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:30 - INFO - train.train_snli_ve - loss is tensor(1.0161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 620/6700 [17:22<2:50:40,  1.68s/it]11/16/2022 23:11:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.1432e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:31 - INFO - train.train_snli_ve - loss is tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 621/6700 [17:24<2:52:14,  1.70s/it]11/16/2022 23:11:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9829e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:33 - INFO - train.train_snli_ve - loss is tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 622/6700 [17:26<2:50:22,  1.68s/it]11/16/2022 23:11:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:35 - INFO - train.train_snli_ve - loss is tensor(1.3128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 623/6700 [17:27<2:51:18,  1.69s/it]11/16/2022 23:11:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:36 - INFO - train.train_snli_ve - loss is tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 624/6700 [17:29<2:48:40,  1.67s/it]11/16/2022 23:11:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.8952e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:38 - INFO - train.train_snli_ve - loss is tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 625/6700 [17:31<2:49:55,  1.68s/it]11/16/2022 23:11:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1009e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:40 - INFO - train.train_snli_ve - loss is tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 626/6700 [17:32<2:49:14,  1.67s/it]11/16/2022 23:11:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.0865e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:41 - INFO - train.train_snli_ve - loss is tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 627/6700 [17:34<2:49:33,  1.68s/it]11/16/2022 23:11:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0214e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:43 - INFO - train.train_snli_ve - loss is tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 628/6700 [17:36<2:49:32,  1.68s/it]11/16/2022 23:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:45 - INFO - train.train_snli_ve - loss is tensor(0.9404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 629/6700 [17:37<2:50:29,  1.69s/it]11/16/2022 23:11:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:46 - INFO - train.train_snli_ve - loss is tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 630/6700 [17:39<2:47:59,  1.66s/it]11/16/2022 23:11:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:48 - INFO - train.train_snli_ve - loss is tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 631/6700 [17:41<2:50:22,  1.68s/it]11/16/2022 23:11:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:50 - INFO - train.train_snli_ve - loss is tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 632/6700 [17:42<2:48:59,  1.67s/it]11/16/2022 23:11:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.4454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:52 - INFO - train.train_snli_ve - loss is tensor(0.9755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 633/6700 [17:44<2:50:52,  1.69s/it]11/16/2022 23:11:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:53 - INFO - train.train_snli_ve - loss is tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 634/6700 [17:46<2:49:23,  1.68s/it]11/16/2022 23:11:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.5395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:55 - INFO - train.train_snli_ve - loss is tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 635/6700 [17:48<2:49:34,  1.68s/it]11/16/2022 23:11:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:56 - INFO - train.train_snli_ve - loss is tensor(0.9881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:   9%|9         | 636/6700 [17:49<2:47:16,  1.66s/it]11/16/2022 23:11:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:11:58 - INFO - train.train_snli_ve - loss is tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 637/6700 [17:51<2:49:01,  1.67s/it]11/16/2022 23:12:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:00 - INFO - train.train_snli_ve - loss is tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 638/6700 [17:52<2:47:49,  1.66s/it]11/16/2022 23:12:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:01 - INFO - train.train_snli_ve - loss is tensor(0.8498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 639/6700 [17:54<2:49:19,  1.68s/it]11/16/2022 23:12:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:03 - INFO - train.train_snli_ve - loss is tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 640/6700 [17:56<2:48:29,  1.67s/it]11/16/2022 23:12:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:05 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 641/6700 [17:58<2:50:35,  1.69s/it]11/16/2022 23:12:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:07 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 642/6700 [17:59<2:49:35,  1.68s/it]11/16/2022 23:12:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:08 - INFO - train.train_snli_ve - loss is tensor(1.0326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 643/6700 [18:01<2:51:06,  1.69s/it]11/16/2022 23:12:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.7259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:10 - INFO - train.train_snli_ve - loss is tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 644/6700 [18:03<2:50:03,  1.68s/it]11/16/2022 23:12:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:12 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 645/6700 [18:04<2:51:32,  1.70s/it]11/16/2022 23:12:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9146e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:13 - INFO - train.train_snli_ve - loss is tensor(0.8309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 646/6700 [18:06<2:48:39,  1.67s/it]11/16/2022 23:12:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6996e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:15 - INFO - train.train_snli_ve - loss is tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 647/6700 [18:08<2:49:29,  1.68s/it]11/16/2022 23:12:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4983e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:17 - INFO - train.train_snli_ve - loss is tensor(1.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 648/6700 [18:09<2:49:16,  1.68s/it]11/16/2022 23:12:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:18 - INFO - train.train_snli_ve - loss is tensor(0.9577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 649/6700 [18:11<2:51:38,  1.70s/it]11/16/2022 23:12:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.4762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:20 - INFO - train.train_snli_ve - loss is tensor(0.8676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 650/6700 [18:13<2:50:22,  1.69s/it]11/16/2022 23:12:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.8277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:22 - INFO - train.train_snli_ve - loss is tensor(0.8938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 651/6700 [18:14<2:51:11,  1.70s/it]11/16/2022 23:12:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.5030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:23 - INFO - train.train_snli_ve - loss is tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 652/6700 [18:16<2:49:00,  1.68s/it]11/16/2022 23:12:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7545e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:25 - INFO - train.train_snli_ve - loss is tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 653/6700 [18:18<2:50:38,  1.69s/it]11/16/2022 23:12:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3064e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:27 - INFO - train.train_snli_ve - loss is tensor(0.8479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 654/6700 [18:19<2:48:44,  1.67s/it]11/16/2022 23:12:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:29 - INFO - train.train_snli_ve - loss is tensor(0.8152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 655/6700 [18:21<2:51:45,  1.70s/it]11/16/2022 23:12:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.6463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:30 - INFO - train.train_snli_ve - loss is tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 656/6700 [18:23<2:49:10,  1.68s/it]11/16/2022 23:12:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:32 - INFO - train.train_snli_ve - loss is tensor(0.8691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 657/6700 [18:25<2:51:27,  1.70s/it]11/16/2022 23:12:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7052e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:34 - INFO - train.train_snli_ve - loss is tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 658/6700 [18:26<2:49:55,  1.69s/it]11/16/2022 23:12:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:35 - INFO - train.train_snli_ve - loss is tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 659/6700 [18:28<2:51:28,  1.70s/it]11/16/2022 23:12:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2291e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:37 - INFO - train.train_snli_ve - loss is tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 660/6700 [18:30<2:49:22,  1.68s/it]11/16/2022 23:12:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:39 - INFO - train.train_snli_ve - loss is tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 661/6700 [18:31<2:52:28,  1.71s/it]11/16/2022 23:12:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:40 - INFO - train.train_snli_ve - loss is tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 662/6700 [18:33<2:50:56,  1.70s/it]11/16/2022 23:12:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0340e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:42 - INFO - train.train_snli_ve - loss is tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 663/6700 [18:35<2:52:24,  1.71s/it]11/16/2022 23:12:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:44 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 664/6700 [18:36<2:50:34,  1.70s/it]11/16/2022 23:12:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4842e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:46 - INFO - train.train_snli_ve - loss is tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 665/6700 [18:38<2:51:41,  1.71s/it]11/16/2022 23:12:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6953e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:47 - INFO - train.train_snli_ve - loss is tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 666/6700 [18:40<2:49:49,  1.69s/it]11/16/2022 23:12:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5397e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:49 - INFO - train.train_snli_ve - loss is tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 667/6700 [18:42<2:52:01,  1.71s/it]11/16/2022 23:12:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0564e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:51 - INFO - train.train_snli_ve - loss is tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 668/6700 [18:43<2:50:20,  1.69s/it]11/16/2022 23:12:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9921e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:52 - INFO - train.train_snli_ve - loss is tensor(0.8673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|9         | 669/6700 [18:45<2:52:10,  1.71s/it]11/16/2022 23:12:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4454e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:54 - INFO - train.train_snli_ve - loss is tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 670/6700 [18:47<2:49:35,  1.69s/it]11/16/2022 23:12:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:56 - INFO - train.train_snli_ve - loss is tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 671/6700 [18:48<2:50:48,  1.70s/it]11/16/2022 23:12:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.9198e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:57 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 672/6700 [18:50<2:48:54,  1.68s/it]11/16/2022 23:12:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:12:59 - INFO - train.train_snli_ve - loss is tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 673/6700 [18:52<2:50:10,  1.69s/it]11/16/2022 23:13:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:01 - INFO - train.train_snli_ve - loss is tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 674/6700 [18:53<2:49:46,  1.69s/it]11/16/2022 23:13:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:02 - INFO - train.train_snli_ve - loss is tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 675/6700 [18:55<2:49:46,  1.69s/it]11/16/2022 23:13:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6626e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:04 - INFO - train.train_snli_ve - loss is tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 676/6700 [18:57<2:49:21,  1.69s/it]11/16/2022 23:13:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:06 - INFO - train.train_snli_ve - loss is tensor(0.7601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 677/6700 [18:59<2:49:53,  1.69s/it]11/16/2022 23:13:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9013e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:07 - INFO - train.train_snli_ve - loss is tensor(0.8707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 678/6700 [19:00<2:47:27,  1.67s/it]11/16/2022 23:13:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:09 - INFO - train.train_snli_ve - loss is tensor(1.0197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 679/6700 [19:02<2:50:15,  1.70s/it]11/16/2022 23:13:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5946e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:11 - INFO - train.train_snli_ve - loss is tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 680/6700 [19:04<2:47:49,  1.67s/it]11/16/2022 23:13:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.4005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:13 - INFO - train.train_snli_ve - loss is tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 681/6700 [19:05<2:48:53,  1.68s/it]11/16/2022 23:13:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4846e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:14 - INFO - train.train_snli_ve - loss is tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 682/6700 [19:07<2:47:21,  1.67s/it]11/16/2022 23:13:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4101e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:16 - INFO - train.train_snli_ve - loss is tensor(0.9092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 683/6700 [19:09<2:50:47,  1.70s/it]11/16/2022 23:13:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4197e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:18 - INFO - train.train_snli_ve - loss is tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 684/6700 [19:10<2:48:36,  1.68s/it]11/16/2022 23:13:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:19 - INFO - train.train_snli_ve - loss is tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 685/6700 [19:12<2:49:52,  1.69s/it]11/16/2022 23:13:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6697e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:21 - INFO - train.train_snli_ve - loss is tensor(0.9154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 686/6700 [19:14<2:48:07,  1.68s/it]11/16/2022 23:13:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6938e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:23 - INFO - train.train_snli_ve - loss is tensor(0.7664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 687/6700 [19:15<2:51:05,  1.71s/it]11/16/2022 23:13:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.7439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:24 - INFO - train.train_snli_ve - loss is tensor(0.9797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 688/6700 [19:17<2:48:08,  1.68s/it]11/16/2022 23:13:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5867e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:26 - INFO - train.train_snli_ve - loss is tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 689/6700 [19:19<2:49:27,  1.69s/it]11/16/2022 23:13:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:28 - INFO - train.train_snli_ve - loss is tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 690/6700 [19:20<2:48:04,  1.68s/it]11/16/2022 23:13:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:29 - INFO - train.train_snli_ve - loss is tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 691/6700 [19:22<2:49:05,  1.69s/it]11/16/2022 23:13:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:31 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 692/6700 [19:24<2:48:20,  1.68s/it]11/16/2022 23:13:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5259e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:33 - INFO - train.train_snli_ve - loss is tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 693/6700 [19:26<2:50:07,  1.70s/it]11/16/2022 23:13:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:34 - INFO - train.train_snli_ve - loss is tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 694/6700 [19:27<2:48:20,  1.68s/it]11/16/2022 23:13:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.4990e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:36 - INFO - train.train_snli_ve - loss is tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 695/6700 [19:29<2:48:27,  1.68s/it]11/16/2022 23:13:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:38 - INFO - train.train_snli_ve - loss is tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 696/6700 [19:30<2:46:08,  1.66s/it]11/16/2022 23:13:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:39 - INFO - train.train_snli_ve - loss is tensor(0.8025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 697/6700 [19:32<2:48:50,  1.69s/it]11/16/2022 23:13:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:41 - INFO - train.train_snli_ve - loss is tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 698/6700 [19:34<2:46:54,  1.67s/it]11/16/2022 23:13:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3327e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:43 - INFO - train.train_snli_ve - loss is tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 699/6700 [19:36<2:48:57,  1.69s/it]11/16/2022 23:13:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1045e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:45 - INFO - train.train_snli_ve - loss is tensor(0.9835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 700/6700 [19:37<2:48:04,  1.68s/it]11/16/2022 23:13:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0125e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:46 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 701/6700 [19:39<2:49:15,  1.69s/it]11/16/2022 23:13:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6471e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:48 - INFO - train.train_snli_ve - loss is tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 702/6700 [19:41<2:47:13,  1.67s/it]11/16/2022 23:13:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:50 - INFO - train.train_snli_ve - loss is tensor(0.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  10%|#         | 703/6700 [19:42<2:47:35,  1.68s/it]11/16/2022 23:13:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7849e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:51 - INFO - train.train_snli_ve - loss is tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 704/6700 [19:44<2:45:44,  1.66s/it]11/16/2022 23:13:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:53 - INFO - train.train_snli_ve - loss is tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 705/6700 [19:46<2:47:39,  1.68s/it]11/16/2022 23:13:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1275e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:55 - INFO - train.train_snli_ve - loss is tensor(0.8944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 706/6700 [19:47<2:46:35,  1.67s/it]11/16/2022 23:13:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.8597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:56 - INFO - train.train_snli_ve - loss is tensor(0.9226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 707/6700 [19:49<2:47:17,  1.67s/it]11/16/2022 23:13:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6925e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:13:58 - INFO - train.train_snli_ve - loss is tensor(0.8637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 708/6700 [19:51<2:45:58,  1.66s/it]11/16/2022 23:14:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:00 - INFO - train.train_snli_ve - loss is tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 709/6700 [19:52<2:47:29,  1.68s/it]11/16/2022 23:14:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:01 - INFO - train.train_snli_ve - loss is tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 710/6700 [19:54<2:46:28,  1.67s/it]11/16/2022 23:14:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:03 - INFO - train.train_snli_ve - loss is tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 711/6700 [19:56<2:48:17,  1.69s/it]11/16/2022 23:14:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:05 - INFO - train.train_snli_ve - loss is tensor(0.9300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 712/6700 [19:57<2:45:36,  1.66s/it]11/16/2022 23:14:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0879e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:06 - INFO - train.train_snli_ve - loss is tensor(0.7976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 713/6700 [19:59<2:48:39,  1.69s/it]11/16/2022 23:14:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:08 - INFO - train.train_snli_ve - loss is tensor(0.8245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 714/6700 [20:01<2:46:42,  1.67s/it]11/16/2022 23:14:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1726e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:10 - INFO - train.train_snli_ve - loss is tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 715/6700 [20:02<2:48:47,  1.69s/it]11/16/2022 23:14:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0692e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:11 - INFO - train.train_snli_ve - loss is tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 716/6700 [20:04<2:47:39,  1.68s/it]11/16/2022 23:14:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:13 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 717/6700 [20:06<2:48:23,  1.69s/it]11/16/2022 23:14:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:15 - INFO - train.train_snli_ve - loss is tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 718/6700 [20:07<2:47:21,  1.68s/it]11/16/2022 23:14:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4866e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:16 - INFO - train.train_snli_ve - loss is tensor(0.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 719/6700 [20:09<2:48:54,  1.69s/it]11/16/2022 23:14:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:18 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 720/6700 [20:11<2:49:11,  1.70s/it]11/16/2022 23:14:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2834e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:20 - INFO - train.train_snli_ve - loss is tensor(0.8832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 721/6700 [20:13<2:49:11,  1.70s/it]11/16/2022 23:14:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.3571e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:21 - INFO - train.train_snli_ve - loss is tensor(0.8869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 722/6700 [20:14<2:48:15,  1.69s/it]11/16/2022 23:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.5044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:23 - INFO - train.train_snli_ve - loss is tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 723/6700 [20:16<2:49:18,  1.70s/it]11/16/2022 23:14:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:25 - INFO - train.train_snli_ve - loss is tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 724/6700 [20:18<2:47:08,  1.68s/it]11/16/2022 23:14:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:27 - INFO - train.train_snli_ve - loss is tensor(0.9251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 725/6700 [20:19<2:48:13,  1.69s/it]11/16/2022 23:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2112e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:28 - INFO - train.train_snli_ve - loss is tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 726/6700 [20:21<2:47:14,  1.68s/it]11/16/2022 23:14:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.6720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:30 - INFO - train.train_snli_ve - loss is tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 727/6700 [20:23<2:48:58,  1.70s/it]11/16/2022 23:14:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:32 - INFO - train.train_snli_ve - loss is tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 728/6700 [20:24<2:46:59,  1.68s/it]11/16/2022 23:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.3401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:33 - INFO - train.train_snli_ve - loss is tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 729/6700 [20:26<2:47:59,  1.69s/it]11/16/2022 23:14:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:35 - INFO - train.train_snli_ve - loss is tensor(0.8464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 730/6700 [20:28<2:46:53,  1.68s/it]11/16/2022 23:14:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:37 - INFO - train.train_snli_ve - loss is tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 731/6700 [20:29<2:48:35,  1.69s/it]11/16/2022 23:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:38 - INFO - train.train_snli_ve - loss is tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 732/6700 [20:31<2:47:16,  1.68s/it]11/16/2022 23:14:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6605e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:40 - INFO - train.train_snli_ve - loss is tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 733/6700 [20:33<2:47:46,  1.69s/it]11/16/2022 23:14:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:42 - INFO - train.train_snli_ve - loss is tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 734/6700 [20:34<2:45:55,  1.67s/it]11/16/2022 23:14:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:43 - INFO - train.train_snli_ve - loss is tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 735/6700 [20:36<2:48:16,  1.69s/it]11/16/2022 23:14:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:45 - INFO - train.train_snli_ve - loss is tensor(0.8162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#         | 736/6700 [20:38<2:48:51,  1.70s/it]11/16/2022 23:14:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:47 - INFO - train.train_snli_ve - loss is tensor(0.8918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 737/6700 [20:40<2:50:03,  1.71s/it]11/16/2022 23:14:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:48 - INFO - train.train_snli_ve - loss is tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 738/6700 [20:41<2:47:19,  1.68s/it]11/16/2022 23:14:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:50 - INFO - train.train_snli_ve - loss is tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 739/6700 [20:43<2:48:03,  1.69s/it]11/16/2022 23:14:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:52 - INFO - train.train_snli_ve - loss is tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 740/6700 [20:45<2:46:02,  1.67s/it]11/16/2022 23:14:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5484e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:54 - INFO - train.train_snli_ve - loss is tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 741/6700 [20:46<2:47:20,  1.68s/it]11/16/2022 23:14:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7319e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:55 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 742/6700 [20:48<2:46:26,  1.68s/it]11/16/2022 23:14:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4606e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:57 - INFO - train.train_snli_ve - loss is tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 743/6700 [20:50<2:48:13,  1.69s/it]11/16/2022 23:14:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:14:59 - INFO - train.train_snli_ve - loss is tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 744/6700 [20:51<2:46:22,  1.68s/it]11/16/2022 23:15:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.7633e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:00 - INFO - train.train_snli_ve - loss is tensor(1.1012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 745/6700 [20:53<2:47:22,  1.69s/it]11/16/2022 23:15:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:02 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 746/6700 [20:55<2:46:06,  1.67s/it]11/16/2022 23:15:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4964e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:04 - INFO - train.train_snli_ve - loss is tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 747/6700 [20:56<2:47:10,  1.68s/it]11/16/2022 23:15:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6944e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:05 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 748/6700 [20:58<2:47:01,  1.68s/it]11/16/2022 23:15:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0560e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:07 - INFO - train.train_snli_ve - loss is tensor(1.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 749/6700 [21:00<2:49:41,  1.71s/it]11/16/2022 23:15:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8868e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:09 - INFO - train.train_snli_ve - loss is tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 750/6700 [21:01<2:47:43,  1.69s/it]11/16/2022 23:15:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.7716e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:10 - INFO - train.train_snli_ve - loss is tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 751/6700 [21:03<2:47:42,  1.69s/it]11/16/2022 23:15:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5870e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:12 - INFO - train.train_snli_ve - loss is tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 752/6700 [21:05<2:46:55,  1.68s/it]11/16/2022 23:15:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:14 - INFO - train.train_snli_ve - loss is tensor(1.2002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 753/6700 [21:06<2:47:07,  1.69s/it]11/16/2022 23:15:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.8910e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:15 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 754/6700 [21:08<2:46:40,  1.68s/it]11/16/2022 23:15:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3200e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:17 - INFO - train.train_snli_ve - loss is tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 755/6700 [21:10<2:46:40,  1.68s/it]11/16/2022 23:15:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:19 - INFO - train.train_snli_ve - loss is tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 756/6700 [21:11<2:45:36,  1.67s/it]11/16/2022 23:15:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:21 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 757/6700 [21:13<2:47:30,  1.69s/it]11/16/2022 23:15:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4014e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:22 - INFO - train.train_snli_ve - loss is tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 758/6700 [21:15<2:45:56,  1.68s/it]11/16/2022 23:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:24 - INFO - train.train_snli_ve - loss is tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 759/6700 [21:17<2:48:02,  1.70s/it]11/16/2022 23:15:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5952e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:26 - INFO - train.train_snli_ve - loss is tensor(0.8439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 760/6700 [21:18<2:46:20,  1.68s/it]11/16/2022 23:15:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.4616e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:27 - INFO - train.train_snli_ve - loss is tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 761/6700 [21:20<2:48:04,  1.70s/it]11/16/2022 23:15:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.5507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:29 - INFO - train.train_snli_ve - loss is tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 762/6700 [21:22<2:46:19,  1.68s/it]11/16/2022 23:15:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:31 - INFO - train.train_snli_ve - loss is tensor(0.9773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 763/6700 [21:23<2:47:59,  1.70s/it]11/16/2022 23:15:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:32 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 764/6700 [21:25<2:47:08,  1.69s/it]11/16/2022 23:15:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.1356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:34 - INFO - train.train_snli_ve - loss is tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 765/6700 [21:27<2:49:14,  1.71s/it]11/16/2022 23:15:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:36 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 766/6700 [21:28<2:47:03,  1.69s/it]11/16/2022 23:15:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5552e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:37 - INFO - train.train_snli_ve - loss is tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 767/6700 [21:30<2:48:16,  1.70s/it]11/16/2022 23:15:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2215e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:39 - INFO - train.train_snli_ve - loss is tensor(0.9260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 768/6700 [21:32<2:45:59,  1.68s/it]11/16/2022 23:15:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5129e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:41 - INFO - train.train_snli_ve - loss is tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 769/6700 [21:34<2:47:09,  1.69s/it]11/16/2022 23:15:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.4843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:42 - INFO - train.train_snli_ve - loss is tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  11%|#1        | 770/6700 [21:35<2:45:36,  1.68s/it]11/16/2022 23:15:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:44 - INFO - train.train_snli_ve - loss is tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 771/6700 [21:37<2:49:41,  1.72s/it]11/16/2022 23:15:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.1404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:46 - INFO - train.train_snli_ve - loss is tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 772/6700 [21:39<2:46:51,  1.69s/it]11/16/2022 23:15:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:48 - INFO - train.train_snli_ve - loss is tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 773/6700 [21:40<2:48:53,  1.71s/it]11/16/2022 23:15:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2827e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:49 - INFO - train.train_snli_ve - loss is tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 774/6700 [21:42<2:46:41,  1.69s/it]11/16/2022 23:15:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.5653e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:51 - INFO - train.train_snli_ve - loss is tensor(1.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 775/6700 [21:44<2:48:50,  1.71s/it]11/16/2022 23:15:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1625e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:53 - INFO - train.train_snli_ve - loss is tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 776/6700 [21:45<2:47:13,  1.69s/it]11/16/2022 23:15:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:54 - INFO - train.train_snli_ve - loss is tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 777/6700 [21:47<2:49:20,  1.72s/it]11/16/2022 23:15:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3912e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:56 - INFO - train.train_snli_ve - loss is tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 778/6700 [21:49<2:46:24,  1.69s/it]11/16/2022 23:15:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1619e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:58 - INFO - train.train_snli_ve - loss is tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 779/6700 [21:51<2:48:23,  1.71s/it]11/16/2022 23:15:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5115e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:15:59 - INFO - train.train_snli_ve - loss is tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 780/6700 [21:52<2:46:31,  1.69s/it]11/16/2022 23:16:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6641e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:01 - INFO - train.train_snli_ve - loss is tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 781/6700 [21:54<2:48:12,  1.71s/it]11/16/2022 23:16:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:03 - INFO - train.train_snli_ve - loss is tensor(0.5841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 782/6700 [21:56<2:46:13,  1.69s/it]11/16/2022 23:16:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:05 - INFO - train.train_snli_ve - loss is tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 783/6700 [21:57<2:47:45,  1.70s/it]11/16/2022 23:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1688e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:06 - INFO - train.train_snli_ve - loss is tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 784/6700 [21:59<2:45:46,  1.68s/it]11/16/2022 23:16:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4716e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:08 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 785/6700 [22:01<2:46:12,  1.69s/it]11/16/2022 23:16:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1135e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:10 - INFO - train.train_snli_ve - loss is tensor(0.8495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 786/6700 [22:02<2:44:50,  1.67s/it]11/16/2022 23:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:11 - INFO - train.train_snli_ve - loss is tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 787/6700 [22:04<2:46:01,  1.68s/it]11/16/2022 23:16:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:13 - INFO - train.train_snli_ve - loss is tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 788/6700 [22:06<2:44:39,  1.67s/it]11/16/2022 23:16:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5193e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:15 - INFO - train.train_snli_ve - loss is tensor(1.1892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 789/6700 [22:07<2:47:03,  1.70s/it]11/16/2022 23:16:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.0747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:16 - INFO - train.train_snli_ve - loss is tensor(1.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 790/6700 [22:09<2:45:40,  1.68s/it]11/16/2022 23:16:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.8316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:18 - INFO - train.train_snli_ve - loss is tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 791/6700 [22:11<2:47:04,  1.70s/it]11/16/2022 23:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.8071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:20 - INFO - train.train_snli_ve - loss is tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 792/6700 [22:12<2:43:57,  1.67s/it]11/16/2022 23:16:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.9410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:21 - INFO - train.train_snli_ve - loss is tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 793/6700 [22:14<2:45:20,  1.68s/it]11/16/2022 23:16:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.0220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:23 - INFO - train.train_snli_ve - loss is tensor(0.9187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 794/6700 [22:16<2:45:35,  1.68s/it]11/16/2022 23:16:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.2529e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:25 - INFO - train.train_snli_ve - loss is tensor(0.8877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 795/6700 [22:17<2:46:20,  1.69s/it]11/16/2022 23:16:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0503e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:26 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 796/6700 [22:19<2:46:54,  1.70s/it]11/16/2022 23:16:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.8617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:28 - INFO - train.train_snli_ve - loss is tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 797/6700 [22:21<2:48:08,  1.71s/it]11/16/2022 23:16:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9877e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:30 - INFO - train.train_snli_ve - loss is tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 798/6700 [22:23<2:45:29,  1.68s/it]11/16/2022 23:16:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7465e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:32 - INFO - train.train_snli_ve - loss is tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 799/6700 [22:24<2:47:52,  1.71s/it]11/16/2022 23:16:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:33 - INFO - train.train_snli_ve - loss is tensor(0.6396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 800/6700 [22:26<2:46:05,  1.69s/it]11/16/2022 23:16:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:35 - INFO - train.train_snli_ve - loss is tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 801/6700 [22:28<2:46:55,  1.70s/it]11/16/2022 23:16:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:37 - INFO - train.train_snli_ve - loss is tensor(0.8696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 802/6700 [22:29<2:45:13,  1.68s/it]11/16/2022 23:16:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:38 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#1        | 803/6700 [22:31<2:45:40,  1.69s/it]11/16/2022 23:16:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.6901e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:40 - INFO - train.train_snli_ve - loss is tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 804/6700 [22:33<2:43:11,  1.66s/it]11/16/2022 23:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8412e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:42 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 805/6700 [22:34<2:44:02,  1.67s/it]11/16/2022 23:16:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1506e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:43 - INFO - train.train_snli_ve - loss is tensor(0.9129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 806/6700 [22:36<2:43:04,  1.66s/it]11/16/2022 23:16:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:45 - INFO - train.train_snli_ve - loss is tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 807/6700 [22:38<2:45:55,  1.69s/it]11/16/2022 23:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.0122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:47 - INFO - train.train_snli_ve - loss is tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 808/6700 [22:39<2:45:53,  1.69s/it]11/16/2022 23:16:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:48 - INFO - train.train_snli_ve - loss is tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 809/6700 [22:41<2:46:00,  1.69s/it]11/16/2022 23:16:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:50 - INFO - train.train_snli_ve - loss is tensor(1.2032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 810/6700 [22:43<2:46:39,  1.70s/it]11/16/2022 23:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.6049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:52 - INFO - train.train_snli_ve - loss is tensor(0.8536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 811/6700 [22:45<2:48:14,  1.71s/it]11/16/2022 23:16:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.4228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:54 - INFO - train.train_snli_ve - loss is tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 812/6700 [22:46<2:46:59,  1.70s/it]11/16/2022 23:16:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.4395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:55 - INFO - train.train_snli_ve - loss is tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 813/6700 [22:48<2:47:44,  1.71s/it]11/16/2022 23:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.3266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:57 - INFO - train.train_snli_ve - loss is tensor(0.8554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 814/6700 [22:50<2:46:28,  1.70s/it]11/16/2022 23:16:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4732e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:16:59 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 815/6700 [22:51<2:47:29,  1.71s/it]11/16/2022 23:17:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5264e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:00 - INFO - train.train_snli_ve - loss is tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 816/6700 [22:53<2:45:19,  1.69s/it]11/16/2022 23:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.1955e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:02 - INFO - train.train_snli_ve - loss is tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 817/6700 [22:55<2:46:09,  1.69s/it]11/16/2022 23:17:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:04 - INFO - train.train_snli_ve - loss is tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 818/6700 [22:56<2:45:00,  1.68s/it]11/16/2022 23:17:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0869e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:05 - INFO - train.train_snli_ve - loss is tensor(0.9492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 819/6700 [22:58<2:47:12,  1.71s/it]11/16/2022 23:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.2191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:07 - INFO - train.train_snli_ve - loss is tensor(0.8681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 820/6700 [23:00<2:44:29,  1.68s/it]11/16/2022 23:17:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1877e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:09 - INFO - train.train_snli_ve - loss is tensor(0.9518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 821/6700 [23:01<2:45:29,  1.69s/it]11/16/2022 23:17:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1382e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:10 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 822/6700 [23:03<2:45:08,  1.69s/it]11/16/2022 23:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:12 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 823/6700 [23:05<2:46:29,  1.70s/it]11/16/2022 23:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:14 - INFO - train.train_snli_ve - loss is tensor(0.8598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 824/6700 [23:06<2:44:12,  1.68s/it]11/16/2022 23:17:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.9603e-07, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:15 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 825/6700 [23:08<2:45:33,  1.69s/it]11/16/2022 23:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1920e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:17 - INFO - train.train_snli_ve - loss is tensor(0.8936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 826/6700 [23:10<2:44:53,  1.68s/it]11/16/2022 23:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.7491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:19 - INFO - train.train_snli_ve - loss is tensor(0.8820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 827/6700 [23:12<2:46:46,  1.70s/it]11/16/2022 23:17:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7529e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:21 - INFO - train.train_snli_ve - loss is tensor(0.8994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 828/6700 [23:13<2:45:51,  1.69s/it]11/16/2022 23:17:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.1678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:22 - INFO - train.train_snli_ve - loss is tensor(0.7705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 829/6700 [23:15<2:46:50,  1.71s/it]11/16/2022 23:17:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6447e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:24 - INFO - train.train_snli_ve - loss is tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 830/6700 [23:17<2:45:54,  1.70s/it]11/16/2022 23:17:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6099e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:26 - INFO - train.train_snli_ve - loss is tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 831/6700 [23:18<2:46:15,  1.70s/it]11/16/2022 23:17:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:27 - INFO - train.train_snli_ve - loss is tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 832/6700 [23:20<2:44:04,  1.68s/it]11/16/2022 23:17:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.1591e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:29 - INFO - train.train_snli_ve - loss is tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 833/6700 [23:22<2:45:35,  1.69s/it]11/16/2022 23:17:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3632e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:31 - INFO - train.train_snli_ve - loss is tensor(1.0347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 834/6700 [23:23<2:43:59,  1.68s/it]11/16/2022 23:17:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.5362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:32 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 835/6700 [23:25<2:46:24,  1.70s/it]11/16/2022 23:17:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.3693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:34 - INFO - train.train_snli_ve - loss is tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 836/6700 [23:27<2:44:23,  1.68s/it]11/16/2022 23:17:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1691e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:36 - INFO - train.train_snli_ve - loss is tensor(0.8281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  12%|#2        | 837/6700 [23:29<2:46:09,  1.70s/it]11/16/2022 23:17:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.1111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:37 - INFO - train.train_snli_ve - loss is tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 838/6700 [23:30<2:44:40,  1.69s/it]11/16/2022 23:17:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2043e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:39 - INFO - train.train_snli_ve - loss is tensor(0.9390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 839/6700 [23:32<2:46:43,  1.71s/it]11/16/2022 23:17:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.3456e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:41 - INFO - train.train_snli_ve - loss is tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 840/6700 [23:34<2:44:05,  1.68s/it]11/16/2022 23:17:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:43 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 841/6700 [23:35<2:46:08,  1.70s/it]11/16/2022 23:17:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1608e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:44 - INFO - train.train_snli_ve - loss is tensor(0.9143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 842/6700 [23:37<2:44:08,  1.68s/it]11/16/2022 23:17:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.3375e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:46 - INFO - train.train_snli_ve - loss is tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 843/6700 [23:39<2:44:20,  1.68s/it]11/16/2022 23:17:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.3977e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:48 - INFO - train.train_snli_ve - loss is tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 844/6700 [23:40<2:42:34,  1.67s/it]11/16/2022 23:17:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.8909e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:49 - INFO - train.train_snli_ve - loss is tensor(1.0165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 845/6700 [23:42<2:44:41,  1.69s/it]11/16/2022 23:17:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.5765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:51 - INFO - train.train_snli_ve - loss is tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 846/6700 [23:44<2:41:57,  1.66s/it]11/16/2022 23:17:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.8597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:53 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 847/6700 [23:45<2:43:49,  1.68s/it]11/16/2022 23:17:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.7226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:54 - INFO - train.train_snli_ve - loss is tensor(0.9659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 848/6700 [23:47<2:42:13,  1.66s/it]11/16/2022 23:17:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.0597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:56 - INFO - train.train_snli_ve - loss is tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 849/6700 [23:49<2:43:46,  1.68s/it]11/16/2022 23:17:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3606e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:58 - INFO - train.train_snli_ve - loss is tensor(1.0492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 850/6700 [23:50<2:43:54,  1.68s/it]11/16/2022 23:17:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.7305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:17:59 - INFO - train.train_snli_ve - loss is tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 851/6700 [23:52<2:44:19,  1.69s/it]11/16/2022 23:18:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.6071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:01 - INFO - train.train_snli_ve - loss is tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 852/6700 [23:54<2:42:42,  1.67s/it]11/16/2022 23:18:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.6987e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:03 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 853/6700 [23:55<2:44:55,  1.69s/it]11/16/2022 23:18:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.7105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:04 - INFO - train.train_snli_ve - loss is tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 854/6700 [23:57<2:44:21,  1.69s/it]11/16/2022 23:18:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.6060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:06 - INFO - train.train_snli_ve - loss is tensor(1.0770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 855/6700 [23:59<2:45:36,  1.70s/it]11/16/2022 23:18:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.0353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:08 - INFO - train.train_snli_ve - loss is tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 856/6700 [24:00<2:43:31,  1.68s/it]11/16/2022 23:18:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.7849e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:09 - INFO - train.train_snli_ve - loss is tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 857/6700 [24:02<2:44:57,  1.69s/it]11/16/2022 23:18:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6735e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:11 - INFO - train.train_snli_ve - loss is tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 858/6700 [24:04<2:44:27,  1.69s/it]11/16/2022 23:18:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:13 - INFO - train.train_snli_ve - loss is tensor(0.9521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 859/6700 [24:06<2:45:28,  1.70s/it]11/16/2022 23:18:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.6321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:15 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 860/6700 [24:07<2:43:25,  1.68s/it]11/16/2022 23:18:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.9999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:16 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 861/6700 [24:09<2:45:22,  1.70s/it]11/16/2022 23:18:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:18 - INFO - train.train_snli_ve - loss is tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 862/6700 [24:11<2:42:30,  1.67s/it]11/16/2022 23:18:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.1229e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:20 - INFO - train.train_snli_ve - loss is tensor(0.8453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 863/6700 [24:12<2:44:42,  1.69s/it]11/16/2022 23:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3572e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:21 - INFO - train.train_snli_ve - loss is tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 864/6700 [24:14<2:43:03,  1.68s/it]11/16/2022 23:18:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.1043e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:23 - INFO - train.train_snli_ve - loss is tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 865/6700 [24:16<2:43:12,  1.68s/it]11/16/2022 23:18:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.9850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:25 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 866/6700 [24:17<2:42:07,  1.67s/it]11/16/2022 23:18:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5419e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:26 - INFO - train.train_snli_ve - loss is tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 867/6700 [24:19<2:45:03,  1.70s/it]11/16/2022 23:18:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.1863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:28 - INFO - train.train_snli_ve - loss is tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 868/6700 [24:21<2:43:39,  1.68s/it]11/16/2022 23:18:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.5887e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:30 - INFO - train.train_snli_ve - loss is tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 869/6700 [24:22<2:44:34,  1.69s/it]11/16/2022 23:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3299e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:31 - INFO - train.train_snli_ve - loss is tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#2        | 870/6700 [24:24<2:42:51,  1.68s/it]11/16/2022 23:18:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:33 - INFO - train.train_snli_ve - loss is tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 871/6700 [24:26<2:44:30,  1.69s/it]11/16/2022 23:18:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:35 - INFO - train.train_snli_ve - loss is tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 872/6700 [24:27<2:41:36,  1.66s/it]11/16/2022 23:18:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.8496e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:36 - INFO - train.train_snli_ve - loss is tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 873/6700 [24:29<2:42:06,  1.67s/it]11/16/2022 23:18:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3819e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:38 - INFO - train.train_snli_ve - loss is tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 874/6700 [24:31<2:41:02,  1.66s/it]11/16/2022 23:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1870e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:40 - INFO - train.train_snli_ve - loss is tensor(0.8805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 875/6700 [24:32<2:41:58,  1.67s/it]11/16/2022 23:18:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:41 - INFO - train.train_snli_ve - loss is tensor(0.9375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 876/6700 [24:34<2:40:01,  1.65s/it]11/16/2022 23:18:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3338e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:43 - INFO - train.train_snli_ve - loss is tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 877/6700 [24:36<2:42:48,  1.68s/it]11/16/2022 23:18:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.8134e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:45 - INFO - train.train_snli_ve - loss is tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 878/6700 [24:37<2:41:04,  1.66s/it]11/16/2022 23:18:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:46 - INFO - train.train_snli_ve - loss is tensor(0.7344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 879/6700 [24:39<2:42:15,  1.67s/it]11/16/2022 23:18:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6234e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:48 - INFO - train.train_snli_ve - loss is tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 880/6700 [24:41<2:40:02,  1.65s/it]11/16/2022 23:18:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:50 - INFO - train.train_snli_ve - loss is tensor(1.0673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 881/6700 [24:42<2:44:23,  1.70s/it]11/16/2022 23:18:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4707e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:51 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 882/6700 [24:44<2:42:15,  1.67s/it]11/16/2022 23:18:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:53 - INFO - train.train_snli_ve - loss is tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 883/6700 [24:46<2:43:09,  1.68s/it]11/16/2022 23:18:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:55 - INFO - train.train_snli_ve - loss is tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 884/6700 [24:47<2:42:10,  1.67s/it]11/16/2022 23:18:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:56 - INFO - train.train_snli_ve - loss is tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 885/6700 [24:49<2:42:43,  1.68s/it]11/16/2022 23:18:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4956e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:18:58 - INFO - train.train_snli_ve - loss is tensor(0.8190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 886/6700 [24:51<2:40:58,  1.66s/it]11/16/2022 23:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.7771e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:00 - INFO - train.train_snli_ve - loss is tensor(1.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 887/6700 [24:52<2:42:22,  1.68s/it]11/16/2022 23:19:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.9993e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:01 - INFO - train.train_snli_ve - loss is tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 888/6700 [24:54<2:40:30,  1.66s/it]11/16/2022 23:19:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8493e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:03 - INFO - train.train_snli_ve - loss is tensor(0.8061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 889/6700 [24:56<2:42:26,  1.68s/it]11/16/2022 23:19:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:05 - INFO - train.train_snli_ve - loss is tensor(0.9212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 890/6700 [24:57<2:40:42,  1.66s/it]11/16/2022 23:19:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.9269e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:06 - INFO - train.train_snli_ve - loss is tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 891/6700 [24:59<2:43:51,  1.69s/it]11/16/2022 23:19:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0348e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:08 - INFO - train.train_snli_ve - loss is tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 892/6700 [25:01<2:42:25,  1.68s/it]11/16/2022 23:19:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:10 - INFO - train.train_snli_ve - loss is tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 893/6700 [25:03<2:43:22,  1.69s/it]11/16/2022 23:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:12 - INFO - train.train_snli_ve - loss is tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 894/6700 [25:04<2:43:19,  1.69s/it]11/16/2022 23:19:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:13 - INFO - train.train_snli_ve - loss is tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 895/6700 [25:06<2:44:18,  1.70s/it]11/16/2022 23:19:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:15 - INFO - train.train_snli_ve - loss is tensor(0.9310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 896/6700 [25:08<2:42:29,  1.68s/it]11/16/2022 23:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3753e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:17 - INFO - train.train_snli_ve - loss is tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 897/6700 [25:09<2:44:02,  1.70s/it]11/16/2022 23:19:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7377e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:18 - INFO - train.train_snli_ve - loss is tensor(0.8223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 898/6700 [25:11<2:41:49,  1.67s/it]11/16/2022 23:19:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:20 - INFO - train.train_snli_ve - loss is tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 899/6700 [25:13<2:43:09,  1.69s/it]11/16/2022 23:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.5039e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:22 - INFO - train.train_snli_ve - loss is tensor(0.8664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 900/6700 [25:14<2:42:00,  1.68s/it]11/16/2022 23:19:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4858e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:23 - INFO - train.train_snli_ve - loss is tensor(0.9749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 901/6700 [25:16<2:44:19,  1.70s/it]11/16/2022 23:19:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.1473e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:25 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 902/6700 [25:18<2:43:54,  1.70s/it]11/16/2022 23:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.3875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:27 - INFO - train.train_snli_ve - loss is tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 903/6700 [25:19<2:45:08,  1.71s/it]11/16/2022 23:19:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.3578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:28 - INFO - train.train_snli_ve - loss is tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  13%|#3        | 904/6700 [25:21<2:43:00,  1.69s/it]11/16/2022 23:19:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:30 - INFO - train.train_snli_ve - loss is tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 905/6700 [25:23<2:43:33,  1.69s/it]11/16/2022 23:19:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:32 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 906/6700 [25:24<2:41:20,  1.67s/it]11/16/2022 23:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6919e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:33 - INFO - train.train_snli_ve - loss is tensor(0.8200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 907/6700 [25:26<2:43:53,  1.70s/it]11/16/2022 23:19:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5913e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:35 - INFO - train.train_snli_ve - loss is tensor(1.0170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 908/6700 [25:28<2:40:47,  1.67s/it]11/16/2022 23:19:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.3898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:37 - INFO - train.train_snli_ve - loss is tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 909/6700 [25:29<2:42:00,  1.68s/it]11/16/2022 23:19:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1039e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:38 - INFO - train.train_snli_ve - loss is tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 910/6700 [25:31<2:40:28,  1.66s/it]11/16/2022 23:19:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6770e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:40 - INFO - train.train_snli_ve - loss is tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 911/6700 [25:33<2:42:40,  1.69s/it]11/16/2022 23:19:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.9178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:42 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 912/6700 [25:35<2:41:43,  1.68s/it]11/16/2022 23:19:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:44 - INFO - train.train_snli_ve - loss is tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 913/6700 [25:36<2:43:22,  1.69s/it]11/16/2022 23:19:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:45 - INFO - train.train_snli_ve - loss is tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 914/6700 [25:38<2:41:48,  1.68s/it]11/16/2022 23:19:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.1085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:47 - INFO - train.train_snli_ve - loss is tensor(0.8702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 915/6700 [25:40<2:44:01,  1.70s/it]11/16/2022 23:19:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:49 - INFO - train.train_snli_ve - loss is tensor(0.7429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 916/6700 [25:41<2:42:09,  1.68s/it]11/16/2022 23:19:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.1523e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:50 - INFO - train.train_snli_ve - loss is tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 917/6700 [25:43<2:43:38,  1.70s/it]11/16/2022 23:19:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.8095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:52 - INFO - train.train_snli_ve - loss is tensor(0.8953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 918/6700 [25:45<2:42:06,  1.68s/it]11/16/2022 23:19:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6780e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:54 - INFO - train.train_snli_ve - loss is tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 919/6700 [25:46<2:42:17,  1.68s/it]11/16/2022 23:19:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1875e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:55 - INFO - train.train_snli_ve - loss is tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 920/6700 [25:48<2:41:54,  1.68s/it]11/16/2022 23:19:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:57 - INFO - train.train_snli_ve - loss is tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 921/6700 [25:50<2:43:13,  1.69s/it]11/16/2022 23:19:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:19:59 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 922/6700 [25:51<2:41:12,  1.67s/it]11/16/2022 23:20:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8168e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:00 - INFO - train.train_snli_ve - loss is tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 923/6700 [25:53<2:42:26,  1.69s/it]11/16/2022 23:20:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0400e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:02 - INFO - train.train_snli_ve - loss is tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 924/6700 [25:55<2:40:14,  1.66s/it]11/16/2022 23:20:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:04 - INFO - train.train_snli_ve - loss is tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 925/6700 [25:56<2:42:06,  1.68s/it]11/16/2022 23:20:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.8804e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:05 - INFO - train.train_snli_ve - loss is tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 926/6700 [25:58<2:40:37,  1.67s/it]11/16/2022 23:20:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.5494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:07 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 927/6700 [26:00<2:41:56,  1.68s/it]11/16/2022 23:20:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:09 - INFO - train.train_snli_ve - loss is tensor(1.0988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 928/6700 [26:01<2:40:01,  1.66s/it]11/16/2022 23:20:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.0629e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:10 - INFO - train.train_snli_ve - loss is tensor(0.7716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 929/6700 [26:03<2:39:47,  1.66s/it]11/16/2022 23:20:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:12 - INFO - train.train_snli_ve - loss is tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 930/6700 [26:05<2:39:20,  1.66s/it]11/16/2022 23:20:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:14 - INFO - train.train_snli_ve - loss is tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 931/6700 [26:06<2:40:12,  1.67s/it]11/16/2022 23:20:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:15 - INFO - train.train_snli_ve - loss is tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 932/6700 [26:08<2:39:29,  1.66s/it]11/16/2022 23:20:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:17 - INFO - train.train_snli_ve - loss is tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 933/6700 [26:10<2:41:24,  1.68s/it]11/16/2022 23:20:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.6330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:19 - INFO - train.train_snli_ve - loss is tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 934/6700 [26:11<2:39:53,  1.66s/it]11/16/2022 23:20:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.5246e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:20 - INFO - train.train_snli_ve - loss is tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 935/6700 [26:13<2:42:35,  1.69s/it]11/16/2022 23:20:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7812e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:22 - INFO - train.train_snli_ve - loss is tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 936/6700 [26:15<2:40:35,  1.67s/it]11/16/2022 23:20:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1941e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:24 - INFO - train.train_snli_ve - loss is tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#3        | 937/6700 [26:16<2:42:02,  1.69s/it]11/16/2022 23:20:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.2460e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:25 - INFO - train.train_snli_ve - loss is tensor(0.7714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 938/6700 [26:18<2:39:03,  1.66s/it]11/16/2022 23:20:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:27 - INFO - train.train_snli_ve - loss is tensor(0.9484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 939/6700 [26:20<2:40:32,  1.67s/it]11/16/2022 23:20:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.5725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:29 - INFO - train.train_snli_ve - loss is tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 940/6700 [26:21<2:40:06,  1.67s/it]11/16/2022 23:20:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.3181e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:30 - INFO - train.train_snli_ve - loss is tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 941/6700 [26:23<2:41:05,  1.68s/it]11/16/2022 23:20:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:32 - INFO - train.train_snli_ve - loss is tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 942/6700 [26:25<2:40:22,  1.67s/it]11/16/2022 23:20:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:34 - INFO - train.train_snli_ve - loss is tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 943/6700 [26:27<2:41:16,  1.68s/it]11/16/2022 23:20:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9662e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:35 - INFO - train.train_snli_ve - loss is tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 944/6700 [26:28<2:39:14,  1.66s/it]11/16/2022 23:20:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:37 - INFO - train.train_snli_ve - loss is tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 945/6700 [26:30<2:40:35,  1.67s/it]11/16/2022 23:20:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.1356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:39 - INFO - train.train_snli_ve - loss is tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 946/6700 [26:31<2:39:41,  1.67s/it]11/16/2022 23:20:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:41 - INFO - train.train_snli_ve - loss is tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 947/6700 [26:33<2:42:19,  1.69s/it]11/16/2022 23:20:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.5767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:42 - INFO - train.train_snli_ve - loss is tensor(0.9189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 948/6700 [26:35<2:40:56,  1.68s/it]11/16/2022 23:20:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1311e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:44 - INFO - train.train_snli_ve - loss is tensor(1.1100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 949/6700 [26:37<2:41:42,  1.69s/it]11/16/2022 23:20:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.5546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:46 - INFO - train.train_snli_ve - loss is tensor(0.7634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 950/6700 [26:38<2:40:56,  1.68s/it]11/16/2022 23:20:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:47 - INFO - train.train_snli_ve - loss is tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 951/6700 [26:40<2:41:02,  1.68s/it]11/16/2022 23:20:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.9011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:49 - INFO - train.train_snli_ve - loss is tensor(0.8800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 952/6700 [26:42<2:40:52,  1.68s/it]11/16/2022 23:20:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.5154e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:51 - INFO - train.train_snli_ve - loss is tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 953/6700 [26:43<2:41:46,  1.69s/it]11/16/2022 23:20:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.6517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:52 - INFO - train.train_snli_ve - loss is tensor(1.0229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 954/6700 [26:45<2:39:53,  1.67s/it]11/16/2022 23:20:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.5527e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:54 - INFO - train.train_snli_ve - loss is tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 955/6700 [26:47<2:41:03,  1.68s/it]11/16/2022 23:20:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.0915e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:56 - INFO - train.train_snli_ve - loss is tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 956/6700 [26:48<2:39:28,  1.67s/it]11/16/2022 23:20:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.0365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:57 - INFO - train.train_snli_ve - loss is tensor(0.9992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 957/6700 [26:50<2:41:45,  1.69s/it]11/16/2022 23:20:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.6802e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:20:59 - INFO - train.train_snli_ve - loss is tensor(0.9446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 958/6700 [26:52<2:39:09,  1.66s/it]11/16/2022 23:21:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.7557e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:01 - INFO - train.train_snli_ve - loss is tensor(0.5450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 959/6700 [26:53<2:40:44,  1.68s/it]11/16/2022 23:21:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.4493e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:02 - INFO - train.train_snli_ve - loss is tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 960/6700 [26:55<2:40:23,  1.68s/it]11/16/2022 23:21:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:04 - INFO - train.train_snli_ve - loss is tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 961/6700 [26:57<2:42:43,  1.70s/it]11/16/2022 23:21:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.4840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:06 - INFO - train.train_snli_ve - loss is tensor(1.0074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 962/6700 [26:58<2:42:06,  1.70s/it]11/16/2022 23:21:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9791e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:08 - INFO - train.train_snli_ve - loss is tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 963/6700 [27:00<2:43:39,  1.71s/it]11/16/2022 23:21:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2092e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:09 - INFO - train.train_snli_ve - loss is tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 964/6700 [27:02<2:41:23,  1.69s/it]11/16/2022 23:21:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.8899e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:11 - INFO - train.train_snli_ve - loss is tensor(0.8436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 965/6700 [27:04<2:43:39,  1.71s/it]11/16/2022 23:21:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:13 - INFO - train.train_snli_ve - loss is tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 966/6700 [27:05<2:42:58,  1.71s/it]11/16/2022 23:21:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:14 - INFO - train.train_snli_ve - loss is tensor(0.8496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 967/6700 [27:07<2:43:48,  1.71s/it]11/16/2022 23:21:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.0713e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:16 - INFO - train.train_snli_ve - loss is tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 968/6700 [27:09<2:42:08,  1.70s/it]11/16/2022 23:21:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.8645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:18 - INFO - train.train_snli_ve - loss is tensor(0.8129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 969/6700 [27:10<2:43:10,  1.71s/it]11/16/2022 23:21:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:19 - INFO - train.train_snli_ve - loss is tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 970/6700 [27:12<2:40:36,  1.68s/it]11/16/2022 23:21:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:21 - INFO - train.train_snli_ve - loss is tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  14%|#4        | 971/6700 [27:14<2:40:24,  1.68s/it]11/16/2022 23:21:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:23 - INFO - train.train_snli_ve - loss is tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 972/6700 [27:15<2:38:58,  1.67s/it]11/16/2022 23:21:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.7561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:24 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 973/6700 [27:17<2:40:09,  1.68s/it]11/16/2022 23:21:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.9138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:26 - INFO - train.train_snli_ve - loss is tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 974/6700 [27:19<2:39:33,  1.67s/it]11/16/2022 23:21:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.0136e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:28 - INFO - train.train_snli_ve - loss is tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 975/6700 [27:20<2:39:53,  1.68s/it]11/16/2022 23:21:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2478e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:29 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 976/6700 [27:22<2:38:44,  1.66s/it]11/16/2022 23:21:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8750e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:31 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 977/6700 [27:24<2:39:40,  1.67s/it]11/16/2022 23:21:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:33 - INFO - train.train_snli_ve - loss is tensor(0.8083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 978/6700 [27:25<2:38:02,  1.66s/it]11/16/2022 23:21:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:34 - INFO - train.train_snli_ve - loss is tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 979/6700 [27:27<2:40:39,  1.68s/it]11/16/2022 23:21:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0012e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:36 - INFO - train.train_snli_ve - loss is tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 980/6700 [27:29<2:38:25,  1.66s/it]11/16/2022 23:21:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:38 - INFO - train.train_snli_ve - loss is tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 981/6700 [27:30<2:39:54,  1.68s/it]11/16/2022 23:21:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.9258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:39 - INFO - train.train_snli_ve - loss is tensor(0.8437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 982/6700 [27:32<2:38:08,  1.66s/it]11/16/2022 23:21:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.2620e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:41 - INFO - train.train_snli_ve - loss is tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 983/6700 [27:34<2:40:31,  1.68s/it]11/16/2022 23:21:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.3739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:43 - INFO - train.train_snli_ve - loss is tensor(0.9588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 984/6700 [27:35<2:39:23,  1.67s/it]11/16/2022 23:21:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.9756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:44 - INFO - train.train_snli_ve - loss is tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 985/6700 [27:37<2:39:19,  1.67s/it]11/16/2022 23:21:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.8267e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:46 - INFO - train.train_snli_ve - loss is tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 986/6700 [27:39<2:40:16,  1.68s/it]11/16/2022 23:21:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.0049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:48 - INFO - train.train_snli_ve - loss is tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 987/6700 [27:41<2:41:27,  1.70s/it]11/16/2022 23:21:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:50 - INFO - train.train_snli_ve - loss is tensor(0.8566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 988/6700 [27:42<2:41:22,  1.70s/it]11/16/2022 23:21:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.1077e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:51 - INFO - train.train_snli_ve - loss is tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 989/6700 [27:44<2:41:32,  1.70s/it]11/16/2022 23:21:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.4192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:53 - INFO - train.train_snli_ve - loss is tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 990/6700 [27:46<2:38:47,  1.67s/it]11/16/2022 23:21:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4495e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:55 - INFO - train.train_snli_ve - loss is tensor(0.8343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 991/6700 [27:47<2:40:28,  1.69s/it]11/16/2022 23:21:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.5370e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:56 - INFO - train.train_snli_ve - loss is tensor(0.7865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 992/6700 [27:49<2:38:51,  1.67s/it]11/16/2022 23:21:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.2849e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:58 - INFO - train.train_snli_ve - loss is tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 993/6700 [27:51<2:39:12,  1.67s/it]11/16/2022 23:21:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:21:59 - INFO - train.train_snli_ve - loss is tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 994/6700 [27:52<2:37:01,  1.65s/it]11/16/2022 23:22:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.1053e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:01 - INFO - train.train_snli_ve - loss is tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 995/6700 [27:54<2:39:25,  1.68s/it]11/16/2022 23:22:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.0576e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:03 - INFO - train.train_snli_ve - loss is tensor(0.8718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 996/6700 [27:56<2:38:49,  1.67s/it]11/16/2022 23:22:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:05 - INFO - train.train_snli_ve - loss is tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 997/6700 [27:57<2:40:31,  1.69s/it]11/16/2022 23:22:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.1054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:06 - INFO - train.train_snli_ve - loss is tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 998/6700 [27:59<2:40:09,  1.69s/it]11/16/2022 23:22:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:08 - INFO - train.train_snli_ve - loss is tensor(0.9360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 999/6700 [28:01<2:40:33,  1.69s/it]11/16/2022 23:22:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.9968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:10 - INFO - train.train_snli_ve - loss is tensor(0.8739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 1000/6700 [28:02<2:40:23,  1.69s/it]11/16/2022 23:22:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:11 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 1001/6700 [28:04<2:41:01,  1.70s/it]11/16/2022 23:22:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2058e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:13 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 1002/6700 [28:06<2:39:52,  1.68s/it]11/16/2022 23:22:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.4243e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:15 - INFO - train.train_snli_ve - loss is tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 1003/6700 [28:07<2:41:34,  1.70s/it]11/16/2022 23:22:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4097e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:16 - INFO - train.train_snli_ve - loss is tensor(0.7987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#4        | 1004/6700 [28:09<2:38:44,  1.67s/it]11/16/2022 23:22:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.6219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:18 - INFO - train.train_snli_ve - loss is tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1005/6700 [28:11<2:39:37,  1.68s/it]11/16/2022 23:22:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:20 - INFO - train.train_snli_ve - loss is tensor(0.9641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1006/6700 [28:12<2:40:10,  1.69s/it]11/16/2022 23:22:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.1107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:22 - INFO - train.train_snli_ve - loss is tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1007/6700 [28:14<2:41:33,  1.70s/it]11/16/2022 23:22:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2607e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:23 - INFO - train.train_snli_ve - loss is tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1008/6700 [28:16<2:40:13,  1.69s/it]11/16/2022 23:22:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.0339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:25 - INFO - train.train_snli_ve - loss is tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1009/6700 [28:18<2:41:04,  1.70s/it]11/16/2022 23:22:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4538e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:27 - INFO - train.train_snli_ve - loss is tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1010/6700 [28:19<2:39:04,  1.68s/it]11/16/2022 23:22:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:28 - INFO - train.train_snli_ve - loss is tensor(0.8648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1011/6700 [28:21<2:40:34,  1.69s/it]11/16/2022 23:22:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.1507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:30 - INFO - train.train_snli_ve - loss is tensor(1.0148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1012/6700 [28:23<2:41:04,  1.70s/it]11/16/2022 23:22:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:32 - INFO - train.train_snli_ve - loss is tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1013/6700 [28:24<2:42:09,  1.71s/it]11/16/2022 23:22:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.3370e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:33 - INFO - train.train_snli_ve - loss is tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1014/6700 [28:26<2:40:02,  1.69s/it]11/16/2022 23:22:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.6201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:35 - INFO - train.train_snli_ve - loss is tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1015/6700 [28:28<2:41:06,  1.70s/it]11/16/2022 23:22:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:37 - INFO - train.train_snli_ve - loss is tensor(0.8708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1016/6700 [28:29<2:41:12,  1.70s/it]11/16/2022 23:22:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4826e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:39 - INFO - train.train_snli_ve - loss is tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1017/6700 [28:31<2:41:48,  1.71s/it]11/16/2022 23:22:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:40 - INFO - train.train_snli_ve - loss is tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1018/6700 [28:33<2:41:00,  1.70s/it]11/16/2022 23:22:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:42 - INFO - train.train_snli_ve - loss is tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1019/6700 [28:35<2:41:47,  1.71s/it]11/16/2022 23:22:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.4929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:44 - INFO - train.train_snli_ve - loss is tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1020/6700 [28:36<2:38:13,  1.67s/it]11/16/2022 23:22:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0371e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:45 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1021/6700 [28:38<2:40:09,  1.69s/it]11/16/2022 23:22:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:47 - INFO - train.train_snli_ve - loss is tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1022/6700 [28:40<2:39:00,  1.68s/it]11/16/2022 23:22:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.1824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:49 - INFO - train.train_snli_ve - loss is tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1023/6700 [28:41<2:40:00,  1.69s/it]11/16/2022 23:22:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.1464e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:50 - INFO - train.train_snli_ve - loss is tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1024/6700 [28:43<2:41:04,  1.70s/it]11/16/2022 23:22:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.3513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:52 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1025/6700 [28:45<2:41:42,  1.71s/it]11/16/2022 23:22:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:54 - INFO - train.train_snli_ve - loss is tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1026/6700 [28:46<2:38:37,  1.68s/it]11/16/2022 23:22:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.3613e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:55 - INFO - train.train_snli_ve - loss is tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1027/6700 [28:48<2:40:29,  1.70s/it]11/16/2022 23:22:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.9185e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:57 - INFO - train.train_snli_ve - loss is tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1028/6700 [28:50<2:38:13,  1.67s/it]11/16/2022 23:22:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.5971e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:22:59 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1029/6700 [28:51<2:39:44,  1.69s/it]11/16/2022 23:23:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:00 - INFO - train.train_snli_ve - loss is tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1030/6700 [28:53<2:38:40,  1.68s/it]11/16/2022 23:23:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.2970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:02 - INFO - train.train_snli_ve - loss is tensor(0.8480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1031/6700 [28:55<2:38:57,  1.68s/it]11/16/2022 23:23:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.1800e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:04 - INFO - train.train_snli_ve - loss is tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1032/6700 [28:56<2:37:26,  1.67s/it]11/16/2022 23:23:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.6966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:05 - INFO - train.train_snli_ve - loss is tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1033/6700 [28:58<2:38:19,  1.68s/it]11/16/2022 23:23:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:07 - INFO - train.train_snli_ve - loss is tensor(1.0969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1034/6700 [29:00<2:36:41,  1.66s/it]11/16/2022 23:23:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.7281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:09 - INFO - train.train_snli_ve - loss is tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1035/6700 [29:01<2:37:22,  1.67s/it]11/16/2022 23:23:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.2139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:10 - INFO - train.train_snli_ve - loss is tensor(0.8570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1036/6700 [29:03<2:37:35,  1.67s/it]11/16/2022 23:23:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.5070e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:12 - INFO - train.train_snli_ve - loss is tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1037/6700 [29:05<2:38:23,  1.68s/it]11/16/2022 23:23:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.8569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:14 - INFO - train.train_snli_ve - loss is tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  15%|#5        | 1038/6700 [29:06<2:37:43,  1.67s/it]11/16/2022 23:23:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.7558e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:15 - INFO - train.train_snli_ve - loss is tensor(0.9153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1039/6700 [29:08<2:38:23,  1.68s/it]11/16/2022 23:23:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.8931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:17 - INFO - train.train_snli_ve - loss is tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1040/6700 [29:10<2:38:14,  1.68s/it]11/16/2022 23:23:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4321e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:19 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1041/6700 [29:12<2:39:51,  1.69s/it]11/16/2022 23:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:21 - INFO - train.train_snli_ve - loss is tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1042/6700 [29:13<2:38:10,  1.68s/it]11/16/2022 23:23:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.1386e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:22 - INFO - train.train_snli_ve - loss is tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1043/6700 [29:15<2:39:13,  1.69s/it]11/16/2022 23:23:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.2083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:24 - INFO - train.train_snli_ve - loss is tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1044/6700 [29:17<2:36:40,  1.66s/it]11/16/2022 23:23:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0641e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:26 - INFO - train.train_snli_ve - loss is tensor(0.7500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1045/6700 [29:18<2:38:51,  1.69s/it]11/16/2022 23:23:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.1830e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:27 - INFO - train.train_snli_ve - loss is tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1046/6700 [29:20<2:37:52,  1.68s/it]11/16/2022 23:23:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:29 - INFO - train.train_snli_ve - loss is tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1047/6700 [29:22<2:39:55,  1.70s/it]11/16/2022 23:23:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.7509e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:31 - INFO - train.train_snli_ve - loss is tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1048/6700 [29:23<2:38:18,  1.68s/it]11/16/2022 23:23:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9563e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:32 - INFO - train.train_snli_ve - loss is tensor(0.9524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1049/6700 [29:25<2:38:41,  1.68s/it]11/16/2022 23:23:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.4174e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:34 - INFO - train.train_snli_ve - loss is tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1050/6700 [29:27<2:38:10,  1.68s/it]11/16/2022 23:23:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.9204e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:36 - INFO - train.train_snli_ve - loss is tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1051/6700 [29:28<2:39:50,  1.70s/it]11/16/2022 23:23:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7646e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:37 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1052/6700 [29:30<2:38:30,  1.68s/it]11/16/2022 23:23:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.9534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:39 - INFO - train.train_snli_ve - loss is tensor(0.9807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1053/6700 [29:32<2:38:41,  1.69s/it]11/16/2022 23:23:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:41 - INFO - train.train_snli_ve - loss is tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1054/6700 [29:33<2:37:26,  1.67s/it]11/16/2022 23:23:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.7054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:42 - INFO - train.train_snli_ve - loss is tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1055/6700 [29:35<2:38:20,  1.68s/it]11/16/2022 23:23:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:44 - INFO - train.train_snli_ve - loss is tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1056/6700 [29:37<2:36:38,  1.67s/it]11/16/2022 23:23:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:46 - INFO - train.train_snli_ve - loss is tensor(0.9078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1057/6700 [29:38<2:38:45,  1.69s/it]11/16/2022 23:23:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.0330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:47 - INFO - train.train_snli_ve - loss is tensor(0.8780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1058/6700 [29:40<2:37:49,  1.68s/it]11/16/2022 23:23:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9886e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:49 - INFO - train.train_snli_ve - loss is tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1059/6700 [29:42<2:39:42,  1.70s/it]11/16/2022 23:23:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:51 - INFO - train.train_snli_ve - loss is tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1060/6700 [29:44<2:38:59,  1.69s/it]11/16/2022 23:23:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.1247e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:53 - INFO - train.train_snli_ve - loss is tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1061/6700 [29:45<2:38:51,  1.69s/it]11/16/2022 23:23:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:54 - INFO - train.train_snli_ve - loss is tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1062/6700 [29:47<2:38:37,  1.69s/it]11/16/2022 23:23:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.0140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:56 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1063/6700 [29:49<2:39:15,  1.70s/it]11/16/2022 23:23:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:58 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1064/6700 [29:50<2:37:07,  1.67s/it]11/16/2022 23:23:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.4442e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:23:59 - INFO - train.train_snli_ve - loss is tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1065/6700 [29:52<2:38:47,  1.69s/it]11/16/2022 23:24:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.0444e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:01 - INFO - train.train_snli_ve - loss is tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1066/6700 [29:54<2:36:55,  1.67s/it]11/16/2022 23:24:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.3102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:03 - INFO - train.train_snli_ve - loss is tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1067/6700 [29:55<2:37:48,  1.68s/it]11/16/2022 23:24:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:04 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1068/6700 [29:57<2:36:07,  1.66s/it]11/16/2022 23:24:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.6792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:06 - INFO - train.train_snli_ve - loss is tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1069/6700 [29:59<2:37:08,  1.67s/it]11/16/2022 23:24:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:08 - INFO - train.train_snli_ve - loss is tensor(0.8037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1070/6700 [30:00<2:35:58,  1.66s/it]11/16/2022 23:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3399e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:09 - INFO - train.train_snli_ve - loss is tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#5        | 1071/6700 [30:02<2:37:43,  1.68s/it]11/16/2022 23:24:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.5056e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:11 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1072/6700 [30:04<2:36:33,  1.67s/it]11/16/2022 23:24:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:13 - INFO - train.train_snli_ve - loss is tensor(0.8646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1073/6700 [30:05<2:38:02,  1.69s/it]11/16/2022 23:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.8367e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:14 - INFO - train.train_snli_ve - loss is tensor(0.9509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1074/6700 [30:07<2:36:24,  1.67s/it]11/16/2022 23:24:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.6724e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:16 - INFO - train.train_snli_ve - loss is tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1075/6700 [30:09<2:37:19,  1.68s/it]11/16/2022 23:24:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.9073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:18 - INFO - train.train_snli_ve - loss is tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1076/6700 [30:10<2:37:25,  1.68s/it]11/16/2022 23:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0563e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:19 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1077/6700 [30:12<2:38:17,  1.69s/it]11/16/2022 23:24:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4700e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:21 - INFO - train.train_snli_ve - loss is tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1078/6700 [30:14<2:36:47,  1.67s/it]11/16/2022 23:24:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:23 - INFO - train.train_snli_ve - loss is tensor(0.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1079/6700 [30:15<2:36:45,  1.67s/it]11/16/2022 23:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:24 - INFO - train.train_snli_ve - loss is tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1080/6700 [30:17<2:35:48,  1.66s/it]11/16/2022 23:24:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3581e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:26 - INFO - train.train_snli_ve - loss is tensor(0.7939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1081/6700 [30:19<2:37:35,  1.68s/it]11/16/2022 23:24:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.0434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:28 - INFO - train.train_snli_ve - loss is tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1082/6700 [30:20<2:36:11,  1.67s/it]11/16/2022 23:24:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.0851e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:29 - INFO - train.train_snli_ve - loss is tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1083/6700 [30:22<2:37:25,  1.68s/it]11/16/2022 23:24:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.0741e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:31 - INFO - train.train_snli_ve - loss is tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1084/6700 [30:24<2:37:34,  1.68s/it]11/16/2022 23:24:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:33 - INFO - train.train_snli_ve - loss is tensor(0.5040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1085/6700 [30:25<2:37:50,  1.69s/it]11/16/2022 23:24:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.5757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:34 - INFO - train.train_snli_ve - loss is tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1086/6700 [30:27<2:37:55,  1.69s/it]11/16/2022 23:24:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4143e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:36 - INFO - train.train_snli_ve - loss is tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1087/6700 [30:29<2:37:33,  1.68s/it]11/16/2022 23:24:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1894e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:38 - INFO - train.train_snli_ve - loss is tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1088/6700 [30:30<2:35:11,  1.66s/it]11/16/2022 23:24:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.1164e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:39 - INFO - train.train_snli_ve - loss is tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1089/6700 [30:32<2:37:35,  1.69s/it]11/16/2022 23:24:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4211e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:41 - INFO - train.train_snli_ve - loss is tensor(0.8517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1090/6700 [30:34<2:36:36,  1.68s/it]11/16/2022 23:24:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0079e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:43 - INFO - train.train_snli_ve - loss is tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1091/6700 [30:36<2:37:06,  1.68s/it]11/16/2022 23:24:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:44 - INFO - train.train_snli_ve - loss is tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1092/6700 [30:37<2:34:52,  1.66s/it]11/16/2022 23:24:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.2843e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:46 - INFO - train.train_snli_ve - loss is tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1093/6700 [30:39<2:36:38,  1.68s/it]11/16/2022 23:24:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.7749e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:48 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1094/6700 [30:40<2:35:43,  1.67s/it]11/16/2022 23:24:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.3332e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:49 - INFO - train.train_snli_ve - loss is tensor(0.9256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1095/6700 [30:42<2:36:18,  1.67s/it]11/16/2022 23:24:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.1324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:51 - INFO - train.train_snli_ve - loss is tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1096/6700 [30:44<2:36:41,  1.68s/it]11/16/2022 23:24:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.2810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:53 - INFO - train.train_snli_ve - loss is tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1097/6700 [30:46<2:36:16,  1.67s/it]11/16/2022 23:24:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.8873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:55 - INFO - train.train_snli_ve - loss is tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1098/6700 [30:47<2:36:16,  1.67s/it]11/16/2022 23:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.5792e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:56 - INFO - train.train_snli_ve - loss is tensor(0.9760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1099/6700 [30:49<2:36:32,  1.68s/it]11/16/2022 23:24:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3781e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:24:58 - INFO - train.train_snli_ve - loss is tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1100/6700 [30:51<2:35:46,  1.67s/it]11/16/2022 23:25:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.1383e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:00 - INFO - train.train_snli_ve - loss is tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1101/6700 [30:52<2:36:25,  1.68s/it]11/16/2022 23:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:01 - INFO - train.train_snli_ve - loss is tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1102/6700 [30:54<2:35:20,  1.66s/it]11/16/2022 23:25:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.1501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:03 - INFO - train.train_snli_ve - loss is tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1103/6700 [30:56<2:37:07,  1.68s/it]11/16/2022 23:25:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.0815e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:05 - INFO - train.train_snli_ve - loss is tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1104/6700 [30:57<2:37:06,  1.68s/it]11/16/2022 23:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3555e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:06 - INFO - train.train_snli_ve - loss is tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  16%|#6        | 1105/6700 [30:59<2:38:38,  1.70s/it]11/16/2022 23:25:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:08 - INFO - train.train_snli_ve - loss is tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1106/6700 [31:01<2:37:54,  1.69s/it]11/16/2022 23:25:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9448e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:10 - INFO - train.train_snli_ve - loss is tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1107/6700 [31:02<2:37:56,  1.69s/it]11/16/2022 23:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:11 - INFO - train.train_snli_ve - loss is tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1108/6700 [31:04<2:36:26,  1.68s/it]11/16/2022 23:25:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:13 - INFO - train.train_snli_ve - loss is tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1109/6700 [31:06<2:38:25,  1.70s/it]11/16/2022 23:25:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:15 - INFO - train.train_snli_ve - loss is tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1110/6700 [31:07<2:34:33,  1.66s/it]11/16/2022 23:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7160e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:16 - INFO - train.train_snli_ve - loss is tensor(0.4601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1111/6700 [31:09<2:36:04,  1.68s/it]11/16/2022 23:25:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.6895e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:18 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1112/6700 [31:11<2:34:45,  1.66s/it]11/16/2022 23:25:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:20 - INFO - train.train_snli_ve - loss is tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1113/6700 [31:12<2:37:13,  1.69s/it]11/16/2022 23:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.8740e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:21 - INFO - train.train_snli_ve - loss is tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1114/6700 [31:14<2:36:20,  1.68s/it]11/16/2022 23:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.3207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:23 - INFO - train.train_snli_ve - loss is tensor(1.1290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1115/6700 [31:16<2:36:53,  1.69s/it]11/16/2022 23:25:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.1636e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:25 - INFO - train.train_snli_ve - loss is tensor(0.8977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1116/6700 [31:17<2:33:44,  1.65s/it]11/16/2022 23:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:26 - INFO - train.train_snli_ve - loss is tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1117/6700 [31:19<2:34:58,  1.67s/it]11/16/2022 23:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:28 - INFO - train.train_snli_ve - loss is tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1118/6700 [31:21<2:33:44,  1.65s/it]11/16/2022 23:25:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.7078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:30 - INFO - train.train_snli_ve - loss is tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1119/6700 [31:22<2:35:44,  1.67s/it]11/16/2022 23:25:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8940e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:31 - INFO - train.train_snli_ve - loss is tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1120/6700 [31:24<2:35:35,  1.67s/it]11/16/2022 23:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9517e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:33 - INFO - train.train_snli_ve - loss is tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1121/6700 [31:26<2:38:04,  1.70s/it]11/16/2022 23:25:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4561e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:35 - INFO - train.train_snli_ve - loss is tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1122/6700 [31:28<2:38:13,  1.70s/it]11/16/2022 23:25:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:37 - INFO - train.train_snli_ve - loss is tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1123/6700 [31:29<2:38:32,  1.71s/it]11/16/2022 23:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:38 - INFO - train.train_snli_ve - loss is tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1124/6700 [31:31<2:36:39,  1.69s/it]11/16/2022 23:25:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:40 - INFO - train.train_snli_ve - loss is tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1125/6700 [31:33<2:37:37,  1.70s/it]11/16/2022 23:25:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.9004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:42 - INFO - train.train_snli_ve - loss is tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1126/6700 [31:34<2:36:19,  1.68s/it]11/16/2022 23:25:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.8431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:43 - INFO - train.train_snli_ve - loss is tensor(0.8711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1127/6700 [31:36<2:37:52,  1.70s/it]11/16/2022 23:25:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.1800e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:45 - INFO - train.train_snli_ve - loss is tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1128/6700 [31:38<2:35:06,  1.67s/it]11/16/2022 23:25:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:47 - INFO - train.train_snli_ve - loss is tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1129/6700 [31:39<2:37:41,  1.70s/it]11/16/2022 23:25:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.0850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:48 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1130/6700 [31:41<2:35:18,  1.67s/it]11/16/2022 23:25:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4672e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:50 - INFO - train.train_snli_ve - loss is tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1131/6700 [31:43<2:36:53,  1.69s/it]11/16/2022 23:25:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9886e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:52 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1132/6700 [31:44<2:34:43,  1.67s/it]11/16/2022 23:25:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.2295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:53 - INFO - train.train_snli_ve - loss is tensor(1.1842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1133/6700 [31:46<2:36:02,  1.68s/it]11/16/2022 23:25:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4132e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:55 - INFO - train.train_snli_ve - loss is tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1134/6700 [31:48<2:33:56,  1.66s/it]11/16/2022 23:25:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.9089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:57 - INFO - train.train_snli_ve - loss is tensor(0.7791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1135/6700 [31:49<2:35:36,  1.68s/it]11/16/2022 23:25:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.9195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:25:58 - INFO - train.train_snli_ve - loss is tensor(0.7828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1136/6700 [31:51<2:33:39,  1.66s/it]11/16/2022 23:26:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.5799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:00 - INFO - train.train_snli_ve - loss is tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1137/6700 [31:53<2:34:25,  1.67s/it]11/16/2022 23:26:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:02 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#6        | 1138/6700 [31:54<2:33:03,  1.65s/it]11/16/2022 23:26:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.5977e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:03 - INFO - train.train_snli_ve - loss is tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1139/6700 [31:56<2:34:33,  1.67s/it]11/16/2022 23:26:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:05 - INFO - train.train_snli_ve - loss is tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1140/6700 [31:58<2:33:01,  1.65s/it]11/16/2022 23:26:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6932e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:07 - INFO - train.train_snli_ve - loss is tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1141/6700 [31:59<2:35:50,  1.68s/it]11/16/2022 23:26:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4667e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:08 - INFO - train.train_snli_ve - loss is tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1142/6700 [32:01<2:34:24,  1.67s/it]11/16/2022 23:26:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:10 - INFO - train.train_snli_ve - loss is tensor(0.9376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1143/6700 [32:03<2:35:42,  1.68s/it]11/16/2022 23:26:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.3271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:12 - INFO - train.train_snli_ve - loss is tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1144/6700 [32:04<2:33:54,  1.66s/it]11/16/2022 23:26:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:13 - INFO - train.train_snli_ve - loss is tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1145/6700 [32:06<2:35:01,  1.67s/it]11/16/2022 23:26:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.2582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:15 - INFO - train.train_snli_ve - loss is tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1146/6700 [32:08<2:36:29,  1.69s/it]11/16/2022 23:26:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.0263e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:17 - INFO - train.train_snli_ve - loss is tensor(0.9137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1147/6700 [32:10<2:38:45,  1.72s/it]11/16/2022 23:26:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.8347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:19 - INFO - train.train_snli_ve - loss is tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1148/6700 [32:11<2:37:16,  1.70s/it]11/16/2022 23:26:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.1585e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:20 - INFO - train.train_snli_ve - loss is tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1149/6700 [32:13<2:37:19,  1.70s/it]11/16/2022 23:26:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.3369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:22 - INFO - train.train_snli_ve - loss is tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1150/6700 [32:15<2:35:17,  1.68s/it]11/16/2022 23:26:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.0624e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:24 - INFO - train.train_snli_ve - loss is tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1151/6700 [32:16<2:36:30,  1.69s/it]11/16/2022 23:26:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.8725e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:25 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1152/6700 [32:18<2:35:13,  1.68s/it]11/16/2022 23:26:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.8048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:27 - INFO - train.train_snli_ve - loss is tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1153/6700 [32:20<2:37:36,  1.70s/it]11/16/2022 23:26:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.4234e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:29 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1154/6700 [32:21<2:34:54,  1.68s/it]11/16/2022 23:26:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8127e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:30 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1155/6700 [32:23<2:38:33,  1.72s/it]11/16/2022 23:26:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.3154e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:32 - INFO - train.train_snli_ve - loss is tensor(1.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1156/6700 [32:25<2:37:03,  1.70s/it]11/16/2022 23:26:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.8921e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:34 - INFO - train.train_snli_ve - loss is tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1157/6700 [32:26<2:36:24,  1.69s/it]11/16/2022 23:26:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.9117e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:35 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1158/6700 [32:28<2:34:23,  1.67s/it]11/16/2022 23:26:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.1642e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:37 - INFO - train.train_snli_ve - loss is tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1159/6700 [32:30<2:36:03,  1.69s/it]11/16/2022 23:26:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.4905e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:39 - INFO - train.train_snli_ve - loss is tensor(0.4573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1160/6700 [32:31<2:34:42,  1.68s/it]11/16/2022 23:26:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.7962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:40 - INFO - train.train_snli_ve - loss is tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1161/6700 [32:33<2:36:46,  1.70s/it]11/16/2022 23:26:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.7892e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:42 - INFO - train.train_snli_ve - loss is tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1162/6700 [32:35<2:35:48,  1.69s/it]11/16/2022 23:26:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.5764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:44 - INFO - train.train_snli_ve - loss is tensor(1.0200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1163/6700 [32:37<2:37:38,  1.71s/it]11/16/2022 23:26:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.4440e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:46 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1164/6700 [32:38<2:35:19,  1.68s/it]11/16/2022 23:26:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.9262e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:47 - INFO - train.train_snli_ve - loss is tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1165/6700 [32:40<2:35:17,  1.68s/it]11/16/2022 23:26:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.6783e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:49 - INFO - train.train_snli_ve - loss is tensor(0.5029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1166/6700 [32:42<2:33:55,  1.67s/it]11/16/2022 23:26:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:51 - INFO - train.train_snli_ve - loss is tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1167/6700 [32:43<2:34:45,  1.68s/it]11/16/2022 23:26:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.3096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:52 - INFO - train.train_snli_ve - loss is tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1168/6700 [32:45<2:33:08,  1.66s/it]11/16/2022 23:26:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.7907e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:54 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1169/6700 [32:47<2:35:37,  1.69s/it]11/16/2022 23:26:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.0089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:56 - INFO - train.train_snli_ve - loss is tensor(0.9112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1170/6700 [32:48<2:35:54,  1.69s/it]11/16/2022 23:26:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.4762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:57 - INFO - train.train_snli_ve - loss is tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1171/6700 [32:50<2:36:54,  1.70s/it]11/16/2022 23:26:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.6202e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:26:59 - INFO - train.train_snli_ve - loss is tensor(0.5756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  17%|#7        | 1172/6700 [32:52<2:34:20,  1.68s/it]11/16/2022 23:27:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:01 - INFO - train.train_snli_ve - loss is tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1173/6700 [32:53<2:35:51,  1.69s/it]11/16/2022 23:27:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.0693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:02 - INFO - train.train_snli_ve - loss is tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1174/6700 [32:55<2:35:05,  1.68s/it]11/16/2022 23:27:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.7208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:04 - INFO - train.train_snli_ve - loss is tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1175/6700 [32:57<2:37:16,  1.71s/it]11/16/2022 23:27:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5951e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:06 - INFO - train.train_snli_ve - loss is tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1176/6700 [32:58<2:35:17,  1.69s/it]11/16/2022 23:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5539e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:08 - INFO - train.train_snli_ve - loss is tensor(1.0340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1177/6700 [33:00<2:36:33,  1.70s/it]11/16/2022 23:27:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.3257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:09 - INFO - train.train_snli_ve - loss is tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1178/6700 [33:02<2:34:58,  1.68s/it]11/16/2022 23:27:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.0431e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:11 - INFO - train.train_snli_ve - loss is tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1179/6700 [33:04<2:35:29,  1.69s/it]11/16/2022 23:27:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.1323e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:12 - INFO - train.train_snli_ve - loss is tensor(0.7433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1180/6700 [33:05<2:33:24,  1.67s/it]11/16/2022 23:27:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.8355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:14 - INFO - train.train_snli_ve - loss is tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1181/6700 [33:07<2:34:39,  1.68s/it]11/16/2022 23:27:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:16 - INFO - train.train_snli_ve - loss is tensor(0.8290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1182/6700 [33:09<2:33:15,  1.67s/it]11/16/2022 23:27:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.4859e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:18 - INFO - train.train_snli_ve - loss is tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1183/6700 [33:10<2:34:10,  1.68s/it]11/16/2022 23:27:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9207e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:19 - INFO - train.train_snli_ve - loss is tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1184/6700 [33:12<2:32:44,  1.66s/it]11/16/2022 23:27:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.5794e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:21 - INFO - train.train_snli_ve - loss is tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1185/6700 [33:14<2:34:49,  1.68s/it]11/16/2022 23:27:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.6096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:23 - INFO - train.train_snli_ve - loss is tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1186/6700 [33:15<2:33:22,  1.67s/it]11/16/2022 23:27:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.3489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:24 - INFO - train.train_snli_ve - loss is tensor(0.8120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1187/6700 [33:17<2:33:41,  1.67s/it]11/16/2022 23:27:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.5249e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:26 - INFO - train.train_snli_ve - loss is tensor(0.8641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1188/6700 [33:19<2:32:57,  1.66s/it]11/16/2022 23:27:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.4519e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:28 - INFO - train.train_snli_ve - loss is tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1189/6700 [33:20<2:34:33,  1.68s/it]11/16/2022 23:27:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.3777e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:29 - INFO - train.train_snli_ve - loss is tensor(0.8570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1190/6700 [33:22<2:34:15,  1.68s/it]11/16/2022 23:27:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.0504e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:31 - INFO - train.train_snli_ve - loss is tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1191/6700 [33:24<2:35:39,  1.70s/it]11/16/2022 23:27:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.5865e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:33 - INFO - train.train_snli_ve - loss is tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1192/6700 [33:25<2:35:03,  1.69s/it]11/16/2022 23:27:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5273e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:34 - INFO - train.train_snli_ve - loss is tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1193/6700 [33:27<2:35:23,  1.69s/it]11/16/2022 23:27:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.7980e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:36 - INFO - train.train_snli_ve - loss is tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1194/6700 [33:29<2:33:09,  1.67s/it]11/16/2022 23:27:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.4215e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:38 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1195/6700 [33:30<2:34:43,  1.69s/it]11/16/2022 23:27:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.8953e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:39 - INFO - train.train_snli_ve - loss is tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1196/6700 [33:32<2:32:58,  1.67s/it]11/16/2022 23:27:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.1813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:41 - INFO - train.train_snli_ve - loss is tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1197/6700 [33:34<2:35:50,  1.70s/it]11/16/2022 23:27:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0182e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:43 - INFO - train.train_snli_ve - loss is tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1198/6700 [33:35<2:32:28,  1.66s/it]11/16/2022 23:27:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.6512e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:44 - INFO - train.train_snli_ve - loss is tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1199/6700 [33:37<2:34:40,  1.69s/it]11/16/2022 23:27:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.0595e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:46 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1200/6700 [33:39<2:34:13,  1.68s/it]11/16/2022 23:27:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.5734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:48 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1201/6700 [33:40<2:33:51,  1.68s/it]11/16/2022 23:27:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.9391e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:49 - INFO - train.train_snli_ve - loss is tensor(0.8144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1202/6700 [33:42<2:32:39,  1.67s/it]11/16/2022 23:27:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:51 - INFO - train.train_snli_ve - loss is tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1203/6700 [33:44<2:33:14,  1.67s/it]11/16/2022 23:27:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.7516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:53 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1204/6700 [33:45<2:31:42,  1.66s/it]11/16/2022 23:27:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.6364e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:54 - INFO - train.train_snli_ve - loss is tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#7        | 1205/6700 [33:47<2:33:47,  1.68s/it]11/16/2022 23:27:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.9038e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:56 - INFO - train.train_snli_ve - loss is tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1206/6700 [33:49<2:31:36,  1.66s/it]11/16/2022 23:27:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.2898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:58 - INFO - train.train_snli_ve - loss is tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1207/6700 [33:50<2:34:23,  1.69s/it]11/16/2022 23:27:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.5116e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:27:59 - INFO - train.train_snli_ve - loss is tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1208/6700 [33:52<2:32:08,  1.66s/it]11/16/2022 23:28:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.2759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:01 - INFO - train.train_snli_ve - loss is tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1209/6700 [33:54<2:33:03,  1.67s/it]11/16/2022 23:28:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9671e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:03 - INFO - train.train_snli_ve - loss is tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1210/6700 [33:55<2:33:43,  1.68s/it]11/16/2022 23:28:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.0414e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:05 - INFO - train.train_snli_ve - loss is tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1211/6700 [33:57<2:35:47,  1.70s/it]11/16/2022 23:28:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.0417e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:06 - INFO - train.train_snli_ve - loss is tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1212/6700 [33:59<2:34:29,  1.69s/it]11/16/2022 23:28:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.3708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:08 - INFO - train.train_snli_ve - loss is tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1213/6700 [34:01<2:34:56,  1.69s/it]11/16/2022 23:28:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.1219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:10 - INFO - train.train_snli_ve - loss is tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1214/6700 [34:02<2:33:57,  1.68s/it]11/16/2022 23:28:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.4999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:11 - INFO - train.train_snli_ve - loss is tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1215/6700 [34:04<2:34:15,  1.69s/it]11/16/2022 23:28:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.6073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:13 - INFO - train.train_snli_ve - loss is tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1216/6700 [34:06<2:32:08,  1.66s/it]11/16/2022 23:28:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.7295e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:15 - INFO - train.train_snli_ve - loss is tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1217/6700 [34:07<2:32:39,  1.67s/it]11/16/2022 23:28:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.3628e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:16 - INFO - train.train_snli_ve - loss is tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1218/6700 [34:09<2:32:27,  1.67s/it]11/16/2022 23:28:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.1150e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:18 - INFO - train.train_snli_ve - loss is tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1219/6700 [34:11<2:34:10,  1.69s/it]11/16/2022 23:28:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.9660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:20 - INFO - train.train_snli_ve - loss is tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1220/6700 [34:12<2:34:33,  1.69s/it]11/16/2022 23:28:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.1034e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:21 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1221/6700 [34:14<2:35:54,  1.71s/it]11/16/2022 23:28:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.2793e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:23 - INFO - train.train_snli_ve - loss is tensor(1.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1222/6700 [34:16<2:34:18,  1.69s/it]11/16/2022 23:28:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.2706e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:25 - INFO - train.train_snli_ve - loss is tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1223/6700 [34:17<2:34:03,  1.69s/it]11/16/2022 23:28:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.1954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:26 - INFO - train.train_snli_ve - loss is tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1224/6700 [34:19<2:31:50,  1.66s/it]11/16/2022 23:28:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.5769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:28 - INFO - train.train_snli_ve - loss is tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1225/6700 [34:21<2:32:03,  1.67s/it]11/16/2022 23:28:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.2226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:30 - INFO - train.train_snli_ve - loss is tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1226/6700 [34:22<2:30:25,  1.65s/it]11/16/2022 23:28:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:31 - INFO - train.train_snli_ve - loss is tensor(1.0321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1227/6700 [34:24<2:31:00,  1.66s/it]11/16/2022 23:28:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:33 - INFO - train.train_snli_ve - loss is tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1228/6700 [34:26<2:30:26,  1.65s/it]11/16/2022 23:28:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.4206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:35 - INFO - train.train_snli_ve - loss is tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1229/6700 [34:27<2:33:22,  1.68s/it]11/16/2022 23:28:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.7949e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:36 - INFO - train.train_snli_ve - loss is tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1230/6700 [34:29<2:31:23,  1.66s/it]11/16/2022 23:28:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.7814e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:38 - INFO - train.train_snli_ve - loss is tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1231/6700 [34:31<2:33:40,  1.69s/it]11/16/2022 23:28:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.4931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:40 - INFO - train.train_snli_ve - loss is tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1232/6700 [34:32<2:33:04,  1.68s/it]11/16/2022 23:28:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:41 - INFO - train.train_snli_ve - loss is tensor(0.8820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1233/6700 [34:34<2:34:24,  1.69s/it]11/16/2022 23:28:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.1639e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:43 - INFO - train.train_snli_ve - loss is tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1234/6700 [34:36<2:32:08,  1.67s/it]11/16/2022 23:28:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3594e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:45 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1235/6700 [34:37<2:33:49,  1.69s/it]11/16/2022 23:28:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.7508e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:46 - INFO - train.train_snli_ve - loss is tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1236/6700 [34:39<2:32:04,  1.67s/it]11/16/2022 23:28:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.7180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:48 - INFO - train.train_snli_ve - loss is tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1237/6700 [34:41<2:33:42,  1.69s/it]11/16/2022 23:28:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.1100e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:50 - INFO - train.train_snli_ve - loss is tensor(0.7760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1238/6700 [34:42<2:32:15,  1.67s/it]11/16/2022 23:28:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.9513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:51 - INFO - train.train_snli_ve - loss is tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  18%|#8        | 1239/6700 [34:44<2:33:50,  1.69s/it]11/16/2022 23:28:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:53 - INFO - train.train_snli_ve - loss is tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1240/6700 [34:46<2:32:18,  1.67s/it]11/16/2022 23:28:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:55 - INFO - train.train_snli_ve - loss is tensor(0.9631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1241/6700 [34:48<2:33:33,  1.69s/it]11/16/2022 23:28:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.5098e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:56 - INFO - train.train_snli_ve - loss is tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1242/6700 [34:49<2:31:24,  1.66s/it]11/16/2022 23:28:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.8388e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:28:58 - INFO - train.train_snli_ve - loss is tensor(0.8523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1243/6700 [34:51<2:32:39,  1.68s/it]11/16/2022 23:29:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.3241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:00 - INFO - train.train_snli_ve - loss is tensor(0.8797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1244/6700 [34:53<2:31:38,  1.67s/it]11/16/2022 23:29:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.3353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:02 - INFO - train.train_snli_ve - loss is tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1245/6700 [34:54<2:32:29,  1.68s/it]11/16/2022 23:29:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.1145e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:03 - INFO - train.train_snli_ve - loss is tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1246/6700 [34:56<2:31:17,  1.66s/it]11/16/2022 23:29:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.4942e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:05 - INFO - train.train_snli_ve - loss is tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1247/6700 [34:58<2:32:09,  1.67s/it]11/16/2022 23:29:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.3660e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:07 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1248/6700 [34:59<2:32:08,  1.67s/it]11/16/2022 23:29:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4495e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:08 - INFO - train.train_snli_ve - loss is tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1249/6700 [35:01<2:34:12,  1.70s/it]11/16/2022 23:29:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:10 - INFO - train.train_snli_ve - loss is tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1250/6700 [35:03<2:33:07,  1.69s/it]11/16/2022 23:29:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.6846e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:12 - INFO - train.train_snli_ve - loss is tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1251/6700 [35:04<2:34:27,  1.70s/it]11/16/2022 23:29:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:13 - INFO - train.train_snli_ve - loss is tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1252/6700 [35:06<2:33:32,  1.69s/it]11/16/2022 23:29:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.4985e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:15 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1253/6700 [35:08<2:33:39,  1.69s/it]11/16/2022 23:29:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.1355e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:17 - INFO - train.train_snli_ve - loss is tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1254/6700 [35:09<2:31:39,  1.67s/it]11/16/2022 23:29:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.9719e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:18 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1255/6700 [35:11<2:33:44,  1.69s/it]11/16/2022 23:29:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.3532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:20 - INFO - train.train_snli_ve - loss is tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1256/6700 [35:13<2:31:39,  1.67s/it]11/16/2022 23:29:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.6673e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:22 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1257/6700 [35:14<2:32:51,  1.69s/it]11/16/2022 23:29:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.5111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:23 - INFO - train.train_snli_ve - loss is tensor(1.0023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1258/6700 [35:16<2:31:53,  1.67s/it]11/16/2022 23:29:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.0091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:25 - INFO - train.train_snli_ve - loss is tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1259/6700 [35:18<2:33:40,  1.69s/it]11/16/2022 23:29:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.9801e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:27 - INFO - train.train_snli_ve - loss is tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1260/6700 [35:19<2:33:18,  1.69s/it]11/16/2022 23:29:29 - INFO - train.train_snli_ve - kd_loss is tensor(7.4648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:29 - INFO - train.train_snli_ve - loss is tensor(0.8024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1261/6700 [35:21<2:33:31,  1.69s/it]11/16/2022 23:29:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.7341e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:30 - INFO - train.train_snli_ve - loss is tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1262/6700 [35:23<2:32:09,  1.68s/it]11/16/2022 23:29:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.4986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:32 - INFO - train.train_snli_ve - loss is tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1263/6700 [35:25<2:33:49,  1.70s/it]11/16/2022 23:29:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5407e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:34 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1264/6700 [35:26<2:33:00,  1.69s/it]11/16/2022 23:29:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.7982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:35 - INFO - train.train_snli_ve - loss is tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1265/6700 [35:28<2:33:23,  1.69s/it]11/16/2022 23:29:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.4121e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:37 - INFO - train.train_snli_ve - loss is tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1266/6700 [35:30<2:31:14,  1.67s/it]11/16/2022 23:29:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.6969e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:39 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1267/6700 [35:31<2:32:13,  1.68s/it]11/16/2022 23:29:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.6958e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:40 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1268/6700 [35:33<2:30:42,  1.66s/it]11/16/2022 23:29:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.3221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:42 - INFO - train.train_snli_ve - loss is tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1269/6700 [35:35<2:32:13,  1.68s/it]11/16/2022 23:29:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.9296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:44 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1270/6700 [35:36<2:30:54,  1.67s/it]11/16/2022 23:29:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.9774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:45 - INFO - train.train_snli_ve - loss is tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1271/6700 [35:38<2:31:21,  1.67s/it]11/16/2022 23:29:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.1469e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:47 - INFO - train.train_snli_ve - loss is tensor(0.8508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#8        | 1272/6700 [35:40<2:30:41,  1.67s/it]11/16/2022 23:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.6534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:49 - INFO - train.train_snli_ve - loss is tensor(1.0106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1273/6700 [35:41<2:33:11,  1.69s/it]11/16/2022 23:29:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.7954e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:50 - INFO - train.train_snli_ve - loss is tensor(1.0427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1274/6700 [35:43<2:32:03,  1.68s/it]11/16/2022 23:29:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:52 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1275/6700 [35:45<2:34:54,  1.71s/it]11/16/2022 23:29:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.5085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:54 - INFO - train.train_snli_ve - loss is tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1276/6700 [35:46<2:31:40,  1.68s/it]11/16/2022 23:29:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.6926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:55 - INFO - train.train_snli_ve - loss is tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1277/6700 [35:48<2:35:08,  1.72s/it]11/16/2022 23:29:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.2526e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:57 - INFO - train.train_snli_ve - loss is tensor(0.8676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1278/6700 [35:50<2:32:44,  1.69s/it]11/16/2022 23:29:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.1222e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:29:59 - INFO - train.train_snli_ve - loss is tensor(0.8694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1279/6700 [35:52<2:34:45,  1.71s/it]11/16/2022 23:30:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.0129e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:01 - INFO - train.train_snli_ve - loss is tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1280/6700 [35:53<2:32:12,  1.68s/it]11/16/2022 23:30:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.5518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:02 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1281/6700 [35:55<2:33:19,  1.70s/it]11/16/2022 23:30:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.9520e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:04 - INFO - train.train_snli_ve - loss is tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1282/6700 [35:57<2:31:33,  1.68s/it]11/16/2022 23:30:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:06 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1283/6700 [35:58<2:31:36,  1.68s/it]11/16/2022 23:30:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.7251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:07 - INFO - train.train_snli_ve - loss is tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1284/6700 [36:00<2:31:00,  1.67s/it]11/16/2022 23:30:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9808e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:09 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1285/6700 [36:02<2:33:03,  1.70s/it]11/16/2022 23:30:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1422e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:11 - INFO - train.train_snli_ve - loss is tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1286/6700 [36:03<2:31:42,  1.68s/it]11/16/2022 23:30:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.9689e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:12 - INFO - train.train_snli_ve - loss is tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1287/6700 [36:05<2:32:27,  1.69s/it]11/16/2022 23:30:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:14 - INFO - train.train_snli_ve - loss is tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1288/6700 [36:07<2:30:45,  1.67s/it]11/16/2022 23:30:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:16 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1289/6700 [36:08<2:31:09,  1.68s/it]11/16/2022 23:30:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3199e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:17 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1290/6700 [36:10<2:30:00,  1.66s/it]11/16/2022 23:30:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.5721e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:19 - INFO - train.train_snli_ve - loss is tensor(0.5011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1291/6700 [36:12<2:31:22,  1.68s/it]11/16/2022 23:30:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.5968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:21 - INFO - train.train_snli_ve - loss is tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1292/6700 [36:13<2:30:03,  1.66s/it]11/16/2022 23:30:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.1975e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:22 - INFO - train.train_snli_ve - loss is tensor(0.9178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1293/6700 [36:15<2:34:00,  1.71s/it]11/16/2022 23:30:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.4525e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:24 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1294/6700 [36:17<2:32:48,  1.70s/it]11/16/2022 23:30:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.4648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:26 - INFO - train.train_snli_ve - loss is tensor(0.9299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1295/6700 [36:19<2:33:08,  1.70s/it]11/16/2022 23:30:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6262e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:27 - INFO - train.train_snli_ve - loss is tensor(0.7910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1296/6700 [36:20<2:32:10,  1.69s/it]11/16/2022 23:30:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.9657e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:29 - INFO - train.train_snli_ve - loss is tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1297/6700 [36:22<2:32:02,  1.69s/it]11/16/2022 23:30:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.5492e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:31 - INFO - train.train_snli_ve - loss is tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1298/6700 [36:24<2:32:20,  1.69s/it]11/16/2022 23:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.3395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:33 - INFO - train.train_snli_ve - loss is tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1299/6700 [36:25<2:33:10,  1.70s/it]11/16/2022 23:30:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.0161e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:34 - INFO - train.train_snli_ve - loss is tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1300/6700 [36:27<2:32:16,  1.69s/it]11/16/2022 23:30:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.0410e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:36 - INFO - train.train_snli_ve - loss is tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1301/6700 [36:29<2:33:28,  1.71s/it]11/16/2022 23:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.5491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:38 - INFO - train.train_snli_ve - loss is tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1302/6700 [36:30<2:31:39,  1.69s/it]11/16/2022 23:30:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.1617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:39 - INFO - train.train_snli_ve - loss is tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1303/6700 [36:32<2:33:39,  1.71s/it]11/16/2022 23:30:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.0789e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:41 - INFO - train.train_snli_ve - loss is tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1304/6700 [36:34<2:31:37,  1.69s/it]11/16/2022 23:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.1709e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:43 - INFO - train.train_snli_ve - loss is tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1305/6700 [36:35<2:32:27,  1.70s/it]11/16/2022 23:30:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.5533e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:44 - INFO - train.train_snli_ve - loss is tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  19%|#9        | 1306/6700 [36:37<2:33:06,  1.70s/it]11/16/2022 23:30:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.5947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:46 - INFO - train.train_snli_ve - loss is tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1307/6700 [36:39<2:33:35,  1.71s/it]11/16/2022 23:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.2404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:48 - INFO - train.train_snli_ve - loss is tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1308/6700 [36:41<2:32:05,  1.69s/it]11/16/2022 23:30:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.4717e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:50 - INFO - train.train_snli_ve - loss is tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1309/6700 [36:42<2:32:51,  1.70s/it]11/16/2022 23:30:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.4310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:51 - INFO - train.train_snli_ve - loss is tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1310/6700 [36:44<2:31:19,  1.68s/it]11/16/2022 23:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.5553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:53 - INFO - train.train_snli_ve - loss is tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1311/6700 [36:46<2:32:44,  1.70s/it]11/16/2022 23:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.3774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:55 - INFO - train.train_snli_ve - loss is tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1312/6700 [36:47<2:30:48,  1.68s/it]11/16/2022 23:30:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.2876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:56 - INFO - train.train_snli_ve - loss is tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1313/6700 [36:49<2:31:35,  1.69s/it]11/16/2022 23:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.5491e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:30:58 - INFO - train.train_snli_ve - loss is tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1314/6700 [36:51<2:32:06,  1.69s/it]11/16/2022 23:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.4105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:00 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1315/6700 [36:52<2:32:32,  1.70s/it]11/16/2022 23:31:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.6365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:01 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1316/6700 [36:54<2:30:13,  1.67s/it]11/16/2022 23:31:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.9655e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:03 - INFO - train.train_snli_ve - loss is tensor(0.8167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1317/6700 [36:56<2:31:37,  1.69s/it]11/16/2022 23:31:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.2732e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:05 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1318/6700 [36:57<2:30:17,  1.68s/it]11/16/2022 23:31:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.4863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:06 - INFO - train.train_snli_ve - loss is tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1319/6700 [36:59<2:32:59,  1.71s/it]11/16/2022 23:31:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.8020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:08 - INFO - train.train_snli_ve - loss is tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1320/6700 [37:01<2:31:06,  1.69s/it]11/16/2022 23:31:10 - INFO - train.train_snli_ve - kd_loss is tensor(9.9947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:10 - INFO - train.train_snli_ve - loss is tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1321/6700 [37:03<2:32:37,  1.70s/it]11/16/2022 23:31:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.8790e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:12 - INFO - train.train_snli_ve - loss is tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1322/6700 [37:04<2:31:43,  1.69s/it]11/16/2022 23:31:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.1513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:13 - INFO - train.train_snli_ve - loss is tensor(0.8950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1323/6700 [37:06<2:32:20,  1.70s/it]11/16/2022 23:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.0564e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:15 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1324/6700 [37:08<2:29:52,  1.67s/it]11/16/2022 23:31:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.8124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:17 - INFO - train.train_snli_ve - loss is tensor(0.7426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1325/6700 [37:09<2:31:11,  1.69s/it]11/16/2022 23:31:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.5208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:18 - INFO - train.train_snli_ve - loss is tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1326/6700 [37:11<2:30:27,  1.68s/it]11/16/2022 23:31:20 - INFO - train.train_snli_ve - kd_loss is tensor(4.1428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:20 - INFO - train.train_snli_ve - loss is tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1327/6700 [37:13<2:31:32,  1.69s/it]11/16/2022 23:31:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.9385e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:22 - INFO - train.train_snli_ve - loss is tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1328/6700 [37:14<2:30:25,  1.68s/it]11/16/2022 23:31:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.0449e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:23 - INFO - train.train_snli_ve - loss is tensor(0.9196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1329/6700 [37:16<2:33:21,  1.71s/it]11/16/2022 23:31:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.2016e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:25 - INFO - train.train_snli_ve - loss is tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1330/6700 [37:18<2:32:18,  1.70s/it]11/16/2022 23:31:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.5277e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:27 - INFO - train.train_snli_ve - loss is tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1331/6700 [37:20<2:33:39,  1.72s/it]11/16/2022 23:31:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.6988e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:29 - INFO - train.train_snli_ve - loss is tensor(1.0072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1332/6700 [37:21<2:32:58,  1.71s/it]11/16/2022 23:31:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.3493e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:30 - INFO - train.train_snli_ve - loss is tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1333/6700 [37:23<2:34:31,  1.73s/it]11/16/2022 23:31:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.1796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:32 - INFO - train.train_snli_ve - loss is tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1334/6700 [37:25<2:31:25,  1.69s/it]11/16/2022 23:31:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.0083e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:34 - INFO - train.train_snli_ve - loss is tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1335/6700 [37:26<2:31:43,  1.70s/it]11/16/2022 23:31:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.1343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:35 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1336/6700 [37:28<2:29:57,  1.68s/it]11/16/2022 23:31:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.2416e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:37 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1337/6700 [37:30<2:31:04,  1.69s/it]11/16/2022 23:31:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.3986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:39 - INFO - train.train_snli_ve - loss is tensor(0.8991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1338/6700 [37:31<2:29:31,  1.67s/it]11/16/2022 23:31:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.8213e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:40 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|#9        | 1339/6700 [37:33<2:31:25,  1.69s/it]11/16/2022 23:31:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.6682e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:42 - INFO - train.train_snli_ve - loss is tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1340/6700 [37:35<2:29:06,  1.67s/it]11/16/2022 23:31:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.9289e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:44 - INFO - train.train_snli_ve - loss is tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1341/6700 [37:36<2:29:48,  1.68s/it]11/16/2022 23:31:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.1824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:45 - INFO - train.train_snli_ve - loss is tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1342/6700 [37:38<2:30:23,  1.68s/it]11/16/2022 23:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.9935e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:47 - INFO - train.train_snli_ve - loss is tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1343/6700 [37:40<2:32:41,  1.71s/it]11/16/2022 23:31:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.3281e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:49 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1344/6700 [37:41<2:31:25,  1.70s/it]11/16/2022 23:31:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.6637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:50 - INFO - train.train_snli_ve - loss is tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1345/6700 [37:43<2:31:31,  1.70s/it]11/16/2022 23:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.9927e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:52 - INFO - train.train_snli_ve - loss is tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1346/6700 [37:45<2:30:27,  1.69s/it]11/16/2022 23:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.7286e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:54 - INFO - train.train_snli_ve - loss is tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1347/6700 [37:47<2:31:03,  1.69s/it]11/16/2022 23:31:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.7822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:55 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1348/6700 [37:48<2:29:22,  1.67s/it]11/16/2022 23:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:57 - INFO - train.train_snli_ve - loss is tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1349/6700 [37:50<2:30:18,  1.69s/it]11/16/2022 23:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.9300e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:31:59 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1350/6700 [37:51<2:28:41,  1.67s/it]11/16/2022 23:32:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.6860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:01 - INFO - train.train_snli_ve - loss is tensor(0.9850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1351/6700 [37:53<2:30:55,  1.69s/it]11/16/2022 23:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.6496e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:02 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1352/6700 [37:55<2:28:40,  1.67s/it]11/16/2022 23:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.5518e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:04 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1353/6700 [37:57<2:29:50,  1.68s/it]11/16/2022 23:32:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.7908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:06 - INFO - train.train_snli_ve - loss is tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1354/6700 [37:58<2:28:33,  1.67s/it]11/16/2022 23:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.0490e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:07 - INFO - train.train_snli_ve - loss is tensor(0.7825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1355/6700 [38:00<2:29:04,  1.67s/it]11/16/2022 23:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.3139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:09 - INFO - train.train_snli_ve - loss is tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1356/6700 [38:02<2:28:44,  1.67s/it]11/16/2022 23:32:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.7076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:11 - INFO - train.train_snli_ve - loss is tensor(0.9947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1357/6700 [38:03<2:29:27,  1.68s/it]11/16/2022 23:32:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.2565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:12 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1358/6700 [38:05<2:27:46,  1.66s/it]11/16/2022 23:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.9057e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:14 - INFO - train.train_snli_ve - loss is tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1359/6700 [38:07<2:29:04,  1.67s/it]11/16/2022 23:32:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.9828e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:16 - INFO - train.train_snli_ve - loss is tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1360/6700 [38:08<2:27:59,  1.66s/it]11/16/2022 23:32:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.5155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:17 - INFO - train.train_snli_ve - loss is tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1361/6700 [38:10<2:29:23,  1.68s/it]11/16/2022 23:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.8999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:19 - INFO - train.train_snli_ve - loss is tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1362/6700 [38:12<2:28:16,  1.67s/it]11/16/2022 23:32:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.9073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:21 - INFO - train.train_snli_ve - loss is tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1363/6700 [38:13<2:30:40,  1.69s/it]11/16/2022 23:32:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.3366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:22 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1364/6700 [38:15<2:28:30,  1.67s/it]11/16/2022 23:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.9072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:24 - INFO - train.train_snli_ve - loss is tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1365/6700 [38:17<2:29:22,  1.68s/it]11/16/2022 23:32:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.1253e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:26 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1366/6700 [38:18<2:28:00,  1.66s/it]11/16/2022 23:32:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.3152e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:27 - INFO - train.train_snli_ve - loss is tensor(1.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1367/6700 [38:20<2:28:16,  1.67s/it]11/16/2022 23:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.5540e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:29 - INFO - train.train_snli_ve - loss is tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1368/6700 [38:22<2:27:53,  1.66s/it]11/16/2022 23:32:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.8553e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:31 - INFO - train.train_snli_ve - loss is tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1369/6700 [38:23<2:30:28,  1.69s/it]11/16/2022 23:32:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.4494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:32 - INFO - train.train_snli_ve - loss is tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1370/6700 [38:25<2:29:35,  1.68s/it]11/16/2022 23:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.2060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:34 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1371/6700 [38:27<2:29:47,  1.69s/it]11/16/2022 23:32:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.3020e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:36 - INFO - train.train_snli_ve - loss is tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1372/6700 [38:28<2:28:31,  1.67s/it]11/16/2022 23:32:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.5179e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:37 - INFO - train.train_snli_ve - loss is tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  20%|##        | 1373/6700 [38:30<2:31:50,  1.71s/it]11/16/2022 23:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(5.4051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:39 - INFO - train.train_snli_ve - loss is tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1374/6700 [38:32<2:30:38,  1.70s/it]11/16/2022 23:32:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.7040e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:41 - INFO - train.train_snli_ve - loss is tensor(0.9339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1375/6700 [38:34<2:30:12,  1.69s/it]11/16/2022 23:32:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.3966e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:42 - INFO - train.train_snli_ve - loss is tensor(0.7989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1376/6700 [38:35<2:29:28,  1.68s/it]11/16/2022 23:32:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.8018e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:44 - INFO - train.train_snli_ve - loss is tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1377/6700 [38:37<2:31:43,  1.71s/it]11/16/2022 23:32:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.7934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:46 - INFO - train.train_snli_ve - loss is tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1378/6700 [38:39<2:31:41,  1.71s/it]11/16/2022 23:32:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.1261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:48 - INFO - train.train_snli_ve - loss is tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1379/6700 [38:40<2:31:21,  1.71s/it]11/16/2022 23:32:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.9510e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:49 - INFO - train.train_snli_ve - loss is tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1380/6700 [38:42<2:29:47,  1.69s/it]11/16/2022 23:32:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.1451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:51 - INFO - train.train_snli_ve - loss is tensor(0.9585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1381/6700 [38:44<2:29:10,  1.68s/it]11/16/2022 23:32:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1159e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:53 - INFO - train.train_snli_ve - loss is tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1382/6700 [38:45<2:28:01,  1.67s/it]11/16/2022 23:32:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.0296e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:54 - INFO - train.train_snli_ve - loss is tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1383/6700 [38:47<2:29:05,  1.68s/it]11/16/2022 23:32:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.9982e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:56 - INFO - train.train_snli_ve - loss is tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1384/6700 [38:49<2:26:52,  1.66s/it]11/16/2022 23:32:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.5720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:58 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1385/6700 [38:50<2:27:54,  1.67s/it]11/16/2022 23:32:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.6142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:32:59 - INFO - train.train_snli_ve - loss is tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1386/6700 [38:52<2:27:17,  1.66s/it]11/16/2022 23:33:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.0992e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:01 - INFO - train.train_snli_ve - loss is tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1387/6700 [38:54<2:28:20,  1.68s/it]11/16/2022 23:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.8091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:03 - INFO - train.train_snli_ve - loss is tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1388/6700 [38:55<2:26:45,  1.66s/it]11/16/2022 23:33:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.8172e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:04 - INFO - train.train_snli_ve - loss is tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1389/6700 [38:57<2:28:26,  1.68s/it]11/16/2022 23:33:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.4900e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:06 - INFO - train.train_snli_ve - loss is tensor(0.9581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1390/6700 [38:59<2:28:18,  1.68s/it]11/16/2022 23:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.3266e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:08 - INFO - train.train_snli_ve - loss is tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1391/6700 [39:00<2:30:05,  1.70s/it]11/16/2022 23:33:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.5645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:09 - INFO - train.train_snli_ve - loss is tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1392/6700 [39:02<2:28:15,  1.68s/it]11/16/2022 23:33:11 - INFO - train.train_snli_ve - kd_loss is tensor(5.6294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:11 - INFO - train.train_snli_ve - loss is tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1393/6700 [39:04<2:29:18,  1.69s/it]11/16/2022 23:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.7698e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:13 - INFO - train.train_snli_ve - loss is tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1394/6700 [39:05<2:28:03,  1.67s/it]11/16/2022 23:33:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.4736e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:14 - INFO - train.train_snli_ve - loss is tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1395/6700 [39:07<2:29:25,  1.69s/it]11/16/2022 23:33:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.4846e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:16 - INFO - train.train_snli_ve - loss is tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1396/6700 [39:09<2:27:54,  1.67s/it]11/16/2022 23:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.8148e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:18 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1397/6700 [39:11<2:30:02,  1.70s/it]11/16/2022 23:33:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.5579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:19 - INFO - train.train_snli_ve - loss is tensor(0.9082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1398/6700 [39:12<2:28:16,  1.68s/it]11/16/2022 23:33:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.8122e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:21 - INFO - train.train_snli_ve - loss is tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1399/6700 [39:14<2:28:31,  1.68s/it]11/16/2022 23:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.8117e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:23 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1400/6700 [39:16<2:28:37,  1.68s/it]11/16/2022 23:33:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.9715e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:25 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1401/6700 [39:17<2:28:47,  1.68s/it]11/16/2022 23:33:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.6474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:26 - INFO - train.train_snli_ve - loss is tensor(0.8488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1402/6700 [39:19<2:29:01,  1.69s/it]11/16/2022 23:33:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.0474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:28 - INFO - train.train_snli_ve - loss is tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1403/6700 [39:21<2:29:36,  1.69s/it]11/16/2022 23:33:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.1731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:30 - INFO - train.train_snli_ve - loss is tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1404/6700 [39:22<2:27:25,  1.67s/it]11/16/2022 23:33:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.3950e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:31 - INFO - train.train_snli_ve - loss is tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1405/6700 [39:24<2:29:01,  1.69s/it]11/16/2022 23:33:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.5754e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:33 - INFO - train.train_snli_ve - loss is tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##        | 1406/6700 [39:26<2:26:26,  1.66s/it]11/16/2022 23:33:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.6009e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:35 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1407/6700 [39:27<2:28:54,  1.69s/it]11/16/2022 23:33:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.2675e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:36 - INFO - train.train_snli_ve - loss is tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1408/6700 [39:29<2:27:59,  1.68s/it]11/16/2022 23:33:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.3004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:38 - INFO - train.train_snli_ve - loss is tensor(0.8567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1409/6700 [39:31<2:29:35,  1.70s/it]11/16/2022 23:33:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.9847e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:40 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1410/6700 [39:32<2:27:08,  1.67s/it]11/16/2022 23:33:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.6872e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:41 - INFO - train.train_snli_ve - loss is tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1411/6700 [39:34<2:28:10,  1.68s/it]11/16/2022 23:33:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.2091e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:43 - INFO - train.train_snli_ve - loss is tensor(0.8387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1412/6700 [39:36<2:26:54,  1.67s/it]11/16/2022 23:33:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.1712e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:45 - INFO - train.train_snli_ve - loss is tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1413/6700 [39:37<2:28:34,  1.69s/it]11/16/2022 23:33:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.6521e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:46 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1414/6700 [39:39<2:26:18,  1.66s/it]11/16/2022 23:33:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.9216e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:48 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1415/6700 [39:41<2:27:42,  1.68s/it]11/16/2022 23:33:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.8038e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:50 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1416/6700 [39:42<2:26:31,  1.66s/it]11/16/2022 23:33:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.7963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:51 - INFO - train.train_snli_ve - loss is tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1417/6700 [39:44<2:27:40,  1.68s/it]11/16/2022 23:33:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:53 - INFO - train.train_snli_ve - loss is tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1418/6700 [39:46<2:25:54,  1.66s/it]11/16/2022 23:33:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.6190e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:55 - INFO - train.train_snli_ve - loss is tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1419/6700 [39:47<2:26:44,  1.67s/it]11/16/2022 23:33:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.3033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:56 - INFO - train.train_snli_ve - loss is tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1420/6700 [39:49<2:26:43,  1.67s/it]11/16/2022 23:33:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.8379e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:33:58 - INFO - train.train_snli_ve - loss is tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1421/6700 [39:51<2:26:45,  1.67s/it]11/16/2022 23:34:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.2217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:00 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1422/6700 [39:52<2:25:54,  1.66s/it]11/16/2022 23:34:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.9416e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:01 - INFO - train.train_snli_ve - loss is tensor(0.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1423/6700 [39:54<2:26:17,  1.66s/it]11/16/2022 23:34:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.5603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:03 - INFO - train.train_snli_ve - loss is tensor(0.8994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1424/6700 [39:56<2:24:07,  1.64s/it]11/16/2022 23:34:05 - INFO - train.train_snli_ve - kd_loss is tensor(6.8138e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:05 - INFO - train.train_snli_ve - loss is tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1425/6700 [39:57<2:26:14,  1.66s/it]11/16/2022 23:34:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.9643e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:06 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1426/6700 [39:59<2:25:04,  1.65s/it]11/16/2022 23:34:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.0093e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:08 - INFO - train.train_snli_ve - loss is tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1427/6700 [40:01<2:26:45,  1.67s/it]11/16/2022 23:34:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.4305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:10 - INFO - train.train_snli_ve - loss is tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1428/6700 [40:02<2:26:18,  1.67s/it]11/16/2022 23:34:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.4070e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:11 - INFO - train.train_snli_ve - loss is tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1429/6700 [40:04<2:26:02,  1.66s/it]11/16/2022 23:34:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.7693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:13 - INFO - train.train_snli_ve - loss is tensor(0.8830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1430/6700 [40:06<2:25:48,  1.66s/it]11/16/2022 23:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.6536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:15 - INFO - train.train_snli_ve - loss is tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1431/6700 [40:07<2:26:39,  1.67s/it]11/16/2022 23:34:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7248e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:16 - INFO - train.train_snli_ve - loss is tensor(0.9890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1432/6700 [40:09<2:26:05,  1.66s/it]11/16/2022 23:34:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.8497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:18 - INFO - train.train_snli_ve - loss is tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1433/6700 [40:11<2:27:46,  1.68s/it]11/16/2022 23:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.5532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:20 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1434/6700 [40:12<2:27:09,  1.68s/it]11/16/2022 23:34:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.8095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:21 - INFO - train.train_snli_ve - loss is tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1435/6700 [40:14<2:28:01,  1.69s/it]11/16/2022 23:34:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.6488e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:23 - INFO - train.train_snli_ve - loss is tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1436/6700 [40:16<2:27:23,  1.68s/it]11/16/2022 23:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.6756e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:25 - INFO - train.train_snli_ve - loss is tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1437/6700 [40:17<2:29:01,  1.70s/it]11/16/2022 23:34:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.1071e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:26 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1438/6700 [40:19<2:27:17,  1.68s/it]11/16/2022 23:34:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.2795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:28 - INFO - train.train_snli_ve - loss is tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1439/6700 [40:21<2:27:38,  1.68s/it]11/16/2022 23:34:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.0810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:30 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  21%|##1       | 1440/6700 [40:22<2:26:16,  1.67s/it]11/16/2022 23:34:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.4380e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:31 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1441/6700 [40:24<2:27:56,  1.69s/it]11/16/2022 23:34:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.2334e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:33 - INFO - train.train_snli_ve - loss is tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1442/6700 [40:26<2:25:23,  1.66s/it]11/16/2022 23:34:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.8841e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:35 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1443/6700 [40:27<2:27:53,  1.69s/it]11/16/2022 23:34:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.5384e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:36 - INFO - train.train_snli_ve - loss is tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1444/6700 [40:29<2:27:00,  1.68s/it]11/16/2022 23:34:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.6734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:38 - INFO - train.train_snli_ve - loss is tensor(0.7270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1445/6700 [40:31<2:27:03,  1.68s/it]11/16/2022 23:34:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.4494e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:40 - INFO - train.train_snli_ve - loss is tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1446/6700 [40:32<2:25:37,  1.66s/it]11/16/2022 23:34:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.8873e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:41 - INFO - train.train_snli_ve - loss is tensor(1.2026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1447/6700 [40:34<2:26:31,  1.67s/it]11/16/2022 23:34:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.5602e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:43 - INFO - train.train_snli_ve - loss is tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1448/6700 [40:36<2:24:50,  1.65s/it]11/16/2022 23:34:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.5852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:45 - INFO - train.train_snli_ve - loss is tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1449/6700 [40:37<2:25:47,  1.67s/it]11/16/2022 23:34:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.7623e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:46 - INFO - train.train_snli_ve - loss is tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1450/6700 [40:39<2:26:16,  1.67s/it]11/16/2022 23:34:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.4006e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:48 - INFO - train.train_snli_ve - loss is tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1451/6700 [40:41<2:27:36,  1.69s/it]11/16/2022 23:34:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.1582e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:50 - INFO - train.train_snli_ve - loss is tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1452/6700 [40:43<2:26:15,  1.67s/it]11/16/2022 23:34:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.4320e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:52 - INFO - train.train_snli_ve - loss is tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1453/6700 [40:44<2:27:05,  1.68s/it]11/16/2022 23:34:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.2757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:53 - INFO - train.train_snli_ve - loss is tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1454/6700 [40:46<2:25:55,  1.67s/it]11/16/2022 23:34:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.4180e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:55 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1455/6700 [40:48<2:26:58,  1.68s/it]11/16/2022 23:34:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.6354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:57 - INFO - train.train_snli_ve - loss is tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1456/6700 [40:49<2:25:40,  1.67s/it]11/16/2022 23:34:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.9393e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:34:58 - INFO - train.train_snli_ve - loss is tensor(0.9236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1457/6700 [40:51<2:27:15,  1.69s/it]11/16/2022 23:35:00 - INFO - train.train_snli_ve - kd_loss is tensor(5.3409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:00 - INFO - train.train_snli_ve - loss is tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1458/6700 [40:53<2:25:52,  1.67s/it]11/16/2022 23:35:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.9677e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:02 - INFO - train.train_snli_ve - loss is tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1459/6700 [40:54<2:27:56,  1.69s/it]11/16/2022 23:35:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.8666e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:03 - INFO - train.train_snli_ve - loss is tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1460/6700 [40:56<2:26:57,  1.68s/it]11/16/2022 23:35:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.6250e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:05 - INFO - train.train_snli_ve - loss is tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1461/6700 [40:58<2:27:35,  1.69s/it]11/16/2022 23:35:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.0124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:07 - INFO - train.train_snli_ve - loss is tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1462/6700 [40:59<2:26:29,  1.68s/it]11/16/2022 23:35:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.8797e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:08 - INFO - train.train_snli_ve - loss is tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1463/6700 [41:01<2:26:49,  1.68s/it]11/16/2022 23:35:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.3855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:10 - INFO - train.train_snli_ve - loss is tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1464/6700 [41:03<2:25:03,  1.66s/it]11/16/2022 23:35:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.9036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:12 - INFO - train.train_snli_ve - loss is tensor(0.9437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1465/6700 [41:04<2:27:17,  1.69s/it]11/16/2022 23:35:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.8413e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:13 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1466/6700 [41:06<2:26:26,  1.68s/it]11/16/2022 23:35:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.9556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:15 - INFO - train.train_snli_ve - loss is tensor(0.9184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1467/6700 [41:08<2:28:34,  1.70s/it]11/16/2022 23:35:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.7171e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:17 - INFO - train.train_snli_ve - loss is tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1468/6700 [41:09<2:27:27,  1.69s/it]11/16/2022 23:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(6.0303e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:18 - INFO - train.train_snli_ve - loss is tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1469/6700 [41:11<2:28:33,  1.70s/it]11/16/2022 23:35:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.1155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:20 - INFO - train.train_snli_ve - loss is tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1470/6700 [41:13<2:26:48,  1.68s/it]11/16/2022 23:35:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.2986e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:22 - INFO - train.train_snli_ve - loss is tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1471/6700 [41:15<2:27:05,  1.69s/it]11/16/2022 23:35:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.9374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:23 - INFO - train.train_snli_ve - loss is tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1472/6700 [41:16<2:24:55,  1.66s/it]11/16/2022 23:35:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.2331e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:25 - INFO - train.train_snli_ve - loss is tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##1       | 1473/6700 [41:18<2:26:36,  1.68s/it]11/16/2022 23:35:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.1113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:27 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1474/6700 [41:20<2:25:43,  1.67s/it]11/16/2022 23:35:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.6085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:29 - INFO - train.train_snli_ve - loss is tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1475/6700 [41:21<2:26:52,  1.69s/it]11/16/2022 23:35:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.4095e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:30 - INFO - train.train_snli_ve - loss is tensor(0.9241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1476/6700 [41:23<2:25:06,  1.67s/it]11/16/2022 23:35:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.8970e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:32 - INFO - train.train_snli_ve - loss is tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1477/6700 [41:25<2:27:05,  1.69s/it]11/16/2022 23:35:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0544e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:34 - INFO - train.train_snli_ve - loss is tensor(0.8846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1478/6700 [41:26<2:25:34,  1.67s/it]11/16/2022 23:35:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.4513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:35 - INFO - train.train_snli_ve - loss is tensor(0.7712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1479/6700 [41:28<2:28:04,  1.70s/it]11/16/2022 23:35:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.5772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:37 - INFO - train.train_snli_ve - loss is tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1480/6700 [41:30<2:25:41,  1.67s/it]11/16/2022 23:35:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:39 - INFO - train.train_snli_ve - loss is tensor(0.8276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1481/6700 [41:31<2:26:34,  1.69s/it]11/16/2022 23:35:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.7895e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:40 - INFO - train.train_snli_ve - loss is tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1482/6700 [41:33<2:25:51,  1.68s/it]11/16/2022 23:35:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8257e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:42 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1483/6700 [41:35<2:27:32,  1.70s/it]11/16/2022 23:35:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5021e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:44 - INFO - train.train_snli_ve - loss is tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1484/6700 [41:36<2:26:14,  1.68s/it]11/16/2022 23:35:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.6977e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:45 - INFO - train.train_snli_ve - loss is tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1485/6700 [41:38<2:27:11,  1.69s/it]11/16/2022 23:35:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.7428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:47 - INFO - train.train_snli_ve - loss is tensor(1.0081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1486/6700 [41:40<2:27:27,  1.70s/it]11/16/2022 23:35:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:49 - INFO - train.train_snli_ve - loss is tensor(0.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1487/6700 [41:41<2:26:56,  1.69s/it]11/16/2022 23:35:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.5463e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:50 - INFO - train.train_snli_ve - loss is tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1488/6700 [41:43<2:25:26,  1.67s/it]11/16/2022 23:35:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.4275e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:52 - INFO - train.train_snli_ve - loss is tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1489/6700 [41:45<2:25:28,  1.68s/it]11/16/2022 23:35:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:54 - INFO - train.train_snli_ve - loss is tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1490/6700 [41:46<2:26:35,  1.69s/it]11/16/2022 23:35:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.0349e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:56 - INFO - train.train_snli_ve - loss is tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1491/6700 [41:48<2:27:17,  1.70s/it]11/16/2022 23:35:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.8578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:57 - INFO - train.train_snli_ve - loss is tensor(0.8384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1492/6700 [41:50<2:25:19,  1.67s/it]11/16/2022 23:35:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.2486e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:35:59 - INFO - train.train_snli_ve - loss is tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1493/6700 [41:52<2:27:46,  1.70s/it]11/16/2022 23:36:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.8535e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:01 - INFO - train.train_snli_ve - loss is tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1494/6700 [41:53<2:25:36,  1.68s/it]11/16/2022 23:36:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.7284e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:02 - INFO - train.train_snli_ve - loss is tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1495/6700 [41:55<2:25:54,  1.68s/it]11/16/2022 23:36:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.8745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:04 - INFO - train.train_snli_ve - loss is tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1496/6700 [41:57<2:25:10,  1.67s/it]11/16/2022 23:36:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.6434e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:06 - INFO - train.train_snli_ve - loss is tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1497/6700 [41:58<2:25:57,  1.68s/it]11/16/2022 23:36:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.1650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:07 - INFO - train.train_snli_ve - loss is tensor(0.8395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1498/6700 [42:00<2:23:38,  1.66s/it]11/16/2022 23:36:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.8838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:09 - INFO - train.train_snli_ve - loss is tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1499/6700 [42:02<2:27:39,  1.70s/it]11/16/2022 23:36:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.3111e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:11 - INFO - train.train_snli_ve - loss is tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1500/6700 [42:03<2:26:33,  1.69s/it]11/16/2022 23:36:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.0968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:12 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1501/6700 [42:05<2:27:58,  1.71s/it]11/16/2022 23:36:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.2130e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:14 - INFO - train.train_snli_ve - loss is tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1502/6700 [42:07<2:25:47,  1.68s/it]11/16/2022 23:36:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.8767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:16 - INFO - train.train_snli_ve - loss is tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1503/6700 [42:08<2:25:37,  1.68s/it]11/16/2022 23:36:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.5065e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:17 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1504/6700 [42:10<2:25:39,  1.68s/it]11/16/2022 23:36:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.4934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:19 - INFO - train.train_snli_ve - loss is tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1505/6700 [42:12<2:26:15,  1.69s/it]11/16/2022 23:36:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.8762e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:21 - INFO - train.train_snli_ve - loss is tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1506/6700 [42:13<2:25:09,  1.68s/it]11/16/2022 23:36:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.6477e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:22 - INFO - train.train_snli_ve - loss is tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  22%|##2       | 1507/6700 [42:15<2:26:27,  1.69s/it]11/16/2022 23:36:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.1961e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:24 - INFO - train.train_snli_ve - loss is tensor(0.9112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1508/6700 [42:17<2:24:44,  1.67s/it]11/16/2022 23:36:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.0471e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:26 - INFO - train.train_snli_ve - loss is tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1509/6700 [42:18<2:25:26,  1.68s/it]11/16/2022 23:36:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.6539e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:27 - INFO - train.train_snli_ve - loss is tensor(0.9298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1510/6700 [42:20<2:24:25,  1.67s/it]11/16/2022 23:36:29 - INFO - train.train_snli_ve - kd_loss is tensor(5.4497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:29 - INFO - train.train_snli_ve - loss is tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1511/6700 [42:22<2:26:10,  1.69s/it]11/16/2022 23:36:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.1476e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:31 - INFO - train.train_snli_ve - loss is tensor(0.8197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1512/6700 [42:24<2:25:50,  1.69s/it]11/16/2022 23:36:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.5175e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:33 - INFO - train.train_snli_ve - loss is tensor(0.4762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1513/6700 [42:25<2:25:59,  1.69s/it]11/16/2022 23:36:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:34 - INFO - train.train_snli_ve - loss is tensor(0.5313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1514/6700 [42:27<2:24:39,  1.67s/it]11/16/2022 23:36:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.8080e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:36 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1515/6700 [42:29<2:25:57,  1.69s/it]11/16/2022 23:36:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.8459e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:38 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1516/6700 [42:30<2:26:20,  1.69s/it]11/16/2022 23:36:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.6147e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:39 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1517/6700 [42:32<2:27:51,  1.71s/it]11/16/2022 23:36:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:41 - INFO - train.train_snli_ve - loss is tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1518/6700 [42:34<2:27:06,  1.70s/it]11/16/2022 23:36:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.7868e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:43 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1519/6700 [42:35<2:27:13,  1.71s/it]11/16/2022 23:36:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:44 - INFO - train.train_snli_ve - loss is tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1520/6700 [42:37<2:24:25,  1.67s/it]11/16/2022 23:36:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.2310e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:46 - INFO - train.train_snli_ve - loss is tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1521/6700 [42:39<2:24:59,  1.68s/it]11/16/2022 23:36:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.2201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:48 - INFO - train.train_snli_ve - loss is tensor(0.4881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1522/6700 [42:40<2:24:32,  1.67s/it]11/16/2022 23:36:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.2139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:49 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1523/6700 [42:42<2:25:11,  1.68s/it]11/16/2022 23:36:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.7852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:51 - INFO - train.train_snli_ve - loss is tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1524/6700 [42:44<2:24:11,  1.67s/it]11/16/2022 23:36:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.5037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:53 - INFO - train.train_snli_ve - loss is tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1525/6700 [42:45<2:26:01,  1.69s/it]11/16/2022 23:36:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.6165e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:54 - INFO - train.train_snli_ve - loss is tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1526/6700 [42:47<2:24:25,  1.67s/it]11/16/2022 23:36:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.8766e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:56 - INFO - train.train_snli_ve - loss is tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1527/6700 [42:49<2:26:27,  1.70s/it]11/16/2022 23:36:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.0827e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:36:58 - INFO - train.train_snli_ve - loss is tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1528/6700 [42:51<2:25:05,  1.68s/it]11/16/2022 23:37:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.2944e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:00 - INFO - train.train_snli_ve - loss is tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1529/6700 [42:52<2:27:29,  1.71s/it]11/16/2022 23:37:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.2228e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:01 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1530/6700 [42:54<2:25:24,  1.69s/it]11/16/2022 23:37:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.8704e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:03 - INFO - train.train_snli_ve - loss is tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1531/6700 [42:56<2:25:27,  1.69s/it]11/16/2022 23:37:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.2746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:05 - INFO - train.train_snli_ve - loss is tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1532/6700 [42:57<2:23:50,  1.67s/it]11/16/2022 23:37:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.1424e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:06 - INFO - train.train_snli_ve - loss is tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1533/6700 [42:59<2:24:12,  1.67s/it]11/16/2022 23:37:08 - INFO - train.train_snli_ve - kd_loss is tensor(9.2132e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:08 - INFO - train.train_snli_ve - loss is tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1534/6700 [43:01<2:24:24,  1.68s/it]11/16/2022 23:37:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.6530e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:10 - INFO - train.train_snli_ve - loss is tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1535/6700 [43:02<2:25:24,  1.69s/it]11/16/2022 23:37:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.6054e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:11 - INFO - train.train_snli_ve - loss is tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1536/6700 [43:04<2:24:00,  1.67s/it]11/16/2022 23:37:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.6025e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:13 - INFO - train.train_snli_ve - loss is tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1537/6700 [43:06<2:26:07,  1.70s/it]11/16/2022 23:37:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.4395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:15 - INFO - train.train_snli_ve - loss is tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1538/6700 [43:07<2:24:20,  1.68s/it]11/16/2022 23:37:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.9663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:16 - INFO - train.train_snli_ve - loss is tensor(1.1441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1539/6700 [43:09<2:25:37,  1.69s/it]11/16/2022 23:37:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.5574e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:18 - INFO - train.train_snli_ve - loss is tensor(0.8942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##2       | 1540/6700 [43:11<2:24:51,  1.68s/it]11/16/2022 23:37:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.4001e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:20 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1541/6700 [43:12<2:24:44,  1.68s/it]11/16/2022 23:37:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.9072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:21 - INFO - train.train_snli_ve - loss is tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1542/6700 [43:14<2:24:47,  1.68s/it]11/16/2022 23:37:23 - INFO - train.train_snli_ve - kd_loss is tensor(4.3818e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:23 - INFO - train.train_snli_ve - loss is tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1543/6700 [43:16<2:25:16,  1.69s/it]11/16/2022 23:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.0288e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:25 - INFO - train.train_snli_ve - loss is tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1544/6700 [43:17<2:24:28,  1.68s/it]11/16/2022 23:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.0087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:27 - INFO - train.train_snli_ve - loss is tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1545/6700 [43:19<2:25:09,  1.69s/it]11/16/2022 23:37:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.1531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:28 - INFO - train.train_snli_ve - loss is tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1546/6700 [43:21<2:25:30,  1.69s/it]11/16/2022 23:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.5205e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:30 - INFO - train.train_snli_ve - loss is tensor(0.9502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1547/6700 [43:23<2:25:03,  1.69s/it]11/16/2022 23:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.4068e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:32 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1548/6700 [43:24<2:23:33,  1.67s/it]11/16/2022 23:37:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.4968e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:33 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1549/6700 [43:26<2:25:05,  1.69s/it]11/16/2022 23:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.2570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:35 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1550/6700 [43:28<2:23:56,  1.68s/it]11/16/2022 23:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.4796e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:37 - INFO - train.train_snli_ve - loss is tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1551/6700 [43:29<2:23:58,  1.68s/it]11/16/2022 23:37:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.0361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:38 - INFO - train.train_snli_ve - loss is tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1552/6700 [43:31<2:24:30,  1.68s/it]11/16/2022 23:37:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.6534e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:40 - INFO - train.train_snli_ve - loss is tensor(0.9553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1553/6700 [43:33<2:25:04,  1.69s/it]11/16/2022 23:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.4599e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:42 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1554/6700 [43:34<2:23:06,  1.67s/it]11/16/2022 23:37:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.2962e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:43 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1555/6700 [43:36<2:23:31,  1.67s/it]11/16/2022 23:37:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.2097e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:45 - INFO - train.train_snli_ve - loss is tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1556/6700 [43:38<2:22:29,  1.66s/it]11/16/2022 23:37:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.2396e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:47 - INFO - train.train_snli_ve - loss is tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1557/6700 [43:39<2:23:01,  1.67s/it]11/16/2022 23:37:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.9343e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:48 - INFO - train.train_snli_ve - loss is tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1558/6700 [43:41<2:21:26,  1.65s/it]11/16/2022 23:37:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.8645e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:50 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1559/6700 [43:43<2:22:43,  1.67s/it]11/16/2022 23:37:52 - INFO - train.train_snli_ve - kd_loss is tensor(8.7316e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:52 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1560/6700 [43:44<2:20:42,  1.64s/it]11/16/2022 23:37:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.9604e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:53 - INFO - train.train_snli_ve - loss is tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1561/6700 [43:46<2:21:01,  1.65s/it]11/16/2022 23:37:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.1665e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:55 - INFO - train.train_snli_ve - loss is tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1562/6700 [43:47<2:19:55,  1.63s/it]11/16/2022 23:37:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.9781e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:56 - INFO - train.train_snli_ve - loss is tensor(1.1249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1563/6700 [43:49<2:20:10,  1.64s/it]11/16/2022 23:37:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.7539e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:37:58 - INFO - train.train_snli_ve - loss is tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1564/6700 [43:51<2:19:23,  1.63s/it]11/16/2022 23:38:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.8570e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:00 - INFO - train.train_snli_ve - loss is tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1565/6700 [43:52<2:20:21,  1.64s/it]11/16/2022 23:38:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.2155e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:01 - INFO - train.train_snli_ve - loss is tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1566/6700 [43:54<2:18:39,  1.62s/it]11/16/2022 23:38:03 - INFO - train.train_snli_ve - kd_loss is tensor(7.5118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:03 - INFO - train.train_snli_ve - loss is tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1567/6700 [43:56<2:18:50,  1.62s/it]11/16/2022 23:38:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.5124e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:04 - INFO - train.train_snli_ve - loss is tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1568/6700 [43:57<2:18:24,  1.62s/it]11/16/2022 23:38:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.0497e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:06 - INFO - train.train_snli_ve - loss is tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1569/6700 [43:59<2:19:22,  1.63s/it]11/16/2022 23:38:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.7106e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:08 - INFO - train.train_snli_ve - loss is tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1570/6700 [44:00<2:17:23,  1.61s/it]11/16/2022 23:38:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0497e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:09 - INFO - train.train_snli_ve - loss is tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1571/6700 [44:02<2:18:07,  1.62s/it]11/16/2022 23:38:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.5924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:11 - INFO - train.train_snli_ve - loss is tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1572/6700 [44:04<2:16:24,  1.60s/it]11/16/2022 23:38:13 - INFO - train.train_snli_ve - kd_loss is tensor(7.1674e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:13 - INFO - train.train_snli_ve - loss is tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1573/6700 [44:05<2:18:17,  1.62s/it]11/16/2022 23:38:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.0680e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:14 - INFO - train.train_snli_ve - loss is tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  23%|##3       | 1574/6700 [44:07<2:17:50,  1.61s/it]11/16/2022 23:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(6.3850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:16 - INFO - train.train_snli_ve - loss is tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1575/6700 [44:08<2:18:14,  1.62s/it]11/16/2022 23:38:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.3658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:17 - INFO - train.train_snli_ve - loss is tensor(0.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1576/6700 [44:10<2:18:25,  1.62s/it]11/16/2022 23:38:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.0890e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:19 - INFO - train.train_snli_ve - loss is tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1577/6700 [44:12<2:18:57,  1.63s/it]11/16/2022 23:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(6.3683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:21 - INFO - train.train_snli_ve - loss is tensor(0.8722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1578/6700 [44:13<2:17:59,  1.62s/it]11/16/2022 23:38:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.3695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:22 - INFO - train.train_snli_ve - loss is tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1579/6700 [44:15<2:18:26,  1.62s/it]11/16/2022 23:38:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.9995e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:24 - INFO - train.train_snli_ve - loss is tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1580/6700 [44:17<2:17:23,  1.61s/it]11/16/2022 23:38:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.9752e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:26 - INFO - train.train_snli_ve - loss is tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1581/6700 [44:18<2:18:27,  1.62s/it]11/16/2022 23:38:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.9507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:27 - INFO - train.train_snli_ve - loss is tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1582/6700 [44:20<2:17:15,  1.61s/it]11/16/2022 23:38:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.5324e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:29 - INFO - train.train_snli_ve - loss is tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1583/6700 [44:21<2:18:11,  1.62s/it]11/16/2022 23:38:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.4844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:30 - INFO - train.train_snli_ve - loss is tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1584/6700 [44:23<2:17:17,  1.61s/it]11/16/2022 23:38:32 - INFO - train.train_snli_ve - kd_loss is tensor(6.5205e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:32 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1585/6700 [44:25<2:18:33,  1.63s/it]11/16/2022 23:38:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.6206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:34 - INFO - train.train_snli_ve - loss is tensor(0.8802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1586/6700 [44:26<2:17:23,  1.61s/it]11/16/2022 23:38:35 - INFO - train.train_snli_ve - kd_loss is tensor(6.4727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:35 - INFO - train.train_snli_ve - loss is tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1587/6700 [44:28<2:18:08,  1.62s/it]11/16/2022 23:38:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.2981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:37 - INFO - train.train_snli_ve - loss is tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1588/6700 [44:29<2:16:32,  1.60s/it]11/16/2022 23:38:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.0768e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:38 - INFO - train.train_snli_ve - loss is tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1589/6700 [44:31<2:17:18,  1.61s/it]11/16/2022 23:38:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.2070e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:40 - INFO - train.train_snli_ve - loss is tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1590/6700 [44:33<2:16:29,  1.60s/it]11/16/2022 23:38:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.5499e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:42 - INFO - train.train_snli_ve - loss is tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1591/6700 [44:34<2:16:43,  1.61s/it]11/16/2022 23:38:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.6881e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:43 - INFO - train.train_snli_ve - loss is tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1592/6700 [44:36<2:16:37,  1.60s/it]11/16/2022 23:38:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.7854e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:45 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1593/6700 [44:38<2:17:38,  1.62s/it]11/16/2022 23:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.9684e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:46 - INFO - train.train_snli_ve - loss is tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1594/6700 [44:39<2:17:15,  1.61s/it]11/16/2022 23:38:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.5268e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:48 - INFO - train.train_snli_ve - loss is tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1595/6700 [44:41<2:18:14,  1.62s/it]11/16/2022 23:38:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.1210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:50 - INFO - train.train_snli_ve - loss is tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1596/6700 [44:42<2:16:40,  1.61s/it]11/16/2022 23:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.2153e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:51 - INFO - train.train_snli_ve - loss is tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1597/6700 [44:44<2:17:20,  1.61s/it]11/16/2022 23:38:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.0354e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:53 - INFO - train.train_snli_ve - loss is tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1598/6700 [44:46<2:15:40,  1.60s/it]11/16/2022 23:38:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.8978e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:55 - INFO - train.train_snli_ve - loss is tensor(0.8852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1599/6700 [44:47<2:17:40,  1.62s/it]11/16/2022 23:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.8353e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:56 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1600/6700 [44:49<2:17:53,  1.62s/it]11/16/2022 23:38:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.4596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:58 - INFO - train.train_snli_ve - loss is tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1601/6700 [44:50<2:18:18,  1.63s/it]11/16/2022 23:38:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.9068e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:38:59 - INFO - train.train_snli_ve - loss is tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1602/6700 [44:52<2:17:31,  1.62s/it]11/16/2022 23:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.1075e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:01 - INFO - train.train_snli_ve - loss is tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1603/6700 [44:54<2:18:58,  1.64s/it]11/16/2022 23:39:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.6261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:03 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1604/6700 [44:55<2:17:07,  1.61s/it]11/16/2022 23:39:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.6231e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:04 - INFO - train.train_snli_ve - loss is tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1605/6700 [44:57<2:17:45,  1.62s/it]11/16/2022 23:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(6.5813e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:06 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1606/6700 [44:59<2:16:01,  1.60s/it]11/16/2022 23:39:07 - INFO - train.train_snli_ve - kd_loss is tensor(6.5772e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:07 - INFO - train.train_snli_ve - loss is tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##3       | 1607/6700 [45:00<2:16:30,  1.61s/it]11/16/2022 23:39:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.7603e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:09 - INFO - train.train_snli_ve - loss is tensor(0.8860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1608/6700 [45:02<2:15:24,  1.60s/it]11/16/2022 23:39:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.6125e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:11 - INFO - train.train_snli_ve - loss is tensor(0.9095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1609/6700 [45:03<2:17:45,  1.62s/it]11/16/2022 23:39:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.7074e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:12 - INFO - train.train_snli_ve - loss is tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1610/6700 [45:05<2:16:20,  1.61s/it]11/16/2022 23:39:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.1974e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:14 - INFO - train.train_snli_ve - loss is tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1611/6700 [45:07<2:17:46,  1.62s/it]11/16/2022 23:39:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.7176e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:16 - INFO - train.train_snli_ve - loss is tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1612/6700 [45:08<2:16:46,  1.61s/it]11/16/2022 23:39:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.8237e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:17 - INFO - train.train_snli_ve - loss is tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1613/6700 [45:10<2:17:22,  1.62s/it]11/16/2022 23:39:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.0142e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:19 - INFO - train.train_snli_ve - loss is tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1614/6700 [45:11<2:16:22,  1.61s/it]11/16/2022 23:39:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.8567e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:20 - INFO - train.train_snli_ve - loss is tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1615/6700 [45:13<2:18:41,  1.64s/it]11/16/2022 23:39:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.0306e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:22 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1616/6700 [45:15<2:17:23,  1.62s/it]11/16/2022 23:39:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.8406e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:24 - INFO - train.train_snli_ve - loss is tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1617/6700 [45:16<2:18:36,  1.64s/it]11/16/2022 23:39:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.8429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:25 - INFO - train.train_snli_ve - loss is tensor(0.7602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1618/6700 [45:18<2:16:38,  1.61s/it]11/16/2022 23:39:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.6757e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:27 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1619/6700 [45:20<2:19:22,  1.65s/it]11/16/2022 23:39:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.2832e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:29 - INFO - train.train_snli_ve - loss is tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1620/6700 [45:21<2:17:46,  1.63s/it]11/16/2022 23:39:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.2596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:30 - INFO - train.train_snli_ve - loss is tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1621/6700 [45:23<2:19:21,  1.65s/it]11/16/2022 23:39:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.8356e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:32 - INFO - train.train_snli_ve - loss is tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1622/6700 [45:25<2:17:55,  1.63s/it]11/16/2022 23:39:34 - INFO - train.train_snli_ve - kd_loss is tensor(6.2374e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:34 - INFO - train.train_snli_ve - loss is tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1623/6700 [45:26<2:18:36,  1.64s/it]11/16/2022 23:39:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.2428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:35 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1624/6700 [45:28<2:17:00,  1.62s/it]11/16/2022 23:39:37 - INFO - train.train_snli_ve - kd_loss is tensor(4.7510e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:37 - INFO - train.train_snli_ve - loss is tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1625/6700 [45:29<2:16:58,  1.62s/it]11/16/2022 23:39:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.8352e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:38 - INFO - train.train_snli_ve - loss is tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1626/6700 [45:31<2:15:21,  1.60s/it]11/16/2022 23:39:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.8679e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:40 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1627/6700 [45:33<2:16:52,  1.62s/it]11/16/2022 23:39:42 - INFO - train.train_snli_ve - kd_loss is tensor(4.4364e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:42 - INFO - train.train_snli_ve - loss is tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1628/6700 [45:34<2:15:39,  1.60s/it]11/16/2022 23:39:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.3922e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:43 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1629/6700 [45:36<2:18:04,  1.63s/it]11/16/2022 23:39:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.5668e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:45 - INFO - train.train_snli_ve - loss is tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1630/6700 [45:37<2:17:05,  1.62s/it]11/16/2022 23:39:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.4298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:46 - INFO - train.train_snli_ve - loss is tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1631/6700 [45:39<2:17:39,  1.63s/it]11/16/2022 23:39:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.8784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:48 - INFO - train.train_snli_ve - loss is tensor(0.7766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1632/6700 [45:41<2:17:13,  1.62s/it]11/16/2022 23:39:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.2977e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:50 - INFO - train.train_snli_ve - loss is tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1633/6700 [45:42<2:17:23,  1.63s/it]11/16/2022 23:39:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.8903e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:51 - INFO - train.train_snli_ve - loss is tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1634/6700 [45:44<2:15:35,  1.61s/it]11/16/2022 23:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.5373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:53 - INFO - train.train_snli_ve - loss is tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1635/6700 [45:46<2:17:15,  1.63s/it]11/16/2022 23:39:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.0195e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:55 - INFO - train.train_snli_ve - loss is tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1636/6700 [45:47<2:16:03,  1.61s/it]11/16/2022 23:39:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.4994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:56 - INFO - train.train_snli_ve - loss is tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1637/6700 [45:49<2:16:18,  1.62s/it]11/16/2022 23:39:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.4278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:58 - INFO - train.train_snli_ve - loss is tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1638/6700 [45:50<2:16:09,  1.61s/it]11/16/2022 23:39:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.1531e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:39:59 - INFO - train.train_snli_ve - loss is tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1639/6700 [45:52<2:17:21,  1.63s/it]11/16/2022 23:40:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.3140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:01 - INFO - train.train_snli_ve - loss is tensor(0.8597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1640/6700 [45:54<2:17:01,  1.62s/it]11/16/2022 23:40:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.6030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:03 - INFO - train.train_snli_ve - loss is tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  24%|##4       | 1641/6700 [45:55<2:17:41,  1.63s/it]11/16/2022 23:40:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.0458e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:04 - INFO - train.train_snli_ve - loss is tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1642/6700 [45:57<2:16:56,  1.62s/it]11/16/2022 23:40:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.9360e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:06 - INFO - train.train_snli_ve - loss is tensor(0.7857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1643/6700 [45:59<2:18:21,  1.64s/it]11/16/2022 23:40:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.4404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:08 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1644/6700 [46:00<2:16:52,  1.62s/it]11/16/2022 23:40:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.5201e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:09 - INFO - train.train_snli_ve - loss is tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1645/6700 [46:02<2:17:41,  1.63s/it]11/16/2022 23:40:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.8318e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:11 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1646/6700 [46:03<2:16:40,  1.62s/it]11/16/2022 23:40:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.0862e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:12 - INFO - train.train_snli_ve - loss is tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1647/6700 [46:05<2:17:04,  1.63s/it]11/16/2022 23:40:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.2312e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:14 - INFO - train.train_snli_ve - loss is tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1648/6700 [46:07<2:16:24,  1.62s/it]11/16/2022 23:40:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.5655e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:16 - INFO - train.train_snli_ve - loss is tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1649/6700 [46:08<2:17:21,  1.63s/it]11/16/2022 23:40:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.5474e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:17 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1650/6700 [46:10<2:15:48,  1.61s/it]11/16/2022 23:40:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.1392e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:19 - INFO - train.train_snli_ve - loss is tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1651/6700 [46:12<2:16:48,  1.63s/it]11/16/2022 23:40:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.8364e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:21 - INFO - train.train_snli_ve - loss is tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1652/6700 [46:13<2:16:06,  1.62s/it]11/16/2022 23:40:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.1036e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:22 - INFO - train.train_snli_ve - loss is tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1653/6700 [46:15<2:16:42,  1.63s/it]11/16/2022 23:40:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.5643e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:24 - INFO - train.train_snli_ve - loss is tensor(1.1857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1654/6700 [46:16<2:16:05,  1.62s/it]11/16/2022 23:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(7.6254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:25 - INFO - train.train_snli_ve - loss is tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1655/6700 [46:18<2:17:03,  1.63s/it]11/16/2022 23:40:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.2076e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:27 - INFO - train.train_snli_ve - loss is tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1656/6700 [46:20<2:15:56,  1.62s/it]11/16/2022 23:40:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.3728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:29 - INFO - train.train_snli_ve - loss is tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1657/6700 [46:21<2:18:50,  1.65s/it]11/16/2022 23:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.3244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:30 - INFO - train.train_snli_ve - loss is tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1658/6700 [46:23<2:17:37,  1.64s/it]11/16/2022 23:40:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.4836e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:32 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1659/6700 [46:25<2:18:36,  1.65s/it]11/16/2022 23:40:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.6928e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:34 - INFO - train.train_snli_ve - loss is tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1660/6700 [46:26<2:16:23,  1.62s/it]11/16/2022 23:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.3150e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:35 - INFO - train.train_snli_ve - loss is tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1661/6700 [46:28<2:16:33,  1.63s/it]11/16/2022 23:40:37 - INFO - train.train_snli_ve - kd_loss is tensor(6.9379e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:37 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1662/6700 [46:30<2:15:47,  1.62s/it]11/16/2022 23:40:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.6679e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:38 - INFO - train.train_snli_ve - loss is tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1663/6700 [46:31<2:16:05,  1.62s/it]11/16/2022 23:40:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.0976e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:40 - INFO - train.train_snli_ve - loss is tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1664/6700 [46:33<2:14:58,  1.61s/it]11/16/2022 23:40:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.6298e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:42 - INFO - train.train_snli_ve - loss is tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1665/6700 [46:34<2:16:15,  1.62s/it]11/16/2022 23:40:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.8855e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:43 - INFO - train.train_snli_ve - loss is tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1666/6700 [46:36<2:15:54,  1.62s/it]11/16/2022 23:40:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.4663e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:45 - INFO - train.train_snli_ve - loss is tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1667/6700 [46:38<2:16:19,  1.63s/it]11/16/2022 23:40:47 - INFO - train.train_snli_ve - kd_loss is tensor(5.4923e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:47 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1668/6700 [46:39<2:15:04,  1.61s/it]11/16/2022 23:40:48 - INFO - train.train_snli_ve - kd_loss is tensor(6.7829e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:48 - INFO - train.train_snli_ve - loss is tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1669/6700 [46:41<2:16:12,  1.62s/it]11/16/2022 23:40:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.7167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:50 - INFO - train.train_snli_ve - loss is tensor(0.8497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1670/6700 [46:42<2:14:31,  1.60s/it]11/16/2022 23:40:51 - INFO - train.train_snli_ve - kd_loss is tensor(5.2330e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:51 - INFO - train.train_snli_ve - loss is tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1671/6700 [46:44<2:16:54,  1.63s/it]11/16/2022 23:40:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.5638e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:53 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1672/6700 [46:46<2:14:54,  1.61s/it]11/16/2022 23:40:55 - INFO - train.train_snli_ve - kd_loss is tensor(5.1607e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:55 - INFO - train.train_snli_ve - loss is tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1673/6700 [46:47<2:15:09,  1.61s/it]11/16/2022 23:40:56 - INFO - train.train_snli_ve - kd_loss is tensor(6.2799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:56 - INFO - train.train_snli_ve - loss is tensor(0.8934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##4       | 1674/6700 [46:49<2:14:31,  1.61s/it]11/16/2022 23:40:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.5844e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:58 - INFO - train.train_snli_ve - loss is tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1675/6700 [46:50<2:14:51,  1.61s/it]11/16/2022 23:40:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.3936e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:40:59 - INFO - train.train_snli_ve - loss is tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1676/6700 [46:52<2:14:01,  1.60s/it]11/16/2022 23:41:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.5727e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:01 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1677/6700 [46:54<2:14:46,  1.61s/it]11/16/2022 23:41:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.8044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:03 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1678/6700 [46:55<2:13:21,  1.59s/it]11/16/2022 23:41:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.6720e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:04 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1679/6700 [46:57<2:14:34,  1.61s/it]11/16/2022 23:41:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.1588e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:06 - INFO - train.train_snli_ve - loss is tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1680/6700 [46:58<2:13:24,  1.59s/it]11/16/2022 23:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.6904e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:07 - INFO - train.train_snli_ve - loss is tensor(0.7851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1681/6700 [47:00<2:15:12,  1.62s/it]11/16/2022 23:41:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.2760e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:09 - INFO - train.train_snli_ve - loss is tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1682/6700 [47:02<2:14:30,  1.61s/it]11/16/2022 23:41:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.5420e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:11 - INFO - train.train_snli_ve - loss is tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1683/6700 [47:03<2:16:20,  1.63s/it]11/16/2022 23:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.4833e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:12 - INFO - train.train_snli_ve - loss is tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1684/6700 [47:05<2:14:15,  1.61s/it]11/16/2022 23:41:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.8294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:14 - INFO - train.train_snli_ve - loss is tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1685/6700 [47:07<2:15:43,  1.62s/it]11/16/2022 23:41:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:16 - INFO - train.train_snli_ve - loss is tensor(0.4772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1686/6700 [47:08<2:15:04,  1.62s/it]11/16/2022 23:41:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.5686e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:17 - INFO - train.train_snli_ve - loss is tensor(0.4664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1687/6700 [47:10<2:16:09,  1.63s/it]11/16/2022 23:41:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.2226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:19 - INFO - train.train_snli_ve - loss is tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1688/6700 [47:11<2:15:10,  1.62s/it]11/16/2022 23:41:20 - INFO - train.train_snli_ve - kd_loss is tensor(6.5485e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:20 - INFO - train.train_snli_ve - loss is tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1689/6700 [47:13<2:16:53,  1.64s/it]11/16/2022 23:41:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.1569e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:22 - INFO - train.train_snli_ve - loss is tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1690/6700 [47:15<2:15:58,  1.63s/it]11/16/2022 23:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.4673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:24 - INFO - train.train_snli_ve - loss is tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1691/6700 [47:16<2:16:59,  1.64s/it]11/16/2022 23:41:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4190e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:25 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1692/6700 [47:18<2:14:52,  1.62s/it]11/16/2022 23:41:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:27 - INFO - train.train_snli_ve - loss is tensor(0.5759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1693/6700 [47:20<2:18:01,  1.65s/it]11/16/2022 23:41:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.3685e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:29 - INFO - train.train_snli_ve - loss is tensor(0.4772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1694/6700 [47:21<2:16:12,  1.63s/it]11/16/2022 23:41:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:30 - INFO - train.train_snli_ve - loss is tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1695/6700 [47:23<2:16:08,  1.63s/it]11/16/2022 23:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.8015e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:32 - INFO - train.train_snli_ve - loss is tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1696/6700 [47:25<2:15:08,  1.62s/it]11/16/2022 23:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.7876e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:34 - INFO - train.train_snli_ve - loss is tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1697/6700 [47:26<2:16:11,  1.63s/it]11/16/2022 23:41:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:35 - INFO - train.train_snli_ve - loss is tensor(0.9698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1698/6700 [47:28<2:14:17,  1.61s/it]11/16/2022 23:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4103e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:37 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1699/6700 [47:29<2:15:32,  1.63s/it]11/16/2022 23:41:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:38 - INFO - train.train_snli_ve - loss is tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1700/6700 [47:31<2:14:58,  1.62s/it]11/16/2022 23:41:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:40 - INFO - train.train_snli_ve - loss is tensor(0.9880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1701/6700 [47:33<2:15:45,  1.63s/it]11/16/2022 23:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.0868e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:42 - INFO - train.train_snli_ve - loss is tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1702/6700 [47:34<2:13:53,  1.61s/it]11/16/2022 23:41:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:43 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1703/6700 [47:36<2:14:40,  1.62s/it]11/16/2022 23:41:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.6667e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:45 - INFO - train.train_snli_ve - loss is tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1704/6700 [47:37<2:12:48,  1.59s/it]11/16/2022 23:41:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:46 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1705/6700 [47:39<2:14:45,  1.62s/it]11/16/2022 23:41:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.0035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:48 - INFO - train.train_snli_ve - loss is tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1706/6700 [47:41<2:16:00,  1.63s/it]11/16/2022 23:41:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.6366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:50 - INFO - train.train_snli_ve - loss is tensor(0.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1707/6700 [47:42<2:15:18,  1.63s/it]11/16/2022 23:41:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.8869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:51 - INFO - train.train_snli_ve - loss is tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  25%|##5       | 1708/6700 [47:44<2:14:50,  1.62s/it]11/16/2022 23:41:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.7822e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:53 - INFO - train.train_snli_ve - loss is tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1709/6700 [47:46<2:14:46,  1.62s/it]11/16/2022 23:41:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.6798e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:55 - INFO - train.train_snli_ve - loss is tensor(0.9692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1710/6700 [47:47<2:15:26,  1.63s/it]11/16/2022 23:41:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.3670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:56 - INFO - train.train_snli_ve - loss is tensor(0.4995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1711/6700 [47:49<2:14:59,  1.62s/it]11/16/2022 23:41:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.7240e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:58 - INFO - train.train_snli_ve - loss is tensor(0.6504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1712/6700 [47:51<2:16:00,  1.64s/it]11/16/2022 23:41:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.7455e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:41:59 - INFO - train.train_snli_ve - loss is tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1713/6700 [47:52<2:16:19,  1.64s/it]11/16/2022 23:42:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.5217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:01 - INFO - train.train_snli_ve - loss is tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1714/6700 [47:54<2:16:23,  1.64s/it]11/16/2022 23:42:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.6009e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:03 - INFO - train.train_snli_ve - loss is tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1715/6700 [47:55<2:15:03,  1.63s/it]11/16/2022 23:42:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:04 - INFO - train.train_snli_ve - loss is tensor(0.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1716/6700 [47:57<2:14:29,  1.62s/it]11/16/2022 23:42:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.0989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:06 - INFO - train.train_snli_ve - loss is tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1717/6700 [47:59<2:17:20,  1.65s/it]11/16/2022 23:42:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:08 - INFO - train.train_snli_ve - loss is tensor(0.8624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1718/6700 [48:00<2:15:47,  1.64s/it]11/16/2022 23:42:09 - INFO - train.train_snli_ve - kd_loss is tensor(7.3263e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:09 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1719/6700 [48:02<2:15:32,  1.63s/it]11/16/2022 23:42:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1532e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:11 - INFO - train.train_snli_ve - loss is tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1720/6700 [48:04<2:15:07,  1.63s/it]11/16/2022 23:42:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.0450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:13 - INFO - train.train_snli_ve - loss is tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1721/6700 [48:05<2:16:03,  1.64s/it]11/16/2022 23:42:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.7375e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:14 - INFO - train.train_snli_ve - loss is tensor(0.7800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1722/6700 [48:07<2:15:15,  1.63s/it]11/16/2022 23:42:16 - INFO - train.train_snli_ve - kd_loss is tensor(7.4113e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:16 - INFO - train.train_snli_ve - loss is tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1723/6700 [48:08<2:14:54,  1.63s/it]11/16/2022 23:42:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.5419e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:17 - INFO - train.train_snli_ve - loss is tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1724/6700 [48:10<2:14:02,  1.62s/it]11/16/2022 23:42:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.6361e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:19 - INFO - train.train_snli_ve - loss is tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1725/6700 [48:12<2:13:37,  1.61s/it]11/16/2022 23:42:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:21 - INFO - train.train_snli_ve - loss is tensor(0.8472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1726/6700 [48:13<2:12:45,  1.60s/it]11/16/2022 23:42:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.0592e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:22 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1727/6700 [48:15<2:13:23,  1.61s/it]11/16/2022 23:42:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.3728e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:24 - INFO - train.train_snli_ve - loss is tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1728/6700 [48:16<2:12:52,  1.60s/it]11/16/2022 23:42:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.6362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:25 - INFO - train.train_snli_ve - loss is tensor(0.7473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1729/6700 [48:18<2:13:09,  1.61s/it]11/16/2022 23:42:27 - INFO - train.train_snli_ve - kd_loss is tensor(7.1333e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:27 - INFO - train.train_snli_ve - loss is tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1730/6700 [48:20<2:13:09,  1.61s/it]11/16/2022 23:42:29 - INFO - train.train_snli_ve - kd_loss is tensor(8.8107e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:29 - INFO - train.train_snli_ve - loss is tensor(0.9105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1731/6700 [48:21<2:14:15,  1.62s/it]11/16/2022 23:42:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.8350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:30 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1732/6700 [48:23<2:13:34,  1.61s/it]11/16/2022 23:42:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.9548e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:32 - INFO - train.train_snli_ve - loss is tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1733/6700 [48:25<2:13:05,  1.61s/it]11/16/2022 23:42:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1064e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:33 - INFO - train.train_snli_ve - loss is tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1734/6700 [48:26<2:13:16,  1.61s/it]11/16/2022 23:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.4087e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:35 - INFO - train.train_snli_ve - loss is tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1735/6700 [48:28<2:13:13,  1.61s/it]11/16/2022 23:42:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.3108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:37 - INFO - train.train_snli_ve - loss is tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1736/6700 [48:29<2:12:59,  1.61s/it]11/16/2022 23:42:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.4708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:38 - INFO - train.train_snli_ve - loss is tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1737/6700 [48:31<2:14:02,  1.62s/it]11/16/2022 23:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.4693e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:40 - INFO - train.train_snli_ve - loss is tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1738/6700 [48:33<2:13:57,  1.62s/it]11/16/2022 23:42:42 - INFO - train.train_snli_ve - kd_loss is tensor(5.1765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:42 - INFO - train.train_snli_ve - loss is tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1739/6700 [48:34<2:13:24,  1.61s/it]11/16/2022 23:42:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.5409e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:43 - INFO - train.train_snli_ve - loss is tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1740/6700 [48:36<2:13:00,  1.61s/it]11/16/2022 23:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(4.5051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:45 - INFO - train.train_snli_ve - loss is tensor(0.7939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##5       | 1741/6700 [48:37<2:13:35,  1.62s/it]11/16/2022 23:42:46 - INFO - train.train_snli_ve - kd_loss is tensor(6.6734e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:46 - INFO - train.train_snli_ve - loss is tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1742/6700 [48:39<2:15:26,  1.64s/it]11/16/2022 23:42:48 - INFO - train.train_snli_ve - kd_loss is tensor(5.6723e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:48 - INFO - train.train_snli_ve - loss is tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1743/6700 [48:41<2:14:49,  1.63s/it]11/16/2022 23:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.3598e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:50 - INFO - train.train_snli_ve - loss is tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1744/6700 [48:42<2:13:52,  1.62s/it]11/16/2022 23:42:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.2948e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:51 - INFO - train.train_snli_ve - loss is tensor(0.7876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1745/6700 [48:44<2:14:15,  1.63s/it]11/16/2022 23:42:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.7392e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:53 - INFO - train.train_snli_ve - loss is tensor(0.8257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1746/6700 [48:46<2:14:24,  1.63s/it]11/16/2022 23:42:55 - INFO - train.train_snli_ve - kd_loss is tensor(6.2545e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:55 - INFO - train.train_snli_ve - loss is tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1747/6700 [48:47<2:13:26,  1.62s/it]11/16/2022 23:42:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.8208e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:56 - INFO - train.train_snli_ve - loss is tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1748/6700 [48:49<2:13:34,  1.62s/it]11/16/2022 23:42:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.3664e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:58 - INFO - train.train_snli_ve - loss is tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1749/6700 [48:50<2:13:27,  1.62s/it]11/16/2022 23:42:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.1369e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:42:59 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1750/6700 [48:52<2:13:34,  1.62s/it]11/16/2022 23:43:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.2850e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:01 - INFO - train.train_snli_ve - loss is tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1751/6700 [48:54<2:13:43,  1.62s/it]11/16/2022 23:43:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.6140e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:03 - INFO - train.train_snli_ve - loss is tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1752/6700 [48:55<2:13:05,  1.61s/it]11/16/2022 23:43:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.1507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:04 - INFO - train.train_snli_ve - loss is tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1753/6700 [48:57<2:13:50,  1.62s/it]11/16/2022 23:43:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.2108e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:06 - INFO - train.train_snli_ve - loss is tensor(0.8543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1754/6700 [48:59<2:13:42,  1.62s/it]11/16/2022 23:43:07 - INFO - train.train_snli_ve - kd_loss is tensor(5.9271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:07 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1755/6700 [49:00<2:12:59,  1.61s/it]11/16/2022 23:43:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.9696e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:09 - INFO - train.train_snli_ve - loss is tensor(0.9058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1756/6700 [49:02<2:12:54,  1.61s/it]11/16/2022 23:43:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:11 - INFO - train.train_snli_ve - loss is tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1757/6700 [49:03<2:14:04,  1.63s/it]11/16/2022 23:43:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.8169e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:12 - INFO - train.train_snli_ve - loss is tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1758/6700 [49:05<2:13:20,  1.62s/it]11/16/2022 23:43:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.6566e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:14 - INFO - train.train_snli_ve - loss is tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1759/6700 [49:07<2:12:52,  1.61s/it]11/16/2022 23:43:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:16 - INFO - train.train_snli_ve - loss is tensor(0.7545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1760/6700 [49:08<2:12:07,  1.60s/it]11/16/2022 23:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1204e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:17 - INFO - train.train_snli_ve - loss is tensor(0.8549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1761/6700 [49:10<2:11:57,  1.60s/it]11/16/2022 23:43:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.6366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:19 - INFO - train.train_snli_ve - loss is tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1762/6700 [49:11<2:12:45,  1.61s/it]11/16/2022 23:43:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.5863e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:20 - INFO - train.train_snli_ve - loss is tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1763/6700 [49:13<2:13:07,  1.62s/it]11/16/2022 23:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1507e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:22 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1764/6700 [49:15<2:13:44,  1.63s/it]11/16/2022 23:43:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.7902e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:24 - INFO - train.train_snli_ve - loss is tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1765/6700 [49:16<2:13:09,  1.62s/it]11/16/2022 23:43:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:25 - INFO - train.train_snli_ve - loss is tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1766/6700 [49:18<2:12:34,  1.61s/it]11/16/2022 23:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1778e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:27 - INFO - train.train_snli_ve - loss is tensor(0.9353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1767/6700 [49:20<2:12:09,  1.61s/it]11/16/2022 23:43:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:28 - INFO - train.train_snli_ve - loss is tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1768/6700 [49:21<2:12:30,  1.61s/it]11/16/2022 23:43:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.1853e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:30 - INFO - train.train_snli_ve - loss is tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1769/6700 [49:23<2:11:17,  1.60s/it]11/16/2022 23:43:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.1039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:32 - INFO - train.train_snli_ve - loss is tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1770/6700 [49:24<2:12:02,  1.61s/it]11/16/2022 23:43:33 - INFO - train.train_snli_ve - kd_loss is tensor(6.8795e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:33 - INFO - train.train_snli_ve - loss is tensor(0.9902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1771/6700 [49:26<2:12:16,  1.61s/it]11/16/2022 23:43:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:35 - INFO - train.train_snli_ve - loss is tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1772/6700 [49:28<2:11:58,  1.61s/it]11/16/2022 23:43:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.2924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:36 - INFO - train.train_snli_ve - loss is tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1773/6700 [49:29<2:11:56,  1.61s/it]11/16/2022 23:43:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:38 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1774/6700 [49:31<2:12:26,  1.61s/it]11/16/2022 23:43:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:40 - INFO - train.train_snli_ve - loss is tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  26%|##6       | 1775/6700 [49:32<2:12:19,  1.61s/it]11/16/2022 23:43:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.6096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:41 - INFO - train.train_snli_ve - loss is tensor(0.9287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1776/6700 [49:34<2:11:58,  1.61s/it]11/16/2022 23:43:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.2431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:43 - INFO - train.train_snli_ve - loss is tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1777/6700 [49:36<2:12:14,  1.61s/it]11/16/2022 23:43:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:45 - INFO - train.train_snli_ve - loss is tensor(0.8537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1778/6700 [49:37<2:11:57,  1.61s/it]11/16/2022 23:43:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.5347e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:46 - INFO - train.train_snli_ve - loss is tensor(0.5191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1779/6700 [49:39<2:12:28,  1.62s/it]11/16/2022 23:43:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.5543e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:48 - INFO - train.train_snli_ve - loss is tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1780/6700 [49:40<2:12:52,  1.62s/it]11/16/2022 23:43:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.4852e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:49 - INFO - train.train_snli_ve - loss is tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1781/6700 [49:42<2:13:38,  1.63s/it]11/16/2022 23:43:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.9450e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:51 - INFO - train.train_snli_ve - loss is tensor(0.9100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1782/6700 [49:44<2:13:26,  1.63s/it]11/16/2022 23:43:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.8989e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:53 - INFO - train.train_snli_ve - loss is tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1783/6700 [49:45<2:13:57,  1.63s/it]11/16/2022 23:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2687e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:54 - INFO - train.train_snli_ve - loss is tensor(0.8405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1784/6700 [49:47<2:14:16,  1.64s/it]11/16/2022 23:43:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.1254e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:56 - INFO - train.train_snli_ve - loss is tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1785/6700 [49:49<2:14:22,  1.64s/it]11/16/2022 23:43:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.3328e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:58 - INFO - train.train_snli_ve - loss is tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1786/6700 [49:50<2:14:12,  1.64s/it]11/16/2022 23:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.8344e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:43:59 - INFO - train.train_snli_ve - loss is tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1787/6700 [49:52<2:13:45,  1.63s/it]11/16/2022 23:44:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.3730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:01 - INFO - train.train_snli_ve - loss is tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1788/6700 [49:54<2:12:43,  1.62s/it]11/16/2022 23:44:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.7816e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:02 - INFO - train.train_snli_ve - loss is tensor(0.8266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1789/6700 [49:55<2:12:29,  1.62s/it]11/16/2022 23:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:04 - INFO - train.train_snli_ve - loss is tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1790/6700 [49:57<2:12:09,  1.61s/it]11/16/2022 23:44:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.6656e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:06 - INFO - train.train_snli_ve - loss is tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1791/6700 [49:58<2:12:15,  1.62s/it]11/16/2022 23:44:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.5837e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:07 - INFO - train.train_snli_ve - loss is tensor(1.0459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1792/6700 [50:00<2:12:17,  1.62s/it]11/16/2022 23:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.2428e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:09 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1793/6700 [50:02<2:12:45,  1.62s/it]11/16/2022 23:44:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.7335e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:11 - INFO - train.train_snli_ve - loss is tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1794/6700 [50:03<2:12:20,  1.62s/it]11/16/2022 23:44:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.9088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:12 - INFO - train.train_snli_ve - loss is tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1795/6700 [50:05<2:12:44,  1.62s/it]11/16/2022 23:44:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.9589e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:14 - INFO - train.train_snli_ve - loss is tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1796/6700 [50:06<2:12:24,  1.62s/it]11/16/2022 23:44:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.7325e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:15 - INFO - train.train_snli_ve - loss is tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1797/6700 [50:08<2:12:01,  1.62s/it]11/16/2022 23:44:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.2398e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:17 - INFO - train.train_snli_ve - loss is tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1798/6700 [50:10<2:12:02,  1.62s/it]11/16/2022 23:44:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.2648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:19 - INFO - train.train_snli_ve - loss is tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1799/6700 [50:11<2:11:20,  1.61s/it]11/16/2022 23:44:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:20 - INFO - train.train_snli_ve - loss is tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1800/6700 [50:13<2:12:29,  1.62s/it]11/16/2022 23:44:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.8118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:22 - INFO - train.train_snli_ve - loss is tensor(0.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1801/6700 [50:15<2:11:50,  1.61s/it]11/16/2022 23:44:24 - INFO - train.train_snli_ve - kd_loss is tensor(8.0932e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:24 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1802/6700 [50:16<2:12:50,  1.63s/it]11/16/2022 23:44:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:25 - INFO - train.train_snli_ve - loss is tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1803/6700 [50:18<2:12:53,  1.63s/it]11/16/2022 23:44:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.3331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:27 - INFO - train.train_snli_ve - loss is tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1804/6700 [50:19<2:12:59,  1.63s/it]11/16/2022 23:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:28 - INFO - train.train_snli_ve - loss is tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1805/6700 [50:21<2:13:18,  1.63s/it]11/16/2022 23:44:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0295e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:30 - INFO - train.train_snli_ve - loss is tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1806/6700 [50:23<2:11:57,  1.62s/it]11/16/2022 23:44:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:32 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1807/6700 [50:24<2:12:34,  1.63s/it]11/16/2022 23:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.0078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:33 - INFO - train.train_snli_ve - loss is tensor(0.5874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##6       | 1808/6700 [50:26<2:12:56,  1.63s/it]11/16/2022 23:44:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:35 - INFO - train.train_snli_ve - loss is tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1809/6700 [50:28<2:11:57,  1.62s/it]11/16/2022 23:44:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:36 - INFO - train.train_snli_ve - loss is tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1810/6700 [50:29<2:11:29,  1.61s/it]11/16/2022 23:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.8794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:38 - INFO - train.train_snli_ve - loss is tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1811/6700 [50:31<2:12:22,  1.62s/it]11/16/2022 23:44:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.3095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:40 - INFO - train.train_snli_ve - loss is tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1812/6700 [50:32<2:11:34,  1.62s/it]11/16/2022 23:44:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:41 - INFO - train.train_snli_ve - loss is tensor(1.0997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1813/6700 [50:34<2:12:34,  1.63s/it]11/16/2022 23:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.8282e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:43 - INFO - train.train_snli_ve - loss is tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1814/6700 [50:36<2:11:34,  1.62s/it]11/16/2022 23:44:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:45 - INFO - train.train_snli_ve - loss is tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1815/6700 [50:37<2:11:30,  1.62s/it]11/16/2022 23:44:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:46 - INFO - train.train_snli_ve - loss is tensor(0.4456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1816/6700 [50:39<2:11:37,  1.62s/it]11/16/2022 23:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:48 - INFO - train.train_snli_ve - loss is tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1817/6700 [50:41<2:11:21,  1.61s/it]11/16/2022 23:44:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.7581e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:49 - INFO - train.train_snli_ve - loss is tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1818/6700 [50:42<2:11:03,  1.61s/it]11/16/2022 23:44:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.1536e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:51 - INFO - train.train_snli_ve - loss is tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1819/6700 [50:44<2:11:27,  1.62s/it]11/16/2022 23:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:53 - INFO - train.train_snli_ve - loss is tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1820/6700 [50:45<2:10:22,  1.60s/it]11/16/2022 23:44:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:54 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1821/6700 [50:47<2:10:12,  1.60s/it]11/16/2022 23:44:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.1142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:56 - INFO - train.train_snli_ve - loss is tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1822/6700 [50:49<2:11:43,  1.62s/it]11/16/2022 23:44:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:58 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1823/6700 [50:50<2:11:40,  1.62s/it]11/16/2022 23:44:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.7748e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:44:59 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1824/6700 [50:52<2:11:32,  1.62s/it]11/16/2022 23:45:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4511e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:01 - INFO - train.train_snli_ve - loss is tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1825/6700 [50:53<2:11:49,  1.62s/it]11/16/2022 23:45:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.5221e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:02 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1826/6700 [50:55<2:12:27,  1.63s/it]11/16/2022 23:45:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.4763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:04 - INFO - train.train_snli_ve - loss is tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1827/6700 [50:57<2:12:55,  1.64s/it]11/16/2022 23:45:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.4339e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:06 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1828/6700 [50:58<2:13:37,  1.65s/it]11/16/2022 23:45:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.1148e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:07 - INFO - train.train_snli_ve - loss is tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1829/6700 [51:00<2:13:09,  1.64s/it]11/16/2022 23:45:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.8088e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:09 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1830/6700 [51:02<2:12:53,  1.64s/it]11/16/2022 23:45:11 - INFO - train.train_snli_ve - kd_loss is tensor(6.9205e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:11 - INFO - train.train_snli_ve - loss is tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1831/6700 [51:03<2:12:32,  1.63s/it]11/16/2022 23:45:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.2315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:12 - INFO - train.train_snli_ve - loss is tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1832/6700 [51:05<2:12:06,  1.63s/it]11/16/2022 23:45:14 - INFO - train.train_snli_ve - kd_loss is tensor(7.6219e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:14 - INFO - train.train_snli_ve - loss is tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1833/6700 [51:07<2:11:39,  1.62s/it]11/16/2022 23:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:15 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1834/6700 [51:08<2:11:32,  1.62s/it]11/16/2022 23:45:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:17 - INFO - train.train_snli_ve - loss is tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1835/6700 [51:10<2:10:55,  1.61s/it]11/16/2022 23:45:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.4217e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:19 - INFO - train.train_snli_ve - loss is tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1836/6700 [51:11<2:11:21,  1.62s/it]11/16/2022 23:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(5.9937e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:20 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1837/6700 [51:13<2:10:59,  1.62s/it]11/16/2022 23:45:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2307e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:22 - INFO - train.train_snli_ve - loss is tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1838/6700 [51:15<2:11:27,  1.62s/it]11/16/2022 23:45:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.0285e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:24 - INFO - train.train_snli_ve - loss is tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1839/6700 [51:16<2:10:39,  1.61s/it]11/16/2022 23:45:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.2406e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:25 - INFO - train.train_snli_ve - loss is tensor(0.8407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1840/6700 [51:18<2:10:16,  1.61s/it]11/16/2022 23:45:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.3078e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:27 - INFO - train.train_snli_ve - loss is tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1841/6700 [51:19<2:10:35,  1.61s/it]11/16/2022 23:45:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.1416e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:28 - INFO - train.train_snli_ve - loss is tensor(0.9319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  27%|##7       | 1842/6700 [51:21<2:10:20,  1.61s/it]11/16/2022 23:45:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.2308e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:30 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1843/6700 [51:23<2:10:57,  1.62s/it]11/16/2022 23:45:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3362e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:32 - INFO - train.train_snli_ve - loss is tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1844/6700 [51:24<2:10:28,  1.61s/it]11/16/2022 23:45:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.3911e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:33 - INFO - train.train_snli_ve - loss is tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1845/6700 [51:26<2:09:41,  1.60s/it]11/16/2022 23:45:35 - INFO - train.train_snli_ve - kd_loss is tensor(8.6350e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:35 - INFO - train.train_snli_ve - loss is tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1846/6700 [51:27<2:10:39,  1.62s/it]11/16/2022 23:45:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.4386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:36 - INFO - train.train_snli_ve - loss is tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1847/6700 [51:29<2:10:30,  1.61s/it]11/16/2022 23:45:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.3455e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:38 - INFO - train.train_snli_ve - loss is tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1848/6700 [51:31<2:12:34,  1.64s/it]11/16/2022 23:45:40 - INFO - train.train_snli_ve - kd_loss is tensor(8.7887e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:40 - INFO - train.train_snli_ve - loss is tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1849/6700 [51:32<2:12:36,  1.64s/it]11/16/2022 23:45:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.0401e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:41 - INFO - train.train_snli_ve - loss is tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1850/6700 [51:34<2:12:09,  1.63s/it]11/16/2022 23:45:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:43 - INFO - train.train_snli_ve - loss is tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1851/6700 [51:36<2:11:29,  1.63s/it]11/16/2022 23:45:45 - INFO - train.train_snli_ve - kd_loss is tensor(7.1439e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:45 - INFO - train.train_snli_ve - loss is tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1852/6700 [51:37<2:11:55,  1.63s/it]11/16/2022 23:45:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.9536e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:46 - INFO - train.train_snli_ve - loss is tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1853/6700 [51:39<2:12:12,  1.64s/it]11/16/2022 23:45:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.6764e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:48 - INFO - train.train_snli_ve - loss is tensor(0.8389, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1854/6700 [51:41<2:10:52,  1.62s/it]11/16/2022 23:45:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.2593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:50 - INFO - train.train_snli_ve - loss is tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1855/6700 [51:42<2:12:01,  1.64s/it]11/16/2022 23:45:51 - INFO - train.train_snli_ve - kd_loss is tensor(6.9683e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:51 - INFO - train.train_snli_ve - loss is tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1856/6700 [51:44<2:11:19,  1.63s/it]11/16/2022 23:45:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.0893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:53 - INFO - train.train_snli_ve - loss is tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1857/6700 [51:45<2:11:55,  1.63s/it]11/16/2022 23:45:54 - INFO - train.train_snli_ve - kd_loss is tensor(7.5621e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:54 - INFO - train.train_snli_ve - loss is tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1858/6700 [51:47<2:12:18,  1.64s/it]11/16/2022 23:45:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.7695e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:56 - INFO - train.train_snli_ve - loss is tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1859/6700 [51:49<2:12:14,  1.64s/it]11/16/2022 23:45:58 - INFO - train.train_snli_ve - kd_loss is tensor(6.1678e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:58 - INFO - train.train_snli_ve - loss is tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1860/6700 [51:50<2:11:28,  1.63s/it]11/16/2022 23:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.5468e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:45:59 - INFO - train.train_snli_ve - loss is tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1861/6700 [51:52<2:10:47,  1.62s/it]11/16/2022 23:46:01 - INFO - train.train_snli_ve - kd_loss is tensor(6.9810e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:01 - INFO - train.train_snli_ve - loss is tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1862/6700 [51:54<2:10:12,  1.61s/it]11/16/2022 23:46:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.4809e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:03 - INFO - train.train_snli_ve - loss is tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1863/6700 [51:55<2:10:35,  1.62s/it]11/16/2022 23:46:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:04 - INFO - train.train_snli_ve - loss is tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1864/6700 [51:57<2:11:11,  1.63s/it]11/16/2022 23:46:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0984e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:06 - INFO - train.train_snli_ve - loss is tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1865/6700 [51:59<2:11:37,  1.63s/it]11/16/2022 23:46:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.6640e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:07 - INFO - train.train_snli_ve - loss is tensor(1.0141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1866/6700 [52:00<2:11:02,  1.63s/it]11/16/2022 23:46:09 - INFO - train.train_snli_ve - kd_loss is tensor(8.4046e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:09 - INFO - train.train_snli_ve - loss is tensor(0.8210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1867/6700 [52:02<2:11:11,  1.63s/it]11/16/2022 23:46:11 - INFO - train.train_snli_ve - kd_loss is tensor(7.4579e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:11 - INFO - train.train_snli_ve - loss is tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1868/6700 [52:03<2:10:33,  1.62s/it]11/16/2022 23:46:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:12 - INFO - train.train_snli_ve - loss is tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1869/6700 [52:05<2:11:17,  1.63s/it]11/16/2022 23:46:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.8232e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:14 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1870/6700 [52:07<2:10:47,  1.62s/it]11/16/2022 23:46:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:16 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1871/6700 [52:08<2:10:04,  1.62s/it]11/16/2022 23:46:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1084e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:17 - INFO - train.train_snli_ve - loss is tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1872/6700 [52:10<2:09:59,  1.62s/it]11/16/2022 23:46:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.6702e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:19 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1873/6700 [52:11<2:09:19,  1.61s/it]11/16/2022 23:46:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.1516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:20 - INFO - train.train_snli_ve - loss is tensor(1.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1874/6700 [52:13<2:09:14,  1.61s/it]11/16/2022 23:46:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.9220e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:22 - INFO - train.train_snli_ve - loss is tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##7       | 1875/6700 [52:15<2:08:43,  1.60s/it]11/16/2022 23:46:24 - INFO - train.train_snli_ve - kd_loss is tensor(6.6784e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:24 - INFO - train.train_snli_ve - loss is tensor(0.8542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1876/6700 [52:16<2:08:51,  1.60s/it]11/16/2022 23:46:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:25 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1877/6700 [52:18<2:08:52,  1.60s/it]11/16/2022 23:46:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.7730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:27 - INFO - train.train_snli_ve - loss is tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1878/6700 [52:19<2:09:39,  1.61s/it]11/16/2022 23:46:28 - INFO - train.train_snli_ve - kd_loss is tensor(5.5492e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:28 - INFO - train.train_snli_ve - loss is tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1879/6700 [52:21<2:10:46,  1.63s/it]11/16/2022 23:46:30 - INFO - train.train_snli_ve - kd_loss is tensor(5.5440e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:30 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1880/6700 [52:23<2:10:28,  1.62s/it]11/16/2022 23:46:32 - INFO - train.train_snli_ve - kd_loss is tensor(7.1597e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:32 - INFO - train.train_snli_ve - loss is tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1881/6700 [52:24<2:10:42,  1.63s/it]11/16/2022 23:46:33 - INFO - train.train_snli_ve - kd_loss is tensor(8.3170e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:33 - INFO - train.train_snli_ve - loss is tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1882/6700 [52:26<2:11:04,  1.63s/it]11/16/2022 23:46:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.2411e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:35 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1883/6700 [52:28<2:10:16,  1.62s/it]11/16/2022 23:46:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.3305e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:36 - INFO - train.train_snli_ve - loss is tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1884/6700 [52:29<2:08:57,  1.61s/it]11/16/2022 23:46:38 - INFO - train.train_snli_ve - kd_loss is tensor(8.6565e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:38 - INFO - train.train_snli_ve - loss is tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1885/6700 [52:31<2:09:32,  1.61s/it]11/16/2022 23:46:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:40 - INFO - train.train_snli_ve - loss is tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1886/6700 [52:32<2:09:14,  1.61s/it]11/16/2022 23:46:41 - INFO - train.train_snli_ve - kd_loss is tensor(6.4898e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:41 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1887/6700 [52:34<2:09:43,  1.62s/it]11/16/2022 23:46:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.7227e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:43 - INFO - train.train_snli_ve - loss is tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1888/6700 [52:36<2:10:38,  1.63s/it]11/16/2022 23:46:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.1158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:45 - INFO - train.train_snli_ve - loss is tensor(0.9196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1889/6700 [52:37<2:10:55,  1.63s/it]11/16/2022 23:46:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:46 - INFO - train.train_snli_ve - loss is tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1890/6700 [52:39<2:11:05,  1.64s/it]11/16/2022 23:46:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.4945e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:48 - INFO - train.train_snli_ve - loss is tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1891/6700 [52:41<2:12:02,  1.65s/it]11/16/2022 23:46:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.7215e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:50 - INFO - train.train_snli_ve - loss is tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1892/6700 [52:42<2:10:58,  1.63s/it]11/16/2022 23:46:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.2921e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:51 - INFO - train.train_snli_ve - loss is tensor(0.8262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1893/6700 [52:44<2:10:48,  1.63s/it]11/16/2022 23:46:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:53 - INFO - train.train_snli_ve - loss is tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1894/6700 [52:46<2:12:01,  1.65s/it]11/16/2022 23:46:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.4658e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:55 - INFO - train.train_snli_ve - loss is tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1895/6700 [52:47<2:11:14,  1.64s/it]11/16/2022 23:46:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:56 - INFO - train.train_snli_ve - loss is tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1896/6700 [52:49<2:10:23,  1.63s/it]11/16/2022 23:46:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:58 - INFO - train.train_snli_ve - loss is tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1897/6700 [52:50<2:09:56,  1.62s/it]11/16/2022 23:46:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.3207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:46:59 - INFO - train.train_snli_ve - loss is tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1898/6700 [52:52<2:09:19,  1.62s/it]11/16/2022 23:47:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.8035e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:01 - INFO - train.train_snli_ve - loss is tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1899/6700 [52:54<2:08:45,  1.61s/it]11/16/2022 23:47:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.2878e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:03 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1900/6700 [52:55<2:09:33,  1.62s/it]11/16/2022 23:47:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:04 - INFO - train.train_snli_ve - loss is tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1901/6700 [52:57<2:08:47,  1.61s/it]11/16/2022 23:47:06 - INFO - train.train_snli_ve - kd_loss is tensor(9.3650e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:06 - INFO - train.train_snli_ve - loss is tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1902/6700 [52:58<2:08:33,  1.61s/it]11/16/2022 23:47:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:07 - INFO - train.train_snli_ve - loss is tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1903/6700 [53:00<2:09:13,  1.62s/it]11/16/2022 23:47:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:09 - INFO - train.train_snli_ve - loss is tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1904/6700 [53:02<2:09:09,  1.62s/it]11/16/2022 23:47:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.2541e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:11 - INFO - train.train_snli_ve - loss is tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1905/6700 [53:03<2:08:22,  1.61s/it]11/16/2022 23:47:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:12 - INFO - train.train_snli_ve - loss is tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1906/6700 [53:05<2:09:00,  1.61s/it]11/16/2022 23:47:14 - INFO - train.train_snli_ve - kd_loss is tensor(5.8105e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:14 - INFO - train.train_snli_ve - loss is tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1907/6700 [53:07<2:09:23,  1.62s/it]11/16/2022 23:47:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:15 - INFO - train.train_snli_ve - loss is tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1908/6700 [53:08<2:09:38,  1.62s/it]11/16/2022 23:47:17 - INFO - train.train_snli_ve - kd_loss is tensor(7.3733e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:17 - INFO - train.train_snli_ve - loss is tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  28%|##8       | 1909/6700 [53:10<2:09:15,  1.62s/it]11/16/2022 23:47:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.9947e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:19 - INFO - train.train_snli_ve - loss is tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1910/6700 [53:11<2:08:46,  1.61s/it]11/16/2022 23:47:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.7480e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:20 - INFO - train.train_snli_ve - loss is tensor(0.7897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1911/6700 [53:13<2:09:39,  1.62s/it]11/16/2022 23:47:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:22 - INFO - train.train_snli_ve - loss is tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1912/6700 [53:15<2:08:47,  1.61s/it]11/16/2022 23:47:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2323e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:24 - INFO - train.train_snli_ve - loss is tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1913/6700 [53:16<2:08:11,  1.61s/it]11/16/2022 23:47:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:25 - INFO - train.train_snli_ve - loss is tensor(0.8474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1914/6700 [53:18<2:07:56,  1.60s/it]11/16/2022 23:47:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:27 - INFO - train.train_snli_ve - loss is tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1915/6700 [53:19<2:07:57,  1.60s/it]11/16/2022 23:47:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.1789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:28 - INFO - train.train_snli_ve - loss is tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1916/6700 [53:21<2:07:42,  1.60s/it]11/16/2022 23:47:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2497e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:30 - INFO - train.train_snli_ve - loss is tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1917/6700 [53:23<2:08:08,  1.61s/it]11/16/2022 23:47:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.0022e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:32 - INFO - train.train_snli_ve - loss is tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1918/6700 [53:24<2:07:13,  1.60s/it]11/16/2022 23:47:33 - INFO - train.train_snli_ve - kd_loss is tensor(5.8860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:33 - INFO - train.train_snli_ve - loss is tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1919/6700 [53:26<2:07:11,  1.60s/it]11/16/2022 23:47:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.4708e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:35 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1920/6700 [53:27<2:08:03,  1.61s/it]11/16/2022 23:47:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1543e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:36 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1921/6700 [53:29<2:08:37,  1.61s/it]11/16/2022 23:47:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.5210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:38 - INFO - train.train_snli_ve - loss is tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1922/6700 [53:31<2:07:51,  1.61s/it]11/16/2022 23:47:40 - INFO - train.train_snli_ve - kd_loss is tensor(9.1924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:40 - INFO - train.train_snli_ve - loss is tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1923/6700 [53:32<2:07:58,  1.61s/it]11/16/2022 23:47:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.0081e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:41 - INFO - train.train_snli_ve - loss is tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1924/6700 [53:34<2:08:15,  1.61s/it]11/16/2022 23:47:43 - INFO - train.train_snli_ve - kd_loss is tensor(6.1389e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:43 - INFO - train.train_snli_ve - loss is tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1925/6700 [53:35<2:08:25,  1.61s/it]11/16/2022 23:47:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.5430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:44 - INFO - train.train_snli_ve - loss is tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1926/6700 [53:37<2:08:10,  1.61s/it]11/16/2022 23:47:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.0779e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:46 - INFO - train.train_snli_ve - loss is tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1927/6700 [53:39<2:08:31,  1.62s/it]11/16/2022 23:47:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:48 - INFO - train.train_snli_ve - loss is tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1928/6700 [53:40<2:08:39,  1.62s/it]11/16/2022 23:47:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:49 - INFO - train.train_snli_ve - loss is tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1929/6700 [53:42<2:08:58,  1.62s/it]11/16/2022 23:47:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3662e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:51 - INFO - train.train_snli_ve - loss is tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1930/6700 [53:44<2:09:27,  1.63s/it]11/16/2022 23:47:53 - INFO - train.train_snli_ve - kd_loss is tensor(7.6019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:53 - INFO - train.train_snli_ve - loss is tensor(0.9233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1931/6700 [53:45<2:08:22,  1.62s/it]11/16/2022 23:47:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.1564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:54 - INFO - train.train_snli_ve - loss is tensor(1.0127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1932/6700 [53:47<2:08:47,  1.62s/it]11/16/2022 23:47:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.2026e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:56 - INFO - train.train_snli_ve - loss is tensor(0.8086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1933/6700 [53:48<2:09:21,  1.63s/it]11/16/2022 23:47:57 - INFO - train.train_snli_ve - kd_loss is tensor(9.8373e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:57 - INFO - train.train_snli_ve - loss is tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1934/6700 [53:50<2:08:59,  1.62s/it]11/16/2022 23:47:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.3981e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:47:59 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1935/6700 [53:52<2:09:45,  1.63s/it]11/16/2022 23:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.0072e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:01 - INFO - train.train_snli_ve - loss is tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1936/6700 [53:53<2:09:33,  1.63s/it]11/16/2022 23:48:02 - INFO - train.train_snli_ve - kd_loss is tensor(5.5577e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:02 - INFO - train.train_snli_ve - loss is tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1937/6700 [53:55<2:09:03,  1.63s/it]11/16/2022 23:48:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.8617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:04 - INFO - train.train_snli_ve - loss is tensor(0.9420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1938/6700 [53:57<2:09:39,  1.63s/it]11/16/2022 23:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(5.0999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:06 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1939/6700 [53:58<2:08:25,  1.62s/it]11/16/2022 23:48:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.5787e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:07 - INFO - train.train_snli_ve - loss is tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1940/6700 [54:00<2:08:17,  1.62s/it]11/16/2022 23:48:09 - INFO - train.train_snli_ve - kd_loss is tensor(6.8209e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:09 - INFO - train.train_snli_ve - loss is tensor(0.7931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1941/6700 [54:01<2:08:14,  1.62s/it]11/16/2022 23:48:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.2884e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:10 - INFO - train.train_snli_ve - loss is tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##8       | 1942/6700 [54:03<2:08:00,  1.61s/it]11/16/2022 23:48:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:12 - INFO - train.train_snli_ve - loss is tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1943/6700 [54:05<2:07:50,  1.61s/it]11/16/2022 23:48:14 - INFO - train.train_snli_ve - kd_loss is tensor(6.1522e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:14 - INFO - train.train_snli_ve - loss is tensor(0.8830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1944/6700 [54:06<2:08:17,  1.62s/it]11/16/2022 23:48:15 - INFO - train.train_snli_ve - kd_loss is tensor(7.1251e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:15 - INFO - train.train_snli_ve - loss is tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1945/6700 [54:08<2:08:04,  1.62s/it]11/16/2022 23:48:17 - INFO - train.train_snli_ve - kd_loss is tensor(6.9003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:17 - INFO - train.train_snli_ve - loss is tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1946/6700 [54:10<2:08:23,  1.62s/it]11/16/2022 23:48:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.8435e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:18 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1947/6700 [54:11<2:08:18,  1.62s/it]11/16/2022 23:48:20 - INFO - train.train_snli_ve - kd_loss is tensor(8.4495e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:20 - INFO - train.train_snli_ve - loss is tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1948/6700 [54:13<2:09:08,  1.63s/it]11/16/2022 23:48:22 - INFO - train.train_snli_ve - kd_loss is tensor(5.9404e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:22 - INFO - train.train_snli_ve - loss is tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1949/6700 [54:14<2:09:26,  1.63s/it]11/16/2022 23:48:23 - INFO - train.train_snli_ve - kd_loss is tensor(7.2578e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:23 - INFO - train.train_snli_ve - loss is tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1950/6700 [54:16<2:08:54,  1.63s/it]11/16/2022 23:48:25 - INFO - train.train_snli_ve - kd_loss is tensor(5.8272e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:25 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1951/6700 [54:18<2:08:58,  1.63s/it]11/16/2022 23:48:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.6000e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:27 - INFO - train.train_snli_ve - loss is tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1952/6700 [54:19<2:08:30,  1.62s/it]11/16/2022 23:48:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.6581e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:28 - INFO - train.train_snli_ve - loss is tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1953/6700 [54:21<2:08:05,  1.62s/it]11/16/2022 23:48:30 - INFO - train.train_snli_ve - kd_loss is tensor(6.2040e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:30 - INFO - train.train_snli_ve - loss is tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1954/6700 [54:23<2:08:37,  1.63s/it]11/16/2022 23:48:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7878e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:32 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1955/6700 [54:24<2:08:31,  1.63s/it]11/16/2022 23:48:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7382e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:33 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1956/6700 [54:26<2:07:55,  1.62s/it]11/16/2022 23:48:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:35 - INFO - train.train_snli_ve - loss is tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1957/6700 [54:27<2:07:27,  1.61s/it]11/16/2022 23:48:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.3390e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:36 - INFO - train.train_snli_ve - loss is tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1958/6700 [54:29<2:07:38,  1.62s/it]11/16/2022 23:48:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:38 - INFO - train.train_snli_ve - loss is tensor(0.8102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1959/6700 [54:31<2:07:21,  1.61s/it]11/16/2022 23:48:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:40 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1960/6700 [54:32<2:07:39,  1.62s/it]11/16/2022 23:48:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.9156e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:41 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1961/6700 [54:34<2:07:34,  1.62s/it]11/16/2022 23:48:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.2065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:43 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1962/6700 [54:35<2:07:26,  1.61s/it]11/16/2022 23:48:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:44 - INFO - train.train_snli_ve - loss is tensor(0.9423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1963/6700 [54:37<2:07:53,  1.62s/it]11/16/2022 23:48:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:46 - INFO - train.train_snli_ve - loss is tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1964/6700 [54:39<2:08:19,  1.63s/it]11/16/2022 23:48:48 - INFO - train.train_snli_ve - kd_loss is tensor(9.8596e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:48 - INFO - train.train_snli_ve - loss is tensor(0.9739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1965/6700 [54:40<2:08:13,  1.62s/it]11/16/2022 23:48:49 - INFO - train.train_snli_ve - kd_loss is tensor(9.5516e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:49 - INFO - train.train_snli_ve - loss is tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1966/6700 [54:42<2:08:22,  1.63s/it]11/16/2022 23:48:51 - INFO - train.train_snli_ve - kd_loss is tensor(9.5556e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:51 - INFO - train.train_snli_ve - loss is tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1967/6700 [54:44<2:07:43,  1.62s/it]11/16/2022 23:48:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:53 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1968/6700 [54:45<2:07:30,  1.62s/it]11/16/2022 23:48:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:54 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1969/6700 [54:47<2:07:11,  1.61s/it]11/16/2022 23:48:56 - INFO - train.train_snli_ve - kd_loss is tensor(7.1157e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:56 - INFO - train.train_snli_ve - loss is tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1970/6700 [54:48<2:07:27,  1.62s/it]11/16/2022 23:48:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:57 - INFO - train.train_snli_ve - loss is tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1971/6700 [54:50<2:08:23,  1.63s/it]11/16/2022 23:48:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.7924e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:48:59 - INFO - train.train_snli_ve - loss is tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1972/6700 [54:52<2:08:07,  1.63s/it]11/16/2022 23:49:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:01 - INFO - train.train_snli_ve - loss is tensor(0.8929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1973/6700 [54:53<2:07:34,  1.62s/it]11/16/2022 23:49:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:02 - INFO - train.train_snli_ve - loss is tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1974/6700 [54:55<2:06:52,  1.61s/it]11/16/2022 23:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(6.6502e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:04 - INFO - train.train_snli_ve - loss is tensor(0.8493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1975/6700 [54:57<2:07:00,  1.61s/it]11/16/2022 23:49:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.8317e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:05 - INFO - train.train_snli_ve - loss is tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  29%|##9       | 1976/6700 [54:58<2:06:38,  1.61s/it]11/16/2022 23:49:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.1573e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:07 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1977/6700 [55:00<2:06:43,  1.61s/it]11/16/2022 23:49:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.9256e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:09 - INFO - train.train_snli_ve - loss is tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1978/6700 [55:01<2:06:35,  1.61s/it]11/16/2022 23:49:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:10 - INFO - train.train_snli_ve - loss is tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1979/6700 [55:03<2:06:56,  1.61s/it]11/16/2022 23:49:12 - INFO - train.train_snli_ve - kd_loss is tensor(9.0467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:12 - INFO - train.train_snli_ve - loss is tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1980/6700 [55:05<2:06:21,  1.61s/it]11/16/2022 23:49:14 - INFO - train.train_snli_ve - kd_loss is tensor(8.9933e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:14 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1981/6700 [55:06<2:07:00,  1.61s/it]11/16/2022 23:49:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:15 - INFO - train.train_snli_ve - loss is tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1982/6700 [55:08<2:08:20,  1.63s/it]11/16/2022 23:49:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.2808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:17 - INFO - train.train_snli_ve - loss is tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1983/6700 [55:09<2:07:40,  1.62s/it]11/16/2022 23:49:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:18 - INFO - train.train_snli_ve - loss is tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1984/6700 [55:11<2:07:29,  1.62s/it]11/16/2022 23:49:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2768e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:20 - INFO - train.train_snli_ve - loss is tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1985/6700 [55:13<2:08:25,  1.63s/it]11/16/2022 23:49:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.1699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:22 - INFO - train.train_snli_ve - loss is tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1986/6700 [55:14<2:07:40,  1.63s/it]11/16/2022 23:49:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.2944e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:23 - INFO - train.train_snli_ve - loss is tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1987/6700 [55:16<2:06:49,  1.61s/it]11/16/2022 23:49:25 - INFO - train.train_snli_ve - kd_loss is tensor(6.4918e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:25 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1988/6700 [55:18<2:07:18,  1.62s/it]11/16/2022 23:49:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2737e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:27 - INFO - train.train_snli_ve - loss is tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1989/6700 [55:19<2:07:04,  1.62s/it]11/16/2022 23:49:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:28 - INFO - train.train_snli_ve - loss is tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1990/6700 [55:21<2:06:58,  1.62s/it]11/16/2022 23:49:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.3946e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:30 - INFO - train.train_snli_ve - loss is tensor(0.9199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1991/6700 [55:22<2:06:25,  1.61s/it]11/16/2022 23:49:31 - INFO - train.train_snli_ve - kd_loss is tensor(7.4365e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:31 - INFO - train.train_snli_ve - loss is tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1992/6700 [55:24<2:06:06,  1.61s/it]11/16/2022 23:49:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.0051e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:33 - INFO - train.train_snli_ve - loss is tensor(0.8308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1993/6700 [55:26<2:06:43,  1.62s/it]11/16/2022 23:49:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:35 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1994/6700 [55:27<2:05:56,  1.61s/it]11/16/2022 23:49:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:36 - INFO - train.train_snli_ve - loss is tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1995/6700 [55:29<2:05:40,  1.60s/it]11/16/2022 23:49:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:38 - INFO - train.train_snli_ve - loss is tensor(0.7470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1996/6700 [55:30<2:07:09,  1.62s/it]11/16/2022 23:49:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:39 - INFO - train.train_snli_ve - loss is tensor(0.9144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1997/6700 [55:32<2:06:48,  1.62s/it]11/16/2022 23:49:41 - INFO - train.train_snli_ve - kd_loss is tensor(8.9746e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:41 - INFO - train.train_snli_ve - loss is tensor(0.8695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1998/6700 [55:34<2:07:02,  1.62s/it]11/16/2022 23:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:43 - INFO - train.train_snli_ve - loss is tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 1999/6700 [55:35<2:07:43,  1.63s/it]11/16/2022 23:49:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.3593e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:44 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2000/6700 [55:37<2:08:31,  1.64s/it]11/16/2022 23:49:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:46 - INFO - train.train_snli_ve - loss is tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2001/6700 [55:39<2:07:54,  1.63s/it]11/16/2022 23:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:48 - INFO - train.train_snli_ve - loss is tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2002/6700 [55:40<2:08:06,  1.64s/it]11/16/2022 23:49:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:49 - INFO - train.train_snli_ve - loss is tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2003/6700 [55:42<2:07:54,  1.63s/it]11/16/2022 23:49:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:51 - INFO - train.train_snli_ve - loss is tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2004/6700 [55:44<2:07:00,  1.62s/it]11/16/2022 23:49:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.2967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:52 - INFO - train.train_snli_ve - loss is tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2005/6700 [55:45<2:06:41,  1.62s/it]11/16/2022 23:49:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:54 - INFO - train.train_snli_ve - loss is tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2006/6700 [55:47<2:06:38,  1.62s/it]11/16/2022 23:49:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:56 - INFO - train.train_snli_ve - loss is tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2007/6700 [55:48<2:06:45,  1.62s/it]11/16/2022 23:49:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.0312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:57 - INFO - train.train_snli_ve - loss is tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2008/6700 [55:50<2:06:53,  1.62s/it]11/16/2022 23:49:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:49:59 - INFO - train.train_snli_ve - loss is tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|##9       | 2009/6700 [55:52<2:06:29,  1.62s/it]11/16/2022 23:50:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:01 - INFO - train.train_snli_ve - loss is tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2010/6700 [55:53<2:06:10,  1.61s/it]11/16/2022 23:50:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:02 - INFO - train.train_snli_ve - loss is tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2011/6700 [55:55<2:06:06,  1.61s/it]11/16/2022 23:50:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:04 - INFO - train.train_snli_ve - loss is tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2012/6700 [55:56<2:06:18,  1.62s/it]11/16/2022 23:50:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6818e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:05 - INFO - train.train_snli_ve - loss is tensor(0.8747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2013/6700 [55:58<2:07:00,  1.63s/it]11/16/2022 23:50:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:07 - INFO - train.train_snli_ve - loss is tensor(0.8897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2014/6700 [56:00<2:06:07,  1.61s/it]11/16/2022 23:50:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.2520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:09 - INFO - train.train_snli_ve - loss is tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2015/6700 [56:01<2:06:00,  1.61s/it]11/16/2022 23:50:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:10 - INFO - train.train_snli_ve - loss is tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2016/6700 [56:03<2:05:35,  1.61s/it]11/16/2022 23:50:12 - INFO - train.train_snli_ve - kd_loss is tensor(6.1823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:12 - INFO - train.train_snli_ve - loss is tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2017/6700 [56:05<2:05:56,  1.61s/it]11/16/2022 23:50:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:13 - INFO - train.train_snli_ve - loss is tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2018/6700 [56:06<2:06:15,  1.62s/it]11/16/2022 23:50:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0710e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:15 - INFO - train.train_snli_ve - loss is tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2019/6700 [56:08<2:05:45,  1.61s/it]11/16/2022 23:50:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:17 - INFO - train.train_snli_ve - loss is tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2020/6700 [56:09<2:05:52,  1.61s/it]11/16/2022 23:50:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1745e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:18 - INFO - train.train_snli_ve - loss is tensor(0.7342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2021/6700 [56:11<2:04:55,  1.60s/it]11/16/2022 23:50:20 - INFO - train.train_snli_ve - kd_loss is tensor(9.3551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:20 - INFO - train.train_snli_ve - loss is tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2022/6700 [56:13<2:05:23,  1.61s/it]11/16/2022 23:50:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:21 - INFO - train.train_snli_ve - loss is tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2023/6700 [56:14<2:05:05,  1.60s/it]11/16/2022 23:50:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.8481e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:23 - INFO - train.train_snli_ve - loss is tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2024/6700 [56:16<2:05:14,  1.61s/it]11/16/2022 23:50:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9339e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:25 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2025/6700 [56:17<2:04:46,  1.60s/it]11/16/2022 23:50:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:26 - INFO - train.train_snli_ve - loss is tensor(0.8745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2026/6700 [56:19<2:04:57,  1.60s/it]11/16/2022 23:50:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.8033e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:28 - INFO - train.train_snli_ve - loss is tensor(0.8226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2027/6700 [56:21<2:05:04,  1.61s/it]11/16/2022 23:50:30 - INFO - train.train_snli_ve - kd_loss is tensor(7.5830e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:30 - INFO - train.train_snli_ve - loss is tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2028/6700 [56:22<2:05:02,  1.61s/it]11/16/2022 23:50:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:31 - INFO - train.train_snli_ve - loss is tensor(0.9191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2029/6700 [56:24<2:05:05,  1.61s/it]11/16/2022 23:50:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.0102e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:33 - INFO - train.train_snli_ve - loss is tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2030/6700 [56:25<2:04:44,  1.60s/it]11/16/2022 23:50:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.8838e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:34 - INFO - train.train_snli_ve - loss is tensor(0.8496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2031/6700 [56:27<2:04:24,  1.60s/it]11/16/2022 23:50:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:36 - INFO - train.train_snli_ve - loss is tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2032/6700 [56:29<2:05:01,  1.61s/it]11/16/2022 23:50:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:38 - INFO - train.train_snli_ve - loss is tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2033/6700 [56:30<2:06:38,  1.63s/it]11/16/2022 23:50:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0410e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:39 - INFO - train.train_snli_ve - loss is tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2034/6700 [56:32<2:05:46,  1.62s/it]11/16/2022 23:50:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.6043e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:41 - INFO - train.train_snli_ve - loss is tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2035/6700 [56:34<2:06:13,  1.62s/it]11/16/2022 23:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:42 - INFO - train.train_snli_ve - loss is tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2036/6700 [56:35<2:06:53,  1.63s/it]11/16/2022 23:50:44 - INFO - train.train_snli_ve - kd_loss is tensor(7.7265e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:44 - INFO - train.train_snli_ve - loss is tensor(0.7839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2037/6700 [56:37<2:07:29,  1.64s/it]11/16/2022 23:50:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.0747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:46 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2038/6700 [56:38<2:06:59,  1.63s/it]11/16/2022 23:50:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:47 - INFO - train.train_snli_ve - loss is tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2039/6700 [56:40<2:08:57,  1.66s/it]11/16/2022 23:50:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:49 - INFO - train.train_snli_ve - loss is tensor(0.5650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2040/6700 [56:42<2:08:31,  1.65s/it]11/16/2022 23:50:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.3914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:51 - INFO - train.train_snli_ve - loss is tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2041/6700 [56:43<2:07:14,  1.64s/it]11/16/2022 23:50:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:52 - INFO - train.train_snli_ve - loss is tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2042/6700 [56:45<2:06:42,  1.63s/it]11/16/2022 23:50:54 - INFO - train.train_snli_ve - kd_loss is tensor(9.7763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:54 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  30%|###       | 2043/6700 [56:47<2:06:38,  1.63s/it]11/16/2022 23:50:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:56 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2044/6700 [56:48<2:06:25,  1.63s/it]11/16/2022 23:50:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1072e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:57 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2045/6700 [56:50<2:06:10,  1.63s/it]11/16/2022 23:50:59 - INFO - train.train_snli_ve - kd_loss is tensor(9.7181e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:50:59 - INFO - train.train_snli_ve - loss is tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2046/6700 [56:52<2:07:04,  1.64s/it]11/16/2022 23:51:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1678e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:01 - INFO - train.train_snli_ve - loss is tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2047/6700 [56:53<2:07:08,  1.64s/it]11/16/2022 23:51:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9996e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:02 - INFO - train.train_snli_ve - loss is tensor(0.7717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2048/6700 [56:55<2:06:13,  1.63s/it]11/16/2022 23:51:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:04 - INFO - train.train_snli_ve - loss is tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2049/6700 [56:56<2:06:21,  1.63s/it]11/16/2022 23:51:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.0141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:05 - INFO - train.train_snli_ve - loss is tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2050/6700 [56:58<2:05:58,  1.63s/it]11/16/2022 23:51:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.4004e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:07 - INFO - train.train_snli_ve - loss is tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2051/6700 [57:00<2:05:17,  1.62s/it]11/16/2022 23:51:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.2456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:09 - INFO - train.train_snli_ve - loss is tensor(0.7643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2052/6700 [57:01<2:06:02,  1.63s/it]11/16/2022 23:51:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3464e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:10 - INFO - train.train_snli_ve - loss is tensor(0.8589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2053/6700 [57:03<2:05:32,  1.62s/it]11/16/2022 23:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(8.5747e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:12 - INFO - train.train_snli_ve - loss is tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2054/6700 [57:05<2:05:46,  1.62s/it]11/16/2022 23:51:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3150e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:13 - INFO - train.train_snli_ve - loss is tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2055/6700 [57:06<2:05:50,  1.63s/it]11/16/2022 23:51:15 - INFO - train.train_snli_ve - kd_loss is tensor(8.6457e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:15 - INFO - train.train_snli_ve - loss is tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2056/6700 [57:08<2:05:47,  1.63s/it]11/16/2022 23:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:17 - INFO - train.train_snli_ve - loss is tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2057/6700 [57:09<2:06:57,  1.64s/it]11/16/2022 23:51:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:18 - INFO - train.train_snli_ve - loss is tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2058/6700 [57:11<2:07:38,  1.65s/it]11/16/2022 23:51:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2656e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:20 - INFO - train.train_snli_ve - loss is tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2059/6700 [57:13<2:08:46,  1.66s/it]11/16/2022 23:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:22 - INFO - train.train_snli_ve - loss is tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2060/6700 [57:15<2:09:29,  1.67s/it]11/16/2022 23:51:24 - INFO - train.train_snli_ve - kd_loss is tensor(7.9637e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:24 - INFO - train.train_snli_ve - loss is tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2061/6700 [57:16<2:08:57,  1.67s/it]11/16/2022 23:51:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:25 - INFO - train.train_snli_ve - loss is tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2062/6700 [57:18<2:07:53,  1.65s/it]11/16/2022 23:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:27 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2063/6700 [57:19<2:07:54,  1.66s/it]11/16/2022 23:51:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4088e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:28 - INFO - train.train_snli_ve - loss is tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2064/6700 [57:21<2:07:07,  1.65s/it]11/16/2022 23:51:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.4048e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:30 - INFO - train.train_snli_ve - loss is tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2065/6700 [57:23<2:06:18,  1.64s/it]11/16/2022 23:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.8037e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:32 - INFO - train.train_snli_ve - loss is tensor(0.7454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2066/6700 [57:24<2:06:38,  1.64s/it]11/16/2022 23:51:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:33 - INFO - train.train_snli_ve - loss is tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2067/6700 [57:26<2:06:32,  1.64s/it]11/16/2022 23:51:35 - INFO - train.train_snli_ve - kd_loss is tensor(7.3731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:35 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2068/6700 [57:28<2:05:58,  1.63s/it]11/16/2022 23:51:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:37 - INFO - train.train_snli_ve - loss is tensor(0.8193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2069/6700 [57:29<2:06:09,  1.63s/it]11/16/2022 23:51:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:38 - INFO - train.train_snli_ve - loss is tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2070/6700 [57:31<2:06:20,  1.64s/it]11/16/2022 23:51:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.4805e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:40 - INFO - train.train_snli_ve - loss is tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2071/6700 [57:33<2:06:46,  1.64s/it]11/16/2022 23:51:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.0278e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:42 - INFO - train.train_snli_ve - loss is tensor(0.7924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2072/6700 [57:34<2:06:34,  1.64s/it]11/16/2022 23:51:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.0731e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:43 - INFO - train.train_snli_ve - loss is tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2073/6700 [57:36<2:07:32,  1.65s/it]11/16/2022 23:51:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:45 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2074/6700 [57:37<2:06:48,  1.64s/it]11/16/2022 23:51:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:46 - INFO - train.train_snli_ve - loss is tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2075/6700 [57:39<2:06:15,  1.64s/it]11/16/2022 23:51:48 - INFO - train.train_snli_ve - kd_loss is tensor(8.4244e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:48 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###       | 2076/6700 [57:41<2:05:41,  1.63s/it]11/16/2022 23:51:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.5290e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:50 - INFO - train.train_snli_ve - loss is tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2077/6700 [57:42<2:05:48,  1.63s/it]11/16/2022 23:51:51 - INFO - train.train_snli_ve - kd_loss is tensor(8.6309e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:51 - INFO - train.train_snli_ve - loss is tensor(0.5942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2078/6700 [57:44<2:05:42,  1.63s/it]11/16/2022 23:51:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:53 - INFO - train.train_snli_ve - loss is tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2079/6700 [57:46<2:05:25,  1.63s/it]11/16/2022 23:51:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.9785e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:55 - INFO - train.train_snli_ve - loss is tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2080/6700 [57:47<2:05:29,  1.63s/it]11/16/2022 23:51:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:56 - INFO - train.train_snli_ve - loss is tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2081/6700 [57:49<2:06:01,  1.64s/it]11/16/2022 23:51:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.5931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:58 - INFO - train.train_snli_ve - loss is tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2082/6700 [57:51<2:06:02,  1.64s/it]11/16/2022 23:51:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.7003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:51:59 - INFO - train.train_snli_ve - loss is tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2083/6700 [57:52<2:05:47,  1.63s/it]11/16/2022 23:52:01 - INFO - train.train_snli_ve - kd_loss is tensor(9.8823e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:01 - INFO - train.train_snli_ve - loss is tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2084/6700 [57:54<2:05:41,  1.63s/it]11/16/2022 23:52:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:03 - INFO - train.train_snli_ve - loss is tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2085/6700 [57:55<2:06:28,  1.64s/it]11/16/2022 23:52:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.2201e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:04 - INFO - train.train_snli_ve - loss is tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2086/6700 [57:57<2:05:56,  1.64s/it]11/16/2022 23:52:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.3238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:06 - INFO - train.train_snli_ve - loss is tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2087/6700 [57:59<2:06:04,  1.64s/it]11/16/2022 23:52:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.8790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:08 - INFO - train.train_snli_ve - loss is tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2088/6700 [58:00<2:05:52,  1.64s/it]11/16/2022 23:52:09 - INFO - train.train_snli_ve - kd_loss is tensor(9.6763e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:09 - INFO - train.train_snli_ve - loss is tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2089/6700 [58:02<2:05:38,  1.63s/it]11/16/2022 23:52:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:11 - INFO - train.train_snli_ve - loss is tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2090/6700 [58:04<2:05:12,  1.63s/it]11/16/2022 23:52:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.1791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:13 - INFO - train.train_snli_ve - loss is tensor(0.8521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2091/6700 [58:05<2:05:03,  1.63s/it]11/16/2022 23:52:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:14 - INFO - train.train_snli_ve - loss is tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2092/6700 [58:07<2:05:21,  1.63s/it]11/16/2022 23:52:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:16 - INFO - train.train_snli_ve - loss is tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2093/6700 [58:09<2:04:56,  1.63s/it]11/16/2022 23:52:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.4484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:17 - INFO - train.train_snli_ve - loss is tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2094/6700 [58:10<2:04:46,  1.63s/it]11/16/2022 23:52:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.6513e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:19 - INFO - train.train_snli_ve - loss is tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2095/6700 [58:12<2:05:26,  1.63s/it]11/16/2022 23:52:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:21 - INFO - train.train_snli_ve - loss is tensor(0.7344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2096/6700 [58:13<2:05:24,  1.63s/it]11/16/2022 23:52:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.3271e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:22 - INFO - train.train_snli_ve - loss is tensor(0.8508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2097/6700 [58:15<2:05:58,  1.64s/it]11/16/2022 23:52:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:24 - INFO - train.train_snli_ve - loss is tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2098/6700 [58:17<2:05:38,  1.64s/it]11/16/2022 23:52:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:26 - INFO - train.train_snli_ve - loss is tensor(0.8685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2099/6700 [58:18<2:05:42,  1.64s/it]11/16/2022 23:52:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:27 - INFO - train.train_snli_ve - loss is tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2100/6700 [58:20<2:06:32,  1.65s/it]11/16/2022 23:52:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:29 - INFO - train.train_snli_ve - loss is tensor(0.8760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2101/6700 [58:22<2:06:15,  1.65s/it]11/16/2022 23:52:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.1404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:31 - INFO - train.train_snli_ve - loss is tensor(0.9721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2102/6700 [58:23<2:06:16,  1.65s/it]11/16/2022 23:52:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.5128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:32 - INFO - train.train_snli_ve - loss is tensor(0.8227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2103/6700 [58:25<2:05:22,  1.64s/it]11/16/2022 23:52:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:34 - INFO - train.train_snli_ve - loss is tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2104/6700 [58:27<2:05:54,  1.64s/it]11/16/2022 23:52:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.3920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:36 - INFO - train.train_snli_ve - loss is tensor(0.9477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2105/6700 [58:28<2:05:41,  1.64s/it]11/16/2022 23:52:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:37 - INFO - train.train_snli_ve - loss is tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2106/6700 [58:30<2:05:35,  1.64s/it]11/16/2022 23:52:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:39 - INFO - train.train_snli_ve - loss is tensor(0.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2107/6700 [58:31<2:04:36,  1.63s/it]11/16/2022 23:52:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.6196e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:40 - INFO - train.train_snli_ve - loss is tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2108/6700 [58:33<2:03:31,  1.61s/it]11/16/2022 23:52:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.0127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:42 - INFO - train.train_snli_ve - loss is tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2109/6700 [58:35<2:04:11,  1.62s/it]11/16/2022 23:52:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1757e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:44 - INFO - train.train_snli_ve - loss is tensor(0.5191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  31%|###1      | 2110/6700 [58:36<2:03:36,  1.62s/it]11/16/2022 23:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.4089e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:45 - INFO - train.train_snli_ve - loss is tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2111/6700 [58:38<2:04:51,  1.63s/it]11/16/2022 23:52:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:47 - INFO - train.train_snli_ve - loss is tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2112/6700 [58:40<2:05:42,  1.64s/it]11/16/2022 23:52:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:49 - INFO - train.train_snli_ve - loss is tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2113/6700 [58:41<2:06:54,  1.66s/it]11/16/2022 23:52:50 - INFO - train.train_snli_ve - kd_loss is tensor(8.6241e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:50 - INFO - train.train_snli_ve - loss is tensor(0.8886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2114/6700 [58:43<2:06:50,  1.66s/it]11/16/2022 23:52:52 - INFO - train.train_snli_ve - kd_loss is tensor(7.4063e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:52 - INFO - train.train_snli_ve - loss is tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2115/6700 [58:45<2:06:38,  1.66s/it]11/16/2022 23:52:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:54 - INFO - train.train_snli_ve - loss is tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2116/6700 [58:46<2:06:21,  1.65s/it]11/16/2022 23:52:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7737e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:55 - INFO - train.train_snli_ve - loss is tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2117/6700 [58:48<2:07:07,  1.66s/it]11/16/2022 23:52:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.5265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:57 - INFO - train.train_snli_ve - loss is tensor(0.4260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2118/6700 [58:50<2:06:54,  1.66s/it]11/16/2022 23:52:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.4664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:52:59 - INFO - train.train_snli_ve - loss is tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2119/6700 [58:51<2:07:03,  1.66s/it]11/16/2022 23:53:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:00 - INFO - train.train_snli_ve - loss is tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2120/6700 [58:53<2:08:39,  1.69s/it]11/16/2022 23:53:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:02 - INFO - train.train_snli_ve - loss is tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2121/6700 [58:55<2:07:29,  1.67s/it]11/16/2022 23:53:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1323e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:04 - INFO - train.train_snli_ve - loss is tensor(0.7269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2122/6700 [58:56<2:07:55,  1.68s/it]11/16/2022 23:53:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:05 - INFO - train.train_snli_ve - loss is tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2123/6700 [58:58<2:07:14,  1.67s/it]11/16/2022 23:53:07 - INFO - train.train_snli_ve - kd_loss is tensor(7.0786e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:07 - INFO - train.train_snli_ve - loss is tensor(0.6513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2124/6700 [59:00<2:07:57,  1.68s/it]11/16/2022 23:53:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:09 - INFO - train.train_snli_ve - loss is tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2125/6700 [59:01<2:07:11,  1.67s/it]11/16/2022 23:53:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:10 - INFO - train.train_snli_ve - loss is tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2126/6700 [59:03<2:06:42,  1.66s/it]11/16/2022 23:53:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:12 - INFO - train.train_snli_ve - loss is tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2127/6700 [59:05<2:08:29,  1.69s/it]11/16/2022 23:53:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:14 - INFO - train.train_snli_ve - loss is tensor(0.4787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2128/6700 [59:06<2:08:20,  1.68s/it]11/16/2022 23:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:15 - INFO - train.train_snli_ve - loss is tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2129/6700 [59:08<2:08:26,  1.69s/it]11/16/2022 23:53:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5650e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:17 - INFO - train.train_snli_ve - loss is tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2130/6700 [59:10<2:08:00,  1.68s/it]11/16/2022 23:53:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.9168e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:19 - INFO - train.train_snli_ve - loss is tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2131/6700 [59:11<2:08:37,  1.69s/it]11/16/2022 23:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8997e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:20 - INFO - train.train_snli_ve - loss is tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2132/6700 [59:13<2:07:40,  1.68s/it]11/16/2022 23:53:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.3347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:22 - INFO - train.train_snli_ve - loss is tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2133/6700 [59:15<2:07:46,  1.68s/it]11/16/2022 23:53:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:24 - INFO - train.train_snli_ve - loss is tensor(0.8724, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2134/6700 [59:17<2:07:57,  1.68s/it]11/16/2022 23:53:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:26 - INFO - train.train_snli_ve - loss is tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2135/6700 [59:18<2:08:31,  1.69s/it]11/16/2022 23:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:27 - INFO - train.train_snli_ve - loss is tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2136/6700 [59:20<2:09:37,  1.70s/it]11/16/2022 23:53:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.4018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:29 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2137/6700 [59:22<2:09:00,  1.70s/it]11/16/2022 23:53:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:31 - INFO - train.train_snli_ve - loss is tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2138/6700 [59:23<2:08:42,  1.69s/it]11/16/2022 23:53:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:32 - INFO - train.train_snli_ve - loss is tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2139/6700 [59:25<2:09:14,  1.70s/it]11/16/2022 23:53:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:34 - INFO - train.train_snli_ve - loss is tensor(0.8402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2140/6700 [59:27<2:08:15,  1.69s/it]11/16/2022 23:53:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.6988e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:36 - INFO - train.train_snli_ve - loss is tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2141/6700 [59:28<2:08:22,  1.69s/it]11/16/2022 23:53:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:37 - INFO - train.train_snli_ve - loss is tensor(1.0265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2142/6700 [59:30<2:07:42,  1.68s/it]11/16/2022 23:53:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:39 - INFO - train.train_snli_ve - loss is tensor(0.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###1      | 2143/6700 [59:32<2:07:38,  1.68s/it]11/16/2022 23:53:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.0665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:41 - INFO - train.train_snli_ve - loss is tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2144/6700 [59:33<2:06:50,  1.67s/it]11/16/2022 23:53:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.9419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:42 - INFO - train.train_snli_ve - loss is tensor(0.8423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2145/6700 [59:35<2:07:10,  1.68s/it]11/16/2022 23:53:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:44 - INFO - train.train_snli_ve - loss is tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2146/6700 [59:37<2:08:23,  1.69s/it]11/16/2022 23:53:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:46 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2147/6700 [59:38<2:07:52,  1.69s/it]11/16/2022 23:53:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:47 - INFO - train.train_snli_ve - loss is tensor(0.9429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2148/6700 [59:40<2:08:30,  1.69s/it]11/16/2022 23:53:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8344e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:49 - INFO - train.train_snli_ve - loss is tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2149/6700 [59:42<2:08:41,  1.70s/it]11/16/2022 23:53:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:51 - INFO - train.train_snli_ve - loss is tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2150/6700 [59:44<2:08:59,  1.70s/it]11/16/2022 23:53:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:53 - INFO - train.train_snli_ve - loss is tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2151/6700 [59:45<2:08:18,  1.69s/it]11/16/2022 23:53:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:54 - INFO - train.train_snli_ve - loss is tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2152/6700 [59:47<2:08:01,  1.69s/it]11/16/2022 23:53:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.0405e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:56 - INFO - train.train_snli_ve - loss is tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2153/6700 [59:49<2:09:08,  1.70s/it]11/16/2022 23:53:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.1821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:58 - INFO - train.train_snli_ve - loss is tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2154/6700 [59:50<2:09:16,  1.71s/it]11/16/2022 23:53:59 - INFO - train.train_snli_ve - kd_loss is tensor(7.4938e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:53:59 - INFO - train.train_snli_ve - loss is tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2155/6700 [59:52<2:07:49,  1.69s/it]11/16/2022 23:54:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:01 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2156/6700 [59:54<2:08:11,  1.69s/it]11/16/2022 23:54:03 - INFO - train.train_snli_ve - kd_loss is tensor(8.0532e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:03 - INFO - train.train_snli_ve - loss is tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2157/6700 [59:55<2:08:30,  1.70s/it]11/16/2022 23:54:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4048e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:04 - INFO - train.train_snli_ve - loss is tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2158/6700 [59:57<2:08:00,  1.69s/it]11/16/2022 23:54:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.9731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:06 - INFO - train.train_snli_ve - loss is tensor(0.7734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2159/6700 [59:59<2:07:53,  1.69s/it]11/16/2022 23:54:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:08 - INFO - train.train_snli_ve - loss is tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2160/6700 [1:00:00<2:07:19,  1.68s/it]11/16/2022 23:54:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:09 - INFO - train.train_snli_ve - loss is tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2161/6700 [1:00:02<2:07:12,  1.68s/it]11/16/2022 23:54:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.5261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:11 - INFO - train.train_snli_ve - loss is tensor(0.8185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2162/6700 [1:00:04<2:06:41,  1.68s/it]11/16/2022 23:54:13 - INFO - train.train_snli_ve - kd_loss is tensor(8.6874e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:13 - INFO - train.train_snli_ve - loss is tensor(0.9645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2163/6700 [1:00:05<2:06:32,  1.67s/it]11/16/2022 23:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:14 - INFO - train.train_snli_ve - loss is tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2164/6700 [1:00:07<2:05:55,  1.67s/it]11/16/2022 23:54:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0099e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:16 - INFO - train.train_snli_ve - loss is tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2165/6700 [1:00:09<2:05:42,  1.66s/it]11/16/2022 23:54:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.0060e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:18 - INFO - train.train_snli_ve - loss is tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2166/6700 [1:00:10<2:05:42,  1.66s/it]11/16/2022 23:54:19 - INFO - train.train_snli_ve - kd_loss is tensor(6.6255e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:19 - INFO - train.train_snli_ve - loss is tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2167/6700 [1:00:12<2:07:02,  1.68s/it]11/16/2022 23:54:21 - INFO - train.train_snli_ve - kd_loss is tensor(7.4011e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:21 - INFO - train.train_snli_ve - loss is tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2168/6700 [1:00:14<2:06:38,  1.68s/it]11/16/2022 23:54:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.1261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:23 - INFO - train.train_snli_ve - loss is tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2169/6700 [1:00:16<2:06:18,  1.67s/it]11/16/2022 23:54:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.7210e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:24 - INFO - train.train_snli_ve - loss is tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2170/6700 [1:00:17<2:05:57,  1.67s/it]11/16/2022 23:54:26 - INFO - train.train_snli_ve - kd_loss is tensor(5.0475e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:26 - INFO - train.train_snli_ve - loss is tensor(0.8222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2171/6700 [1:00:19<2:06:32,  1.68s/it]11/16/2022 23:54:28 - INFO - train.train_snli_ve - kd_loss is tensor(6.0601e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:28 - INFO - train.train_snli_ve - loss is tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2172/6700 [1:00:21<2:06:15,  1.67s/it]11/16/2022 23:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0077e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:30 - INFO - train.train_snli_ve - loss is tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2173/6700 [1:00:22<2:06:48,  1.68s/it]11/16/2022 23:54:31 - INFO - train.train_snli_ve - kd_loss is tensor(6.5598e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:31 - INFO - train.train_snli_ve - loss is tensor(0.7864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2174/6700 [1:00:24<2:07:25,  1.69s/it]11/16/2022 23:54:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6552e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:33 - INFO - train.train_snli_ve - loss is tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2175/6700 [1:00:26<2:08:02,  1.70s/it]11/16/2022 23:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(5.1824e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:35 - INFO - train.train_snli_ve - loss is tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2176/6700 [1:00:27<2:06:53,  1.68s/it]11/16/2022 23:54:36 - INFO - train.train_snli_ve - kd_loss is tensor(7.7418e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:36 - INFO - train.train_snli_ve - loss is tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  32%|###2      | 2177/6700 [1:00:29<2:06:31,  1.68s/it]11/16/2022 23:54:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.1881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:38 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2178/6700 [1:00:31<2:07:19,  1.69s/it]11/16/2022 23:54:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.2139e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:40 - INFO - train.train_snli_ve - loss is tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2179/6700 [1:00:32<2:06:36,  1.68s/it]11/16/2022 23:54:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.2510e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:41 - INFO - train.train_snli_ve - loss is tensor(0.9135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2180/6700 [1:00:34<2:07:26,  1.69s/it]11/16/2022 23:54:43 - INFO - train.train_snli_ve - kd_loss is tensor(9.5032e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:43 - INFO - train.train_snli_ve - loss is tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2181/6700 [1:00:36<2:06:37,  1.68s/it]11/16/2022 23:54:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.2682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:45 - INFO - train.train_snli_ve - loss is tensor(0.5492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2182/6700 [1:00:37<2:05:40,  1.67s/it]11/16/2022 23:54:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.2926e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:46 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2183/6700 [1:00:39<2:05:42,  1.67s/it]11/16/2022 23:54:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.3145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:48 - INFO - train.train_snli_ve - loss is tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2184/6700 [1:00:41<2:04:48,  1.66s/it]11/16/2022 23:54:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4578e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:50 - INFO - train.train_snli_ve - loss is tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2185/6700 [1:00:42<2:05:08,  1.66s/it]11/16/2022 23:54:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.2227e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:51 - INFO - train.train_snli_ve - loss is tensor(0.4577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2186/6700 [1:00:44<2:04:41,  1.66s/it]11/16/2022 23:54:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:53 - INFO - train.train_snli_ve - loss is tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2187/6700 [1:00:46<2:04:58,  1.66s/it]11/16/2022 23:54:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:55 - INFO - train.train_snli_ve - loss is tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2188/6700 [1:00:47<2:05:06,  1.66s/it]11/16/2022 23:54:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5428e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:56 - INFO - train.train_snli_ve - loss is tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2189/6700 [1:00:49<2:05:23,  1.67s/it]11/16/2022 23:54:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.8110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:54:58 - INFO - train.train_snli_ve - loss is tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2190/6700 [1:00:51<2:04:52,  1.66s/it]11/16/2022 23:55:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.1966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:00 - INFO - train.train_snli_ve - loss is tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2191/6700 [1:00:52<2:05:51,  1.67s/it]11/16/2022 23:55:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6324e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:01 - INFO - train.train_snli_ve - loss is tensor(0.9664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2192/6700 [1:00:54<2:05:57,  1.68s/it]11/16/2022 23:55:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:03 - INFO - train.train_snli_ve - loss is tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2193/6700 [1:00:56<2:03:55,  1.65s/it]11/16/2022 23:55:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:05 - INFO - train.train_snli_ve - loss is tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2194/6700 [1:00:57<2:04:43,  1.66s/it]11/16/2022 23:55:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:06 - INFO - train.train_snli_ve - loss is tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2195/6700 [1:00:59<2:05:02,  1.67s/it]11/16/2022 23:55:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:08 - INFO - train.train_snli_ve - loss is tensor(1.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2196/6700 [1:01:01<2:04:45,  1.66s/it]11/16/2022 23:55:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:10 - INFO - train.train_snli_ve - loss is tensor(0.7194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2197/6700 [1:01:02<2:04:29,  1.66s/it]11/16/2022 23:55:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:11 - INFO - train.train_snli_ve - loss is tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2198/6700 [1:01:04<2:04:33,  1.66s/it]11/16/2022 23:55:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.3158e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:13 - INFO - train.train_snli_ve - loss is tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2199/6700 [1:01:06<2:05:14,  1.67s/it]11/16/2022 23:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0908e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:15 - INFO - train.train_snli_ve - loss is tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2200/6700 [1:01:07<2:05:51,  1.68s/it]11/16/2022 23:55:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:16 - INFO - train.train_snli_ve - loss is tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2201/6700 [1:01:09<2:05:36,  1.68s/it]11/16/2022 23:55:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:18 - INFO - train.train_snli_ve - loss is tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2202/6700 [1:01:11<2:05:08,  1.67s/it]11/16/2022 23:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:20 - INFO - train.train_snli_ve - loss is tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2203/6700 [1:01:12<2:04:53,  1.67s/it]11/16/2022 23:55:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:21 - INFO - train.train_snli_ve - loss is tensor(0.9387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2204/6700 [1:01:14<2:04:43,  1.66s/it]11/16/2022 23:55:23 - INFO - train.train_snli_ve - kd_loss is tensor(9.6008e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:23 - INFO - train.train_snli_ve - loss is tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2205/6700 [1:01:16<2:04:33,  1.66s/it]11/16/2022 23:55:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.6189e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:25 - INFO - train.train_snli_ve - loss is tensor(0.9690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2206/6700 [1:01:17<2:04:19,  1.66s/it]11/16/2022 23:55:26 - INFO - train.train_snli_ve - kd_loss is tensor(6.1489e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:26 - INFO - train.train_snli_ve - loss is tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2207/6700 [1:01:19<2:04:06,  1.66s/it]11/16/2022 23:55:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:28 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2208/6700 [1:01:21<2:02:55,  1.64s/it]11/16/2022 23:55:30 - INFO - train.train_snli_ve - kd_loss is tensor(8.9840e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:30 - INFO - train.train_snli_ve - loss is tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2209/6700 [1:01:22<2:04:30,  1.66s/it]11/16/2022 23:55:31 - INFO - train.train_snli_ve - kd_loss is tensor(8.1437e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:31 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###2      | 2210/6700 [1:01:24<2:04:15,  1.66s/it]11/16/2022 23:55:33 - INFO - train.train_snli_ve - kd_loss is tensor(7.6769e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:33 - INFO - train.train_snli_ve - loss is tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2211/6700 [1:01:26<2:04:54,  1.67s/it]11/16/2022 23:55:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:35 - INFO - train.train_snli_ve - loss is tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2212/6700 [1:01:27<2:05:03,  1.67s/it]11/16/2022 23:55:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:36 - INFO - train.train_snli_ve - loss is tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2213/6700 [1:01:29<2:05:09,  1.67s/it]11/16/2022 23:55:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:38 - INFO - train.train_snli_ve - loss is tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2214/6700 [1:01:31<2:05:59,  1.69s/it]11/16/2022 23:55:40 - INFO - train.train_snli_ve - kd_loss is tensor(6.6669e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:40 - INFO - train.train_snli_ve - loss is tensor(0.8542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2215/6700 [1:01:32<2:05:42,  1.68s/it]11/16/2022 23:55:41 - INFO - train.train_snli_ve - kd_loss is tensor(7.9934e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:41 - INFO - train.train_snli_ve - loss is tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2216/6700 [1:01:34<2:05:30,  1.68s/it]11/16/2022 23:55:43 - INFO - train.train_snli_ve - kd_loss is tensor(8.7004e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:43 - INFO - train.train_snli_ve - loss is tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2217/6700 [1:01:36<2:04:52,  1.67s/it]11/16/2022 23:55:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.6554e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:45 - INFO - train.train_snli_ve - loss is tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2218/6700 [1:01:37<2:04:23,  1.67s/it]11/16/2022 23:55:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.3687e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:46 - INFO - train.train_snli_ve - loss is tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2219/6700 [1:01:39<2:04:30,  1.67s/it]11/16/2022 23:55:48 - INFO - train.train_snli_ve - kd_loss is tensor(7.5066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:48 - INFO - train.train_snli_ve - loss is tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2220/6700 [1:01:41<2:04:38,  1.67s/it]11/16/2022 23:55:50 - INFO - train.train_snli_ve - kd_loss is tensor(6.7315e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:50 - INFO - train.train_snli_ve - loss is tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2221/6700 [1:01:42<2:05:23,  1.68s/it]11/16/2022 23:55:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:51 - INFO - train.train_snli_ve - loss is tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2222/6700 [1:01:44<2:05:27,  1.68s/it]11/16/2022 23:55:53 - INFO - train.train_snli_ve - kd_loss is tensor(6.7745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:53 - INFO - train.train_snli_ve - loss is tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2223/6700 [1:01:46<2:04:52,  1.67s/it]11/16/2022 23:55:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.0192e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:55 - INFO - train.train_snli_ve - loss is tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2224/6700 [1:01:47<2:04:25,  1.67s/it]11/16/2022 23:55:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.0799e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:56 - INFO - train.train_snli_ve - loss is tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2225/6700 [1:01:49<2:04:24,  1.67s/it]11/16/2022 23:55:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.5319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:55:58 - INFO - train.train_snli_ve - loss is tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2226/6700 [1:01:51<2:04:37,  1.67s/it]11/16/2022 23:56:00 - INFO - train.train_snli_ve - kd_loss is tensor(8.8422e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:00 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2227/6700 [1:01:52<2:05:06,  1.68s/it]11/16/2022 23:56:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.1893e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:01 - INFO - train.train_snli_ve - loss is tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2228/6700 [1:01:54<2:04:36,  1.67s/it]11/16/2022 23:56:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:03 - INFO - train.train_snli_ve - loss is tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2229/6700 [1:01:56<2:04:24,  1.67s/it]11/16/2022 23:56:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.3051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:05 - INFO - train.train_snli_ve - loss is tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2230/6700 [1:01:57<2:03:56,  1.66s/it]11/16/2022 23:56:06 - INFO - train.train_snli_ve - kd_loss is tensor(7.0178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:06 - INFO - train.train_snli_ve - loss is tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2231/6700 [1:01:59<2:03:53,  1.66s/it]11/16/2022 23:56:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2260e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:08 - INFO - train.train_snli_ve - loss is tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2232/6700 [1:02:01<2:04:34,  1.67s/it]11/16/2022 23:56:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:10 - INFO - train.train_snli_ve - loss is tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2233/6700 [1:02:02<2:04:39,  1.67s/it]11/16/2022 23:56:11 - INFO - train.train_snli_ve - kd_loss is tensor(8.4781e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:11 - INFO - train.train_snli_ve - loss is tensor(0.8991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2234/6700 [1:02:04<2:05:13,  1.68s/it]11/16/2022 23:56:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:13 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2235/6700 [1:02:06<2:05:26,  1.69s/it]11/16/2022 23:56:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.4834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:15 - INFO - train.train_snli_ve - loss is tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2236/6700 [1:02:07<2:04:43,  1.68s/it]11/16/2022 23:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:16 - INFO - train.train_snli_ve - loss is tensor(0.8711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2237/6700 [1:02:09<2:04:50,  1.68s/it]11/16/2022 23:56:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:18 - INFO - train.train_snli_ve - loss is tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2238/6700 [1:02:11<2:05:04,  1.68s/it]11/16/2022 23:56:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:20 - INFO - train.train_snli_ve - loss is tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2239/6700 [1:02:12<2:04:30,  1.67s/it]11/16/2022 23:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:21 - INFO - train.train_snli_ve - loss is tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2240/6700 [1:02:14<2:04:15,  1.67s/it]11/16/2022 23:56:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:23 - INFO - train.train_snli_ve - loss is tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2241/6700 [1:02:16<2:05:37,  1.69s/it]11/16/2022 23:56:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:25 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2242/6700 [1:02:18<2:05:53,  1.69s/it]11/16/2022 23:56:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:27 - INFO - train.train_snli_ve - loss is tensor(0.9582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2243/6700 [1:02:19<2:05:22,  1.69s/it]11/16/2022 23:56:28 - INFO - train.train_snli_ve - kd_loss is tensor(9.8507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:28 - INFO - train.train_snli_ve - loss is tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  33%|###3      | 2244/6700 [1:02:21<2:04:59,  1.68s/it]11/16/2022 23:56:30 - INFO - train.train_snli_ve - kd_loss is tensor(9.8462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:30 - INFO - train.train_snli_ve - loss is tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2245/6700 [1:02:23<2:04:24,  1.68s/it]11/16/2022 23:56:32 - INFO - train.train_snli_ve - kd_loss is tensor(8.3671e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:32 - INFO - train.train_snli_ve - loss is tensor(0.9070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2246/6700 [1:02:24<2:04:12,  1.67s/it]11/16/2022 23:56:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1159e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:33 - INFO - train.train_snli_ve - loss is tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2247/6700 [1:02:26<2:04:59,  1.68s/it]11/16/2022 23:56:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.7177e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:35 - INFO - train.train_snli_ve - loss is tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2248/6700 [1:02:28<2:05:23,  1.69s/it]11/16/2022 23:56:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.1940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:37 - INFO - train.train_snli_ve - loss is tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2249/6700 [1:02:29<2:05:59,  1.70s/it]11/16/2022 23:56:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.3667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:38 - INFO - train.train_snli_ve - loss is tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2250/6700 [1:02:31<2:06:04,  1.70s/it]11/16/2022 23:56:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.8467e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:40 - INFO - train.train_snli_ve - loss is tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2251/6700 [1:02:33<2:05:10,  1.69s/it]11/16/2022 23:56:42 - INFO - train.train_snli_ve - kd_loss is tensor(7.9183e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:42 - INFO - train.train_snli_ve - loss is tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2252/6700 [1:02:34<2:05:39,  1.70s/it]11/16/2022 23:56:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:43 - INFO - train.train_snli_ve - loss is tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2253/6700 [1:02:36<2:05:18,  1.69s/it]11/16/2022 23:56:45 - INFO - train.train_snli_ve - kd_loss is tensor(8.9691e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:45 - INFO - train.train_snli_ve - loss is tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2254/6700 [1:02:38<2:04:34,  1.68s/it]11/16/2022 23:56:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1268e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:47 - INFO - train.train_snli_ve - loss is tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2255/6700 [1:02:40<2:05:17,  1.69s/it]11/16/2022 23:56:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:49 - INFO - train.train_snli_ve - loss is tensor(0.8011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2256/6700 [1:02:41<2:05:29,  1.69s/it]11/16/2022 23:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.6085e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:50 - INFO - train.train_snli_ve - loss is tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2257/6700 [1:02:43<2:04:45,  1.68s/it]11/16/2022 23:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.8963e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:52 - INFO - train.train_snli_ve - loss is tensor(0.8485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2258/6700 [1:02:45<2:05:08,  1.69s/it]11/16/2022 23:56:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.3285e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:54 - INFO - train.train_snli_ve - loss is tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2259/6700 [1:02:46<2:04:46,  1.69s/it]11/16/2022 23:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.1042e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:55 - INFO - train.train_snli_ve - loss is tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2260/6700 [1:02:48<2:04:50,  1.69s/it]11/16/2022 23:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(7.2156e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:57 - INFO - train.train_snli_ve - loss is tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2261/6700 [1:02:50<2:05:01,  1.69s/it]11/16/2022 23:56:59 - INFO - train.train_snli_ve - kd_loss is tensor(8.8191e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:56:59 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2262/6700 [1:02:51<2:05:08,  1.69s/it]11/16/2022 23:57:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.4044e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:00 - INFO - train.train_snli_ve - loss is tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2263/6700 [1:02:53<2:04:39,  1.69s/it]11/16/2022 23:57:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.8237e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:02 - INFO - train.train_snli_ve - loss is tensor(0.4433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2264/6700 [1:02:55<2:04:53,  1.69s/it]11/16/2022 23:57:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:04 - INFO - train.train_snli_ve - loss is tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2265/6700 [1:02:56<2:04:18,  1.68s/it]11/16/2022 23:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.5046e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:05 - INFO - train.train_snli_ve - loss is tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2266/6700 [1:02:58<2:03:35,  1.67s/it]11/16/2022 23:57:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.1062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:07 - INFO - train.train_snli_ve - loss is tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2267/6700 [1:03:00<2:03:59,  1.68s/it]11/16/2022 23:57:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:09 - INFO - train.train_snli_ve - loss is tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2268/6700 [1:03:01<2:04:30,  1.69s/it]11/16/2022 23:57:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.0991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:10 - INFO - train.train_snli_ve - loss is tensor(0.9679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2269/6700 [1:03:03<2:04:40,  1.69s/it]11/16/2022 23:57:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:12 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2270/6700 [1:03:05<2:04:56,  1.69s/it]11/16/2022 23:57:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.6782e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:14 - INFO - train.train_snli_ve - loss is tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2271/6700 [1:03:07<2:05:19,  1.70s/it]11/16/2022 23:57:16 - INFO - train.train_snli_ve - kd_loss is tensor(9.5066e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:16 - INFO - train.train_snli_ve - loss is tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2272/6700 [1:03:08<2:05:21,  1.70s/it]11/16/2022 23:57:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:17 - INFO - train.train_snli_ve - loss is tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2273/6700 [1:03:10<2:05:20,  1.70s/it]11/16/2022 23:57:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:19 - INFO - train.train_snli_ve - loss is tensor(0.9259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2274/6700 [1:03:12<2:04:57,  1.69s/it]11/16/2022 23:57:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:21 - INFO - train.train_snli_ve - loss is tensor(0.8390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2275/6700 [1:03:13<2:04:47,  1.69s/it]11/16/2022 23:57:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:22 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2276/6700 [1:03:15<2:05:18,  1.70s/it]11/16/2022 23:57:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:24 - INFO - train.train_snli_ve - loss is tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###3      | 2277/6700 [1:03:17<2:05:19,  1.70s/it]11/16/2022 23:57:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:26 - INFO - train.train_snli_ve - loss is tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2278/6700 [1:03:18<2:05:43,  1.71s/it]11/16/2022 23:57:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.2774e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:27 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2279/6700 [1:03:20<2:06:09,  1.71s/it]11/16/2022 23:57:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:29 - INFO - train.train_snli_ve - loss is tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2280/6700 [1:03:22<2:06:56,  1.72s/it]11/16/2022 23:57:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:31 - INFO - train.train_snli_ve - loss is tensor(0.4342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2281/6700 [1:03:24<2:06:35,  1.72s/it]11/16/2022 23:57:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2150e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:33 - INFO - train.train_snli_ve - loss is tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2282/6700 [1:03:25<2:06:11,  1.71s/it]11/16/2022 23:57:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:34 - INFO - train.train_snli_ve - loss is tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2283/6700 [1:03:27<2:05:18,  1.70s/it]11/16/2022 23:57:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.1423e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:36 - INFO - train.train_snli_ve - loss is tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2284/6700 [1:03:29<2:03:56,  1.68s/it]11/16/2022 23:57:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:38 - INFO - train.train_snli_ve - loss is tensor(0.9477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2285/6700 [1:03:30<2:02:53,  1.67s/it]11/16/2022 23:57:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:39 - INFO - train.train_snli_ve - loss is tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2286/6700 [1:03:32<2:03:58,  1.69s/it]11/16/2022 23:57:41 - INFO - train.train_snli_ve - kd_loss is tensor(9.9362e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:41 - INFO - train.train_snli_ve - loss is tensor(0.8767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2287/6700 [1:03:34<2:03:20,  1.68s/it]11/16/2022 23:57:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:43 - INFO - train.train_snli_ve - loss is tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2288/6700 [1:03:35<2:03:33,  1.68s/it]11/16/2022 23:57:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:44 - INFO - train.train_snli_ve - loss is tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2289/6700 [1:03:37<2:03:36,  1.68s/it]11/16/2022 23:57:46 - INFO - train.train_snli_ve - kd_loss is tensor(8.7883e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:46 - INFO - train.train_snli_ve - loss is tensor(0.9525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2290/6700 [1:03:39<2:02:35,  1.67s/it]11/16/2022 23:57:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.1692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:48 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2291/6700 [1:03:40<2:02:08,  1.66s/it]11/16/2022 23:57:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:49 - INFO - train.train_snli_ve - loss is tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2292/6700 [1:03:42<2:01:52,  1.66s/it]11/16/2022 23:57:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.0800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:51 - INFO - train.train_snli_ve - loss is tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2293/6700 [1:03:44<2:02:16,  1.66s/it]11/16/2022 23:57:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0537e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:53 - INFO - train.train_snli_ve - loss is tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2294/6700 [1:03:45<2:02:12,  1.66s/it]11/16/2022 23:57:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.4265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:54 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2295/6700 [1:03:47<2:02:48,  1.67s/it]11/16/2022 23:57:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6755e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:56 - INFO - train.train_snli_ve - loss is tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2296/6700 [1:03:49<2:02:14,  1.67s/it]11/16/2022 23:57:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:58 - INFO - train.train_snli_ve - loss is tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2297/6700 [1:03:50<2:03:09,  1.68s/it]11/16/2022 23:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5544e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:57:59 - INFO - train.train_snli_ve - loss is tensor(0.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2298/6700 [1:03:52<2:02:40,  1.67s/it]11/16/2022 23:58:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.4730e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:01 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2299/6700 [1:03:54<2:02:21,  1.67s/it]11/16/2022 23:58:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4680e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:03 - INFO - train.train_snli_ve - loss is tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2300/6700 [1:03:55<2:03:52,  1.69s/it]11/16/2022 23:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.4096e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:04 - INFO - train.train_snli_ve - loss is tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2301/6700 [1:03:57<2:03:39,  1.69s/it]11/16/2022 23:58:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:06 - INFO - train.train_snli_ve - loss is tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2302/6700 [1:03:59<2:03:08,  1.68s/it]11/16/2022 23:58:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.1015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:08 - INFO - train.train_snli_ve - loss is tensor(0.8246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2303/6700 [1:04:00<2:03:26,  1.68s/it]11/16/2022 23:58:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.7550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:09 - INFO - train.train_snli_ve - loss is tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2304/6700 [1:04:02<2:03:25,  1.68s/it]11/16/2022 23:58:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:11 - INFO - train.train_snli_ve - loss is tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2305/6700 [1:04:04<2:03:53,  1.69s/it]11/16/2022 23:58:13 - INFO - train.train_snli_ve - kd_loss is tensor(9.7258e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:13 - INFO - train.train_snli_ve - loss is tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2306/6700 [1:04:06<2:03:41,  1.69s/it]11/16/2022 23:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(9.8206e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:15 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2307/6700 [1:04:07<2:03:33,  1.69s/it]11/16/2022 23:58:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:16 - INFO - train.train_snli_ve - loss is tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2308/6700 [1:04:09<2:03:20,  1.69s/it]11/16/2022 23:58:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.4443e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:18 - INFO - train.train_snli_ve - loss is tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2309/6700 [1:04:11<2:02:46,  1.68s/it]11/16/2022 23:58:20 - INFO - train.train_snli_ve - kd_loss is tensor(7.0055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:20 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2310/6700 [1:04:12<2:02:26,  1.67s/it]11/16/2022 23:58:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.0745e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:21 - INFO - train.train_snli_ve - loss is tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  34%|###4      | 2311/6700 [1:04:14<2:02:09,  1.67s/it]11/16/2022 23:58:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.5030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:23 - INFO - train.train_snli_ve - loss is tensor(0.3571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2312/6700 [1:04:16<2:01:47,  1.67s/it]11/16/2022 23:58:25 - INFO - train.train_snli_ve - kd_loss is tensor(8.1182e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:25 - INFO - train.train_snli_ve - loss is tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2313/6700 [1:04:17<2:02:50,  1.68s/it]11/16/2022 23:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:26 - INFO - train.train_snli_ve - loss is tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2314/6700 [1:04:19<2:03:08,  1.68s/it]11/16/2022 23:58:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.8376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:28 - INFO - train.train_snli_ve - loss is tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2315/6700 [1:04:21<2:03:12,  1.69s/it]11/16/2022 23:58:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:30 - INFO - train.train_snli_ve - loss is tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2316/6700 [1:04:22<2:03:33,  1.69s/it]11/16/2022 23:58:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.6482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:31 - INFO - train.train_snli_ve - loss is tensor(0.9880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2317/6700 [1:04:24<2:02:48,  1.68s/it]11/16/2022 23:58:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:33 - INFO - train.train_snli_ve - loss is tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2318/6700 [1:04:26<2:02:22,  1.68s/it]11/16/2022 23:58:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:35 - INFO - train.train_snli_ve - loss is tensor(0.4423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2319/6700 [1:04:27<2:02:12,  1.67s/it]11/16/2022 23:58:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:36 - INFO - train.train_snli_ve - loss is tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2320/6700 [1:04:29<2:03:11,  1.69s/it]11/16/2022 23:58:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0915e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:38 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2321/6700 [1:04:31<2:03:45,  1.70s/it]11/16/2022 23:58:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0897e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:40 - INFO - train.train_snli_ve - loss is tensor(0.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2322/6700 [1:04:32<2:03:09,  1.69s/it]11/16/2022 23:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:41 - INFO - train.train_snli_ve - loss is tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2323/6700 [1:04:34<2:02:45,  1.68s/it]11/16/2022 23:58:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.7635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:43 - INFO - train.train_snli_ve - loss is tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2324/6700 [1:04:36<2:01:52,  1.67s/it]11/16/2022 23:58:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.8302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:45 - INFO - train.train_snli_ve - loss is tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2325/6700 [1:04:37<2:01:58,  1.67s/it]11/16/2022 23:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(9.2317e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:46 - INFO - train.train_snli_ve - loss is tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2326/6700 [1:04:39<2:01:20,  1.66s/it]11/16/2022 23:58:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:48 - INFO - train.train_snli_ve - loss is tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2327/6700 [1:04:41<2:01:24,  1.67s/it]11/16/2022 23:58:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3878e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:50 - INFO - train.train_snli_ve - loss is tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2328/6700 [1:04:42<2:01:29,  1.67s/it]11/16/2022 23:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.8390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:51 - INFO - train.train_snli_ve - loss is tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2329/6700 [1:04:44<2:02:25,  1.68s/it]11/16/2022 23:58:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:53 - INFO - train.train_snli_ve - loss is tensor(0.9483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2330/6700 [1:04:46<2:01:42,  1.67s/it]11/16/2022 23:58:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:55 - INFO - train.train_snli_ve - loss is tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2331/6700 [1:04:47<2:01:42,  1.67s/it]11/16/2022 23:58:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6294e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:56 - INFO - train.train_snli_ve - loss is tensor(0.8601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2332/6700 [1:04:49<2:02:23,  1.68s/it]11/16/2022 23:58:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:58:58 - INFO - train.train_snli_ve - loss is tensor(0.8734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2333/6700 [1:04:51<2:02:58,  1.69s/it]11/16/2022 23:59:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:00 - INFO - train.train_snli_ve - loss is tensor(0.9108, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2334/6700 [1:04:53<2:02:57,  1.69s/it]11/16/2022 23:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.0171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:01 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2335/6700 [1:04:54<2:01:44,  1.67s/it]11/16/2022 23:59:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:03 - INFO - train.train_snli_ve - loss is tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2336/6700 [1:04:56<2:01:54,  1.68s/it]11/16/2022 23:59:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:05 - INFO - train.train_snli_ve - loss is tensor(0.9920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2337/6700 [1:04:58<2:02:28,  1.68s/it]11/16/2022 23:59:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9638e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:07 - INFO - train.train_snli_ve - loss is tensor(0.7736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2338/6700 [1:04:59<2:02:46,  1.69s/it]11/16/2022 23:59:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.6550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:08 - INFO - train.train_snli_ve - loss is tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2339/6700 [1:05:01<2:02:42,  1.69s/it]11/16/2022 23:59:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:10 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2340/6700 [1:05:03<2:03:07,  1.69s/it]11/16/2022 23:59:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:12 - INFO - train.train_snli_ve - loss is tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2341/6700 [1:05:04<2:02:25,  1.69s/it]11/16/2022 23:59:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:13 - INFO - train.train_snli_ve - loss is tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2342/6700 [1:05:06<2:03:32,  1.70s/it]11/16/2022 23:59:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:15 - INFO - train.train_snli_ve - loss is tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2343/6700 [1:05:08<2:02:49,  1.69s/it]11/16/2022 23:59:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:17 - INFO - train.train_snli_ve - loss is tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###4      | 2344/6700 [1:05:09<2:02:04,  1.68s/it]11/16/2022 23:59:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8399e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:18 - INFO - train.train_snli_ve - loss is tensor(0.7808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2345/6700 [1:05:11<2:02:32,  1.69s/it]11/16/2022 23:59:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.4459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:20 - INFO - train.train_snli_ve - loss is tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2346/6700 [1:05:13<2:02:40,  1.69s/it]11/16/2022 23:59:22 - INFO - train.train_snli_ve - kd_loss is tensor(7.7664e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:22 - INFO - train.train_snli_ve - loss is tensor(0.9332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2347/6700 [1:05:14<2:01:54,  1.68s/it]11/16/2022 23:59:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.3689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:24 - INFO - train.train_snli_ve - loss is tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2348/6700 [1:05:16<2:03:56,  1.71s/it]11/16/2022 23:59:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:25 - INFO - train.train_snli_ve - loss is tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2349/6700 [1:05:18<2:02:24,  1.69s/it]11/16/2022 23:59:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.0414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:27 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2350/6700 [1:05:20<2:02:30,  1.69s/it]11/16/2022 23:59:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:29 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2351/6700 [1:05:21<2:01:43,  1.68s/it]11/16/2022 23:59:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:30 - INFO - train.train_snli_ve - loss is tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2352/6700 [1:05:23<2:01:55,  1.68s/it]11/16/2022 23:59:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.1198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:32 - INFO - train.train_snli_ve - loss is tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2353/6700 [1:05:25<2:02:49,  1.70s/it]11/16/2022 23:59:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.5918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:34 - INFO - train.train_snli_ve - loss is tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2354/6700 [1:05:26<2:02:29,  1.69s/it]11/16/2022 23:59:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:35 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2355/6700 [1:05:28<2:02:35,  1.69s/it]11/16/2022 23:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:37 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2356/6700 [1:05:30<2:02:56,  1.70s/it]11/16/2022 23:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2951e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:39 - INFO - train.train_snli_ve - loss is tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2357/6700 [1:05:31<2:02:24,  1.69s/it]11/16/2022 23:59:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9420e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:40 - INFO - train.train_snli_ve - loss is tensor(0.8257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2358/6700 [1:05:33<2:02:28,  1.69s/it]11/16/2022 23:59:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.6667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:42 - INFO - train.train_snli_ve - loss is tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2359/6700 [1:05:35<2:02:03,  1.69s/it]11/16/2022 23:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.0418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:44 - INFO - train.train_snli_ve - loss is tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2360/6700 [1:05:36<2:02:00,  1.69s/it]11/16/2022 23:59:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:45 - INFO - train.train_snli_ve - loss is tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2361/6700 [1:05:38<2:02:14,  1.69s/it]11/16/2022 23:59:47 - INFO - train.train_snli_ve - kd_loss is tensor(8.9163e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:47 - INFO - train.train_snli_ve - loss is tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2362/6700 [1:05:40<2:01:54,  1.69s/it]11/16/2022 23:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.3901e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:49 - INFO - train.train_snli_ve - loss is tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2363/6700 [1:05:42<2:02:43,  1.70s/it]11/16/2022 23:59:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.5192e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:51 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2364/6700 [1:05:43<2:01:37,  1.68s/it]11/16/2022 23:59:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.6293e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:52 - INFO - train.train_snli_ve - loss is tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2365/6700 [1:05:45<2:02:24,  1.69s/it]11/16/2022 23:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.1961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:54 - INFO - train.train_snli_ve - loss is tensor(0.4780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2366/6700 [1:05:47<2:01:22,  1.68s/it]11/16/2022 23:59:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.0834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:56 - INFO - train.train_snli_ve - loss is tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2367/6700 [1:05:48<2:01:37,  1.68s/it]11/16/2022 23:59:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0614e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:57 - INFO - train.train_snli_ve - loss is tensor(0.5808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2368/6700 [1:05:50<2:01:15,  1.68s/it]11/16/2022 23:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/16/2022 23:59:59 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2369/6700 [1:05:52<2:02:27,  1.70s/it]11/17/2022 00:00:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:01 - INFO - train.train_snli_ve - loss is tensor(0.5827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2370/6700 [1:05:53<2:03:42,  1.71s/it]11/17/2022 00:00:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:02 - INFO - train.train_snli_ve - loss is tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2371/6700 [1:05:55<2:03:07,  1.71s/it]11/17/2022 00:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:04 - INFO - train.train_snli_ve - loss is tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2372/6700 [1:05:57<2:03:25,  1.71s/it]11/17/2022 00:00:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.9875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:06 - INFO - train.train_snli_ve - loss is tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2373/6700 [1:05:59<2:03:26,  1.71s/it]11/17/2022 00:00:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9684e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:08 - INFO - train.train_snli_ve - loss is tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2374/6700 [1:06:00<2:02:38,  1.70s/it]11/17/2022 00:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:09 - INFO - train.train_snli_ve - loss is tensor(0.9033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2375/6700 [1:06:02<2:02:08,  1.69s/it]11/17/2022 00:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:11 - INFO - train.train_snli_ve - loss is tensor(0.8773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2376/6700 [1:06:04<2:01:20,  1.68s/it]11/17/2022 00:00:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:13 - INFO - train.train_snli_ve - loss is tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2377/6700 [1:06:05<2:01:01,  1.68s/it]11/17/2022 00:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.5464e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:14 - INFO - train.train_snli_ve - loss is tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  35%|###5      | 2378/6700 [1:06:07<2:01:15,  1.68s/it]11/17/2022 00:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2969e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:16 - INFO - train.train_snli_ve - loss is tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2379/6700 [1:06:09<2:01:10,  1.68s/it]11/17/2022 00:00:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.1627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:18 - INFO - train.train_snli_ve - loss is tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2380/6700 [1:06:10<2:01:15,  1.68s/it]11/17/2022 00:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:19 - INFO - train.train_snli_ve - loss is tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2381/6700 [1:06:12<2:01:35,  1.69s/it]11/17/2022 00:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.6127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:21 - INFO - train.train_snli_ve - loss is tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2382/6700 [1:06:14<2:01:38,  1.69s/it]11/17/2022 00:00:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:23 - INFO - train.train_snli_ve - loss is tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2383/6700 [1:06:15<2:00:56,  1.68s/it]11/17/2022 00:00:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:24 - INFO - train.train_snli_ve - loss is tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2384/6700 [1:06:17<2:00:30,  1.68s/it]11/17/2022 00:00:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:26 - INFO - train.train_snli_ve - loss is tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2385/6700 [1:06:19<1:59:52,  1.67s/it]11/17/2022 00:00:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.7054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:28 - INFO - train.train_snli_ve - loss is tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2386/6700 [1:06:20<1:59:29,  1.66s/it]11/17/2022 00:00:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:29 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2387/6700 [1:06:22<2:00:00,  1.67s/it]11/17/2022 00:00:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.5318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:31 - INFO - train.train_snli_ve - loss is tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2388/6700 [1:06:24<2:00:15,  1.67s/it]11/17/2022 00:00:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:33 - INFO - train.train_snli_ve - loss is tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2389/6700 [1:06:25<2:01:05,  1.69s/it]11/17/2022 00:00:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:34 - INFO - train.train_snli_ve - loss is tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2390/6700 [1:06:27<2:01:44,  1.69s/it]11/17/2022 00:00:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:36 - INFO - train.train_snli_ve - loss is tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2391/6700 [1:06:29<2:00:24,  1.68s/it]11/17/2022 00:00:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1930e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:38 - INFO - train.train_snli_ve - loss is tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2392/6700 [1:06:30<2:00:44,  1.68s/it]11/17/2022 00:00:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.3824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:39 - INFO - train.train_snli_ve - loss is tensor(0.7967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2393/6700 [1:06:32<2:02:35,  1.71s/it]11/17/2022 00:00:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:41 - INFO - train.train_snli_ve - loss is tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2394/6700 [1:06:34<2:02:33,  1.71s/it]11/17/2022 00:00:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:43 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2395/6700 [1:06:36<2:02:24,  1.71s/it]11/17/2022 00:00:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.8898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:45 - INFO - train.train_snli_ve - loss is tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2396/6700 [1:06:37<2:02:17,  1.70s/it]11/17/2022 00:00:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:46 - INFO - train.train_snli_ve - loss is tensor(0.9625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2397/6700 [1:06:39<2:01:06,  1.69s/it]11/17/2022 00:00:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:48 - INFO - train.train_snli_ve - loss is tensor(0.8442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2398/6700 [1:06:41<2:02:00,  1.70s/it]11/17/2022 00:00:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.0244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:50 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2399/6700 [1:06:42<2:01:21,  1.69s/it]11/17/2022 00:00:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.1705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:51 - INFO - train.train_snli_ve - loss is tensor(0.9997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2400/6700 [1:06:44<2:01:09,  1.69s/it]11/17/2022 00:00:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.1118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:53 - INFO - train.train_snli_ve - loss is tensor(0.9550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2401/6700 [1:06:46<2:00:40,  1.68s/it]11/17/2022 00:00:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.4474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:55 - INFO - train.train_snli_ve - loss is tensor(0.9316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2402/6700 [1:06:47<2:00:21,  1.68s/it]11/17/2022 00:00:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.4235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:56 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2403/6700 [1:06:49<2:01:45,  1.70s/it]11/17/2022 00:00:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.8306e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:00:58 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2404/6700 [1:06:51<2:02:08,  1.71s/it]11/17/2022 00:01:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6950e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:00 - INFO - train.train_snli_ve - loss is tensor(0.9004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2405/6700 [1:06:53<2:01:53,  1.70s/it]11/17/2022 00:01:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:02 - INFO - train.train_snli_ve - loss is tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2406/6700 [1:06:54<2:01:36,  1.70s/it]11/17/2022 00:01:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3536e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:03 - INFO - train.train_snli_ve - loss is tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2407/6700 [1:06:56<2:00:39,  1.69s/it]11/17/2022 00:01:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1253e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:05 - INFO - train.train_snli_ve - loss is tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2408/6700 [1:06:58<2:01:08,  1.69s/it]11/17/2022 00:01:07 - INFO - train.train_snli_ve - kd_loss is tensor(9.8551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:07 - INFO - train.train_snli_ve - loss is tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2409/6700 [1:06:59<2:00:43,  1.69s/it]11/17/2022 00:01:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.1626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:08 - INFO - train.train_snli_ve - loss is tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2410/6700 [1:07:01<2:00:57,  1.69s/it]11/17/2022 00:01:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:10 - INFO - train.train_snli_ve - loss is tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###5      | 2411/6700 [1:07:03<2:00:47,  1.69s/it]11/17/2022 00:01:12 - INFO - train.train_snli_ve - kd_loss is tensor(7.0162e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:12 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2412/6700 [1:07:04<2:01:18,  1.70s/it]11/17/2022 00:01:13 - INFO - train.train_snli_ve - kd_loss is tensor(6.0049e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:13 - INFO - train.train_snli_ve - loss is tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2413/6700 [1:07:06<2:00:38,  1.69s/it]11/17/2022 00:01:15 - INFO - train.train_snli_ve - kd_loss is tensor(6.8737e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:15 - INFO - train.train_snli_ve - loss is tensor(0.8776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2414/6700 [1:07:08<2:00:37,  1.69s/it]11/17/2022 00:01:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0687e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:17 - INFO - train.train_snli_ve - loss is tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2415/6700 [1:07:09<2:00:19,  1.68s/it]11/17/2022 00:01:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.7440e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:18 - INFO - train.train_snli_ve - loss is tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2416/6700 [1:07:11<1:59:45,  1.68s/it]11/17/2022 00:01:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:20 - INFO - train.train_snli_ve - loss is tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2417/6700 [1:07:13<1:59:39,  1.68s/it]11/17/2022 00:01:22 - INFO - train.train_snli_ve - kd_loss is tensor(9.1501e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:22 - INFO - train.train_snli_ve - loss is tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2418/6700 [1:07:14<1:59:44,  1.68s/it]11/17/2022 00:01:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4593e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:23 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2419/6700 [1:07:16<1:59:40,  1.68s/it]11/17/2022 00:01:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0011e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:25 - INFO - train.train_snli_ve - loss is tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2420/6700 [1:07:18<2:00:01,  1.68s/it]11/17/2022 00:01:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.8752e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:27 - INFO - train.train_snli_ve - loss is tensor(0.9159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2421/6700 [1:07:19<1:59:35,  1.68s/it]11/17/2022 00:01:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:28 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2422/6700 [1:07:21<1:59:11,  1.67s/it]11/17/2022 00:01:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.3512e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:30 - INFO - train.train_snli_ve - loss is tensor(0.4605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2423/6700 [1:07:23<1:59:24,  1.68s/it]11/17/2022 00:01:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9951e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:32 - INFO - train.train_snli_ve - loss is tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2424/6700 [1:07:25<1:59:55,  1.68s/it]11/17/2022 00:01:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:34 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2425/6700 [1:07:26<2:01:11,  1.70s/it]11/17/2022 00:01:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:35 - INFO - train.train_snli_ve - loss is tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2426/6700 [1:07:28<2:01:14,  1.70s/it]11/17/2022 00:01:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:37 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2427/6700 [1:07:30<2:00:12,  1.69s/it]11/17/2022 00:01:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.3286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:39 - INFO - train.train_snli_ve - loss is tensor(0.4491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2428/6700 [1:07:31<2:01:39,  1.71s/it]11/17/2022 00:01:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:40 - INFO - train.train_snli_ve - loss is tensor(0.5146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2429/6700 [1:07:33<2:00:39,  1.70s/it]11/17/2022 00:01:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:42 - INFO - train.train_snli_ve - loss is tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2430/6700 [1:07:35<2:00:22,  1.69s/it]11/17/2022 00:01:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.5019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:44 - INFO - train.train_snli_ve - loss is tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2431/6700 [1:07:36<1:59:30,  1.68s/it]11/17/2022 00:01:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:45 - INFO - train.train_snli_ve - loss is tensor(0.8406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2432/6700 [1:07:38<1:59:06,  1.67s/it]11/17/2022 00:01:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:47 - INFO - train.train_snli_ve - loss is tensor(0.4174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2433/6700 [1:07:40<1:58:50,  1.67s/it]11/17/2022 00:01:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:49 - INFO - train.train_snli_ve - loss is tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2434/6700 [1:07:41<1:59:19,  1.68s/it]11/17/2022 00:01:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8949e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:50 - INFO - train.train_snli_ve - loss is tensor(1.0399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2435/6700 [1:07:43<1:59:52,  1.69s/it]11/17/2022 00:01:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:52 - INFO - train.train_snli_ve - loss is tensor(0.8745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2436/6700 [1:07:45<1:59:31,  1.68s/it]11/17/2022 00:01:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.3828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:54 - INFO - train.train_snli_ve - loss is tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2437/6700 [1:07:46<1:59:54,  1.69s/it]11/17/2022 00:01:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.6282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:55 - INFO - train.train_snli_ve - loss is tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2438/6700 [1:07:48<1:59:22,  1.68s/it]11/17/2022 00:01:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:57 - INFO - train.train_snli_ve - loss is tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2439/6700 [1:07:50<1:58:21,  1.67s/it]11/17/2022 00:01:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:01:59 - INFO - train.train_snli_ve - loss is tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2440/6700 [1:07:51<1:59:36,  1.68s/it]11/17/2022 00:02:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.5121e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:00 - INFO - train.train_snli_ve - loss is tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2441/6700 [1:07:53<1:59:17,  1.68s/it]11/17/2022 00:02:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:02 - INFO - train.train_snli_ve - loss is tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2442/6700 [1:07:55<1:58:48,  1.67s/it]11/17/2022 00:02:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:04 - INFO - train.train_snli_ve - loss is tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2443/6700 [1:07:57<1:59:00,  1.68s/it]11/17/2022 00:02:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:05 - INFO - train.train_snli_ve - loss is tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2444/6700 [1:07:58<1:58:47,  1.67s/it]11/17/2022 00:02:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:07 - INFO - train.train_snli_ve - loss is tensor(0.8398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  36%|###6      | 2445/6700 [1:08:00<2:00:30,  1.70s/it]11/17/2022 00:02:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.7867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:09 - INFO - train.train_snli_ve - loss is tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2446/6700 [1:08:02<2:00:44,  1.70s/it]11/17/2022 00:02:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.8562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:11 - INFO - train.train_snli_ve - loss is tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2447/6700 [1:08:03<1:59:52,  1.69s/it]11/17/2022 00:02:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:12 - INFO - train.train_snli_ve - loss is tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2448/6700 [1:08:05<1:58:57,  1.68s/it]11/17/2022 00:02:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0861e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:14 - INFO - train.train_snli_ve - loss is tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2449/6700 [1:08:07<1:59:03,  1.68s/it]11/17/2022 00:02:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.2998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:16 - INFO - train.train_snli_ve - loss is tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2450/6700 [1:08:08<1:59:47,  1.69s/it]11/17/2022 00:02:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:17 - INFO - train.train_snli_ve - loss is tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2451/6700 [1:08:10<1:59:04,  1.68s/it]11/17/2022 00:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:19 - INFO - train.train_snli_ve - loss is tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2452/6700 [1:08:12<1:59:32,  1.69s/it]11/17/2022 00:02:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4064e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:21 - INFO - train.train_snli_ve - loss is tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2453/6700 [1:08:13<2:00:25,  1.70s/it]11/17/2022 00:02:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.1567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:22 - INFO - train.train_snli_ve - loss is tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2454/6700 [1:08:15<1:58:52,  1.68s/it]11/17/2022 00:02:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.9291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:24 - INFO - train.train_snli_ve - loss is tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2455/6700 [1:08:17<1:57:50,  1.67s/it]11/17/2022 00:02:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6125e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:26 - INFO - train.train_snli_ve - loss is tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2456/6700 [1:08:18<1:57:52,  1.67s/it]11/17/2022 00:02:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.4509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:27 - INFO - train.train_snli_ve - loss is tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2457/6700 [1:08:20<1:57:41,  1.66s/it]11/17/2022 00:02:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:29 - INFO - train.train_snli_ve - loss is tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2458/6700 [1:08:22<1:58:31,  1.68s/it]11/17/2022 00:02:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:31 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2459/6700 [1:08:23<1:58:28,  1.68s/it]11/17/2022 00:02:32 - INFO - train.train_snli_ve - kd_loss is tensor(9.6615e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:32 - INFO - train.train_snli_ve - loss is tensor(0.4858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2460/6700 [1:08:25<1:59:29,  1.69s/it]11/17/2022 00:02:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.2813e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:34 - INFO - train.train_snli_ve - loss is tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2461/6700 [1:08:27<1:58:50,  1.68s/it]11/17/2022 00:02:36 - INFO - train.train_snli_ve - kd_loss is tensor(8.8462e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:36 - INFO - train.train_snli_ve - loss is tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2462/6700 [1:08:28<1:58:29,  1.68s/it]11/17/2022 00:02:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.3720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:37 - INFO - train.train_snli_ve - loss is tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2463/6700 [1:08:30<1:58:34,  1.68s/it]11/17/2022 00:02:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:39 - INFO - train.train_snli_ve - loss is tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2464/6700 [1:08:32<1:58:13,  1.67s/it]11/17/2022 00:02:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:41 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2465/6700 [1:08:34<1:58:35,  1.68s/it]11/17/2022 00:02:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:43 - INFO - train.train_snli_ve - loss is tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2466/6700 [1:08:35<1:59:09,  1.69s/it]11/17/2022 00:02:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:44 - INFO - train.train_snli_ve - loss is tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2467/6700 [1:08:37<1:58:54,  1.69s/it]11/17/2022 00:02:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:46 - INFO - train.train_snli_ve - loss is tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2468/6700 [1:08:39<1:58:20,  1.68s/it]11/17/2022 00:02:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.2992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:48 - INFO - train.train_snli_ve - loss is tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2469/6700 [1:08:40<1:57:33,  1.67s/it]11/17/2022 00:02:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.1526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:49 - INFO - train.train_snli_ve - loss is tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2470/6700 [1:08:42<1:58:55,  1.69s/it]11/17/2022 00:02:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:51 - INFO - train.train_snli_ve - loss is tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2471/6700 [1:08:44<1:58:21,  1.68s/it]11/17/2022 00:02:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.7024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:53 - INFO - train.train_snli_ve - loss is tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2472/6700 [1:08:45<1:58:13,  1.68s/it]11/17/2022 00:02:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:54 - INFO - train.train_snli_ve - loss is tensor(0.8585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2473/6700 [1:08:47<1:58:33,  1.68s/it]11/17/2022 00:02:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:56 - INFO - train.train_snli_ve - loss is tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2474/6700 [1:08:49<1:58:10,  1.68s/it]11/17/2022 00:02:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.8898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:58 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2475/6700 [1:08:50<1:57:40,  1.67s/it]11/17/2022 00:02:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:02:59 - INFO - train.train_snli_ve - loss is tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2476/6700 [1:08:52<1:57:43,  1.67s/it]11/17/2022 00:03:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:01 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2477/6700 [1:08:54<1:58:33,  1.68s/it]11/17/2022 00:03:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:03 - INFO - train.train_snli_ve - loss is tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###6      | 2478/6700 [1:08:55<1:57:39,  1.67s/it]11/17/2022 00:03:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:04 - INFO - train.train_snli_ve - loss is tensor(0.9260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2479/6700 [1:08:57<1:56:28,  1.66s/it]11/17/2022 00:03:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:06 - INFO - train.train_snli_ve - loss is tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2480/6700 [1:08:59<1:56:23,  1.65s/it]11/17/2022 00:03:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:08 - INFO - train.train_snli_ve - loss is tensor(1.0358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2481/6700 [1:09:00<1:56:36,  1.66s/it]11/17/2022 00:03:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:09 - INFO - train.train_snli_ve - loss is tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2482/6700 [1:09:02<1:56:39,  1.66s/it]11/17/2022 00:03:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7022e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:11 - INFO - train.train_snli_ve - loss is tensor(0.5057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2483/6700 [1:09:04<1:56:45,  1.66s/it]11/17/2022 00:03:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:13 - INFO - train.train_snli_ve - loss is tensor(0.8479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2484/6700 [1:09:05<1:58:23,  1.68s/it]11/17/2022 00:03:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:14 - INFO - train.train_snli_ve - loss is tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2485/6700 [1:09:07<1:57:49,  1.68s/it]11/17/2022 00:03:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:16 - INFO - train.train_snli_ve - loss is tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2486/6700 [1:09:09<1:57:45,  1.68s/it]11/17/2022 00:03:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.3706e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:18 - INFO - train.train_snli_ve - loss is tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2487/6700 [1:09:10<1:57:53,  1.68s/it]11/17/2022 00:03:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.9070e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:19 - INFO - train.train_snli_ve - loss is tensor(0.5551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2488/6700 [1:09:12<1:58:11,  1.68s/it]11/17/2022 00:03:21 - INFO - train.train_snli_ve - kd_loss is tensor(8.4019e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:21 - INFO - train.train_snli_ve - loss is tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2489/6700 [1:09:14<1:58:08,  1.68s/it]11/17/2022 00:03:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1741e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:23 - INFO - train.train_snli_ve - loss is tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2490/6700 [1:09:15<1:58:18,  1.69s/it]11/17/2022 00:03:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.1681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:24 - INFO - train.train_snli_ve - loss is tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2491/6700 [1:09:17<1:58:28,  1.69s/it]11/17/2022 00:03:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:26 - INFO - train.train_snli_ve - loss is tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2492/6700 [1:09:19<1:59:11,  1.70s/it]11/17/2022 00:03:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.9294e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:28 - INFO - train.train_snli_ve - loss is tensor(1.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2493/6700 [1:09:21<1:58:37,  1.69s/it]11/17/2022 00:03:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.2144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:30 - INFO - train.train_snli_ve - loss is tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2494/6700 [1:09:22<1:59:26,  1.70s/it]11/17/2022 00:03:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:31 - INFO - train.train_snli_ve - loss is tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2495/6700 [1:09:24<1:58:34,  1.69s/it]11/17/2022 00:03:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7757e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:33 - INFO - train.train_snli_ve - loss is tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2496/6700 [1:09:26<1:58:30,  1.69s/it]11/17/2022 00:03:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:35 - INFO - train.train_snli_ve - loss is tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2497/6700 [1:09:27<1:57:54,  1.68s/it]11/17/2022 00:03:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5252e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:36 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2498/6700 [1:09:29<1:56:26,  1.66s/it]11/17/2022 00:03:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:38 - INFO - train.train_snli_ve - loss is tensor(0.8772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2499/6700 [1:09:31<1:56:11,  1.66s/it]11/17/2022 00:03:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.2024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:40 - INFO - train.train_snli_ve - loss is tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2500/6700 [1:09:32<1:57:13,  1.67s/it]11/17/2022 00:03:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:41 - INFO - train.train_snli_ve - loss is tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2501/6700 [1:09:34<1:58:42,  1.70s/it]11/17/2022 00:03:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:43 - INFO - train.train_snli_ve - loss is tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2502/6700 [1:09:36<1:57:59,  1.69s/it]11/17/2022 00:03:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:45 - INFO - train.train_snli_ve - loss is tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2503/6700 [1:09:37<1:57:37,  1.68s/it]11/17/2022 00:03:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8763e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:46 - INFO - train.train_snli_ve - loss is tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2504/6700 [1:09:39<1:56:48,  1.67s/it]11/17/2022 00:03:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:48 - INFO - train.train_snli_ve - loss is tensor(0.8855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2505/6700 [1:09:41<1:57:09,  1.68s/it]11/17/2022 00:03:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:50 - INFO - train.train_snli_ve - loss is tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2506/6700 [1:09:42<1:58:11,  1.69s/it]11/17/2022 00:03:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:51 - INFO - train.train_snli_ve - loss is tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2507/6700 [1:09:44<1:57:20,  1.68s/it]11/17/2022 00:03:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.2116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:53 - INFO - train.train_snli_ve - loss is tensor(0.4359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2508/6700 [1:09:46<1:57:48,  1.69s/it]11/17/2022 00:03:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:55 - INFO - train.train_snli_ve - loss is tensor(0.9787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2509/6700 [1:09:47<1:57:31,  1.68s/it]11/17/2022 00:03:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:56 - INFO - train.train_snli_ve - loss is tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2510/6700 [1:09:49<1:57:26,  1.68s/it]11/17/2022 00:03:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.8644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:03:58 - INFO - train.train_snli_ve - loss is tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2511/6700 [1:09:51<1:57:05,  1.68s/it]11/17/2022 00:04:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.7596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:00 - INFO - train.train_snli_ve - loss is tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  37%|###7      | 2512/6700 [1:09:52<1:57:27,  1.68s/it]11/17/2022 00:04:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:01 - INFO - train.train_snli_ve - loss is tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2513/6700 [1:09:54<1:57:40,  1.69s/it]11/17/2022 00:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:03 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2514/6700 [1:09:56<1:56:56,  1.68s/it]11/17/2022 00:04:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:05 - INFO - train.train_snli_ve - loss is tensor(0.7866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2515/6700 [1:09:57<1:56:36,  1.67s/it]11/17/2022 00:04:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.6581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:06 - INFO - train.train_snli_ve - loss is tensor(0.8262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2516/6700 [1:09:59<1:56:25,  1.67s/it]11/17/2022 00:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:08 - INFO - train.train_snli_ve - loss is tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2517/6700 [1:10:01<1:56:26,  1.67s/it]11/17/2022 00:04:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:10 - INFO - train.train_snli_ve - loss is tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2518/6700 [1:10:02<1:56:26,  1.67s/it]11/17/2022 00:04:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.2268e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:11 - INFO - train.train_snli_ve - loss is tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2519/6700 [1:10:04<1:56:53,  1.68s/it]11/17/2022 00:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2476e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:13 - INFO - train.train_snli_ve - loss is tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2520/6700 [1:10:06<1:57:02,  1.68s/it]11/17/2022 00:04:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:15 - INFO - train.train_snli_ve - loss is tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2521/6700 [1:10:08<1:57:34,  1.69s/it]11/17/2022 00:04:17 - INFO - train.train_snli_ve - kd_loss is tensor(9.4931e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:17 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2522/6700 [1:10:09<1:57:00,  1.68s/it]11/17/2022 00:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(9.1957e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:18 - INFO - train.train_snli_ve - loss is tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2523/6700 [1:10:11<1:56:01,  1.67s/it]11/17/2022 00:04:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3566e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:20 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2524/6700 [1:10:13<1:56:49,  1.68s/it]11/17/2022 00:04:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.3466e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:22 - INFO - train.train_snli_ve - loss is tensor(0.7821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2525/6700 [1:10:14<1:57:35,  1.69s/it]11/17/2022 00:04:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.2583e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:23 - INFO - train.train_snli_ve - loss is tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2526/6700 [1:10:16<1:56:47,  1.68s/it]11/17/2022 00:04:25 - INFO - train.train_snli_ve - kd_loss is tensor(9.8580e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:25 - INFO - train.train_snli_ve - loss is tensor(0.7221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2527/6700 [1:10:18<1:57:34,  1.69s/it]11/17/2022 00:04:27 - INFO - train.train_snli_ve - kd_loss is tensor(9.8055e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:27 - INFO - train.train_snli_ve - loss is tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2528/6700 [1:10:19<1:56:46,  1.68s/it]11/17/2022 00:04:28 - INFO - train.train_snli_ve - kd_loss is tensor(7.2030e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:28 - INFO - train.train_snli_ve - loss is tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2529/6700 [1:10:21<1:57:00,  1.68s/it]11/17/2022 00:04:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7954e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:30 - INFO - train.train_snli_ve - loss is tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2530/6700 [1:10:23<1:56:39,  1.68s/it]11/17/2022 00:04:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:32 - INFO - train.train_snli_ve - loss is tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2531/6700 [1:10:24<1:56:29,  1.68s/it]11/17/2022 00:04:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:33 - INFO - train.train_snli_ve - loss is tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2532/6700 [1:10:26<1:57:14,  1.69s/it]11/17/2022 00:04:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3299e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:35 - INFO - train.train_snli_ve - loss is tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2533/6700 [1:10:28<1:57:40,  1.69s/it]11/17/2022 00:04:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4130e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:37 - INFO - train.train_snli_ve - loss is tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2534/6700 [1:10:29<1:57:30,  1.69s/it]11/17/2022 00:04:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:38 - INFO - train.train_snli_ve - loss is tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2535/6700 [1:10:31<1:57:35,  1.69s/it]11/17/2022 00:04:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.0599e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:40 - INFO - train.train_snli_ve - loss is tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2536/6700 [1:10:33<1:56:54,  1.68s/it]11/17/2022 00:04:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.2765e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:42 - INFO - train.train_snli_ve - loss is tensor(0.8743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2537/6700 [1:10:34<1:56:27,  1.68s/it]11/17/2022 00:04:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:43 - INFO - train.train_snli_ve - loss is tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2538/6700 [1:10:36<1:56:36,  1.68s/it]11/17/2022 00:04:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.0051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:45 - INFO - train.train_snli_ve - loss is tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2539/6700 [1:10:38<1:56:40,  1.68s/it]11/17/2022 00:04:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1908e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:47 - INFO - train.train_snli_ve - loss is tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2540/6700 [1:10:40<1:57:03,  1.69s/it]11/17/2022 00:04:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:48 - INFO - train.train_snli_ve - loss is tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2541/6700 [1:10:41<1:56:00,  1.67s/it]11/17/2022 00:04:50 - INFO - train.train_snli_ve - kd_loss is tensor(7.1507e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:50 - INFO - train.train_snli_ve - loss is tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2542/6700 [1:10:43<1:56:48,  1.69s/it]11/17/2022 00:04:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:52 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2543/6700 [1:10:45<1:56:34,  1.68s/it]11/17/2022 00:04:54 - INFO - train.train_snli_ve - kd_loss is tensor(8.1908e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:54 - INFO - train.train_snli_ve - loss is tensor(0.8336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2544/6700 [1:10:46<1:56:58,  1.69s/it]11/17/2022 00:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:55 - INFO - train.train_snli_ve - loss is tensor(0.8567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###7      | 2545/6700 [1:10:48<1:57:15,  1.69s/it]11/17/2022 00:04:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:57 - INFO - train.train_snli_ve - loss is tensor(0.8541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2546/6700 [1:10:50<1:57:21,  1.70s/it]11/17/2022 00:04:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6130e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:04:59 - INFO - train.train_snli_ve - loss is tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2547/6700 [1:10:51<1:56:58,  1.69s/it]11/17/2022 00:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:00 - INFO - train.train_snli_ve - loss is tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2548/6700 [1:10:53<1:57:39,  1.70s/it]11/17/2022 00:05:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:02 - INFO - train.train_snli_ve - loss is tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2549/6700 [1:10:55<1:58:40,  1.72s/it]11/17/2022 00:05:04 - INFO - train.train_snli_ve - kd_loss is tensor(7.9147e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:04 - INFO - train.train_snli_ve - loss is tensor(0.8811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2550/6700 [1:10:57<1:58:01,  1.71s/it]11/17/2022 00:05:05 - INFO - train.train_snli_ve - kd_loss is tensor(9.9446e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:05 - INFO - train.train_snli_ve - loss is tensor(0.8602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2551/6700 [1:10:58<1:57:33,  1.70s/it]11/17/2022 00:05:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.4382e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:07 - INFO - train.train_snli_ve - loss is tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2552/6700 [1:11:00<1:58:02,  1.71s/it]11/17/2022 00:05:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1555e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:09 - INFO - train.train_snli_ve - loss is tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2553/6700 [1:11:02<1:57:38,  1.70s/it]11/17/2022 00:05:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5716e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:11 - INFO - train.train_snli_ve - loss is tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2554/6700 [1:11:03<1:57:33,  1.70s/it]11/17/2022 00:05:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:12 - INFO - train.train_snli_ve - loss is tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2555/6700 [1:11:05<1:56:46,  1.69s/it]11/17/2022 00:05:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:14 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2556/6700 [1:11:07<1:57:01,  1.69s/it]11/17/2022 00:05:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:16 - INFO - train.train_snli_ve - loss is tensor(0.5193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2557/6700 [1:11:08<1:56:17,  1.68s/it]11/17/2022 00:05:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:17 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2558/6700 [1:11:10<1:56:07,  1.68s/it]11/17/2022 00:05:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:19 - INFO - train.train_snli_ve - loss is tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2559/6700 [1:11:12<1:56:36,  1.69s/it]11/17/2022 00:05:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.3955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:21 - INFO - train.train_snli_ve - loss is tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2560/6700 [1:11:13<1:57:02,  1.70s/it]11/17/2022 00:05:22 - INFO - train.train_snli_ve - kd_loss is tensor(6.3247e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:22 - INFO - train.train_snli_ve - loss is tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2561/6700 [1:11:15<1:57:02,  1.70s/it]11/17/2022 00:05:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.5471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:24 - INFO - train.train_snli_ve - loss is tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2562/6700 [1:11:17<1:56:38,  1.69s/it]11/17/2022 00:05:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.0167e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:26 - INFO - train.train_snli_ve - loss is tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2563/6700 [1:11:19<1:56:46,  1.69s/it]11/17/2022 00:05:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:27 - INFO - train.train_snli_ve - loss is tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2564/6700 [1:11:20<1:55:57,  1.68s/it]11/17/2022 00:05:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9258e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:29 - INFO - train.train_snli_ve - loss is tensor(0.8680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2565/6700 [1:11:22<1:55:57,  1.68s/it]11/17/2022 00:05:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:31 - INFO - train.train_snli_ve - loss is tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2566/6700 [1:11:24<1:56:00,  1.68s/it]11/17/2022 00:05:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:32 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2567/6700 [1:11:25<1:55:18,  1.67s/it]11/17/2022 00:05:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:34 - INFO - train.train_snli_ve - loss is tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2568/6700 [1:11:27<1:55:15,  1.67s/it]11/17/2022 00:05:36 - INFO - train.train_snli_ve - kd_loss is tensor(9.4226e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:36 - INFO - train.train_snli_ve - loss is tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2569/6700 [1:11:29<1:54:51,  1.67s/it]11/17/2022 00:05:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.0589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:38 - INFO - train.train_snli_ve - loss is tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2570/6700 [1:11:30<1:56:15,  1.69s/it]11/17/2022 00:05:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:39 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2571/6700 [1:11:32<1:55:48,  1.68s/it]11/17/2022 00:05:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:41 - INFO - train.train_snli_ve - loss is tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2572/6700 [1:11:34<1:55:44,  1.68s/it]11/17/2022 00:05:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1779e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:43 - INFO - train.train_snli_ve - loss is tensor(0.9508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2573/6700 [1:11:35<1:55:03,  1.67s/it]11/17/2022 00:05:44 - INFO - train.train_snli_ve - kd_loss is tensor(8.0385e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:44 - INFO - train.train_snli_ve - loss is tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2574/6700 [1:11:37<1:55:47,  1.68s/it]11/17/2022 00:05:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.0059e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:46 - INFO - train.train_snli_ve - loss is tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2575/6700 [1:11:39<1:56:02,  1.69s/it]11/17/2022 00:05:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.3125e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:48 - INFO - train.train_snli_ve - loss is tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2576/6700 [1:11:40<1:55:55,  1.69s/it]11/17/2022 00:05:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:49 - INFO - train.train_snli_ve - loss is tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2577/6700 [1:11:42<1:55:41,  1.68s/it]11/17/2022 00:05:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7473e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:51 - INFO - train.train_snli_ve - loss is tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2578/6700 [1:11:44<1:55:33,  1.68s/it]11/17/2022 00:05:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:53 - INFO - train.train_snli_ve - loss is tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  38%|###8      | 2579/6700 [1:11:45<1:55:28,  1.68s/it]11/17/2022 00:05:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:54 - INFO - train.train_snli_ve - loss is tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2580/6700 [1:11:47<1:55:56,  1.69s/it]11/17/2022 00:05:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:56 - INFO - train.train_snli_ve - loss is tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2581/6700 [1:11:49<1:55:53,  1.69s/it]11/17/2022 00:05:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:05:58 - INFO - train.train_snli_ve - loss is tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2582/6700 [1:11:50<1:56:08,  1.69s/it]11/17/2022 00:06:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.1425e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:00 - INFO - train.train_snli_ve - loss is tensor(0.4772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2583/6700 [1:11:52<1:57:27,  1.71s/it]11/17/2022 00:06:01 - INFO - train.train_snli_ve - kd_loss is tensor(7.0767e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:01 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2584/6700 [1:11:54<1:56:35,  1.70s/it]11/17/2022 00:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.2561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:03 - INFO - train.train_snli_ve - loss is tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2585/6700 [1:11:56<1:55:44,  1.69s/it]11/17/2022 00:06:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:05 - INFO - train.train_snli_ve - loss is tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2586/6700 [1:11:57<1:55:52,  1.69s/it]11/17/2022 00:06:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:06 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2587/6700 [1:11:59<1:55:28,  1.68s/it]11/17/2022 00:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.4981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:08 - INFO - train.train_snli_ve - loss is tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2588/6700 [1:12:01<1:55:27,  1.68s/it]11/17/2022 00:06:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1591e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:10 - INFO - train.train_snli_ve - loss is tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2589/6700 [1:12:02<1:56:40,  1.70s/it]11/17/2022 00:06:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:11 - INFO - train.train_snli_ve - loss is tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2590/6700 [1:12:04<1:55:50,  1.69s/it]11/17/2022 00:06:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:13 - INFO - train.train_snli_ve - loss is tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2591/6700 [1:12:06<1:55:01,  1.68s/it]11/17/2022 00:06:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.7840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:15 - INFO - train.train_snli_ve - loss is tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2592/6700 [1:12:07<1:54:47,  1.68s/it]11/17/2022 00:06:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5335e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:16 - INFO - train.train_snli_ve - loss is tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2593/6700 [1:12:09<1:54:43,  1.68s/it]11/17/2022 00:06:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:18 - INFO - train.train_snli_ve - loss is tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2594/6700 [1:12:11<1:54:39,  1.68s/it]11/17/2022 00:06:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:20 - INFO - train.train_snli_ve - loss is tensor(0.8893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2595/6700 [1:12:12<1:54:17,  1.67s/it]11/17/2022 00:06:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:21 - INFO - train.train_snli_ve - loss is tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2596/6700 [1:12:14<1:55:13,  1.68s/it]11/17/2022 00:06:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.8095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:23 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2597/6700 [1:12:16<1:54:44,  1.68s/it]11/17/2022 00:06:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:25 - INFO - train.train_snli_ve - loss is tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2598/6700 [1:12:17<1:54:25,  1.67s/it]11/17/2022 00:06:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:26 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2599/6700 [1:12:19<1:54:29,  1.68s/it]11/17/2022 00:06:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:28 - INFO - train.train_snli_ve - loss is tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2600/6700 [1:12:21<1:54:51,  1.68s/it]11/17/2022 00:06:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:30 - INFO - train.train_snli_ve - loss is tensor(0.8159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2601/6700 [1:12:22<1:55:13,  1.69s/it]11/17/2022 00:06:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.1084e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:31 - INFO - train.train_snli_ve - loss is tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2602/6700 [1:12:24<1:54:25,  1.68s/it]11/17/2022 00:06:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:33 - INFO - train.train_snli_ve - loss is tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2603/6700 [1:12:26<1:54:18,  1.67s/it]11/17/2022 00:06:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:35 - INFO - train.train_snli_ve - loss is tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2604/6700 [1:12:27<1:53:58,  1.67s/it]11/17/2022 00:06:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.0733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:36 - INFO - train.train_snli_ve - loss is tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2605/6700 [1:12:29<1:54:09,  1.67s/it]11/17/2022 00:06:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.8298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:38 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2606/6700 [1:12:31<1:55:08,  1.69s/it]11/17/2022 00:06:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.9769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:40 - INFO - train.train_snli_ve - loss is tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2607/6700 [1:12:33<1:54:51,  1.68s/it]11/17/2022 00:06:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:42 - INFO - train.train_snli_ve - loss is tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2608/6700 [1:12:34<1:55:04,  1.69s/it]11/17/2022 00:06:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:43 - INFO - train.train_snli_ve - loss is tensor(0.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2609/6700 [1:12:36<1:55:03,  1.69s/it]11/17/2022 00:06:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.5243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:45 - INFO - train.train_snli_ve - loss is tensor(0.7720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2610/6700 [1:12:38<1:55:35,  1.70s/it]11/17/2022 00:06:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.9457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:47 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2611/6700 [1:12:39<1:56:10,  1.70s/it]11/17/2022 00:06:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:48 - INFO - train.train_snli_ve - loss is tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###8      | 2612/6700 [1:12:41<1:56:04,  1.70s/it]11/17/2022 00:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.8751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:50 - INFO - train.train_snli_ve - loss is tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2613/6700 [1:12:43<1:55:46,  1.70s/it]11/17/2022 00:06:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:52 - INFO - train.train_snli_ve - loss is tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2614/6700 [1:12:44<1:55:28,  1.70s/it]11/17/2022 00:06:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:53 - INFO - train.train_snli_ve - loss is tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2615/6700 [1:12:46<1:54:45,  1.69s/it]11/17/2022 00:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.4559e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:55 - INFO - train.train_snli_ve - loss is tensor(0.8599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2616/6700 [1:12:48<1:55:06,  1.69s/it]11/17/2022 00:06:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:57 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2617/6700 [1:12:49<1:54:32,  1.68s/it]11/17/2022 00:06:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:06:58 - INFO - train.train_snli_ve - loss is tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2618/6700 [1:12:51<1:55:06,  1.69s/it]11/17/2022 00:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8159e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:00 - INFO - train.train_snli_ve - loss is tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2619/6700 [1:12:53<1:54:37,  1.69s/it]11/17/2022 00:07:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2741e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:02 - INFO - train.train_snli_ve - loss is tensor(0.9474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2620/6700 [1:12:55<1:56:23,  1.71s/it]11/17/2022 00:07:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:04 - INFO - train.train_snli_ve - loss is tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2621/6700 [1:12:56<1:55:57,  1.71s/it]11/17/2022 00:07:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7163e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:05 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2622/6700 [1:12:58<1:55:36,  1.70s/it]11/17/2022 00:07:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:07 - INFO - train.train_snli_ve - loss is tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2623/6700 [1:13:00<1:54:57,  1.69s/it]11/17/2022 00:07:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.7721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:09 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2624/6700 [1:13:01<1:54:45,  1.69s/it]11/17/2022 00:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.2695e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:10 - INFO - train.train_snli_ve - loss is tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2625/6700 [1:13:03<1:55:07,  1.70s/it]11/17/2022 00:07:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:12 - INFO - train.train_snli_ve - loss is tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2626/6700 [1:13:05<1:55:27,  1.70s/it]11/17/2022 00:07:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:14 - INFO - train.train_snli_ve - loss is tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2627/6700 [1:13:06<1:54:40,  1.69s/it]11/17/2022 00:07:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:15 - INFO - train.train_snli_ve - loss is tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2628/6700 [1:13:08<1:54:00,  1.68s/it]11/17/2022 00:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.1583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:17 - INFO - train.train_snli_ve - loss is tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2629/6700 [1:13:10<1:53:30,  1.67s/it]11/17/2022 00:07:19 - INFO - train.train_snli_ve - kd_loss is tensor(7.5860e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:19 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2630/6700 [1:13:11<1:53:22,  1.67s/it]11/17/2022 00:07:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9861e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:20 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2631/6700 [1:13:13<1:52:55,  1.67s/it]11/17/2022 00:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:22 - INFO - train.train_snli_ve - loss is tensor(0.5375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2632/6700 [1:13:15<1:53:52,  1.68s/it]11/17/2022 00:07:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:24 - INFO - train.train_snli_ve - loss is tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2633/6700 [1:13:16<1:52:53,  1.67s/it]11/17/2022 00:07:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5407e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:25 - INFO - train.train_snli_ve - loss is tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2634/6700 [1:13:18<1:54:03,  1.68s/it]11/17/2022 00:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.2178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:27 - INFO - train.train_snli_ve - loss is tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2635/6700 [1:13:20<1:54:38,  1.69s/it]11/17/2022 00:07:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:29 - INFO - train.train_snli_ve - loss is tensor(0.8064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2636/6700 [1:13:22<1:54:24,  1.69s/it]11/17/2022 00:07:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.0600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:30 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2637/6700 [1:13:23<1:53:31,  1.68s/it]11/17/2022 00:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3299e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:32 - INFO - train.train_snli_ve - loss is tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2638/6700 [1:13:25<1:53:08,  1.67s/it]11/17/2022 00:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3634e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:34 - INFO - train.train_snli_ve - loss is tensor(0.9057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2639/6700 [1:13:27<1:53:17,  1.67s/it]11/17/2022 00:07:35 - INFO - train.train_snli_ve - kd_loss is tensor(9.8366e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:35 - INFO - train.train_snli_ve - loss is tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2640/6700 [1:13:28<1:52:54,  1.67s/it]11/17/2022 00:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:37 - INFO - train.train_snli_ve - loss is tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2641/6700 [1:13:30<1:53:27,  1.68s/it]11/17/2022 00:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:39 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2642/6700 [1:13:32<1:52:56,  1.67s/it]11/17/2022 00:07:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:41 - INFO - train.train_snli_ve - loss is tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2643/6700 [1:13:33<1:53:05,  1.67s/it]11/17/2022 00:07:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.6846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:42 - INFO - train.train_snli_ve - loss is tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2644/6700 [1:13:35<1:53:02,  1.67s/it]11/17/2022 00:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:44 - INFO - train.train_snli_ve - loss is tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2645/6700 [1:13:37<1:53:08,  1.67s/it]11/17/2022 00:07:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:46 - INFO - train.train_snli_ve - loss is tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  39%|###9      | 2646/6700 [1:13:38<1:52:54,  1.67s/it]11/17/2022 00:07:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7067e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:47 - INFO - train.train_snli_ve - loss is tensor(0.8102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2647/6700 [1:13:40<1:53:01,  1.67s/it]11/17/2022 00:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0547e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:49 - INFO - train.train_snli_ve - loss is tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2648/6700 [1:13:42<1:53:15,  1.68s/it]11/17/2022 00:07:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4025e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:51 - INFO - train.train_snli_ve - loss is tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2649/6700 [1:13:43<1:53:13,  1.68s/it]11/17/2022 00:07:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:52 - INFO - train.train_snli_ve - loss is tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2650/6700 [1:13:45<1:53:36,  1.68s/it]11/17/2022 00:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:54 - INFO - train.train_snli_ve - loss is tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2651/6700 [1:13:47<1:53:52,  1.69s/it]11/17/2022 00:07:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.4081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:56 - INFO - train.train_snli_ve - loss is tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2652/6700 [1:13:48<1:54:15,  1.69s/it]11/17/2022 00:07:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:57 - INFO - train.train_snli_ve - loss is tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2653/6700 [1:13:50<1:53:29,  1.68s/it]11/17/2022 00:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:07:59 - INFO - train.train_snli_ve - loss is tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2654/6700 [1:13:52<1:53:00,  1.68s/it]11/17/2022 00:08:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4000e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:01 - INFO - train.train_snli_ve - loss is tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2655/6700 [1:13:53<1:52:44,  1.67s/it]11/17/2022 00:08:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:02 - INFO - train.train_snli_ve - loss is tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2656/6700 [1:13:55<1:53:08,  1.68s/it]11/17/2022 00:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:04 - INFO - train.train_snli_ve - loss is tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2657/6700 [1:13:57<1:52:20,  1.67s/it]11/17/2022 00:08:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.3536e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:06 - INFO - train.train_snli_ve - loss is tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2658/6700 [1:13:58<1:52:25,  1.67s/it]11/17/2022 00:08:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:07 - INFO - train.train_snli_ve - loss is tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2659/6700 [1:14:00<1:52:32,  1.67s/it]11/17/2022 00:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:09 - INFO - train.train_snli_ve - loss is tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2660/6700 [1:14:02<1:52:31,  1.67s/it]11/17/2022 00:08:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.6188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:11 - INFO - train.train_snli_ve - loss is tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2661/6700 [1:14:03<1:52:53,  1.68s/it]11/17/2022 00:08:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:12 - INFO - train.train_snli_ve - loss is tensor(0.8035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2662/6700 [1:14:05<1:53:42,  1.69s/it]11/17/2022 00:08:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.2646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:14 - INFO - train.train_snli_ve - loss is tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2663/6700 [1:14:07<1:53:06,  1.68s/it]11/17/2022 00:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.0735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:16 - INFO - train.train_snli_ve - loss is tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2664/6700 [1:14:08<1:53:34,  1.69s/it]11/17/2022 00:08:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:17 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2665/6700 [1:14:10<1:52:45,  1.68s/it]11/17/2022 00:08:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1011e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:19 - INFO - train.train_snli_ve - loss is tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2666/6700 [1:14:12<1:52:22,  1.67s/it]11/17/2022 00:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.8754e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:21 - INFO - train.train_snli_ve - loss is tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2667/6700 [1:14:13<1:52:16,  1.67s/it]11/17/2022 00:08:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.8452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:22 - INFO - train.train_snli_ve - loss is tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2668/6700 [1:14:15<1:52:43,  1.68s/it]11/17/2022 00:08:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.8306e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:24 - INFO - train.train_snli_ve - loss is tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2669/6700 [1:14:17<1:53:29,  1.69s/it]11/17/2022 00:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:26 - INFO - train.train_snli_ve - loss is tensor(0.7269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2670/6700 [1:14:19<1:53:44,  1.69s/it]11/17/2022 00:08:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.9028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:28 - INFO - train.train_snli_ve - loss is tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2671/6700 [1:14:20<1:53:27,  1.69s/it]11/17/2022 00:08:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:29 - INFO - train.train_snli_ve - loss is tensor(0.4523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2672/6700 [1:14:22<1:53:36,  1.69s/it]11/17/2022 00:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:31 - INFO - train.train_snli_ve - loss is tensor(0.5081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2673/6700 [1:14:24<1:52:59,  1.68s/it]11/17/2022 00:08:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:33 - INFO - train.train_snli_ve - loss is tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2674/6700 [1:14:25<1:52:38,  1.68s/it]11/17/2022 00:08:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.8522e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:34 - INFO - train.train_snli_ve - loss is tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2675/6700 [1:14:27<1:51:47,  1.67s/it]11/17/2022 00:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:36 - INFO - train.train_snli_ve - loss is tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2676/6700 [1:14:29<1:53:09,  1.69s/it]11/17/2022 00:08:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.9618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:38 - INFO - train.train_snli_ve - loss is tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2677/6700 [1:14:30<1:52:31,  1.68s/it]11/17/2022 00:08:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6662e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:39 - INFO - train.train_snli_ve - loss is tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2678/6700 [1:14:32<1:52:15,  1.67s/it]11/17/2022 00:08:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:41 - INFO - train.train_snli_ve - loss is tensor(0.4685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|###9      | 2679/6700 [1:14:34<1:51:48,  1.67s/it]11/17/2022 00:08:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:43 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2680/6700 [1:14:35<1:52:06,  1.67s/it]11/17/2022 00:08:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:44 - INFO - train.train_snli_ve - loss is tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2681/6700 [1:14:37<1:52:21,  1.68s/it]11/17/2022 00:08:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:46 - INFO - train.train_snli_ve - loss is tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2682/6700 [1:14:39<1:52:45,  1.68s/it]11/17/2022 00:08:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:48 - INFO - train.train_snli_ve - loss is tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2683/6700 [1:14:40<1:52:40,  1.68s/it]11/17/2022 00:08:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9817e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:49 - INFO - train.train_snli_ve - loss is tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2684/6700 [1:14:42<1:53:39,  1.70s/it]11/17/2022 00:08:51 - INFO - train.train_snli_ve - kd_loss is tensor(7.6110e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:51 - INFO - train.train_snli_ve - loss is tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2685/6700 [1:14:44<1:53:59,  1.70s/it]11/17/2022 00:08:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.4668e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:53 - INFO - train.train_snli_ve - loss is tensor(1.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2686/6700 [1:14:45<1:53:21,  1.69s/it]11/17/2022 00:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.8450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:54 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2687/6700 [1:14:47<1:52:07,  1.68s/it]11/17/2022 00:08:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:56 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2688/6700 [1:14:49<1:51:52,  1.67s/it]11/17/2022 00:08:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.1482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:58 - INFO - train.train_snli_ve - loss is tensor(0.8637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2689/6700 [1:14:50<1:51:22,  1.67s/it]11/17/2022 00:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7280e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:08:59 - INFO - train.train_snli_ve - loss is tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2690/6700 [1:14:52<1:52:39,  1.69s/it]11/17/2022 00:09:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.0621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:01 - INFO - train.train_snli_ve - loss is tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2691/6700 [1:14:54<1:51:50,  1.67s/it]11/17/2022 00:09:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:03 - INFO - train.train_snli_ve - loss is tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2692/6700 [1:14:55<1:51:35,  1.67s/it]11/17/2022 00:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:04 - INFO - train.train_snli_ve - loss is tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2693/6700 [1:14:57<1:51:46,  1.67s/it]11/17/2022 00:09:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.6389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:06 - INFO - train.train_snli_ve - loss is tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2694/6700 [1:14:59<1:52:13,  1.68s/it]11/17/2022 00:09:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:08 - INFO - train.train_snli_ve - loss is tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2695/6700 [1:15:01<1:51:34,  1.67s/it]11/17/2022 00:09:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:10 - INFO - train.train_snli_ve - loss is tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2696/6700 [1:15:02<1:51:48,  1.68s/it]11/17/2022 00:09:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:11 - INFO - train.train_snli_ve - loss is tensor(0.7715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2697/6700 [1:15:04<1:51:48,  1.68s/it]11/17/2022 00:09:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:13 - INFO - train.train_snli_ve - loss is tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2698/6700 [1:15:06<1:51:30,  1.67s/it]11/17/2022 00:09:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:15 - INFO - train.train_snli_ve - loss is tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2699/6700 [1:15:07<1:51:27,  1.67s/it]11/17/2022 00:09:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.6083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:16 - INFO - train.train_snli_ve - loss is tensor(0.8085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2700/6700 [1:15:09<1:52:04,  1.68s/it]11/17/2022 00:09:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:18 - INFO - train.train_snli_ve - loss is tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2701/6700 [1:15:11<1:52:05,  1.68s/it]11/17/2022 00:09:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:20 - INFO - train.train_snli_ve - loss is tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2702/6700 [1:15:12<1:51:41,  1.68s/it]11/17/2022 00:09:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.5247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:21 - INFO - train.train_snli_ve - loss is tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2703/6700 [1:15:14<1:51:29,  1.67s/it]11/17/2022 00:09:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:23 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2704/6700 [1:15:16<1:51:57,  1.68s/it]11/17/2022 00:09:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.7994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:25 - INFO - train.train_snli_ve - loss is tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2705/6700 [1:15:17<1:51:20,  1.67s/it]11/17/2022 00:09:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.3690e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:26 - INFO - train.train_snli_ve - loss is tensor(0.8907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2706/6700 [1:15:19<1:51:47,  1.68s/it]11/17/2022 00:09:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.8486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:28 - INFO - train.train_snli_ve - loss is tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2707/6700 [1:15:21<1:51:28,  1.68s/it]11/17/2022 00:09:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:30 - INFO - train.train_snli_ve - loss is tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2708/6700 [1:15:22<1:50:57,  1.67s/it]11/17/2022 00:09:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.3044e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:31 - INFO - train.train_snli_ve - loss is tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2709/6700 [1:15:24<1:50:49,  1.67s/it]11/17/2022 00:09:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:33 - INFO - train.train_snli_ve - loss is tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2710/6700 [1:15:26<1:50:32,  1.66s/it]11/17/2022 00:09:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.5033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:35 - INFO - train.train_snli_ve - loss is tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2711/6700 [1:15:27<1:50:14,  1.66s/it]11/17/2022 00:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:36 - INFO - train.train_snli_ve - loss is tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2712/6700 [1:15:29<1:51:08,  1.67s/it]11/17/2022 00:09:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.2556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:38 - INFO - train.train_snli_ve - loss is tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  40%|####      | 2713/6700 [1:15:31<1:51:00,  1.67s/it]11/17/2022 00:09:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.9936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:40 - INFO - train.train_snli_ve - loss is tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2714/6700 [1:15:32<1:50:46,  1.67s/it]11/17/2022 00:09:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:41 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2715/6700 [1:15:34<1:51:24,  1.68s/it]11/17/2022 00:09:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.3630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:43 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2716/6700 [1:15:36<1:52:15,  1.69s/it]11/17/2022 00:09:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.6175e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:45 - INFO - train.train_snli_ve - loss is tensor(0.8263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2717/6700 [1:15:37<1:51:12,  1.68s/it]11/17/2022 00:09:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.1754e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:46 - INFO - train.train_snli_ve - loss is tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2718/6700 [1:15:39<1:51:08,  1.67s/it]11/17/2022 00:09:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:48 - INFO - train.train_snli_ve - loss is tensor(0.9079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2719/6700 [1:15:41<1:50:26,  1.66s/it]11/17/2022 00:09:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:50 - INFO - train.train_snli_ve - loss is tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2720/6700 [1:15:42<1:49:59,  1.66s/it]11/17/2022 00:09:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.5213e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:51 - INFO - train.train_snli_ve - loss is tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2721/6700 [1:15:44<1:50:02,  1.66s/it]11/17/2022 00:09:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.5567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:53 - INFO - train.train_snli_ve - loss is tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2722/6700 [1:15:46<1:52:12,  1.69s/it]11/17/2022 00:09:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:55 - INFO - train.train_snli_ve - loss is tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2723/6700 [1:15:47<1:51:57,  1.69s/it]11/17/2022 00:09:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.5602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:56 - INFO - train.train_snli_ve - loss is tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2724/6700 [1:15:49<1:51:47,  1.69s/it]11/17/2022 00:09:58 - INFO - train.train_snli_ve - kd_loss is tensor(5.5030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:09:58 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2725/6700 [1:15:51<1:53:06,  1.71s/it]11/17/2022 00:10:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.4641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:00 - INFO - train.train_snli_ve - loss is tensor(1.0155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2726/6700 [1:15:53<1:52:39,  1.70s/it]11/17/2022 00:10:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:01 - INFO - train.train_snli_ve - loss is tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2727/6700 [1:15:54<1:51:31,  1.68s/it]11/17/2022 00:10:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.7619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:03 - INFO - train.train_snli_ve - loss is tensor(0.8078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2728/6700 [1:15:56<1:52:41,  1.70s/it]11/17/2022 00:10:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.5146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:05 - INFO - train.train_snli_ve - loss is tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2729/6700 [1:15:58<1:52:18,  1.70s/it]11/17/2022 00:10:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:07 - INFO - train.train_snli_ve - loss is tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2730/6700 [1:15:59<1:52:39,  1.70s/it]11/17/2022 00:10:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.6925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:08 - INFO - train.train_snli_ve - loss is tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2731/6700 [1:16:01<1:51:53,  1.69s/it]11/17/2022 00:10:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.7282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:10 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2732/6700 [1:16:03<1:51:25,  1.68s/it]11/17/2022 00:10:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.8438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:12 - INFO - train.train_snli_ve - loss is tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2733/6700 [1:16:04<1:51:30,  1.69s/it]11/17/2022 00:10:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:13 - INFO - train.train_snli_ve - loss is tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2734/6700 [1:16:06<1:51:25,  1.69s/it]11/17/2022 00:10:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.4927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:15 - INFO - train.train_snli_ve - loss is tensor(0.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2735/6700 [1:16:08<1:51:21,  1.69s/it]11/17/2022 00:10:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.8254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:17 - INFO - train.train_snli_ve - loss is tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2736/6700 [1:16:09<1:51:27,  1.69s/it]11/17/2022 00:10:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:18 - INFO - train.train_snli_ve - loss is tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2737/6700 [1:16:11<1:51:26,  1.69s/it]11/17/2022 00:10:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.3761e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:20 - INFO - train.train_snli_ve - loss is tensor(0.9232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2738/6700 [1:16:13<1:51:40,  1.69s/it]11/17/2022 00:10:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.8138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:22 - INFO - train.train_snli_ve - loss is tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2739/6700 [1:16:15<1:52:11,  1.70s/it]11/17/2022 00:10:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0913e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:24 - INFO - train.train_snli_ve - loss is tensor(0.8155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2740/6700 [1:16:16<1:51:42,  1.69s/it]11/17/2022 00:10:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:25 - INFO - train.train_snli_ve - loss is tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2741/6700 [1:16:18<1:52:23,  1.70s/it]11/17/2022 00:10:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:27 - INFO - train.train_snli_ve - loss is tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2742/6700 [1:16:20<1:51:18,  1.69s/it]11/17/2022 00:10:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1444e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:29 - INFO - train.train_snli_ve - loss is tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2743/6700 [1:16:21<1:52:26,  1.70s/it]11/17/2022 00:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0889e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:30 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2744/6700 [1:16:23<1:53:04,  1.71s/it]11/17/2022 00:10:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.3584e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:32 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2745/6700 [1:16:25<1:52:06,  1.70s/it]11/17/2022 00:10:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:34 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####      | 2746/6700 [1:16:26<1:51:11,  1.69s/it]11/17/2022 00:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:35 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2747/6700 [1:16:28<1:50:17,  1.67s/it]11/17/2022 00:10:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:37 - INFO - train.train_snli_ve - loss is tensor(0.4448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2748/6700 [1:16:30<1:50:56,  1.68s/it]11/17/2022 00:10:39 - INFO - train.train_snli_ve - kd_loss is tensor(8.9546e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:39 - INFO - train.train_snli_ve - loss is tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2749/6700 [1:16:31<1:51:09,  1.69s/it]11/17/2022 00:10:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:40 - INFO - train.train_snli_ve - loss is tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2750/6700 [1:16:33<1:51:27,  1.69s/it]11/17/2022 00:10:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.2670e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:42 - INFO - train.train_snli_ve - loss is tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2751/6700 [1:16:35<1:51:38,  1.70s/it]11/17/2022 00:10:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1478e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:44 - INFO - train.train_snli_ve - loss is tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2752/6700 [1:16:37<1:51:35,  1.70s/it]11/17/2022 00:10:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3697e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:46 - INFO - train.train_snli_ve - loss is tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2753/6700 [1:16:38<1:50:58,  1.69s/it]11/17/2022 00:10:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:47 - INFO - train.train_snli_ve - loss is tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2754/6700 [1:16:40<1:51:14,  1.69s/it]11/17/2022 00:10:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.6483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:49 - INFO - train.train_snli_ve - loss is tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2755/6700 [1:16:42<1:50:02,  1.67s/it]11/17/2022 00:10:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:51 - INFO - train.train_snli_ve - loss is tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2756/6700 [1:16:43<1:49:50,  1.67s/it]11/17/2022 00:10:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:52 - INFO - train.train_snli_ve - loss is tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2757/6700 [1:16:45<1:50:13,  1.68s/it]11/17/2022 00:10:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:54 - INFO - train.train_snli_ve - loss is tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2758/6700 [1:16:47<1:51:07,  1.69s/it]11/17/2022 00:10:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.0171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:56 - INFO - train.train_snli_ve - loss is tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2759/6700 [1:16:48<1:50:54,  1.69s/it]11/17/2022 00:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:57 - INFO - train.train_snli_ve - loss is tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2760/6700 [1:16:50<1:50:24,  1.68s/it]11/17/2022 00:10:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:10:59 - INFO - train.train_snli_ve - loss is tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2761/6700 [1:16:52<1:50:40,  1.69s/it]11/17/2022 00:11:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.5423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:01 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2762/6700 [1:16:53<1:50:35,  1.68s/it]11/17/2022 00:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:02 - INFO - train.train_snli_ve - loss is tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2763/6700 [1:16:55<1:50:23,  1.68s/it]11/17/2022 00:11:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.1573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:04 - INFO - train.train_snli_ve - loss is tensor(0.9092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2764/6700 [1:16:57<1:50:11,  1.68s/it]11/17/2022 00:11:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8919e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:06 - INFO - train.train_snli_ve - loss is tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2765/6700 [1:16:58<1:50:55,  1.69s/it]11/17/2022 00:11:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:07 - INFO - train.train_snli_ve - loss is tensor(0.8538, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2766/6700 [1:17:00<1:50:07,  1.68s/it]11/17/2022 00:11:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:09 - INFO - train.train_snli_ve - loss is tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2767/6700 [1:17:02<1:49:47,  1.68s/it]11/17/2022 00:11:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.4221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:11 - INFO - train.train_snli_ve - loss is tensor(0.5497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2768/6700 [1:17:03<1:50:40,  1.69s/it]11/17/2022 00:11:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:12 - INFO - train.train_snli_ve - loss is tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2769/6700 [1:17:05<1:50:31,  1.69s/it]11/17/2022 00:11:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.9109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:14 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2770/6700 [1:17:07<1:51:09,  1.70s/it]11/17/2022 00:11:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:16 - INFO - train.train_snli_ve - loss is tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2771/6700 [1:17:09<1:51:02,  1.70s/it]11/17/2022 00:11:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5852e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:18 - INFO - train.train_snli_ve - loss is tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2772/6700 [1:17:10<1:51:08,  1.70s/it]11/17/2022 00:11:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:19 - INFO - train.train_snli_ve - loss is tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2773/6700 [1:17:12<1:51:23,  1.70s/it]11/17/2022 00:11:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.6673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:21 - INFO - train.train_snli_ve - loss is tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2774/6700 [1:17:14<1:50:16,  1.69s/it]11/17/2022 00:11:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:23 - INFO - train.train_snli_ve - loss is tensor(0.7302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2775/6700 [1:17:15<1:50:06,  1.68s/it]11/17/2022 00:11:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.4991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:24 - INFO - train.train_snli_ve - loss is tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2776/6700 [1:17:17<1:50:04,  1.68s/it]11/17/2022 00:11:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:26 - INFO - train.train_snli_ve - loss is tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2777/6700 [1:17:19<1:51:23,  1.70s/it]11/17/2022 00:11:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:28 - INFO - train.train_snli_ve - loss is tensor(0.9273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2778/6700 [1:17:20<1:51:44,  1.71s/it]11/17/2022 00:11:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:29 - INFO - train.train_snli_ve - loss is tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2779/6700 [1:17:22<1:51:12,  1.70s/it]11/17/2022 00:11:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:31 - INFO - train.train_snli_ve - loss is tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  41%|####1     | 2780/6700 [1:17:24<1:50:05,  1.69s/it]11/17/2022 00:11:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7443e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:33 - INFO - train.train_snli_ve - loss is tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2781/6700 [1:17:25<1:49:08,  1.67s/it]11/17/2022 00:11:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:34 - INFO - train.train_snli_ve - loss is tensor(0.8197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2782/6700 [1:17:27<1:49:32,  1.68s/it]11/17/2022 00:11:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.9526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:36 - INFO - train.train_snli_ve - loss is tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2783/6700 [1:17:29<1:50:13,  1.69s/it]11/17/2022 00:11:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:38 - INFO - train.train_snli_ve - loss is tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2784/6700 [1:17:30<1:49:47,  1.68s/it]11/17/2022 00:11:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:39 - INFO - train.train_snli_ve - loss is tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2785/6700 [1:17:32<1:49:07,  1.67s/it]11/17/2022 00:11:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:41 - INFO - train.train_snli_ve - loss is tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2786/6700 [1:17:34<1:49:50,  1.68s/it]11/17/2022 00:11:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:43 - INFO - train.train_snli_ve - loss is tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2787/6700 [1:17:36<1:49:25,  1.68s/it]11/17/2022 00:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:45 - INFO - train.train_snli_ve - loss is tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2788/6700 [1:17:37<1:49:46,  1.68s/it]11/17/2022 00:11:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.7803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:46 - INFO - train.train_snli_ve - loss is tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2789/6700 [1:17:39<1:49:53,  1.69s/it]11/17/2022 00:11:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5860e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:48 - INFO - train.train_snli_ve - loss is tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2790/6700 [1:17:41<1:49:51,  1.69s/it]11/17/2022 00:11:50 - INFO - train.train_snli_ve - kd_loss is tensor(9.9367e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:50 - INFO - train.train_snli_ve - loss is tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2791/6700 [1:17:42<1:49:50,  1.69s/it]11/17/2022 00:11:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.2664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:51 - INFO - train.train_snli_ve - loss is tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2792/6700 [1:17:44<1:49:46,  1.69s/it]11/17/2022 00:11:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:53 - INFO - train.train_snli_ve - loss is tensor(1.0461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2793/6700 [1:17:46<1:49:57,  1.69s/it]11/17/2022 00:11:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.5340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:55 - INFO - train.train_snli_ve - loss is tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2794/6700 [1:17:47<1:49:55,  1.69s/it]11/17/2022 00:11:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.0427e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:56 - INFO - train.train_snli_ve - loss is tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2795/6700 [1:17:49<1:49:31,  1.68s/it]11/17/2022 00:11:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:11:58 - INFO - train.train_snli_ve - loss is tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2796/6700 [1:17:51<1:50:00,  1.69s/it]11/17/2022 00:12:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6540e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:00 - INFO - train.train_snli_ve - loss is tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2797/6700 [1:17:52<1:49:20,  1.68s/it]11/17/2022 00:12:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1044e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:01 - INFO - train.train_snli_ve - loss is tensor(0.8407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2798/6700 [1:17:54<1:48:58,  1.68s/it]11/17/2022 00:12:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2072e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:03 - INFO - train.train_snli_ve - loss is tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2799/6700 [1:17:56<1:47:58,  1.66s/it]11/17/2022 00:12:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:05 - INFO - train.train_snli_ve - loss is tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2800/6700 [1:17:57<1:48:22,  1.67s/it]11/17/2022 00:12:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0251e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:06 - INFO - train.train_snli_ve - loss is tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2801/6700 [1:17:59<1:48:00,  1.66s/it]11/17/2022 00:12:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.1489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:08 - INFO - train.train_snli_ve - loss is tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2802/6700 [1:18:01<1:48:22,  1.67s/it]11/17/2022 00:12:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:10 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2803/6700 [1:18:02<1:47:35,  1.66s/it]11/17/2022 00:12:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:11 - INFO - train.train_snli_ve - loss is tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2804/6700 [1:18:04<1:47:31,  1.66s/it]11/17/2022 00:12:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.8179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:13 - INFO - train.train_snli_ve - loss is tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2805/6700 [1:18:06<1:47:52,  1.66s/it]11/17/2022 00:12:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:15 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2806/6700 [1:18:07<1:47:50,  1.66s/it]11/17/2022 00:12:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:16 - INFO - train.train_snli_ve - loss is tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2807/6700 [1:18:09<1:48:13,  1.67s/it]11/17/2022 00:12:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:18 - INFO - train.train_snli_ve - loss is tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2808/6700 [1:18:11<1:48:22,  1.67s/it]11/17/2022 00:12:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:20 - INFO - train.train_snli_ve - loss is tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2809/6700 [1:18:12<1:48:25,  1.67s/it]11/17/2022 00:12:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:21 - INFO - train.train_snli_ve - loss is tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2810/6700 [1:18:14<1:49:17,  1.69s/it]11/17/2022 00:12:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:23 - INFO - train.train_snli_ve - loss is tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2811/6700 [1:18:16<1:47:59,  1.67s/it]11/17/2022 00:12:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.0492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:25 - INFO - train.train_snli_ve - loss is tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2812/6700 [1:18:17<1:46:24,  1.64s/it]11/17/2022 00:12:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:26 - INFO - train.train_snli_ve - loss is tensor(0.7064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####1     | 2813/6700 [1:18:19<1:45:38,  1.63s/it]11/17/2022 00:12:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:28 - INFO - train.train_snli_ve - loss is tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2814/6700 [1:18:20<1:45:18,  1.63s/it]11/17/2022 00:12:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.8336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:29 - INFO - train.train_snli_ve - loss is tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2815/6700 [1:18:22<1:44:12,  1.61s/it]11/17/2022 00:12:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.0453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:31 - INFO - train.train_snli_ve - loss is tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2816/6700 [1:18:24<1:44:14,  1.61s/it]11/17/2022 00:12:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:33 - INFO - train.train_snli_ve - loss is tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2817/6700 [1:18:25<1:44:00,  1.61s/it]11/17/2022 00:12:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.3312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:34 - INFO - train.train_snli_ve - loss is tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2818/6700 [1:18:27<1:43:52,  1.61s/it]11/17/2022 00:12:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1449e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:36 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2819/6700 [1:18:28<1:44:32,  1.62s/it]11/17/2022 00:12:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:37 - INFO - train.train_snli_ve - loss is tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2820/6700 [1:18:30<1:44:32,  1.62s/it]11/17/2022 00:12:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:39 - INFO - train.train_snli_ve - loss is tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2821/6700 [1:18:32<1:43:56,  1.61s/it]11/17/2022 00:12:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.8730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:41 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2822/6700 [1:18:33<1:43:24,  1.60s/it]11/17/2022 00:12:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:42 - INFO - train.train_snli_ve - loss is tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2823/6700 [1:18:35<1:43:37,  1.60s/it]11/17/2022 00:12:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:44 - INFO - train.train_snli_ve - loss is tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2824/6700 [1:18:36<1:43:27,  1.60s/it]11/17/2022 00:12:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:45 - INFO - train.train_snli_ve - loss is tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2825/6700 [1:18:38<1:43:27,  1.60s/it]11/17/2022 00:12:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:47 - INFO - train.train_snli_ve - loss is tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2826/6700 [1:18:40<1:43:57,  1.61s/it]11/17/2022 00:12:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:49 - INFO - train.train_snli_ve - loss is tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2827/6700 [1:18:41<1:43:30,  1.60s/it]11/17/2022 00:12:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.0432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:50 - INFO - train.train_snli_ve - loss is tensor(0.9228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2828/6700 [1:18:43<1:43:40,  1.61s/it]11/17/2022 00:12:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.3937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:52 - INFO - train.train_snli_ve - loss is tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2829/6700 [1:18:45<1:44:32,  1.62s/it]11/17/2022 00:12:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:53 - INFO - train.train_snli_ve - loss is tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2830/6700 [1:18:46<1:43:52,  1.61s/it]11/17/2022 00:12:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2511e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:55 - INFO - train.train_snli_ve - loss is tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2831/6700 [1:18:48<1:43:14,  1.60s/it]11/17/2022 00:12:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.9131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:57 - INFO - train.train_snli_ve - loss is tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2832/6700 [1:18:49<1:42:53,  1.60s/it]11/17/2022 00:12:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:12:58 - INFO - train.train_snli_ve - loss is tensor(0.9512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2833/6700 [1:18:51<1:43:31,  1.61s/it]11/17/2022 00:13:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3685e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:00 - INFO - train.train_snli_ve - loss is tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2834/6700 [1:18:53<1:43:09,  1.60s/it]11/17/2022 00:13:01 - INFO - train.train_snli_ve - kd_loss is tensor(8.7551e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:01 - INFO - train.train_snli_ve - loss is tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2835/6700 [1:18:54<1:43:04,  1.60s/it]11/17/2022 00:13:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1148e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:03 - INFO - train.train_snli_ve - loss is tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2836/6700 [1:18:56<1:43:30,  1.61s/it]11/17/2022 00:13:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.2998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:05 - INFO - train.train_snli_ve - loss is tensor(0.6168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2837/6700 [1:18:57<1:43:49,  1.61s/it]11/17/2022 00:13:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:06 - INFO - train.train_snli_ve - loss is tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2838/6700 [1:18:59<1:43:40,  1.61s/it]11/17/2022 00:13:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:08 - INFO - train.train_snli_ve - loss is tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2839/6700 [1:19:01<1:42:57,  1.60s/it]11/17/2022 00:13:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:09 - INFO - train.train_snli_ve - loss is tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2840/6700 [1:19:02<1:42:52,  1.60s/it]11/17/2022 00:13:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.0990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:11 - INFO - train.train_snli_ve - loss is tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2841/6700 [1:19:04<1:42:54,  1.60s/it]11/17/2022 00:13:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:13 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2842/6700 [1:19:05<1:44:29,  1.62s/it]11/17/2022 00:13:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.1935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:14 - INFO - train.train_snli_ve - loss is tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2843/6700 [1:19:07<1:45:02,  1.63s/it]11/17/2022 00:13:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.0956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:16 - INFO - train.train_snli_ve - loss is tensor(0.8697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2844/6700 [1:19:09<1:45:53,  1.65s/it]11/17/2022 00:13:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:18 - INFO - train.train_snli_ve - loss is tensor(0.8161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2845/6700 [1:19:10<1:45:21,  1.64s/it]11/17/2022 00:13:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.8021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:19 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2846/6700 [1:19:12<1:44:38,  1.63s/it]11/17/2022 00:13:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:21 - INFO - train.train_snli_ve - loss is tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  42%|####2     | 2847/6700 [1:19:14<1:45:46,  1.65s/it]11/17/2022 00:13:23 - INFO - train.train_snli_ve - kd_loss is tensor(6.6003e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:23 - INFO - train.train_snli_ve - loss is tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2848/6700 [1:19:15<1:46:03,  1.65s/it]11/17/2022 00:13:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:24 - INFO - train.train_snli_ve - loss is tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2849/6700 [1:19:17<1:45:37,  1.65s/it]11/17/2022 00:13:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:26 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2850/6700 [1:19:19<1:46:08,  1.65s/it]11/17/2022 00:13:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:28 - INFO - train.train_snli_ve - loss is tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2851/6700 [1:19:20<1:45:57,  1.65s/it]11/17/2022 00:13:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:29 - INFO - train.train_snli_ve - loss is tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2852/6700 [1:19:22<1:45:27,  1.64s/it]11/17/2022 00:13:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4949e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:31 - INFO - train.train_snli_ve - loss is tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2853/6700 [1:19:24<1:44:43,  1.63s/it]11/17/2022 00:13:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.4607e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:33 - INFO - train.train_snli_ve - loss is tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2854/6700 [1:19:25<1:44:58,  1.64s/it]11/17/2022 00:13:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.0361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:34 - INFO - train.train_snli_ve - loss is tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2855/6700 [1:19:27<1:44:58,  1.64s/it]11/17/2022 00:13:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:36 - INFO - train.train_snli_ve - loss is tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2856/6700 [1:19:28<1:44:57,  1.64s/it]11/17/2022 00:13:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5173e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:37 - INFO - train.train_snli_ve - loss is tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2857/6700 [1:19:30<1:44:24,  1.63s/it]11/17/2022 00:13:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:39 - INFO - train.train_snli_ve - loss is tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2858/6700 [1:19:32<1:44:31,  1.63s/it]11/17/2022 00:13:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.3236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:41 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2859/6700 [1:19:33<1:44:49,  1.64s/it]11/17/2022 00:13:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.4138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:42 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2860/6700 [1:19:35<1:44:39,  1.64s/it]11/17/2022 00:13:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:44 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2861/6700 [1:19:37<1:44:22,  1.63s/it]11/17/2022 00:13:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:46 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2862/6700 [1:19:38<1:44:08,  1.63s/it]11/17/2022 00:13:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.9427e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:47 - INFO - train.train_snli_ve - loss is tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2863/6700 [1:19:40<1:43:32,  1.62s/it]11/17/2022 00:13:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:49 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2864/6700 [1:19:41<1:43:39,  1.62s/it]11/17/2022 00:13:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:50 - INFO - train.train_snli_ve - loss is tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2865/6700 [1:19:43<1:43:52,  1.63s/it]11/17/2022 00:13:52 - INFO - train.train_snli_ve - kd_loss is tensor(9.3186e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:52 - INFO - train.train_snli_ve - loss is tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2866/6700 [1:19:45<1:43:45,  1.62s/it]11/17/2022 00:13:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:54 - INFO - train.train_snli_ve - loss is tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2867/6700 [1:19:46<1:44:09,  1.63s/it]11/17/2022 00:13:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:55 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2868/6700 [1:19:48<1:44:17,  1.63s/it]11/17/2022 00:13:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:57 - INFO - train.train_snli_ve - loss is tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2869/6700 [1:19:50<1:44:41,  1.64s/it]11/17/2022 00:13:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.3448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:13:59 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2870/6700 [1:19:51<1:44:48,  1.64s/it]11/17/2022 00:14:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:00 - INFO - train.train_snli_ve - loss is tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2871/6700 [1:19:53<1:45:09,  1.65s/it]11/17/2022 00:14:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.5242e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:02 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2872/6700 [1:19:55<1:45:01,  1.65s/it]11/17/2022 00:14:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9061e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:04 - INFO - train.train_snli_ve - loss is tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2873/6700 [1:19:56<1:44:11,  1.63s/it]11/17/2022 00:14:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:05 - INFO - train.train_snli_ve - loss is tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2874/6700 [1:19:58<1:44:13,  1.63s/it]11/17/2022 00:14:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:07 - INFO - train.train_snli_ve - loss is tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2875/6700 [1:19:59<1:43:39,  1.63s/it]11/17/2022 00:14:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5559e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:08 - INFO - train.train_snli_ve - loss is tensor(0.8908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2876/6700 [1:20:01<1:43:43,  1.63s/it]11/17/2022 00:14:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:10 - INFO - train.train_snli_ve - loss is tensor(0.8633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2877/6700 [1:20:03<1:44:07,  1.63s/it]11/17/2022 00:14:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:12 - INFO - train.train_snli_ve - loss is tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2878/6700 [1:20:04<1:45:52,  1.66s/it]11/17/2022 00:14:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.8465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:13 - INFO - train.train_snli_ve - loss is tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2879/6700 [1:20:06<1:45:41,  1.66s/it]11/17/2022 00:14:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:15 - INFO - train.train_snli_ve - loss is tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####2     | 2880/6700 [1:20:08<1:45:55,  1.66s/it]11/17/2022 00:14:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4362e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:17 - INFO - train.train_snli_ve - loss is tensor(0.8366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2881/6700 [1:20:09<1:45:12,  1.65s/it]11/17/2022 00:14:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:18 - INFO - train.train_snli_ve - loss is tensor(0.8139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2882/6700 [1:20:11<1:45:34,  1.66s/it]11/17/2022 00:14:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.0593e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:20 - INFO - train.train_snli_ve - loss is tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2883/6700 [1:20:13<1:45:24,  1.66s/it]11/17/2022 00:14:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:22 - INFO - train.train_snli_ve - loss is tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2884/6700 [1:20:14<1:45:08,  1.65s/it]11/17/2022 00:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.3714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:23 - INFO - train.train_snli_ve - loss is tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2885/6700 [1:20:16<1:44:20,  1.64s/it]11/17/2022 00:14:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.3412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:25 - INFO - train.train_snli_ve - loss is tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2886/6700 [1:20:18<1:44:11,  1.64s/it]11/17/2022 00:14:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.4145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:27 - INFO - train.train_snli_ve - loss is tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2887/6700 [1:20:19<1:44:03,  1.64s/it]11/17/2022 00:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.7364e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:28 - INFO - train.train_snli_ve - loss is tensor(0.8747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2888/6700 [1:20:21<1:43:29,  1.63s/it]11/17/2022 00:14:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:30 - INFO - train.train_snli_ve - loss is tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2889/6700 [1:20:23<1:43:50,  1.63s/it]11/17/2022 00:14:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:31 - INFO - train.train_snli_ve - loss is tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2890/6700 [1:20:24<1:44:03,  1.64s/it]11/17/2022 00:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:33 - INFO - train.train_snli_ve - loss is tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2891/6700 [1:20:26<1:44:28,  1.65s/it]11/17/2022 00:14:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:35 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2892/6700 [1:20:27<1:43:57,  1.64s/it]11/17/2022 00:14:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.4421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:36 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2893/6700 [1:20:29<1:43:22,  1.63s/it]11/17/2022 00:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:38 - INFO - train.train_snli_ve - loss is tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2894/6700 [1:20:31<1:43:26,  1.63s/it]11/17/2022 00:14:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:40 - INFO - train.train_snli_ve - loss is tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2895/6700 [1:20:32<1:44:17,  1.64s/it]11/17/2022 00:14:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:41 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2896/6700 [1:20:34<1:44:59,  1.66s/it]11/17/2022 00:14:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.5222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:43 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2897/6700 [1:20:36<1:46:20,  1.68s/it]11/17/2022 00:14:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:45 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2898/6700 [1:20:38<1:46:49,  1.69s/it]11/17/2022 00:14:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:46 - INFO - train.train_snli_ve - loss is tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2899/6700 [1:20:39<1:46:01,  1.67s/it]11/17/2022 00:14:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.9854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:48 - INFO - train.train_snli_ve - loss is tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2900/6700 [1:20:41<1:46:39,  1.68s/it]11/17/2022 00:14:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:50 - INFO - train.train_snli_ve - loss is tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2901/6700 [1:20:43<1:46:52,  1.69s/it]11/17/2022 00:14:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:52 - INFO - train.train_snli_ve - loss is tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2902/6700 [1:20:44<1:46:47,  1.69s/it]11/17/2022 00:14:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:53 - INFO - train.train_snli_ve - loss is tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2903/6700 [1:20:46<1:47:19,  1.70s/it]11/17/2022 00:14:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:55 - INFO - train.train_snli_ve - loss is tensor(0.7429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2904/6700 [1:20:48<1:47:47,  1.70s/it]11/17/2022 00:14:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.6667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:57 - INFO - train.train_snli_ve - loss is tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2905/6700 [1:20:49<1:46:31,  1.68s/it]11/17/2022 00:14:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4503e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:14:58 - INFO - train.train_snli_ve - loss is tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2906/6700 [1:20:51<1:45:50,  1.67s/it]11/17/2022 00:15:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:00 - INFO - train.train_snli_ve - loss is tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2907/6700 [1:20:53<1:46:09,  1.68s/it]11/17/2022 00:15:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.3575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:02 - INFO - train.train_snli_ve - loss is tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2908/6700 [1:20:54<1:46:24,  1.68s/it]11/17/2022 00:15:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:03 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2909/6700 [1:20:56<1:45:10,  1.66s/it]11/17/2022 00:15:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.8488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:05 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2910/6700 [1:20:58<1:45:14,  1.67s/it]11/17/2022 00:15:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:07 - INFO - train.train_snli_ve - loss is tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2911/6700 [1:20:59<1:46:04,  1.68s/it]11/17/2022 00:15:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:08 - INFO - train.train_snli_ve - loss is tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2912/6700 [1:21:01<1:45:11,  1.67s/it]11/17/2022 00:15:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4401e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:10 - INFO - train.train_snli_ve - loss is tensor(0.7709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2913/6700 [1:21:03<1:45:56,  1.68s/it]11/17/2022 00:15:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:12 - INFO - train.train_snli_ve - loss is tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  43%|####3     | 2914/6700 [1:21:04<1:46:25,  1.69s/it]11/17/2022 00:15:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.4585e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:13 - INFO - train.train_snli_ve - loss is tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2915/6700 [1:21:06<1:47:02,  1.70s/it]11/17/2022 00:15:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:15 - INFO - train.train_snli_ve - loss is tensor(0.7834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2916/6700 [1:21:08<1:46:10,  1.68s/it]11/17/2022 00:15:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6189e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:17 - INFO - train.train_snli_ve - loss is tensor(0.8256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2917/6700 [1:21:09<1:46:03,  1.68s/it]11/17/2022 00:15:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:18 - INFO - train.train_snli_ve - loss is tensor(0.4990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2918/6700 [1:21:11<1:46:55,  1.70s/it]11/17/2022 00:15:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:20 - INFO - train.train_snli_ve - loss is tensor(1.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2919/6700 [1:21:13<1:47:11,  1.70s/it]11/17/2022 00:15:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.9927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:22 - INFO - train.train_snli_ve - loss is tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2920/6700 [1:21:15<1:47:16,  1.70s/it]11/17/2022 00:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(9.7938e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:24 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2921/6700 [1:21:16<1:46:19,  1.69s/it]11/17/2022 00:15:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:25 - INFO - train.train_snli_ve - loss is tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2922/6700 [1:21:18<1:45:48,  1.68s/it]11/17/2022 00:15:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:27 - INFO - train.train_snli_ve - loss is tensor(0.8249, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2923/6700 [1:21:20<1:45:44,  1.68s/it]11/17/2022 00:15:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:29 - INFO - train.train_snli_ve - loss is tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2924/6700 [1:21:21<1:46:21,  1.69s/it]11/17/2022 00:15:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:30 - INFO - train.train_snli_ve - loss is tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2925/6700 [1:21:23<1:46:06,  1.69s/it]11/17/2022 00:15:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4930e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:32 - INFO - train.train_snli_ve - loss is tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2926/6700 [1:21:25<1:45:30,  1.68s/it]11/17/2022 00:15:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.2378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:34 - INFO - train.train_snli_ve - loss is tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2927/6700 [1:21:26<1:46:11,  1.69s/it]11/17/2022 00:15:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:35 - INFO - train.train_snli_ve - loss is tensor(0.8245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2928/6700 [1:21:28<1:45:58,  1.69s/it]11/17/2022 00:15:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.2384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:37 - INFO - train.train_snli_ve - loss is tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2929/6700 [1:21:30<1:45:10,  1.67s/it]11/17/2022 00:15:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:39 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2930/6700 [1:21:31<1:44:55,  1.67s/it]11/17/2022 00:15:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:40 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2931/6700 [1:21:33<1:43:46,  1.65s/it]11/17/2022 00:15:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.4929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:42 - INFO - train.train_snli_ve - loss is tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2932/6700 [1:21:35<1:45:30,  1.68s/it]11/17/2022 00:15:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:44 - INFO - train.train_snli_ve - loss is tensor(0.5138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2933/6700 [1:21:36<1:46:13,  1.69s/it]11/17/2022 00:15:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:45 - INFO - train.train_snli_ve - loss is tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2934/6700 [1:21:38<1:46:30,  1.70s/it]11/17/2022 00:15:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:47 - INFO - train.train_snli_ve - loss is tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2935/6700 [1:21:40<1:46:41,  1.70s/it]11/17/2022 00:15:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.4701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:49 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2936/6700 [1:21:41<1:45:41,  1.68s/it]11/17/2022 00:15:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:50 - INFO - train.train_snli_ve - loss is tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2937/6700 [1:21:43<1:45:46,  1.69s/it]11/17/2022 00:15:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:52 - INFO - train.train_snli_ve - loss is tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2938/6700 [1:21:45<1:45:07,  1.68s/it]11/17/2022 00:15:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:54 - INFO - train.train_snli_ve - loss is tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2939/6700 [1:21:47<1:45:56,  1.69s/it]11/17/2022 00:15:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.1260e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:56 - INFO - train.train_snli_ve - loss is tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2940/6700 [1:21:48<1:46:06,  1.69s/it]11/17/2022 00:15:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:57 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2941/6700 [1:21:50<1:45:38,  1.69s/it]11/17/2022 00:15:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:15:59 - INFO - train.train_snli_ve - loss is tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2942/6700 [1:21:52<1:45:08,  1.68s/it]11/17/2022 00:16:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:01 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2943/6700 [1:21:53<1:44:27,  1.67s/it]11/17/2022 00:16:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0353e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:02 - INFO - train.train_snli_ve - loss is tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2944/6700 [1:21:55<1:44:50,  1.67s/it]11/17/2022 00:16:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:04 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2945/6700 [1:21:57<1:45:25,  1.68s/it]11/17/2022 00:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0552e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:06 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2946/6700 [1:21:58<1:45:35,  1.69s/it]11/17/2022 00:16:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:07 - INFO - train.train_snli_ve - loss is tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####3     | 2947/6700 [1:22:00<1:45:20,  1.68s/it]11/17/2022 00:16:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:09 - INFO - train.train_snli_ve - loss is tensor(0.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2948/6700 [1:22:02<1:45:14,  1.68s/it]11/17/2022 00:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:11 - INFO - train.train_snli_ve - loss is tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2949/6700 [1:22:03<1:44:02,  1.66s/it]11/17/2022 00:16:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:12 - INFO - train.train_snli_ve - loss is tensor(0.7438, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2950/6700 [1:22:05<1:44:14,  1.67s/it]11/17/2022 00:16:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:14 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2951/6700 [1:22:07<1:43:27,  1.66s/it]11/17/2022 00:16:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:16 - INFO - train.train_snli_ve - loss is tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2952/6700 [1:22:08<1:43:11,  1.65s/it]11/17/2022 00:16:17 - INFO - train.train_snli_ve - kd_loss is tensor(8.7005e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:17 - INFO - train.train_snli_ve - loss is tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2953/6700 [1:22:10<1:43:02,  1.65s/it]11/17/2022 00:16:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:19 - INFO - train.train_snli_ve - loss is tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2954/6700 [1:22:12<1:43:02,  1.65s/it]11/17/2022 00:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.1243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:20 - INFO - train.train_snli_ve - loss is tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2955/6700 [1:22:13<1:42:50,  1.65s/it]11/17/2022 00:16:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:22 - INFO - train.train_snli_ve - loss is tensor(0.5908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2956/6700 [1:22:15<1:43:30,  1.66s/it]11/17/2022 00:16:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.2117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:24 - INFO - train.train_snli_ve - loss is tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2957/6700 [1:22:17<1:43:47,  1.66s/it]11/17/2022 00:16:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8189e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:26 - INFO - train.train_snli_ve - loss is tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2958/6700 [1:22:18<1:44:42,  1.68s/it]11/17/2022 00:16:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:27 - INFO - train.train_snli_ve - loss is tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2959/6700 [1:22:20<1:45:15,  1.69s/it]11/17/2022 00:16:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:29 - INFO - train.train_snli_ve - loss is tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2960/6700 [1:22:22<1:45:44,  1.70s/it]11/17/2022 00:16:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:31 - INFO - train.train_snli_ve - loss is tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2961/6700 [1:22:23<1:45:05,  1.69s/it]11/17/2022 00:16:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.1597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:32 - INFO - train.train_snli_ve - loss is tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2962/6700 [1:22:25<1:46:11,  1.70s/it]11/17/2022 00:16:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:34 - INFO - train.train_snli_ve - loss is tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2963/6700 [1:22:27<1:45:35,  1.70s/it]11/17/2022 00:16:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:36 - INFO - train.train_snli_ve - loss is tensor(0.8729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2964/6700 [1:22:28<1:44:45,  1.68s/it]11/17/2022 00:16:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:37 - INFO - train.train_snli_ve - loss is tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2965/6700 [1:22:30<1:44:25,  1.68s/it]11/17/2022 00:16:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:39 - INFO - train.train_snli_ve - loss is tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2966/6700 [1:22:32<1:45:07,  1.69s/it]11/17/2022 00:16:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:41 - INFO - train.train_snli_ve - loss is tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2967/6700 [1:22:33<1:43:49,  1.67s/it]11/17/2022 00:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.9846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:42 - INFO - train.train_snli_ve - loss is tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2968/6700 [1:22:35<1:43:51,  1.67s/it]11/17/2022 00:16:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:44 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2969/6700 [1:22:37<1:43:23,  1.66s/it]11/17/2022 00:16:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.2035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:46 - INFO - train.train_snli_ve - loss is tensor(0.7845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2970/6700 [1:22:38<1:42:24,  1.65s/it]11/17/2022 00:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4786e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:47 - INFO - train.train_snli_ve - loss is tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2971/6700 [1:22:40<1:42:55,  1.66s/it]11/17/2022 00:16:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.3919e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:49 - INFO - train.train_snli_ve - loss is tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2972/6700 [1:22:42<1:43:02,  1.66s/it]11/17/2022 00:16:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.2561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:51 - INFO - train.train_snli_ve - loss is tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2973/6700 [1:22:43<1:42:12,  1.65s/it]11/17/2022 00:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.5648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:52 - INFO - train.train_snli_ve - loss is tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2974/6700 [1:22:45<1:43:27,  1.67s/it]11/17/2022 00:16:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:54 - INFO - train.train_snli_ve - loss is tensor(0.8170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2975/6700 [1:22:47<1:42:45,  1.66s/it]11/17/2022 00:16:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.4539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:56 - INFO - train.train_snli_ve - loss is tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2976/6700 [1:22:48<1:43:03,  1.66s/it]11/17/2022 00:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.9234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:57 - INFO - train.train_snli_ve - loss is tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2977/6700 [1:22:50<1:42:26,  1.65s/it]11/17/2022 00:16:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:16:59 - INFO - train.train_snli_ve - loss is tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2978/6700 [1:22:52<1:43:39,  1.67s/it]11/17/2022 00:17:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:01 - INFO - train.train_snli_ve - loss is tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2979/6700 [1:22:53<1:44:00,  1.68s/it]11/17/2022 00:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:02 - INFO - train.train_snli_ve - loss is tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2980/6700 [1:22:55<1:43:45,  1.67s/it]11/17/2022 00:17:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:04 - INFO - train.train_snli_ve - loss is tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  44%|####4     | 2981/6700 [1:22:57<1:43:46,  1.67s/it]11/17/2022 00:17:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.8475e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:06 - INFO - train.train_snli_ve - loss is tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2982/6700 [1:22:58<1:43:20,  1.67s/it]11/17/2022 00:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:07 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2983/6700 [1:23:00<1:43:56,  1.68s/it]11/17/2022 00:17:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:09 - INFO - train.train_snli_ve - loss is tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2984/6700 [1:23:02<1:44:40,  1.69s/it]11/17/2022 00:17:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.6868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:11 - INFO - train.train_snli_ve - loss is tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2985/6700 [1:23:03<1:44:25,  1.69s/it]11/17/2022 00:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.0012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:12 - INFO - train.train_snli_ve - loss is tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2986/6700 [1:23:05<1:43:52,  1.68s/it]11/17/2022 00:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.3371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:14 - INFO - train.train_snli_ve - loss is tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2987/6700 [1:23:07<1:43:47,  1.68s/it]11/17/2022 00:17:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:16 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2988/6700 [1:23:08<1:43:44,  1.68s/it]11/17/2022 00:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6728e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:17 - INFO - train.train_snli_ve - loss is tensor(0.9186, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2989/6700 [1:23:10<1:43:31,  1.67s/it]11/17/2022 00:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:19 - INFO - train.train_snli_ve - loss is tensor(0.8349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2990/6700 [1:23:12<1:43:26,  1.67s/it]11/17/2022 00:17:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.1017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:21 - INFO - train.train_snli_ve - loss is tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2991/6700 [1:23:13<1:43:54,  1.68s/it]11/17/2022 00:17:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:23 - INFO - train.train_snli_ve - loss is tensor(0.7671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2992/6700 [1:23:15<1:44:27,  1.69s/it]11/17/2022 00:17:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.3182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:24 - INFO - train.train_snli_ve - loss is tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2993/6700 [1:23:17<1:44:29,  1.69s/it]11/17/2022 00:17:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:26 - INFO - train.train_snli_ve - loss is tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2994/6700 [1:23:19<1:44:07,  1.69s/it]11/17/2022 00:17:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.3609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:28 - INFO - train.train_snli_ve - loss is tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2995/6700 [1:23:20<1:43:54,  1.68s/it]11/17/2022 00:17:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:29 - INFO - train.train_snli_ve - loss is tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2996/6700 [1:23:22<1:43:54,  1.68s/it]11/17/2022 00:17:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.5862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:31 - INFO - train.train_snli_ve - loss is tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2997/6700 [1:23:24<1:42:42,  1.66s/it]11/17/2022 00:17:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:33 - INFO - train.train_snli_ve - loss is tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2998/6700 [1:23:25<1:42:15,  1.66s/it]11/17/2022 00:17:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:34 - INFO - train.train_snli_ve - loss is tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 2999/6700 [1:23:27<1:41:54,  1.65s/it]11/17/2022 00:17:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.2098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:36 - INFO - train.train_snli_ve - loss is tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3000/6700 [1:23:29<1:42:23,  1.66s/it]11/17/2022 00:17:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:38 - INFO - train.train_snli_ve - loss is tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3001/6700 [1:23:30<1:43:14,  1.67s/it]11/17/2022 00:17:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:39 - INFO - train.train_snli_ve - loss is tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3002/6700 [1:23:32<1:43:48,  1.68s/it]11/17/2022 00:17:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.4562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:41 - INFO - train.train_snli_ve - loss is tensor(0.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3003/6700 [1:23:34<1:43:48,  1.68s/it]11/17/2022 00:17:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:43 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3004/6700 [1:23:35<1:44:01,  1.69s/it]11/17/2022 00:17:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.7171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:44 - INFO - train.train_snli_ve - loss is tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3005/6700 [1:23:37<1:43:22,  1.68s/it]11/17/2022 00:17:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1725e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:46 - INFO - train.train_snli_ve - loss is tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3006/6700 [1:23:39<1:43:17,  1.68s/it]11/17/2022 00:17:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.3828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:48 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3007/6700 [1:23:40<1:43:30,  1.68s/it]11/17/2022 00:17:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.9620e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:49 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3008/6700 [1:23:42<1:43:43,  1.69s/it]11/17/2022 00:17:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.6412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:51 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3009/6700 [1:23:44<1:43:49,  1.69s/it]11/17/2022 00:17:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.4022e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:53 - INFO - train.train_snli_ve - loss is tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3010/6700 [1:23:45<1:44:21,  1.70s/it]11/17/2022 00:17:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.3823e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:54 - INFO - train.train_snli_ve - loss is tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3011/6700 [1:23:47<1:44:04,  1.69s/it]11/17/2022 00:17:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.1973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:56 - INFO - train.train_snli_ve - loss is tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3012/6700 [1:23:49<1:44:11,  1.70s/it]11/17/2022 00:17:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.8285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:17:58 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3013/6700 [1:23:51<1:44:02,  1.69s/it]11/17/2022 00:18:00 - INFO - train.train_snli_ve - kd_loss is tensor(7.8363e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:00 - INFO - train.train_snli_ve - loss is tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####4     | 3014/6700 [1:23:52<1:43:43,  1.69s/it]11/17/2022 00:18:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.0234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:01 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3015/6700 [1:23:54<1:43:22,  1.68s/it]11/17/2022 00:18:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.1419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:03 - INFO - train.train_snli_ve - loss is tensor(1.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3016/6700 [1:23:56<1:43:25,  1.68s/it]11/17/2022 00:18:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.9658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:05 - INFO - train.train_snli_ve - loss is tensor(0.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3017/6700 [1:23:57<1:43:09,  1.68s/it]11/17/2022 00:18:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.8355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:06 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3018/6700 [1:23:59<1:43:36,  1.69s/it]11/17/2022 00:18:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.0637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:08 - INFO - train.train_snli_ve - loss is tensor(0.8682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3019/6700 [1:24:01<1:43:32,  1.69s/it]11/17/2022 00:18:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.7226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:10 - INFO - train.train_snli_ve - loss is tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3020/6700 [1:24:02<1:44:29,  1.70s/it]11/17/2022 00:18:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:11 - INFO - train.train_snli_ve - loss is tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3021/6700 [1:24:04<1:44:22,  1.70s/it]11/17/2022 00:18:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.2090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:13 - INFO - train.train_snli_ve - loss is tensor(0.9393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3022/6700 [1:24:06<1:44:24,  1.70s/it]11/17/2022 00:18:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.0333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:15 - INFO - train.train_snli_ve - loss is tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3023/6700 [1:24:07<1:43:36,  1.69s/it]11/17/2022 00:18:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:16 - INFO - train.train_snli_ve - loss is tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3024/6700 [1:24:09<1:43:25,  1.69s/it]11/17/2022 00:18:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:18 - INFO - train.train_snli_ve - loss is tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3025/6700 [1:24:11<1:43:12,  1.69s/it]11/17/2022 00:18:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7734e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:20 - INFO - train.train_snli_ve - loss is tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3026/6700 [1:24:12<1:43:15,  1.69s/it]11/17/2022 00:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:21 - INFO - train.train_snli_ve - loss is tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3027/6700 [1:24:14<1:43:33,  1.69s/it]11/17/2022 00:18:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.6113e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:23 - INFO - train.train_snli_ve - loss is tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3028/6700 [1:24:16<1:43:22,  1.69s/it]11/17/2022 00:18:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:25 - INFO - train.train_snli_ve - loss is tensor(0.7528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3029/6700 [1:24:18<1:43:39,  1.69s/it]11/17/2022 00:18:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:27 - INFO - train.train_snli_ve - loss is tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3030/6700 [1:24:19<1:43:04,  1.69s/it]11/17/2022 00:18:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7401e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:28 - INFO - train.train_snli_ve - loss is tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3031/6700 [1:24:21<1:42:54,  1.68s/it]11/17/2022 00:18:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:30 - INFO - train.train_snli_ve - loss is tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3032/6700 [1:24:23<1:42:44,  1.68s/it]11/17/2022 00:18:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.2776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:32 - INFO - train.train_snli_ve - loss is tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3033/6700 [1:24:24<1:42:25,  1.68s/it]11/17/2022 00:18:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:33 - INFO - train.train_snli_ve - loss is tensor(1.0477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3034/6700 [1:24:26<1:42:40,  1.68s/it]11/17/2022 00:18:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:35 - INFO - train.train_snli_ve - loss is tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3035/6700 [1:24:28<1:42:00,  1.67s/it]11/17/2022 00:18:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.3701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:37 - INFO - train.train_snli_ve - loss is tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3036/6700 [1:24:29<1:41:57,  1.67s/it]11/17/2022 00:18:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:38 - INFO - train.train_snli_ve - loss is tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3037/6700 [1:24:31<1:42:39,  1.68s/it]11/17/2022 00:18:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6772e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:40 - INFO - train.train_snli_ve - loss is tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3038/6700 [1:24:33<1:42:36,  1.68s/it]11/17/2022 00:18:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7449e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:42 - INFO - train.train_snli_ve - loss is tensor(0.7356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3039/6700 [1:24:34<1:42:25,  1.68s/it]11/17/2022 00:18:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:43 - INFO - train.train_snli_ve - loss is tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3040/6700 [1:24:36<1:42:52,  1.69s/it]11/17/2022 00:18:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:45 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3041/6700 [1:24:38<1:42:09,  1.68s/it]11/17/2022 00:18:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6719e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:47 - INFO - train.train_snli_ve - loss is tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3042/6700 [1:24:39<1:42:25,  1.68s/it]11/17/2022 00:18:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:48 - INFO - train.train_snli_ve - loss is tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3043/6700 [1:24:41<1:42:57,  1.69s/it]11/17/2022 00:18:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.2721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:50 - INFO - train.train_snli_ve - loss is tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3044/6700 [1:24:43<1:42:46,  1.69s/it]11/17/2022 00:18:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.4927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:52 - INFO - train.train_snli_ve - loss is tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3045/6700 [1:24:44<1:42:24,  1.68s/it]11/17/2022 00:18:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2475e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:53 - INFO - train.train_snli_ve - loss is tensor(0.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3046/6700 [1:24:46<1:41:52,  1.67s/it]11/17/2022 00:18:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1428e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:55 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3047/6700 [1:24:48<1:42:34,  1.68s/it]11/17/2022 00:18:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.5545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:57 - INFO - train.train_snli_ve - loss is tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  45%|####5     | 3048/6700 [1:24:49<1:42:33,  1.68s/it]11/17/2022 00:18:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2203e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:18:58 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3049/6700 [1:24:51<1:42:12,  1.68s/it]11/17/2022 00:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:00 - INFO - train.train_snli_ve - loss is tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3050/6700 [1:24:53<1:43:13,  1.70s/it]11/17/2022 00:19:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.5178e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:02 - INFO - train.train_snli_ve - loss is tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3051/6700 [1:24:55<1:43:42,  1.71s/it]11/17/2022 00:19:04 - INFO - train.train_snli_ve - kd_loss is tensor(9.3640e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:04 - INFO - train.train_snli_ve - loss is tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3052/6700 [1:24:56<1:43:43,  1.71s/it]11/17/2022 00:19:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:05 - INFO - train.train_snli_ve - loss is tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3053/6700 [1:24:58<1:43:18,  1.70s/it]11/17/2022 00:19:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.2439e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:07 - INFO - train.train_snli_ve - loss is tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3054/6700 [1:25:00<1:42:51,  1.69s/it]11/17/2022 00:19:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:09 - INFO - train.train_snli_ve - loss is tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3055/6700 [1:25:01<1:43:26,  1.70s/it]11/17/2022 00:19:10 - INFO - train.train_snli_ve - kd_loss is tensor(8.7045e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:10 - INFO - train.train_snli_ve - loss is tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3056/6700 [1:25:03<1:42:30,  1.69s/it]11/17/2022 00:19:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.4828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:12 - INFO - train.train_snli_ve - loss is tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3057/6700 [1:25:05<1:42:26,  1.69s/it]11/17/2022 00:19:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.5703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:14 - INFO - train.train_snli_ve - loss is tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3058/6700 [1:25:06<1:42:32,  1.69s/it]11/17/2022 00:19:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:15 - INFO - train.train_snli_ve - loss is tensor(0.8359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3059/6700 [1:25:08<1:42:10,  1.68s/it]11/17/2022 00:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:17 - INFO - train.train_snli_ve - loss is tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3060/6700 [1:25:10<1:42:20,  1.69s/it]11/17/2022 00:19:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5585e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:19 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3061/6700 [1:25:11<1:41:54,  1.68s/it]11/17/2022 00:19:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:20 - INFO - train.train_snli_ve - loss is tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3062/6700 [1:25:13<1:42:30,  1.69s/it]11/17/2022 00:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4678e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:22 - INFO - train.train_snli_ve - loss is tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3063/6700 [1:25:15<1:42:23,  1.69s/it]11/17/2022 00:19:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:24 - INFO - train.train_snli_ve - loss is tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3064/6700 [1:25:17<1:41:57,  1.68s/it]11/17/2022 00:19:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.1679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:25 - INFO - train.train_snli_ve - loss is tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3065/6700 [1:25:18<1:41:29,  1.68s/it]11/17/2022 00:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:27 - INFO - train.train_snli_ve - loss is tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3066/6700 [1:25:20<1:42:29,  1.69s/it]11/17/2022 00:19:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:29 - INFO - train.train_snli_ve - loss is tensor(0.9751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3067/6700 [1:25:22<1:41:40,  1.68s/it]11/17/2022 00:19:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.7197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:31 - INFO - train.train_snli_ve - loss is tensor(0.5541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3068/6700 [1:25:23<1:42:05,  1.69s/it]11/17/2022 00:19:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.2359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:32 - INFO - train.train_snli_ve - loss is tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3069/6700 [1:25:25<1:41:42,  1.68s/it]11/17/2022 00:19:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:34 - INFO - train.train_snli_ve - loss is tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3070/6700 [1:25:27<1:41:42,  1.68s/it]11/17/2022 00:19:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:36 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3071/6700 [1:25:28<1:42:14,  1.69s/it]11/17/2022 00:19:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4532e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:37 - INFO - train.train_snli_ve - loss is tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3072/6700 [1:25:30<1:41:46,  1.68s/it]11/17/2022 00:19:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9435e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:39 - INFO - train.train_snli_ve - loss is tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3073/6700 [1:25:32<1:40:49,  1.67s/it]11/17/2022 00:19:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4764e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:41 - INFO - train.train_snli_ve - loss is tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3074/6700 [1:25:33<1:41:03,  1.67s/it]11/17/2022 00:19:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:42 - INFO - train.train_snli_ve - loss is tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3075/6700 [1:25:35<1:40:56,  1.67s/it]11/17/2022 00:19:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.4877e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:44 - INFO - train.train_snli_ve - loss is tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3076/6700 [1:25:37<1:41:38,  1.68s/it]11/17/2022 00:19:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:46 - INFO - train.train_snli_ve - loss is tensor(0.5065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3077/6700 [1:25:38<1:41:11,  1.68s/it]11/17/2022 00:19:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.0790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:47 - INFO - train.train_snli_ve - loss is tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3078/6700 [1:25:40<1:41:13,  1.68s/it]11/17/2022 00:19:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.2317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:49 - INFO - train.train_snli_ve - loss is tensor(0.8456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3079/6700 [1:25:42<1:41:33,  1.68s/it]11/17/2022 00:19:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.9126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:51 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3080/6700 [1:25:43<1:41:09,  1.68s/it]11/17/2022 00:19:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:52 - INFO - train.train_snli_ve - loss is tensor(0.9854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####5     | 3081/6700 [1:25:45<1:40:37,  1.67s/it]11/17/2022 00:19:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:54 - INFO - train.train_snli_ve - loss is tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3082/6700 [1:25:47<1:41:22,  1.68s/it]11/17/2022 00:19:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:56 - INFO - train.train_snli_ve - loss is tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3083/6700 [1:25:48<1:42:13,  1.70s/it]11/17/2022 00:19:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:58 - INFO - train.train_snli_ve - loss is tensor(0.8766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3084/6700 [1:25:50<1:42:54,  1.71s/it]11/17/2022 00:19:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:19:59 - INFO - train.train_snli_ve - loss is tensor(0.3478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3085/6700 [1:25:52<1:42:13,  1.70s/it]11/17/2022 00:20:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:01 - INFO - train.train_snli_ve - loss is tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3086/6700 [1:25:54<1:41:21,  1.68s/it]11/17/2022 00:20:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:03 - INFO - train.train_snli_ve - loss is tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3087/6700 [1:25:55<1:41:49,  1.69s/it]11/17/2022 00:20:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.5653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:04 - INFO - train.train_snli_ve - loss is tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3088/6700 [1:25:57<1:42:20,  1.70s/it]11/17/2022 00:20:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:06 - INFO - train.train_snli_ve - loss is tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3089/6700 [1:25:59<1:41:52,  1.69s/it]11/17/2022 00:20:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:08 - INFO - train.train_snli_ve - loss is tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3090/6700 [1:26:00<1:41:54,  1.69s/it]11/17/2022 00:20:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:09 - INFO - train.train_snli_ve - loss is tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3091/6700 [1:26:02<1:42:11,  1.70s/it]11/17/2022 00:20:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:11 - INFO - train.train_snli_ve - loss is tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3092/6700 [1:26:04<1:41:49,  1.69s/it]11/17/2022 00:20:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.5249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:13 - INFO - train.train_snli_ve - loss is tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3093/6700 [1:26:05<1:41:38,  1.69s/it]11/17/2022 00:20:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:14 - INFO - train.train_snli_ve - loss is tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3094/6700 [1:26:07<1:40:51,  1.68s/it]11/17/2022 00:20:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:16 - INFO - train.train_snli_ve - loss is tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3095/6700 [1:26:09<1:40:26,  1.67s/it]11/17/2022 00:20:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:18 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3096/6700 [1:26:10<1:40:54,  1.68s/it]11/17/2022 00:20:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.0930e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:19 - INFO - train.train_snli_ve - loss is tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3097/6700 [1:26:12<1:40:41,  1.68s/it]11/17/2022 00:20:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:21 - INFO - train.train_snli_ve - loss is tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3098/6700 [1:26:14<1:40:41,  1.68s/it]11/17/2022 00:20:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:23 - INFO - train.train_snli_ve - loss is tensor(0.7470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3099/6700 [1:26:15<1:40:54,  1.68s/it]11/17/2022 00:20:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:24 - INFO - train.train_snli_ve - loss is tensor(0.8260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3100/6700 [1:26:17<1:40:36,  1.68s/it]11/17/2022 00:20:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.4730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:26 - INFO - train.train_snli_ve - loss is tensor(0.7751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3101/6700 [1:26:19<1:40:47,  1.68s/it]11/17/2022 00:20:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:28 - INFO - train.train_snli_ve - loss is tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3102/6700 [1:26:21<1:40:54,  1.68s/it]11/17/2022 00:20:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.1839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:30 - INFO - train.train_snli_ve - loss is tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3103/6700 [1:26:22<1:41:11,  1.69s/it]11/17/2022 00:20:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:31 - INFO - train.train_snli_ve - loss is tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3104/6700 [1:26:24<1:41:09,  1.69s/it]11/17/2022 00:20:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:33 - INFO - train.train_snli_ve - loss is tensor(0.9526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3105/6700 [1:26:26<1:41:07,  1.69s/it]11/17/2022 00:20:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:35 - INFO - train.train_snli_ve - loss is tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3106/6700 [1:26:27<1:40:20,  1.68s/it]11/17/2022 00:20:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2004e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:36 - INFO - train.train_snli_ve - loss is tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3107/6700 [1:26:29<1:40:38,  1.68s/it]11/17/2022 00:20:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:38 - INFO - train.train_snli_ve - loss is tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3108/6700 [1:26:31<1:39:38,  1.66s/it]11/17/2022 00:20:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1754e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:40 - INFO - train.train_snli_ve - loss is tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3109/6700 [1:26:32<1:39:57,  1.67s/it]11/17/2022 00:20:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:41 - INFO - train.train_snli_ve - loss is tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3110/6700 [1:26:34<1:40:14,  1.68s/it]11/17/2022 00:20:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1757e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:43 - INFO - train.train_snli_ve - loss is tensor(0.7941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3111/6700 [1:26:36<1:40:58,  1.69s/it]11/17/2022 00:20:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.4001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:45 - INFO - train.train_snli_ve - loss is tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3112/6700 [1:26:37<1:40:41,  1.68s/it]11/17/2022 00:20:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.5321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:46 - INFO - train.train_snli_ve - loss is tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3113/6700 [1:26:39<1:40:10,  1.68s/it]11/17/2022 00:20:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:48 - INFO - train.train_snli_ve - loss is tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3114/6700 [1:26:41<1:40:10,  1.68s/it]11/17/2022 00:20:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.1688e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:50 - INFO - train.train_snli_ve - loss is tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  46%|####6     | 3115/6700 [1:26:42<1:40:24,  1.68s/it]11/17/2022 00:20:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.1630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:51 - INFO - train.train_snli_ve - loss is tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3116/6700 [1:26:44<1:40:10,  1.68s/it]11/17/2022 00:20:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.7104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:53 - INFO - train.train_snli_ve - loss is tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3117/6700 [1:26:46<1:40:05,  1.68s/it]11/17/2022 00:20:55 - INFO - train.train_snli_ve - kd_loss is tensor(7.8917e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:55 - INFO - train.train_snli_ve - loss is tensor(0.9032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3118/6700 [1:26:47<1:40:35,  1.68s/it]11/17/2022 00:20:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2099e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:56 - INFO - train.train_snli_ve - loss is tensor(0.8257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3119/6700 [1:26:49<1:41:27,  1.70s/it]11/17/2022 00:20:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:20:58 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3120/6700 [1:26:51<1:40:49,  1.69s/it]11/17/2022 00:21:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:00 - INFO - train.train_snli_ve - loss is tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3121/6700 [1:26:52<1:40:54,  1.69s/it]11/17/2022 00:21:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.9719e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:01 - INFO - train.train_snli_ve - loss is tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3122/6700 [1:26:54<1:40:40,  1.69s/it]11/17/2022 00:21:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.0245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:03 - INFO - train.train_snli_ve - loss is tensor(0.7760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3123/6700 [1:26:56<1:39:41,  1.67s/it]11/17/2022 00:21:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.3806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:05 - INFO - train.train_snli_ve - loss is tensor(0.8663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3124/6700 [1:26:57<1:40:15,  1.68s/it]11/17/2022 00:21:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:06 - INFO - train.train_snli_ve - loss is tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3125/6700 [1:26:59<1:40:29,  1.69s/it]11/17/2022 00:21:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.1622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:08 - INFO - train.train_snli_ve - loss is tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3126/6700 [1:27:01<1:39:42,  1.67s/it]11/17/2022 00:21:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.3157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:10 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3127/6700 [1:27:03<1:40:10,  1.68s/it]11/17/2022 00:21:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:12 - INFO - train.train_snli_ve - loss is tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3128/6700 [1:27:04<1:39:49,  1.68s/it]11/17/2022 00:21:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.8873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:13 - INFO - train.train_snli_ve - loss is tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3129/6700 [1:27:06<1:40:00,  1.68s/it]11/17/2022 00:21:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:15 - INFO - train.train_snli_ve - loss is tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3130/6700 [1:27:08<1:39:55,  1.68s/it]11/17/2022 00:21:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:17 - INFO - train.train_snli_ve - loss is tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3131/6700 [1:27:09<1:39:41,  1.68s/it]11/17/2022 00:21:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.9580e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:18 - INFO - train.train_snli_ve - loss is tensor(0.9213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3132/6700 [1:27:11<1:38:50,  1.66s/it]11/17/2022 00:21:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.2487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:20 - INFO - train.train_snli_ve - loss is tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3133/6700 [1:27:13<1:38:58,  1.66s/it]11/17/2022 00:21:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:22 - INFO - train.train_snli_ve - loss is tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3134/6700 [1:27:14<1:39:26,  1.67s/it]11/17/2022 00:21:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3327e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:23 - INFO - train.train_snli_ve - loss is tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3135/6700 [1:27:16<1:39:06,  1.67s/it]11/17/2022 00:21:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:25 - INFO - train.train_snli_ve - loss is tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3136/6700 [1:27:18<1:38:43,  1.66s/it]11/17/2022 00:21:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:26 - INFO - train.train_snli_ve - loss is tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3137/6700 [1:27:19<1:38:25,  1.66s/it]11/17/2022 00:21:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:28 - INFO - train.train_snli_ve - loss is tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3138/6700 [1:27:21<1:38:45,  1.66s/it]11/17/2022 00:21:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.9559e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:30 - INFO - train.train_snli_ve - loss is tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3139/6700 [1:27:22<1:38:20,  1.66s/it]11/17/2022 00:21:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.8664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:31 - INFO - train.train_snli_ve - loss is tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3140/6700 [1:27:24<1:37:40,  1.65s/it]11/17/2022 00:21:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:33 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3141/6700 [1:27:26<1:38:28,  1.66s/it]11/17/2022 00:21:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:35 - INFO - train.train_snli_ve - loss is tensor(0.9672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3142/6700 [1:27:27<1:38:39,  1.66s/it]11/17/2022 00:21:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.4093e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:36 - INFO - train.train_snli_ve - loss is tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3143/6700 [1:27:29<1:39:16,  1.67s/it]11/17/2022 00:21:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:38 - INFO - train.train_snli_ve - loss is tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3144/6700 [1:27:31<1:39:01,  1.67s/it]11/17/2022 00:21:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:40 - INFO - train.train_snli_ve - loss is tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3145/6700 [1:27:33<1:39:56,  1.69s/it]11/17/2022 00:21:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.0510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:42 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3146/6700 [1:27:34<1:39:57,  1.69s/it]11/17/2022 00:21:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:43 - INFO - train.train_snli_ve - loss is tensor(0.9383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3147/6700 [1:27:36<1:39:41,  1.68s/it]11/17/2022 00:21:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.5640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:45 - INFO - train.train_snli_ve - loss is tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3148/6700 [1:27:38<1:39:17,  1.68s/it]11/17/2022 00:21:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7707e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:47 - INFO - train.train_snli_ve - loss is tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####6     | 3149/6700 [1:27:39<1:38:55,  1.67s/it]11/17/2022 00:21:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:48 - INFO - train.train_snli_ve - loss is tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3150/6700 [1:27:41<1:39:11,  1.68s/it]11/17/2022 00:21:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.2810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:50 - INFO - train.train_snli_ve - loss is tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3151/6700 [1:27:43<1:39:29,  1.68s/it]11/17/2022 00:21:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.4509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:52 - INFO - train.train_snli_ve - loss is tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3152/6700 [1:27:44<1:38:49,  1.67s/it]11/17/2022 00:21:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.0724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:53 - INFO - train.train_snli_ve - loss is tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3153/6700 [1:27:46<1:38:01,  1.66s/it]11/17/2022 00:21:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:55 - INFO - train.train_snli_ve - loss is tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3154/6700 [1:27:48<1:38:17,  1.66s/it]11/17/2022 00:21:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.9937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:57 - INFO - train.train_snli_ve - loss is tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3155/6700 [1:27:49<1:38:09,  1.66s/it]11/17/2022 00:21:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.1174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:21:58 - INFO - train.train_snli_ve - loss is tensor(0.8053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3156/6700 [1:27:51<1:38:38,  1.67s/it]11/17/2022 00:22:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:00 - INFO - train.train_snli_ve - loss is tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3157/6700 [1:27:53<1:39:05,  1.68s/it]11/17/2022 00:22:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.5515e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:02 - INFO - train.train_snli_ve - loss is tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3158/6700 [1:27:54<1:39:32,  1.69s/it]11/17/2022 00:22:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.1987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:03 - INFO - train.train_snli_ve - loss is tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3159/6700 [1:27:56<1:39:29,  1.69s/it]11/17/2022 00:22:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:05 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3160/6700 [1:27:58<1:39:12,  1.68s/it]11/17/2022 00:22:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.3185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:07 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3161/6700 [1:27:59<1:38:37,  1.67s/it]11/17/2022 00:22:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:08 - INFO - train.train_snli_ve - loss is tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3162/6700 [1:28:01<1:38:15,  1.67s/it]11/17/2022 00:22:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.4469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:10 - INFO - train.train_snli_ve - loss is tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3163/6700 [1:28:03<1:38:20,  1.67s/it]11/17/2022 00:22:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:12 - INFO - train.train_snli_ve - loss is tensor(0.8200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3164/6700 [1:28:04<1:38:57,  1.68s/it]11/17/2022 00:22:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.4939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:13 - INFO - train.train_snli_ve - loss is tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3165/6700 [1:28:06<1:38:17,  1.67s/it]11/17/2022 00:22:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:15 - INFO - train.train_snli_ve - loss is tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3166/6700 [1:28:08<1:37:34,  1.66s/it]11/17/2022 00:22:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.1356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:17 - INFO - train.train_snli_ve - loss is tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3167/6700 [1:28:09<1:37:25,  1.65s/it]11/17/2022 00:22:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:18 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3168/6700 [1:28:11<1:38:00,  1.66s/it]11/17/2022 00:22:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:20 - INFO - train.train_snli_ve - loss is tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3169/6700 [1:28:13<1:38:55,  1.68s/it]11/17/2022 00:22:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.5600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:22 - INFO - train.train_snli_ve - loss is tensor(0.8102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3170/6700 [1:28:14<1:39:21,  1.69s/it]11/17/2022 00:22:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:23 - INFO - train.train_snli_ve - loss is tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3171/6700 [1:28:16<1:40:11,  1.70s/it]11/17/2022 00:22:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9725e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:25 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3172/6700 [1:28:18<1:39:44,  1.70s/it]11/17/2022 00:22:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:27 - INFO - train.train_snli_ve - loss is tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3173/6700 [1:28:20<1:39:35,  1.69s/it]11/17/2022 00:22:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1739e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:29 - INFO - train.train_snli_ve - loss is tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3174/6700 [1:28:21<1:39:14,  1.69s/it]11/17/2022 00:22:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:30 - INFO - train.train_snli_ve - loss is tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3175/6700 [1:28:23<1:39:15,  1.69s/it]11/17/2022 00:22:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:32 - INFO - train.train_snli_ve - loss is tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3176/6700 [1:28:25<1:39:08,  1.69s/it]11/17/2022 00:22:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.3277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:34 - INFO - train.train_snli_ve - loss is tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3177/6700 [1:28:26<1:38:49,  1.68s/it]11/17/2022 00:22:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:35 - INFO - train.train_snli_ve - loss is tensor(0.7953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3178/6700 [1:28:28<1:38:55,  1.69s/it]11/17/2022 00:22:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:37 - INFO - train.train_snli_ve - loss is tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3179/6700 [1:28:30<1:39:13,  1.69s/it]11/17/2022 00:22:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:39 - INFO - train.train_snli_ve - loss is tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3180/6700 [1:28:31<1:38:31,  1.68s/it]11/17/2022 00:22:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.0738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:40 - INFO - train.train_snli_ve - loss is tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3181/6700 [1:28:33<1:38:30,  1.68s/it]11/17/2022 00:22:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:42 - INFO - train.train_snli_ve - loss is tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  47%|####7     | 3182/6700 [1:28:35<1:39:02,  1.69s/it]11/17/2022 00:22:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.9894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:44 - INFO - train.train_snli_ve - loss is tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3183/6700 [1:28:36<1:38:56,  1.69s/it]11/17/2022 00:22:45 - INFO - train.train_snli_ve - kd_loss is tensor(6.7531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:45 - INFO - train.train_snli_ve - loss is tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3184/6700 [1:28:38<1:39:12,  1.69s/it]11/17/2022 00:22:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.7526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:47 - INFO - train.train_snli_ve - loss is tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3185/6700 [1:28:40<1:38:32,  1.68s/it]11/17/2022 00:22:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.3492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:49 - INFO - train.train_snli_ve - loss is tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3186/6700 [1:28:41<1:37:48,  1.67s/it]11/17/2022 00:22:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.0771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:50 - INFO - train.train_snli_ve - loss is tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3187/6700 [1:28:43<1:38:41,  1.69s/it]11/17/2022 00:22:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:52 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3188/6700 [1:28:45<1:38:05,  1.68s/it]11/17/2022 00:22:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5986e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:54 - INFO - train.train_snli_ve - loss is tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3189/6700 [1:28:46<1:38:05,  1.68s/it]11/17/2022 00:22:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:55 - INFO - train.train_snli_ve - loss is tensor(0.5271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3190/6700 [1:28:48<1:38:38,  1.69s/it]11/17/2022 00:22:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.3644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:57 - INFO - train.train_snli_ve - loss is tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3191/6700 [1:28:50<1:38:39,  1.69s/it]11/17/2022 00:22:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.3406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:22:59 - INFO - train.train_snli_ve - loss is tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3192/6700 [1:28:52<1:39:11,  1.70s/it]11/17/2022 00:23:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:01 - INFO - train.train_snli_ve - loss is tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3193/6700 [1:28:53<1:39:06,  1.70s/it]11/17/2022 00:23:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:02 - INFO - train.train_snli_ve - loss is tensor(0.4603, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3194/6700 [1:28:55<1:38:27,  1.68s/it]11/17/2022 00:23:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:04 - INFO - train.train_snli_ve - loss is tensor(0.4491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3195/6700 [1:28:57<1:37:50,  1.67s/it]11/17/2022 00:23:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:06 - INFO - train.train_snli_ve - loss is tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3196/6700 [1:28:58<1:37:38,  1.67s/it]11/17/2022 00:23:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.3960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:07 - INFO - train.train_snli_ve - loss is tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3197/6700 [1:29:00<1:37:46,  1.67s/it]11/17/2022 00:23:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:09 - INFO - train.train_snli_ve - loss is tensor(1.0743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3198/6700 [1:29:02<1:38:31,  1.69s/it]11/17/2022 00:23:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:11 - INFO - train.train_snli_ve - loss is tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3199/6700 [1:29:03<1:38:32,  1.69s/it]11/17/2022 00:23:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.9013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:12 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3200/6700 [1:29:05<1:38:39,  1.69s/it]11/17/2022 00:23:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.4577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:14 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3201/6700 [1:29:07<1:37:47,  1.68s/it]11/17/2022 00:23:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.0979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:16 - INFO - train.train_snli_ve - loss is tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3202/6700 [1:29:08<1:36:57,  1.66s/it]11/17/2022 00:23:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.0541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:17 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3203/6700 [1:29:10<1:37:20,  1.67s/it]11/17/2022 00:23:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.0805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:19 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3204/6700 [1:29:12<1:37:42,  1.68s/it]11/17/2022 00:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.8910e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:21 - INFO - train.train_snli_ve - loss is tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3205/6700 [1:29:13<1:37:37,  1.68s/it]11/17/2022 00:23:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4559e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:22 - INFO - train.train_snli_ve - loss is tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3206/6700 [1:29:15<1:38:00,  1.68s/it]11/17/2022 00:23:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.3224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:24 - INFO - train.train_snli_ve - loss is tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3207/6700 [1:29:17<1:37:48,  1.68s/it]11/17/2022 00:23:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:26 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3208/6700 [1:29:18<1:37:38,  1.68s/it]11/17/2022 00:23:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:27 - INFO - train.train_snli_ve - loss is tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3209/6700 [1:29:20<1:37:07,  1.67s/it]11/17/2022 00:23:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.7283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:29 - INFO - train.train_snli_ve - loss is tensor(0.4710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3210/6700 [1:29:22<1:38:07,  1.69s/it]11/17/2022 00:23:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.9380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:31 - INFO - train.train_snli_ve - loss is tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3211/6700 [1:29:23<1:38:13,  1.69s/it]11/17/2022 00:23:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:32 - INFO - train.train_snli_ve - loss is tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3212/6700 [1:29:25<1:38:39,  1.70s/it]11/17/2022 00:23:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0064e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:34 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3213/6700 [1:29:27<1:38:51,  1.70s/it]11/17/2022 00:23:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.0049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:36 - INFO - train.train_snli_ve - loss is tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3214/6700 [1:29:29<1:37:59,  1.69s/it]11/17/2022 00:23:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3885e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:38 - INFO - train.train_snli_ve - loss is tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####7     | 3215/6700 [1:29:30<1:38:07,  1.69s/it]11/17/2022 00:23:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5993e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:39 - INFO - train.train_snli_ve - loss is tensor(0.8631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3216/6700 [1:29:32<1:37:08,  1.67s/it]11/17/2022 00:23:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:41 - INFO - train.train_snli_ve - loss is tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3217/6700 [1:29:34<1:37:43,  1.68s/it]11/17/2022 00:23:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:43 - INFO - train.train_snli_ve - loss is tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3218/6700 [1:29:35<1:37:16,  1.68s/it]11/17/2022 00:23:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.2543e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:44 - INFO - train.train_snli_ve - loss is tensor(0.8897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3219/6700 [1:29:37<1:36:57,  1.67s/it]11/17/2022 00:23:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:46 - INFO - train.train_snli_ve - loss is tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3220/6700 [1:29:39<1:37:21,  1.68s/it]11/17/2022 00:23:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.4459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:48 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3221/6700 [1:29:40<1:37:36,  1.68s/it]11/17/2022 00:23:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.3286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:49 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3222/6700 [1:29:42<1:37:48,  1.69s/it]11/17/2022 00:23:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.8260e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:51 - INFO - train.train_snli_ve - loss is tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3223/6700 [1:29:44<1:38:31,  1.70s/it]11/17/2022 00:23:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.5314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:53 - INFO - train.train_snli_ve - loss is tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3224/6700 [1:29:45<1:38:23,  1.70s/it]11/17/2022 00:23:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:54 - INFO - train.train_snli_ve - loss is tensor(0.7773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3225/6700 [1:29:47<1:37:33,  1.68s/it]11/17/2022 00:23:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:56 - INFO - train.train_snli_ve - loss is tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3226/6700 [1:29:49<1:36:54,  1.67s/it]11/17/2022 00:23:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:58 - INFO - train.train_snli_ve - loss is tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3227/6700 [1:29:50<1:36:56,  1.67s/it]11/17/2022 00:23:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8415e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:23:59 - INFO - train.train_snli_ve - loss is tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3228/6700 [1:29:52<1:36:42,  1.67s/it]11/17/2022 00:24:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.5380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:01 - INFO - train.train_snli_ve - loss is tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3229/6700 [1:29:54<1:37:24,  1.68s/it]11/17/2022 00:24:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.6211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:03 - INFO - train.train_snli_ve - loss is tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3230/6700 [1:29:55<1:37:28,  1.69s/it]11/17/2022 00:24:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2478e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:04 - INFO - train.train_snli_ve - loss is tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3231/6700 [1:29:57<1:36:30,  1.67s/it]11/17/2022 00:24:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.3091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:06 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3232/6700 [1:29:59<1:36:20,  1.67s/it]11/17/2022 00:24:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:08 - INFO - train.train_snli_ve - loss is tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3233/6700 [1:30:00<1:37:08,  1.68s/it]11/17/2022 00:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:09 - INFO - train.train_snli_ve - loss is tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3234/6700 [1:30:02<1:37:56,  1.70s/it]11/17/2022 00:24:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:11 - INFO - train.train_snli_ve - loss is tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3235/6700 [1:30:04<1:37:25,  1.69s/it]11/17/2022 00:24:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:13 - INFO - train.train_snli_ve - loss is tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3236/6700 [1:30:06<1:37:17,  1.69s/it]11/17/2022 00:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.6629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:14 - INFO - train.train_snli_ve - loss is tensor(0.8966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3237/6700 [1:30:07<1:36:32,  1.67s/it]11/17/2022 00:24:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:16 - INFO - train.train_snli_ve - loss is tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3238/6700 [1:30:09<1:36:20,  1.67s/it]11/17/2022 00:24:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.1595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:18 - INFO - train.train_snli_ve - loss is tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3239/6700 [1:30:10<1:35:52,  1.66s/it]11/17/2022 00:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.8481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:19 - INFO - train.train_snli_ve - loss is tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3240/6700 [1:30:12<1:35:42,  1.66s/it]11/17/2022 00:24:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.8767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:21 - INFO - train.train_snli_ve - loss is tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3241/6700 [1:30:14<1:35:59,  1.67s/it]11/17/2022 00:24:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.0360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:23 - INFO - train.train_snli_ve - loss is tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3242/6700 [1:30:15<1:36:26,  1.67s/it]11/17/2022 00:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.5280e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:24 - INFO - train.train_snli_ve - loss is tensor(0.8512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3243/6700 [1:30:17<1:36:23,  1.67s/it]11/17/2022 00:24:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.8919e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:26 - INFO - train.train_snli_ve - loss is tensor(0.5476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3244/6700 [1:30:19<1:36:23,  1.67s/it]11/17/2022 00:24:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.1429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:28 - INFO - train.train_snli_ve - loss is tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3245/6700 [1:30:20<1:36:09,  1.67s/it]11/17/2022 00:24:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.1814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:29 - INFO - train.train_snli_ve - loss is tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3246/6700 [1:30:22<1:35:35,  1.66s/it]11/17/2022 00:24:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.0943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:31 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3247/6700 [1:30:24<1:36:19,  1.67s/it]11/17/2022 00:24:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:33 - INFO - train.train_snli_ve - loss is tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3248/6700 [1:30:25<1:35:54,  1.67s/it]11/17/2022 00:24:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.6019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:34 - INFO - train.train_snli_ve - loss is tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  48%|####8     | 3249/6700 [1:30:27<1:36:17,  1.67s/it]11/17/2022 00:24:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:36 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3250/6700 [1:30:29<1:36:49,  1.68s/it]11/17/2022 00:24:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.7226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:38 - INFO - train.train_snli_ve - loss is tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3251/6700 [1:30:31<1:36:08,  1.67s/it]11/17/2022 00:24:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:40 - INFO - train.train_snli_ve - loss is tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3252/6700 [1:30:32<1:36:08,  1.67s/it]11/17/2022 00:24:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:41 - INFO - train.train_snli_ve - loss is tensor(0.4407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3253/6700 [1:30:34<1:36:03,  1.67s/it]11/17/2022 00:24:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:43 - INFO - train.train_snli_ve - loss is tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3254/6700 [1:30:36<1:35:47,  1.67s/it]11/17/2022 00:24:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:45 - INFO - train.train_snli_ve - loss is tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3255/6700 [1:30:37<1:35:58,  1.67s/it]11/17/2022 00:24:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:46 - INFO - train.train_snli_ve - loss is tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3256/6700 [1:30:39<1:36:21,  1.68s/it]11/17/2022 00:24:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:48 - INFO - train.train_snli_ve - loss is tensor(0.8865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3257/6700 [1:30:41<1:36:18,  1.68s/it]11/17/2022 00:24:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:50 - INFO - train.train_snli_ve - loss is tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3258/6700 [1:30:42<1:36:39,  1.69s/it]11/17/2022 00:24:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.9947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:51 - INFO - train.train_snli_ve - loss is tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3259/6700 [1:30:44<1:36:14,  1.68s/it]11/17/2022 00:24:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6905e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:53 - INFO - train.train_snli_ve - loss is tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3260/6700 [1:30:46<1:34:59,  1.66s/it]11/17/2022 00:24:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:54 - INFO - train.train_snli_ve - loss is tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3261/6700 [1:30:47<1:34:04,  1.64s/it]11/17/2022 00:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.2961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:56 - INFO - train.train_snli_ve - loss is tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3262/6700 [1:30:49<1:34:09,  1.64s/it]11/17/2022 00:24:58 - INFO - train.train_snli_ve - kd_loss is tensor(9.9999e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:58 - INFO - train.train_snli_ve - loss is tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3263/6700 [1:30:50<1:34:44,  1.65s/it]11/17/2022 00:24:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6242e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:24:59 - INFO - train.train_snli_ve - loss is tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3264/6700 [1:30:52<1:35:15,  1.66s/it]11/17/2022 00:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7034e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:01 - INFO - train.train_snli_ve - loss is tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3265/6700 [1:30:54<1:35:02,  1.66s/it]11/17/2022 00:25:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3248e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:03 - INFO - train.train_snli_ve - loss is tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3266/6700 [1:30:56<1:35:53,  1.68s/it]11/17/2022 00:25:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:05 - INFO - train.train_snli_ve - loss is tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3267/6700 [1:30:57<1:35:42,  1.67s/it]11/17/2022 00:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.4305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:06 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3268/6700 [1:30:59<1:35:58,  1.68s/it]11/17/2022 00:25:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.2359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:08 - INFO - train.train_snli_ve - loss is tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3269/6700 [1:31:01<1:36:00,  1.68s/it]11/17/2022 00:25:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4113e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:10 - INFO - train.train_snli_ve - loss is tensor(0.7675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3270/6700 [1:31:02<1:35:56,  1.68s/it]11/17/2022 00:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:11 - INFO - train.train_snli_ve - loss is tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3271/6700 [1:31:04<1:36:02,  1.68s/it]11/17/2022 00:25:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.1805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:13 - INFO - train.train_snli_ve - loss is tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3272/6700 [1:31:06<1:35:49,  1.68s/it]11/17/2022 00:25:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.0218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:15 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3273/6700 [1:31:07<1:35:01,  1.66s/it]11/17/2022 00:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:16 - INFO - train.train_snli_ve - loss is tensor(0.5231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3274/6700 [1:31:09<1:35:32,  1.67s/it]11/17/2022 00:25:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:18 - INFO - train.train_snli_ve - loss is tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3275/6700 [1:31:11<1:35:42,  1.68s/it]11/17/2022 00:25:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:20 - INFO - train.train_snli_ve - loss is tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3276/6700 [1:31:12<1:36:02,  1.68s/it]11/17/2022 00:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:21 - INFO - train.train_snli_ve - loss is tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3277/6700 [1:31:14<1:36:10,  1.69s/it]11/17/2022 00:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:23 - INFO - train.train_snli_ve - loss is tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3278/6700 [1:31:16<1:36:08,  1.69s/it]11/17/2022 00:25:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:25 - INFO - train.train_snli_ve - loss is tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3279/6700 [1:31:17<1:36:00,  1.68s/it]11/17/2022 00:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.9252e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:26 - INFO - train.train_snli_ve - loss is tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3280/6700 [1:31:19<1:35:32,  1.68s/it]11/17/2022 00:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:28 - INFO - train.train_snli_ve - loss is tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3281/6700 [1:31:21<1:35:54,  1.68s/it]11/17/2022 00:25:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.2107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:30 - INFO - train.train_snli_ve - loss is tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####8     | 3282/6700 [1:31:22<1:35:36,  1.68s/it]11/17/2022 00:25:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.4025e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:31 - INFO - train.train_snli_ve - loss is tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3283/6700 [1:31:24<1:35:22,  1.67s/it]11/17/2022 00:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.0187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:33 - INFO - train.train_snli_ve - loss is tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3284/6700 [1:31:26<1:36:00,  1.69s/it]11/17/2022 00:25:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.6267e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:35 - INFO - train.train_snli_ve - loss is tensor(1.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3285/6700 [1:31:27<1:35:31,  1.68s/it]11/17/2022 00:25:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:36 - INFO - train.train_snli_ve - loss is tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3286/6700 [1:31:29<1:35:43,  1.68s/it]11/17/2022 00:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:38 - INFO - train.train_snli_ve - loss is tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3287/6700 [1:31:31<1:35:38,  1.68s/it]11/17/2022 00:25:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:40 - INFO - train.train_snli_ve - loss is tensor(0.9591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3288/6700 [1:31:32<1:35:17,  1.68s/it]11/17/2022 00:25:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:41 - INFO - train.train_snli_ve - loss is tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3289/6700 [1:31:34<1:35:18,  1.68s/it]11/17/2022 00:25:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.4882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:43 - INFO - train.train_snli_ve - loss is tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3290/6700 [1:31:36<1:34:51,  1.67s/it]11/17/2022 00:25:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:45 - INFO - train.train_snli_ve - loss is tensor(0.8889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3291/6700 [1:31:38<1:35:31,  1.68s/it]11/17/2022 00:25:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3662e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:47 - INFO - train.train_snli_ve - loss is tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3292/6700 [1:31:39<1:35:46,  1.69s/it]11/17/2022 00:25:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:48 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3293/6700 [1:31:41<1:35:49,  1.69s/it]11/17/2022 00:25:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.2016e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:50 - INFO - train.train_snli_ve - loss is tensor(0.8111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3294/6700 [1:31:43<1:35:53,  1.69s/it]11/17/2022 00:25:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.2524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:52 - INFO - train.train_snli_ve - loss is tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3295/6700 [1:31:44<1:36:23,  1.70s/it]11/17/2022 00:25:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9415e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:53 - INFO - train.train_snli_ve - loss is tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3296/6700 [1:31:46<1:35:26,  1.68s/it]11/17/2022 00:25:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:55 - INFO - train.train_snli_ve - loss is tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3297/6700 [1:31:48<1:35:17,  1.68s/it]11/17/2022 00:25:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5865e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:57 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3298/6700 [1:31:49<1:35:06,  1.68s/it]11/17/2022 00:25:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.1944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:25:58 - INFO - train.train_snli_ve - loss is tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3299/6700 [1:31:51<1:35:34,  1.69s/it]11/17/2022 00:26:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5560e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:00 - INFO - train.train_snli_ve - loss is tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3300/6700 [1:31:53<1:34:45,  1.67s/it]11/17/2022 00:26:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:02 - INFO - train.train_snli_ve - loss is tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3301/6700 [1:31:54<1:35:24,  1.68s/it]11/17/2022 00:26:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:03 - INFO - train.train_snli_ve - loss is tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3302/6700 [1:31:56<1:34:20,  1.67s/it]11/17/2022 00:26:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:05 - INFO - train.train_snli_ve - loss is tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3303/6700 [1:31:58<1:33:36,  1.65s/it]11/17/2022 00:26:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:07 - INFO - train.train_snli_ve - loss is tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3304/6700 [1:31:59<1:33:59,  1.66s/it]11/17/2022 00:26:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:08 - INFO - train.train_snli_ve - loss is tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3305/6700 [1:32:01<1:34:26,  1.67s/it]11/17/2022 00:26:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:10 - INFO - train.train_snli_ve - loss is tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3306/6700 [1:32:03<1:35:41,  1.69s/it]11/17/2022 00:26:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:12 - INFO - train.train_snli_ve - loss is tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3307/6700 [1:32:04<1:35:27,  1.69s/it]11/17/2022 00:26:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:13 - INFO - train.train_snli_ve - loss is tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3308/6700 [1:32:06<1:35:40,  1.69s/it]11/17/2022 00:26:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.5893e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:15 - INFO - train.train_snli_ve - loss is tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3309/6700 [1:32:08<1:34:29,  1.67s/it]11/17/2022 00:26:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:17 - INFO - train.train_snli_ve - loss is tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3310/6700 [1:32:09<1:34:31,  1.67s/it]11/17/2022 00:26:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:18 - INFO - train.train_snli_ve - loss is tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3311/6700 [1:32:11<1:34:24,  1.67s/it]11/17/2022 00:26:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.4696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:20 - INFO - train.train_snli_ve - loss is tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3312/6700 [1:32:13<1:33:46,  1.66s/it]11/17/2022 00:26:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:22 - INFO - train.train_snli_ve - loss is tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3313/6700 [1:32:14<1:34:24,  1.67s/it]11/17/2022 00:26:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:23 - INFO - train.train_snli_ve - loss is tensor(0.8361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3314/6700 [1:32:16<1:33:46,  1.66s/it]11/17/2022 00:26:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:25 - INFO - train.train_snli_ve - loss is tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3315/6700 [1:32:18<1:33:38,  1.66s/it]11/17/2022 00:26:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:27 - INFO - train.train_snli_ve - loss is tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  49%|####9     | 3316/6700 [1:32:19<1:32:57,  1.65s/it]11/17/2022 00:26:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.2245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:28 - INFO - train.train_snli_ve - loss is tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3317/6700 [1:32:21<1:33:50,  1.66s/it]11/17/2022 00:26:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:30 - INFO - train.train_snli_ve - loss is tensor(0.9239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3318/6700 [1:32:23<1:33:42,  1.66s/it]11/17/2022 00:26:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8750e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:32 - INFO - train.train_snli_ve - loss is tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3319/6700 [1:32:24<1:34:23,  1.68s/it]11/17/2022 00:26:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:33 - INFO - train.train_snli_ve - loss is tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3320/6700 [1:32:26<1:34:37,  1.68s/it]11/17/2022 00:26:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:35 - INFO - train.train_snli_ve - loss is tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3321/6700 [1:32:28<1:34:25,  1.68s/it]11/17/2022 00:26:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4251e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:37 - INFO - train.train_snli_ve - loss is tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3322/6700 [1:32:29<1:34:42,  1.68s/it]11/17/2022 00:26:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:38 - INFO - train.train_snli_ve - loss is tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3323/6700 [1:32:31<1:34:34,  1.68s/it]11/17/2022 00:26:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5786e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:40 - INFO - train.train_snli_ve - loss is tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3324/6700 [1:32:33<1:34:18,  1.68s/it]11/17/2022 00:26:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.6080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:42 - INFO - train.train_snli_ve - loss is tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3325/6700 [1:32:34<1:34:41,  1.68s/it]11/17/2022 00:26:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8253e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:43 - INFO - train.train_snli_ve - loss is tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3326/6700 [1:32:36<1:34:19,  1.68s/it]11/17/2022 00:26:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9270e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:45 - INFO - train.train_snli_ve - loss is tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3327/6700 [1:32:38<1:34:01,  1.67s/it]11/17/2022 00:26:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8834e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:47 - INFO - train.train_snli_ve - loss is tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3328/6700 [1:32:39<1:33:54,  1.67s/it]11/17/2022 00:26:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8607e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:48 - INFO - train.train_snli_ve - loss is tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3329/6700 [1:32:41<1:33:57,  1.67s/it]11/17/2022 00:26:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:50 - INFO - train.train_snli_ve - loss is tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3330/6700 [1:32:43<1:34:11,  1.68s/it]11/17/2022 00:26:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:52 - INFO - train.train_snli_ve - loss is tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3331/6700 [1:32:45<1:33:55,  1.67s/it]11/17/2022 00:26:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.7708e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:53 - INFO - train.train_snli_ve - loss is tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3332/6700 [1:32:46<1:33:42,  1.67s/it]11/17/2022 00:26:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.5017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:55 - INFO - train.train_snli_ve - loss is tensor(0.7833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3333/6700 [1:32:48<1:33:19,  1.66s/it]11/17/2022 00:26:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.5816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:57 - INFO - train.train_snli_ve - loss is tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3334/6700 [1:32:49<1:33:07,  1.66s/it]11/17/2022 00:26:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.9490e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:26:58 - INFO - train.train_snli_ve - loss is tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3335/6700 [1:32:51<1:33:06,  1.66s/it]11/17/2022 00:27:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.7676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:00 - INFO - train.train_snli_ve - loss is tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3336/6700 [1:32:53<1:33:17,  1.66s/it]11/17/2022 00:27:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.8311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:02 - INFO - train.train_snli_ve - loss is tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3337/6700 [1:32:54<1:32:53,  1.66s/it]11/17/2022 00:27:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2064e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:03 - INFO - train.train_snli_ve - loss is tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3338/6700 [1:32:56<1:33:02,  1.66s/it]11/17/2022 00:27:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6793e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:05 - INFO - train.train_snli_ve - loss is tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3339/6700 [1:32:58<1:33:06,  1.66s/it]11/17/2022 00:27:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:07 - INFO - train.train_snli_ve - loss is tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3340/6700 [1:32:59<1:33:37,  1.67s/it]11/17/2022 00:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.1120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:08 - INFO - train.train_snli_ve - loss is tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3341/6700 [1:33:01<1:33:42,  1.67s/it]11/17/2022 00:27:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.1886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:10 - INFO - train.train_snli_ve - loss is tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3342/6700 [1:33:03<1:33:04,  1.66s/it]11/17/2022 00:27:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.2758e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:12 - INFO - train.train_snli_ve - loss is tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3343/6700 [1:33:04<1:32:58,  1.66s/it]11/17/2022 00:27:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:13 - INFO - train.train_snli_ve - loss is tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3344/6700 [1:33:06<1:32:50,  1.66s/it]11/17/2022 00:27:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.3708e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:15 - INFO - train.train_snli_ve - loss is tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3345/6700 [1:33:08<1:33:10,  1.67s/it]11/17/2022 00:27:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.4467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:17 - INFO - train.train_snli_ve - loss is tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3346/6700 [1:33:09<1:33:08,  1.67s/it]11/17/2022 00:27:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:18 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3347/6700 [1:33:11<1:33:10,  1.67s/it]11/17/2022 00:27:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.1340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:20 - INFO - train.train_snli_ve - loss is tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3348/6700 [1:33:13<1:33:34,  1.67s/it]11/17/2022 00:27:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:22 - INFO - train.train_snli_ve - loss is tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|####9     | 3349/6700 [1:33:14<1:33:24,  1.67s/it]11/17/2022 00:27:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.9113e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:24 - INFO - train.train_snli_ve - loss is tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3350/6700 [1:33:16<1:35:02,  1.70s/it]11/17/2022 00:27:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:25 - INFO - train.train_snli_ve - loss is tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3351/6700 [1:33:18<1:35:10,  1.71s/it]11/17/2022 00:27:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7435e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:27 - INFO - train.train_snli_ve - loss is tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3352/6700 [1:33:20<1:35:17,  1.71s/it]11/17/2022 00:27:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:29 - INFO - train.train_snli_ve - loss is tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3353/6700 [1:33:21<1:34:53,  1.70s/it]11/17/2022 00:27:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:30 - INFO - train.train_snli_ve - loss is tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3354/6700 [1:33:23<1:33:57,  1.68s/it]11/17/2022 00:27:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:32 - INFO - train.train_snli_ve - loss is tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3355/6700 [1:33:25<1:33:59,  1.69s/it]11/17/2022 00:27:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:34 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3356/6700 [1:33:26<1:34:24,  1.69s/it]11/17/2022 00:27:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:35 - INFO - train.train_snli_ve - loss is tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3357/6700 [1:33:28<1:33:59,  1.69s/it]11/17/2022 00:27:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.7785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:37 - INFO - train.train_snli_ve - loss is tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3358/6700 [1:33:30<1:33:33,  1.68s/it]11/17/2022 00:27:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:39 - INFO - train.train_snli_ve - loss is tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3359/6700 [1:33:31<1:33:26,  1.68s/it]11/17/2022 00:27:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8716e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:40 - INFO - train.train_snli_ve - loss is tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3360/6700 [1:33:33<1:32:57,  1.67s/it]11/17/2022 00:27:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:42 - INFO - train.train_snli_ve - loss is tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3361/6700 [1:33:35<1:32:48,  1.67s/it]11/17/2022 00:27:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.2319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:44 - INFO - train.train_snli_ve - loss is tensor(0.8046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3362/6700 [1:33:36<1:33:02,  1.67s/it]11/17/2022 00:27:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.5889e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:45 - INFO - train.train_snli_ve - loss is tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3363/6700 [1:33:38<1:32:44,  1.67s/it]11/17/2022 00:27:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:47 - INFO - train.train_snli_ve - loss is tensor(0.5406, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3364/6700 [1:33:40<1:33:00,  1.67s/it]11/17/2022 00:27:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:49 - INFO - train.train_snli_ve - loss is tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3365/6700 [1:33:41<1:32:49,  1.67s/it]11/17/2022 00:27:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:50 - INFO - train.train_snli_ve - loss is tensor(0.7247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3366/6700 [1:33:43<1:33:18,  1.68s/it]11/17/2022 00:27:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.3602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:52 - INFO - train.train_snli_ve - loss is tensor(0.8354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3367/6700 [1:33:45<1:33:28,  1.68s/it]11/17/2022 00:27:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4937e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:54 - INFO - train.train_snli_ve - loss is tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3368/6700 [1:33:46<1:33:20,  1.68s/it]11/17/2022 00:27:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.7424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:55 - INFO - train.train_snli_ve - loss is tensor(0.8970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3369/6700 [1:33:48<1:33:31,  1.68s/it]11/17/2022 00:27:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:57 - INFO - train.train_snli_ve - loss is tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3370/6700 [1:33:50<1:34:03,  1.69s/it]11/17/2022 00:27:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:27:59 - INFO - train.train_snli_ve - loss is tensor(0.8449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3371/6700 [1:33:52<1:34:02,  1.69s/it]11/17/2022 00:28:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:01 - INFO - train.train_snli_ve - loss is tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3372/6700 [1:33:53<1:33:31,  1.69s/it]11/17/2022 00:28:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.7206e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:02 - INFO - train.train_snli_ve - loss is tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3373/6700 [1:33:55<1:33:32,  1.69s/it]11/17/2022 00:28:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:04 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3374/6700 [1:33:57<1:33:31,  1.69s/it]11/17/2022 00:28:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.7532e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:06 - INFO - train.train_snli_ve - loss is tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3375/6700 [1:33:58<1:33:26,  1.69s/it]11/17/2022 00:28:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:07 - INFO - train.train_snli_ve - loss is tensor(0.8169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3376/6700 [1:34:00<1:33:09,  1.68s/it]11/17/2022 00:28:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:09 - INFO - train.train_snli_ve - loss is tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3377/6700 [1:34:02<1:32:26,  1.67s/it]11/17/2022 00:28:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.7223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:11 - INFO - train.train_snli_ve - loss is tensor(0.5301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3378/6700 [1:34:03<1:32:41,  1.67s/it]11/17/2022 00:28:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:12 - INFO - train.train_snli_ve - loss is tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3379/6700 [1:34:05<1:33:15,  1.68s/it]11/17/2022 00:28:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:14 - INFO - train.train_snli_ve - loss is tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3380/6700 [1:34:07<1:33:39,  1.69s/it]11/17/2022 00:28:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.8462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:16 - INFO - train.train_snli_ve - loss is tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3381/6700 [1:34:08<1:33:09,  1.68s/it]11/17/2022 00:28:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:17 - INFO - train.train_snli_ve - loss is tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3382/6700 [1:34:10<1:32:59,  1.68s/it]11/17/2022 00:28:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:19 - INFO - train.train_snli_ve - loss is tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  50%|#####     | 3383/6700 [1:34:12<1:33:13,  1.69s/it]11/17/2022 00:28:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4034e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:21 - INFO - train.train_snli_ve - loss is tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3384/6700 [1:34:13<1:33:00,  1.68s/it]11/17/2022 00:28:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:22 - INFO - train.train_snli_ve - loss is tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3385/6700 [1:34:15<1:32:12,  1.67s/it]11/17/2022 00:28:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:24 - INFO - train.train_snli_ve - loss is tensor(0.5115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3386/6700 [1:34:17<1:32:02,  1.67s/it]11/17/2022 00:28:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:26 - INFO - train.train_snli_ve - loss is tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3387/6700 [1:34:18<1:32:24,  1.67s/it]11/17/2022 00:28:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:27 - INFO - train.train_snli_ve - loss is tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3388/6700 [1:34:20<1:32:58,  1.68s/it]11/17/2022 00:28:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:29 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3389/6700 [1:34:22<1:32:37,  1.68s/it]11/17/2022 00:28:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:31 - INFO - train.train_snli_ve - loss is tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3390/6700 [1:34:24<1:33:17,  1.69s/it]11/17/2022 00:28:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:32 - INFO - train.train_snli_ve - loss is tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3391/6700 [1:34:25<1:32:31,  1.68s/it]11/17/2022 00:28:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.2239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:34 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3392/6700 [1:34:27<1:31:59,  1.67s/it]11/17/2022 00:28:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.1642e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:36 - INFO - train.train_snli_ve - loss is tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3393/6700 [1:34:29<1:32:35,  1.68s/it]11/17/2022 00:28:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2965e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:38 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3394/6700 [1:34:30<1:32:36,  1.68s/it]11/17/2022 00:28:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.2176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:39 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3395/6700 [1:34:32<1:32:33,  1.68s/it]11/17/2022 00:28:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6253e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:41 - INFO - train.train_snli_ve - loss is tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3396/6700 [1:34:34<1:33:13,  1.69s/it]11/17/2022 00:28:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.3675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:43 - INFO - train.train_snli_ve - loss is tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3397/6700 [1:34:35<1:33:00,  1.69s/it]11/17/2022 00:28:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:44 - INFO - train.train_snli_ve - loss is tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3398/6700 [1:34:37<1:31:41,  1.67s/it]11/17/2022 00:28:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.0329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:46 - INFO - train.train_snli_ve - loss is tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3399/6700 [1:34:39<1:30:57,  1.65s/it]11/17/2022 00:28:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:48 - INFO - train.train_snli_ve - loss is tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3400/6700 [1:34:40<1:31:59,  1.67s/it]11/17/2022 00:28:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:49 - INFO - train.train_snli_ve - loss is tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3401/6700 [1:34:42<1:31:41,  1.67s/it]11/17/2022 00:28:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:51 - INFO - train.train_snli_ve - loss is tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3402/6700 [1:34:44<1:31:56,  1.67s/it]11/17/2022 00:28:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.7857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:53 - INFO - train.train_snli_ve - loss is tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3403/6700 [1:34:45<1:32:09,  1.68s/it]11/17/2022 00:28:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:54 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3404/6700 [1:34:47<1:31:45,  1.67s/it]11/17/2022 00:28:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.1901e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:56 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3405/6700 [1:34:49<1:32:17,  1.68s/it]11/17/2022 00:28:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:58 - INFO - train.train_snli_ve - loss is tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3406/6700 [1:34:50<1:33:09,  1.70s/it]11/17/2022 00:28:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.5082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:28:59 - INFO - train.train_snli_ve - loss is tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3407/6700 [1:34:52<1:32:18,  1.68s/it]11/17/2022 00:29:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:01 - INFO - train.train_snli_ve - loss is tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3408/6700 [1:34:54<1:32:05,  1.68s/it]11/17/2022 00:29:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:03 - INFO - train.train_snli_ve - loss is tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3409/6700 [1:34:55<1:31:57,  1.68s/it]11/17/2022 00:29:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:04 - INFO - train.train_snli_ve - loss is tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3410/6700 [1:34:57<1:31:13,  1.66s/it]11/17/2022 00:29:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:06 - INFO - train.train_snli_ve - loss is tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3411/6700 [1:34:59<1:31:08,  1.66s/it]11/17/2022 00:29:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.0361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:08 - INFO - train.train_snli_ve - loss is tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3412/6700 [1:35:00<1:30:50,  1.66s/it]11/17/2022 00:29:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8833e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:09 - INFO - train.train_snli_ve - loss is tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3413/6700 [1:35:02<1:30:51,  1.66s/it]11/17/2022 00:29:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:11 - INFO - train.train_snli_ve - loss is tensor(0.4402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3414/6700 [1:35:04<1:30:44,  1.66s/it]11/17/2022 00:29:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.2941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:13 - INFO - train.train_snli_ve - loss is tensor(0.8610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3415/6700 [1:35:05<1:31:08,  1.66s/it]11/17/2022 00:29:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:14 - INFO - train.train_snli_ve - loss is tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####     | 3416/6700 [1:35:07<1:30:57,  1.66s/it]11/17/2022 00:29:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.7700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:16 - INFO - train.train_snli_ve - loss is tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3417/6700 [1:35:09<1:31:13,  1.67s/it]11/17/2022 00:29:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:18 - INFO - train.train_snli_ve - loss is tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3418/6700 [1:35:10<1:31:40,  1.68s/it]11/17/2022 00:29:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:19 - INFO - train.train_snli_ve - loss is tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3419/6700 [1:35:12<1:31:43,  1.68s/it]11/17/2022 00:29:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:21 - INFO - train.train_snli_ve - loss is tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3420/6700 [1:35:14<1:31:39,  1.68s/it]11/17/2022 00:29:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.0784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:23 - INFO - train.train_snli_ve - loss is tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3421/6700 [1:35:15<1:32:00,  1.68s/it]11/17/2022 00:29:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:24 - INFO - train.train_snli_ve - loss is tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3422/6700 [1:35:17<1:32:06,  1.69s/it]11/17/2022 00:29:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.4270e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:26 - INFO - train.train_snli_ve - loss is tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3423/6700 [1:35:19<1:32:33,  1.69s/it]11/17/2022 00:29:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.4311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:28 - INFO - train.train_snli_ve - loss is tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3424/6700 [1:35:20<1:32:18,  1.69s/it]11/17/2022 00:29:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.1364e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:29 - INFO - train.train_snli_ve - loss is tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3425/6700 [1:35:22<1:31:29,  1.68s/it]11/17/2022 00:29:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.2133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:31 - INFO - train.train_snli_ve - loss is tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3426/6700 [1:35:24<1:31:18,  1.67s/it]11/17/2022 00:29:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:33 - INFO - train.train_snli_ve - loss is tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3427/6700 [1:35:25<1:31:19,  1.67s/it]11/17/2022 00:29:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:34 - INFO - train.train_snli_ve - loss is tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3428/6700 [1:35:27<1:30:50,  1.67s/it]11/17/2022 00:29:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.4133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:36 - INFO - train.train_snli_ve - loss is tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3429/6700 [1:35:29<1:30:48,  1.67s/it]11/17/2022 00:29:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:38 - INFO - train.train_snli_ve - loss is tensor(0.8251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3430/6700 [1:35:30<1:31:12,  1.67s/it]11/17/2022 00:29:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.4759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:39 - INFO - train.train_snli_ve - loss is tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3431/6700 [1:35:32<1:31:11,  1.67s/it]11/17/2022 00:29:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.7477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:41 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3432/6700 [1:35:34<1:30:49,  1.67s/it]11/17/2022 00:29:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:43 - INFO - train.train_snli_ve - loss is tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3433/6700 [1:35:36<1:31:44,  1.68s/it]11/17/2022 00:29:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.5002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:44 - INFO - train.train_snli_ve - loss is tensor(0.8863, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3434/6700 [1:35:37<1:31:22,  1.68s/it]11/17/2022 00:29:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8026e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:46 - INFO - train.train_snli_ve - loss is tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3435/6700 [1:35:39<1:30:57,  1.67s/it]11/17/2022 00:29:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:48 - INFO - train.train_snli_ve - loss is tensor(0.4352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3436/6700 [1:35:40<1:30:38,  1.67s/it]11/17/2022 00:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.1740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:49 - INFO - train.train_snli_ve - loss is tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3437/6700 [1:35:42<1:30:47,  1.67s/it]11/17/2022 00:29:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:51 - INFO - train.train_snli_ve - loss is tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3438/6700 [1:35:44<1:31:36,  1.69s/it]11/17/2022 00:29:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.1967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:53 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3439/6700 [1:35:46<1:31:14,  1.68s/it]11/17/2022 00:29:55 - INFO - train.train_snli_ve - kd_loss is tensor(9.5889e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:55 - INFO - train.train_snli_ve - loss is tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3440/6700 [1:35:47<1:31:48,  1.69s/it]11/17/2022 00:29:56 - INFO - train.train_snli_ve - kd_loss is tensor(8.9929e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:56 - INFO - train.train_snli_ve - loss is tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3441/6700 [1:35:49<1:32:05,  1.70s/it]11/17/2022 00:29:58 - INFO - train.train_snli_ve - kd_loss is tensor(8.9376e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:29:58 - INFO - train.train_snli_ve - loss is tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3442/6700 [1:35:51<1:31:54,  1.69s/it]11/17/2022 00:30:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0327e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:00 - INFO - train.train_snli_ve - loss is tensor(0.7534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3443/6700 [1:35:52<1:31:37,  1.69s/it]11/17/2022 00:30:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1260e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:01 - INFO - train.train_snli_ve - loss is tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3444/6700 [1:35:54<1:32:03,  1.70s/it]11/17/2022 00:30:03 - INFO - train.train_snli_ve - kd_loss is tensor(9.7648e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:03 - INFO - train.train_snli_ve - loss is tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3445/6700 [1:35:56<1:31:48,  1.69s/it]11/17/2022 00:30:05 - INFO - train.train_snli_ve - kd_loss is tensor(7.5173e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:05 - INFO - train.train_snli_ve - loss is tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3446/6700 [1:35:57<1:32:00,  1.70s/it]11/17/2022 00:30:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.9424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:06 - INFO - train.train_snli_ve - loss is tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3447/6700 [1:35:59<1:31:30,  1.69s/it]11/17/2022 00:30:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.2078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:08 - INFO - train.train_snli_ve - loss is tensor(0.5551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3448/6700 [1:36:01<1:30:54,  1.68s/it]11/17/2022 00:30:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:10 - INFO - train.train_snli_ve - loss is tensor(0.9431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3449/6700 [1:36:02<1:30:23,  1.67s/it]11/17/2022 00:30:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:11 - INFO - train.train_snli_ve - loss is tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  51%|#####1    | 3450/6700 [1:36:04<1:31:18,  1.69s/it]11/17/2022 00:30:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:13 - INFO - train.train_snli_ve - loss is tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3451/6700 [1:36:06<1:30:55,  1.68s/it]11/17/2022 00:30:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1817e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:15 - INFO - train.train_snli_ve - loss is tensor(0.8019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3452/6700 [1:36:07<1:31:05,  1.68s/it]11/17/2022 00:30:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.1784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:16 - INFO - train.train_snli_ve - loss is tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3453/6700 [1:36:09<1:30:42,  1.68s/it]11/17/2022 00:30:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.4733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:18 - INFO - train.train_snli_ve - loss is tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3454/6700 [1:36:11<1:30:28,  1.67s/it]11/17/2022 00:30:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.0768e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:20 - INFO - train.train_snli_ve - loss is tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3455/6700 [1:36:12<1:30:14,  1.67s/it]11/17/2022 00:30:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7786e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:21 - INFO - train.train_snli_ve - loss is tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3456/6700 [1:36:14<1:30:27,  1.67s/it]11/17/2022 00:30:23 - INFO - train.train_snli_ve - kd_loss is tensor(8.4073e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:23 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3457/6700 [1:36:16<1:30:18,  1.67s/it]11/17/2022 00:30:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.0312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:25 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3458/6700 [1:36:18<1:31:01,  1.68s/it]11/17/2022 00:30:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.0419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:26 - INFO - train.train_snli_ve - loss is tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3459/6700 [1:36:19<1:30:12,  1.67s/it]11/17/2022 00:30:28 - INFO - train.train_snli_ve - kd_loss is tensor(8.5371e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:28 - INFO - train.train_snli_ve - loss is tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3460/6700 [1:36:21<1:29:58,  1.67s/it]11/17/2022 00:30:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.2032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:30 - INFO - train.train_snli_ve - loss is tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3461/6700 [1:36:23<1:31:09,  1.69s/it]11/17/2022 00:30:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3503e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:32 - INFO - train.train_snli_ve - loss is tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3462/6700 [1:36:24<1:31:16,  1.69s/it]11/17/2022 00:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.4612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:33 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3463/6700 [1:36:26<1:30:45,  1.68s/it]11/17/2022 00:30:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:35 - INFO - train.train_snli_ve - loss is tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3464/6700 [1:36:28<1:30:09,  1.67s/it]11/17/2022 00:30:37 - INFO - train.train_snli_ve - kd_loss is tensor(9.6395e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:37 - INFO - train.train_snli_ve - loss is tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3465/6700 [1:36:29<1:30:50,  1.68s/it]11/17/2022 00:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:38 - INFO - train.train_snli_ve - loss is tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3466/6700 [1:36:31<1:30:23,  1.68s/it]11/17/2022 00:30:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.2127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:40 - INFO - train.train_snli_ve - loss is tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3467/6700 [1:36:33<1:29:45,  1.67s/it]11/17/2022 00:30:42 - INFO - train.train_snli_ve - kd_loss is tensor(9.6059e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:42 - INFO - train.train_snli_ve - loss is tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3468/6700 [1:36:34<1:29:54,  1.67s/it]11/17/2022 00:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:43 - INFO - train.train_snli_ve - loss is tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3469/6700 [1:36:36<1:29:53,  1.67s/it]11/17/2022 00:30:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.2999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:45 - INFO - train.train_snli_ve - loss is tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3470/6700 [1:36:38<1:30:08,  1.67s/it]11/17/2022 00:30:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.3197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:47 - INFO - train.train_snli_ve - loss is tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3471/6700 [1:36:39<1:30:28,  1.68s/it]11/17/2022 00:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:48 - INFO - train.train_snli_ve - loss is tensor(0.8471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3472/6700 [1:36:41<1:30:52,  1.69s/it]11/17/2022 00:30:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:50 - INFO - train.train_snli_ve - loss is tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3473/6700 [1:36:43<1:30:43,  1.69s/it]11/17/2022 00:30:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:52 - INFO - train.train_snli_ve - loss is tensor(0.4812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3474/6700 [1:36:44<1:30:04,  1.68s/it]11/17/2022 00:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:53 - INFO - train.train_snli_ve - loss is tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3475/6700 [1:36:46<1:30:58,  1.69s/it]11/17/2022 00:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:55 - INFO - train.train_snli_ve - loss is tensor(0.9792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3476/6700 [1:36:48<1:31:12,  1.70s/it]11/17/2022 00:30:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.7001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:57 - INFO - train.train_snli_ve - loss is tensor(0.9092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3477/6700 [1:36:49<1:30:52,  1.69s/it]11/17/2022 00:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.7171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:30:58 - INFO - train.train_snli_ve - loss is tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3478/6700 [1:36:51<1:31:10,  1.70s/it]11/17/2022 00:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.5153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:00 - INFO - train.train_snli_ve - loss is tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3479/6700 [1:36:53<1:30:36,  1.69s/it]11/17/2022 00:31:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.3018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:02 - INFO - train.train_snli_ve - loss is tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3480/6700 [1:36:55<1:30:52,  1.69s/it]11/17/2022 00:31:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:04 - INFO - train.train_snli_ve - loss is tensor(0.8102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3481/6700 [1:36:56<1:31:00,  1.70s/it]11/17/2022 00:31:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:05 - INFO - train.train_snli_ve - loss is tensor(0.8317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3482/6700 [1:36:58<1:30:52,  1.69s/it]11/17/2022 00:31:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.4595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:07 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####1    | 3483/6700 [1:37:00<1:30:55,  1.70s/it]11/17/2022 00:31:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.0726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:09 - INFO - train.train_snli_ve - loss is tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3484/6700 [1:37:01<1:30:32,  1.69s/it]11/17/2022 00:31:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0910e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:10 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3485/6700 [1:37:03<1:29:43,  1.67s/it]11/17/2022 00:31:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.0999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:12 - INFO - train.train_snli_ve - loss is tensor(0.8729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3486/6700 [1:37:05<1:29:15,  1.67s/it]11/17/2022 00:31:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:14 - INFO - train.train_snli_ve - loss is tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3487/6700 [1:37:06<1:29:34,  1.67s/it]11/17/2022 00:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:15 - INFO - train.train_snli_ve - loss is tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3488/6700 [1:37:08<1:29:36,  1.67s/it]11/17/2022 00:31:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.7617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:17 - INFO - train.train_snli_ve - loss is tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3489/6700 [1:37:10<1:29:20,  1.67s/it]11/17/2022 00:31:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.7914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:19 - INFO - train.train_snli_ve - loss is tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3490/6700 [1:37:11<1:30:11,  1.69s/it]11/17/2022 00:31:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.1153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:20 - INFO - train.train_snli_ve - loss is tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3491/6700 [1:37:13<1:29:48,  1.68s/it]11/17/2022 00:31:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4780e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:22 - INFO - train.train_snli_ve - loss is tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3492/6700 [1:37:15<1:30:16,  1.69s/it]11/17/2022 00:31:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.0138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:24 - INFO - train.train_snli_ve - loss is tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3493/6700 [1:37:16<1:29:52,  1.68s/it]11/17/2022 00:31:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:25 - INFO - train.train_snli_ve - loss is tensor(0.9816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3494/6700 [1:37:18<1:29:51,  1.68s/it]11/17/2022 00:31:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.1857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:27 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3495/6700 [1:37:20<1:30:44,  1.70s/it]11/17/2022 00:31:29 - INFO - train.train_snli_ve - kd_loss is tensor(9.6451e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:29 - INFO - train.train_snli_ve - loss is tensor(0.8185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3496/6700 [1:37:21<1:29:59,  1.69s/it]11/17/2022 00:31:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5446e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:30 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3497/6700 [1:37:23<1:30:25,  1.69s/it]11/17/2022 00:31:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:32 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3498/6700 [1:37:25<1:29:50,  1.68s/it]11/17/2022 00:31:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:34 - INFO - train.train_snli_ve - loss is tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3499/6700 [1:37:26<1:29:13,  1.67s/it]11/17/2022 00:31:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.2511e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:35 - INFO - train.train_snli_ve - loss is tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3500/6700 [1:37:28<1:29:28,  1.68s/it]11/17/2022 00:31:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.2081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:37 - INFO - train.train_snli_ve - loss is tensor(0.9271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3501/6700 [1:37:30<1:29:06,  1.67s/it]11/17/2022 00:31:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4899e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:39 - INFO - train.train_snli_ve - loss is tensor(0.7851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3502/6700 [1:37:31<1:28:38,  1.66s/it]11/17/2022 00:31:40 - INFO - train.train_snli_ve - kd_loss is tensor(7.1770e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:40 - INFO - train.train_snli_ve - loss is tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3503/6700 [1:37:33<1:29:02,  1.67s/it]11/17/2022 00:31:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:42 - INFO - train.train_snli_ve - loss is tensor(0.4573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3504/6700 [1:37:35<1:29:17,  1.68s/it]11/17/2022 00:31:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.1405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:44 - INFO - train.train_snli_ve - loss is tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3505/6700 [1:37:37<1:29:37,  1.68s/it]11/17/2022 00:31:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:46 - INFO - train.train_snli_ve - loss is tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3506/6700 [1:37:38<1:29:21,  1.68s/it]11/17/2022 00:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.5232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:47 - INFO - train.train_snli_ve - loss is tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3507/6700 [1:37:40<1:29:20,  1.68s/it]11/17/2022 00:31:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:49 - INFO - train.train_snli_ve - loss is tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3508/6700 [1:37:42<1:29:33,  1.68s/it]11/17/2022 00:31:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:51 - INFO - train.train_snli_ve - loss is tensor(0.5045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3509/6700 [1:37:43<1:29:10,  1.68s/it]11/17/2022 00:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:52 - INFO - train.train_snli_ve - loss is tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3510/6700 [1:37:45<1:29:02,  1.67s/it]11/17/2022 00:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.3925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:54 - INFO - train.train_snli_ve - loss is tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3511/6700 [1:37:47<1:28:39,  1.67s/it]11/17/2022 00:31:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:56 - INFO - train.train_snli_ve - loss is tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3512/6700 [1:37:48<1:28:25,  1.66s/it]11/17/2022 00:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:57 - INFO - train.train_snli_ve - loss is tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3513/6700 [1:37:50<1:27:51,  1.65s/it]11/17/2022 00:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9782e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:31:59 - INFO - train.train_snli_ve - loss is tensor(0.7941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3514/6700 [1:37:52<1:28:18,  1.66s/it]11/17/2022 00:32:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:01 - INFO - train.train_snli_ve - loss is tensor(0.5234, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3515/6700 [1:37:53<1:28:58,  1.68s/it]11/17/2022 00:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:02 - INFO - train.train_snli_ve - loss is tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3516/6700 [1:37:55<1:29:52,  1.69s/it]11/17/2022 00:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.8191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:04 - INFO - train.train_snli_ve - loss is tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  52%|#####2    | 3517/6700 [1:37:57<1:29:06,  1.68s/it]11/17/2022 00:32:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5950e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:06 - INFO - train.train_snli_ve - loss is tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3518/6700 [1:37:58<1:28:30,  1.67s/it]11/17/2022 00:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:07 - INFO - train.train_snli_ve - loss is tensor(0.8669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3519/6700 [1:38:00<1:28:37,  1.67s/it]11/17/2022 00:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.6659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:09 - INFO - train.train_snli_ve - loss is tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3520/6700 [1:38:02<1:29:56,  1.70s/it]11/17/2022 00:32:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.3136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:11 - INFO - train.train_snli_ve - loss is tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3521/6700 [1:38:03<1:29:19,  1.69s/it]11/17/2022 00:32:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:12 - INFO - train.train_snli_ve - loss is tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3522/6700 [1:38:05<1:29:47,  1.70s/it]11/17/2022 00:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.9415e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:14 - INFO - train.train_snli_ve - loss is tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3523/6700 [1:38:07<1:29:39,  1.69s/it]11/17/2022 00:32:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.7195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:16 - INFO - train.train_snli_ve - loss is tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3524/6700 [1:38:08<1:28:55,  1.68s/it]11/17/2022 00:32:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.8426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:17 - INFO - train.train_snli_ve - loss is tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3525/6700 [1:38:10<1:28:44,  1.68s/it]11/17/2022 00:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.2128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:19 - INFO - train.train_snli_ve - loss is tensor(0.8874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3526/6700 [1:38:12<1:28:29,  1.67s/it]11/17/2022 00:32:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:21 - INFO - train.train_snli_ve - loss is tensor(0.7492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3527/6700 [1:38:13<1:29:15,  1.69s/it]11/17/2022 00:32:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.7392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:23 - INFO - train.train_snli_ve - loss is tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3528/6700 [1:38:15<1:29:43,  1.70s/it]11/17/2022 00:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3345e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:24 - INFO - train.train_snli_ve - loss is tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3529/6700 [1:38:17<1:29:30,  1.69s/it]11/17/2022 00:32:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.4990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:26 - INFO - train.train_snli_ve - loss is tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3530/6700 [1:38:19<1:30:13,  1.71s/it]11/17/2022 00:32:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.2350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:28 - INFO - train.train_snli_ve - loss is tensor(0.8480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3531/6700 [1:38:20<1:29:52,  1.70s/it]11/17/2022 00:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.1065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:29 - INFO - train.train_snli_ve - loss is tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3532/6700 [1:38:22<1:30:31,  1.71s/it]11/17/2022 00:32:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:31 - INFO - train.train_snli_ve - loss is tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3533/6700 [1:38:24<1:29:27,  1.69s/it]11/17/2022 00:32:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6401e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:33 - INFO - train.train_snli_ve - loss is tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3534/6700 [1:38:25<1:29:05,  1.69s/it]11/17/2022 00:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2708e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:34 - INFO - train.train_snli_ve - loss is tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3535/6700 [1:38:27<1:28:53,  1.69s/it]11/17/2022 00:32:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:36 - INFO - train.train_snli_ve - loss is tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3536/6700 [1:38:29<1:28:25,  1.68s/it]11/17/2022 00:32:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:38 - INFO - train.train_snli_ve - loss is tensor(0.7445, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3537/6700 [1:38:30<1:28:11,  1.67s/it]11/17/2022 00:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.3713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:39 - INFO - train.train_snli_ve - loss is tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3538/6700 [1:38:32<1:27:59,  1.67s/it]11/17/2022 00:32:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:41 - INFO - train.train_snli_ve - loss is tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3539/6700 [1:38:34<1:28:01,  1.67s/it]11/17/2022 00:32:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:43 - INFO - train.train_snli_ve - loss is tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3540/6700 [1:38:35<1:29:10,  1.69s/it]11/17/2022 00:32:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.8454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:45 - INFO - train.train_snli_ve - loss is tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3541/6700 [1:38:37<1:30:07,  1.71s/it]11/17/2022 00:32:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.6839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:46 - INFO - train.train_snli_ve - loss is tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3542/6700 [1:38:39<1:29:52,  1.71s/it]11/17/2022 00:32:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:48 - INFO - train.train_snli_ve - loss is tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3543/6700 [1:38:41<1:29:12,  1.70s/it]11/17/2022 00:32:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:50 - INFO - train.train_snli_ve - loss is tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3544/6700 [1:38:42<1:29:39,  1.70s/it]11/17/2022 00:32:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.9677e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:51 - INFO - train.train_snli_ve - loss is tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3545/6700 [1:38:44<1:29:48,  1.71s/it]11/17/2022 00:32:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.6700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:53 - INFO - train.train_snli_ve - loss is tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3546/6700 [1:38:46<1:29:16,  1.70s/it]11/17/2022 00:32:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.3033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:55 - INFO - train.train_snli_ve - loss is tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3547/6700 [1:38:47<1:29:04,  1.69s/it]11/17/2022 00:32:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6237e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:56 - INFO - train.train_snli_ve - loss is tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3548/6700 [1:38:49<1:29:27,  1.70s/it]11/17/2022 00:32:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:32:58 - INFO - train.train_snli_ve - loss is tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3549/6700 [1:38:51<1:29:10,  1.70s/it]11/17/2022 00:33:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:00 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####2    | 3550/6700 [1:38:52<1:29:02,  1.70s/it]11/17/2022 00:33:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:01 - INFO - train.train_snli_ve - loss is tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3551/6700 [1:38:54<1:28:41,  1.69s/it]11/17/2022 00:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:03 - INFO - train.train_snli_ve - loss is tensor(0.7808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3552/6700 [1:38:56<1:28:17,  1.68s/it]11/17/2022 00:33:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:05 - INFO - train.train_snli_ve - loss is tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3553/6700 [1:38:58<1:28:34,  1.69s/it]11/17/2022 00:33:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:07 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3554/6700 [1:38:59<1:28:24,  1.69s/it]11/17/2022 00:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.7733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:08 - INFO - train.train_snli_ve - loss is tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3555/6700 [1:39:01<1:27:55,  1.68s/it]11/17/2022 00:33:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.2823e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:10 - INFO - train.train_snli_ve - loss is tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3556/6700 [1:39:03<1:28:01,  1.68s/it]11/17/2022 00:33:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.1659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:12 - INFO - train.train_snli_ve - loss is tensor(0.5061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3557/6700 [1:39:04<1:28:44,  1.69s/it]11/17/2022 00:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.4579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:13 - INFO - train.train_snli_ve - loss is tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3558/6700 [1:39:06<1:28:52,  1.70s/it]11/17/2022 00:33:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.2041e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:15 - INFO - train.train_snli_ve - loss is tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3559/6700 [1:39:08<1:29:11,  1.70s/it]11/17/2022 00:33:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:17 - INFO - train.train_snli_ve - loss is tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3560/6700 [1:39:09<1:28:49,  1.70s/it]11/17/2022 00:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:18 - INFO - train.train_snli_ve - loss is tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3561/6700 [1:39:11<1:28:56,  1.70s/it]11/17/2022 00:33:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:20 - INFO - train.train_snli_ve - loss is tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3562/6700 [1:39:13<1:28:18,  1.69s/it]11/17/2022 00:33:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:22 - INFO - train.train_snli_ve - loss is tensor(0.6830, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3563/6700 [1:39:14<1:28:03,  1.68s/it]11/17/2022 00:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.1235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:23 - INFO - train.train_snli_ve - loss is tensor(0.8709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3564/6700 [1:39:16<1:27:54,  1.68s/it]11/17/2022 00:33:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9307e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:25 - INFO - train.train_snli_ve - loss is tensor(0.9677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3565/6700 [1:39:18<1:27:17,  1.67s/it]11/17/2022 00:33:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:27 - INFO - train.train_snli_ve - loss is tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3566/6700 [1:39:19<1:27:11,  1.67s/it]11/17/2022 00:33:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5829e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:28 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3567/6700 [1:39:21<1:27:10,  1.67s/it]11/17/2022 00:33:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:30 - INFO - train.train_snli_ve - loss is tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3568/6700 [1:39:23<1:27:23,  1.67s/it]11/17/2022 00:33:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.6828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:32 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3569/6700 [1:39:24<1:27:47,  1.68s/it]11/17/2022 00:33:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:33 - INFO - train.train_snli_ve - loss is tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3570/6700 [1:39:26<1:27:47,  1.68s/it]11/17/2022 00:33:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:35 - INFO - train.train_snli_ve - loss is tensor(1.0107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3571/6700 [1:39:28<1:27:28,  1.68s/it]11/17/2022 00:33:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3591e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:37 - INFO - train.train_snli_ve - loss is tensor(0.8114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3572/6700 [1:39:30<1:27:24,  1.68s/it]11/17/2022 00:33:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:39 - INFO - train.train_snli_ve - loss is tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3573/6700 [1:39:31<1:27:50,  1.69s/it]11/17/2022 00:33:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:40 - INFO - train.train_snli_ve - loss is tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3574/6700 [1:39:33<1:27:13,  1.67s/it]11/17/2022 00:33:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7184e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:42 - INFO - train.train_snli_ve - loss is tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3575/6700 [1:39:35<1:27:22,  1.68s/it]11/17/2022 00:33:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:44 - INFO - train.train_snli_ve - loss is tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3576/6700 [1:39:36<1:26:55,  1.67s/it]11/17/2022 00:33:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:45 - INFO - train.train_snli_ve - loss is tensor(0.8608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3577/6700 [1:39:38<1:27:18,  1.68s/it]11/17/2022 00:33:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:47 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3578/6700 [1:39:40<1:26:50,  1.67s/it]11/17/2022 00:33:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:48 - INFO - train.train_snli_ve - loss is tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3579/6700 [1:39:41<1:26:07,  1.66s/it]11/17/2022 00:33:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5796e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:50 - INFO - train.train_snli_ve - loss is tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3580/6700 [1:39:43<1:26:13,  1.66s/it]11/17/2022 00:33:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:52 - INFO - train.train_snli_ve - loss is tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3581/6700 [1:39:44<1:26:16,  1.66s/it]11/17/2022 00:33:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:53 - INFO - train.train_snli_ve - loss is tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3582/6700 [1:39:46<1:26:39,  1.67s/it]11/17/2022 00:33:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.5744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:55 - INFO - train.train_snli_ve - loss is tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3583/6700 [1:39:48<1:26:17,  1.66s/it]11/17/2022 00:33:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1996e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:57 - INFO - train.train_snli_ve - loss is tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  53%|#####3    | 3584/6700 [1:39:49<1:26:21,  1.66s/it]11/17/2022 00:33:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:33:59 - INFO - train.train_snli_ve - loss is tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3585/6700 [1:39:51<1:27:03,  1.68s/it]11/17/2022 00:34:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:00 - INFO - train.train_snli_ve - loss is tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3586/6700 [1:39:53<1:26:45,  1.67s/it]11/17/2022 00:34:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.0518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:02 - INFO - train.train_snli_ve - loss is tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3587/6700 [1:39:55<1:26:59,  1.68s/it]11/17/2022 00:34:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.0179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:04 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3588/6700 [1:39:56<1:26:58,  1.68s/it]11/17/2022 00:34:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.1991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:05 - INFO - train.train_snli_ve - loss is tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3589/6700 [1:39:58<1:26:39,  1.67s/it]11/17/2022 00:34:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:07 - INFO - train.train_snli_ve - loss is tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3590/6700 [1:40:00<1:27:09,  1.68s/it]11/17/2022 00:34:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.1927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:09 - INFO - train.train_snli_ve - loss is tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3591/6700 [1:40:01<1:27:18,  1.69s/it]11/17/2022 00:34:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:10 - INFO - train.train_snli_ve - loss is tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3592/6700 [1:40:03<1:27:02,  1.68s/it]11/17/2022 00:34:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.5326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:12 - INFO - train.train_snli_ve - loss is tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3593/6700 [1:40:05<1:26:57,  1.68s/it]11/17/2022 00:34:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.8814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:14 - INFO - train.train_snli_ve - loss is tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3594/6700 [1:40:06<1:27:25,  1.69s/it]11/17/2022 00:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:15 - INFO - train.train_snli_ve - loss is tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3595/6700 [1:40:08<1:27:09,  1.68s/it]11/17/2022 00:34:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.8506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:17 - INFO - train.train_snli_ve - loss is tensor(0.8351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3596/6700 [1:40:10<1:26:58,  1.68s/it]11/17/2022 00:34:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:19 - INFO - train.train_snli_ve - loss is tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3597/6700 [1:40:11<1:27:03,  1.68s/it]11/17/2022 00:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:20 - INFO - train.train_snli_ve - loss is tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3598/6700 [1:40:13<1:26:59,  1.68s/it]11/17/2022 00:34:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:22 - INFO - train.train_snli_ve - loss is tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3599/6700 [1:40:15<1:26:42,  1.68s/it]11/17/2022 00:34:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:24 - INFO - train.train_snli_ve - loss is tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3600/6700 [1:40:16<1:27:03,  1.69s/it]11/17/2022 00:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:25 - INFO - train.train_snli_ve - loss is tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3601/6700 [1:40:18<1:26:35,  1.68s/it]11/17/2022 00:34:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.9627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:27 - INFO - train.train_snli_ve - loss is tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3602/6700 [1:40:20<1:26:22,  1.67s/it]11/17/2022 00:34:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.1650e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:29 - INFO - train.train_snli_ve - loss is tensor(0.4859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3603/6700 [1:40:21<1:26:17,  1.67s/it]11/17/2022 00:34:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:30 - INFO - train.train_snli_ve - loss is tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3604/6700 [1:40:23<1:25:39,  1.66s/it]11/17/2022 00:34:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.2354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:32 - INFO - train.train_snli_ve - loss is tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3605/6700 [1:40:25<1:25:34,  1.66s/it]11/17/2022 00:34:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.5903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:34 - INFO - train.train_snli_ve - loss is tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3606/6700 [1:40:26<1:25:38,  1.66s/it]11/17/2022 00:34:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:35 - INFO - train.train_snli_ve - loss is tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3607/6700 [1:40:28<1:25:39,  1.66s/it]11/17/2022 00:34:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.5331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:37 - INFO - train.train_snli_ve - loss is tensor(0.5161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3608/6700 [1:40:30<1:26:20,  1.68s/it]11/17/2022 00:34:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7084e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:39 - INFO - train.train_snli_ve - loss is tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3609/6700 [1:40:31<1:26:29,  1.68s/it]11/17/2022 00:34:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.2862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:40 - INFO - train.train_snli_ve - loss is tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3610/6700 [1:40:33<1:26:48,  1.69s/it]11/17/2022 00:34:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:42 - INFO - train.train_snli_ve - loss is tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3611/6700 [1:40:35<1:26:43,  1.68s/it]11/17/2022 00:34:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:44 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3612/6700 [1:40:36<1:26:41,  1.68s/it]11/17/2022 00:34:46 - INFO - train.train_snli_ve - kd_loss is tensor(7.6758e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:46 - INFO - train.train_snli_ve - loss is tensor(0.8683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3613/6700 [1:40:38<1:27:36,  1.70s/it]11/17/2022 00:34:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:47 - INFO - train.train_snli_ve - loss is tensor(0.8400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3614/6700 [1:40:40<1:27:47,  1.71s/it]11/17/2022 00:34:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:49 - INFO - train.train_snli_ve - loss is tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3615/6700 [1:40:42<1:28:02,  1.71s/it]11/17/2022 00:34:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.9147e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:51 - INFO - train.train_snli_ve - loss is tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3616/6700 [1:40:43<1:27:51,  1.71s/it]11/17/2022 00:34:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:52 - INFO - train.train_snli_ve - loss is tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####3    | 3617/6700 [1:40:45<1:26:57,  1.69s/it]11/17/2022 00:34:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:54 - INFO - train.train_snli_ve - loss is tensor(0.9303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3618/6700 [1:40:47<1:26:27,  1.68s/it]11/17/2022 00:34:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.3917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:56 - INFO - train.train_snli_ve - loss is tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3619/6700 [1:40:48<1:25:50,  1.67s/it]11/17/2022 00:34:57 - INFO - train.train_snli_ve - kd_loss is tensor(8.5261e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:57 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3620/6700 [1:40:50<1:25:53,  1.67s/it]11/17/2022 00:34:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:34:59 - INFO - train.train_snli_ve - loss is tensor(0.9555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3621/6700 [1:40:52<1:25:32,  1.67s/it]11/17/2022 00:35:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5357e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:01 - INFO - train.train_snli_ve - loss is tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3622/6700 [1:40:53<1:26:02,  1.68s/it]11/17/2022 00:35:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.1038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:02 - INFO - train.train_snli_ve - loss is tensor(0.8453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3623/6700 [1:40:55<1:25:38,  1.67s/it]11/17/2022 00:35:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.3539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:04 - INFO - train.train_snli_ve - loss is tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3624/6700 [1:40:57<1:25:36,  1.67s/it]11/17/2022 00:35:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.7699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:06 - INFO - train.train_snli_ve - loss is tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3625/6700 [1:40:58<1:25:39,  1.67s/it]11/17/2022 00:35:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3569e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:07 - INFO - train.train_snli_ve - loss is tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3626/6700 [1:41:00<1:25:58,  1.68s/it]11/17/2022 00:35:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9167e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:09 - INFO - train.train_snli_ve - loss is tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3627/6700 [1:41:02<1:25:56,  1.68s/it]11/17/2022 00:35:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.4009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:11 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3628/6700 [1:41:03<1:25:44,  1.67s/it]11/17/2022 00:35:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:12 - INFO - train.train_snli_ve - loss is tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3629/6700 [1:41:05<1:25:35,  1.67s/it]11/17/2022 00:35:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3261e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:14 - INFO - train.train_snli_ve - loss is tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3630/6700 [1:41:07<1:25:57,  1.68s/it]11/17/2022 00:35:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.8486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:16 - INFO - train.train_snli_ve - loss is tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3631/6700 [1:41:08<1:26:02,  1.68s/it]11/17/2022 00:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.5854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:18 - INFO - train.train_snli_ve - loss is tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3632/6700 [1:41:10<1:27:01,  1.70s/it]11/17/2022 00:35:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.3617e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:19 - INFO - train.train_snli_ve - loss is tensor(0.5388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3633/6700 [1:41:12<1:27:10,  1.71s/it]11/17/2022 00:35:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.0090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:21 - INFO - train.train_snli_ve - loss is tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3634/6700 [1:41:14<1:26:51,  1.70s/it]11/17/2022 00:35:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.5289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:23 - INFO - train.train_snli_ve - loss is tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3635/6700 [1:41:15<1:27:28,  1.71s/it]11/17/2022 00:35:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:24 - INFO - train.train_snli_ve - loss is tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3636/6700 [1:41:17<1:26:20,  1.69s/it]11/17/2022 00:35:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:26 - INFO - train.train_snli_ve - loss is tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3637/6700 [1:41:19<1:25:25,  1.67s/it]11/17/2022 00:35:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.0092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:28 - INFO - train.train_snli_ve - loss is tensor(0.4201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3638/6700 [1:41:20<1:25:42,  1.68s/it]11/17/2022 00:35:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.3355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:29 - INFO - train.train_snli_ve - loss is tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3639/6700 [1:41:22<1:25:55,  1.68s/it]11/17/2022 00:35:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.6665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:31 - INFO - train.train_snli_ve - loss is tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3640/6700 [1:41:24<1:25:59,  1.69s/it]11/17/2022 00:35:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.7375e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:33 - INFO - train.train_snli_ve - loss is tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3641/6700 [1:41:25<1:25:49,  1.68s/it]11/17/2022 00:35:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:34 - INFO - train.train_snli_ve - loss is tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3642/6700 [1:41:27<1:25:52,  1.68s/it]11/17/2022 00:35:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:36 - INFO - train.train_snli_ve - loss is tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3643/6700 [1:41:29<1:25:20,  1.68s/it]11/17/2022 00:35:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:38 - INFO - train.train_snli_ve - loss is tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3644/6700 [1:41:30<1:25:15,  1.67s/it]11/17/2022 00:35:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:39 - INFO - train.train_snli_ve - loss is tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3645/6700 [1:41:32<1:25:39,  1.68s/it]11/17/2022 00:35:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.0475e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:41 - INFO - train.train_snli_ve - loss is tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3646/6700 [1:41:34<1:25:28,  1.68s/it]11/17/2022 00:35:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:43 - INFO - train.train_snli_ve - loss is tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3647/6700 [1:41:35<1:24:46,  1.67s/it]11/17/2022 00:35:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:44 - INFO - train.train_snli_ve - loss is tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3648/6700 [1:41:37<1:24:39,  1.66s/it]11/17/2022 00:35:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:46 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3649/6700 [1:41:39<1:24:42,  1.67s/it]11/17/2022 00:35:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.6099e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:48 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3650/6700 [1:41:40<1:24:33,  1.66s/it]11/17/2022 00:35:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.9816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:49 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  54%|#####4    | 3651/6700 [1:41:42<1:25:15,  1.68s/it]11/17/2022 00:35:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.9058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:51 - INFO - train.train_snli_ve - loss is tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3652/6700 [1:41:44<1:24:48,  1.67s/it]11/17/2022 00:35:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.1958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:53 - INFO - train.train_snli_ve - loss is tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3653/6700 [1:41:45<1:24:45,  1.67s/it]11/17/2022 00:35:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:54 - INFO - train.train_snli_ve - loss is tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3654/6700 [1:41:47<1:24:58,  1.67s/it]11/17/2022 00:35:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:56 - INFO - train.train_snli_ve - loss is tensor(0.8160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3655/6700 [1:41:49<1:24:25,  1.66s/it]11/17/2022 00:35:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:58 - INFO - train.train_snli_ve - loss is tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3656/6700 [1:41:50<1:25:02,  1.68s/it]11/17/2022 00:35:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:35:59 - INFO - train.train_snli_ve - loss is tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3657/6700 [1:41:52<1:24:17,  1.66s/it]11/17/2022 00:36:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8905e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:01 - INFO - train.train_snli_ve - loss is tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3658/6700 [1:41:54<1:24:37,  1.67s/it]11/17/2022 00:36:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.4510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:03 - INFO - train.train_snli_ve - loss is tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3659/6700 [1:41:55<1:25:03,  1.68s/it]11/17/2022 00:36:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.2550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:04 - INFO - train.train_snli_ve - loss is tensor(0.7947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3660/6700 [1:41:57<1:24:42,  1.67s/it]11/17/2022 00:36:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.9397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:06 - INFO - train.train_snli_ve - loss is tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3661/6700 [1:41:59<1:24:23,  1.67s/it]11/17/2022 00:36:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.1438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:08 - INFO - train.train_snli_ve - loss is tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3662/6700 [1:42:00<1:24:40,  1.67s/it]11/17/2022 00:36:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:09 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3663/6700 [1:42:02<1:25:00,  1.68s/it]11/17/2022 00:36:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:11 - INFO - train.train_snli_ve - loss is tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3664/6700 [1:42:04<1:25:43,  1.69s/it]11/17/2022 00:36:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7443e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:13 - INFO - train.train_snli_ve - loss is tensor(0.8377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3665/6700 [1:42:06<1:24:48,  1.68s/it]11/17/2022 00:36:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:14 - INFO - train.train_snli_ve - loss is tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3666/6700 [1:42:07<1:24:33,  1.67s/it]11/17/2022 00:36:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.8518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:16 - INFO - train.train_snli_ve - loss is tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3667/6700 [1:42:09<1:24:39,  1.67s/it]11/17/2022 00:36:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:18 - INFO - train.train_snli_ve - loss is tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3668/6700 [1:42:11<1:24:55,  1.68s/it]11/17/2022 00:36:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8878e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:20 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3669/6700 [1:42:12<1:24:35,  1.67s/it]11/17/2022 00:36:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.1392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:21 - INFO - train.train_snli_ve - loss is tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3670/6700 [1:42:14<1:24:40,  1.68s/it]11/17/2022 00:36:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:23 - INFO - train.train_snli_ve - loss is tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3671/6700 [1:42:16<1:24:37,  1.68s/it]11/17/2022 00:36:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.1212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:25 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3672/6700 [1:42:17<1:25:18,  1.69s/it]11/17/2022 00:36:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:26 - INFO - train.train_snli_ve - loss is tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3673/6700 [1:42:19<1:24:26,  1.67s/it]11/17/2022 00:36:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:28 - INFO - train.train_snli_ve - loss is tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3674/6700 [1:42:21<1:24:24,  1.67s/it]11/17/2022 00:36:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.6970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:30 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3675/6700 [1:42:22<1:25:50,  1.70s/it]11/17/2022 00:36:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.0408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:31 - INFO - train.train_snli_ve - loss is tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3676/6700 [1:42:24<1:25:43,  1.70s/it]11/17/2022 00:36:33 - INFO - train.train_snli_ve - kd_loss is tensor(9.5752e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:33 - INFO - train.train_snli_ve - loss is tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3677/6700 [1:42:26<1:25:49,  1.70s/it]11/17/2022 00:36:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:35 - INFO - train.train_snli_ve - loss is tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3678/6700 [1:42:27<1:25:11,  1.69s/it]11/17/2022 00:36:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.8585e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:36 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3679/6700 [1:42:29<1:25:05,  1.69s/it]11/17/2022 00:36:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.2367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:38 - INFO - train.train_snli_ve - loss is tensor(0.8734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3680/6700 [1:42:31<1:24:33,  1.68s/it]11/17/2022 00:36:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:40 - INFO - train.train_snli_ve - loss is tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3681/6700 [1:42:32<1:24:09,  1.67s/it]11/17/2022 00:36:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:41 - INFO - train.train_snli_ve - loss is tensor(0.9600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3682/6700 [1:42:34<1:24:02,  1.67s/it]11/17/2022 00:36:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:43 - INFO - train.train_snli_ve - loss is tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3683/6700 [1:42:36<1:24:37,  1.68s/it]11/17/2022 00:36:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:45 - INFO - train.train_snli_ve - loss is tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####4    | 3684/6700 [1:42:38<1:24:58,  1.69s/it]11/17/2022 00:36:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.4284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:47 - INFO - train.train_snli_ve - loss is tensor(0.8393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3685/6700 [1:42:39<1:24:48,  1.69s/it]11/17/2022 00:36:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.2187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:48 - INFO - train.train_snli_ve - loss is tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3686/6700 [1:42:41<1:24:18,  1.68s/it]11/17/2022 00:36:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4614e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:50 - INFO - train.train_snli_ve - loss is tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3687/6700 [1:42:43<1:24:19,  1.68s/it]11/17/2022 00:36:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.8367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:52 - INFO - train.train_snli_ve - loss is tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3688/6700 [1:42:44<1:24:19,  1.68s/it]11/17/2022 00:36:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.8626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:53 - INFO - train.train_snli_ve - loss is tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3689/6700 [1:42:46<1:24:04,  1.68s/it]11/17/2022 00:36:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:55 - INFO - train.train_snli_ve - loss is tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3690/6700 [1:42:48<1:23:51,  1.67s/it]11/17/2022 00:36:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:57 - INFO - train.train_snli_ve - loss is tensor(0.8100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3691/6700 [1:42:49<1:24:05,  1.68s/it]11/17/2022 00:36:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0132e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:36:58 - INFO - train.train_snli_ve - loss is tensor(0.8622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3692/6700 [1:42:51<1:24:21,  1.68s/it]11/17/2022 00:37:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:00 - INFO - train.train_snli_ve - loss is tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3693/6700 [1:42:53<1:24:22,  1.68s/it]11/17/2022 00:37:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.8112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:02 - INFO - train.train_snli_ve - loss is tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3694/6700 [1:42:54<1:24:18,  1.68s/it]11/17/2022 00:37:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:03 - INFO - train.train_snli_ve - loss is tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3695/6700 [1:42:56<1:23:45,  1.67s/it]11/17/2022 00:37:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.3703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:05 - INFO - train.train_snli_ve - loss is tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3696/6700 [1:42:58<1:24:17,  1.68s/it]11/17/2022 00:37:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:07 - INFO - train.train_snli_ve - loss is tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3697/6700 [1:42:59<1:24:19,  1.68s/it]11/17/2022 00:37:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:08 - INFO - train.train_snli_ve - loss is tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3698/6700 [1:43:01<1:24:30,  1.69s/it]11/17/2022 00:37:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.1674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:10 - INFO - train.train_snli_ve - loss is tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3699/6700 [1:43:03<1:24:31,  1.69s/it]11/17/2022 00:37:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:12 - INFO - train.train_snli_ve - loss is tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3700/6700 [1:43:04<1:24:29,  1.69s/it]11/17/2022 00:37:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:13 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3701/6700 [1:43:06<1:24:04,  1.68s/it]11/17/2022 00:37:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:15 - INFO - train.train_snli_ve - loss is tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3702/6700 [1:43:08<1:23:57,  1.68s/it]11/17/2022 00:37:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:17 - INFO - train.train_snli_ve - loss is tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3703/6700 [1:43:09<1:24:10,  1.69s/it]11/17/2022 00:37:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:18 - INFO - train.train_snli_ve - loss is tensor(0.7866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3704/6700 [1:43:11<1:23:42,  1.68s/it]11/17/2022 00:37:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.2453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:20 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3705/6700 [1:43:13<1:23:46,  1.68s/it]11/17/2022 00:37:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:22 - INFO - train.train_snli_ve - loss is tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3706/6700 [1:43:14<1:23:35,  1.68s/it]11/17/2022 00:37:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:23 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3707/6700 [1:43:16<1:23:36,  1.68s/it]11/17/2022 00:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:25 - INFO - train.train_snli_ve - loss is tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3708/6700 [1:43:18<1:23:59,  1.68s/it]11/17/2022 00:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.7065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:27 - INFO - train.train_snli_ve - loss is tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3709/6700 [1:43:20<1:24:15,  1.69s/it]11/17/2022 00:37:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:29 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3710/6700 [1:43:21<1:24:34,  1.70s/it]11/17/2022 00:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:30 - INFO - train.train_snli_ve - loss is tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3711/6700 [1:43:23<1:24:24,  1.69s/it]11/17/2022 00:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.6395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:32 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3712/6700 [1:43:25<1:23:56,  1.69s/it]11/17/2022 00:37:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0461e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:34 - INFO - train.train_snli_ve - loss is tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3713/6700 [1:43:26<1:23:32,  1.68s/it]11/17/2022 00:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:35 - INFO - train.train_snli_ve - loss is tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3714/6700 [1:43:28<1:23:02,  1.67s/it]11/17/2022 00:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:37 - INFO - train.train_snli_ve - loss is tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3715/6700 [1:43:30<1:22:48,  1.66s/it]11/17/2022 00:37:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:39 - INFO - train.train_snli_ve - loss is tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3716/6700 [1:43:31<1:22:42,  1.66s/it]11/17/2022 00:37:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3097e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:40 - INFO - train.train_snli_ve - loss is tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3717/6700 [1:43:33<1:23:12,  1.67s/it]11/17/2022 00:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.5229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:42 - INFO - train.train_snli_ve - loss is tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  55%|#####5    | 3718/6700 [1:43:35<1:23:41,  1.68s/it]11/17/2022 00:37:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.6749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:44 - INFO - train.train_snli_ve - loss is tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3719/6700 [1:43:36<1:23:02,  1.67s/it]11/17/2022 00:37:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:45 - INFO - train.train_snli_ve - loss is tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3720/6700 [1:43:38<1:23:26,  1.68s/it]11/17/2022 00:37:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:47 - INFO - train.train_snli_ve - loss is tensor(0.8632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3721/6700 [1:43:40<1:23:22,  1.68s/it]11/17/2022 00:37:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:49 - INFO - train.train_snli_ve - loss is tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3722/6700 [1:43:41<1:23:31,  1.68s/it]11/17/2022 00:37:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:50 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3723/6700 [1:43:43<1:23:41,  1.69s/it]11/17/2022 00:37:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:52 - INFO - train.train_snli_ve - loss is tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3724/6700 [1:43:45<1:23:10,  1.68s/it]11/17/2022 00:37:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.4958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:54 - INFO - train.train_snli_ve - loss is tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3725/6700 [1:43:46<1:22:46,  1.67s/it]11/17/2022 00:37:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9052e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:55 - INFO - train.train_snli_ve - loss is tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3726/6700 [1:43:48<1:22:41,  1.67s/it]11/17/2022 00:37:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6472e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:57 - INFO - train.train_snli_ve - loss is tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3727/6700 [1:43:50<1:23:03,  1.68s/it]11/17/2022 00:37:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4119e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:37:59 - INFO - train.train_snli_ve - loss is tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3728/6700 [1:43:51<1:23:41,  1.69s/it]11/17/2022 00:38:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.3872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:00 - INFO - train.train_snli_ve - loss is tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3729/6700 [1:43:53<1:23:31,  1.69s/it]11/17/2022 00:38:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.8854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:02 - INFO - train.train_snli_ve - loss is tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3730/6700 [1:43:55<1:23:32,  1.69s/it]11/17/2022 00:38:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:04 - INFO - train.train_snli_ve - loss is tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3731/6700 [1:43:56<1:22:56,  1.68s/it]11/17/2022 00:38:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.1690e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:06 - INFO - train.train_snli_ve - loss is tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3732/6700 [1:43:58<1:23:44,  1.69s/it]11/17/2022 00:38:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:07 - INFO - train.train_snli_ve - loss is tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3733/6700 [1:44:00<1:23:25,  1.69s/it]11/17/2022 00:38:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.9430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:09 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3734/6700 [1:44:02<1:23:30,  1.69s/it]11/17/2022 00:38:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.4441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:11 - INFO - train.train_snli_ve - loss is tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3735/6700 [1:44:03<1:23:19,  1.69s/it]11/17/2022 00:38:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.2792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:12 - INFO - train.train_snli_ve - loss is tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3736/6700 [1:44:05<1:23:05,  1.68s/it]11/17/2022 00:38:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.6909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:14 - INFO - train.train_snli_ve - loss is tensor(0.4698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3737/6700 [1:44:07<1:23:24,  1.69s/it]11/17/2022 00:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:16 - INFO - train.train_snli_ve - loss is tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3738/6700 [1:44:08<1:23:03,  1.68s/it]11/17/2022 00:38:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.5333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:17 - INFO - train.train_snli_ve - loss is tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3739/6700 [1:44:10<1:22:46,  1.68s/it]11/17/2022 00:38:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.8017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:19 - INFO - train.train_snli_ve - loss is tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3740/6700 [1:44:12<1:22:09,  1.67s/it]11/17/2022 00:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.5402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:21 - INFO - train.train_snli_ve - loss is tensor(0.8647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3741/6700 [1:44:13<1:21:53,  1.66s/it]11/17/2022 00:38:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.0722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:22 - INFO - train.train_snli_ve - loss is tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3742/6700 [1:44:15<1:21:17,  1.65s/it]11/17/2022 00:38:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.6582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:24 - INFO - train.train_snli_ve - loss is tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3743/6700 [1:44:17<1:21:38,  1.66s/it]11/17/2022 00:38:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.3434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:26 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3744/6700 [1:44:18<1:21:31,  1.65s/it]11/17/2022 00:38:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.7696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:27 - INFO - train.train_snli_ve - loss is tensor(0.9710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3745/6700 [1:44:20<1:22:06,  1.67s/it]11/17/2022 00:38:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.8379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:29 - INFO - train.train_snli_ve - loss is tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3746/6700 [1:44:22<1:22:13,  1.67s/it]11/17/2022 00:38:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:31 - INFO - train.train_snli_ve - loss is tensor(0.5244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3747/6700 [1:44:23<1:23:02,  1.69s/it]11/17/2022 00:38:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:32 - INFO - train.train_snli_ve - loss is tensor(0.9047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3748/6700 [1:44:25<1:22:41,  1.68s/it]11/17/2022 00:38:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.7210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:34 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3749/6700 [1:44:27<1:22:40,  1.68s/it]11/17/2022 00:38:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.8514e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:36 - INFO - train.train_snli_ve - loss is tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3750/6700 [1:44:28<1:22:12,  1.67s/it]11/17/2022 00:38:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:37 - INFO - train.train_snli_ve - loss is tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####5    | 3751/6700 [1:44:30<1:22:13,  1.67s/it]11/17/2022 00:38:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.5809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:39 - INFO - train.train_snli_ve - loss is tensor(0.7824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3752/6700 [1:44:32<1:22:16,  1.67s/it]11/17/2022 00:38:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.6418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:41 - INFO - train.train_snli_ve - loss is tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3753/6700 [1:44:33<1:21:58,  1.67s/it]11/17/2022 00:38:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:42 - INFO - train.train_snli_ve - loss is tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3754/6700 [1:44:35<1:22:05,  1.67s/it]11/17/2022 00:38:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.7247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:44 - INFO - train.train_snli_ve - loss is tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3755/6700 [1:44:37<1:22:00,  1.67s/it]11/17/2022 00:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.3504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:46 - INFO - train.train_snli_ve - loss is tensor(0.8919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3756/6700 [1:44:38<1:22:18,  1.68s/it]11/17/2022 00:38:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.0476e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:47 - INFO - train.train_snli_ve - loss is tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3757/6700 [1:44:40<1:21:55,  1.67s/it]11/17/2022 00:38:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:49 - INFO - train.train_snli_ve - loss is tensor(0.7342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3758/6700 [1:44:42<1:22:33,  1.68s/it]11/17/2022 00:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:51 - INFO - train.train_snli_ve - loss is tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3759/6700 [1:44:43<1:22:11,  1.68s/it]11/17/2022 00:38:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.3869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:52 - INFO - train.train_snli_ve - loss is tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3760/6700 [1:44:45<1:21:53,  1.67s/it]11/17/2022 00:38:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.1405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:54 - INFO - train.train_snli_ve - loss is tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3761/6700 [1:44:47<1:22:28,  1.68s/it]11/17/2022 00:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.8929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:56 - INFO - train.train_snli_ve - loss is tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3762/6700 [1:44:48<1:22:11,  1.68s/it]11/17/2022 00:38:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4263e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:57 - INFO - train.train_snli_ve - loss is tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3763/6700 [1:44:50<1:22:33,  1.69s/it]11/17/2022 00:38:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:38:59 - INFO - train.train_snli_ve - loss is tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3764/6700 [1:44:52<1:22:20,  1.68s/it]11/17/2022 00:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.5564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:01 - INFO - train.train_snli_ve - loss is tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3765/6700 [1:44:53<1:21:59,  1.68s/it]11/17/2022 00:39:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:02 - INFO - train.train_snli_ve - loss is tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3766/6700 [1:44:55<1:22:39,  1.69s/it]11/17/2022 00:39:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:04 - INFO - train.train_snli_ve - loss is tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3767/6700 [1:44:57<1:22:52,  1.70s/it]11/17/2022 00:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.7088e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:06 - INFO - train.train_snli_ve - loss is tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3768/6700 [1:44:59<1:22:28,  1.69s/it]11/17/2022 00:39:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.4743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:08 - INFO - train.train_snli_ve - loss is tensor(1.0655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3769/6700 [1:45:00<1:21:59,  1.68s/it]11/17/2022 00:39:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.8221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:09 - INFO - train.train_snli_ve - loss is tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3770/6700 [1:45:02<1:22:04,  1.68s/it]11/17/2022 00:39:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.1635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:11 - INFO - train.train_snli_ve - loss is tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3771/6700 [1:45:04<1:22:28,  1.69s/it]11/17/2022 00:39:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:13 - INFO - train.train_snli_ve - loss is tensor(0.9947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3772/6700 [1:45:05<1:22:55,  1.70s/it]11/17/2022 00:39:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:14 - INFO - train.train_snli_ve - loss is tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3773/6700 [1:45:07<1:22:51,  1.70s/it]11/17/2022 00:39:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3540e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:16 - INFO - train.train_snli_ve - loss is tensor(0.8989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3774/6700 [1:45:09<1:22:12,  1.69s/it]11/17/2022 00:39:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8103e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:18 - INFO - train.train_snli_ve - loss is tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3775/6700 [1:45:10<1:21:33,  1.67s/it]11/17/2022 00:39:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.6111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:19 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3776/6700 [1:45:12<1:22:24,  1.69s/it]11/17/2022 00:39:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:21 - INFO - train.train_snli_ve - loss is tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3777/6700 [1:45:14<1:21:53,  1.68s/it]11/17/2022 00:39:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7553e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:23 - INFO - train.train_snli_ve - loss is tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3778/6700 [1:45:15<1:22:23,  1.69s/it]11/17/2022 00:39:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:24 - INFO - train.train_snli_ve - loss is tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3779/6700 [1:45:17<1:22:16,  1.69s/it]11/17/2022 00:39:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5230e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:26 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3780/6700 [1:45:19<1:22:05,  1.69s/it]11/17/2022 00:39:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5568e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:28 - INFO - train.train_snli_ve - loss is tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3781/6700 [1:45:20<1:21:38,  1.68s/it]11/17/2022 00:39:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.4847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:29 - INFO - train.train_snli_ve - loss is tensor(0.7513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3782/6700 [1:45:22<1:21:38,  1.68s/it]11/17/2022 00:39:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.4533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:31 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3783/6700 [1:45:24<1:21:52,  1.68s/it]11/17/2022 00:39:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:33 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3784/6700 [1:45:26<1:21:43,  1.68s/it]11/17/2022 00:39:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:35 - INFO - train.train_snli_ve - loss is tensor(0.8879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  56%|#####6    | 3785/6700 [1:45:27<1:21:59,  1.69s/it]11/17/2022 00:39:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:36 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3786/6700 [1:45:29<1:22:59,  1.71s/it]11/17/2022 00:39:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:38 - INFO - train.train_snli_ve - loss is tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3787/6700 [1:45:31<1:22:48,  1.71s/it]11/17/2022 00:39:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:40 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3788/6700 [1:45:32<1:22:46,  1.71s/it]11/17/2022 00:39:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:41 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3789/6700 [1:45:34<1:21:51,  1.69s/it]11/17/2022 00:39:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1893e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:43 - INFO - train.train_snli_ve - loss is tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3790/6700 [1:45:36<1:21:53,  1.69s/it]11/17/2022 00:39:45 - INFO - train.train_snli_ve - kd_loss is tensor(9.6622e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:45 - INFO - train.train_snli_ve - loss is tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3791/6700 [1:45:37<1:21:27,  1.68s/it]11/17/2022 00:39:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:46 - INFO - train.train_snli_ve - loss is tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3792/6700 [1:45:39<1:21:59,  1.69s/it]11/17/2022 00:39:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:48 - INFO - train.train_snli_ve - loss is tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3793/6700 [1:45:41<1:21:31,  1.68s/it]11/17/2022 00:39:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3316e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:50 - INFO - train.train_snli_ve - loss is tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3794/6700 [1:45:42<1:21:57,  1.69s/it]11/17/2022 00:39:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.1340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:51 - INFO - train.train_snli_ve - loss is tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3795/6700 [1:45:44<1:21:47,  1.69s/it]11/17/2022 00:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.8198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:53 - INFO - train.train_snli_ve - loss is tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3796/6700 [1:45:46<1:21:44,  1.69s/it]11/17/2022 00:39:55 - INFO - train.train_snli_ve - kd_loss is tensor(8.0994e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:55 - INFO - train.train_snli_ve - loss is tensor(0.5404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3797/6700 [1:45:48<1:22:08,  1.70s/it]11/17/2022 00:39:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.5929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:57 - INFO - train.train_snli_ve - loss is tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3798/6700 [1:45:49<1:21:24,  1.68s/it]11/17/2022 00:39:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:39:58 - INFO - train.train_snli_ve - loss is tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3799/6700 [1:45:51<1:20:37,  1.67s/it]11/17/2022 00:40:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.0538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:00 - INFO - train.train_snli_ve - loss is tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3800/6700 [1:45:53<1:20:57,  1.67s/it]11/17/2022 00:40:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:01 - INFO - train.train_snli_ve - loss is tensor(0.8022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3801/6700 [1:45:54<1:20:44,  1.67s/it]11/17/2022 00:40:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:03 - INFO - train.train_snli_ve - loss is tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3802/6700 [1:45:56<1:21:11,  1.68s/it]11/17/2022 00:40:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:05 - INFO - train.train_snli_ve - loss is tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3803/6700 [1:45:58<1:20:47,  1.67s/it]11/17/2022 00:40:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:07 - INFO - train.train_snli_ve - loss is tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3804/6700 [1:45:59<1:20:59,  1.68s/it]11/17/2022 00:40:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:08 - INFO - train.train_snli_ve - loss is tensor(0.5637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3805/6700 [1:46:01<1:20:44,  1.67s/it]11/17/2022 00:40:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.7883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:10 - INFO - train.train_snli_ve - loss is tensor(0.7674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3806/6700 [1:46:03<1:21:42,  1.69s/it]11/17/2022 00:40:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:12 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3807/6700 [1:46:04<1:21:06,  1.68s/it]11/17/2022 00:40:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.3563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:13 - INFO - train.train_snli_ve - loss is tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3808/6700 [1:46:06<1:20:50,  1.68s/it]11/17/2022 00:40:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.8149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:15 - INFO - train.train_snli_ve - loss is tensor(0.4658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3809/6700 [1:46:08<1:20:36,  1.67s/it]11/17/2022 00:40:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.0989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:17 - INFO - train.train_snli_ve - loss is tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3810/6700 [1:46:09<1:20:39,  1.67s/it]11/17/2022 00:40:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.1091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:18 - INFO - train.train_snli_ve - loss is tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3811/6700 [1:46:11<1:20:43,  1.68s/it]11/17/2022 00:40:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:20 - INFO - train.train_snli_ve - loss is tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3812/6700 [1:46:13<1:20:53,  1.68s/it]11/17/2022 00:40:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6614e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:22 - INFO - train.train_snli_ve - loss is tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3813/6700 [1:46:14<1:21:00,  1.68s/it]11/17/2022 00:40:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.1289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:23 - INFO - train.train_snli_ve - loss is tensor(0.8306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3814/6700 [1:46:16<1:20:54,  1.68s/it]11/17/2022 00:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:25 - INFO - train.train_snli_ve - loss is tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3815/6700 [1:46:18<1:20:55,  1.68s/it]11/17/2022 00:40:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8874e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:27 - INFO - train.train_snli_ve - loss is tensor(0.4710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3816/6700 [1:46:19<1:22:00,  1.71s/it]11/17/2022 00:40:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.8752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:28 - INFO - train.train_snli_ve - loss is tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3817/6700 [1:46:21<1:21:01,  1.69s/it]11/17/2022 00:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.2326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:30 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3818/6700 [1:46:23<1:20:48,  1.68s/it]11/17/2022 00:40:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4020e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:32 - INFO - train.train_snli_ve - loss is tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####6    | 3819/6700 [1:46:25<1:21:17,  1.69s/it]11/17/2022 00:40:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.4005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:34 - INFO - train.train_snli_ve - loss is tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3820/6700 [1:46:26<1:21:58,  1.71s/it]11/17/2022 00:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:35 - INFO - train.train_snli_ve - loss is tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3821/6700 [1:46:28<1:21:23,  1.70s/it]11/17/2022 00:40:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:37 - INFO - train.train_snli_ve - loss is tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3822/6700 [1:46:30<1:21:28,  1.70s/it]11/17/2022 00:40:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:39 - INFO - train.train_snli_ve - loss is tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3823/6700 [1:46:31<1:21:01,  1.69s/it]11/17/2022 00:40:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:40 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3824/6700 [1:46:33<1:21:27,  1.70s/it]11/17/2022 00:40:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.5624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:42 - INFO - train.train_snli_ve - loss is tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3825/6700 [1:46:35<1:20:54,  1.69s/it]11/17/2022 00:40:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.3936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:44 - INFO - train.train_snli_ve - loss is tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3826/6700 [1:46:36<1:20:48,  1.69s/it]11/17/2022 00:40:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.2651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:45 - INFO - train.train_snli_ve - loss is tensor(0.7678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3827/6700 [1:46:38<1:20:59,  1.69s/it]11/17/2022 00:40:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:47 - INFO - train.train_snli_ve - loss is tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3828/6700 [1:46:40<1:19:57,  1.67s/it]11/17/2022 00:40:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:49 - INFO - train.train_snli_ve - loss is tensor(0.4994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3829/6700 [1:46:41<1:19:23,  1.66s/it]11/17/2022 00:40:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:50 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3830/6700 [1:46:43<1:19:32,  1.66s/it]11/17/2022 00:40:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5823e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:52 - INFO - train.train_snli_ve - loss is tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3831/6700 [1:46:45<1:19:29,  1.66s/it]11/17/2022 00:40:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:54 - INFO - train.train_snli_ve - loss is tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3832/6700 [1:46:46<1:20:12,  1.68s/it]11/17/2022 00:40:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.5147e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:55 - INFO - train.train_snli_ve - loss is tensor(0.4837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3833/6700 [1:46:48<1:20:16,  1.68s/it]11/17/2022 00:40:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:57 - INFO - train.train_snli_ve - loss is tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3834/6700 [1:46:50<1:19:30,  1.66s/it]11/17/2022 00:40:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.7402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:40:59 - INFO - train.train_snli_ve - loss is tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3835/6700 [1:46:51<1:19:36,  1.67s/it]11/17/2022 00:41:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.9567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:00 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3836/6700 [1:46:53<1:19:47,  1.67s/it]11/17/2022 00:41:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:02 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3837/6700 [1:46:55<1:19:51,  1.67s/it]11/17/2022 00:41:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.5782e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:04 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3838/6700 [1:46:56<1:20:03,  1.68s/it]11/17/2022 00:41:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9782e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:05 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3839/6700 [1:46:58<1:20:09,  1.68s/it]11/17/2022 00:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.0437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:07 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3840/6700 [1:47:00<1:19:40,  1.67s/it]11/17/2022 00:41:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:09 - INFO - train.train_snli_ve - loss is tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3841/6700 [1:47:01<1:19:43,  1.67s/it]11/17/2022 00:41:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5991e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:10 - INFO - train.train_snli_ve - loss is tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3842/6700 [1:47:03<1:20:09,  1.68s/it]11/17/2022 00:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:12 - INFO - train.train_snli_ve - loss is tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3843/6700 [1:47:05<1:20:08,  1.68s/it]11/17/2022 00:41:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:14 - INFO - train.train_snli_ve - loss is tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3844/6700 [1:47:06<1:20:01,  1.68s/it]11/17/2022 00:41:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.4945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:15 - INFO - train.train_snli_ve - loss is tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3845/6700 [1:47:08<1:20:02,  1.68s/it]11/17/2022 00:41:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:17 - INFO - train.train_snli_ve - loss is tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3846/6700 [1:47:10<1:20:30,  1.69s/it]11/17/2022 00:41:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:19 - INFO - train.train_snli_ve - loss is tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3847/6700 [1:47:12<1:19:56,  1.68s/it]11/17/2022 00:41:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1719e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:21 - INFO - train.train_snli_ve - loss is tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3848/6700 [1:47:13<1:20:46,  1.70s/it]11/17/2022 00:41:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:22 - INFO - train.train_snli_ve - loss is tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3849/6700 [1:47:15<1:20:16,  1.69s/it]11/17/2022 00:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.4613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:24 - INFO - train.train_snli_ve - loss is tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3850/6700 [1:47:17<1:20:42,  1.70s/it]11/17/2022 00:41:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.7166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:26 - INFO - train.train_snli_ve - loss is tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3851/6700 [1:47:18<1:21:00,  1.71s/it]11/17/2022 00:41:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5580e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:27 - INFO - train.train_snli_ve - loss is tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  57%|#####7    | 3852/6700 [1:47:20<1:20:41,  1.70s/it]11/17/2022 00:41:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.8951e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:29 - INFO - train.train_snli_ve - loss is tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3853/6700 [1:47:22<1:20:23,  1.69s/it]11/17/2022 00:41:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:31 - INFO - train.train_snli_ve - loss is tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3854/6700 [1:47:23<1:19:58,  1.69s/it]11/17/2022 00:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.3133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:32 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3855/6700 [1:47:25<1:19:59,  1.69s/it]11/17/2022 00:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.5758e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:34 - INFO - train.train_snli_ve - loss is tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3856/6700 [1:47:27<1:20:03,  1.69s/it]11/17/2022 00:41:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:36 - INFO - train.train_snli_ve - loss is tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3857/6700 [1:47:28<1:19:46,  1.68s/it]11/17/2022 00:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:37 - INFO - train.train_snli_ve - loss is tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3858/6700 [1:47:30<1:19:22,  1.68s/it]11/17/2022 00:41:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.8524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:39 - INFO - train.train_snli_ve - loss is tensor(0.9166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3859/6700 [1:47:32<1:19:08,  1.67s/it]11/17/2022 00:41:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:41 - INFO - train.train_snli_ve - loss is tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3860/6700 [1:47:33<1:18:54,  1.67s/it]11/17/2022 00:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.5141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:42 - INFO - train.train_snli_ve - loss is tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3861/6700 [1:47:35<1:19:00,  1.67s/it]11/17/2022 00:41:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:44 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3862/6700 [1:47:37<1:19:13,  1.67s/it]11/17/2022 00:41:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1775e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:46 - INFO - train.train_snli_ve - loss is tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3863/6700 [1:47:38<1:18:46,  1.67s/it]11/17/2022 00:41:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:47 - INFO - train.train_snli_ve - loss is tensor(0.4326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3864/6700 [1:47:40<1:19:08,  1.67s/it]11/17/2022 00:41:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:49 - INFO - train.train_snli_ve - loss is tensor(0.5425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3865/6700 [1:47:42<1:19:12,  1.68s/it]11/17/2022 00:41:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.8193e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:51 - INFO - train.train_snli_ve - loss is tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3866/6700 [1:47:43<1:18:47,  1.67s/it]11/17/2022 00:41:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.4346e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:52 - INFO - train.train_snli_ve - loss is tensor(0.8173, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3867/6700 [1:47:45<1:19:02,  1.67s/it]11/17/2022 00:41:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:54 - INFO - train.train_snli_ve - loss is tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3868/6700 [1:47:47<1:19:25,  1.68s/it]11/17/2022 00:41:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:56 - INFO - train.train_snli_ve - loss is tensor(1.0570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3869/6700 [1:47:49<1:19:35,  1.69s/it]11/17/2022 00:41:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.6236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:58 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3870/6700 [1:47:50<1:19:32,  1.69s/it]11/17/2022 00:41:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.8538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:41:59 - INFO - train.train_snli_ve - loss is tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3871/6700 [1:47:52<1:19:57,  1.70s/it]11/17/2022 00:42:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:01 - INFO - train.train_snli_ve - loss is tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3872/6700 [1:47:54<1:19:08,  1.68s/it]11/17/2022 00:42:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:03 - INFO - train.train_snli_ve - loss is tensor(0.8035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3873/6700 [1:47:55<1:18:55,  1.68s/it]11/17/2022 00:42:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2512e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:04 - INFO - train.train_snli_ve - loss is tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3874/6700 [1:47:57<1:18:53,  1.68s/it]11/17/2022 00:42:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:06 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3875/6700 [1:47:59<1:19:19,  1.68s/it]11/17/2022 00:42:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3741e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:08 - INFO - train.train_snli_ve - loss is tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3876/6700 [1:48:00<1:19:08,  1.68s/it]11/17/2022 00:42:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:09 - INFO - train.train_snli_ve - loss is tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3877/6700 [1:48:02<1:19:26,  1.69s/it]11/17/2022 00:42:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:11 - INFO - train.train_snli_ve - loss is tensor(0.4799, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3878/6700 [1:48:04<1:19:10,  1.68s/it]11/17/2022 00:42:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:13 - INFO - train.train_snli_ve - loss is tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3879/6700 [1:48:05<1:19:56,  1.70s/it]11/17/2022 00:42:14 - INFO - train.train_snli_ve - kd_loss is tensor(9.8118e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:14 - INFO - train.train_snli_ve - loss is tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3880/6700 [1:48:07<1:18:54,  1.68s/it]11/17/2022 00:42:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:16 - INFO - train.train_snli_ve - loss is tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3881/6700 [1:48:09<1:18:41,  1.67s/it]11/17/2022 00:42:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:18 - INFO - train.train_snli_ve - loss is tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3882/6700 [1:48:10<1:19:15,  1.69s/it]11/17/2022 00:42:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.8214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:19 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3883/6700 [1:48:12<1:18:52,  1.68s/it]11/17/2022 00:42:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:21 - INFO - train.train_snli_ve - loss is tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3884/6700 [1:48:14<1:18:54,  1.68s/it]11/17/2022 00:42:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:23 - INFO - train.train_snli_ve - loss is tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####7    | 3885/6700 [1:48:15<1:18:30,  1.67s/it]11/17/2022 00:42:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:24 - INFO - train.train_snli_ve - loss is tensor(0.9408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3886/6700 [1:48:17<1:19:07,  1.69s/it]11/17/2022 00:42:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.0017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:26 - INFO - train.train_snli_ve - loss is tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3887/6700 [1:48:19<1:19:19,  1.69s/it]11/17/2022 00:42:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:28 - INFO - train.train_snli_ve - loss is tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3888/6700 [1:48:21<1:18:47,  1.68s/it]11/17/2022 00:42:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5365e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:30 - INFO - train.train_snli_ve - loss is tensor(0.5732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3889/6700 [1:48:22<1:18:27,  1.67s/it]11/17/2022 00:42:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8954e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:31 - INFO - train.train_snli_ve - loss is tensor(0.5239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3890/6700 [1:48:24<1:19:21,  1.69s/it]11/17/2022 00:42:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:33 - INFO - train.train_snli_ve - loss is tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3891/6700 [1:48:26<1:18:59,  1.69s/it]11/17/2022 00:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5048e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:35 - INFO - train.train_snli_ve - loss is tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3892/6700 [1:48:27<1:18:36,  1.68s/it]11/17/2022 00:42:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:36 - INFO - train.train_snli_ve - loss is tensor(0.8714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3893/6700 [1:48:29<1:18:25,  1.68s/it]11/17/2022 00:42:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.7038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:38 - INFO - train.train_snli_ve - loss is tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3894/6700 [1:48:31<1:17:51,  1.66s/it]11/17/2022 00:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:40 - INFO - train.train_snli_ve - loss is tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3895/6700 [1:48:32<1:18:01,  1.67s/it]11/17/2022 00:42:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:41 - INFO - train.train_snli_ve - loss is tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3896/6700 [1:48:34<1:18:03,  1.67s/it]11/17/2022 00:42:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.4672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:43 - INFO - train.train_snli_ve - loss is tensor(0.9095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3897/6700 [1:48:36<1:18:43,  1.69s/it]11/17/2022 00:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.5896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:45 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3898/6700 [1:48:37<1:18:17,  1.68s/it]11/17/2022 00:42:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:46 - INFO - train.train_snli_ve - loss is tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3899/6700 [1:48:39<1:18:18,  1.68s/it]11/17/2022 00:42:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:48 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3900/6700 [1:48:41<1:18:46,  1.69s/it]11/17/2022 00:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3497e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:50 - INFO - train.train_snli_ve - loss is tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3901/6700 [1:48:42<1:18:56,  1.69s/it]11/17/2022 00:42:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.3321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:51 - INFO - train.train_snli_ve - loss is tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3902/6700 [1:48:44<1:19:02,  1.69s/it]11/17/2022 00:42:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:53 - INFO - train.train_snli_ve - loss is tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3903/6700 [1:48:46<1:18:56,  1.69s/it]11/17/2022 00:42:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:55 - INFO - train.train_snli_ve - loss is tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3904/6700 [1:48:47<1:18:46,  1.69s/it]11/17/2022 00:42:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.0286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:56 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3905/6700 [1:48:49<1:18:03,  1.68s/it]11/17/2022 00:42:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.1998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:42:58 - INFO - train.train_snli_ve - loss is tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3906/6700 [1:48:51<1:18:41,  1.69s/it]11/17/2022 00:43:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.1921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:00 - INFO - train.train_snli_ve - loss is tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3907/6700 [1:48:53<1:18:15,  1.68s/it]11/17/2022 00:43:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:02 - INFO - train.train_snli_ve - loss is tensor(0.5638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3908/6700 [1:48:54<1:18:36,  1.69s/it]11/17/2022 00:43:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:03 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3909/6700 [1:48:56<1:18:53,  1.70s/it]11/17/2022 00:43:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:05 - INFO - train.train_snli_ve - loss is tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3910/6700 [1:48:58<1:19:03,  1.70s/it]11/17/2022 00:43:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:07 - INFO - train.train_snli_ve - loss is tensor(0.4814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3911/6700 [1:48:59<1:18:51,  1.70s/it]11/17/2022 00:43:08 - INFO - train.train_snli_ve - kd_loss is tensor(7.1368e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:08 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3912/6700 [1:49:01<1:18:55,  1.70s/it]11/17/2022 00:43:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:10 - INFO - train.train_snli_ve - loss is tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3913/6700 [1:49:03<1:18:40,  1.69s/it]11/17/2022 00:43:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.5929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:12 - INFO - train.train_snli_ve - loss is tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3914/6700 [1:49:04<1:18:43,  1.70s/it]11/17/2022 00:43:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.1825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:13 - INFO - train.train_snli_ve - loss is tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3915/6700 [1:49:06<1:18:44,  1.70s/it]11/17/2022 00:43:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.2541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:15 - INFO - train.train_snli_ve - loss is tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3916/6700 [1:49:08<1:18:51,  1.70s/it]11/17/2022 00:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.5921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:17 - INFO - train.train_snli_ve - loss is tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3917/6700 [1:49:09<1:18:32,  1.69s/it]11/17/2022 00:43:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:18 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3918/6700 [1:49:11<1:18:13,  1.69s/it]11/17/2022 00:43:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5649e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:20 - INFO - train.train_snli_ve - loss is tensor(1.0852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  58%|#####8    | 3919/6700 [1:49:13<1:17:54,  1.68s/it]11/17/2022 00:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:22 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3920/6700 [1:49:14<1:17:25,  1.67s/it]11/17/2022 00:43:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.0376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:23 - INFO - train.train_snli_ve - loss is tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3921/6700 [1:49:16<1:17:37,  1.68s/it]11/17/2022 00:43:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.8223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:25 - INFO - train.train_snli_ve - loss is tensor(0.7629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3922/6700 [1:49:18<1:17:52,  1.68s/it]11/17/2022 00:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:27 - INFO - train.train_snli_ve - loss is tensor(0.7993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3923/6700 [1:49:20<1:17:56,  1.68s/it]11/17/2022 00:43:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:29 - INFO - train.train_snli_ve - loss is tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3924/6700 [1:49:21<1:18:11,  1.69s/it]11/17/2022 00:43:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:30 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3925/6700 [1:49:23<1:17:55,  1.68s/it]11/17/2022 00:43:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.5437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:32 - INFO - train.train_snli_ve - loss is tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3926/6700 [1:49:25<1:17:49,  1.68s/it]11/17/2022 00:43:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:34 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3927/6700 [1:49:26<1:18:11,  1.69s/it]11/17/2022 00:43:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.3425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:35 - INFO - train.train_snli_ve - loss is tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3928/6700 [1:49:28<1:18:17,  1.69s/it]11/17/2022 00:43:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.5500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:37 - INFO - train.train_snli_ve - loss is tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3929/6700 [1:49:30<1:17:46,  1.68s/it]11/17/2022 00:43:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.5494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:39 - INFO - train.train_snli_ve - loss is tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3930/6700 [1:49:31<1:17:47,  1.69s/it]11/17/2022 00:43:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.2483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:40 - INFO - train.train_snli_ve - loss is tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3931/6700 [1:49:33<1:17:33,  1.68s/it]11/17/2022 00:43:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.9650e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:42 - INFO - train.train_snli_ve - loss is tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3932/6700 [1:49:35<1:17:50,  1.69s/it]11/17/2022 00:43:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.9793e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:44 - INFO - train.train_snli_ve - loss is tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3933/6700 [1:49:36<1:17:35,  1.68s/it]11/17/2022 00:43:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:45 - INFO - train.train_snli_ve - loss is tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3934/6700 [1:49:38<1:18:03,  1.69s/it]11/17/2022 00:43:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:47 - INFO - train.train_snli_ve - loss is tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3935/6700 [1:49:40<1:18:01,  1.69s/it]11/17/2022 00:43:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.4718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:49 - INFO - train.train_snli_ve - loss is tensor(0.8035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3936/6700 [1:49:42<1:17:57,  1.69s/it]11/17/2022 00:43:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:50 - INFO - train.train_snli_ve - loss is tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3937/6700 [1:49:43<1:16:57,  1.67s/it]11/17/2022 00:43:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.8814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:52 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3938/6700 [1:49:45<1:17:48,  1.69s/it]11/17/2022 00:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:54 - INFO - train.train_snli_ve - loss is tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3939/6700 [1:49:47<1:18:04,  1.70s/it]11/17/2022 00:43:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.2255e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:56 - INFO - train.train_snli_ve - loss is tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3940/6700 [1:49:48<1:17:49,  1.69s/it]11/17/2022 00:43:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:57 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3941/6700 [1:49:50<1:17:49,  1.69s/it]11/17/2022 00:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.5504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:43:59 - INFO - train.train_snli_ve - loss is tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3942/6700 [1:49:52<1:17:32,  1.69s/it]11/17/2022 00:44:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.0231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:01 - INFO - train.train_snli_ve - loss is tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3943/6700 [1:49:53<1:17:51,  1.69s/it]11/17/2022 00:44:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.7028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:02 - INFO - train.train_snli_ve - loss is tensor(0.8450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3944/6700 [1:49:55<1:17:39,  1.69s/it]11/17/2022 00:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:04 - INFO - train.train_snli_ve - loss is tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3945/6700 [1:49:57<1:17:54,  1.70s/it]11/17/2022 00:44:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:06 - INFO - train.train_snli_ve - loss is tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3946/6700 [1:49:58<1:18:21,  1.71s/it]11/17/2022 00:44:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:07 - INFO - train.train_snli_ve - loss is tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3947/6700 [1:50:00<1:17:56,  1.70s/it]11/17/2022 00:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:09 - INFO - train.train_snli_ve - loss is tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3948/6700 [1:50:02<1:18:24,  1.71s/it]11/17/2022 00:44:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.3661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:11 - INFO - train.train_snli_ve - loss is tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3949/6700 [1:50:04<1:18:30,  1.71s/it]11/17/2022 00:44:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6685e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:13 - INFO - train.train_snli_ve - loss is tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3950/6700 [1:50:05<1:18:25,  1.71s/it]11/17/2022 00:44:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:14 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3951/6700 [1:50:07<1:17:49,  1.70s/it]11/17/2022 00:44:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.8460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:16 - INFO - train.train_snli_ve - loss is tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3952/6700 [1:50:09<1:17:23,  1.69s/it]11/17/2022 00:44:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:18 - INFO - train.train_snli_ve - loss is tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####8    | 3953/6700 [1:50:10<1:17:05,  1.68s/it]11/17/2022 00:44:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.3687e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:19 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3954/6700 [1:50:12<1:16:26,  1.67s/it]11/17/2022 00:44:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.9144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:21 - INFO - train.train_snli_ve - loss is tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3955/6700 [1:50:14<1:16:20,  1.67s/it]11/17/2022 00:44:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.9811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:23 - INFO - train.train_snli_ve - loss is tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3956/6700 [1:50:15<1:16:25,  1.67s/it]11/17/2022 00:44:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:24 - INFO - train.train_snli_ve - loss is tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3957/6700 [1:50:17<1:16:12,  1.67s/it]11/17/2022 00:44:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:26 - INFO - train.train_snli_ve - loss is tensor(0.9723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3958/6700 [1:50:19<1:16:33,  1.68s/it]11/17/2022 00:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5044e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:28 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3959/6700 [1:50:20<1:17:24,  1.69s/it]11/17/2022 00:44:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.8727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:29 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3960/6700 [1:50:22<1:17:39,  1.70s/it]11/17/2022 00:44:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:31 - INFO - train.train_snli_ve - loss is tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3961/6700 [1:50:24<1:17:45,  1.70s/it]11/17/2022 00:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:33 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3962/6700 [1:50:25<1:17:23,  1.70s/it]11/17/2022 00:44:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.4837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:34 - INFO - train.train_snli_ve - loss is tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3963/6700 [1:50:27<1:16:58,  1.69s/it]11/17/2022 00:44:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:36 - INFO - train.train_snli_ve - loss is tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3964/6700 [1:50:29<1:17:07,  1.69s/it]11/17/2022 00:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:38 - INFO - train.train_snli_ve - loss is tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3965/6700 [1:50:31<1:16:20,  1.67s/it]11/17/2022 00:44:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:39 - INFO - train.train_snli_ve - loss is tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3966/6700 [1:50:32<1:16:13,  1.67s/it]11/17/2022 00:44:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:41 - INFO - train.train_snli_ve - loss is tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3967/6700 [1:50:34<1:17:56,  1.71s/it]11/17/2022 00:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(5.5062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:43 - INFO - train.train_snli_ve - loss is tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3968/6700 [1:50:36<1:17:13,  1.70s/it]11/17/2022 00:44:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:45 - INFO - train.train_snli_ve - loss is tensor(0.4393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3969/6700 [1:50:37<1:17:11,  1.70s/it]11/17/2022 00:44:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.1257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:46 - INFO - train.train_snli_ve - loss is tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3970/6700 [1:50:39<1:17:40,  1.71s/it]11/17/2022 00:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:48 - INFO - train.train_snli_ve - loss is tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3971/6700 [1:50:41<1:17:44,  1.71s/it]11/17/2022 00:44:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.1144e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:50 - INFO - train.train_snli_ve - loss is tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3972/6700 [1:50:42<1:17:07,  1.70s/it]11/17/2022 00:44:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.6691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:51 - INFO - train.train_snli_ve - loss is tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3973/6700 [1:50:44<1:16:58,  1.69s/it]11/17/2022 00:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:53 - INFO - train.train_snli_ve - loss is tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3974/6700 [1:50:46<1:17:13,  1.70s/it]11/17/2022 00:44:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.3844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:55 - INFO - train.train_snli_ve - loss is tensor(0.4517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3975/6700 [1:50:48<1:17:30,  1.71s/it]11/17/2022 00:44:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:57 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3976/6700 [1:50:49<1:17:15,  1.70s/it]11/17/2022 00:44:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:44:58 - INFO - train.train_snli_ve - loss is tensor(0.4916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3977/6700 [1:50:51<1:17:11,  1.70s/it]11/17/2022 00:45:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.1970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:00 - INFO - train.train_snli_ve - loss is tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3978/6700 [1:50:53<1:16:55,  1.70s/it]11/17/2022 00:45:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:02 - INFO - train.train_snli_ve - loss is tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3979/6700 [1:50:54<1:17:08,  1.70s/it]11/17/2022 00:45:03 - INFO - train.train_snli_ve - kd_loss is tensor(5.1439e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:03 - INFO - train.train_snli_ve - loss is tensor(0.8606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3980/6700 [1:50:56<1:17:04,  1.70s/it]11/17/2022 00:45:05 - INFO - train.train_snli_ve - kd_loss is tensor(8.1518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:05 - INFO - train.train_snli_ve - loss is tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3981/6700 [1:50:58<1:16:31,  1.69s/it]11/17/2022 00:45:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.2471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:07 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3982/6700 [1:50:59<1:16:24,  1.69s/it]11/17/2022 00:45:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.2183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:08 - INFO - train.train_snli_ve - loss is tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3983/6700 [1:51:01<1:15:57,  1.68s/it]11/17/2022 00:45:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.0410e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:10 - INFO - train.train_snli_ve - loss is tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3984/6700 [1:51:03<1:16:41,  1.69s/it]11/17/2022 00:45:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:12 - INFO - train.train_snli_ve - loss is tensor(0.4760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3985/6700 [1:51:04<1:16:33,  1.69s/it]11/17/2022 00:45:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.0822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:13 - INFO - train.train_snli_ve - loss is tensor(0.5168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  59%|#####9    | 3986/6700 [1:51:06<1:16:38,  1.69s/it]11/17/2022 00:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.7769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:15 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3987/6700 [1:51:08<1:17:40,  1.72s/it]11/17/2022 00:45:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.4483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:17 - INFO - train.train_snli_ve - loss is tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3988/6700 [1:51:10<1:17:25,  1.71s/it]11/17/2022 00:45:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:19 - INFO - train.train_snli_ve - loss is tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3989/6700 [1:51:11<1:17:15,  1.71s/it]11/17/2022 00:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.9023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:20 - INFO - train.train_snli_ve - loss is tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3990/6700 [1:51:13<1:17:18,  1.71s/it]11/17/2022 00:45:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.0140e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:22 - INFO - train.train_snli_ve - loss is tensor(0.4564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3991/6700 [1:51:15<1:16:48,  1.70s/it]11/17/2022 00:45:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:24 - INFO - train.train_snli_ve - loss is tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3992/6700 [1:51:16<1:17:08,  1.71s/it]11/17/2022 00:45:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.6286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:25 - INFO - train.train_snli_ve - loss is tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3993/6700 [1:51:18<1:16:40,  1.70s/it]11/17/2022 00:45:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.6318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:27 - INFO - train.train_snli_ve - loss is tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3994/6700 [1:51:20<1:16:39,  1.70s/it]11/17/2022 00:45:29 - INFO - train.train_snli_ve - kd_loss is tensor(6.2882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:29 - INFO - train.train_snli_ve - loss is tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3995/6700 [1:51:21<1:15:42,  1.68s/it]11/17/2022 00:45:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.0254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:30 - INFO - train.train_snli_ve - loss is tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3996/6700 [1:51:23<1:15:24,  1.67s/it]11/17/2022 00:45:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.1770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:32 - INFO - train.train_snli_ve - loss is tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3997/6700 [1:51:25<1:15:27,  1.68s/it]11/17/2022 00:45:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:34 - INFO - train.train_snli_ve - loss is tensor(0.7616, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3998/6700 [1:51:26<1:15:25,  1.67s/it]11/17/2022 00:45:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.3255e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:35 - INFO - train.train_snli_ve - loss is tensor(1.0071, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 3999/6700 [1:51:28<1:15:00,  1.67s/it]11/17/2022 00:45:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:37 - INFO - train.train_snli_ve - loss is tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4000/6700 [1:51:30<1:15:40,  1.68s/it]11/17/2022 00:45:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4027e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:39 - INFO - train.train_snli_ve - loss is tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4001/6700 [1:51:32<1:15:55,  1.69s/it]11/17/2022 00:45:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.2060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:41 - INFO - train.train_snli_ve - loss is tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4002/6700 [1:51:33<1:16:17,  1.70s/it]11/17/2022 00:45:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.8990e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:42 - INFO - train.train_snli_ve - loss is tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4003/6700 [1:51:35<1:16:27,  1.70s/it]11/17/2022 00:45:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.4366e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:44 - INFO - train.train_snli_ve - loss is tensor(0.4554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4004/6700 [1:51:37<1:16:53,  1.71s/it]11/17/2022 00:45:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:46 - INFO - train.train_snli_ve - loss is tensor(0.8846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4005/6700 [1:51:38<1:16:49,  1.71s/it]11/17/2022 00:45:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.9191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:47 - INFO - train.train_snli_ve - loss is tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4006/6700 [1:51:40<1:16:28,  1.70s/it]11/17/2022 00:45:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:49 - INFO - train.train_snli_ve - loss is tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4007/6700 [1:51:42<1:16:10,  1.70s/it]11/17/2022 00:45:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:51 - INFO - train.train_snli_ve - loss is tensor(0.8796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4008/6700 [1:51:44<1:16:16,  1.70s/it]11/17/2022 00:45:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:53 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4009/6700 [1:51:45<1:16:36,  1.71s/it]11/17/2022 00:45:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:54 - INFO - train.train_snli_ve - loss is tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4010/6700 [1:51:47<1:16:26,  1.70s/it]11/17/2022 00:45:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.8035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:56 - INFO - train.train_snli_ve - loss is tensor(0.9772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4011/6700 [1:51:49<1:15:54,  1.69s/it]11/17/2022 00:45:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.4292e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:58 - INFO - train.train_snli_ve - loss is tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4012/6700 [1:51:50<1:15:49,  1.69s/it]11/17/2022 00:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.4538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:45:59 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4013/6700 [1:51:52<1:15:15,  1.68s/it]11/17/2022 00:46:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.9418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:01 - INFO - train.train_snli_ve - loss is tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4014/6700 [1:51:54<1:14:43,  1.67s/it]11/17/2022 00:46:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.5310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:03 - INFO - train.train_snli_ve - loss is tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4015/6700 [1:51:55<1:14:47,  1.67s/it]11/17/2022 00:46:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:04 - INFO - train.train_snli_ve - loss is tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4016/6700 [1:51:57<1:14:26,  1.66s/it]11/17/2022 00:46:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:06 - INFO - train.train_snli_ve - loss is tensor(0.8647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4017/6700 [1:51:59<1:14:20,  1.66s/it]11/17/2022 00:46:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.0433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:08 - INFO - train.train_snli_ve - loss is tensor(0.4823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4018/6700 [1:52:00<1:15:01,  1.68s/it]11/17/2022 00:46:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1263e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:09 - INFO - train.train_snli_ve - loss is tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|#####9    | 4019/6700 [1:52:02<1:14:43,  1.67s/it]11/17/2022 00:46:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.6821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:11 - INFO - train.train_snli_ve - loss is tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4020/6700 [1:52:04<1:14:51,  1.68s/it]11/17/2022 00:46:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.5715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:13 - INFO - train.train_snli_ve - loss is tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4021/6700 [1:52:05<1:14:27,  1.67s/it]11/17/2022 00:46:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.0490e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:14 - INFO - train.train_snli_ve - loss is tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4022/6700 [1:52:07<1:14:39,  1.67s/it]11/17/2022 00:46:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.4149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:16 - INFO - train.train_snli_ve - loss is tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4023/6700 [1:52:09<1:15:21,  1.69s/it]11/17/2022 00:46:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:18 - INFO - train.train_snli_ve - loss is tensor(0.8443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4024/6700 [1:52:10<1:15:17,  1.69s/it]11/17/2022 00:46:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.1776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:19 - INFO - train.train_snli_ve - loss is tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4025/6700 [1:52:12<1:15:53,  1.70s/it]11/17/2022 00:46:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:21 - INFO - train.train_snli_ve - loss is tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4026/6700 [1:52:14<1:15:33,  1.70s/it]11/17/2022 00:46:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.3031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:23 - INFO - train.train_snli_ve - loss is tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4027/6700 [1:52:16<1:16:24,  1.71s/it]11/17/2022 00:46:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.2098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:25 - INFO - train.train_snli_ve - loss is tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4028/6700 [1:52:17<1:16:34,  1.72s/it]11/17/2022 00:46:26 - INFO - train.train_snli_ve - kd_loss is tensor(8.8739e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:26 - INFO - train.train_snli_ve - loss is tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4029/6700 [1:52:19<1:16:06,  1.71s/it]11/17/2022 00:46:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:28 - INFO - train.train_snli_ve - loss is tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4030/6700 [1:52:21<1:15:33,  1.70s/it]11/17/2022 00:46:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:30 - INFO - train.train_snli_ve - loss is tensor(0.5647, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4031/6700 [1:52:22<1:15:44,  1.70s/it]11/17/2022 00:46:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.3795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:31 - INFO - train.train_snli_ve - loss is tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4032/6700 [1:52:24<1:15:13,  1.69s/it]11/17/2022 00:46:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:33 - INFO - train.train_snli_ve - loss is tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4033/6700 [1:52:26<1:15:14,  1.69s/it]11/17/2022 00:46:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:35 - INFO - train.train_snli_ve - loss is tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4034/6700 [1:52:27<1:14:57,  1.69s/it]11/17/2022 00:46:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.1395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:36 - INFO - train.train_snli_ve - loss is tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4035/6700 [1:52:29<1:14:40,  1.68s/it]11/17/2022 00:46:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:38 - INFO - train.train_snli_ve - loss is tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4036/6700 [1:52:31<1:14:27,  1.68s/it]11/17/2022 00:46:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.7262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:40 - INFO - train.train_snli_ve - loss is tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4037/6700 [1:52:32<1:14:26,  1.68s/it]11/17/2022 00:46:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.3293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:41 - INFO - train.train_snli_ve - loss is tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4038/6700 [1:52:34<1:14:25,  1.68s/it]11/17/2022 00:46:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:43 - INFO - train.train_snli_ve - loss is tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4039/6700 [1:52:36<1:14:19,  1.68s/it]11/17/2022 00:46:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.4845e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:45 - INFO - train.train_snli_ve - loss is tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4040/6700 [1:52:37<1:14:02,  1.67s/it]11/17/2022 00:46:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.5133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:46 - INFO - train.train_snli_ve - loss is tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4041/6700 [1:52:39<1:14:14,  1.68s/it]11/17/2022 00:46:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.5439e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:48 - INFO - train.train_snli_ve - loss is tensor(0.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4042/6700 [1:52:41<1:14:37,  1.68s/it]11/17/2022 00:46:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:50 - INFO - train.train_snli_ve - loss is tensor(0.8451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4043/6700 [1:52:42<1:14:34,  1.68s/it]11/17/2022 00:46:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:51 - INFO - train.train_snli_ve - loss is tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4044/6700 [1:52:44<1:14:31,  1.68s/it]11/17/2022 00:46:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.1822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:53 - INFO - train.train_snli_ve - loss is tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4045/6700 [1:52:46<1:15:38,  1.71s/it]11/17/2022 00:46:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.6046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:55 - INFO - train.train_snli_ve - loss is tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4046/6700 [1:52:48<1:15:00,  1.70s/it]11/17/2022 00:46:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1773e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:57 - INFO - train.train_snli_ve - loss is tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4047/6700 [1:52:49<1:14:58,  1.70s/it]11/17/2022 00:46:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.9736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:46:58 - INFO - train.train_snli_ve - loss is tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4048/6700 [1:52:51<1:15:12,  1.70s/it]11/17/2022 00:47:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:00 - INFO - train.train_snli_ve - loss is tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4049/6700 [1:52:53<1:14:38,  1.69s/it]11/17/2022 00:47:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.6280e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:02 - INFO - train.train_snli_ve - loss is tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4050/6700 [1:52:54<1:14:20,  1.68s/it]11/17/2022 00:47:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:03 - INFO - train.train_snli_ve - loss is tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4051/6700 [1:52:56<1:14:52,  1.70s/it]11/17/2022 00:47:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:05 - INFO - train.train_snli_ve - loss is tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4052/6700 [1:52:58<1:14:55,  1.70s/it]11/17/2022 00:47:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:07 - INFO - train.train_snli_ve - loss is tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  60%|######    | 4053/6700 [1:52:59<1:14:49,  1.70s/it]11/17/2022 00:47:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:08 - INFO - train.train_snli_ve - loss is tensor(0.8540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4054/6700 [1:53:01<1:14:43,  1.69s/it]11/17/2022 00:47:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:10 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4055/6700 [1:53:03<1:14:39,  1.69s/it]11/17/2022 00:47:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4064e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:12 - INFO - train.train_snli_ve - loss is tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4056/6700 [1:53:05<1:14:38,  1.69s/it]11/17/2022 00:47:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.5666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:13 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4057/6700 [1:53:06<1:14:12,  1.68s/it]11/17/2022 00:47:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:15 - INFO - train.train_snli_ve - loss is tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4058/6700 [1:53:08<1:14:35,  1.69s/it]11/17/2022 00:47:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.3891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:17 - INFO - train.train_snli_ve - loss is tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4059/6700 [1:53:10<1:15:05,  1.71s/it]11/17/2022 00:47:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:19 - INFO - train.train_snli_ve - loss is tensor(0.8256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4060/6700 [1:53:11<1:15:13,  1.71s/it]11/17/2022 00:47:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.8285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:20 - INFO - train.train_snli_ve - loss is tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4061/6700 [1:53:13<1:15:23,  1.71s/it]11/17/2022 00:47:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6115e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:22 - INFO - train.train_snli_ve - loss is tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4062/6700 [1:53:15<1:15:17,  1.71s/it]11/17/2022 00:47:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:24 - INFO - train.train_snli_ve - loss is tensor(0.8497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4063/6700 [1:53:16<1:15:12,  1.71s/it]11/17/2022 00:47:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:25 - INFO - train.train_snli_ve - loss is tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4064/6700 [1:53:18<1:14:49,  1.70s/it]11/17/2022 00:47:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:27 - INFO - train.train_snli_ve - loss is tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4065/6700 [1:53:20<1:15:45,  1.72s/it]11/17/2022 00:47:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:29 - INFO - train.train_snli_ve - loss is tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4066/6700 [1:53:22<1:15:20,  1.72s/it]11/17/2022 00:47:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.3256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:31 - INFO - train.train_snli_ve - loss is tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4067/6700 [1:53:23<1:15:32,  1.72s/it]11/17/2022 00:47:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:32 - INFO - train.train_snli_ve - loss is tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4068/6700 [1:53:25<1:15:20,  1.72s/it]11/17/2022 00:47:34 - INFO - train.train_snli_ve - kd_loss is tensor(8.5759e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:34 - INFO - train.train_snli_ve - loss is tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4069/6700 [1:53:27<1:15:21,  1.72s/it]11/17/2022 00:47:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.2798e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:36 - INFO - train.train_snli_ve - loss is tensor(0.8223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4070/6700 [1:53:28<1:14:27,  1.70s/it]11/17/2022 00:47:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:37 - INFO - train.train_snli_ve - loss is tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4071/6700 [1:53:30<1:14:25,  1.70s/it]11/17/2022 00:47:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.0955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:39 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4072/6700 [1:53:32<1:13:45,  1.68s/it]11/17/2022 00:47:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5052e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:41 - INFO - train.train_snli_ve - loss is tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4073/6700 [1:53:33<1:13:33,  1.68s/it]11/17/2022 00:47:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:42 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4074/6700 [1:53:35<1:13:26,  1.68s/it]11/17/2022 00:47:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.6721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:44 - INFO - train.train_snli_ve - loss is tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4075/6700 [1:53:37<1:13:24,  1.68s/it]11/17/2022 00:47:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:46 - INFO - train.train_snli_ve - loss is tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4076/6700 [1:53:38<1:13:06,  1.67s/it]11/17/2022 00:47:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.3130e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:48 - INFO - train.train_snli_ve - loss is tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4077/6700 [1:53:40<1:13:26,  1.68s/it]11/17/2022 00:47:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.0853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:49 - INFO - train.train_snli_ve - loss is tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4078/6700 [1:53:42<1:12:49,  1.67s/it]11/17/2022 00:47:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:51 - INFO - train.train_snli_ve - loss is tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4079/6700 [1:53:44<1:13:04,  1.67s/it]11/17/2022 00:47:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:53 - INFO - train.train_snli_ve - loss is tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4080/6700 [1:53:45<1:13:09,  1.68s/it]11/17/2022 00:47:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:54 - INFO - train.train_snli_ve - loss is tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4081/6700 [1:53:47<1:13:14,  1.68s/it]11/17/2022 00:47:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:56 - INFO - train.train_snli_ve - loss is tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4082/6700 [1:53:49<1:13:34,  1.69s/it]11/17/2022 00:47:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:58 - INFO - train.train_snli_ve - loss is tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4083/6700 [1:53:50<1:14:03,  1.70s/it]11/17/2022 00:47:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:47:59 - INFO - train.train_snli_ve - loss is tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4084/6700 [1:53:52<1:14:26,  1.71s/it]11/17/2022 00:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:01 - INFO - train.train_snli_ve - loss is tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4085/6700 [1:53:54<1:14:12,  1.70s/it]11/17/2022 00:48:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.1153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:03 - INFO - train.train_snli_ve - loss is tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######    | 4086/6700 [1:53:55<1:14:18,  1.71s/it]11/17/2022 00:48:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7587e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:04 - INFO - train.train_snli_ve - loss is tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4087/6700 [1:53:57<1:14:39,  1.71s/it]11/17/2022 00:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2798e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:06 - INFO - train.train_snli_ve - loss is tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4088/6700 [1:53:59<1:13:56,  1.70s/it]11/17/2022 00:48:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:08 - INFO - train.train_snli_ve - loss is tensor(0.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4089/6700 [1:54:01<1:13:49,  1.70s/it]11/17/2022 00:48:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:10 - INFO - train.train_snli_ve - loss is tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4090/6700 [1:54:02<1:14:19,  1.71s/it]11/17/2022 00:48:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.6805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:11 - INFO - train.train_snli_ve - loss is tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4091/6700 [1:54:04<1:13:28,  1.69s/it]11/17/2022 00:48:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.5546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:13 - INFO - train.train_snli_ve - loss is tensor(0.9399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4092/6700 [1:54:06<1:13:34,  1.69s/it]11/17/2022 00:48:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3522e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:15 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4093/6700 [1:54:07<1:13:24,  1.69s/it]11/17/2022 00:48:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.4852e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:16 - INFO - train.train_snli_ve - loss is tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4094/6700 [1:54:09<1:13:04,  1.68s/it]11/17/2022 00:48:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:18 - INFO - train.train_snli_ve - loss is tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4095/6700 [1:54:11<1:12:58,  1.68s/it]11/17/2022 00:48:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.3935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:20 - INFO - train.train_snli_ve - loss is tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4096/6700 [1:54:12<1:13:17,  1.69s/it]11/17/2022 00:48:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:21 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4097/6700 [1:54:14<1:13:01,  1.68s/it]11/17/2022 00:48:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2485e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:23 - INFO - train.train_snli_ve - loss is tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4098/6700 [1:54:16<1:13:02,  1.68s/it]11/17/2022 00:48:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7507e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:25 - INFO - train.train_snli_ve - loss is tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4099/6700 [1:54:17<1:12:35,  1.67s/it]11/17/2022 00:48:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.1059e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:26 - INFO - train.train_snli_ve - loss is tensor(0.7741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4100/6700 [1:54:19<1:13:09,  1.69s/it]11/17/2022 00:48:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:28 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4101/6700 [1:54:21<1:12:52,  1.68s/it]11/17/2022 00:48:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:30 - INFO - train.train_snli_ve - loss is tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4102/6700 [1:54:22<1:13:06,  1.69s/it]11/17/2022 00:48:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.6055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:31 - INFO - train.train_snli_ve - loss is tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4103/6700 [1:54:24<1:12:37,  1.68s/it]11/17/2022 00:48:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.1151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:33 - INFO - train.train_snli_ve - loss is tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4104/6700 [1:54:26<1:12:45,  1.68s/it]11/17/2022 00:48:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:35 - INFO - train.train_snli_ve - loss is tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4105/6700 [1:54:28<1:13:06,  1.69s/it]11/17/2022 00:48:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0755e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:36 - INFO - train.train_snli_ve - loss is tensor(0.5433, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4106/6700 [1:54:29<1:12:39,  1.68s/it]11/17/2022 00:48:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:38 - INFO - train.train_snli_ve - loss is tensor(0.4501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4107/6700 [1:54:31<1:13:01,  1.69s/it]11/17/2022 00:48:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.5630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:40 - INFO - train.train_snli_ve - loss is tensor(0.8843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4108/6700 [1:54:33<1:13:19,  1.70s/it]11/17/2022 00:48:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3512e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:42 - INFO - train.train_snli_ve - loss is tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4109/6700 [1:54:34<1:13:26,  1.70s/it]11/17/2022 00:48:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:43 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4110/6700 [1:54:36<1:13:15,  1.70s/it]11/17/2022 00:48:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.1047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:45 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4111/6700 [1:54:38<1:13:11,  1.70s/it]11/17/2022 00:48:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2866e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:47 - INFO - train.train_snli_ve - loss is tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4112/6700 [1:54:39<1:12:27,  1.68s/it]11/17/2022 00:48:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:48 - INFO - train.train_snli_ve - loss is tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4113/6700 [1:54:41<1:13:00,  1.69s/it]11/17/2022 00:48:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:50 - INFO - train.train_snli_ve - loss is tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4114/6700 [1:54:43<1:12:39,  1.69s/it]11/17/2022 00:48:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5374e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:52 - INFO - train.train_snli_ve - loss is tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4115/6700 [1:54:44<1:12:25,  1.68s/it]11/17/2022 00:48:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.2069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:53 - INFO - train.train_snli_ve - loss is tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4116/6700 [1:54:46<1:12:43,  1.69s/it]11/17/2022 00:48:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:55 - INFO - train.train_snli_ve - loss is tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4117/6700 [1:54:48<1:12:56,  1.69s/it]11/17/2022 00:48:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0741e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:57 - INFO - train.train_snli_ve - loss is tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4118/6700 [1:54:49<1:12:33,  1.69s/it]11/17/2022 00:48:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.9044e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:48:58 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4119/6700 [1:54:51<1:12:53,  1.69s/it]11/17/2022 00:49:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.7249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:00 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  61%|######1   | 4120/6700 [1:54:53<1:13:02,  1.70s/it]11/17/2022 00:49:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.5895e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:02 - INFO - train.train_snli_ve - loss is tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4121/6700 [1:54:55<1:12:54,  1.70s/it]11/17/2022 00:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:04 - INFO - train.train_snli_ve - loss is tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4122/6700 [1:54:56<1:13:00,  1.70s/it]11/17/2022 00:49:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.1977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:05 - INFO - train.train_snli_ve - loss is tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4123/6700 [1:54:58<1:13:11,  1.70s/it]11/17/2022 00:49:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:07 - INFO - train.train_snli_ve - loss is tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4124/6700 [1:55:00<1:12:39,  1.69s/it]11/17/2022 00:49:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:09 - INFO - train.train_snli_ve - loss is tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4125/6700 [1:55:01<1:12:49,  1.70s/it]11/17/2022 00:49:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:10 - INFO - train.train_snli_ve - loss is tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4126/6700 [1:55:03<1:12:41,  1.69s/it]11/17/2022 00:49:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.7920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:12 - INFO - train.train_snli_ve - loss is tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4127/6700 [1:55:05<1:12:02,  1.68s/it]11/17/2022 00:49:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:14 - INFO - train.train_snli_ve - loss is tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4128/6700 [1:55:06<1:13:27,  1.71s/it]11/17/2022 00:49:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6041e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:16 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4129/6700 [1:55:08<1:13:14,  1.71s/it]11/17/2022 00:49:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:17 - INFO - train.train_snli_ve - loss is tensor(0.8520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4130/6700 [1:55:10<1:12:56,  1.70s/it]11/17/2022 00:49:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.7182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:19 - INFO - train.train_snli_ve - loss is tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4131/6700 [1:55:12<1:12:26,  1.69s/it]11/17/2022 00:49:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.8090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:21 - INFO - train.train_snli_ve - loss is tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4132/6700 [1:55:13<1:12:35,  1.70s/it]11/17/2022 00:49:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:22 - INFO - train.train_snli_ve - loss is tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4133/6700 [1:55:15<1:13:16,  1.71s/it]11/17/2022 00:49:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.7382e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:24 - INFO - train.train_snli_ve - loss is tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4134/6700 [1:55:17<1:14:15,  1.74s/it]11/17/2022 00:49:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:26 - INFO - train.train_snli_ve - loss is tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4135/6700 [1:55:18<1:13:21,  1.72s/it]11/17/2022 00:49:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:27 - INFO - train.train_snli_ve - loss is tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4136/6700 [1:55:20<1:12:57,  1.71s/it]11/17/2022 00:49:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.3306e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:29 - INFO - train.train_snli_ve - loss is tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4137/6700 [1:55:22<1:12:24,  1.69s/it]11/17/2022 00:49:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:31 - INFO - train.train_snli_ve - loss is tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4138/6700 [1:55:24<1:12:12,  1.69s/it]11/17/2022 00:49:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.2983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:33 - INFO - train.train_snli_ve - loss is tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4139/6700 [1:55:25<1:12:22,  1.70s/it]11/17/2022 00:49:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.4589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:34 - INFO - train.train_snli_ve - loss is tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4140/6700 [1:55:27<1:12:05,  1.69s/it]11/17/2022 00:49:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.4630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:36 - INFO - train.train_snli_ve - loss is tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4141/6700 [1:55:29<1:11:43,  1.68s/it]11/17/2022 00:49:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:38 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4142/6700 [1:55:30<1:12:03,  1.69s/it]11/17/2022 00:49:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:39 - INFO - train.train_snli_ve - loss is tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4143/6700 [1:55:32<1:12:09,  1.69s/it]11/17/2022 00:49:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:41 - INFO - train.train_snli_ve - loss is tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4144/6700 [1:55:34<1:12:10,  1.69s/it]11/17/2022 00:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:43 - INFO - train.train_snli_ve - loss is tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4145/6700 [1:55:35<1:12:08,  1.69s/it]11/17/2022 00:49:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:44 - INFO - train.train_snli_ve - loss is tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4146/6700 [1:55:37<1:11:57,  1.69s/it]11/17/2022 00:49:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:46 - INFO - train.train_snli_ve - loss is tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4147/6700 [1:55:39<1:11:49,  1.69s/it]11/17/2022 00:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8746e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:48 - INFO - train.train_snli_ve - loss is tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4148/6700 [1:55:40<1:11:51,  1.69s/it]11/17/2022 00:49:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:49 - INFO - train.train_snli_ve - loss is tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4149/6700 [1:55:42<1:11:31,  1.68s/it]11/17/2022 00:49:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.5232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:51 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4150/6700 [1:55:44<1:11:23,  1.68s/it]11/17/2022 00:49:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:53 - INFO - train.train_snli_ve - loss is tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4151/6700 [1:55:45<1:11:09,  1.68s/it]11/17/2022 00:49:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:54 - INFO - train.train_snli_ve - loss is tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4152/6700 [1:55:47<1:11:52,  1.69s/it]11/17/2022 00:49:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.1908e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:56 - INFO - train.train_snli_ve - loss is tensor(0.8504, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######1   | 4153/6700 [1:55:49<1:12:06,  1.70s/it]11/17/2022 00:49:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.5507e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:49:58 - INFO - train.train_snli_ve - loss is tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4154/6700 [1:55:51<1:12:20,  1.70s/it]11/17/2022 00:50:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.3928e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:00 - INFO - train.train_snli_ve - loss is tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4155/6700 [1:55:52<1:12:03,  1.70s/it]11/17/2022 00:50:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4345e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:01 - INFO - train.train_snli_ve - loss is tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4156/6700 [1:55:54<1:12:11,  1.70s/it]11/17/2022 00:50:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.0825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:03 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4157/6700 [1:55:56<1:11:53,  1.70s/it]11/17/2022 00:50:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.3120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:05 - INFO - train.train_snli_ve - loss is tensor(0.4599, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4158/6700 [1:55:57<1:11:13,  1.68s/it]11/17/2022 00:50:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.6947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:06 - INFO - train.train_snli_ve - loss is tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4159/6700 [1:55:59<1:10:41,  1.67s/it]11/17/2022 00:50:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5967e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:08 - INFO - train.train_snli_ve - loss is tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4160/6700 [1:56:01<1:10:58,  1.68s/it]11/17/2022 00:50:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.6002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:10 - INFO - train.train_snli_ve - loss is tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4161/6700 [1:56:02<1:10:34,  1.67s/it]11/17/2022 00:50:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.4301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:11 - INFO - train.train_snli_ve - loss is tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4162/6700 [1:56:04<1:11:00,  1.68s/it]11/17/2022 00:50:13 - INFO - train.train_snli_ve - kd_loss is tensor(4.0188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:13 - INFO - train.train_snli_ve - loss is tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4163/6700 [1:56:06<1:11:19,  1.69s/it]11/17/2022 00:50:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.8206e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:15 - INFO - train.train_snli_ve - loss is tensor(1.0212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4164/6700 [1:56:07<1:11:36,  1.69s/it]11/17/2022 00:50:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:16 - INFO - train.train_snli_ve - loss is tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4165/6700 [1:56:09<1:11:59,  1.70s/it]11/17/2022 00:50:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:18 - INFO - train.train_snli_ve - loss is tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4166/6700 [1:56:11<1:12:01,  1.71s/it]11/17/2022 00:50:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.4296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:20 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4167/6700 [1:56:12<1:11:24,  1.69s/it]11/17/2022 00:50:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.9418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:21 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4168/6700 [1:56:14<1:10:57,  1.68s/it]11/17/2022 00:50:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:23 - INFO - train.train_snli_ve - loss is tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4169/6700 [1:56:16<1:10:39,  1.68s/it]11/17/2022 00:50:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.4751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:25 - INFO - train.train_snli_ve - loss is tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4170/6700 [1:56:18<1:10:56,  1.68s/it]11/17/2022 00:50:27 - INFO - train.train_snli_ve - kd_loss is tensor(6.6268e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:27 - INFO - train.train_snli_ve - loss is tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4171/6700 [1:56:19<1:11:17,  1.69s/it]11/17/2022 00:50:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.6640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:28 - INFO - train.train_snli_ve - loss is tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4172/6700 [1:56:21<1:11:03,  1.69s/it]11/17/2022 00:50:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.2431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:30 - INFO - train.train_snli_ve - loss is tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4173/6700 [1:56:23<1:11:25,  1.70s/it]11/17/2022 00:50:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.3355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:32 - INFO - train.train_snli_ve - loss is tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4174/6700 [1:56:24<1:11:21,  1.70s/it]11/17/2022 00:50:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9372e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:33 - INFO - train.train_snli_ve - loss is tensor(0.8090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4175/6700 [1:56:26<1:11:10,  1.69s/it]11/17/2022 00:50:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.7087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:35 - INFO - train.train_snli_ve - loss is tensor(0.9802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4176/6700 [1:56:28<1:11:13,  1.69s/it]11/17/2022 00:50:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.9610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:37 - INFO - train.train_snli_ve - loss is tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4177/6700 [1:56:29<1:11:32,  1.70s/it]11/17/2022 00:50:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3685e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:38 - INFO - train.train_snli_ve - loss is tensor(0.4376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4178/6700 [1:56:31<1:11:09,  1.69s/it]11/17/2022 00:50:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:40 - INFO - train.train_snli_ve - loss is tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4179/6700 [1:56:33<1:10:34,  1.68s/it]11/17/2022 00:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.2802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:42 - INFO - train.train_snli_ve - loss is tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4180/6700 [1:56:34<1:11:03,  1.69s/it]11/17/2022 00:50:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:43 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4181/6700 [1:56:36<1:10:46,  1.69s/it]11/17/2022 00:50:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:45 - INFO - train.train_snli_ve - loss is tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4182/6700 [1:56:38<1:10:37,  1.68s/it]11/17/2022 00:50:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.6151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:47 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4183/6700 [1:56:40<1:10:50,  1.69s/it]11/17/2022 00:50:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:48 - INFO - train.train_snli_ve - loss is tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4184/6700 [1:56:41<1:10:30,  1.68s/it]11/17/2022 00:50:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:50 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4185/6700 [1:56:43<1:10:30,  1.68s/it]11/17/2022 00:50:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:52 - INFO - train.train_snli_ve - loss is tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4186/6700 [1:56:45<1:10:18,  1.68s/it]11/17/2022 00:50:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.2600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:54 - INFO - train.train_snli_ve - loss is tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  62%|######2   | 4187/6700 [1:56:46<1:10:47,  1.69s/it]11/17/2022 00:50:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:55 - INFO - train.train_snli_ve - loss is tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4188/6700 [1:56:48<1:10:36,  1.69s/it]11/17/2022 00:50:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.5373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:57 - INFO - train.train_snli_ve - loss is tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4189/6700 [1:56:50<1:10:26,  1.68s/it]11/17/2022 00:50:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:50:59 - INFO - train.train_snli_ve - loss is tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4190/6700 [1:56:51<1:10:10,  1.68s/it]11/17/2022 00:51:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.7411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:00 - INFO - train.train_snli_ve - loss is tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4191/6700 [1:56:53<1:10:02,  1.68s/it]11/17/2022 00:51:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.5480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:02 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4192/6700 [1:56:55<1:10:20,  1.68s/it]11/17/2022 00:51:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.2228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:04 - INFO - train.train_snli_ve - loss is tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4193/6700 [1:56:56<1:09:38,  1.67s/it]11/17/2022 00:51:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:05 - INFO - train.train_snli_ve - loss is tensor(0.9462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4194/6700 [1:56:58<1:09:50,  1.67s/it]11/17/2022 00:51:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.6989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:07 - INFO - train.train_snli_ve - loss is tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4195/6700 [1:57:00<1:10:11,  1.68s/it]11/17/2022 00:51:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:09 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4196/6700 [1:57:01<1:10:02,  1.68s/it]11/17/2022 00:51:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.8941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:10 - INFO - train.train_snli_ve - loss is tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4197/6700 [1:57:03<1:10:18,  1.69s/it]11/17/2022 00:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.2328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:12 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4198/6700 [1:57:05<1:10:35,  1.69s/it]11/17/2022 00:51:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:14 - INFO - train.train_snli_ve - loss is tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4199/6700 [1:57:06<1:10:20,  1.69s/it]11/17/2022 00:51:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:15 - INFO - train.train_snli_ve - loss is tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4200/6700 [1:57:08<1:10:47,  1.70s/it]11/17/2022 00:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.5887e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:17 - INFO - train.train_snli_ve - loss is tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4201/6700 [1:57:10<1:10:40,  1.70s/it]11/17/2022 00:51:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:19 - INFO - train.train_snli_ve - loss is tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4202/6700 [1:57:12<1:10:53,  1.70s/it]11/17/2022 00:51:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:21 - INFO - train.train_snli_ve - loss is tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4203/6700 [1:57:13<1:10:10,  1.69s/it]11/17/2022 00:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.5920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:22 - INFO - train.train_snli_ve - loss is tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4204/6700 [1:57:15<1:10:08,  1.69s/it]11/17/2022 00:51:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.2350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:24 - INFO - train.train_snli_ve - loss is tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4205/6700 [1:57:17<1:10:10,  1.69s/it]11/17/2022 00:51:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:26 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4206/6700 [1:57:18<1:10:00,  1.68s/it]11/17/2022 00:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.5658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:27 - INFO - train.train_snli_ve - loss is tensor(0.5650, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4207/6700 [1:57:20<1:09:44,  1.68s/it]11/17/2022 00:51:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.5786e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:29 - INFO - train.train_snli_ve - loss is tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4208/6700 [1:57:22<1:09:49,  1.68s/it]11/17/2022 00:51:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.2556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:31 - INFO - train.train_snli_ve - loss is tensor(0.8144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4209/6700 [1:57:23<1:09:35,  1.68s/it]11/17/2022 00:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:32 - INFO - train.train_snli_ve - loss is tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4210/6700 [1:57:25<1:09:18,  1.67s/it]11/17/2022 00:51:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.6835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:34 - INFO - train.train_snli_ve - loss is tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4211/6700 [1:57:27<1:09:37,  1.68s/it]11/17/2022 00:51:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.0458e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:36 - INFO - train.train_snli_ve - loss is tensor(0.7910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4212/6700 [1:57:28<1:09:21,  1.67s/it]11/17/2022 00:51:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:37 - INFO - train.train_snli_ve - loss is tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4213/6700 [1:57:30<1:09:14,  1.67s/it]11/17/2022 00:51:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:39 - INFO - train.train_snli_ve - loss is tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4214/6700 [1:57:32<1:10:06,  1.69s/it]11/17/2022 00:51:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.3211e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:41 - INFO - train.train_snli_ve - loss is tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4215/6700 [1:57:33<1:09:54,  1.69s/it]11/17/2022 00:51:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.9307e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:42 - INFO - train.train_snli_ve - loss is tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4216/6700 [1:57:35<1:08:55,  1.67s/it]11/17/2022 00:51:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:44 - INFO - train.train_snli_ve - loss is tensor(0.8949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4217/6700 [1:57:37<1:08:23,  1.65s/it]11/17/2022 00:51:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:46 - INFO - train.train_snli_ve - loss is tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4218/6700 [1:57:38<1:09:07,  1.67s/it]11/17/2022 00:51:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:47 - INFO - train.train_snli_ve - loss is tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4219/6700 [1:57:40<1:08:54,  1.67s/it]11/17/2022 00:51:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.5790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:49 - INFO - train.train_snli_ve - loss is tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######2   | 4220/6700 [1:57:42<1:09:14,  1.68s/it]11/17/2022 00:51:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.8743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:51 - INFO - train.train_snli_ve - loss is tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4221/6700 [1:57:43<1:09:08,  1.67s/it]11/17/2022 00:51:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:52 - INFO - train.train_snli_ve - loss is tensor(0.9300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4222/6700 [1:57:45<1:09:28,  1.68s/it]11/17/2022 00:51:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.6049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:54 - INFO - train.train_snli_ve - loss is tensor(0.9133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4223/6700 [1:57:47<1:09:12,  1.68s/it]11/17/2022 00:51:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:56 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4224/6700 [1:57:48<1:08:57,  1.67s/it]11/17/2022 00:51:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6251e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:57 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4225/6700 [1:57:50<1:09:12,  1.68s/it]11/17/2022 00:51:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.1001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:51:59 - INFO - train.train_snli_ve - loss is tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4226/6700 [1:57:52<1:09:19,  1.68s/it]11/17/2022 00:52:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.0387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:01 - INFO - train.train_snli_ve - loss is tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4227/6700 [1:57:53<1:09:12,  1.68s/it]11/17/2022 00:52:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:02 - INFO - train.train_snli_ve - loss is tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4228/6700 [1:57:55<1:09:29,  1.69s/it]11/17/2022 00:52:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:04 - INFO - train.train_snli_ve - loss is tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4229/6700 [1:57:57<1:09:42,  1.69s/it]11/17/2022 00:52:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.4431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:06 - INFO - train.train_snli_ve - loss is tensor(0.8404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4230/6700 [1:57:58<1:09:25,  1.69s/it]11/17/2022 00:52:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:08 - INFO - train.train_snli_ve - loss is tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4231/6700 [1:58:00<1:10:13,  1.71s/it]11/17/2022 00:52:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.7962e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:09 - INFO - train.train_snli_ve - loss is tensor(0.7493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4232/6700 [1:58:02<1:10:04,  1.70s/it]11/17/2022 00:52:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:11 - INFO - train.train_snli_ve - loss is tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4233/6700 [1:58:04<1:09:54,  1.70s/it]11/17/2022 00:52:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:13 - INFO - train.train_snli_ve - loss is tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4234/6700 [1:58:05<1:09:17,  1.69s/it]11/17/2022 00:52:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:14 - INFO - train.train_snli_ve - loss is tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4235/6700 [1:58:07<1:09:03,  1.68s/it]11/17/2022 00:52:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:16 - INFO - train.train_snli_ve - loss is tensor(0.5767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4236/6700 [1:58:09<1:09:15,  1.69s/it]11/17/2022 00:52:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.3632e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:18 - INFO - train.train_snli_ve - loss is tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4237/6700 [1:58:10<1:09:18,  1.69s/it]11/17/2022 00:52:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:19 - INFO - train.train_snli_ve - loss is tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4238/6700 [1:58:12<1:09:25,  1.69s/it]11/17/2022 00:52:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:21 - INFO - train.train_snli_ve - loss is tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4239/6700 [1:58:14<1:09:04,  1.68s/it]11/17/2022 00:52:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:23 - INFO - train.train_snli_ve - loss is tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4240/6700 [1:58:15<1:09:16,  1.69s/it]11/17/2022 00:52:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.2686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:24 - INFO - train.train_snli_ve - loss is tensor(0.7347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4241/6700 [1:58:17<1:08:52,  1.68s/it]11/17/2022 00:52:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.9465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:26 - INFO - train.train_snli_ve - loss is tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4242/6700 [1:58:19<1:08:22,  1.67s/it]11/17/2022 00:52:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:28 - INFO - train.train_snli_ve - loss is tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4243/6700 [1:58:20<1:07:59,  1.66s/it]11/17/2022 00:52:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.4227e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:29 - INFO - train.train_snli_ve - loss is tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4244/6700 [1:58:22<1:08:27,  1.67s/it]11/17/2022 00:52:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:31 - INFO - train.train_snli_ve - loss is tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4245/6700 [1:58:24<1:08:27,  1.67s/it]11/17/2022 00:52:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:33 - INFO - train.train_snli_ve - loss is tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4246/6700 [1:58:25<1:08:48,  1.68s/it]11/17/2022 00:52:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1312e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:34 - INFO - train.train_snli_ve - loss is tensor(0.7875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4247/6700 [1:58:27<1:08:22,  1.67s/it]11/17/2022 00:52:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:36 - INFO - train.train_snli_ve - loss is tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4248/6700 [1:58:29<1:08:44,  1.68s/it]11/17/2022 00:52:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:38 - INFO - train.train_snli_ve - loss is tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4249/6700 [1:58:30<1:08:45,  1.68s/it]11/17/2022 00:52:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.9520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:39 - INFO - train.train_snli_ve - loss is tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4250/6700 [1:58:32<1:08:54,  1.69s/it]11/17/2022 00:52:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6831e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:41 - INFO - train.train_snli_ve - loss is tensor(0.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4251/6700 [1:58:34<1:08:41,  1.68s/it]11/17/2022 00:52:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.4536e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:43 - INFO - train.train_snli_ve - loss is tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4252/6700 [1:58:35<1:08:06,  1.67s/it]11/17/2022 00:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6163e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:45 - INFO - train.train_snli_ve - loss is tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4253/6700 [1:58:37<1:09:10,  1.70s/it]11/17/2022 00:52:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.7510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:46 - INFO - train.train_snli_ve - loss is tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  63%|######3   | 4254/6700 [1:58:39<1:09:07,  1.70s/it]11/17/2022 00:52:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:48 - INFO - train.train_snli_ve - loss is tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4255/6700 [1:58:41<1:09:00,  1.69s/it]11/17/2022 00:52:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.2682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:50 - INFO - train.train_snli_ve - loss is tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4256/6700 [1:58:42<1:08:46,  1.69s/it]11/17/2022 00:52:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:51 - INFO - train.train_snli_ve - loss is tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4257/6700 [1:58:44<1:09:10,  1.70s/it]11/17/2022 00:52:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.6656e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:53 - INFO - train.train_snli_ve - loss is tensor(0.8523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4258/6700 [1:58:46<1:09:15,  1.70s/it]11/17/2022 00:52:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.6889e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:55 - INFO - train.train_snli_ve - loss is tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4259/6700 [1:58:47<1:08:36,  1.69s/it]11/17/2022 00:52:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.8197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:56 - INFO - train.train_snli_ve - loss is tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4260/6700 [1:58:49<1:08:31,  1.68s/it]11/17/2022 00:52:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.9872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:52:58 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4261/6700 [1:58:51<1:08:13,  1.68s/it]11/17/2022 00:53:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.4700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:00 - INFO - train.train_snli_ve - loss is tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4262/6700 [1:58:52<1:08:19,  1.68s/it]11/17/2022 00:53:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.5507e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:01 - INFO - train.train_snli_ve - loss is tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4263/6700 [1:58:54<1:08:53,  1.70s/it]11/17/2022 00:53:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.5934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:03 - INFO - train.train_snli_ve - loss is tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4264/6700 [1:58:56<1:08:48,  1.69s/it]11/17/2022 00:53:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6642e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:05 - INFO - train.train_snli_ve - loss is tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4265/6700 [1:58:58<1:08:45,  1.69s/it]11/17/2022 00:53:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.0350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:07 - INFO - train.train_snli_ve - loss is tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4266/6700 [1:58:59<1:08:43,  1.69s/it]11/17/2022 00:53:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.7824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:08 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4267/6700 [1:59:01<1:08:21,  1.69s/it]11/17/2022 00:53:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.8802e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:10 - INFO - train.train_snli_ve - loss is tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4268/6700 [1:59:03<1:08:03,  1.68s/it]11/17/2022 00:53:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:12 - INFO - train.train_snli_ve - loss is tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4269/6700 [1:59:04<1:08:05,  1.68s/it]11/17/2022 00:53:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:13 - INFO - train.train_snli_ve - loss is tensor(0.8883, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4270/6700 [1:59:06<1:08:03,  1.68s/it]11/17/2022 00:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.0855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:15 - INFO - train.train_snli_ve - loss is tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4271/6700 [1:59:08<1:07:52,  1.68s/it]11/17/2022 00:53:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.0681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:17 - INFO - train.train_snli_ve - loss is tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4272/6700 [1:59:09<1:07:59,  1.68s/it]11/17/2022 00:53:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.6980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:18 - INFO - train.train_snli_ve - loss is tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4273/6700 [1:59:11<1:07:38,  1.67s/it]11/17/2022 00:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7872e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:20 - INFO - train.train_snli_ve - loss is tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4274/6700 [1:59:13<1:07:26,  1.67s/it]11/17/2022 00:53:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4566e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:22 - INFO - train.train_snli_ve - loss is tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4275/6700 [1:59:14<1:08:09,  1.69s/it]11/17/2022 00:53:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.1413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:23 - INFO - train.train_snli_ve - loss is tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4276/6700 [1:59:16<1:08:07,  1.69s/it]11/17/2022 00:53:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.6074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:25 - INFO - train.train_snli_ve - loss is tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4277/6700 [1:59:18<1:07:46,  1.68s/it]11/17/2022 00:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.7029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:27 - INFO - train.train_snli_ve - loss is tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4278/6700 [1:59:19<1:08:00,  1.68s/it]11/17/2022 00:53:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:28 - INFO - train.train_snli_ve - loss is tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4279/6700 [1:59:21<1:07:50,  1.68s/it]11/17/2022 00:53:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.1424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:30 - INFO - train.train_snli_ve - loss is tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4280/6700 [1:59:23<1:07:36,  1.68s/it]11/17/2022 00:53:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.7038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:32 - INFO - train.train_snli_ve - loss is tensor(0.9364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4281/6700 [1:59:24<1:07:32,  1.68s/it]11/17/2022 00:53:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.2718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:33 - INFO - train.train_snli_ve - loss is tensor(0.8248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4282/6700 [1:59:26<1:07:39,  1.68s/it]11/17/2022 00:53:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.0666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:35 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4283/6700 [1:59:28<1:07:56,  1.69s/it]11/17/2022 00:53:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:37 - INFO - train.train_snli_ve - loss is tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4284/6700 [1:59:29<1:07:55,  1.69s/it]11/17/2022 00:53:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.3074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:38 - INFO - train.train_snli_ve - loss is tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4285/6700 [1:59:31<1:07:41,  1.68s/it]11/17/2022 00:53:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:40 - INFO - train.train_snli_ve - loss is tensor(0.8792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4286/6700 [1:59:33<1:07:48,  1.69s/it]11/17/2022 00:53:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:42 - INFO - train.train_snli_ve - loss is tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######3   | 4287/6700 [1:59:35<1:07:45,  1.68s/it]11/17/2022 00:53:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.5426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:43 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4288/6700 [1:59:36<1:07:24,  1.68s/it]11/17/2022 00:53:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.7589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:45 - INFO - train.train_snli_ve - loss is tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4289/6700 [1:59:38<1:07:20,  1.68s/it]11/17/2022 00:53:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.9947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:47 - INFO - train.train_snli_ve - loss is tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4290/6700 [1:59:40<1:07:50,  1.69s/it]11/17/2022 00:53:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.8889e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:49 - INFO - train.train_snli_ve - loss is tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4291/6700 [1:59:41<1:07:52,  1.69s/it]11/17/2022 00:53:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:50 - INFO - train.train_snli_ve - loss is tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4292/6700 [1:59:43<1:07:43,  1.69s/it]11/17/2022 00:53:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.4679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:52 - INFO - train.train_snli_ve - loss is tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4293/6700 [1:59:45<1:08:04,  1.70s/it]11/17/2022 00:53:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.1225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:54 - INFO - train.train_snli_ve - loss is tensor(0.8793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4294/6700 [1:59:46<1:07:46,  1.69s/it]11/17/2022 00:53:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.5481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:55 - INFO - train.train_snli_ve - loss is tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4295/6700 [1:59:48<1:07:23,  1.68s/it]11/17/2022 00:53:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:57 - INFO - train.train_snli_ve - loss is tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4296/6700 [1:59:50<1:07:22,  1.68s/it]11/17/2022 00:53:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.9058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:53:59 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4297/6700 [1:59:51<1:07:42,  1.69s/it]11/17/2022 00:54:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.7032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:00 - INFO - train.train_snli_ve - loss is tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4298/6700 [1:59:53<1:07:28,  1.69s/it]11/17/2022 00:54:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.4787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:02 - INFO - train.train_snli_ve - loss is tensor(0.8890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4299/6700 [1:59:55<1:07:17,  1.68s/it]11/17/2022 00:54:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.7448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:04 - INFO - train.train_snli_ve - loss is tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4300/6700 [1:59:56<1:06:57,  1.67s/it]11/17/2022 00:54:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:05 - INFO - train.train_snli_ve - loss is tensor(0.8582, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4301/6700 [1:59:58<1:07:34,  1.69s/it]11/17/2022 00:54:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.5330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:07 - INFO - train.train_snli_ve - loss is tensor(0.5004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4302/6700 [2:00:00<1:07:13,  1.68s/it]11/17/2022 00:54:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.5085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:09 - INFO - train.train_snli_ve - loss is tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4303/6700 [2:00:01<1:07:32,  1.69s/it]11/17/2022 00:54:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.7254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:10 - INFO - train.train_snli_ve - loss is tensor(0.9631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4304/6700 [2:00:03<1:07:37,  1.69s/it]11/17/2022 00:54:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.9798e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:12 - INFO - train.train_snli_ve - loss is tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4305/6700 [2:00:05<1:08:04,  1.71s/it]11/17/2022 00:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:14 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4306/6700 [2:00:07<1:07:29,  1.69s/it]11/17/2022 00:54:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:16 - INFO - train.train_snli_ve - loss is tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4307/6700 [2:00:08<1:07:36,  1.70s/it]11/17/2022 00:54:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.3444e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:17 - INFO - train.train_snli_ve - loss is tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4308/6700 [2:00:10<1:07:45,  1.70s/it]11/17/2022 00:54:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2577e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:19 - INFO - train.train_snli_ve - loss is tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4309/6700 [2:00:12<1:07:28,  1.69s/it]11/17/2022 00:54:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.8664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:21 - INFO - train.train_snli_ve - loss is tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4310/6700 [2:00:13<1:07:30,  1.69s/it]11/17/2022 00:54:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.5398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:22 - INFO - train.train_snli_ve - loss is tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4311/6700 [2:00:15<1:07:12,  1.69s/it]11/17/2022 00:54:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4119e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:24 - INFO - train.train_snli_ve - loss is tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4312/6700 [2:00:17<1:07:07,  1.69s/it]11/17/2022 00:54:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.6474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:26 - INFO - train.train_snli_ve - loss is tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4313/6700 [2:00:18<1:06:51,  1.68s/it]11/17/2022 00:54:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:27 - INFO - train.train_snli_ve - loss is tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4314/6700 [2:00:20<1:06:27,  1.67s/it]11/17/2022 00:54:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:29 - INFO - train.train_snli_ve - loss is tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4315/6700 [2:00:22<1:06:45,  1.68s/it]11/17/2022 00:54:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:31 - INFO - train.train_snli_ve - loss is tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4316/6700 [2:00:23<1:06:46,  1.68s/it]11/17/2022 00:54:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:32 - INFO - train.train_snli_ve - loss is tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4317/6700 [2:00:25<1:07:02,  1.69s/it]11/17/2022 00:54:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.8438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:34 - INFO - train.train_snli_ve - loss is tensor(0.9597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4318/6700 [2:00:27<1:07:22,  1.70s/it]11/17/2022 00:54:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:36 - INFO - train.train_snli_ve - loss is tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4319/6700 [2:00:29<1:07:29,  1.70s/it]11/17/2022 00:54:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.4169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:38 - INFO - train.train_snli_ve - loss is tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4320/6700 [2:00:30<1:07:29,  1.70s/it]11/17/2022 00:54:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:39 - INFO - train.train_snli_ve - loss is tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  64%|######4   | 4321/6700 [2:00:32<1:06:59,  1.69s/it]11/17/2022 00:54:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:41 - INFO - train.train_snli_ve - loss is tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4322/6700 [2:00:34<1:06:44,  1.68s/it]11/17/2022 00:54:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:43 - INFO - train.train_snli_ve - loss is tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4323/6700 [2:00:35<1:06:38,  1.68s/it]11/17/2022 00:54:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.6548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:44 - INFO - train.train_snli_ve - loss is tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4324/6700 [2:00:37<1:06:52,  1.69s/it]11/17/2022 00:54:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.9575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:46 - INFO - train.train_snli_ve - loss is tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4325/6700 [2:00:39<1:06:54,  1.69s/it]11/17/2022 00:54:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.2827e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:48 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4326/6700 [2:00:40<1:06:27,  1.68s/it]11/17/2022 00:54:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.1601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:49 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4327/6700 [2:00:42<1:06:31,  1.68s/it]11/17/2022 00:54:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4633e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:51 - INFO - train.train_snli_ve - loss is tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4328/6700 [2:00:44<1:06:18,  1.68s/it]11/17/2022 00:54:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.5871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:53 - INFO - train.train_snli_ve - loss is tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4329/6700 [2:00:45<1:06:22,  1.68s/it]11/17/2022 00:54:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.9301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:54 - INFO - train.train_snli_ve - loss is tensor(0.5005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4330/6700 [2:00:47<1:06:01,  1.67s/it]11/17/2022 00:54:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.4579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:56 - INFO - train.train_snli_ve - loss is tensor(0.9043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4331/6700 [2:00:49<1:05:51,  1.67s/it]11/17/2022 00:54:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.8903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:58 - INFO - train.train_snli_ve - loss is tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4332/6700 [2:00:50<1:06:34,  1.69s/it]11/17/2022 00:54:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.7154e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:54:59 - INFO - train.train_snli_ve - loss is tensor(0.8518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4333/6700 [2:00:52<1:06:26,  1.68s/it]11/17/2022 00:55:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8509e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:01 - INFO - train.train_snli_ve - loss is tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4334/6700 [2:00:54<1:06:09,  1.68s/it]11/17/2022 00:55:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.0660e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:03 - INFO - train.train_snli_ve - loss is tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4335/6700 [2:00:55<1:06:14,  1.68s/it]11/17/2022 00:55:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.4282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:04 - INFO - train.train_snli_ve - loss is tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4336/6700 [2:00:57<1:06:31,  1.69s/it]11/17/2022 00:55:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.3445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:06 - INFO - train.train_snli_ve - loss is tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4337/6700 [2:00:59<1:06:28,  1.69s/it]11/17/2022 00:55:08 - INFO - train.train_snli_ve - kd_loss is tensor(4.6563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:08 - INFO - train.train_snli_ve - loss is tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4338/6700 [2:01:00<1:06:17,  1.68s/it]11/17/2022 00:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.9420e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:09 - INFO - train.train_snli_ve - loss is tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4339/6700 [2:01:02<1:06:11,  1.68s/it]11/17/2022 00:55:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:11 - INFO - train.train_snli_ve - loss is tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4340/6700 [2:01:04<1:06:17,  1.69s/it]11/17/2022 00:55:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.7749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:13 - INFO - train.train_snli_ve - loss is tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4341/6700 [2:01:06<1:06:17,  1.69s/it]11/17/2022 00:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.2841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:15 - INFO - train.train_snli_ve - loss is tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4342/6700 [2:01:07<1:06:20,  1.69s/it]11/17/2022 00:55:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.0233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:16 - INFO - train.train_snli_ve - loss is tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4343/6700 [2:01:09<1:06:05,  1.68s/it]11/17/2022 00:55:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.9771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:18 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4344/6700 [2:01:11<1:06:10,  1.69s/it]11/17/2022 00:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:20 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4345/6700 [2:01:12<1:06:29,  1.69s/it]11/17/2022 00:55:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.0649e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:21 - INFO - train.train_snli_ve - loss is tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4346/6700 [2:01:14<1:06:16,  1.69s/it]11/17/2022 00:55:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.1625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:23 - INFO - train.train_snli_ve - loss is tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4347/6700 [2:01:16<1:06:10,  1.69s/it]11/17/2022 00:55:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.1661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:25 - INFO - train.train_snli_ve - loss is tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4348/6700 [2:01:17<1:06:32,  1.70s/it]11/17/2022 00:55:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.5385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:26 - INFO - train.train_snli_ve - loss is tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4349/6700 [2:01:19<1:06:42,  1.70s/it]11/17/2022 00:55:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:28 - INFO - train.train_snli_ve - loss is tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4350/6700 [2:01:21<1:06:43,  1.70s/it]11/17/2022 00:55:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.2898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:30 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4351/6700 [2:01:23<1:06:57,  1.71s/it]11/17/2022 00:55:32 - INFO - train.train_snli_ve - kd_loss is tensor(4.1884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:32 - INFO - train.train_snli_ve - loss is tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4352/6700 [2:01:24<1:07:09,  1.72s/it]11/17/2022 00:55:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.8906e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:33 - INFO - train.train_snli_ve - loss is tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4353/6700 [2:01:26<1:06:42,  1.71s/it]11/17/2022 00:55:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.4747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:35 - INFO - train.train_snli_ve - loss is tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######4   | 4354/6700 [2:01:28<1:06:46,  1.71s/it]11/17/2022 00:55:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:37 - INFO - train.train_snli_ve - loss is tensor(0.7876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4355/6700 [2:01:29<1:06:36,  1.70s/it]11/17/2022 00:55:38 - INFO - train.train_snli_ve - kd_loss is tensor(5.0313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:38 - INFO - train.train_snli_ve - loss is tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4356/6700 [2:01:31<1:06:55,  1.71s/it]11/17/2022 00:55:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:40 - INFO - train.train_snli_ve - loss is tensor(0.4467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4357/6700 [2:01:33<1:06:16,  1.70s/it]11/17/2022 00:55:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.7036e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:42 - INFO - train.train_snli_ve - loss is tensor(0.5213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4358/6700 [2:01:34<1:05:45,  1.68s/it]11/17/2022 00:55:43 - INFO - train.train_snli_ve - kd_loss is tensor(4.0234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:43 - INFO - train.train_snli_ve - loss is tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4359/6700 [2:01:36<1:06:09,  1.70s/it]11/17/2022 00:55:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:45 - INFO - train.train_snli_ve - loss is tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4360/6700 [2:01:38<1:06:07,  1.70s/it]11/17/2022 00:55:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5187e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:47 - INFO - train.train_snli_ve - loss is tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4361/6700 [2:01:40<1:06:02,  1.69s/it]11/17/2022 00:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:49 - INFO - train.train_snli_ve - loss is tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4362/6700 [2:01:41<1:05:58,  1.69s/it]11/17/2022 00:55:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.1743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:50 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4363/6700 [2:01:43<1:05:37,  1.68s/it]11/17/2022 00:55:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.3567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:52 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4364/6700 [2:01:45<1:05:33,  1.68s/it]11/17/2022 00:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:54 - INFO - train.train_snli_ve - loss is tensor(0.8554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4365/6700 [2:01:46<1:05:21,  1.68s/it]11/17/2022 00:55:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:55 - INFO - train.train_snli_ve - loss is tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4366/6700 [2:01:48<1:05:25,  1.68s/it]11/17/2022 00:55:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:57 - INFO - train.train_snli_ve - loss is tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4367/6700 [2:01:50<1:05:39,  1.69s/it]11/17/2022 00:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:55:59 - INFO - train.train_snli_ve - loss is tensor(0.9519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4368/6700 [2:01:51<1:05:33,  1.69s/it]11/17/2022 00:56:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:00 - INFO - train.train_snli_ve - loss is tensor(0.8787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4369/6700 [2:01:53<1:05:21,  1.68s/it]11/17/2022 00:56:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.6381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:02 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4370/6700 [2:01:55<1:05:11,  1.68s/it]11/17/2022 00:56:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0782e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:04 - INFO - train.train_snli_ve - loss is tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4371/6700 [2:01:56<1:05:23,  1.68s/it]11/17/2022 00:56:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:05 - INFO - train.train_snli_ve - loss is tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4372/6700 [2:01:58<1:05:52,  1.70s/it]11/17/2022 00:56:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:07 - INFO - train.train_snli_ve - loss is tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4373/6700 [2:02:00<1:05:32,  1.69s/it]11/17/2022 00:56:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:09 - INFO - train.train_snli_ve - loss is tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4374/6700 [2:02:01<1:05:14,  1.68s/it]11/17/2022 00:56:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:10 - INFO - train.train_snli_ve - loss is tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4375/6700 [2:02:03<1:04:48,  1.67s/it]11/17/2022 00:56:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8829e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:12 - INFO - train.train_snli_ve - loss is tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4376/6700 [2:02:05<1:04:57,  1.68s/it]11/17/2022 00:56:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.1322e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:14 - INFO - train.train_snli_ve - loss is tensor(0.9174, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4377/6700 [2:02:06<1:04:37,  1.67s/it]11/17/2022 00:56:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.2635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:15 - INFO - train.train_snli_ve - loss is tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4378/6700 [2:02:08<1:04:56,  1.68s/it]11/17/2022 00:56:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.8636e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:17 - INFO - train.train_snli_ve - loss is tensor(0.5015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4379/6700 [2:02:10<1:04:51,  1.68s/it]11/17/2022 00:56:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.2134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:19 - INFO - train.train_snli_ve - loss is tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4380/6700 [2:02:12<1:05:37,  1.70s/it]11/17/2022 00:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:21 - INFO - train.train_snli_ve - loss is tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4381/6700 [2:02:13<1:05:29,  1.69s/it]11/17/2022 00:56:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.7737e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:22 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4382/6700 [2:02:15<1:05:26,  1.69s/it]11/17/2022 00:56:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:24 - INFO - train.train_snli_ve - loss is tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4383/6700 [2:02:17<1:05:15,  1.69s/it]11/17/2022 00:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:26 - INFO - train.train_snli_ve - loss is tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4384/6700 [2:02:18<1:05:14,  1.69s/it]11/17/2022 00:56:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.3386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:27 - INFO - train.train_snli_ve - loss is tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4385/6700 [2:02:20<1:05:00,  1.69s/it]11/17/2022 00:56:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.2290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:29 - INFO - train.train_snli_ve - loss is tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4386/6700 [2:02:22<1:05:24,  1.70s/it]11/17/2022 00:56:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.7314e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:31 - INFO - train.train_snli_ve - loss is tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4387/6700 [2:02:23<1:05:33,  1.70s/it]11/17/2022 00:56:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:32 - INFO - train.train_snli_ve - loss is tensor(0.4507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  65%|######5   | 4388/6700 [2:02:25<1:05:25,  1.70s/it]11/17/2022 00:56:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:34 - INFO - train.train_snli_ve - loss is tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4389/6700 [2:02:27<1:05:05,  1.69s/it]11/17/2022 00:56:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.8784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:36 - INFO - train.train_snli_ve - loss is tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4390/6700 [2:02:28<1:04:51,  1.68s/it]11/17/2022 00:56:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.6709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:37 - INFO - train.train_snli_ve - loss is tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4391/6700 [2:02:30<1:04:37,  1.68s/it]11/17/2022 00:56:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.6023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:39 - INFO - train.train_snli_ve - loss is tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4392/6700 [2:02:32<1:04:23,  1.67s/it]11/17/2022 00:56:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:41 - INFO - train.train_snli_ve - loss is tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4393/6700 [2:02:33<1:04:35,  1.68s/it]11/17/2022 00:56:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:42 - INFO - train.train_snli_ve - loss is tensor(0.7834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4394/6700 [2:02:35<1:04:43,  1.68s/it]11/17/2022 00:56:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:44 - INFO - train.train_snli_ve - loss is tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4395/6700 [2:02:37<1:05:05,  1.69s/it]11/17/2022 00:56:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:46 - INFO - train.train_snli_ve - loss is tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4396/6700 [2:02:39<1:04:58,  1.69s/it]11/17/2022 00:56:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:48 - INFO - train.train_snli_ve - loss is tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4397/6700 [2:02:40<1:04:39,  1.68s/it]11/17/2022 00:56:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5372e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:49 - INFO - train.train_snli_ve - loss is tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4398/6700 [2:02:42<1:04:39,  1.69s/it]11/17/2022 00:56:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.6183e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:51 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4399/6700 [2:02:43<1:03:32,  1.66s/it]11/17/2022 00:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:52 - INFO - train.train_snli_ve - loss is tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4400/6700 [2:02:45<1:04:12,  1.67s/it]11/17/2022 00:56:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.9365e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:54 - INFO - train.train_snli_ve - loss is tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4401/6700 [2:02:47<1:03:59,  1.67s/it]11/17/2022 00:56:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.5643e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:56 - INFO - train.train_snli_ve - loss is tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4402/6700 [2:02:48<1:03:35,  1.66s/it]11/17/2022 00:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:57 - INFO - train.train_snli_ve - loss is tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4403/6700 [2:02:50<1:03:38,  1.66s/it]11/17/2022 00:56:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.3578e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:56:59 - INFO - train.train_snli_ve - loss is tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4404/6700 [2:02:52<1:04:10,  1.68s/it]11/17/2022 00:57:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:01 - INFO - train.train_snli_ve - loss is tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4405/6700 [2:02:54<1:04:37,  1.69s/it]11/17/2022 00:57:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.0398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:03 - INFO - train.train_snli_ve - loss is tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4406/6700 [2:02:55<1:04:52,  1.70s/it]11/17/2022 00:57:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.1062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:04 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4407/6700 [2:02:57<1:04:26,  1.69s/it]11/17/2022 00:57:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:06 - INFO - train.train_snli_ve - loss is tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4408/6700 [2:02:59<1:04:55,  1.70s/it]11/17/2022 00:57:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.8703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:08 - INFO - train.train_snli_ve - loss is tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4409/6700 [2:03:00<1:05:07,  1.71s/it]11/17/2022 00:57:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.8451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:09 - INFO - train.train_snli_ve - loss is tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4410/6700 [2:03:02<1:05:01,  1.70s/it]11/17/2022 00:57:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.7546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:11 - INFO - train.train_snli_ve - loss is tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4411/6700 [2:03:04<1:04:46,  1.70s/it]11/17/2022 00:57:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:13 - INFO - train.train_snli_ve - loss is tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4412/6700 [2:03:06<1:05:00,  1.70s/it]11/17/2022 00:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.2082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:15 - INFO - train.train_snli_ve - loss is tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4413/6700 [2:03:07<1:05:08,  1.71s/it]11/17/2022 00:57:16 - INFO - train.train_snli_ve - kd_loss is tensor(8.5758e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:16 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4414/6700 [2:03:09<1:04:35,  1.70s/it]11/17/2022 00:57:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.7318e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:18 - INFO - train.train_snli_ve - loss is tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4415/6700 [2:03:11<1:04:07,  1.68s/it]11/17/2022 00:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:20 - INFO - train.train_snli_ve - loss is tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4416/6700 [2:03:12<1:04:28,  1.69s/it]11/17/2022 00:57:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.8056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:21 - INFO - train.train_snli_ve - loss is tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4417/6700 [2:03:14<1:04:05,  1.68s/it]11/17/2022 00:57:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.0175e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:23 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4418/6700 [2:03:16<1:04:31,  1.70s/it]11/17/2022 00:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.6974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:25 - INFO - train.train_snli_ve - loss is tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4419/6700 [2:03:17<1:04:10,  1.69s/it]11/17/2022 00:57:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.4927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:26 - INFO - train.train_snli_ve - loss is tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4420/6700 [2:03:19<1:04:23,  1.69s/it]11/17/2022 00:57:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.1162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:28 - INFO - train.train_snli_ve - loss is tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######5   | 4421/6700 [2:03:21<1:04:28,  1.70s/it]11/17/2022 00:57:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.0133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:30 - INFO - train.train_snli_ve - loss is tensor(0.8959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4422/6700 [2:03:22<1:04:34,  1.70s/it]11/17/2022 00:57:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.0531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:31 - INFO - train.train_snli_ve - loss is tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4423/6700 [2:03:24<1:04:14,  1.69s/it]11/17/2022 00:57:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.6566e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:33 - INFO - train.train_snli_ve - loss is tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4424/6700 [2:03:26<1:04:21,  1.70s/it]11/17/2022 00:57:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:35 - INFO - train.train_snli_ve - loss is tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4425/6700 [2:03:27<1:03:40,  1.68s/it]11/17/2022 00:57:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.0804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:36 - INFO - train.train_snli_ve - loss is tensor(0.5480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4426/6700 [2:03:29<1:03:50,  1.68s/it]11/17/2022 00:57:38 - INFO - train.train_snli_ve - kd_loss is tensor(7.4496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:38 - INFO - train.train_snli_ve - loss is tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4427/6700 [2:03:31<1:04:10,  1.69s/it]11/17/2022 00:57:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:40 - INFO - train.train_snli_ve - loss is tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4428/6700 [2:03:33<1:03:50,  1.69s/it]11/17/2022 00:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.9589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:42 - INFO - train.train_snli_ve - loss is tensor(0.7352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4429/6700 [2:03:34<1:03:36,  1.68s/it]11/17/2022 00:57:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.4392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:43 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4430/6700 [2:03:36<1:03:36,  1.68s/it]11/17/2022 00:57:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.4182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:45 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4431/6700 [2:03:38<1:03:23,  1.68s/it]11/17/2022 00:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.6717e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:47 - INFO - train.train_snli_ve - loss is tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4432/6700 [2:03:39<1:03:30,  1.68s/it]11/17/2022 00:57:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.6848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:48 - INFO - train.train_snli_ve - loss is tensor(0.5175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4433/6700 [2:03:41<1:03:39,  1.68s/it]11/17/2022 00:57:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.9693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:50 - INFO - train.train_snli_ve - loss is tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4434/6700 [2:03:43<1:04:14,  1.70s/it]11/17/2022 00:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(5.2720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:52 - INFO - train.train_snli_ve - loss is tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4435/6700 [2:03:44<1:04:28,  1.71s/it]11/17/2022 00:57:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0163e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:53 - INFO - train.train_snli_ve - loss is tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4436/6700 [2:03:46<1:03:54,  1.69s/it]11/17/2022 00:57:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:55 - INFO - train.train_snli_ve - loss is tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4437/6700 [2:03:48<1:03:58,  1.70s/it]11/17/2022 00:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.5788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:57 - INFO - train.train_snli_ve - loss is tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4438/6700 [2:03:49<1:04:03,  1.70s/it]11/17/2022 00:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.9015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:57:59 - INFO - train.train_snli_ve - loss is tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4439/6700 [2:03:51<1:04:09,  1.70s/it]11/17/2022 00:58:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:00 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4440/6700 [2:03:53<1:04:30,  1.71s/it]11/17/2022 00:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2546e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:02 - INFO - train.train_snli_ve - loss is tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4441/6700 [2:03:55<1:04:14,  1.71s/it]11/17/2022 00:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:04 - INFO - train.train_snli_ve - loss is tensor(0.9257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4442/6700 [2:03:56<1:04:01,  1.70s/it]11/17/2022 00:58:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.4411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:05 - INFO - train.train_snli_ve - loss is tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4443/6700 [2:03:58<1:03:36,  1.69s/it]11/17/2022 00:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:07 - INFO - train.train_snli_ve - loss is tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4444/6700 [2:04:00<1:03:25,  1.69s/it]11/17/2022 00:58:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.3185e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:09 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4445/6700 [2:04:01<1:03:21,  1.69s/it]11/17/2022 00:58:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:10 - INFO - train.train_snli_ve - loss is tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4446/6700 [2:04:03<1:03:18,  1.69s/it]11/17/2022 00:58:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.1131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:12 - INFO - train.train_snli_ve - loss is tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4447/6700 [2:04:05<1:02:58,  1.68s/it]11/17/2022 00:58:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.6264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:14 - INFO - train.train_snli_ve - loss is tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4448/6700 [2:04:06<1:04:07,  1.71s/it]11/17/2022 00:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.5331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:15 - INFO - train.train_snli_ve - loss is tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4449/6700 [2:04:08<1:03:25,  1.69s/it]11/17/2022 00:58:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.5895e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:17 - INFO - train.train_snli_ve - loss is tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4450/6700 [2:04:10<1:03:01,  1.68s/it]11/17/2022 00:58:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.0317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:19 - INFO - train.train_snli_ve - loss is tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4451/6700 [2:04:11<1:03:15,  1.69s/it]11/17/2022 00:58:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.5236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:21 - INFO - train.train_snli_ve - loss is tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4452/6700 [2:04:13<1:03:49,  1.70s/it]11/17/2022 00:58:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.7348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:22 - INFO - train.train_snli_ve - loss is tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4453/6700 [2:04:15<1:03:20,  1.69s/it]11/17/2022 00:58:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.0450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:24 - INFO - train.train_snli_ve - loss is tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4454/6700 [2:04:17<1:03:26,  1.69s/it]11/17/2022 00:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3835e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:26 - INFO - train.train_snli_ve - loss is tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  66%|######6   | 4455/6700 [2:04:18<1:03:19,  1.69s/it]11/17/2022 00:58:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.4344e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:27 - INFO - train.train_snli_ve - loss is tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4456/6700 [2:04:20<1:03:03,  1.69s/it]11/17/2022 00:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.6459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:29 - INFO - train.train_snli_ve - loss is tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4457/6700 [2:04:22<1:03:24,  1.70s/it]11/17/2022 00:58:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.7976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:31 - INFO - train.train_snli_ve - loss is tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4458/6700 [2:04:23<1:02:52,  1.68s/it]11/17/2022 00:58:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:32 - INFO - train.train_snli_ve - loss is tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4459/6700 [2:04:25<1:02:32,  1.67s/it]11/17/2022 00:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:34 - INFO - train.train_snli_ve - loss is tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4460/6700 [2:04:27<1:01:56,  1.66s/it]11/17/2022 00:58:36 - INFO - train.train_snli_ve - kd_loss is tensor(5.2999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:36 - INFO - train.train_snli_ve - loss is tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4461/6700 [2:04:28<1:02:51,  1.68s/it]11/17/2022 00:58:37 - INFO - train.train_snli_ve - kd_loss is tensor(5.7504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:37 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4462/6700 [2:04:30<1:02:47,  1.68s/it]11/17/2022 00:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.8806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:39 - INFO - train.train_snli_ve - loss is tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4463/6700 [2:04:32<1:02:45,  1.68s/it]11/17/2022 00:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.4722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:41 - INFO - train.train_snli_ve - loss is tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4464/6700 [2:04:33<1:03:09,  1.69s/it]11/17/2022 00:58:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:42 - INFO - train.train_snli_ve - loss is tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4465/6700 [2:04:35<1:03:39,  1.71s/it]11/17/2022 00:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.1956e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:44 - INFO - train.train_snli_ve - loss is tensor(0.5092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4466/6700 [2:04:37<1:03:40,  1.71s/it]11/17/2022 00:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:46 - INFO - train.train_snli_ve - loss is tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4467/6700 [2:04:39<1:03:02,  1.69s/it]11/17/2022 00:58:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.6142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:48 - INFO - train.train_snli_ve - loss is tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4468/6700 [2:04:40<1:03:12,  1.70s/it]11/17/2022 00:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3734e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:49 - INFO - train.train_snli_ve - loss is tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4469/6700 [2:04:42<1:03:13,  1.70s/it]11/17/2022 00:58:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.2273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:51 - INFO - train.train_snli_ve - loss is tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4470/6700 [2:04:44<1:03:12,  1.70s/it]11/17/2022 00:58:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:53 - INFO - train.train_snli_ve - loss is tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4471/6700 [2:04:45<1:02:59,  1.70s/it]11/17/2022 00:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.8416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:54 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4472/6700 [2:04:47<1:02:56,  1.70s/it]11/17/2022 00:58:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2691e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:56 - INFO - train.train_snli_ve - loss is tensor(0.9256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4473/6700 [2:04:49<1:03:05,  1.70s/it]11/17/2022 00:58:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.3172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:58 - INFO - train.train_snli_ve - loss is tensor(0.5787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4474/6700 [2:04:50<1:03:08,  1.70s/it]11/17/2022 00:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.1626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:58:59 - INFO - train.train_snli_ve - loss is tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4475/6700 [2:04:52<1:03:41,  1.72s/it]11/17/2022 00:59:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.0339e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:01 - INFO - train.train_snli_ve - loss is tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4476/6700 [2:04:54<1:03:13,  1.71s/it]11/17/2022 00:59:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.4672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:03 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4477/6700 [2:04:56<1:03:51,  1.72s/it]11/17/2022 00:59:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:05 - INFO - train.train_snli_ve - loss is tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4478/6700 [2:04:57<1:03:20,  1.71s/it]11/17/2022 00:59:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:06 - INFO - train.train_snli_ve - loss is tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4479/6700 [2:04:59<1:03:06,  1.70s/it]11/17/2022 00:59:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.7123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:08 - INFO - train.train_snli_ve - loss is tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4480/6700 [2:05:01<1:02:38,  1.69s/it]11/17/2022 00:59:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.5002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:10 - INFO - train.train_snli_ve - loss is tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4481/6700 [2:05:02<1:02:15,  1.68s/it]11/17/2022 00:59:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:11 - INFO - train.train_snli_ve - loss is tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4482/6700 [2:05:04<1:02:31,  1.69s/it]11/17/2022 00:59:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.5573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:13 - INFO - train.train_snli_ve - loss is tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4483/6700 [2:05:06<1:02:22,  1.69s/it]11/17/2022 00:59:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:15 - INFO - train.train_snli_ve - loss is tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4484/6700 [2:05:07<1:02:57,  1.70s/it]11/17/2022 00:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.8671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:16 - INFO - train.train_snli_ve - loss is tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4485/6700 [2:05:09<1:03:00,  1.71s/it]11/17/2022 00:59:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:18 - INFO - train.train_snli_ve - loss is tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4486/6700 [2:05:11<1:03:07,  1.71s/it]11/17/2022 00:59:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.6383e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:20 - INFO - train.train_snli_ve - loss is tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4487/6700 [2:05:13<1:02:53,  1.71s/it]11/17/2022 00:59:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.0182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:22 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######6   | 4488/6700 [2:05:14<1:02:36,  1.70s/it]11/17/2022 00:59:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:23 - INFO - train.train_snli_ve - loss is tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4489/6700 [2:05:16<1:02:05,  1.68s/it]11/17/2022 00:59:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.6465e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:25 - INFO - train.train_snli_ve - loss is tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4490/6700 [2:05:18<1:02:21,  1.69s/it]11/17/2022 00:59:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.1047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:27 - INFO - train.train_snli_ve - loss is tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4491/6700 [2:05:19<1:02:16,  1.69s/it]11/17/2022 00:59:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.2666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:28 - INFO - train.train_snli_ve - loss is tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4492/6700 [2:05:21<1:01:55,  1.68s/it]11/17/2022 00:59:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:30 - INFO - train.train_snli_ve - loss is tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4493/6700 [2:05:23<1:02:01,  1.69s/it]11/17/2022 00:59:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.6668e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:32 - INFO - train.train_snli_ve - loss is tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4494/6700 [2:05:24<1:03:11,  1.72s/it]11/17/2022 00:59:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:33 - INFO - train.train_snli_ve - loss is tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4495/6700 [2:05:26<1:02:40,  1.71s/it]11/17/2022 00:59:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.8130e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:35 - INFO - train.train_snli_ve - loss is tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4496/6700 [2:05:28<1:03:29,  1.73s/it]11/17/2022 00:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.5258e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:37 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4497/6700 [2:05:30<1:02:42,  1.71s/it]11/17/2022 00:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:39 - INFO - train.train_snli_ve - loss is tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4498/6700 [2:05:31<1:02:22,  1.70s/it]11/17/2022 00:59:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.6289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:40 - INFO - train.train_snli_ve - loss is tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4499/6700 [2:05:33<1:02:06,  1.69s/it]11/17/2022 00:59:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.7445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:42 - INFO - train.train_snli_ve - loss is tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4500/6700 [2:05:35<1:02:42,  1.71s/it]11/17/2022 00:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:44 - INFO - train.train_snli_ve - loss is tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4501/6700 [2:05:36<1:02:46,  1.71s/it]11/17/2022 00:59:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.2381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:45 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4502/6700 [2:05:38<1:03:14,  1.73s/it]11/17/2022 00:59:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.1799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:47 - INFO - train.train_snli_ve - loss is tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4503/6700 [2:05:40<1:02:49,  1.72s/it]11/17/2022 00:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.8828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:49 - INFO - train.train_snli_ve - loss is tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4504/6700 [2:05:42<1:02:45,  1.71s/it]11/17/2022 00:59:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.8584e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:51 - INFO - train.train_snli_ve - loss is tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4505/6700 [2:05:43<1:02:19,  1.70s/it]11/17/2022 00:59:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:52 - INFO - train.train_snli_ve - loss is tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4506/6700 [2:05:45<1:01:53,  1.69s/it]11/17/2022 00:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.8498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:54 - INFO - train.train_snli_ve - loss is tensor(0.8218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4507/6700 [2:05:47<1:01:55,  1.69s/it]11/17/2022 00:59:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.4122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:56 - INFO - train.train_snli_ve - loss is tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4508/6700 [2:05:48<1:01:36,  1.69s/it]11/17/2022 00:59:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:57 - INFO - train.train_snli_ve - loss is tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4509/6700 [2:05:50<1:01:18,  1.68s/it]11/17/2022 00:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.6212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 00:59:59 - INFO - train.train_snli_ve - loss is tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4510/6700 [2:05:52<1:01:16,  1.68s/it]11/17/2022 01:00:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1959e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:01 - INFO - train.train_snli_ve - loss is tensor(0.8704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4511/6700 [2:05:53<1:01:16,  1.68s/it]11/17/2022 01:00:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.5935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:02 - INFO - train.train_snli_ve - loss is tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4512/6700 [2:05:55<1:01:05,  1.68s/it]11/17/2022 01:00:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.1111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:04 - INFO - train.train_snli_ve - loss is tensor(0.5801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4513/6700 [2:05:57<1:01:17,  1.68s/it]11/17/2022 01:00:06 - INFO - train.train_snli_ve - kd_loss is tensor(4.5267e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:06 - INFO - train.train_snli_ve - loss is tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4514/6700 [2:05:58<1:01:31,  1.69s/it]11/17/2022 01:00:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.4266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:07 - INFO - train.train_snli_ve - loss is tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4515/6700 [2:06:00<1:01:33,  1.69s/it]11/17/2022 01:00:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:09 - INFO - train.train_snli_ve - loss is tensor(0.9215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4516/6700 [2:06:02<1:01:26,  1.69s/it]11/17/2022 01:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.0523e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:11 - INFO - train.train_snli_ve - loss is tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4517/6700 [2:06:03<1:01:29,  1.69s/it]11/17/2022 01:00:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.6352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:12 - INFO - train.train_snli_ve - loss is tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4518/6700 [2:06:05<1:01:42,  1.70s/it]11/17/2022 01:00:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.6700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:14 - INFO - train.train_snli_ve - loss is tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4519/6700 [2:06:07<1:01:15,  1.69s/it]11/17/2022 01:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:16 - INFO - train.train_snli_ve - loss is tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4520/6700 [2:06:09<1:01:09,  1.68s/it]11/17/2022 01:00:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9299e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:18 - INFO - train.train_snli_ve - loss is tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4521/6700 [2:06:10<1:01:11,  1.68s/it]11/17/2022 01:00:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:19 - INFO - train.train_snli_ve - loss is tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  67%|######7   | 4522/6700 [2:06:12<1:00:38,  1.67s/it]11/17/2022 01:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:21 - INFO - train.train_snli_ve - loss is tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4523/6700 [2:06:14<1:01:01,  1.68s/it]11/17/2022 01:00:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7566e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:23 - INFO - train.train_snli_ve - loss is tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4524/6700 [2:06:15<1:00:48,  1.68s/it]11/17/2022 01:00:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:24 - INFO - train.train_snli_ve - loss is tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4525/6700 [2:06:17<1:00:44,  1.68s/it]11/17/2022 01:00:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:26 - INFO - train.train_snli_ve - loss is tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4526/6700 [2:06:19<1:01:02,  1.68s/it]11/17/2022 01:00:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.9489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:28 - INFO - train.train_snli_ve - loss is tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4527/6700 [2:06:20<1:01:20,  1.69s/it]11/17/2022 01:00:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.6868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:29 - INFO - train.train_snli_ve - loss is tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4528/6700 [2:06:22<1:01:29,  1.70s/it]11/17/2022 01:00:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.4544e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:31 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4529/6700 [2:06:24<1:01:42,  1.71s/it]11/17/2022 01:00:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.8073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:33 - INFO - train.train_snli_ve - loss is tensor(0.5021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4530/6700 [2:06:25<1:01:33,  1.70s/it]11/17/2022 01:00:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.5309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:34 - INFO - train.train_snli_ve - loss is tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4531/6700 [2:06:27<1:01:33,  1.70s/it]11/17/2022 01:00:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.7894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:36 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4532/6700 [2:06:29<1:01:29,  1.70s/it]11/17/2022 01:00:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.0824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:38 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4533/6700 [2:06:30<1:00:48,  1.68s/it]11/17/2022 01:00:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:39 - INFO - train.train_snli_ve - loss is tensor(0.8018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4534/6700 [2:06:32<1:00:33,  1.68s/it]11/17/2022 01:00:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.7732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:41 - INFO - train.train_snli_ve - loss is tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4535/6700 [2:06:34<1:01:06,  1.69s/it]11/17/2022 01:00:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:43 - INFO - train.train_snli_ve - loss is tensor(0.9331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4536/6700 [2:06:36<1:01:45,  1.71s/it]11/17/2022 01:00:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:45 - INFO - train.train_snli_ve - loss is tensor(0.7345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4537/6700 [2:06:37<1:01:13,  1.70s/it]11/17/2022 01:00:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.7003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:46 - INFO - train.train_snli_ve - loss is tensor(0.9994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4538/6700 [2:06:39<1:00:57,  1.69s/it]11/17/2022 01:00:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.0901e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:48 - INFO - train.train_snli_ve - loss is tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4539/6700 [2:06:41<1:00:47,  1.69s/it]11/17/2022 01:00:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.2452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:50 - INFO - train.train_snli_ve - loss is tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4540/6700 [2:06:42<1:00:54,  1.69s/it]11/17/2022 01:00:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.8817e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:51 - INFO - train.train_snli_ve - loss is tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4541/6700 [2:06:44<1:00:48,  1.69s/it]11/17/2022 01:00:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.6789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:53 - INFO - train.train_snli_ve - loss is tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4542/6700 [2:06:46<1:00:30,  1.68s/it]11/17/2022 01:00:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.2720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:55 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4543/6700 [2:06:47<1:00:12,  1.67s/it]11/17/2022 01:00:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:56 - INFO - train.train_snli_ve - loss is tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4544/6700 [2:06:49<1:00:28,  1.68s/it]11/17/2022 01:00:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:00:58 - INFO - train.train_snli_ve - loss is tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4545/6700 [2:06:51<1:00:47,  1.69s/it]11/17/2022 01:01:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:00 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4546/6700 [2:06:52<1:00:32,  1.69s/it]11/17/2022 01:01:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.1231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:01 - INFO - train.train_snli_ve - loss is tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4547/6700 [2:06:54<1:00:55,  1.70s/it]11/17/2022 01:01:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.5836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:03 - INFO - train.train_snli_ve - loss is tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4548/6700 [2:06:56<1:01:04,  1.70s/it]11/17/2022 01:01:05 - INFO - train.train_snli_ve - kd_loss is tensor(4.1970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:05 - INFO - train.train_snli_ve - loss is tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4549/6700 [2:06:58<1:00:46,  1.70s/it]11/17/2022 01:01:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.0715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:07 - INFO - train.train_snli_ve - loss is tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4550/6700 [2:06:59<1:00:47,  1.70s/it]11/17/2022 01:01:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.2039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:08 - INFO - train.train_snli_ve - loss is tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4551/6700 [2:07:01<1:00:26,  1.69s/it]11/17/2022 01:01:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.3994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:10 - INFO - train.train_snli_ve - loss is tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4552/6700 [2:07:03<1:01:00,  1.70s/it]11/17/2022 01:01:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:12 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4553/6700 [2:07:04<1:00:51,  1.70s/it]11/17/2022 01:01:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.9057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:13 - INFO - train.train_snli_ve - loss is tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4554/6700 [2:07:06<1:00:46,  1.70s/it]11/17/2022 01:01:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.2766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:15 - INFO - train.train_snli_ve - loss is tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######7   | 4555/6700 [2:07:08<1:00:31,  1.69s/it]11/17/2022 01:01:17 - INFO - train.train_snli_ve - kd_loss is tensor(5.7647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:17 - INFO - train.train_snli_ve - loss is tensor(0.9105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4556/6700 [2:07:09<1:00:51,  1.70s/it]11/17/2022 01:01:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.2417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:18 - INFO - train.train_snli_ve - loss is tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4557/6700 [2:07:11<1:00:44,  1.70s/it]11/17/2022 01:01:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.3337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:20 - INFO - train.train_snli_ve - loss is tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4558/6700 [2:07:13<1:00:27,  1.69s/it]11/17/2022 01:01:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.6666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:22 - INFO - train.train_snli_ve - loss is tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4559/6700 [2:07:15<1:00:40,  1.70s/it]11/17/2022 01:01:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:24 - INFO - train.train_snli_ve - loss is tensor(0.9082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4560/6700 [2:07:16<1:00:48,  1.70s/it]11/17/2022 01:01:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.4278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:25 - INFO - train.train_snli_ve - loss is tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4561/6700 [2:07:18<1:00:45,  1.70s/it]11/17/2022 01:01:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.3038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:27 - INFO - train.train_snli_ve - loss is tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4562/6700 [2:07:20<1:00:20,  1.69s/it]11/17/2022 01:01:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9375e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:29 - INFO - train.train_snli_ve - loss is tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4563/6700 [2:07:21<1:00:11,  1.69s/it]11/17/2022 01:01:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.9716e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:30 - INFO - train.train_snli_ve - loss is tensor(0.8769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4564/6700 [2:07:23<59:46,  1.68s/it]  11/17/2022 01:01:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:32 - INFO - train.train_snli_ve - loss is tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4565/6700 [2:07:25<59:53,  1.68s/it]11/17/2022 01:01:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:34 - INFO - train.train_snli_ve - loss is tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4566/6700 [2:07:26<59:53,  1.68s/it]11/17/2022 01:01:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.3132e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:35 - INFO - train.train_snli_ve - loss is tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4567/6700 [2:07:28<59:46,  1.68s/it]11/17/2022 01:01:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.6069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:37 - INFO - train.train_snli_ve - loss is tensor(0.7838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4568/6700 [2:07:30<59:57,  1.69s/it]11/17/2022 01:01:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.1961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:39 - INFO - train.train_snli_ve - loss is tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4569/6700 [2:07:31<59:57,  1.69s/it]11/17/2022 01:01:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.8361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:40 - INFO - train.train_snli_ve - loss is tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4570/6700 [2:07:33<59:47,  1.68s/it]11/17/2022 01:01:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.4361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:42 - INFO - train.train_snli_ve - loss is tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4571/6700 [2:07:35<59:29,  1.68s/it]11/17/2022 01:01:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.8071e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:44 - INFO - train.train_snli_ve - loss is tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4572/6700 [2:07:36<59:41,  1.68s/it]11/17/2022 01:01:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:45 - INFO - train.train_snli_ve - loss is tensor(0.7746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4573/6700 [2:07:38<59:53,  1.69s/it]11/17/2022 01:01:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:47 - INFO - train.train_snli_ve - loss is tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4574/6700 [2:07:40<59:35,  1.68s/it]11/17/2022 01:01:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.0482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:49 - INFO - train.train_snli_ve - loss is tensor(0.9062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4575/6700 [2:07:41<59:26,  1.68s/it]11/17/2022 01:01:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.3181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:50 - INFO - train.train_snli_ve - loss is tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4576/6700 [2:07:43<59:15,  1.67s/it]11/17/2022 01:01:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.7964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:52 - INFO - train.train_snli_ve - loss is tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4577/6700 [2:07:45<59:29,  1.68s/it]11/17/2022 01:01:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.8478e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:54 - INFO - train.train_snli_ve - loss is tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4578/6700 [2:07:47<59:36,  1.69s/it]11/17/2022 01:01:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:55 - INFO - train.train_snli_ve - loss is tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4579/6700 [2:07:48<59:03,  1.67s/it]11/17/2022 01:01:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:57 - INFO - train.train_snli_ve - loss is tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4580/6700 [2:07:50<59:27,  1.68s/it]11/17/2022 01:01:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.5407e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:01:59 - INFO - train.train_snli_ve - loss is tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4581/6700 [2:07:52<59:25,  1.68s/it]11/17/2022 01:02:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.3139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:01 - INFO - train.train_snli_ve - loss is tensor(0.7645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4582/6700 [2:07:53<59:15,  1.68s/it]11/17/2022 01:02:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.1524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:02 - INFO - train.train_snli_ve - loss is tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4583/6700 [2:07:55<59:41,  1.69s/it]11/17/2022 01:02:04 - INFO - train.train_snli_ve - kd_loss is tensor(5.2397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:04 - INFO - train.train_snli_ve - loss is tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4584/6700 [2:07:57<59:43,  1.69s/it]11/17/2022 01:02:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5242e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:06 - INFO - train.train_snli_ve - loss is tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4585/6700 [2:07:58<59:48,  1.70s/it]11/17/2022 01:02:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:07 - INFO - train.train_snli_ve - loss is tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4586/6700 [2:08:00<59:53,  1.70s/it]11/17/2022 01:02:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2089e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:09 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4587/6700 [2:08:02<59:56,  1.70s/it]11/17/2022 01:02:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.8311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:11 - INFO - train.train_snli_ve - loss is tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4588/6700 [2:08:03<59:45,  1.70s/it]11/17/2022 01:02:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.6966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:12 - INFO - train.train_snli_ve - loss is tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  68%|######8   | 4589/6700 [2:08:05<59:54,  1.70s/it]11/17/2022 01:02:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:14 - INFO - train.train_snli_ve - loss is tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4590/6700 [2:08:07<59:36,  1.70s/it]11/17/2022 01:02:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.8413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:16 - INFO - train.train_snli_ve - loss is tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4591/6700 [2:08:09<59:53,  1.70s/it]11/17/2022 01:02:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.9816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:18 - INFO - train.train_snli_ve - loss is tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4592/6700 [2:08:10<1:00:01,  1.71s/it]11/17/2022 01:02:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.6927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:19 - INFO - train.train_snli_ve - loss is tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4593/6700 [2:08:12<59:54,  1.71s/it]  11/17/2022 01:02:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:21 - INFO - train.train_snli_ve - loss is tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4594/6700 [2:08:14<59:42,  1.70s/it]11/17/2022 01:02:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4928e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:23 - INFO - train.train_snli_ve - loss is tensor(0.4699, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4595/6700 [2:08:15<59:25,  1.69s/it]11/17/2022 01:02:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0620e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:24 - INFO - train.train_snli_ve - loss is tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4596/6700 [2:08:17<59:25,  1.69s/it]11/17/2022 01:02:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.6728e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:26 - INFO - train.train_snli_ve - loss is tensor(0.7545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4597/6700 [2:08:19<59:54,  1.71s/it]11/17/2022 01:02:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:28 - INFO - train.train_snli_ve - loss is tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4598/6700 [2:08:20<59:33,  1.70s/it]11/17/2022 01:02:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:29 - INFO - train.train_snli_ve - loss is tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4599/6700 [2:08:22<59:23,  1.70s/it]11/17/2022 01:02:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.2932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:31 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4600/6700 [2:08:24<59:33,  1.70s/it]11/17/2022 01:02:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.8070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:33 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4601/6700 [2:08:26<59:18,  1.70s/it]11/17/2022 01:02:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.0448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:35 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4602/6700 [2:08:27<58:59,  1.69s/it]11/17/2022 01:02:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:36 - INFO - train.train_snli_ve - loss is tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4603/6700 [2:08:29<58:25,  1.67s/it]11/17/2022 01:02:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.7347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:38 - INFO - train.train_snli_ve - loss is tensor(0.7228, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4604/6700 [2:08:31<58:28,  1.67s/it]11/17/2022 01:02:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:40 - INFO - train.train_snli_ve - loss is tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4605/6700 [2:08:32<58:32,  1.68s/it]11/17/2022 01:02:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:41 - INFO - train.train_snli_ve - loss is tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4606/6700 [2:08:34<58:59,  1.69s/it]11/17/2022 01:02:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1061e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:43 - INFO - train.train_snli_ve - loss is tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4607/6700 [2:08:36<59:18,  1.70s/it]11/17/2022 01:02:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3829e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:45 - INFO - train.train_snli_ve - loss is tensor(0.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4608/6700 [2:08:37<59:17,  1.70s/it]11/17/2022 01:02:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:46 - INFO - train.train_snli_ve - loss is tensor(1.0078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4609/6700 [2:08:39<59:25,  1.71s/it]11/17/2022 01:02:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.0924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:48 - INFO - train.train_snli_ve - loss is tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4610/6700 [2:08:41<59:16,  1.70s/it]11/17/2022 01:02:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:50 - INFO - train.train_snli_ve - loss is tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4611/6700 [2:08:42<58:54,  1.69s/it]11/17/2022 01:02:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.6154e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:51 - INFO - train.train_snli_ve - loss is tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4612/6700 [2:08:44<59:09,  1.70s/it]11/17/2022 01:02:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.0315e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:53 - INFO - train.train_snli_ve - loss is tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4613/6700 [2:08:46<58:43,  1.69s/it]11/17/2022 01:02:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.5453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:55 - INFO - train.train_snli_ve - loss is tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4614/6700 [2:08:48<58:33,  1.68s/it]11/17/2022 01:02:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.0516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:57 - INFO - train.train_snli_ve - loss is tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4615/6700 [2:08:49<58:42,  1.69s/it]11/17/2022 01:02:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.1038e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:02:58 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4616/6700 [2:08:51<59:05,  1.70s/it]11/17/2022 01:03:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.3923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:00 - INFO - train.train_snli_ve - loss is tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4617/6700 [2:08:53<58:41,  1.69s/it]11/17/2022 01:03:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.1451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:02 - INFO - train.train_snli_ve - loss is tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4618/6700 [2:08:54<58:53,  1.70s/it]11/17/2022 01:03:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.6556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:03 - INFO - train.train_snli_ve - loss is tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4619/6700 [2:08:56<58:42,  1.69s/it]11/17/2022 01:03:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.5609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:05 - INFO - train.train_snli_ve - loss is tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4620/6700 [2:08:58<58:22,  1.68s/it]11/17/2022 01:03:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.9596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:07 - INFO - train.train_snli_ve - loss is tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4621/6700 [2:08:59<58:21,  1.68s/it]11/17/2022 01:03:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.8214e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:08 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######8   | 4622/6700 [2:09:01<58:03,  1.68s/it]11/17/2022 01:03:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.4625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:10 - INFO - train.train_snli_ve - loss is tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4623/6700 [2:09:03<58:00,  1.68s/it]11/17/2022 01:03:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.6983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:12 - INFO - train.train_snli_ve - loss is tensor(0.7208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4624/6700 [2:09:04<58:29,  1.69s/it]11/17/2022 01:03:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:13 - INFO - train.train_snli_ve - loss is tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4625/6700 [2:09:06<58:36,  1.69s/it]11/17/2022 01:03:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:15 - INFO - train.train_snli_ve - loss is tensor(0.8531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4626/6700 [2:09:08<59:00,  1.71s/it]11/17/2022 01:03:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5204e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:17 - INFO - train.train_snli_ve - loss is tensor(0.9166, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4627/6700 [2:09:10<59:06,  1.71s/it]11/17/2022 01:03:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.7161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:19 - INFO - train.train_snli_ve - loss is tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4628/6700 [2:09:11<59:03,  1.71s/it]11/17/2022 01:03:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.3769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:20 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4629/6700 [2:09:13<58:28,  1.69s/it]11/17/2022 01:03:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:22 - INFO - train.train_snli_ve - loss is tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4630/6700 [2:09:15<58:27,  1.69s/it]11/17/2022 01:03:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.0460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:24 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4631/6700 [2:09:16<58:19,  1.69s/it]11/17/2022 01:03:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.5926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:25 - INFO - train.train_snli_ve - loss is tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4632/6700 [2:09:18<58:43,  1.70s/it]11/17/2022 01:03:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:27 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4633/6700 [2:09:20<58:25,  1.70s/it]11/17/2022 01:03:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2453e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:29 - INFO - train.train_snli_ve - loss is tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4634/6700 [2:09:21<57:54,  1.68s/it]11/17/2022 01:03:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:30 - INFO - train.train_snli_ve - loss is tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4635/6700 [2:09:23<57:34,  1.67s/it]11/17/2022 01:03:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:32 - INFO - train.train_snli_ve - loss is tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4636/6700 [2:09:25<57:38,  1.68s/it]11/17/2022 01:03:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:34 - INFO - train.train_snli_ve - loss is tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4637/6700 [2:09:26<58:01,  1.69s/it]11/17/2022 01:03:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8397e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:35 - INFO - train.train_snli_ve - loss is tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4638/6700 [2:09:28<58:12,  1.69s/it]11/17/2022 01:03:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:37 - INFO - train.train_snli_ve - loss is tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4639/6700 [2:09:30<57:32,  1.68s/it]11/17/2022 01:03:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7406e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:39 - INFO - train.train_snli_ve - loss is tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4640/6700 [2:09:31<57:38,  1.68s/it]11/17/2022 01:03:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:40 - INFO - train.train_snli_ve - loss is tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4641/6700 [2:09:33<58:04,  1.69s/it]11/17/2022 01:03:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.5157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:42 - INFO - train.train_snli_ve - loss is tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4642/6700 [2:09:35<58:10,  1.70s/it]11/17/2022 01:03:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:44 - INFO - train.train_snli_ve - loss is tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4643/6700 [2:09:37<58:09,  1.70s/it]11/17/2022 01:03:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.4788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:46 - INFO - train.train_snli_ve - loss is tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4644/6700 [2:09:38<57:39,  1.68s/it]11/17/2022 01:03:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.9712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:47 - INFO - train.train_snli_ve - loss is tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4645/6700 [2:09:40<57:33,  1.68s/it]11/17/2022 01:03:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:49 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4646/6700 [2:09:42<57:28,  1.68s/it]11/17/2022 01:03:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.9812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:51 - INFO - train.train_snli_ve - loss is tensor(0.7897, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4647/6700 [2:09:43<58:36,  1.71s/it]11/17/2022 01:03:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:52 - INFO - train.train_snli_ve - loss is tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4648/6700 [2:09:45<58:42,  1.72s/it]11/17/2022 01:03:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:54 - INFO - train.train_snli_ve - loss is tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4649/6700 [2:09:47<58:18,  1.71s/it]11/17/2022 01:03:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:56 - INFO - train.train_snli_ve - loss is tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4650/6700 [2:09:48<58:23,  1.71s/it]11/17/2022 01:03:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:57 - INFO - train.train_snli_ve - loss is tensor(0.9621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4651/6700 [2:09:50<58:04,  1.70s/it]11/17/2022 01:03:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.0966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:03:59 - INFO - train.train_snli_ve - loss is tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4652/6700 [2:09:52<58:13,  1.71s/it]11/17/2022 01:04:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:01 - INFO - train.train_snli_ve - loss is tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4653/6700 [2:09:54<57:45,  1.69s/it]11/17/2022 01:04:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:03 - INFO - train.train_snli_ve - loss is tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4654/6700 [2:09:55<57:55,  1.70s/it]11/17/2022 01:04:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.1075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:04 - INFO - train.train_snli_ve - loss is tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4655/6700 [2:09:57<58:01,  1.70s/it]11/17/2022 01:04:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.4391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:06 - INFO - train.train_snli_ve - loss is tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  69%|######9   | 4656/6700 [2:09:59<58:04,  1.70s/it]11/17/2022 01:04:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.2891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:08 - INFO - train.train_snli_ve - loss is tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4657/6700 [2:10:00<57:48,  1.70s/it]11/17/2022 01:04:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:09 - INFO - train.train_snli_ve - loss is tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4658/6700 [2:10:02<58:00,  1.70s/it]11/17/2022 01:04:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9979e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:11 - INFO - train.train_snli_ve - loss is tensor(0.5273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4659/6700 [2:10:04<57:41,  1.70s/it]11/17/2022 01:04:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6697e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:13 - INFO - train.train_snli_ve - loss is tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4660/6700 [2:10:05<57:56,  1.70s/it]11/17/2022 01:04:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.0581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:14 - INFO - train.train_snli_ve - loss is tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4661/6700 [2:10:07<57:39,  1.70s/it]11/17/2022 01:04:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.5769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:16 - INFO - train.train_snli_ve - loss is tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4662/6700 [2:10:09<57:23,  1.69s/it]11/17/2022 01:04:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.9723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:18 - INFO - train.train_snli_ve - loss is tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4663/6700 [2:10:10<57:01,  1.68s/it]11/17/2022 01:04:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.0717e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:20 - INFO - train.train_snli_ve - loss is tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4664/6700 [2:10:12<57:25,  1.69s/it]11/17/2022 01:04:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:21 - INFO - train.train_snli_ve - loss is tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4665/6700 [2:10:14<57:12,  1.69s/it]11/17/2022 01:04:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:23 - INFO - train.train_snli_ve - loss is tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4666/6700 [2:10:16<56:59,  1.68s/it]11/17/2022 01:04:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.8841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:25 - INFO - train.train_snli_ve - loss is tensor(0.4932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4667/6700 [2:10:17<57:05,  1.68s/it]11/17/2022 01:04:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:26 - INFO - train.train_snli_ve - loss is tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4668/6700 [2:10:19<56:59,  1.68s/it]11/17/2022 01:04:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:28 - INFO - train.train_snli_ve - loss is tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4669/6700 [2:10:21<56:57,  1.68s/it]11/17/2022 01:04:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:30 - INFO - train.train_snli_ve - loss is tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4670/6700 [2:10:22<57:08,  1.69s/it]11/17/2022 01:04:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.9809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:31 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4671/6700 [2:10:24<57:44,  1.71s/it]11/17/2022 01:04:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.5019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:33 - INFO - train.train_snli_ve - loss is tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4672/6700 [2:10:26<57:30,  1.70s/it]11/17/2022 01:04:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.8697e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:35 - INFO - train.train_snli_ve - loss is tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4673/6700 [2:10:27<56:52,  1.68s/it]11/17/2022 01:04:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.8539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:36 - INFO - train.train_snli_ve - loss is tensor(0.8684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4674/6700 [2:10:29<57:10,  1.69s/it]11/17/2022 01:04:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0914e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:38 - INFO - train.train_snli_ve - loss is tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4675/6700 [2:10:31<57:14,  1.70s/it]11/17/2022 01:04:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.2816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:40 - INFO - train.train_snli_ve - loss is tensor(0.8942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4676/6700 [2:10:33<57:22,  1.70s/it]11/17/2022 01:04:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.4826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:42 - INFO - train.train_snli_ve - loss is tensor(0.8998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4677/6700 [2:10:34<57:12,  1.70s/it]11/17/2022 01:04:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.4035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:43 - INFO - train.train_snli_ve - loss is tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4678/6700 [2:10:36<56:55,  1.69s/it]11/17/2022 01:04:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:45 - INFO - train.train_snli_ve - loss is tensor(0.5011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4679/6700 [2:10:38<56:38,  1.68s/it]11/17/2022 01:04:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.0543e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:47 - INFO - train.train_snli_ve - loss is tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4680/6700 [2:10:39<56:34,  1.68s/it]11/17/2022 01:04:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1230e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:48 - INFO - train.train_snli_ve - loss is tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4681/6700 [2:10:41<56:34,  1.68s/it]11/17/2022 01:04:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.3086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:50 - INFO - train.train_snli_ve - loss is tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4682/6700 [2:10:43<56:40,  1.69s/it]11/17/2022 01:04:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.8450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:52 - INFO - train.train_snli_ve - loss is tensor(0.8563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4683/6700 [2:10:44<56:26,  1.68s/it]11/17/2022 01:04:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:53 - INFO - train.train_snli_ve - loss is tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4684/6700 [2:10:46<56:14,  1.67s/it]11/17/2022 01:04:55 - INFO - train.train_snli_ve - kd_loss is tensor(4.4009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:55 - INFO - train.train_snli_ve - loss is tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4685/6700 [2:10:48<56:31,  1.68s/it]11/17/2022 01:04:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.8766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:57 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4686/6700 [2:10:49<56:44,  1.69s/it]11/17/2022 01:04:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:04:58 - INFO - train.train_snli_ve - loss is tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4687/6700 [2:10:51<56:29,  1.68s/it]11/17/2022 01:05:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:00 - INFO - train.train_snli_ve - loss is tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4688/6700 [2:10:53<56:20,  1.68s/it]11/17/2022 01:05:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.7302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:02 - INFO - train.train_snli_ve - loss is tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|######9   | 4689/6700 [2:10:54<56:17,  1.68s/it]11/17/2022 01:05:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.3604e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:03 - INFO - train.train_snli_ve - loss is tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4690/6700 [2:10:56<55:59,  1.67s/it]11/17/2022 01:05:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.4310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:05 - INFO - train.train_snli_ve - loss is tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4691/6700 [2:10:58<55:47,  1.67s/it]11/17/2022 01:05:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:07 - INFO - train.train_snli_ve - loss is tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4692/6700 [2:10:59<56:07,  1.68s/it]11/17/2022 01:05:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.8325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:08 - INFO - train.train_snli_ve - loss is tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4693/6700 [2:11:01<55:42,  1.67s/it]11/17/2022 01:05:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1294e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:10 - INFO - train.train_snli_ve - loss is tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4694/6700 [2:11:03<55:24,  1.66s/it]11/17/2022 01:05:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:12 - INFO - train.train_snli_ve - loss is tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4695/6700 [2:11:04<55:38,  1.67s/it]11/17/2022 01:05:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.5310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:13 - INFO - train.train_snli_ve - loss is tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4696/6700 [2:11:06<55:38,  1.67s/it]11/17/2022 01:05:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9849e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:15 - INFO - train.train_snli_ve - loss is tensor(0.4577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4697/6700 [2:11:08<55:05,  1.65s/it]11/17/2022 01:05:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.5996e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:17 - INFO - train.train_snli_ve - loss is tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4698/6700 [2:11:09<54:40,  1.64s/it]11/17/2022 01:05:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:18 - INFO - train.train_snli_ve - loss is tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4699/6700 [2:11:11<55:01,  1.65s/it]11/17/2022 01:05:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.0455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:20 - INFO - train.train_snli_ve - loss is tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4700/6700 [2:11:13<55:22,  1.66s/it]11/17/2022 01:05:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.6617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:22 - INFO - train.train_snli_ve - loss is tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4701/6700 [2:11:14<55:25,  1.66s/it]11/17/2022 01:05:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7566e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:23 - INFO - train.train_snli_ve - loss is tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4702/6700 [2:11:16<55:19,  1.66s/it]11/17/2022 01:05:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:25 - INFO - train.train_snli_ve - loss is tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4703/6700 [2:11:18<54:40,  1.64s/it]11/17/2022 01:05:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.7948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:26 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4704/6700 [2:11:19<54:33,  1.64s/it]11/17/2022 01:05:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.3644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:28 - INFO - train.train_snli_ve - loss is tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4705/6700 [2:11:21<54:32,  1.64s/it]11/17/2022 01:05:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.5275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:30 - INFO - train.train_snli_ve - loss is tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4706/6700 [2:11:22<54:23,  1.64s/it]11/17/2022 01:05:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:31 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4707/6700 [2:11:24<54:42,  1.65s/it]11/17/2022 01:05:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1708e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:33 - INFO - train.train_snli_ve - loss is tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4708/6700 [2:11:26<54:30,  1.64s/it]11/17/2022 01:05:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0884e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:35 - INFO - train.train_snli_ve - loss is tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4709/6700 [2:11:27<54:10,  1.63s/it]11/17/2022 01:05:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:36 - INFO - train.train_snli_ve - loss is tensor(0.7815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4710/6700 [2:11:29<54:19,  1.64s/it]11/17/2022 01:05:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:38 - INFO - train.train_snli_ve - loss is tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4711/6700 [2:11:31<54:17,  1.64s/it]11/17/2022 01:05:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3568e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:40 - INFO - train.train_snli_ve - loss is tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4712/6700 [2:11:32<54:39,  1.65s/it]11/17/2022 01:05:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:41 - INFO - train.train_snli_ve - loss is tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4713/6700 [2:11:34<54:32,  1.65s/it]11/17/2022 01:05:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:43 - INFO - train.train_snli_ve - loss is tensor(0.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4714/6700 [2:11:36<54:27,  1.65s/it]11/17/2022 01:05:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.4824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:45 - INFO - train.train_snli_ve - loss is tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4715/6700 [2:11:37<54:51,  1.66s/it]11/17/2022 01:05:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.7262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:46 - INFO - train.train_snli_ve - loss is tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4716/6700 [2:11:39<54:38,  1.65s/it]11/17/2022 01:05:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.2533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:48 - INFO - train.train_snli_ve - loss is tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4717/6700 [2:11:41<54:09,  1.64s/it]11/17/2022 01:05:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.5542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:49 - INFO - train.train_snli_ve - loss is tensor(1.1158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4718/6700 [2:11:42<54:24,  1.65s/it]11/17/2022 01:05:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.6339e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:51 - INFO - train.train_snli_ve - loss is tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4719/6700 [2:11:44<54:26,  1.65s/it]11/17/2022 01:05:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4394e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:53 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4720/6700 [2:11:45<54:08,  1.64s/it]11/17/2022 01:05:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:54 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4721/6700 [2:11:47<54:15,  1.65s/it]11/17/2022 01:05:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.6410e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:56 - INFO - train.train_snli_ve - loss is tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4722/6700 [2:11:49<54:10,  1.64s/it]11/17/2022 01:05:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.5086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:58 - INFO - train.train_snli_ve - loss is tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  70%|#######   | 4723/6700 [2:11:50<54:15,  1.65s/it]11/17/2022 01:05:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.2029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:05:59 - INFO - train.train_snli_ve - loss is tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4724/6700 [2:11:52<54:09,  1.64s/it]11/17/2022 01:06:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:01 - INFO - train.train_snli_ve - loss is tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4725/6700 [2:11:54<53:42,  1.63s/it]11/17/2022 01:06:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:03 - INFO - train.train_snli_ve - loss is tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4726/6700 [2:11:55<53:44,  1.63s/it]11/17/2022 01:06:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3022e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:04 - INFO - train.train_snli_ve - loss is tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4727/6700 [2:11:57<53:51,  1.64s/it]11/17/2022 01:06:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:06 - INFO - train.train_snli_ve - loss is tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4728/6700 [2:11:59<53:37,  1.63s/it]11/17/2022 01:06:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.4970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:08 - INFO - train.train_snli_ve - loss is tensor(0.7751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4729/6700 [2:12:00<53:50,  1.64s/it]11/17/2022 01:06:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:09 - INFO - train.train_snli_ve - loss is tensor(0.8387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4730/6700 [2:12:02<54:10,  1.65s/it]11/17/2022 01:06:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5680e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:11 - INFO - train.train_snli_ve - loss is tensor(0.8387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4731/6700 [2:12:04<53:55,  1.64s/it]11/17/2022 01:06:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:12 - INFO - train.train_snli_ve - loss is tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4732/6700 [2:12:05<53:44,  1.64s/it]11/17/2022 01:06:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.2416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:14 - INFO - train.train_snli_ve - loss is tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4733/6700 [2:12:07<53:30,  1.63s/it]11/17/2022 01:06:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:16 - INFO - train.train_snli_ve - loss is tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4734/6700 [2:12:08<53:39,  1.64s/it]11/17/2022 01:06:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.5807e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:17 - INFO - train.train_snli_ve - loss is tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4735/6700 [2:12:10<53:49,  1.64s/it]11/17/2022 01:06:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.2080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:19 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4736/6700 [2:12:12<53:40,  1.64s/it]11/17/2022 01:06:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.5462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:21 - INFO - train.train_snli_ve - loss is tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4737/6700 [2:12:13<53:42,  1.64s/it]11/17/2022 01:06:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:22 - INFO - train.train_snli_ve - loss is tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4738/6700 [2:12:15<53:26,  1.63s/it]11/17/2022 01:06:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.1164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:24 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4739/6700 [2:12:17<53:35,  1.64s/it]11/17/2022 01:06:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:26 - INFO - train.train_snli_ve - loss is tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4740/6700 [2:12:18<53:34,  1.64s/it]11/17/2022 01:06:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.1760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:27 - INFO - train.train_snli_ve - loss is tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4741/6700 [2:12:20<53:36,  1.64s/it]11/17/2022 01:06:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.7123e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:29 - INFO - train.train_snli_ve - loss is tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4742/6700 [2:12:22<53:56,  1.65s/it]11/17/2022 01:06:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.9494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:31 - INFO - train.train_snli_ve - loss is tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4743/6700 [2:12:23<53:37,  1.64s/it]11/17/2022 01:06:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.9909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:32 - INFO - train.train_snli_ve - loss is tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4744/6700 [2:12:25<53:32,  1.64s/it]11/17/2022 01:06:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1331e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:34 - INFO - train.train_snli_ve - loss is tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4745/6700 [2:12:26<53:13,  1.63s/it]11/17/2022 01:06:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.0615e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:35 - INFO - train.train_snli_ve - loss is tensor(0.5266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4746/6700 [2:12:28<52:57,  1.63s/it]11/17/2022 01:06:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.8816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:37 - INFO - train.train_snli_ve - loss is tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4747/6700 [2:12:30<53:08,  1.63s/it]11/17/2022 01:06:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.9056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:39 - INFO - train.train_snli_ve - loss is tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4748/6700 [2:12:31<53:40,  1.65s/it]11/17/2022 01:06:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.9726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:40 - INFO - train.train_snli_ve - loss is tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4749/6700 [2:12:33<53:42,  1.65s/it]11/17/2022 01:06:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.2800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:42 - INFO - train.train_snli_ve - loss is tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4750/6700 [2:12:35<54:01,  1.66s/it]11/17/2022 01:06:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.3941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:44 - INFO - train.train_snli_ve - loss is tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4751/6700 [2:12:36<54:04,  1.66s/it]11/17/2022 01:06:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:45 - INFO - train.train_snli_ve - loss is tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4752/6700 [2:12:38<54:00,  1.66s/it]11/17/2022 01:06:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6709e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:47 - INFO - train.train_snli_ve - loss is tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4753/6700 [2:12:40<53:57,  1.66s/it]11/17/2022 01:06:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:49 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4754/6700 [2:12:41<54:16,  1.67s/it]11/17/2022 01:06:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:50 - INFO - train.train_snli_ve - loss is tensor(0.7442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4755/6700 [2:12:43<53:54,  1.66s/it]11/17/2022 01:06:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:52 - INFO - train.train_snli_ve - loss is tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######   | 4756/6700 [2:12:45<54:15,  1.67s/it]11/17/2022 01:06:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.7517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:54 - INFO - train.train_snli_ve - loss is tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4757/6700 [2:12:46<54:03,  1.67s/it]11/17/2022 01:06:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:55 - INFO - train.train_snli_ve - loss is tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4758/6700 [2:12:48<54:01,  1.67s/it]11/17/2022 01:06:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3763e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:57 - INFO - train.train_snli_ve - loss is tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4759/6700 [2:12:50<53:54,  1.67s/it]11/17/2022 01:06:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.6245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:06:59 - INFO - train.train_snli_ve - loss is tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4760/6700 [2:12:51<53:48,  1.66s/it]11/17/2022 01:07:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.8352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:00 - INFO - train.train_snli_ve - loss is tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4761/6700 [2:12:53<53:40,  1.66s/it]11/17/2022 01:07:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.7422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:02 - INFO - train.train_snli_ve - loss is tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4762/6700 [2:12:55<54:24,  1.68s/it]11/17/2022 01:07:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:04 - INFO - train.train_snli_ve - loss is tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4763/6700 [2:12:56<54:28,  1.69s/it]11/17/2022 01:07:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.5200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:05 - INFO - train.train_snli_ve - loss is tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4764/6700 [2:12:58<54:05,  1.68s/it]11/17/2022 01:07:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:07 - INFO - train.train_snli_ve - loss is tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4765/6700 [2:13:00<53:55,  1.67s/it]11/17/2022 01:07:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.9387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:09 - INFO - train.train_snli_ve - loss is tensor(0.8360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4766/6700 [2:13:01<53:53,  1.67s/it]11/17/2022 01:07:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.3657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:10 - INFO - train.train_snli_ve - loss is tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4767/6700 [2:13:03<53:57,  1.67s/it]11/17/2022 01:07:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.9255e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:12 - INFO - train.train_snli_ve - loss is tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4768/6700 [2:13:05<53:50,  1.67s/it]11/17/2022 01:07:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.2791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:14 - INFO - train.train_snli_ve - loss is tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4769/6700 [2:13:06<53:46,  1.67s/it]11/17/2022 01:07:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:16 - INFO - train.train_snli_ve - loss is tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4770/6700 [2:13:08<54:03,  1.68s/it]11/17/2022 01:07:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:17 - INFO - train.train_snli_ve - loss is tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4771/6700 [2:13:10<53:44,  1.67s/it]11/17/2022 01:07:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.6069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:19 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4772/6700 [2:13:12<53:45,  1.67s/it]11/17/2022 01:07:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.9827e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:21 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4773/6700 [2:13:13<53:51,  1.68s/it]11/17/2022 01:07:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.6501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:22 - INFO - train.train_snli_ve - loss is tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4774/6700 [2:13:15<53:28,  1.67s/it]11/17/2022 01:07:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.5715e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:24 - INFO - train.train_snli_ve - loss is tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4775/6700 [2:13:17<53:47,  1.68s/it]11/17/2022 01:07:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.7381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:26 - INFO - train.train_snli_ve - loss is tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4776/6700 [2:13:18<53:56,  1.68s/it]11/17/2022 01:07:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.3258e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:27 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4777/6700 [2:13:20<53:54,  1.68s/it]11/17/2022 01:07:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:29 - INFO - train.train_snli_ve - loss is tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4778/6700 [2:13:22<53:57,  1.68s/it]11/17/2022 01:07:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.9730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:31 - INFO - train.train_snli_ve - loss is tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4779/6700 [2:13:23<54:14,  1.69s/it]11/17/2022 01:07:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.9232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:32 - INFO - train.train_snli_ve - loss is tensor(0.8230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4780/6700 [2:13:25<53:42,  1.68s/it]11/17/2022 01:07:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.5455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:34 - INFO - train.train_snli_ve - loss is tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4781/6700 [2:13:27<54:02,  1.69s/it]11/17/2022 01:07:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:36 - INFO - train.train_snli_ve - loss is tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4782/6700 [2:13:28<54:21,  1.70s/it]11/17/2022 01:07:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:37 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4783/6700 [2:13:30<54:23,  1.70s/it]11/17/2022 01:07:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.8388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:39 - INFO - train.train_snli_ve - loss is tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4784/6700 [2:13:32<54:29,  1.71s/it]11/17/2022 01:07:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:41 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4785/6700 [2:13:34<54:01,  1.69s/it]11/17/2022 01:07:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9422e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:43 - INFO - train.train_snli_ve - loss is tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4786/6700 [2:13:35<54:10,  1.70s/it]11/17/2022 01:07:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.5219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:44 - INFO - train.train_snli_ve - loss is tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4787/6700 [2:13:37<54:42,  1.72s/it]11/17/2022 01:07:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.7380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:46 - INFO - train.train_snli_ve - loss is tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4788/6700 [2:13:39<53:59,  1.69s/it]11/17/2022 01:07:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.5146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:48 - INFO - train.train_snli_ve - loss is tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4789/6700 [2:13:40<54:12,  1.70s/it]11/17/2022 01:07:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:49 - INFO - train.train_snli_ve - loss is tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  71%|#######1  | 4790/6700 [2:13:42<53:47,  1.69s/it]11/17/2022 01:07:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.1336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:51 - INFO - train.train_snli_ve - loss is tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4791/6700 [2:13:44<53:43,  1.69s/it]11/17/2022 01:07:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:53 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4792/6700 [2:13:45<53:19,  1.68s/it]11/17/2022 01:07:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:54 - INFO - train.train_snli_ve - loss is tensor(0.5428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4793/6700 [2:13:47<52:44,  1.66s/it]11/17/2022 01:07:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.3606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:56 - INFO - train.train_snli_ve - loss is tensor(0.8748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4794/6700 [2:13:49<52:31,  1.65s/it]11/17/2022 01:07:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2255e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:58 - INFO - train.train_snli_ve - loss is tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4795/6700 [2:13:50<53:02,  1.67s/it]11/17/2022 01:07:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9877e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:07:59 - INFO - train.train_snli_ve - loss is tensor(0.8280, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4796/6700 [2:13:52<53:06,  1.67s/it]11/17/2022 01:08:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5549e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:01 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4797/6700 [2:13:54<52:58,  1.67s/it]11/17/2022 01:08:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:03 - INFO - train.train_snli_ve - loss is tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4798/6700 [2:13:55<53:08,  1.68s/it]11/17/2022 01:08:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.2430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:04 - INFO - train.train_snli_ve - loss is tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4799/6700 [2:13:57<53:11,  1.68s/it]11/17/2022 01:08:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:06 - INFO - train.train_snli_ve - loss is tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4800/6700 [2:13:59<53:29,  1.69s/it]11/17/2022 01:08:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:08 - INFO - train.train_snli_ve - loss is tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4801/6700 [2:14:00<53:53,  1.70s/it]11/17/2022 01:08:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.5140e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:09 - INFO - train.train_snli_ve - loss is tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4802/6700 [2:14:02<53:51,  1.70s/it]11/17/2022 01:08:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:11 - INFO - train.train_snli_ve - loss is tensor(0.9094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4803/6700 [2:14:04<53:30,  1.69s/it]11/17/2022 01:08:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0526e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:13 - INFO - train.train_snli_ve - loss is tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4804/6700 [2:14:06<53:45,  1.70s/it]11/17/2022 01:08:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.5766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:15 - INFO - train.train_snli_ve - loss is tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4805/6700 [2:14:07<53:21,  1.69s/it]11/17/2022 01:08:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3450e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:16 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4806/6700 [2:14:09<53:00,  1.68s/it]11/17/2022 01:08:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.2848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:18 - INFO - train.train_snli_ve - loss is tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4807/6700 [2:14:11<53:17,  1.69s/it]11/17/2022 01:08:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:20 - INFO - train.train_snli_ve - loss is tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4808/6700 [2:14:12<53:18,  1.69s/it]11/17/2022 01:08:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.6915e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:21 - INFO - train.train_snli_ve - loss is tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4809/6700 [2:14:14<53:07,  1.69s/it]11/17/2022 01:08:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.6671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:23 - INFO - train.train_snli_ve - loss is tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4810/6700 [2:14:16<53:13,  1.69s/it]11/17/2022 01:08:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:25 - INFO - train.train_snli_ve - loss is tensor(0.9439, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4811/6700 [2:14:17<53:17,  1.69s/it]11/17/2022 01:08:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:26 - INFO - train.train_snli_ve - loss is tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4812/6700 [2:14:19<53:15,  1.69s/it]11/17/2022 01:08:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.3611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:28 - INFO - train.train_snli_ve - loss is tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4813/6700 [2:14:21<53:40,  1.71s/it]11/17/2022 01:08:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:30 - INFO - train.train_snli_ve - loss is tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4814/6700 [2:14:22<53:30,  1.70s/it]11/17/2022 01:08:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.6283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:31 - INFO - train.train_snli_ve - loss is tensor(0.4728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4815/6700 [2:14:24<53:33,  1.70s/it]11/17/2022 01:08:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:33 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4816/6700 [2:14:26<53:16,  1.70s/it]11/17/2022 01:08:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4053e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:35 - INFO - train.train_snli_ve - loss is tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4817/6700 [2:14:28<52:59,  1.69s/it]11/17/2022 01:08:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.5308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:36 - INFO - train.train_snli_ve - loss is tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4818/6700 [2:14:29<52:32,  1.67s/it]11/17/2022 01:08:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.9905e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:38 - INFO - train.train_snli_ve - loss is tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4819/6700 [2:14:31<52:23,  1.67s/it]11/17/2022 01:08:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:40 - INFO - train.train_snli_ve - loss is tensor(0.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4820/6700 [2:14:33<52:43,  1.68s/it]11/17/2022 01:08:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.4055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:42 - INFO - train.train_snli_ve - loss is tensor(0.8025, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4821/6700 [2:14:34<52:40,  1.68s/it]11/17/2022 01:08:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.1261e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:43 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4822/6700 [2:14:36<52:53,  1.69s/it]11/17/2022 01:08:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:45 - INFO - train.train_snli_ve - loss is tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######1  | 4823/6700 [2:14:38<52:42,  1.68s/it]11/17/2022 01:08:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:47 - INFO - train.train_snli_ve - loss is tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4824/6700 [2:14:39<53:11,  1.70s/it]11/17/2022 01:08:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.2207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:48 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4825/6700 [2:14:41<53:14,  1.70s/it]11/17/2022 01:08:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.2354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:50 - INFO - train.train_snli_ve - loss is tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4826/6700 [2:14:43<52:56,  1.70s/it]11/17/2022 01:08:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.4504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:52 - INFO - train.train_snli_ve - loss is tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4827/6700 [2:14:45<53:37,  1.72s/it]11/17/2022 01:08:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:54 - INFO - train.train_snli_ve - loss is tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4828/6700 [2:14:46<53:28,  1.71s/it]11/17/2022 01:08:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.1694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:55 - INFO - train.train_snli_ve - loss is tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4829/6700 [2:14:48<52:58,  1.70s/it]11/17/2022 01:08:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1483e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:57 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4830/6700 [2:14:50<52:30,  1.68s/it]11/17/2022 01:08:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:08:59 - INFO - train.train_snli_ve - loss is tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4831/6700 [2:14:51<52:45,  1.69s/it]11/17/2022 01:09:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.5295e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:00 - INFO - train.train_snli_ve - loss is tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4832/6700 [2:14:53<52:19,  1.68s/it]11/17/2022 01:09:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:02 - INFO - train.train_snli_ve - loss is tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4833/6700 [2:14:55<52:26,  1.69s/it]11/17/2022 01:09:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.1978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:04 - INFO - train.train_snli_ve - loss is tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4834/6700 [2:14:56<52:28,  1.69s/it]11/17/2022 01:09:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.3350e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:05 - INFO - train.train_snli_ve - loss is tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4835/6700 [2:14:58<51:57,  1.67s/it]11/17/2022 01:09:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:07 - INFO - train.train_snli_ve - loss is tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4836/6700 [2:15:00<51:44,  1.67s/it]11/17/2022 01:09:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:09 - INFO - train.train_snli_ve - loss is tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4837/6700 [2:15:01<52:11,  1.68s/it]11/17/2022 01:09:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.2109e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:10 - INFO - train.train_snli_ve - loss is tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4838/6700 [2:15:03<52:33,  1.69s/it]11/17/2022 01:09:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.3528e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:12 - INFO - train.train_snli_ve - loss is tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4839/6700 [2:15:05<52:22,  1.69s/it]11/17/2022 01:09:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:14 - INFO - train.train_snli_ve - loss is tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4840/6700 [2:15:06<52:05,  1.68s/it]11/17/2022 01:09:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.1334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:15 - INFO - train.train_snli_ve - loss is tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4841/6700 [2:15:08<52:00,  1.68s/it]11/17/2022 01:09:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.2952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:17 - INFO - train.train_snli_ve - loss is tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4842/6700 [2:15:10<52:09,  1.68s/it]11/17/2022 01:09:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.5379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:19 - INFO - train.train_snli_ve - loss is tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4843/6700 [2:15:11<51:43,  1.67s/it]11/17/2022 01:09:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:20 - INFO - train.train_snli_ve - loss is tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4844/6700 [2:15:13<51:23,  1.66s/it]11/17/2022 01:09:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:22 - INFO - train.train_snli_ve - loss is tensor(0.7685, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4845/6700 [2:15:15<51:26,  1.66s/it]11/17/2022 01:09:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.9264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:24 - INFO - train.train_snli_ve - loss is tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4846/6700 [2:15:16<51:38,  1.67s/it]11/17/2022 01:09:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:25 - INFO - train.train_snli_ve - loss is tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4847/6700 [2:15:18<51:35,  1.67s/it]11/17/2022 01:09:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.2009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:27 - INFO - train.train_snli_ve - loss is tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4848/6700 [2:15:20<51:51,  1.68s/it]11/17/2022 01:09:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.7076e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:29 - INFO - train.train_snli_ve - loss is tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4849/6700 [2:15:21<52:06,  1.69s/it]11/17/2022 01:09:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.3844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:30 - INFO - train.train_snli_ve - loss is tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4850/6700 [2:15:23<52:44,  1.71s/it]11/17/2022 01:09:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:32 - INFO - train.train_snli_ve - loss is tensor(0.5404, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4851/6700 [2:15:25<52:22,  1.70s/it]11/17/2022 01:09:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.6574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:34 - INFO - train.train_snli_ve - loss is tensor(0.9189, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4852/6700 [2:15:27<52:28,  1.70s/it]11/17/2022 01:09:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.9347e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:36 - INFO - train.train_snli_ve - loss is tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4853/6700 [2:15:28<52:06,  1.69s/it]11/17/2022 01:09:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.1527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:37 - INFO - train.train_snli_ve - loss is tensor(0.5161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4854/6700 [2:15:30<52:01,  1.69s/it]11/17/2022 01:09:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.6837e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:39 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4855/6700 [2:15:32<52:12,  1.70s/it]11/17/2022 01:09:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.8529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:41 - INFO - train.train_snli_ve - loss is tensor(0.9084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4856/6700 [2:15:33<52:26,  1.71s/it]11/17/2022 01:09:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:42 - INFO - train.train_snli_ve - loss is tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  72%|#######2  | 4857/6700 [2:15:35<52:03,  1.69s/it]11/17/2022 01:09:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:44 - INFO - train.train_snli_ve - loss is tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4858/6700 [2:15:37<52:01,  1.69s/it]11/17/2022 01:09:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.5980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:46 - INFO - train.train_snli_ve - loss is tensor(0.5510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4859/6700 [2:15:38<51:25,  1.68s/it]11/17/2022 01:09:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.9047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:47 - INFO - train.train_snli_ve - loss is tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4860/6700 [2:15:40<51:40,  1.69s/it]11/17/2022 01:09:49 - INFO - train.train_snli_ve - kd_loss is tensor(8.6640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:49 - INFO - train.train_snli_ve - loss is tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4861/6700 [2:15:42<51:39,  1.69s/it]11/17/2022 01:09:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.7274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:51 - INFO - train.train_snli_ve - loss is tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4862/6700 [2:15:43<51:29,  1.68s/it]11/17/2022 01:09:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.6035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:52 - INFO - train.train_snli_ve - loss is tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4863/6700 [2:15:45<52:02,  1.70s/it]11/17/2022 01:09:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:54 - INFO - train.train_snli_ve - loss is tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4864/6700 [2:15:47<51:39,  1.69s/it]11/17/2022 01:09:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.5623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:56 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4865/6700 [2:15:48<51:15,  1.68s/it]11/17/2022 01:09:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:57 - INFO - train.train_snli_ve - loss is tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4866/6700 [2:15:50<51:30,  1.68s/it]11/17/2022 01:09:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:09:59 - INFO - train.train_snli_ve - loss is tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4867/6700 [2:15:52<50:58,  1.67s/it]11/17/2022 01:10:01 - INFO - train.train_snli_ve - kd_loss is tensor(5.2424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:01 - INFO - train.train_snli_ve - loss is tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4868/6700 [2:15:53<50:49,  1.66s/it]11/17/2022 01:10:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.1032e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:02 - INFO - train.train_snli_ve - loss is tensor(0.4653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4869/6700 [2:15:55<51:07,  1.68s/it]11/17/2022 01:10:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.7792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:04 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4870/6700 [2:15:57<50:58,  1.67s/it]11/17/2022 01:10:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5584e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:06 - INFO - train.train_snli_ve - loss is tensor(0.8219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4871/6700 [2:15:59<50:58,  1.67s/it]11/17/2022 01:10:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.2018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:08 - INFO - train.train_snli_ve - loss is tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4872/6700 [2:16:00<50:57,  1.67s/it]11/17/2022 01:10:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.5291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:09 - INFO - train.train_snli_ve - loss is tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4873/6700 [2:16:02<51:20,  1.69s/it]11/17/2022 01:10:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:11 - INFO - train.train_snli_ve - loss is tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4874/6700 [2:16:04<51:34,  1.69s/it]11/17/2022 01:10:13 - INFO - train.train_snli_ve - kd_loss is tensor(5.4484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:13 - INFO - train.train_snli_ve - loss is tensor(0.8611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4875/6700 [2:16:05<51:12,  1.68s/it]11/17/2022 01:10:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.5583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:14 - INFO - train.train_snli_ve - loss is tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4876/6700 [2:16:07<51:34,  1.70s/it]11/17/2022 01:10:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.8498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:16 - INFO - train.train_snli_ve - loss is tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4877/6700 [2:16:09<51:18,  1.69s/it]11/17/2022 01:10:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6658e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:18 - INFO - train.train_snli_ve - loss is tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4878/6700 [2:16:10<51:13,  1.69s/it]11/17/2022 01:10:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.8585e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:19 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4879/6700 [2:16:12<51:00,  1.68s/it]11/17/2022 01:10:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:21 - INFO - train.train_snli_ve - loss is tensor(0.5243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4880/6700 [2:16:14<51:08,  1.69s/it]11/17/2022 01:10:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:23 - INFO - train.train_snli_ve - loss is tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4881/6700 [2:16:15<51:19,  1.69s/it]11/17/2022 01:10:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.1804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:24 - INFO - train.train_snli_ve - loss is tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4882/6700 [2:16:17<51:33,  1.70s/it]11/17/2022 01:10:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:26 - INFO - train.train_snli_ve - loss is tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4883/6700 [2:16:19<51:47,  1.71s/it]11/17/2022 01:10:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.8711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:28 - INFO - train.train_snli_ve - loss is tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4884/6700 [2:16:21<51:54,  1.71s/it]11/17/2022 01:10:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.4832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:30 - INFO - train.train_snli_ve - loss is tensor(0.8452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4885/6700 [2:16:22<51:22,  1.70s/it]11/17/2022 01:10:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:31 - INFO - train.train_snli_ve - loss is tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4886/6700 [2:16:24<50:47,  1.68s/it]11/17/2022 01:10:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.5670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:33 - INFO - train.train_snli_ve - loss is tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4887/6700 [2:16:26<51:17,  1.70s/it]11/17/2022 01:10:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:35 - INFO - train.train_snli_ve - loss is tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4888/6700 [2:16:27<51:10,  1.69s/it]11/17/2022 01:10:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.7155e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:36 - INFO - train.train_snli_ve - loss is tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4889/6700 [2:16:29<50:58,  1.69s/it]11/17/2022 01:10:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:38 - INFO - train.train_snli_ve - loss is tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######2  | 4890/6700 [2:16:31<51:17,  1.70s/it]11/17/2022 01:10:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.4444e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:40 - INFO - train.train_snli_ve - loss is tensor(0.5782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4891/6700 [2:16:32<50:36,  1.68s/it]11/17/2022 01:10:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:41 - INFO - train.train_snli_ve - loss is tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4892/6700 [2:16:34<50:35,  1.68s/it]11/17/2022 01:10:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:43 - INFO - train.train_snli_ve - loss is tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4893/6700 [2:16:36<50:27,  1.68s/it]11/17/2022 01:10:45 - INFO - train.train_snli_ve - kd_loss is tensor(5.0866e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:45 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4894/6700 [2:16:37<51:03,  1.70s/it]11/17/2022 01:10:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.1431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:46 - INFO - train.train_snli_ve - loss is tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4895/6700 [2:16:39<50:44,  1.69s/it]11/17/2022 01:10:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.0321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:48 - INFO - train.train_snli_ve - loss is tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4896/6700 [2:16:41<50:19,  1.67s/it]11/17/2022 01:10:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.5042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:50 - INFO - train.train_snli_ve - loss is tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4897/6700 [2:16:42<50:26,  1.68s/it]11/17/2022 01:10:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.0277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:51 - INFO - train.train_snli_ve - loss is tensor(0.4767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4898/6700 [2:16:44<50:42,  1.69s/it]11/17/2022 01:10:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.1736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:53 - INFO - train.train_snli_ve - loss is tensor(0.8276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4899/6700 [2:16:46<50:14,  1.67s/it]11/17/2022 01:10:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.6616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:55 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4900/6700 [2:16:48<50:45,  1.69s/it]11/17/2022 01:10:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:57 - INFO - train.train_snli_ve - loss is tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4901/6700 [2:16:49<50:26,  1.68s/it]11/17/2022 01:10:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.6900e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:10:58 - INFO - train.train_snli_ve - loss is tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4902/6700 [2:16:51<50:09,  1.67s/it]11/17/2022 01:11:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4119e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:00 - INFO - train.train_snli_ve - loss is tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4903/6700 [2:16:53<50:16,  1.68s/it]11/17/2022 01:11:02 - INFO - train.train_snli_ve - kd_loss is tensor(6.2082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:02 - INFO - train.train_snli_ve - loss is tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4904/6700 [2:16:54<50:09,  1.68s/it]11/17/2022 01:11:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:03 - INFO - train.train_snli_ve - loss is tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4905/6700 [2:16:56<49:58,  1.67s/it]11/17/2022 01:11:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.7803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:05 - INFO - train.train_snli_ve - loss is tensor(0.7653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4906/6700 [2:16:58<49:48,  1.67s/it]11/17/2022 01:11:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.1031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:06 - INFO - train.train_snli_ve - loss is tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4907/6700 [2:16:59<49:44,  1.66s/it]11/17/2022 01:11:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:08 - INFO - train.train_snli_ve - loss is tensor(0.8733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4908/6700 [2:17:01<49:46,  1.67s/it]11/17/2022 01:11:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.5256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:10 - INFO - train.train_snli_ve - loss is tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4909/6700 [2:17:03<49:58,  1.67s/it]11/17/2022 01:11:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.0370e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:12 - INFO - train.train_snli_ve - loss is tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4910/6700 [2:17:04<49:47,  1.67s/it]11/17/2022 01:11:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.4266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:13 - INFO - train.train_snli_ve - loss is tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4911/6700 [2:17:06<49:52,  1.67s/it]11/17/2022 01:11:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.0457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:15 - INFO - train.train_snli_ve - loss is tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4912/6700 [2:17:08<50:05,  1.68s/it]11/17/2022 01:11:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.0264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:17 - INFO - train.train_snli_ve - loss is tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4913/6700 [2:17:09<49:43,  1.67s/it]11/17/2022 01:11:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:18 - INFO - train.train_snli_ve - loss is tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4914/6700 [2:17:11<50:10,  1.69s/it]11/17/2022 01:11:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:20 - INFO - train.train_snli_ve - loss is tensor(0.8416, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4915/6700 [2:17:13<50:09,  1.69s/it]11/17/2022 01:11:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7580e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:22 - INFO - train.train_snli_ve - loss is tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4916/6700 [2:17:14<49:57,  1.68s/it]11/17/2022 01:11:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:23 - INFO - train.train_snli_ve - loss is tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4917/6700 [2:17:16<50:43,  1.71s/it]11/17/2022 01:11:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.6858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:25 - INFO - train.train_snli_ve - loss is tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4918/6700 [2:17:18<50:29,  1.70s/it]11/17/2022 01:11:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.2742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:27 - INFO - train.train_snli_ve - loss is tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4919/6700 [2:17:19<50:22,  1.70s/it]11/17/2022 01:11:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.0875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:28 - INFO - train.train_snli_ve - loss is tensor(0.5837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4920/6700 [2:17:21<50:16,  1.69s/it]11/17/2022 01:11:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:30 - INFO - train.train_snli_ve - loss is tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4921/6700 [2:17:23<49:59,  1.69s/it]11/17/2022 01:11:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:32 - INFO - train.train_snli_ve - loss is tensor(0.8693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4922/6700 [2:17:25<50:11,  1.69s/it]11/17/2022 01:11:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.1814e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:34 - INFO - train.train_snli_ve - loss is tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4923/6700 [2:17:26<50:11,  1.69s/it]11/17/2022 01:11:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:35 - INFO - train.train_snli_ve - loss is tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  73%|#######3  | 4924/6700 [2:17:28<50:04,  1.69s/it]11/17/2022 01:11:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.8194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:37 - INFO - train.train_snli_ve - loss is tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4925/6700 [2:17:30<50:07,  1.69s/it]11/17/2022 01:11:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.4586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:39 - INFO - train.train_snli_ve - loss is tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4926/6700 [2:17:31<50:20,  1.70s/it]11/17/2022 01:11:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.9609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:40 - INFO - train.train_snli_ve - loss is tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4927/6700 [2:17:33<50:03,  1.69s/it]11/17/2022 01:11:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.7329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:42 - INFO - train.train_snli_ve - loss is tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4928/6700 [2:17:35<50:08,  1.70s/it]11/17/2022 01:11:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.3930e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:44 - INFO - train.train_snli_ve - loss is tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4929/6700 [2:17:36<49:55,  1.69s/it]11/17/2022 01:11:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.5866e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:45 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4930/6700 [2:17:38<49:32,  1.68s/it]11/17/2022 01:11:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.2256e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:47 - INFO - train.train_snli_ve - loss is tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4931/6700 [2:17:40<49:40,  1.68s/it]11/17/2022 01:11:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8115e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:49 - INFO - train.train_snli_ve - loss is tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4932/6700 [2:17:41<49:47,  1.69s/it]11/17/2022 01:11:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.5359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:50 - INFO - train.train_snli_ve - loss is tensor(0.9459, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4933/6700 [2:17:43<49:32,  1.68s/it]11/17/2022 01:11:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:52 - INFO - train.train_snli_ve - loss is tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4934/6700 [2:17:45<49:28,  1.68s/it]11/17/2022 01:11:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.8631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:54 - INFO - train.train_snli_ve - loss is tensor(0.7301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4935/6700 [2:17:46<49:13,  1.67s/it]11/17/2022 01:11:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:55 - INFO - train.train_snli_ve - loss is tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4936/6700 [2:17:48<49:20,  1.68s/it]11/17/2022 01:11:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:57 - INFO - train.train_snli_ve - loss is tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4937/6700 [2:17:50<49:27,  1.68s/it]11/17/2022 01:11:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.0054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:11:59 - INFO - train.train_snli_ve - loss is tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4938/6700 [2:17:51<49:27,  1.68s/it]11/17/2022 01:12:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:00 - INFO - train.train_snli_ve - loss is tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4939/6700 [2:17:53<48:59,  1.67s/it]11/17/2022 01:12:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0796e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:02 - INFO - train.train_snli_ve - loss is tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4940/6700 [2:17:55<49:10,  1.68s/it]11/17/2022 01:12:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:04 - INFO - train.train_snli_ve - loss is tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4941/6700 [2:17:57<49:15,  1.68s/it]11/17/2022 01:12:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:06 - INFO - train.train_snli_ve - loss is tensor(0.8498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4942/6700 [2:17:58<49:22,  1.68s/it]11/17/2022 01:12:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:07 - INFO - train.train_snli_ve - loss is tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4943/6700 [2:18:00<49:20,  1.68s/it]11/17/2022 01:12:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:09 - INFO - train.train_snli_ve - loss is tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4944/6700 [2:18:02<49:04,  1.68s/it]11/17/2022 01:12:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:11 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4945/6700 [2:18:03<48:49,  1.67s/it]11/17/2022 01:12:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.3840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:12 - INFO - train.train_snli_ve - loss is tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4946/6700 [2:18:05<49:01,  1.68s/it]11/17/2022 01:12:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.4504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:14 - INFO - train.train_snli_ve - loss is tensor(0.9924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4947/6700 [2:18:07<48:50,  1.67s/it]11/17/2022 01:12:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.7572e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:16 - INFO - train.train_snli_ve - loss is tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4948/6700 [2:18:08<48:51,  1.67s/it]11/17/2022 01:12:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:17 - INFO - train.train_snli_ve - loss is tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4949/6700 [2:18:10<48:33,  1.66s/it]11/17/2022 01:12:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:19 - INFO - train.train_snli_ve - loss is tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4950/6700 [2:18:12<48:40,  1.67s/it]11/17/2022 01:12:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:21 - INFO - train.train_snli_ve - loss is tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4951/6700 [2:18:13<48:59,  1.68s/it]11/17/2022 01:12:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.7753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:22 - INFO - train.train_snli_ve - loss is tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4952/6700 [2:18:15<49:00,  1.68s/it]11/17/2022 01:12:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.3936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:24 - INFO - train.train_snli_ve - loss is tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4953/6700 [2:18:17<48:43,  1.67s/it]11/17/2022 01:12:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:26 - INFO - train.train_snli_ve - loss is tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4954/6700 [2:18:18<48:46,  1.68s/it]11/17/2022 01:12:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:27 - INFO - train.train_snli_ve - loss is tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4955/6700 [2:18:20<48:32,  1.67s/it]11/17/2022 01:12:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.6529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:29 - INFO - train.train_snli_ve - loss is tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4956/6700 [2:18:22<48:31,  1.67s/it]11/17/2022 01:12:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.9440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:31 - INFO - train.train_snli_ve - loss is tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######3  | 4957/6700 [2:18:23<48:36,  1.67s/it]11/17/2022 01:12:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.2246e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:32 - INFO - train.train_snli_ve - loss is tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4958/6700 [2:18:25<48:30,  1.67s/it]11/17/2022 01:12:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.8425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:34 - INFO - train.train_snli_ve - loss is tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4959/6700 [2:18:27<48:40,  1.68s/it]11/17/2022 01:12:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.1662e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:36 - INFO - train.train_snli_ve - loss is tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4960/6700 [2:18:28<48:56,  1.69s/it]11/17/2022 01:12:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.8226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:37 - INFO - train.train_snli_ve - loss is tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4961/6700 [2:18:30<48:54,  1.69s/it]11/17/2022 01:12:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:39 - INFO - train.train_snli_ve - loss is tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4962/6700 [2:18:32<49:13,  1.70s/it]11/17/2022 01:12:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.4291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:41 - INFO - train.train_snli_ve - loss is tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4963/6700 [2:18:33<48:50,  1.69s/it]11/17/2022 01:12:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8382e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:42 - INFO - train.train_snli_ve - loss is tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4964/6700 [2:18:35<48:45,  1.69s/it]11/17/2022 01:12:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.9525e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:44 - INFO - train.train_snli_ve - loss is tensor(0.5502, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4965/6700 [2:18:37<48:55,  1.69s/it]11/17/2022 01:12:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.3412e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:46 - INFO - train.train_snli_ve - loss is tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4966/6700 [2:18:39<48:57,  1.69s/it]11/17/2022 01:12:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.4432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:47 - INFO - train.train_snli_ve - loss is tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4967/6700 [2:18:40<48:31,  1.68s/it]11/17/2022 01:12:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.0196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:49 - INFO - train.train_snli_ve - loss is tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4968/6700 [2:18:42<48:41,  1.69s/it]11/17/2022 01:12:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.9270e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:51 - INFO - train.train_snli_ve - loss is tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4969/6700 [2:18:44<48:32,  1.68s/it]11/17/2022 01:12:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:52 - INFO - train.train_snli_ve - loss is tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4970/6700 [2:18:45<47:54,  1.66s/it]11/17/2022 01:12:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.7703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:54 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4971/6700 [2:18:47<48:14,  1.67s/it]11/17/2022 01:12:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:56 - INFO - train.train_snli_ve - loss is tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4972/6700 [2:18:49<48:09,  1.67s/it]11/17/2022 01:12:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0300e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:58 - INFO - train.train_snli_ve - loss is tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4973/6700 [2:18:50<48:01,  1.67s/it]11/17/2022 01:12:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.3337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:12:59 - INFO - train.train_snli_ve - loss is tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4974/6700 [2:18:52<48:27,  1.68s/it]11/17/2022 01:13:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.2923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:01 - INFO - train.train_snli_ve - loss is tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4975/6700 [2:18:54<48:23,  1.68s/it]11/17/2022 01:13:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:03 - INFO - train.train_snli_ve - loss is tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4976/6700 [2:18:55<49:13,  1.71s/it]11/17/2022 01:13:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:04 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4977/6700 [2:18:57<49:30,  1.72s/it]11/17/2022 01:13:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:06 - INFO - train.train_snli_ve - loss is tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4978/6700 [2:18:59<48:36,  1.69s/it]11/17/2022 01:13:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5140e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:08 - INFO - train.train_snli_ve - loss is tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4979/6700 [2:19:00<48:18,  1.68s/it]11/17/2022 01:13:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.0776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:09 - INFO - train.train_snli_ve - loss is tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4980/6700 [2:19:02<48:27,  1.69s/it]11/17/2022 01:13:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.4022e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:11 - INFO - train.train_snli_ve - loss is tensor(0.4543, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4981/6700 [2:19:04<48:07,  1.68s/it]11/17/2022 01:13:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.1120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:13 - INFO - train.train_snli_ve - loss is tensor(0.9002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4982/6700 [2:19:05<48:14,  1.68s/it]11/17/2022 01:13:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.9939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:14 - INFO - train.train_snli_ve - loss is tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4983/6700 [2:19:07<48:05,  1.68s/it]11/17/2022 01:13:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:16 - INFO - train.train_snli_ve - loss is tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4984/6700 [2:19:09<48:52,  1.71s/it]11/17/2022 01:13:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.5019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:18 - INFO - train.train_snli_ve - loss is tensor(0.7798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4985/6700 [2:19:11<48:53,  1.71s/it]11/17/2022 01:13:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.6871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:20 - INFO - train.train_snli_ve - loss is tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4986/6700 [2:19:12<48:53,  1.71s/it]11/17/2022 01:13:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.3462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:21 - INFO - train.train_snli_ve - loss is tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4987/6700 [2:19:14<48:21,  1.69s/it]11/17/2022 01:13:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7853e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:23 - INFO - train.train_snli_ve - loss is tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4988/6700 [2:19:16<48:14,  1.69s/it]11/17/2022 01:13:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.1771e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:25 - INFO - train.train_snli_ve - loss is tensor(0.9812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4989/6700 [2:19:17<48:00,  1.68s/it]11/17/2022 01:13:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.8273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:26 - INFO - train.train_snli_ve - loss is tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4990/6700 [2:19:19<47:57,  1.68s/it]11/17/2022 01:13:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.3533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:28 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  74%|#######4  | 4991/6700 [2:19:21<47:53,  1.68s/it]11/17/2022 01:13:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9807e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:30 - INFO - train.train_snli_ve - loss is tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4992/6700 [2:19:22<47:53,  1.68s/it]11/17/2022 01:13:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.2907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:31 - INFO - train.train_snli_ve - loss is tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4993/6700 [2:19:24<47:41,  1.68s/it]11/17/2022 01:13:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:33 - INFO - train.train_snli_ve - loss is tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4994/6700 [2:19:26<47:43,  1.68s/it]11/17/2022 01:13:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.5506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:35 - INFO - train.train_snli_ve - loss is tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4995/6700 [2:19:27<47:50,  1.68s/it]11/17/2022 01:13:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.5249e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:36 - INFO - train.train_snli_ve - loss is tensor(0.7623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4996/6700 [2:19:29<48:00,  1.69s/it]11/17/2022 01:13:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:38 - INFO - train.train_snli_ve - loss is tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4997/6700 [2:19:31<47:53,  1.69s/it]11/17/2022 01:13:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:40 - INFO - train.train_snli_ve - loss is tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4998/6700 [2:19:33<47:53,  1.69s/it]11/17/2022 01:13:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9585e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:41 - INFO - train.train_snli_ve - loss is tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 4999/6700 [2:19:34<47:49,  1.69s/it]11/17/2022 01:13:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:43 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5000/6700 [2:19:36<48:02,  1.70s/it]11/17/2022 01:13:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3816e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:45 - INFO - train.train_snli_ve - loss is tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5001/6700 [2:19:38<47:53,  1.69s/it]11/17/2022 01:13:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.0598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:47 - INFO - train.train_snli_ve - loss is tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5002/6700 [2:19:39<47:31,  1.68s/it]11/17/2022 01:13:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1645e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:48 - INFO - train.train_snli_ve - loss is tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5003/6700 [2:19:41<47:16,  1.67s/it]11/17/2022 01:13:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7876e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:50 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5004/6700 [2:19:43<47:24,  1.68s/it]11/17/2022 01:13:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:52 - INFO - train.train_snli_ve - loss is tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5005/6700 [2:19:44<47:13,  1.67s/it]11/17/2022 01:13:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.8854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:53 - INFO - train.train_snli_ve - loss is tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5006/6700 [2:19:46<47:16,  1.67s/it]11/17/2022 01:13:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.7637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:55 - INFO - train.train_snli_ve - loss is tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5007/6700 [2:19:48<47:42,  1.69s/it]11/17/2022 01:13:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.1180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:57 - INFO - train.train_snli_ve - loss is tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5008/6700 [2:19:49<47:58,  1.70s/it]11/17/2022 01:13:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.7660e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:13:58 - INFO - train.train_snli_ve - loss is tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5009/6700 [2:19:51<47:41,  1.69s/it]11/17/2022 01:14:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.3098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:00 - INFO - train.train_snli_ve - loss is tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5010/6700 [2:19:53<47:34,  1.69s/it]11/17/2022 01:14:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:02 - INFO - train.train_snli_ve - loss is tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5011/6700 [2:19:54<47:18,  1.68s/it]11/17/2022 01:14:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.1765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:03 - INFO - train.train_snli_ve - loss is tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5012/6700 [2:19:56<47:22,  1.68s/it]11/17/2022 01:14:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4448e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:05 - INFO - train.train_snli_ve - loss is tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5013/6700 [2:19:58<47:32,  1.69s/it]11/17/2022 01:14:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:07 - INFO - train.train_snli_ve - loss is tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5014/6700 [2:19:59<47:19,  1.68s/it]11/17/2022 01:14:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9860e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:08 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5015/6700 [2:20:01<47:20,  1.69s/it]11/17/2022 01:14:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.7764e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:10 - INFO - train.train_snli_ve - loss is tensor(0.8916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5016/6700 [2:20:03<47:02,  1.68s/it]11/17/2022 01:14:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.9669e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:12 - INFO - train.train_snli_ve - loss is tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5017/6700 [2:20:04<46:46,  1.67s/it]11/17/2022 01:14:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.6630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:13 - INFO - train.train_snli_ve - loss is tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5018/6700 [2:20:06<46:34,  1.66s/it]11/17/2022 01:14:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:15 - INFO - train.train_snli_ve - loss is tensor(0.5787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5019/6700 [2:20:08<46:17,  1.65s/it]11/17/2022 01:14:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.2776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:17 - INFO - train.train_snli_ve - loss is tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5020/6700 [2:20:09<46:12,  1.65s/it]11/17/2022 01:14:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.4111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:18 - INFO - train.train_snli_ve - loss is tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5021/6700 [2:20:11<46:23,  1.66s/it]11/17/2022 01:14:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.6810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:20 - INFO - train.train_snli_ve - loss is tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5022/6700 [2:20:13<46:29,  1.66s/it]11/17/2022 01:14:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2717e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:22 - INFO - train.train_snli_ve - loss is tensor(0.8197, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5023/6700 [2:20:14<46:10,  1.65s/it]11/17/2022 01:14:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:23 - INFO - train.train_snli_ve - loss is tensor(0.7875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######4  | 5024/6700 [2:20:16<46:14,  1.66s/it]11/17/2022 01:14:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:25 - INFO - train.train_snli_ve - loss is tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5025/6700 [2:20:18<46:35,  1.67s/it]11/17/2022 01:14:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:27 - INFO - train.train_snli_ve - loss is tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5026/6700 [2:20:19<46:12,  1.66s/it]11/17/2022 01:14:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.5182e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:28 - INFO - train.train_snli_ve - loss is tensor(0.8477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5027/6700 [2:20:21<46:03,  1.65s/it]11/17/2022 01:14:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:30 - INFO - train.train_snli_ve - loss is tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5028/6700 [2:20:23<46:01,  1.65s/it]11/17/2022 01:14:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.0264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:32 - INFO - train.train_snli_ve - loss is tensor(0.4381, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5029/6700 [2:20:24<46:17,  1.66s/it]11/17/2022 01:14:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.3094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:33 - INFO - train.train_snli_ve - loss is tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5030/6700 [2:20:26<46:38,  1.68s/it]11/17/2022 01:14:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.2657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:35 - INFO - train.train_snli_ve - loss is tensor(1.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5031/6700 [2:20:28<47:13,  1.70s/it]11/17/2022 01:14:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9829e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:37 - INFO - train.train_snli_ve - loss is tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5032/6700 [2:20:29<47:25,  1.71s/it]11/17/2022 01:14:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.5228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:38 - INFO - train.train_snli_ve - loss is tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5033/6700 [2:20:31<47:11,  1.70s/it]11/17/2022 01:14:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4086e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:40 - INFO - train.train_snli_ve - loss is tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5034/6700 [2:20:33<47:09,  1.70s/it]11/17/2022 01:14:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:42 - INFO - train.train_snli_ve - loss is tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5035/6700 [2:20:35<47:03,  1.70s/it]11/17/2022 01:14:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:44 - INFO - train.train_snli_ve - loss is tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5036/6700 [2:20:36<47:15,  1.70s/it]11/17/2022 01:14:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.5871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:45 - INFO - train.train_snli_ve - loss is tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5037/6700 [2:20:38<47:07,  1.70s/it]11/17/2022 01:14:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.6976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:47 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5038/6700 [2:20:40<47:06,  1.70s/it]11/17/2022 01:14:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.0949e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:49 - INFO - train.train_snli_ve - loss is tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5039/6700 [2:20:41<46:49,  1.69s/it]11/17/2022 01:14:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.1971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:50 - INFO - train.train_snli_ve - loss is tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5040/6700 [2:20:43<46:30,  1.68s/it]11/17/2022 01:14:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:52 - INFO - train.train_snli_ve - loss is tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5041/6700 [2:20:45<46:28,  1.68s/it]11/17/2022 01:14:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:54 - INFO - train.train_snli_ve - loss is tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5042/6700 [2:20:46<46:22,  1.68s/it]11/17/2022 01:14:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:55 - INFO - train.train_snli_ve - loss is tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5043/6700 [2:20:48<46:11,  1.67s/it]11/17/2022 01:14:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.6610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:57 - INFO - train.train_snli_ve - loss is tensor(0.5061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5044/6700 [2:20:50<46:12,  1.67s/it]11/17/2022 01:14:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2490e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:14:59 - INFO - train.train_snli_ve - loss is tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5045/6700 [2:20:51<46:34,  1.69s/it]11/17/2022 01:15:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.4713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:00 - INFO - train.train_snli_ve - loss is tensor(0.8442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5046/6700 [2:20:53<46:49,  1.70s/it]11/17/2022 01:15:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:02 - INFO - train.train_snli_ve - loss is tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5047/6700 [2:20:55<46:39,  1.69s/it]11/17/2022 01:15:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:04 - INFO - train.train_snli_ve - loss is tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5048/6700 [2:20:56<46:24,  1.69s/it]11/17/2022 01:15:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1514e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:05 - INFO - train.train_snli_ve - loss is tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5049/6700 [2:20:58<46:09,  1.68s/it]11/17/2022 01:15:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:07 - INFO - train.train_snli_ve - loss is tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5050/6700 [2:21:00<46:14,  1.68s/it]11/17/2022 01:15:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.4614e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:09 - INFO - train.train_snli_ve - loss is tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5051/6700 [2:21:02<46:04,  1.68s/it]11/17/2022 01:15:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:10 - INFO - train.train_snli_ve - loss is tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5052/6700 [2:21:03<46:00,  1.68s/it]11/17/2022 01:15:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.5583e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:12 - INFO - train.train_snli_ve - loss is tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5053/6700 [2:21:05<45:49,  1.67s/it]11/17/2022 01:15:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:14 - INFO - train.train_snli_ve - loss is tensor(0.4427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5054/6700 [2:21:06<45:38,  1.66s/it]11/17/2022 01:15:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.3575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:15 - INFO - train.train_snli_ve - loss is tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5055/6700 [2:21:08<45:57,  1.68s/it]11/17/2022 01:15:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:17 - INFO - train.train_snli_ve - loss is tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5056/6700 [2:21:10<46:01,  1.68s/it]11/17/2022 01:15:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:19 - INFO - train.train_snli_ve - loss is tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5057/6700 [2:21:12<46:25,  1.70s/it]11/17/2022 01:15:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:21 - INFO - train.train_snli_ve - loss is tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  75%|#######5  | 5058/6700 [2:21:13<46:28,  1.70s/it]11/17/2022 01:15:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:22 - INFO - train.train_snli_ve - loss is tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5059/6700 [2:21:15<46:13,  1.69s/it]11/17/2022 01:15:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.7707e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:24 - INFO - train.train_snli_ve - loss is tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5060/6700 [2:21:17<46:15,  1.69s/it]11/17/2022 01:15:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:26 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5061/6700 [2:21:18<45:59,  1.68s/it]11/17/2022 01:15:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.2766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:27 - INFO - train.train_snli_ve - loss is tensor(0.5263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5062/6700 [2:21:20<45:49,  1.68s/it]11/17/2022 01:15:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:29 - INFO - train.train_snli_ve - loss is tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5063/6700 [2:21:22<45:49,  1.68s/it]11/17/2022 01:15:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:31 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5064/6700 [2:21:23<46:12,  1.69s/it]11/17/2022 01:15:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.1782e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:32 - INFO - train.train_snli_ve - loss is tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5065/6700 [2:21:25<46:19,  1.70s/it]11/17/2022 01:15:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.7385e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:34 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5066/6700 [2:21:27<46:13,  1.70s/it]11/17/2022 01:15:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.8877e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:36 - INFO - train.train_snli_ve - loss is tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5067/6700 [2:21:29<46:07,  1.69s/it]11/17/2022 01:15:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:38 - INFO - train.train_snli_ve - loss is tensor(0.5903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5068/6700 [2:21:30<45:56,  1.69s/it]11/17/2022 01:15:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.5941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:39 - INFO - train.train_snli_ve - loss is tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5069/6700 [2:21:32<45:49,  1.69s/it]11/17/2022 01:15:41 - INFO - train.train_snli_ve - kd_loss is tensor(4.3202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:41 - INFO - train.train_snli_ve - loss is tensor(0.8625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5070/6700 [2:21:34<45:27,  1.67s/it]11/17/2022 01:15:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1674e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:43 - INFO - train.train_snli_ve - loss is tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5071/6700 [2:21:35<45:37,  1.68s/it]11/17/2022 01:15:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.4901e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:44 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5072/6700 [2:21:37<45:35,  1.68s/it]11/17/2022 01:15:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9644e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:46 - INFO - train.train_snli_ve - loss is tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5073/6700 [2:21:39<45:26,  1.68s/it]11/17/2022 01:15:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.2594e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:48 - INFO - train.train_snli_ve - loss is tensor(0.8215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5074/6700 [2:21:40<45:18,  1.67s/it]11/17/2022 01:15:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:49 - INFO - train.train_snli_ve - loss is tensor(0.8078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5075/6700 [2:21:42<45:09,  1.67s/it]11/17/2022 01:15:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:51 - INFO - train.train_snli_ve - loss is tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5076/6700 [2:21:44<45:11,  1.67s/it]11/17/2022 01:15:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:53 - INFO - train.train_snli_ve - loss is tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5077/6700 [2:21:45<45:18,  1.67s/it]11/17/2022 01:15:54 - INFO - train.train_snli_ve - kd_loss is tensor(4.1775e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:54 - INFO - train.train_snli_ve - loss is tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5078/6700 [2:21:47<45:50,  1.70s/it]11/17/2022 01:15:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.9629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:56 - INFO - train.train_snli_ve - loss is tensor(0.5285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5079/6700 [2:21:49<45:42,  1.69s/it]11/17/2022 01:15:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.9793e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:58 - INFO - train.train_snli_ve - loss is tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5080/6700 [2:21:50<45:34,  1.69s/it]11/17/2022 01:15:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:15:59 - INFO - train.train_snli_ve - loss is tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5081/6700 [2:21:52<45:36,  1.69s/it]11/17/2022 01:16:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:01 - INFO - train.train_snli_ve - loss is tensor(0.8010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5082/6700 [2:21:54<45:48,  1.70s/it]11/17/2022 01:16:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:03 - INFO - train.train_snli_ve - loss is tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5083/6700 [2:21:55<45:22,  1.68s/it]11/17/2022 01:16:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.4612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:04 - INFO - train.train_snli_ve - loss is tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5084/6700 [2:21:57<45:29,  1.69s/it]11/17/2022 01:16:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5464e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:06 - INFO - train.train_snli_ve - loss is tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5085/6700 [2:21:59<45:38,  1.70s/it]11/17/2022 01:16:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:08 - INFO - train.train_snli_ve - loss is tensor(0.4952, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5086/6700 [2:22:00<45:06,  1.68s/it]11/17/2022 01:16:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.3386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:09 - INFO - train.train_snli_ve - loss is tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5087/6700 [2:22:02<45:18,  1.69s/it]11/17/2022 01:16:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.9452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:11 - INFO - train.train_snli_ve - loss is tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5088/6700 [2:22:04<45:12,  1.68s/it]11/17/2022 01:16:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6950e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:13 - INFO - train.train_snli_ve - loss is tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5089/6700 [2:22:06<45:15,  1.69s/it]11/17/2022 01:16:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:15 - INFO - train.train_snli_ve - loss is tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5090/6700 [2:22:07<45:16,  1.69s/it]11/17/2022 01:16:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.4606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:16 - INFO - train.train_snli_ve - loss is tensor(0.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######5  | 5091/6700 [2:22:09<45:25,  1.69s/it]11/17/2022 01:16:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.1238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:18 - INFO - train.train_snli_ve - loss is tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5092/6700 [2:22:11<45:21,  1.69s/it]11/17/2022 01:16:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.2217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:20 - INFO - train.train_snli_ve - loss is tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5093/6700 [2:22:12<45:22,  1.69s/it]11/17/2022 01:16:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.8985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:21 - INFO - train.train_snli_ve - loss is tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5094/6700 [2:22:14<45:32,  1.70s/it]11/17/2022 01:16:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.9517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:23 - INFO - train.train_snli_ve - loss is tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5095/6700 [2:22:16<45:05,  1.69s/it]11/17/2022 01:16:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:25 - INFO - train.train_snli_ve - loss is tensor(0.9221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5096/6700 [2:22:17<44:54,  1.68s/it]11/17/2022 01:16:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.9570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:26 - INFO - train.train_snli_ve - loss is tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5097/6700 [2:22:19<45:05,  1.69s/it]11/17/2022 01:16:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:28 - INFO - train.train_snli_ve - loss is tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5098/6700 [2:22:21<44:45,  1.68s/it]11/17/2022 01:16:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.7562e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:30 - INFO - train.train_snli_ve - loss is tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5099/6700 [2:22:22<44:49,  1.68s/it]11/17/2022 01:16:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.8257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:31 - INFO - train.train_snli_ve - loss is tensor(0.6070, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5100/6700 [2:22:24<44:58,  1.69s/it]11/17/2022 01:16:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:33 - INFO - train.train_snli_ve - loss is tensor(0.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5101/6700 [2:22:26<45:01,  1.69s/it]11/17/2022 01:16:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.9683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:35 - INFO - train.train_snli_ve - loss is tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5102/6700 [2:22:28<45:22,  1.70s/it]11/17/2022 01:16:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.2326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:36 - INFO - train.train_snli_ve - loss is tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5103/6700 [2:22:29<44:57,  1.69s/it]11/17/2022 01:16:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.4491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:38 - INFO - train.train_snli_ve - loss is tensor(1.0125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5104/6700 [2:22:31<45:16,  1.70s/it]11/17/2022 01:16:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.6519e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:40 - INFO - train.train_snli_ve - loss is tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5105/6700 [2:22:33<44:58,  1.69s/it]11/17/2022 01:16:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.8652e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:42 - INFO - train.train_snli_ve - loss is tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5106/6700 [2:22:34<44:43,  1.68s/it]11/17/2022 01:16:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:43 - INFO - train.train_snli_ve - loss is tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5107/6700 [2:22:36<44:47,  1.69s/it]11/17/2022 01:16:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:45 - INFO - train.train_snli_ve - loss is tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5108/6700 [2:22:38<44:37,  1.68s/it]11/17/2022 01:16:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:47 - INFO - train.train_snli_ve - loss is tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5109/6700 [2:22:39<44:45,  1.69s/it]11/17/2022 01:16:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1836e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:48 - INFO - train.train_snli_ve - loss is tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5110/6700 [2:22:41<44:45,  1.69s/it]11/17/2022 01:16:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.3198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:50 - INFO - train.train_snli_ve - loss is tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5111/6700 [2:22:43<44:36,  1.68s/it]11/17/2022 01:16:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.9710e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:52 - INFO - train.train_snli_ve - loss is tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5112/6700 [2:22:44<44:40,  1.69s/it]11/17/2022 01:16:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.2606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:53 - INFO - train.train_snli_ve - loss is tensor(0.8276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5113/6700 [2:22:46<44:21,  1.68s/it]11/17/2022 01:16:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.0301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:55 - INFO - train.train_snli_ve - loss is tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5114/6700 [2:22:48<43:50,  1.66s/it]11/17/2022 01:16:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.1310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:57 - INFO - train.train_snli_ve - loss is tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5115/6700 [2:22:49<44:01,  1.67s/it]11/17/2022 01:16:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:16:58 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5116/6700 [2:22:51<44:20,  1.68s/it]11/17/2022 01:17:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.6016e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:00 - INFO - train.train_snli_ve - loss is tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5117/6700 [2:22:53<44:13,  1.68s/it]11/17/2022 01:17:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9522e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:02 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5118/6700 [2:22:54<44:23,  1.68s/it]11/17/2022 01:17:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.8357e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:03 - INFO - train.train_snli_ve - loss is tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5119/6700 [2:22:56<44:32,  1.69s/it]11/17/2022 01:17:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2571e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:05 - INFO - train.train_snli_ve - loss is tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5120/6700 [2:22:58<44:43,  1.70s/it]11/17/2022 01:17:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:07 - INFO - train.train_snli_ve - loss is tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5121/6700 [2:23:00<44:51,  1.70s/it]11/17/2022 01:17:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:09 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5122/6700 [2:23:01<44:38,  1.70s/it]11/17/2022 01:17:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:10 - INFO - train.train_snli_ve - loss is tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5123/6700 [2:23:03<44:33,  1.70s/it]11/17/2022 01:17:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:12 - INFO - train.train_snli_ve - loss is tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5124/6700 [2:23:05<44:24,  1.69s/it]11/17/2022 01:17:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.5305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:14 - INFO - train.train_snli_ve - loss is tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  76%|#######6  | 5125/6700 [2:23:06<44:00,  1.68s/it]11/17/2022 01:17:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7443e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:15 - INFO - train.train_snli_ve - loss is tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5126/6700 [2:23:08<43:55,  1.67s/it]11/17/2022 01:17:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8335e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:17 - INFO - train.train_snli_ve - loss is tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5127/6700 [2:23:10<44:02,  1.68s/it]11/17/2022 01:17:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.4888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:19 - INFO - train.train_snli_ve - loss is tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5128/6700 [2:23:11<44:20,  1.69s/it]11/17/2022 01:17:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:20 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5129/6700 [2:23:13<43:57,  1.68s/it]11/17/2022 01:17:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:22 - INFO - train.train_snli_ve - loss is tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5130/6700 [2:23:15<44:18,  1.69s/it]11/17/2022 01:17:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.6030e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:24 - INFO - train.train_snli_ve - loss is tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5131/6700 [2:23:16<44:05,  1.69s/it]11/17/2022 01:17:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.7337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:25 - INFO - train.train_snli_ve - loss is tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5132/6700 [2:23:18<44:17,  1.69s/it]11/17/2022 01:17:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:27 - INFO - train.train_snli_ve - loss is tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5133/6700 [2:23:20<44:12,  1.69s/it]11/17/2022 01:17:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.3135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:29 - INFO - train.train_snli_ve - loss is tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5134/6700 [2:23:21<44:17,  1.70s/it]11/17/2022 01:17:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.3163e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:30 - INFO - train.train_snli_ve - loss is tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5135/6700 [2:23:23<44:20,  1.70s/it]11/17/2022 01:17:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:32 - INFO - train.train_snli_ve - loss is tensor(0.9209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5136/6700 [2:23:25<44:07,  1.69s/it]11/17/2022 01:17:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:34 - INFO - train.train_snli_ve - loss is tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5137/6700 [2:23:27<43:52,  1.68s/it]11/17/2022 01:17:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.0859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:36 - INFO - train.train_snli_ve - loss is tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5138/6700 [2:23:28<43:50,  1.68s/it]11/17/2022 01:17:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.2502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:37 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5139/6700 [2:23:30<43:43,  1.68s/it]11/17/2022 01:17:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0638e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:39 - INFO - train.train_snli_ve - loss is tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5140/6700 [2:23:32<43:35,  1.68s/it]11/17/2022 01:17:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.2371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:41 - INFO - train.train_snli_ve - loss is tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5141/6700 [2:23:33<43:36,  1.68s/it]11/17/2022 01:17:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.2600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:42 - INFO - train.train_snli_ve - loss is tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5142/6700 [2:23:35<43:25,  1.67s/it]11/17/2022 01:17:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.9367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:44 - INFO - train.train_snli_ve - loss is tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5143/6700 [2:23:37<43:22,  1.67s/it]11/17/2022 01:17:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:45 - INFO - train.train_snli_ve - loss is tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5144/6700 [2:23:38<42:57,  1.66s/it]11/17/2022 01:17:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:47 - INFO - train.train_snli_ve - loss is tensor(0.7086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5145/6700 [2:23:40<43:01,  1.66s/it]11/17/2022 01:17:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.4922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:49 - INFO - train.train_snli_ve - loss is tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5146/6700 [2:23:42<43:06,  1.66s/it]11/17/2022 01:17:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0846e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:51 - INFO - train.train_snli_ve - loss is tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5147/6700 [2:23:43<43:19,  1.67s/it]11/17/2022 01:17:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:52 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5148/6700 [2:23:45<43:25,  1.68s/it]11/17/2022 01:17:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:54 - INFO - train.train_snli_ve - loss is tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5149/6700 [2:23:47<43:30,  1.68s/it]11/17/2022 01:17:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.7518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:56 - INFO - train.train_snli_ve - loss is tensor(0.5319, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5150/6700 [2:23:48<43:52,  1.70s/it]11/17/2022 01:17:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.4281e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:57 - INFO - train.train_snli_ve - loss is tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5151/6700 [2:23:50<43:46,  1.70s/it]11/17/2022 01:17:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.9839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:17:59 - INFO - train.train_snli_ve - loss is tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5152/6700 [2:23:52<44:01,  1.71s/it]11/17/2022 01:18:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:01 - INFO - train.train_snli_ve - loss is tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5153/6700 [2:23:53<43:57,  1.70s/it]11/17/2022 01:18:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:02 - INFO - train.train_snli_ve - loss is tensor(0.8798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5154/6700 [2:23:55<43:35,  1.69s/it]11/17/2022 01:18:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.5474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:04 - INFO - train.train_snli_ve - loss is tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5155/6700 [2:23:57<43:21,  1.68s/it]11/17/2022 01:18:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.4701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:06 - INFO - train.train_snli_ve - loss is tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5156/6700 [2:23:58<43:29,  1.69s/it]11/17/2022 01:18:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:07 - INFO - train.train_snli_ve - loss is tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5157/6700 [2:24:00<43:23,  1.69s/it]11/17/2022 01:18:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.7894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:09 - INFO - train.train_snli_ve - loss is tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######6  | 5158/6700 [2:24:02<43:13,  1.68s/it]11/17/2022 01:18:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.8567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:11 - INFO - train.train_snli_ve - loss is tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5159/6700 [2:24:04<43:27,  1.69s/it]11/17/2022 01:18:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9308e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:13 - INFO - train.train_snli_ve - loss is tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5160/6700 [2:24:05<43:29,  1.69s/it]11/17/2022 01:18:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.7142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:14 - INFO - train.train_snli_ve - loss is tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5161/6700 [2:24:07<43:20,  1.69s/it]11/17/2022 01:18:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.5841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:16 - INFO - train.train_snli_ve - loss is tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5162/6700 [2:24:09<43:03,  1.68s/it]11/17/2022 01:18:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7119e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:18 - INFO - train.train_snli_ve - loss is tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5163/6700 [2:24:10<43:18,  1.69s/it]11/17/2022 01:18:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:19 - INFO - train.train_snli_ve - loss is tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5164/6700 [2:24:12<42:36,  1.66s/it]11/17/2022 01:18:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:21 - INFO - train.train_snli_ve - loss is tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5165/6700 [2:24:14<42:41,  1.67s/it]11/17/2022 01:18:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:23 - INFO - train.train_snli_ve - loss is tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5166/6700 [2:24:15<42:34,  1.67s/it]11/17/2022 01:18:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:24 - INFO - train.train_snli_ve - loss is tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5167/6700 [2:24:17<42:34,  1.67s/it]11/17/2022 01:18:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.5116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:26 - INFO - train.train_snli_ve - loss is tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5168/6700 [2:24:19<42:23,  1.66s/it]11/17/2022 01:18:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0003e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:28 - INFO - train.train_snli_ve - loss is tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5169/6700 [2:24:20<42:42,  1.67s/it]11/17/2022 01:18:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0617e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:29 - INFO - train.train_snli_ve - loss is tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5170/6700 [2:24:22<42:49,  1.68s/it]11/17/2022 01:18:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.1012e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:31 - INFO - train.train_snli_ve - loss is tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5171/6700 [2:24:24<42:42,  1.68s/it]11/17/2022 01:18:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.3346e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:33 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5172/6700 [2:24:25<42:54,  1.69s/it]11/17/2022 01:18:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.8664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:34 - INFO - train.train_snli_ve - loss is tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5173/6700 [2:24:27<43:03,  1.69s/it]11/17/2022 01:18:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.7600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:36 - INFO - train.train_snli_ve - loss is tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5174/6700 [2:24:29<43:00,  1.69s/it]11/17/2022 01:18:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.7232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:38 - INFO - train.train_snli_ve - loss is tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5175/6700 [2:24:30<43:03,  1.69s/it]11/17/2022 01:18:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.9272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:39 - INFO - train.train_snli_ve - loss is tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5176/6700 [2:24:32<42:59,  1.69s/it]11/17/2022 01:18:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9825e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:41 - INFO - train.train_snli_ve - loss is tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5177/6700 [2:24:34<42:46,  1.69s/it]11/17/2022 01:18:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:43 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5178/6700 [2:24:35<42:49,  1.69s/it]11/17/2022 01:18:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:44 - INFO - train.train_snli_ve - loss is tensor(0.7938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5179/6700 [2:24:37<42:46,  1.69s/it]11/17/2022 01:18:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.5441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:46 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5180/6700 [2:24:39<42:41,  1.69s/it]11/17/2022 01:18:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.7218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:48 - INFO - train.train_snli_ve - loss is tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5181/6700 [2:24:41<42:22,  1.67s/it]11/17/2022 01:18:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.4797e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:50 - INFO - train.train_snli_ve - loss is tensor(0.7801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5182/6700 [2:24:42<42:24,  1.68s/it]11/17/2022 01:18:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.2137e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:51 - INFO - train.train_snli_ve - loss is tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5183/6700 [2:24:44<42:22,  1.68s/it]11/17/2022 01:18:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.1221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:53 - INFO - train.train_snli_ve - loss is tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5184/6700 [2:24:46<42:57,  1.70s/it]11/17/2022 01:18:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0282e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:55 - INFO - train.train_snli_ve - loss is tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5185/6700 [2:24:47<42:44,  1.69s/it]11/17/2022 01:18:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2401e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:56 - INFO - train.train_snli_ve - loss is tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5186/6700 [2:24:49<42:40,  1.69s/it]11/17/2022 01:18:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.0930e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:18:58 - INFO - train.train_snli_ve - loss is tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5187/6700 [2:24:51<42:56,  1.70s/it]11/17/2022 01:19:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:00 - INFO - train.train_snli_ve - loss is tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5188/6700 [2:24:52<42:49,  1.70s/it]11/17/2022 01:19:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.1356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:01 - INFO - train.train_snli_ve - loss is tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5189/6700 [2:24:54<42:14,  1.68s/it]11/17/2022 01:19:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.2388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:03 - INFO - train.train_snli_ve - loss is tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5190/6700 [2:24:56<42:16,  1.68s/it]11/17/2022 01:19:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:05 - INFO - train.train_snli_ve - loss is tensor(0.8205, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5191/6700 [2:24:57<42:15,  1.68s/it]11/17/2022 01:19:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.8142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:06 - INFO - train.train_snli_ve - loss is tensor(0.9230, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  77%|#######7  | 5192/6700 [2:24:59<42:07,  1.68s/it]11/17/2022 01:19:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.5743e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:08 - INFO - train.train_snli_ve - loss is tensor(0.7415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5193/6700 [2:25:01<42:23,  1.69s/it]11/17/2022 01:19:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:10 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5194/6700 [2:25:02<42:25,  1.69s/it]11/17/2022 01:19:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.8485e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:11 - INFO - train.train_snli_ve - loss is tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5195/6700 [2:25:04<42:16,  1.69s/it]11/17/2022 01:19:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:13 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5196/6700 [2:25:06<42:23,  1.69s/it]11/17/2022 01:19:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.0293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:15 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5197/6700 [2:25:08<42:16,  1.69s/it]11/17/2022 01:19:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:17 - INFO - train.train_snli_ve - loss is tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5198/6700 [2:25:09<42:16,  1.69s/it]11/17/2022 01:19:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.1942e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:18 - INFO - train.train_snli_ve - loss is tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5199/6700 [2:25:11<42:04,  1.68s/it]11/17/2022 01:19:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8627e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:20 - INFO - train.train_snli_ve - loss is tensor(0.8887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5200/6700 [2:25:13<42:21,  1.69s/it]11/17/2022 01:19:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.5821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:22 - INFO - train.train_snli_ve - loss is tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5201/6700 [2:25:14<42:16,  1.69s/it]11/17/2022 01:19:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.4370e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:23 - INFO - train.train_snli_ve - loss is tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5202/6700 [2:25:16<42:20,  1.70s/it]11/17/2022 01:19:25 - INFO - train.train_snli_ve - kd_loss is tensor(4.0848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:25 - INFO - train.train_snli_ve - loss is tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5203/6700 [2:25:18<42:19,  1.70s/it]11/17/2022 01:19:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:27 - INFO - train.train_snli_ve - loss is tensor(0.5435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5204/6700 [2:25:19<42:08,  1.69s/it]11/17/2022 01:19:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.8074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:28 - INFO - train.train_snli_ve - loss is tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5205/6700 [2:25:21<42:03,  1.69s/it]11/17/2022 01:19:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8537e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:30 - INFO - train.train_snli_ve - loss is tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5206/6700 [2:25:23<41:55,  1.68s/it]11/17/2022 01:19:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.7666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:32 - INFO - train.train_snli_ve - loss is tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5207/6700 [2:25:24<41:46,  1.68s/it]11/17/2022 01:19:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.2440e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:33 - INFO - train.train_snli_ve - loss is tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5208/6700 [2:25:26<41:44,  1.68s/it]11/17/2022 01:19:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0489e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:35 - INFO - train.train_snli_ve - loss is tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5209/6700 [2:25:28<41:30,  1.67s/it]11/17/2022 01:19:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.1132e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:37 - INFO - train.train_snli_ve - loss is tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5210/6700 [2:25:29<42:02,  1.69s/it]11/17/2022 01:19:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.8167e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:38 - INFO - train.train_snli_ve - loss is tensor(0.8086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5211/6700 [2:25:31<41:31,  1.67s/it]11/17/2022 01:19:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.7912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:40 - INFO - train.train_snli_ve - loss is tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5212/6700 [2:25:33<41:40,  1.68s/it]11/17/2022 01:19:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7424e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:42 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5213/6700 [2:25:34<41:26,  1.67s/it]11/17/2022 01:19:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.3945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:43 - INFO - train.train_snli_ve - loss is tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5214/6700 [2:25:36<41:13,  1.66s/it]11/17/2022 01:19:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.6239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:45 - INFO - train.train_snli_ve - loss is tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5215/6700 [2:25:38<41:22,  1.67s/it]11/17/2022 01:19:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:47 - INFO - train.train_snli_ve - loss is tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5216/6700 [2:25:39<41:29,  1.68s/it]11/17/2022 01:19:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:48 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5217/6700 [2:25:41<41:20,  1.67s/it]11/17/2022 01:19:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5504e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:50 - INFO - train.train_snli_ve - loss is tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5218/6700 [2:25:43<41:20,  1.67s/it]11/17/2022 01:19:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.1989e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:52 - INFO - train.train_snli_ve - loss is tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5219/6700 [2:25:45<41:38,  1.69s/it]11/17/2022 01:19:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:54 - INFO - train.train_snli_ve - loss is tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5220/6700 [2:25:46<41:36,  1.69s/it]11/17/2022 01:19:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.8290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:55 - INFO - train.train_snli_ve - loss is tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5221/6700 [2:25:48<41:26,  1.68s/it]11/17/2022 01:19:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:57 - INFO - train.train_snli_ve - loss is tensor(0.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5222/6700 [2:25:50<41:17,  1.68s/it]11/17/2022 01:19:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:19:59 - INFO - train.train_snli_ve - loss is tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5223/6700 [2:25:51<41:27,  1.68s/it]11/17/2022 01:20:00 - INFO - train.train_snli_ve - kd_loss is tensor(9.2452e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:00 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5224/6700 [2:25:53<41:19,  1.68s/it]11/17/2022 01:20:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.8843e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:02 - INFO - train.train_snli_ve - loss is tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######7  | 5225/6700 [2:25:55<41:24,  1.68s/it]11/17/2022 01:20:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.5296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:04 - INFO - train.train_snli_ve - loss is tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5226/6700 [2:25:56<41:22,  1.68s/it]11/17/2022 01:20:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.2261e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:05 - INFO - train.train_snli_ve - loss is tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5227/6700 [2:25:58<41:25,  1.69s/it]11/17/2022 01:20:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.6405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:07 - INFO - train.train_snli_ve - loss is tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5228/6700 [2:26:00<41:18,  1.68s/it]11/17/2022 01:20:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.6044e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:09 - INFO - train.train_snli_ve - loss is tensor(0.4922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5229/6700 [2:26:01<40:59,  1.67s/it]11/17/2022 01:20:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.1838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:10 - INFO - train.train_snli_ve - loss is tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5230/6700 [2:26:03<41:11,  1.68s/it]11/17/2022 01:20:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:12 - INFO - train.train_snli_ve - loss is tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5231/6700 [2:26:05<41:10,  1.68s/it]11/17/2022 01:20:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.8518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:14 - INFO - train.train_snli_ve - loss is tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5232/6700 [2:26:06<41:06,  1.68s/it]11/17/2022 01:20:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.7156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:15 - INFO - train.train_snli_ve - loss is tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5233/6700 [2:26:08<40:58,  1.68s/it]11/17/2022 01:20:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.6560e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:17 - INFO - train.train_snli_ve - loss is tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5234/6700 [2:26:10<41:10,  1.68s/it]11/17/2022 01:20:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.5229e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:19 - INFO - train.train_snli_ve - loss is tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5235/6700 [2:26:11<41:16,  1.69s/it]11/17/2022 01:20:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.5281e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:20 - INFO - train.train_snli_ve - loss is tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5236/6700 [2:26:13<41:20,  1.69s/it]11/17/2022 01:20:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.8355e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:22 - INFO - train.train_snli_ve - loss is tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5237/6700 [2:26:15<41:05,  1.69s/it]11/17/2022 01:20:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:24 - INFO - train.train_snli_ve - loss is tensor(0.7991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5238/6700 [2:26:17<41:21,  1.70s/it]11/17/2022 01:20:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:26 - INFO - train.train_snli_ve - loss is tensor(0.9349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5239/6700 [2:26:18<41:11,  1.69s/it]11/17/2022 01:20:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:27 - INFO - train.train_snli_ve - loss is tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5240/6700 [2:26:20<41:12,  1.69s/it]11/17/2022 01:20:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.8812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:29 - INFO - train.train_snli_ve - loss is tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5241/6700 [2:26:22<41:01,  1.69s/it]11/17/2022 01:20:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.7236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:31 - INFO - train.train_snli_ve - loss is tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5242/6700 [2:26:23<40:57,  1.69s/it]11/17/2022 01:20:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8127e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:32 - INFO - train.train_snli_ve - loss is tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5243/6700 [2:26:25<41:08,  1.69s/it]11/17/2022 01:20:34 - INFO - train.train_snli_ve - kd_loss is tensor(9.9203e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:34 - INFO - train.train_snli_ve - loss is tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5244/6700 [2:26:27<41:11,  1.70s/it]11/17/2022 01:20:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.2376e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:36 - INFO - train.train_snli_ve - loss is tensor(0.9305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5245/6700 [2:26:28<40:58,  1.69s/it]11/17/2022 01:20:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.6190e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:37 - INFO - train.train_snli_ve - loss is tensor(0.4540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5246/6700 [2:26:30<40:37,  1.68s/it]11/17/2022 01:20:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0833e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:39 - INFO - train.train_snli_ve - loss is tensor(0.4533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5247/6700 [2:26:32<40:39,  1.68s/it]11/17/2022 01:20:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:41 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5248/6700 [2:26:33<40:47,  1.69s/it]11/17/2022 01:20:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.9432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:42 - INFO - train.train_snli_ve - loss is tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5249/6700 [2:26:35<40:49,  1.69s/it]11/17/2022 01:20:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.4868e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:44 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5250/6700 [2:26:37<40:57,  1.69s/it]11/17/2022 01:20:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.6756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:46 - INFO - train.train_snli_ve - loss is tensor(0.5505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5251/6700 [2:26:38<40:37,  1.68s/it]11/17/2022 01:20:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.7807e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:47 - INFO - train.train_snli_ve - loss is tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5252/6700 [2:26:40<40:41,  1.69s/it]11/17/2022 01:20:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.8992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:49 - INFO - train.train_snli_ve - loss is tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5253/6700 [2:26:42<40:29,  1.68s/it]11/17/2022 01:20:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0278e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:51 - INFO - train.train_snli_ve - loss is tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5254/6700 [2:26:43<40:28,  1.68s/it]11/17/2022 01:20:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.7122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:52 - INFO - train.train_snli_ve - loss is tensor(0.4449, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5255/6700 [2:26:45<40:30,  1.68s/it]11/17/2022 01:20:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:54 - INFO - train.train_snli_ve - loss is tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5256/6700 [2:26:47<40:29,  1.68s/it]11/17/2022 01:20:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.0015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:56 - INFO - train.train_snli_ve - loss is tensor(0.8306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5257/6700 [2:26:49<40:23,  1.68s/it]11/17/2022 01:20:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:58 - INFO - train.train_snli_ve - loss is tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5258/6700 [2:26:50<40:17,  1.68s/it]11/17/2022 01:20:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:20:59 - INFO - train.train_snli_ve - loss is tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  78%|#######8  | 5259/6700 [2:26:52<40:17,  1.68s/it]11/17/2022 01:21:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.6700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:01 - INFO - train.train_snli_ve - loss is tensor(0.9715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5260/6700 [2:26:54<40:25,  1.68s/it]11/17/2022 01:21:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.8765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:03 - INFO - train.train_snli_ve - loss is tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5261/6700 [2:26:55<40:13,  1.68s/it]11/17/2022 01:21:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.5911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:04 - INFO - train.train_snli_ve - loss is tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5262/6700 [2:26:57<40:24,  1.69s/it]11/17/2022 01:21:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.0097e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:06 - INFO - train.train_snli_ve - loss is tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5263/6700 [2:26:59<40:18,  1.68s/it]11/17/2022 01:21:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.0532e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:08 - INFO - train.train_snli_ve - loss is tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5264/6700 [2:27:00<40:05,  1.67s/it]11/17/2022 01:21:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:09 - INFO - train.train_snli_ve - loss is tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5265/6700 [2:27:02<39:56,  1.67s/it]11/17/2022 01:21:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.7945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:11 - INFO - train.train_snli_ve - loss is tensor(0.4334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5266/6700 [2:27:04<39:59,  1.67s/it]11/17/2022 01:21:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.6166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:13 - INFO - train.train_snli_ve - loss is tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5267/6700 [2:27:05<40:27,  1.69s/it]11/17/2022 01:21:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:14 - INFO - train.train_snli_ve - loss is tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5268/6700 [2:27:07<40:33,  1.70s/it]11/17/2022 01:21:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.6615e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:16 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5269/6700 [2:27:09<40:14,  1.69s/it]11/17/2022 01:21:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7121e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:18 - INFO - train.train_snli_ve - loss is tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5270/6700 [2:27:10<40:19,  1.69s/it]11/17/2022 01:21:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1216e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:19 - INFO - train.train_snli_ve - loss is tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5271/6700 [2:27:12<40:30,  1.70s/it]11/17/2022 01:21:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.3751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:21 - INFO - train.train_snli_ve - loss is tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5272/6700 [2:27:14<40:36,  1.71s/it]11/17/2022 01:21:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.1328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:23 - INFO - train.train_snli_ve - loss is tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5273/6700 [2:27:16<40:33,  1.71s/it]11/17/2022 01:21:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7704e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:25 - INFO - train.train_snli_ve - loss is tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5274/6700 [2:27:17<40:18,  1.70s/it]11/17/2022 01:21:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0783e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:26 - INFO - train.train_snli_ve - loss is tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5275/6700 [2:27:19<40:13,  1.69s/it]11/17/2022 01:21:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.0492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:28 - INFO - train.train_snli_ve - loss is tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5276/6700 [2:27:21<40:09,  1.69s/it]11/17/2022 01:21:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.7409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:30 - INFO - train.train_snli_ve - loss is tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5277/6700 [2:27:22<39:43,  1.67s/it]11/17/2022 01:21:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.4408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:31 - INFO - train.train_snli_ve - loss is tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5278/6700 [2:27:24<39:45,  1.68s/it]11/17/2022 01:21:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:33 - INFO - train.train_snli_ve - loss is tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5279/6700 [2:27:26<39:42,  1.68s/it]11/17/2022 01:21:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:35 - INFO - train.train_snli_ve - loss is tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5280/6700 [2:27:27<40:01,  1.69s/it]11/17/2022 01:21:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.5155e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:36 - INFO - train.train_snli_ve - loss is tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5281/6700 [2:27:29<39:46,  1.68s/it]11/17/2022 01:21:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.4373e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:38 - INFO - train.train_snli_ve - loss is tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5282/6700 [2:27:31<39:58,  1.69s/it]11/17/2022 01:21:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:40 - INFO - train.train_snli_ve - loss is tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5283/6700 [2:27:32<39:52,  1.69s/it]11/17/2022 01:21:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1663e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:41 - INFO - train.train_snli_ve - loss is tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5284/6700 [2:27:34<39:50,  1.69s/it]11/17/2022 01:21:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.5005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:43 - INFO - train.train_snli_ve - loss is tensor(0.8288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5285/6700 [2:27:36<39:29,  1.67s/it]11/17/2022 01:21:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:45 - INFO - train.train_snli_ve - loss is tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5286/6700 [2:27:37<39:44,  1.69s/it]11/17/2022 01:21:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:46 - INFO - train.train_snli_ve - loss is tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5287/6700 [2:27:39<39:26,  1.67s/it]11/17/2022 01:21:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.3046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:48 - INFO - train.train_snli_ve - loss is tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5288/6700 [2:27:41<39:03,  1.66s/it]11/17/2022 01:21:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9796e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:50 - INFO - train.train_snli_ve - loss is tensor(1.0455, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5289/6700 [2:27:42<39:12,  1.67s/it]11/17/2022 01:21:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0742e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:51 - INFO - train.train_snli_ve - loss is tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5290/6700 [2:27:44<39:13,  1.67s/it]11/17/2022 01:21:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.3651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:53 - INFO - train.train_snli_ve - loss is tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5291/6700 [2:27:46<39:00,  1.66s/it]11/17/2022 01:21:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:55 - INFO - train.train_snli_ve - loss is tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######8  | 5292/6700 [2:27:47<39:14,  1.67s/it]11/17/2022 01:21:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.5688e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:56 - INFO - train.train_snli_ve - loss is tensor(0.7913, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5293/6700 [2:27:49<38:53,  1.66s/it]11/17/2022 01:21:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.7815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:21:58 - INFO - train.train_snli_ve - loss is tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5294/6700 [2:27:51<39:19,  1.68s/it]11/17/2022 01:22:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4262e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:00 - INFO - train.train_snli_ve - loss is tensor(0.7341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5295/6700 [2:27:52<39:16,  1.68s/it]11/17/2022 01:22:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.4075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:01 - INFO - train.train_snli_ve - loss is tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5296/6700 [2:27:54<39:23,  1.68s/it]11/17/2022 01:22:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.7533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:03 - INFO - train.train_snli_ve - loss is tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5297/6700 [2:27:56<39:17,  1.68s/it]11/17/2022 01:22:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.8021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:05 - INFO - train.train_snli_ve - loss is tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5298/6700 [2:27:58<39:21,  1.68s/it]11/17/2022 01:22:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.5663e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:06 - INFO - train.train_snli_ve - loss is tensor(0.7627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5299/6700 [2:27:59<39:00,  1.67s/it]11/17/2022 01:22:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:08 - INFO - train.train_snli_ve - loss is tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5300/6700 [2:28:01<39:17,  1.68s/it]11/17/2022 01:22:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:10 - INFO - train.train_snli_ve - loss is tensor(0.8990, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5301/6700 [2:28:03<39:16,  1.68s/it]11/17/2022 01:22:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:12 - INFO - train.train_snli_ve - loss is tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5302/6700 [2:28:04<39:09,  1.68s/it]11/17/2022 01:22:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.9082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:13 - INFO - train.train_snli_ve - loss is tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5303/6700 [2:28:06<39:17,  1.69s/it]11/17/2022 01:22:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:15 - INFO - train.train_snli_ve - loss is tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5304/6700 [2:28:08<39:09,  1.68s/it]11/17/2022 01:22:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.4468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:17 - INFO - train.train_snli_ve - loss is tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5305/6700 [2:28:09<39:16,  1.69s/it]11/17/2022 01:22:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.6769e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:18 - INFO - train.train_snli_ve - loss is tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5306/6700 [2:28:11<39:22,  1.69s/it]11/17/2022 01:22:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.3254e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:20 - INFO - train.train_snli_ve - loss is tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5307/6700 [2:28:13<39:02,  1.68s/it]11/17/2022 01:22:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.6824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:22 - INFO - train.train_snli_ve - loss is tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5308/6700 [2:28:14<38:35,  1.66s/it]11/17/2022 01:22:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.5839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:23 - INFO - train.train_snli_ve - loss is tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5309/6700 [2:28:16<39:04,  1.69s/it]11/17/2022 01:22:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.4140e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:25 - INFO - train.train_snli_ve - loss is tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5310/6700 [2:28:18<38:48,  1.68s/it]11/17/2022 01:22:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9932e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:27 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5311/6700 [2:28:19<38:58,  1.68s/it]11/17/2022 01:22:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8068e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:28 - INFO - train.train_snli_ve - loss is tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5312/6700 [2:28:21<38:47,  1.68s/it]11/17/2022 01:22:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:30 - INFO - train.train_snli_ve - loss is tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5313/6700 [2:28:23<39:04,  1.69s/it]11/17/2022 01:22:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.3985e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:32 - INFO - train.train_snli_ve - loss is tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5314/6700 [2:28:24<38:55,  1.68s/it]11/17/2022 01:22:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.4288e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:33 - INFO - train.train_snli_ve - loss is tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5315/6700 [2:28:26<39:04,  1.69s/it]11/17/2022 01:22:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.1530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:35 - INFO - train.train_snli_ve - loss is tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5316/6700 [2:28:28<39:09,  1.70s/it]11/17/2022 01:22:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7584e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:37 - INFO - train.train_snli_ve - loss is tensor(0.8190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5317/6700 [2:28:30<39:14,  1.70s/it]11/17/2022 01:22:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.1699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:39 - INFO - train.train_snli_ve - loss is tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5318/6700 [2:28:31<39:12,  1.70s/it]11/17/2022 01:22:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.6541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:40 - INFO - train.train_snli_ve - loss is tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5319/6700 [2:28:33<38:58,  1.69s/it]11/17/2022 01:22:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.3051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:42 - INFO - train.train_snli_ve - loss is tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5320/6700 [2:28:35<39:02,  1.70s/it]11/17/2022 01:22:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.5622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:44 - INFO - train.train_snli_ve - loss is tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5321/6700 [2:28:36<38:49,  1.69s/it]11/17/2022 01:22:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.3478e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:45 - INFO - train.train_snli_ve - loss is tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5322/6700 [2:28:38<38:43,  1.69s/it]11/17/2022 01:22:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.1973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:47 - INFO - train.train_snli_ve - loss is tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5323/6700 [2:28:40<38:42,  1.69s/it]11/17/2022 01:22:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.5516e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:49 - INFO - train.train_snli_ve - loss is tensor(0.4371, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5324/6700 [2:28:41<38:48,  1.69s/it]11/17/2022 01:22:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.0619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:50 - INFO - train.train_snli_ve - loss is tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5325/6700 [2:28:43<38:49,  1.69s/it]11/17/2022 01:22:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.6955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:52 - INFO - train.train_snli_ve - loss is tensor(0.7034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  79%|#######9  | 5326/6700 [2:28:45<38:40,  1.69s/it]11/17/2022 01:22:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:54 - INFO - train.train_snli_ve - loss is tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5327/6700 [2:28:46<38:37,  1.69s/it]11/17/2022 01:22:56 - INFO - train.train_snli_ve - kd_loss is tensor(9.8731e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:56 - INFO - train.train_snli_ve - loss is tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5328/6700 [2:28:48<39:07,  1.71s/it]11/17/2022 01:22:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:57 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5329/6700 [2:28:50<38:45,  1.70s/it]11/17/2022 01:22:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4633e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:22:59 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5330/6700 [2:28:52<39:08,  1.71s/it]11/17/2022 01:23:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.4186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:01 - INFO - train.train_snli_ve - loss is tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5331/6700 [2:28:53<38:38,  1.69s/it]11/17/2022 01:23:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:02 - INFO - train.train_snli_ve - loss is tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5332/6700 [2:28:55<38:31,  1.69s/it]11/17/2022 01:23:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:04 - INFO - train.train_snli_ve - loss is tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5333/6700 [2:28:57<38:32,  1.69s/it]11/17/2022 01:23:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.7066e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:06 - INFO - train.train_snli_ve - loss is tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5334/6700 [2:28:58<38:18,  1.68s/it]11/17/2022 01:23:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:07 - INFO - train.train_snli_ve - loss is tensor(0.5013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5335/6700 [2:29:00<38:51,  1.71s/it]11/17/2022 01:23:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0037e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:09 - INFO - train.train_snli_ve - loss is tensor(0.4476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5336/6700 [2:29:02<39:10,  1.72s/it]11/17/2022 01:23:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.2290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:11 - INFO - train.train_snli_ve - loss is tensor(0.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5337/6700 [2:29:04<39:02,  1.72s/it]11/17/2022 01:23:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.5286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:13 - INFO - train.train_snli_ve - loss is tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5338/6700 [2:29:05<38:56,  1.72s/it]11/17/2022 01:23:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.6517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:14 - INFO - train.train_snli_ve - loss is tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5339/6700 [2:29:07<38:37,  1.70s/it]11/17/2022 01:23:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.3075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:16 - INFO - train.train_snli_ve - loss is tensor(0.5100, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5340/6700 [2:29:09<38:36,  1.70s/it]11/17/2022 01:23:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.5457e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:18 - INFO - train.train_snli_ve - loss is tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5341/6700 [2:29:10<38:28,  1.70s/it]11/17/2022 01:23:19 - INFO - train.train_snli_ve - kd_loss is tensor(4.6998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:19 - INFO - train.train_snli_ve - loss is tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5342/6700 [2:29:12<38:33,  1.70s/it]11/17/2022 01:23:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.5548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:21 - INFO - train.train_snli_ve - loss is tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5343/6700 [2:29:14<38:28,  1.70s/it]11/17/2022 01:23:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.3264e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:23 - INFO - train.train_snli_ve - loss is tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5344/6700 [2:29:15<38:29,  1.70s/it]11/17/2022 01:23:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.0218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:24 - INFO - train.train_snli_ve - loss is tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5345/6700 [2:29:17<38:22,  1.70s/it]11/17/2022 01:23:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.5304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:26 - INFO - train.train_snli_ve - loss is tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5346/6700 [2:29:19<38:15,  1.70s/it]11/17/2022 01:23:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:28 - INFO - train.train_snli_ve - loss is tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5347/6700 [2:29:21<38:07,  1.69s/it]11/17/2022 01:23:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.6179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:30 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5348/6700 [2:29:22<38:06,  1.69s/it]11/17/2022 01:23:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.0161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:31 - INFO - train.train_snli_ve - loss is tensor(0.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5349/6700 [2:29:24<38:34,  1.71s/it]11/17/2022 01:23:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7594e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:33 - INFO - train.train_snli_ve - loss is tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5350/6700 [2:29:26<38:17,  1.70s/it]11/17/2022 01:23:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:35 - INFO - train.train_snli_ve - loss is tensor(0.9652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5351/6700 [2:29:27<38:12,  1.70s/it]11/17/2022 01:23:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.1692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:36 - INFO - train.train_snli_ve - loss is tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5352/6700 [2:29:29<38:13,  1.70s/it]11/17/2022 01:23:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.1277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:38 - INFO - train.train_snli_ve - loss is tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5353/6700 [2:29:31<37:51,  1.69s/it]11/17/2022 01:23:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.3106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:40 - INFO - train.train_snli_ve - loss is tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5354/6700 [2:29:32<38:03,  1.70s/it]11/17/2022 01:23:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.4648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:41 - INFO - train.train_snli_ve - loss is tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5355/6700 [2:29:34<37:54,  1.69s/it]11/17/2022 01:23:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:43 - INFO - train.train_snli_ve - loss is tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5356/6700 [2:29:36<37:56,  1.69s/it]11/17/2022 01:23:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.2922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:45 - INFO - train.train_snli_ve - loss is tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5357/6700 [2:29:37<37:55,  1.69s/it]11/17/2022 01:23:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:46 - INFO - train.train_snli_ve - loss is tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5358/6700 [2:29:39<37:55,  1.70s/it]11/17/2022 01:23:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.0024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:48 - INFO - train.train_snli_ve - loss is tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|#######9  | 5359/6700 [2:29:41<37:56,  1.70s/it]11/17/2022 01:23:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.4332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:50 - INFO - train.train_snli_ve - loss is tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5360/6700 [2:29:43<38:09,  1.71s/it]11/17/2022 01:23:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.9107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:52 - INFO - train.train_snli_ve - loss is tensor(0.5301, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5361/6700 [2:29:44<37:56,  1.70s/it]11/17/2022 01:23:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.7467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:53 - INFO - train.train_snli_ve - loss is tensor(0.7835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5362/6700 [2:29:46<37:56,  1.70s/it]11/17/2022 01:23:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.3159e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:55 - INFO - train.train_snli_ve - loss is tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5363/6700 [2:29:48<37:48,  1.70s/it]11/17/2022 01:23:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.7647e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:57 - INFO - train.train_snli_ve - loss is tensor(1.0878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5364/6700 [2:29:49<37:31,  1.69s/it]11/17/2022 01:23:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.6152e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:23:58 - INFO - train.train_snli_ve - loss is tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5365/6700 [2:29:51<37:14,  1.67s/it]11/17/2022 01:24:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:00 - INFO - train.train_snli_ve - loss is tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5366/6700 [2:29:53<37:39,  1.69s/it]11/17/2022 01:24:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.3100e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:02 - INFO - train.train_snli_ve - loss is tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5367/6700 [2:29:54<37:49,  1.70s/it]11/17/2022 01:24:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.5505e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:04 - INFO - train.train_snli_ve - loss is tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5368/6700 [2:29:56<38:01,  1.71s/it]11/17/2022 01:24:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.9270e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:05 - INFO - train.train_snli_ve - loss is tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5369/6700 [2:29:58<37:58,  1.71s/it]11/17/2022 01:24:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.8161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:07 - INFO - train.train_snli_ve - loss is tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5370/6700 [2:30:00<37:58,  1.71s/it]11/17/2022 01:24:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.5047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:09 - INFO - train.train_snli_ve - loss is tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5371/6700 [2:30:01<37:42,  1.70s/it]11/17/2022 01:24:10 - INFO - train.train_snli_ve - kd_loss is tensor(4.6598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:10 - INFO - train.train_snli_ve - loss is tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5372/6700 [2:30:03<37:41,  1.70s/it]11/17/2022 01:24:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:12 - INFO - train.train_snli_ve - loss is tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5373/6700 [2:30:05<37:27,  1.69s/it]11/17/2022 01:24:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:14 - INFO - train.train_snli_ve - loss is tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5374/6700 [2:30:06<37:17,  1.69s/it]11/17/2022 01:24:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.3682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:15 - INFO - train.train_snli_ve - loss is tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5375/6700 [2:30:08<37:18,  1.69s/it]11/17/2022 01:24:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.6693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:17 - INFO - train.train_snli_ve - loss is tensor(0.9456, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5376/6700 [2:30:10<37:09,  1.68s/it]11/17/2022 01:24:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.3609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:19 - INFO - train.train_snli_ve - loss is tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5377/6700 [2:30:11<37:12,  1.69s/it]11/17/2022 01:24:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.8205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:20 - INFO - train.train_snli_ve - loss is tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5378/6700 [2:30:13<37:21,  1.70s/it]11/17/2022 01:24:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:22 - INFO - train.train_snli_ve - loss is tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5379/6700 [2:30:15<37:29,  1.70s/it]11/17/2022 01:24:24 - INFO - train.train_snli_ve - kd_loss is tensor(4.4008e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:24 - INFO - train.train_snli_ve - loss is tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5380/6700 [2:30:17<37:45,  1.72s/it]11/17/2022 01:24:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.4952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:26 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5381/6700 [2:30:18<37:29,  1.71s/it]11/17/2022 01:24:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.6589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:27 - INFO - train.train_snli_ve - loss is tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5382/6700 [2:30:20<37:40,  1.71s/it]11/17/2022 01:24:29 - INFO - train.train_snli_ve - kd_loss is tensor(1.9046e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:29 - INFO - train.train_snli_ve - loss is tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5383/6700 [2:30:22<37:37,  1.71s/it]11/17/2022 01:24:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.3983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:31 - INFO - train.train_snli_ve - loss is tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5384/6700 [2:30:23<37:19,  1.70s/it]11/17/2022 01:24:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:32 - INFO - train.train_snli_ve - loss is tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5385/6700 [2:30:25<37:11,  1.70s/it]11/17/2022 01:24:34 - INFO - train.train_snli_ve - kd_loss is tensor(4.4456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:34 - INFO - train.train_snli_ve - loss is tensor(0.8122, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5386/6700 [2:30:27<37:20,  1.71s/it]11/17/2022 01:24:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.4558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:36 - INFO - train.train_snli_ve - loss is tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5387/6700 [2:30:28<36:57,  1.69s/it]11/17/2022 01:24:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:37 - INFO - train.train_snli_ve - loss is tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5388/6700 [2:30:30<36:56,  1.69s/it]11/17/2022 01:24:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.4168e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:39 - INFO - train.train_snli_ve - loss is tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5389/6700 [2:30:32<36:49,  1.69s/it]11/17/2022 01:24:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:41 - INFO - train.train_snli_ve - loss is tensor(0.4997, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5390/6700 [2:30:34<36:55,  1.69s/it]11/17/2022 01:24:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.9639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:42 - INFO - train.train_snli_ve - loss is tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5391/6700 [2:30:35<36:40,  1.68s/it]11/17/2022 01:24:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7727e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:44 - INFO - train.train_snli_ve - loss is tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5392/6700 [2:30:37<36:18,  1.67s/it]11/17/2022 01:24:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.6384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:46 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  80%|########  | 5393/6700 [2:30:38<36:24,  1.67s/it]11/17/2022 01:24:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1243e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:48 - INFO - train.train_snli_ve - loss is tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5394/6700 [2:30:40<36:35,  1.68s/it]11/17/2022 01:24:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.3484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:49 - INFO - train.train_snli_ve - loss is tensor(0.5810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5395/6700 [2:30:42<36:19,  1.67s/it]11/17/2022 01:24:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.8592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:51 - INFO - train.train_snli_ve - loss is tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5396/6700 [2:30:44<36:41,  1.69s/it]11/17/2022 01:24:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.4113e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:53 - INFO - train.train_snli_ve - loss is tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5397/6700 [2:30:45<36:25,  1.68s/it]11/17/2022 01:24:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.0337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:54 - INFO - train.train_snli_ve - loss is tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5398/6700 [2:30:47<36:18,  1.67s/it]11/17/2022 01:24:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.4380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:56 - INFO - train.train_snli_ve - loss is tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5399/6700 [2:30:49<36:22,  1.68s/it]11/17/2022 01:24:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.3512e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:58 - INFO - train.train_snli_ve - loss is tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5400/6700 [2:30:50<36:23,  1.68s/it]11/17/2022 01:24:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1480e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:24:59 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5401/6700 [2:30:52<36:39,  1.69s/it]11/17/2022 01:25:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.2043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:01 - INFO - train.train_snli_ve - loss is tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5402/6700 [2:30:54<36:43,  1.70s/it]11/17/2022 01:25:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:03 - INFO - train.train_snli_ve - loss is tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5403/6700 [2:30:55<36:34,  1.69s/it]11/17/2022 01:25:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.1784e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:04 - INFO - train.train_snli_ve - loss is tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5404/6700 [2:30:57<36:21,  1.68s/it]11/17/2022 01:25:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:06 - INFO - train.train_snli_ve - loss is tensor(0.8876, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5405/6700 [2:30:59<36:17,  1.68s/it]11/17/2022 01:25:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.6701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:08 - INFO - train.train_snli_ve - loss is tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5406/6700 [2:31:00<36:31,  1.69s/it]11/17/2022 01:25:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.4039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:09 - INFO - train.train_snli_ve - loss is tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5407/6700 [2:31:02<36:30,  1.69s/it]11/17/2022 01:25:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9219e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:11 - INFO - train.train_snli_ve - loss is tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5408/6700 [2:31:04<36:47,  1.71s/it]11/17/2022 01:25:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.1277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:13 - INFO - train.train_snli_ve - loss is tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5409/6700 [2:31:06<36:48,  1.71s/it]11/17/2022 01:25:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.6671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:15 - INFO - train.train_snli_ve - loss is tensor(0.9318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5410/6700 [2:31:07<36:52,  1.72s/it]11/17/2022 01:25:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.2135e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:16 - INFO - train.train_snli_ve - loss is tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5411/6700 [2:31:09<36:38,  1.71s/it]11/17/2022 01:25:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.0477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:18 - INFO - train.train_snli_ve - loss is tensor(0.7305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5412/6700 [2:31:11<36:27,  1.70s/it]11/17/2022 01:25:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:20 - INFO - train.train_snli_ve - loss is tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5413/6700 [2:31:12<36:24,  1.70s/it]11/17/2022 01:25:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.4071e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:21 - INFO - train.train_snli_ve - loss is tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5414/6700 [2:31:14<36:27,  1.70s/it]11/17/2022 01:25:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:23 - INFO - train.train_snli_ve - loss is tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5415/6700 [2:31:16<36:15,  1.69s/it]11/17/2022 01:25:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.0841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:25 - INFO - train.train_snli_ve - loss is tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5416/6700 [2:31:17<36:20,  1.70s/it]11/17/2022 01:25:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:26 - INFO - train.train_snli_ve - loss is tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5417/6700 [2:31:19<36:08,  1.69s/it]11/17/2022 01:25:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4431e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:28 - INFO - train.train_snli_ve - loss is tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5418/6700 [2:31:21<36:20,  1.70s/it]11/17/2022 01:25:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.5462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:30 - INFO - train.train_snli_ve - loss is tensor(0.7429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5419/6700 [2:31:23<36:34,  1.71s/it]11/17/2022 01:25:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.2779e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:32 - INFO - train.train_snli_ve - loss is tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5420/6700 [2:31:24<36:22,  1.70s/it]11/17/2022 01:25:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.6359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:33 - INFO - train.train_snli_ve - loss is tensor(0.9857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5421/6700 [2:31:26<36:06,  1.69s/it]11/17/2022 01:25:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.6671e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:35 - INFO - train.train_snli_ve - loss is tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5422/6700 [2:31:28<36:00,  1.69s/it]11/17/2022 01:25:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.4592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:37 - INFO - train.train_snli_ve - loss is tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5423/6700 [2:31:29<36:11,  1.70s/it]11/17/2022 01:25:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1925e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:38 - INFO - train.train_snli_ve - loss is tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5424/6700 [2:31:31<36:06,  1.70s/it]11/17/2022 01:25:40 - INFO - train.train_snli_ve - kd_loss is tensor(5.3433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:40 - INFO - train.train_snli_ve - loss is tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5425/6700 [2:31:33<36:00,  1.69s/it]11/17/2022 01:25:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:42 - INFO - train.train_snli_ve - loss is tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########  | 5426/6700 [2:31:34<35:54,  1.69s/it]11/17/2022 01:25:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9548e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:43 - INFO - train.train_snli_ve - loss is tensor(0.8376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5427/6700 [2:31:36<35:49,  1.69s/it]11/17/2022 01:25:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.8289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:45 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5428/6700 [2:31:38<35:43,  1.69s/it]11/17/2022 01:25:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.1862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:47 - INFO - train.train_snli_ve - loss is tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5429/6700 [2:31:39<35:36,  1.68s/it]11/17/2022 01:25:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:48 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5430/6700 [2:31:41<35:32,  1.68s/it]11/17/2022 01:25:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.0421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:50 - INFO - train.train_snli_ve - loss is tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5431/6700 [2:31:43<35:50,  1.69s/it]11/17/2022 01:25:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5107e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:52 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5432/6700 [2:31:45<35:36,  1.68s/it]11/17/2022 01:25:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.0106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:54 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5433/6700 [2:31:46<35:32,  1.68s/it]11/17/2022 01:25:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:55 - INFO - train.train_snli_ve - loss is tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5434/6700 [2:31:48<35:31,  1.68s/it]11/17/2022 01:25:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3904e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:57 - INFO - train.train_snli_ve - loss is tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5435/6700 [2:31:50<35:16,  1.67s/it]11/17/2022 01:25:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.5488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:25:59 - INFO - train.train_snli_ve - loss is tensor(0.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5436/6700 [2:31:51<35:36,  1.69s/it]11/17/2022 01:26:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:00 - INFO - train.train_snli_ve - loss is tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5437/6700 [2:31:53<35:37,  1.69s/it]11/17/2022 01:26:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.9612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:02 - INFO - train.train_snli_ve - loss is tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5438/6700 [2:31:55<35:26,  1.68s/it]11/17/2022 01:26:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:04 - INFO - train.train_snli_ve - loss is tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5439/6700 [2:31:56<35:30,  1.69s/it]11/17/2022 01:26:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:05 - INFO - train.train_snli_ve - loss is tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5440/6700 [2:31:58<35:13,  1.68s/it]11/17/2022 01:26:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9186e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:07 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5441/6700 [2:32:00<35:08,  1.68s/it]11/17/2022 01:26:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3817e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:09 - INFO - train.train_snli_ve - loss is tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5442/6700 [2:32:01<35:02,  1.67s/it]11/17/2022 01:26:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.5095e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:10 - INFO - train.train_snli_ve - loss is tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5443/6700 [2:32:03<35:00,  1.67s/it]11/17/2022 01:26:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4873e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:12 - INFO - train.train_snli_ve - loss is tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5444/6700 [2:32:05<35:15,  1.68s/it]11/17/2022 01:26:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.3891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:14 - INFO - train.train_snli_ve - loss is tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5445/6700 [2:32:06<35:15,  1.69s/it]11/17/2022 01:26:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.9804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:15 - INFO - train.train_snli_ve - loss is tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5446/6700 [2:32:08<35:08,  1.68s/it]11/17/2022 01:26:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.6157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:17 - INFO - train.train_snli_ve - loss is tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5447/6700 [2:32:10<35:24,  1.70s/it]11/17/2022 01:26:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2090e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:19 - INFO - train.train_snli_ve - loss is tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5448/6700 [2:32:11<35:11,  1.69s/it]11/17/2022 01:26:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:21 - INFO - train.train_snli_ve - loss is tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5449/6700 [2:32:13<35:31,  1.70s/it]11/17/2022 01:26:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.7748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:22 - INFO - train.train_snli_ve - loss is tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5450/6700 [2:32:15<36:11,  1.74s/it]11/17/2022 01:26:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:24 - INFO - train.train_snli_ve - loss is tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5451/6700 [2:32:17<35:51,  1.72s/it]11/17/2022 01:26:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.3916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:26 - INFO - train.train_snli_ve - loss is tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5452/6700 [2:32:18<35:37,  1.71s/it]11/17/2022 01:26:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:27 - INFO - train.train_snli_ve - loss is tensor(0.7838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5453/6700 [2:32:20<35:16,  1.70s/it]11/17/2022 01:26:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4021e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:29 - INFO - train.train_snli_ve - loss is tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5454/6700 [2:32:22<35:14,  1.70s/it]11/17/2022 01:26:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.4775e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:31 - INFO - train.train_snli_ve - loss is tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5455/6700 [2:32:23<34:58,  1.69s/it]11/17/2022 01:26:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1745e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:32 - INFO - train.train_snli_ve - loss is tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5456/6700 [2:32:25<35:01,  1.69s/it]11/17/2022 01:26:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.7702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:34 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5457/6700 [2:32:27<35:00,  1.69s/it]11/17/2022 01:26:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:36 - INFO - train.train_snli_ve - loss is tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5458/6700 [2:32:28<34:50,  1.68s/it]11/17/2022 01:26:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:37 - INFO - train.train_snli_ve - loss is tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5459/6700 [2:32:30<34:52,  1.69s/it]11/17/2022 01:26:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:39 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  81%|########1 | 5460/6700 [2:32:32<34:44,  1.68s/it]11/17/2022 01:26:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.0515e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:41 - INFO - train.train_snli_ve - loss is tensor(0.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5461/6700 [2:32:34<34:42,  1.68s/it]11/17/2022 01:26:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:43 - INFO - train.train_snli_ve - loss is tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5462/6700 [2:32:35<34:44,  1.68s/it]11/17/2022 01:26:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5845e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:44 - INFO - train.train_snli_ve - loss is tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5463/6700 [2:32:37<34:38,  1.68s/it]11/17/2022 01:26:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.9432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:46 - INFO - train.train_snli_ve - loss is tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5464/6700 [2:32:39<34:49,  1.69s/it]11/17/2022 01:26:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:48 - INFO - train.train_snli_ve - loss is tensor(0.8193, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5465/6700 [2:32:40<34:44,  1.69s/it]11/17/2022 01:26:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.6231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:49 - INFO - train.train_snli_ve - loss is tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5466/6700 [2:32:42<34:23,  1.67s/it]11/17/2022 01:26:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:51 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5467/6700 [2:32:44<34:18,  1.67s/it]11/17/2022 01:26:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.9462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:53 - INFO - train.train_snli_ve - loss is tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5468/6700 [2:32:45<34:26,  1.68s/it]11/17/2022 01:26:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.6563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:54 - INFO - train.train_snli_ve - loss is tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5469/6700 [2:32:47<34:24,  1.68s/it]11/17/2022 01:26:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.7239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:56 - INFO - train.train_snli_ve - loss is tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5470/6700 [2:32:49<34:29,  1.68s/it]11/17/2022 01:26:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.5356e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:58 - INFO - train.train_snli_ve - loss is tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5471/6700 [2:32:50<34:31,  1.69s/it]11/17/2022 01:26:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8147e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:26:59 - INFO - train.train_snli_ve - loss is tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5472/6700 [2:32:52<34:37,  1.69s/it]11/17/2022 01:27:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.7207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:01 - INFO - train.train_snli_ve - loss is tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5473/6700 [2:32:54<34:42,  1.70s/it]11/17/2022 01:27:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.7767e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:03 - INFO - train.train_snli_ve - loss is tensor(0.4552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5474/6700 [2:32:55<34:32,  1.69s/it]11/17/2022 01:27:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.6888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:04 - INFO - train.train_snli_ve - loss is tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5475/6700 [2:32:57<34:48,  1.70s/it]11/17/2022 01:27:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.6272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:06 - INFO - train.train_snli_ve - loss is tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5476/6700 [2:32:59<34:58,  1.71s/it]11/17/2022 01:27:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.9232e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:08 - INFO - train.train_snli_ve - loss is tensor(0.7299, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5477/6700 [2:33:01<34:48,  1.71s/it]11/17/2022 01:27:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0600e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:10 - INFO - train.train_snli_ve - loss is tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5478/6700 [2:33:02<34:47,  1.71s/it]11/17/2022 01:27:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.4468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:11 - INFO - train.train_snli_ve - loss is tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5479/6700 [2:33:04<34:36,  1.70s/it]11/17/2022 01:27:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2070e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:13 - INFO - train.train_snli_ve - loss is tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5480/6700 [2:33:06<34:27,  1.69s/it]11/17/2022 01:27:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.5800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:15 - INFO - train.train_snli_ve - loss is tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5481/6700 [2:33:07<34:17,  1.69s/it]11/17/2022 01:27:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3757e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:16 - INFO - train.train_snli_ve - loss is tensor(0.7834, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5482/6700 [2:33:09<34:14,  1.69s/it]11/17/2022 01:27:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.7923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:18 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5483/6700 [2:33:11<34:00,  1.68s/it]11/17/2022 01:27:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.5923e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:20 - INFO - train.train_snli_ve - loss is tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5484/6700 [2:33:12<33:53,  1.67s/it]11/17/2022 01:27:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.9094e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:21 - INFO - train.train_snli_ve - loss is tensor(0.5113, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5485/6700 [2:33:14<33:40,  1.66s/it]11/17/2022 01:27:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:23 - INFO - train.train_snli_ve - loss is tensor(0.5578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5486/6700 [2:33:16<33:36,  1.66s/it]11/17/2022 01:27:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.2434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:25 - INFO - train.train_snli_ve - loss is tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5487/6700 [2:33:17<33:42,  1.67s/it]11/17/2022 01:27:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:26 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5488/6700 [2:33:19<33:52,  1.68s/it]11/17/2022 01:27:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.5460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:28 - INFO - train.train_snli_ve - loss is tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5489/6700 [2:33:21<33:53,  1.68s/it]11/17/2022 01:27:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5950e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:30 - INFO - train.train_snli_ve - loss is tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5490/6700 [2:33:22<33:58,  1.68s/it]11/17/2022 01:27:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.6938e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:31 - INFO - train.train_snli_ve - loss is tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5491/6700 [2:33:24<34:02,  1.69s/it]11/17/2022 01:27:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.3609e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:33 - INFO - train.train_snli_ve - loss is tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5492/6700 [2:33:26<33:59,  1.69s/it]11/17/2022 01:27:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:35 - INFO - train.train_snli_ve - loss is tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########1 | 5493/6700 [2:33:27<33:55,  1.69s/it]11/17/2022 01:27:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6687e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:36 - INFO - train.train_snli_ve - loss is tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5494/6700 [2:33:29<33:55,  1.69s/it]11/17/2022 01:27:38 - INFO - train.train_snli_ve - kd_loss is tensor(4.1754e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:38 - INFO - train.train_snli_ve - loss is tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5495/6700 [2:33:31<33:52,  1.69s/it]11/17/2022 01:27:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.5581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:40 - INFO - train.train_snli_ve - loss is tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5496/6700 [2:33:33<34:01,  1.70s/it]11/17/2022 01:27:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8540e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:42 - INFO - train.train_snli_ve - loss is tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5497/6700 [2:33:34<34:00,  1.70s/it]11/17/2022 01:27:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.1188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:43 - INFO - train.train_snli_ve - loss is tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5498/6700 [2:33:36<33:51,  1.69s/it]11/17/2022 01:27:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:45 - INFO - train.train_snli_ve - loss is tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5499/6700 [2:33:38<34:11,  1.71s/it]11/17/2022 01:27:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:47 - INFO - train.train_snli_ve - loss is tensor(0.4705, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5500/6700 [2:33:39<34:19,  1.72s/it]11/17/2022 01:27:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.9287e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:48 - INFO - train.train_snli_ve - loss is tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5501/6700 [2:33:41<34:25,  1.72s/it]11/17/2022 01:27:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:50 - INFO - train.train_snli_ve - loss is tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5502/6700 [2:33:43<34:08,  1.71s/it]11/17/2022 01:27:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1461e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:52 - INFO - train.train_snli_ve - loss is tensor(0.5488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5503/6700 [2:33:45<34:13,  1.72s/it]11/17/2022 01:27:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.3266e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:54 - INFO - train.train_snli_ve - loss is tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5504/6700 [2:33:46<34:06,  1.71s/it]11/17/2022 01:27:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:55 - INFO - train.train_snli_ve - loss is tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5505/6700 [2:33:48<34:17,  1.72s/it]11/17/2022 01:27:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5511e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:57 - INFO - train.train_snli_ve - loss is tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5506/6700 [2:33:50<34:05,  1.71s/it]11/17/2022 01:27:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.3031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:27:59 - INFO - train.train_snli_ve - loss is tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5507/6700 [2:33:51<34:04,  1.71s/it]11/17/2022 01:28:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.9462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:00 - INFO - train.train_snli_ve - loss is tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5508/6700 [2:33:53<33:59,  1.71s/it]11/17/2022 01:28:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.1994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:02 - INFO - train.train_snli_ve - loss is tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5509/6700 [2:33:55<34:05,  1.72s/it]11/17/2022 01:28:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9833e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:04 - INFO - train.train_snli_ve - loss is tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5510/6700 [2:33:57<33:39,  1.70s/it]11/17/2022 01:28:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.6542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:06 - INFO - train.train_snli_ve - loss is tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5511/6700 [2:33:58<33:36,  1.70s/it]11/17/2022 01:28:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:07 - INFO - train.train_snli_ve - loss is tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5512/6700 [2:34:00<33:36,  1.70s/it]11/17/2022 01:28:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.2390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:09 - INFO - train.train_snli_ve - loss is tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5513/6700 [2:34:02<33:25,  1.69s/it]11/17/2022 01:28:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.0720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:11 - INFO - train.train_snli_ve - loss is tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5514/6700 [2:34:03<33:27,  1.69s/it]11/17/2022 01:28:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.4822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:12 - INFO - train.train_snli_ve - loss is tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5515/6700 [2:34:05<33:24,  1.69s/it]11/17/2022 01:28:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.4472e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:14 - INFO - train.train_snli_ve - loss is tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5516/6700 [2:34:07<33:18,  1.69s/it]11/17/2022 01:28:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.9195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:16 - INFO - train.train_snli_ve - loss is tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5517/6700 [2:34:08<33:23,  1.69s/it]11/17/2022 01:28:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.4867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:17 - INFO - train.train_snli_ve - loss is tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5518/6700 [2:34:10<33:29,  1.70s/it]11/17/2022 01:28:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.3317e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:19 - INFO - train.train_snli_ve - loss is tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5519/6700 [2:34:12<33:32,  1.70s/it]11/17/2022 01:28:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7657e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:21 - INFO - train.train_snli_ve - loss is tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5520/6700 [2:34:13<33:30,  1.70s/it]11/17/2022 01:28:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.7239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:22 - INFO - train.train_snli_ve - loss is tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5521/6700 [2:34:15<33:17,  1.69s/it]11/17/2022 01:28:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.4477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:24 - INFO - train.train_snli_ve - loss is tensor(0.9714, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5522/6700 [2:34:17<33:15,  1.69s/it]11/17/2022 01:28:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:26 - INFO - train.train_snli_ve - loss is tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5523/6700 [2:34:19<33:24,  1.70s/it]11/17/2022 01:28:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:28 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5524/6700 [2:34:20<32:59,  1.68s/it]11/17/2022 01:28:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0267e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:29 - INFO - train.train_snli_ve - loss is tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5525/6700 [2:34:22<32:49,  1.68s/it]11/17/2022 01:28:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.1982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:31 - INFO - train.train_snli_ve - loss is tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5526/6700 [2:34:24<32:39,  1.67s/it]11/17/2022 01:28:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.6983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:33 - INFO - train.train_snli_ve - loss is tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  82%|########2 | 5527/6700 [2:34:25<32:44,  1.67s/it]11/17/2022 01:28:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:34 - INFO - train.train_snli_ve - loss is tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5528/6700 [2:34:27<32:48,  1.68s/it]11/17/2022 01:28:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.8631e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:36 - INFO - train.train_snli_ve - loss is tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5529/6700 [2:34:29<32:51,  1.68s/it]11/17/2022 01:28:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.7352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:38 - INFO - train.train_snli_ve - loss is tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5530/6700 [2:34:30<32:58,  1.69s/it]11/17/2022 01:28:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.4366e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:39 - INFO - train.train_snli_ve - loss is tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5531/6700 [2:34:32<32:56,  1.69s/it]11/17/2022 01:28:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.1613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:41 - INFO - train.train_snli_ve - loss is tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5532/6700 [2:34:34<32:50,  1.69s/it]11/17/2022 01:28:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.8360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:43 - INFO - train.train_snli_ve - loss is tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5533/6700 [2:34:35<32:46,  1.69s/it]11/17/2022 01:28:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:44 - INFO - train.train_snli_ve - loss is tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5534/6700 [2:34:37<32:41,  1.68s/it]11/17/2022 01:28:46 - INFO - train.train_snli_ve - kd_loss is tensor(4.5954e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:46 - INFO - train.train_snli_ve - loss is tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5535/6700 [2:34:39<32:23,  1.67s/it]11/17/2022 01:28:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.3724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:48 - INFO - train.train_snli_ve - loss is tensor(0.8168, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5536/6700 [2:34:40<32:31,  1.68s/it]11/17/2022 01:28:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.5947e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:49 - INFO - train.train_snli_ve - loss is tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5537/6700 [2:34:42<32:39,  1.68s/it]11/17/2022 01:28:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.3713e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:51 - INFO - train.train_snli_ve - loss is tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5538/6700 [2:34:44<32:51,  1.70s/it]11/17/2022 01:28:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.6358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:53 - INFO - train.train_snli_ve - loss is tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5539/6700 [2:34:45<32:46,  1.69s/it]11/17/2022 01:28:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:55 - INFO - train.train_snli_ve - loss is tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5540/6700 [2:34:47<32:54,  1.70s/it]11/17/2022 01:28:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.3964e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:56 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5541/6700 [2:34:49<33:04,  1.71s/it]11/17/2022 01:28:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.8138e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:28:58 - INFO - train.train_snli_ve - loss is tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5542/6700 [2:34:51<32:48,  1.70s/it]11/17/2022 01:29:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.1750e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:00 - INFO - train.train_snli_ve - loss is tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5543/6700 [2:34:52<32:43,  1.70s/it]11/17/2022 01:29:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.1248e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:01 - INFO - train.train_snli_ve - loss is tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5544/6700 [2:34:54<32:27,  1.68s/it]11/17/2022 01:29:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9121e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:03 - INFO - train.train_snli_ve - loss is tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5545/6700 [2:34:56<32:25,  1.68s/it]11/17/2022 01:29:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.7118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:05 - INFO - train.train_snli_ve - loss is tensor(0.7981, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5546/6700 [2:34:57<32:12,  1.67s/it]11/17/2022 01:29:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.0432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:06 - INFO - train.train_snli_ve - loss is tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5547/6700 [2:34:59<32:15,  1.68s/it]11/17/2022 01:29:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:08 - INFO - train.train_snli_ve - loss is tensor(0.8384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5548/6700 [2:35:01<32:30,  1.69s/it]11/17/2022 01:29:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.5150e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:10 - INFO - train.train_snli_ve - loss is tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5549/6700 [2:35:02<32:30,  1.69s/it]11/17/2022 01:29:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.0698e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:11 - INFO - train.train_snli_ve - loss is tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5550/6700 [2:35:04<32:22,  1.69s/it]11/17/2022 01:29:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.2768e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:13 - INFO - train.train_snli_ve - loss is tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5551/6700 [2:35:06<32:11,  1.68s/it]11/17/2022 01:29:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.5756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:15 - INFO - train.train_snli_ve - loss is tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5552/6700 [2:35:07<32:10,  1.68s/it]11/17/2022 01:29:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:16 - INFO - train.train_snli_ve - loss is tensor(0.7953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5553/6700 [2:35:09<32:01,  1.68s/it]11/17/2022 01:29:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.1240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:18 - INFO - train.train_snli_ve - loss is tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5554/6700 [2:35:11<31:52,  1.67s/it]11/17/2022 01:29:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9643e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:20 - INFO - train.train_snli_ve - loss is tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5555/6700 [2:35:12<31:58,  1.68s/it]11/17/2022 01:29:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.4125e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:21 - INFO - train.train_snli_ve - loss is tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5556/6700 [2:35:14<32:04,  1.68s/it]11/17/2022 01:29:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:23 - INFO - train.train_snli_ve - loss is tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5557/6700 [2:35:16<32:02,  1.68s/it]11/17/2022 01:29:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.9417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:25 - INFO - train.train_snli_ve - loss is tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5558/6700 [2:35:18<32:10,  1.69s/it]11/17/2022 01:29:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.6471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:26 - INFO - train.train_snli_ve - loss is tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5559/6700 [2:35:19<31:49,  1.67s/it]11/17/2022 01:29:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.2879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:28 - INFO - train.train_snli_ve - loss is tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5560/6700 [2:35:21<32:01,  1.69s/it]11/17/2022 01:29:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.4762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:30 - INFO - train.train_snli_ve - loss is tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########2 | 5561/6700 [2:35:23<31:55,  1.68s/it]11/17/2022 01:29:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.8789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:32 - INFO - train.train_snli_ve - loss is tensor(0.8549, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5562/6700 [2:35:24<31:49,  1.68s/it]11/17/2022 01:29:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.7541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:33 - INFO - train.train_snli_ve - loss is tensor(0.5975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5563/6700 [2:35:26<32:07,  1.69s/it]11/17/2022 01:29:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.2215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:35 - INFO - train.train_snli_ve - loss is tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5564/6700 [2:35:28<31:56,  1.69s/it]11/17/2022 01:29:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:37 - INFO - train.train_snli_ve - loss is tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5565/6700 [2:35:29<32:03,  1.69s/it]11/17/2022 01:29:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.2325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:38 - INFO - train.train_snli_ve - loss is tensor(0.7703, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5566/6700 [2:35:31<32:18,  1.71s/it]11/17/2022 01:29:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:40 - INFO - train.train_snli_ve - loss is tensor(0.7545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5567/6700 [2:35:33<31:48,  1.68s/it]11/17/2022 01:29:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8496e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:42 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5568/6700 [2:35:34<32:04,  1.70s/it]11/17/2022 01:29:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:43 - INFO - train.train_snli_ve - loss is tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5569/6700 [2:35:36<32:05,  1.70s/it]11/17/2022 01:29:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1363e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:45 - INFO - train.train_snli_ve - loss is tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5570/6700 [2:35:38<31:56,  1.70s/it]11/17/2022 01:29:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.9237e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:47 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5571/6700 [2:35:40<32:05,  1.71s/it]11/17/2022 01:29:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.8159e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:49 - INFO - train.train_snli_ve - loss is tensor(0.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5572/6700 [2:35:41<32:06,  1.71s/it]11/17/2022 01:29:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9688e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:50 - INFO - train.train_snli_ve - loss is tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5573/6700 [2:35:43<32:08,  1.71s/it]11/17/2022 01:29:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:52 - INFO - train.train_snli_ve - loss is tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5574/6700 [2:35:45<31:57,  1.70s/it]11/17/2022 01:29:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4261e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:54 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5575/6700 [2:35:46<31:37,  1.69s/it]11/17/2022 01:29:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:55 - INFO - train.train_snli_ve - loss is tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5576/6700 [2:35:48<31:45,  1.70s/it]11/17/2022 01:29:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.1054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:57 - INFO - train.train_snli_ve - loss is tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5577/6700 [2:35:50<31:33,  1.69s/it]11/17/2022 01:29:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.5105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:29:59 - INFO - train.train_snli_ve - loss is tensor(0.4481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5578/6700 [2:35:51<31:41,  1.70s/it]11/17/2022 01:30:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0927e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:00 - INFO - train.train_snli_ve - loss is tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5579/6700 [2:35:53<31:42,  1.70s/it]11/17/2022 01:30:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.3821e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:02 - INFO - train.train_snli_ve - loss is tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5580/6700 [2:35:55<31:55,  1.71s/it]11/17/2022 01:30:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.2170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:04 - INFO - train.train_snli_ve - loss is tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5581/6700 [2:35:57<31:44,  1.70s/it]11/17/2022 01:30:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.0745e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:05 - INFO - train.train_snli_ve - loss is tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5582/6700 [2:35:58<31:20,  1.68s/it]11/17/2022 01:30:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.7615e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:07 - INFO - train.train_snli_ve - loss is tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5583/6700 [2:36:00<31:21,  1.68s/it]11/17/2022 01:30:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.3449e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:09 - INFO - train.train_snli_ve - loss is tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5584/6700 [2:36:02<31:18,  1.68s/it]11/17/2022 01:30:11 - INFO - train.train_snli_ve - kd_loss is tensor(4.3790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:11 - INFO - train.train_snli_ve - loss is tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5585/6700 [2:36:03<31:19,  1.69s/it]11/17/2022 01:30:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.3379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:12 - INFO - train.train_snli_ve - loss is tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5586/6700 [2:36:05<31:07,  1.68s/it]11/17/2022 01:30:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.9101e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:14 - INFO - train.train_snli_ve - loss is tensor(0.7455, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5587/6700 [2:36:07<31:07,  1.68s/it]11/17/2022 01:30:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:16 - INFO - train.train_snli_ve - loss is tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5588/6700 [2:36:08<31:13,  1.68s/it]11/17/2022 01:30:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:17 - INFO - train.train_snli_ve - loss is tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5589/6700 [2:36:10<31:03,  1.68s/it]11/17/2022 01:30:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.5268e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:19 - INFO - train.train_snli_ve - loss is tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5590/6700 [2:36:12<31:09,  1.68s/it]11/17/2022 01:30:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.0408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:21 - INFO - train.train_snli_ve - loss is tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5591/6700 [2:36:13<31:13,  1.69s/it]11/17/2022 01:30:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.3310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:22 - INFO - train.train_snli_ve - loss is tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5592/6700 [2:36:15<30:51,  1.67s/it]11/17/2022 01:30:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.9700e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:24 - INFO - train.train_snli_ve - loss is tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5593/6700 [2:36:17<31:01,  1.68s/it]11/17/2022 01:30:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.4803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:26 - INFO - train.train_snli_ve - loss is tensor(0.5263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  83%|########3 | 5594/6700 [2:36:18<31:24,  1.70s/it]11/17/2022 01:30:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.3935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:27 - INFO - train.train_snli_ve - loss is tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5595/6700 [2:36:20<31:18,  1.70s/it]11/17/2022 01:30:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:29 - INFO - train.train_snli_ve - loss is tensor(0.8478, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5596/6700 [2:36:22<31:24,  1.71s/it]11/17/2022 01:30:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.4170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:31 - INFO - train.train_snli_ve - loss is tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5597/6700 [2:36:24<31:26,  1.71s/it]11/17/2022 01:30:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.0325e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:33 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5598/6700 [2:36:25<31:30,  1.72s/it]11/17/2022 01:30:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5188e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:34 - INFO - train.train_snli_ve - loss is tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5599/6700 [2:36:27<31:24,  1.71s/it]11/17/2022 01:30:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.8929e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:36 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5600/6700 [2:36:29<31:35,  1.72s/it]11/17/2022 01:30:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.9113e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:38 - INFO - train.train_snli_ve - loss is tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5601/6700 [2:36:30<31:26,  1.72s/it]11/17/2022 01:30:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.6611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:39 - INFO - train.train_snli_ve - loss is tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5602/6700 [2:36:32<31:16,  1.71s/it]11/17/2022 01:30:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6619e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:41 - INFO - train.train_snli_ve - loss is tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5603/6700 [2:36:34<30:53,  1.69s/it]11/17/2022 01:30:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.2570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:43 - INFO - train.train_snli_ve - loss is tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5604/6700 [2:36:35<30:49,  1.69s/it]11/17/2022 01:30:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.2474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:44 - INFO - train.train_snli_ve - loss is tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5605/6700 [2:36:37<30:37,  1.68s/it]11/17/2022 01:30:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.0780e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:46 - INFO - train.train_snli_ve - loss is tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5606/6700 [2:36:39<30:36,  1.68s/it]11/17/2022 01:30:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6670e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:48 - INFO - train.train_snli_ve - loss is tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5607/6700 [2:36:40<30:24,  1.67s/it]11/17/2022 01:30:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.2205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:49 - INFO - train.train_snli_ve - loss is tensor(0.4702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5608/6700 [2:36:42<30:32,  1.68s/it]11/17/2022 01:30:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.4141e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:51 - INFO - train.train_snli_ve - loss is tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5609/6700 [2:36:44<30:22,  1.67s/it]11/17/2022 01:30:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.1306e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:53 - INFO - train.train_snli_ve - loss is tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5610/6700 [2:36:45<30:29,  1.68s/it]11/17/2022 01:30:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8017e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:55 - INFO - train.train_snli_ve - loss is tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5611/6700 [2:36:47<30:45,  1.69s/it]11/17/2022 01:30:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.7958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:56 - INFO - train.train_snli_ve - loss is tensor(0.4922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5612/6700 [2:36:49<30:46,  1.70s/it]11/17/2022 01:30:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:30:58 - INFO - train.train_snli_ve - loss is tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5613/6700 [2:36:51<30:30,  1.68s/it]11/17/2022 01:31:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.7178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:00 - INFO - train.train_snli_ve - loss is tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5614/6700 [2:36:52<30:40,  1.70s/it]11/17/2022 01:31:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.6139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:01 - INFO - train.train_snli_ve - loss is tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5615/6700 [2:36:54<30:31,  1.69s/it]11/17/2022 01:31:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:03 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5616/6700 [2:36:56<30:37,  1.69s/it]11/17/2022 01:31:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.4683e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:05 - INFO - train.train_snli_ve - loss is tensor(0.8309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5617/6700 [2:36:57<30:46,  1.71s/it]11/17/2022 01:31:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.5081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:06 - INFO - train.train_snli_ve - loss is tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5618/6700 [2:36:59<30:33,  1.69s/it]11/17/2022 01:31:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7395e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:08 - INFO - train.train_snli_ve - loss is tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5619/6700 [2:37:01<30:27,  1.69s/it]11/17/2022 01:31:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.3467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:10 - INFO - train.train_snli_ve - loss is tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5620/6700 [2:37:02<30:22,  1.69s/it]11/17/2022 01:31:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.9200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:11 - INFO - train.train_snli_ve - loss is tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5621/6700 [2:37:04<30:04,  1.67s/it]11/17/2022 01:31:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:13 - INFO - train.train_snli_ve - loss is tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5622/6700 [2:37:06<29:49,  1.66s/it]11/17/2022 01:31:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.0520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:15 - INFO - train.train_snli_ve - loss is tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5623/6700 [2:37:07<29:54,  1.67s/it]11/17/2022 01:31:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6993e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:16 - INFO - train.train_snli_ve - loss is tensor(0.9001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5624/6700 [2:37:09<29:44,  1.66s/it]11/17/2022 01:31:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:18 - INFO - train.train_snli_ve - loss is tensor(0.7833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5625/6700 [2:37:11<29:43,  1.66s/it]11/17/2022 01:31:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6790e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:20 - INFO - train.train_snli_ve - loss is tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5626/6700 [2:37:12<29:48,  1.67s/it]11/17/2022 01:31:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.4224e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:21 - INFO - train.train_snli_ve - loss is tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########3 | 5627/6700 [2:37:14<30:00,  1.68s/it]11/17/2022 01:31:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2963e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:23 - INFO - train.train_snli_ve - loss is tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5628/6700 [2:37:16<30:02,  1.68s/it]11/17/2022 01:31:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.2960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:25 - INFO - train.train_snli_ve - loss is tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5629/6700 [2:37:17<30:17,  1.70s/it]11/17/2022 01:31:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.0681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:27 - INFO - train.train_snli_ve - loss is tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5630/6700 [2:37:19<30:21,  1.70s/it]11/17/2022 01:31:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.7987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:28 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5631/6700 [2:37:21<30:14,  1.70s/it]11/17/2022 01:31:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.0881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:30 - INFO - train.train_snli_ve - loss is tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5632/6700 [2:37:23<30:22,  1.71s/it]11/17/2022 01:31:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.9892e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:32 - INFO - train.train_snli_ve - loss is tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5633/6700 [2:37:24<30:07,  1.69s/it]11/17/2022 01:31:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:33 - INFO - train.train_snli_ve - loss is tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5634/6700 [2:37:26<29:52,  1.68s/it]11/17/2022 01:31:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.6599e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:35 - INFO - train.train_snli_ve - loss is tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5635/6700 [2:37:28<29:45,  1.68s/it]11/17/2022 01:31:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.0459e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:37 - INFO - train.train_snli_ve - loss is tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5636/6700 [2:37:29<29:46,  1.68s/it]11/17/2022 01:31:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:38 - INFO - train.train_snli_ve - loss is tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5637/6700 [2:37:31<29:59,  1.69s/it]11/17/2022 01:31:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.9468e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:40 - INFO - train.train_snli_ve - loss is tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5638/6700 [2:37:33<30:03,  1.70s/it]11/17/2022 01:31:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.6279e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:42 - INFO - train.train_snli_ve - loss is tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5639/6700 [2:37:34<30:00,  1.70s/it]11/17/2022 01:31:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:43 - INFO - train.train_snli_ve - loss is tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5640/6700 [2:37:36<29:59,  1.70s/it]11/17/2022 01:31:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.7358e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:45 - INFO - train.train_snli_ve - loss is tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5641/6700 [2:37:38<29:56,  1.70s/it]11/17/2022 01:31:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:47 - INFO - train.train_snli_ve - loss is tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5642/6700 [2:37:39<29:46,  1.69s/it]11/17/2022 01:31:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:48 - INFO - train.train_snli_ve - loss is tensor(0.5900, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5643/6700 [2:37:41<29:49,  1.69s/it]11/17/2022 01:31:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:50 - INFO - train.train_snli_ve - loss is tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5644/6700 [2:37:43<29:57,  1.70s/it]11/17/2022 01:31:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.8433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:52 - INFO - train.train_snli_ve - loss is tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5645/6700 [2:37:45<29:51,  1.70s/it]11/17/2022 01:31:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:54 - INFO - train.train_snli_ve - loss is tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5646/6700 [2:37:46<29:54,  1.70s/it]11/17/2022 01:31:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.1075e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:55 - INFO - train.train_snli_ve - loss is tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5647/6700 [2:37:48<29:42,  1.69s/it]11/17/2022 01:31:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2794e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:57 - INFO - train.train_snli_ve - loss is tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5648/6700 [2:37:50<29:47,  1.70s/it]11/17/2022 01:31:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:31:59 - INFO - train.train_snli_ve - loss is tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5649/6700 [2:37:51<29:49,  1.70s/it]11/17/2022 01:32:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.6958e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:00 - INFO - train.train_snli_ve - loss is tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5650/6700 [2:37:53<29:40,  1.70s/it]11/17/2022 01:32:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.6785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:02 - INFO - train.train_snli_ve - loss is tensor(0.8079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5651/6700 [2:37:55<29:39,  1.70s/it]11/17/2022 01:32:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.1915e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:04 - INFO - train.train_snli_ve - loss is tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5652/6700 [2:37:56<29:35,  1.69s/it]11/17/2022 01:32:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.7913e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:06 - INFO - train.train_snli_ve - loss is tensor(0.9069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5653/6700 [2:37:58<29:53,  1.71s/it]11/17/2022 01:32:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.8283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:07 - INFO - train.train_snli_ve - loss is tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5654/6700 [2:38:00<29:29,  1.69s/it]11/17/2022 01:32:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.5545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:09 - INFO - train.train_snli_ve - loss is tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5655/6700 [2:38:02<29:38,  1.70s/it]11/17/2022 01:32:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.2590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:11 - INFO - train.train_snli_ve - loss is tensor(0.9412, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5656/6700 [2:38:03<29:33,  1.70s/it]11/17/2022 01:32:12 - INFO - train.train_snli_ve - kd_loss is tensor(5.2486e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:12 - INFO - train.train_snli_ve - loss is tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5657/6700 [2:38:05<29:19,  1.69s/it]11/17/2022 01:32:14 - INFO - train.train_snli_ve - kd_loss is tensor(4.2026e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:14 - INFO - train.train_snli_ve - loss is tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5658/6700 [2:38:07<29:27,  1.70s/it]11/17/2022 01:32:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.5649e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:16 - INFO - train.train_snli_ve - loss is tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5659/6700 [2:38:08<29:34,  1.70s/it]11/17/2022 01:32:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:17 - INFO - train.train_snli_ve - loss is tensor(0.4829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5660/6700 [2:38:10<29:47,  1.72s/it]11/17/2022 01:32:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9620e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:19 - INFO - train.train_snli_ve - loss is tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  84%|########4 | 5661/6700 [2:38:12<29:43,  1.72s/it]11/17/2022 01:32:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.5886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:21 - INFO - train.train_snli_ve - loss is tensor(0.4294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5662/6700 [2:38:14<29:23,  1.70s/it]11/17/2022 01:32:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9886e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:22 - INFO - train.train_snli_ve - loss is tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5663/6700 [2:38:15<29:14,  1.69s/it]11/17/2022 01:32:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.9311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:24 - INFO - train.train_snli_ve - loss is tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5664/6700 [2:38:17<29:32,  1.71s/it]11/17/2022 01:32:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.8009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:26 - INFO - train.train_snli_ve - loss is tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5665/6700 [2:38:19<29:17,  1.70s/it]11/17/2022 01:32:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.8712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:28 - INFO - train.train_snli_ve - loss is tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5666/6700 [2:38:20<29:19,  1.70s/it]11/17/2022 01:32:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.2479e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:29 - INFO - train.train_snli_ve - loss is tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5667/6700 [2:38:22<29:16,  1.70s/it]11/17/2022 01:32:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.9131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:31 - INFO - train.train_snli_ve - loss is tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5668/6700 [2:38:24<28:57,  1.68s/it]11/17/2022 01:32:33 - INFO - train.train_snli_ve - kd_loss is tensor(3.3225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:33 - INFO - train.train_snli_ve - loss is tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5669/6700 [2:38:25<28:54,  1.68s/it]11/17/2022 01:32:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.0074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:34 - INFO - train.train_snli_ve - loss is tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5670/6700 [2:38:27<28:43,  1.67s/it]11/17/2022 01:32:36 - INFO - train.train_snli_ve - kd_loss is tensor(4.2414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:36 - INFO - train.train_snli_ve - loss is tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5671/6700 [2:38:29<28:50,  1.68s/it]11/17/2022 01:32:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.5581e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:38 - INFO - train.train_snli_ve - loss is tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5672/6700 [2:38:30<28:59,  1.69s/it]11/17/2022 01:32:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4949e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:39 - INFO - train.train_snli_ve - loss is tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5673/6700 [2:38:32<29:06,  1.70s/it]11/17/2022 01:32:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:41 - INFO - train.train_snli_ve - loss is tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5674/6700 [2:38:34<29:12,  1.71s/it]11/17/2022 01:32:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:43 - INFO - train.train_snli_ve - loss is tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5675/6700 [2:38:36<29:00,  1.70s/it]11/17/2022 01:32:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.7611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:45 - INFO - train.train_snli_ve - loss is tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5676/6700 [2:38:37<29:05,  1.70s/it]11/17/2022 01:32:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:46 - INFO - train.train_snli_ve - loss is tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5677/6700 [2:38:39<28:56,  1.70s/it]11/17/2022 01:32:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.0332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:48 - INFO - train.train_snli_ve - loss is tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5678/6700 [2:38:41<28:59,  1.70s/it]11/17/2022 01:32:50 - INFO - train.train_snli_ve - kd_loss is tensor(4.6172e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:50 - INFO - train.train_snli_ve - loss is tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5679/6700 [2:38:42<28:58,  1.70s/it]11/17/2022 01:32:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.5164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:51 - INFO - train.train_snli_ve - loss is tensor(0.5610, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5680/6700 [2:38:44<28:49,  1.70s/it]11/17/2022 01:32:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.7513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:53 - INFO - train.train_snli_ve - loss is tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5681/6700 [2:38:46<28:47,  1.70s/it]11/17/2022 01:32:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.0338e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:55 - INFO - train.train_snli_ve - loss is tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5682/6700 [2:38:47<28:46,  1.70s/it]11/17/2022 01:32:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:56 - INFO - train.train_snli_ve - loss is tensor(0.7831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5683/6700 [2:38:49<28:44,  1.70s/it]11/17/2022 01:32:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.7999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:32:58 - INFO - train.train_snli_ve - loss is tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5684/6700 [2:38:51<28:47,  1.70s/it]11/17/2022 01:33:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.5666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:00 - INFO - train.train_snli_ve - loss is tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5685/6700 [2:38:53<28:43,  1.70s/it]11/17/2022 01:33:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.7033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:02 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5686/6700 [2:38:54<28:37,  1.69s/it]11/17/2022 01:33:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:03 - INFO - train.train_snli_ve - loss is tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5687/6700 [2:38:56<28:44,  1.70s/it]11/17/2022 01:33:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:05 - INFO - train.train_snli_ve - loss is tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5688/6700 [2:38:58<28:53,  1.71s/it]11/17/2022 01:33:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.3419e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:07 - INFO - train.train_snli_ve - loss is tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5689/6700 [2:38:59<28:49,  1.71s/it]11/17/2022 01:33:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.6809e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:08 - INFO - train.train_snli_ve - loss is tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5690/6700 [2:39:01<28:44,  1.71s/it]11/17/2022 01:33:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.3815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:10 - INFO - train.train_snli_ve - loss is tensor(0.7717, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5691/6700 [2:39:03<28:31,  1.70s/it]11/17/2022 01:33:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.6162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:12 - INFO - train.train_snli_ve - loss is tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5692/6700 [2:39:04<28:27,  1.69s/it]11/17/2022 01:33:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.6043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:13 - INFO - train.train_snli_ve - loss is tensor(0.8170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5693/6700 [2:39:06<28:10,  1.68s/it]11/17/2022 01:33:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.4971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:15 - INFO - train.train_snli_ve - loss is tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########4 | 5694/6700 [2:39:08<28:12,  1.68s/it]11/17/2022 01:33:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:17 - INFO - train.train_snli_ve - loss is tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5695/6700 [2:39:09<28:09,  1.68s/it]11/17/2022 01:33:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.0215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:18 - INFO - train.train_snli_ve - loss is tensor(0.8211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5696/6700 [2:39:11<28:01,  1.68s/it]11/17/2022 01:33:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.2606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:20 - INFO - train.train_snli_ve - loss is tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5697/6700 [2:39:13<27:57,  1.67s/it]11/17/2022 01:33:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2646e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:22 - INFO - train.train_snli_ve - loss is tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5698/6700 [2:39:14<27:55,  1.67s/it]11/17/2022 01:33:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.9753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:23 - INFO - train.train_snli_ve - loss is tensor(0.8107, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5699/6700 [2:39:16<28:14,  1.69s/it]11/17/2022 01:33:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.5693e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:25 - INFO - train.train_snli_ve - loss is tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5700/6700 [2:39:18<28:14,  1.69s/it]11/17/2022 01:33:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.7801e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:27 - INFO - train.train_snli_ve - loss is tensor(0.7786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5701/6700 [2:39:20<28:20,  1.70s/it]11/17/2022 01:33:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.8195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:29 - INFO - train.train_snli_ve - loss is tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5702/6700 [2:39:21<28:21,  1.70s/it]11/17/2022 01:33:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.4787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:30 - INFO - train.train_snli_ve - loss is tensor(0.9112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5703/6700 [2:39:23<28:14,  1.70s/it]11/17/2022 01:33:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:32 - INFO - train.train_snli_ve - loss is tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5704/6700 [2:39:25<28:02,  1.69s/it]11/17/2022 01:33:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1747e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:34 - INFO - train.train_snli_ve - loss is tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5705/6700 [2:39:26<27:54,  1.68s/it]11/17/2022 01:33:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3606e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:35 - INFO - train.train_snli_ve - loss is tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5706/6700 [2:39:28<28:01,  1.69s/it]11/17/2022 01:33:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.1863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:37 - INFO - train.train_snli_ve - loss is tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5707/6700 [2:39:30<28:04,  1.70s/it]11/17/2022 01:33:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:39 - INFO - train.train_snli_ve - loss is tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5708/6700 [2:39:31<27:58,  1.69s/it]11/17/2022 01:33:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1660e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:40 - INFO - train.train_snli_ve - loss is tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5709/6700 [2:39:33<28:06,  1.70s/it]11/17/2022 01:33:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:42 - INFO - train.train_snli_ve - loss is tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5710/6700 [2:39:35<27:53,  1.69s/it]11/17/2022 01:33:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.2493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:44 - INFO - train.train_snli_ve - loss is tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5711/6700 [2:39:36<27:33,  1.67s/it]11/17/2022 01:33:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.3840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:45 - INFO - train.train_snli_ve - loss is tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5712/6700 [2:39:38<27:27,  1.67s/it]11/17/2022 01:33:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.2223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:47 - INFO - train.train_snli_ve - loss is tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5713/6700 [2:39:40<27:14,  1.66s/it]11/17/2022 01:33:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.9328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:49 - INFO - train.train_snli_ve - loss is tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5714/6700 [2:39:41<27:21,  1.67s/it]11/17/2022 01:33:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.4289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:50 - INFO - train.train_snli_ve - loss is tensor(0.5065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5715/6700 [2:39:43<27:26,  1.67s/it]11/17/2022 01:33:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5269e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:52 - INFO - train.train_snli_ve - loss is tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5716/6700 [2:39:45<27:35,  1.68s/it]11/17/2022 01:33:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.9962e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:54 - INFO - train.train_snli_ve - loss is tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5717/6700 [2:39:46<27:29,  1.68s/it]11/17/2022 01:33:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.9663e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:55 - INFO - train.train_snli_ve - loss is tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5718/6700 [2:39:48<27:11,  1.66s/it]11/17/2022 01:33:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.6799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:57 - INFO - train.train_snli_ve - loss is tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5719/6700 [2:39:50<27:01,  1.65s/it]11/17/2022 01:33:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4699e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:33:59 - INFO - train.train_snli_ve - loss is tensor(0.4427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5720/6700 [2:39:51<26:51,  1.64s/it]11/17/2022 01:34:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:00 - INFO - train.train_snli_ve - loss is tensor(0.6546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5721/6700 [2:39:53<26:32,  1.63s/it]11/17/2022 01:34:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:02 - INFO - train.train_snli_ve - loss is tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5722/6700 [2:39:55<26:21,  1.62s/it]11/17/2022 01:34:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9354e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:03 - INFO - train.train_snli_ve - loss is tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5723/6700 [2:39:56<26:22,  1.62s/it]11/17/2022 01:34:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.2146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:05 - INFO - train.train_snli_ve - loss is tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5724/6700 [2:39:58<26:13,  1.61s/it]11/17/2022 01:34:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.3893e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:07 - INFO - train.train_snli_ve - loss is tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5725/6700 [2:39:59<26:34,  1.64s/it]11/17/2022 01:34:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:08 - INFO - train.train_snli_ve - loss is tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5726/6700 [2:40:01<26:28,  1.63s/it]11/17/2022 01:34:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.4972e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:10 - INFO - train.train_snli_ve - loss is tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5727/6700 [2:40:03<26:26,  1.63s/it]11/17/2022 01:34:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.7759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:12 - INFO - train.train_snli_ve - loss is tensor(0.7866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  85%|########5 | 5728/6700 [2:40:04<26:29,  1.64s/it]11/17/2022 01:34:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.9002e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:13 - INFO - train.train_snli_ve - loss is tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5729/6700 [2:40:06<26:23,  1.63s/it]11/17/2022 01:34:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.9477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:15 - INFO - train.train_snli_ve - loss is tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5730/6700 [2:40:08<26:22,  1.63s/it]11/17/2022 01:34:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.1124e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:17 - INFO - train.train_snli_ve - loss is tensor(0.6539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5731/6700 [2:40:09<26:09,  1.62s/it]11/17/2022 01:34:18 - INFO - train.train_snli_ve - kd_loss is tensor(4.0881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:18 - INFO - train.train_snli_ve - loss is tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5732/6700 [2:40:11<25:56,  1.61s/it]11/17/2022 01:34:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.8720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:20 - INFO - train.train_snli_ve - loss is tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5733/6700 [2:40:12<26:12,  1.63s/it]11/17/2022 01:34:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.8735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:21 - INFO - train.train_snli_ve - loss is tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5734/6700 [2:40:14<26:10,  1.63s/it]11/17/2022 01:34:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4828e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:23 - INFO - train.train_snli_ve - loss is tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5735/6700 [2:40:16<25:54,  1.61s/it]11/17/2022 01:34:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.6911e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:25 - INFO - train.train_snli_ve - loss is tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5736/6700 [2:40:17<26:00,  1.62s/it]11/17/2022 01:34:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.9013e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:26 - INFO - train.train_snli_ve - loss is tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5737/6700 [2:40:19<25:53,  1.61s/it]11/17/2022 01:34:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.6701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:28 - INFO - train.train_snli_ve - loss is tensor(0.9627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5738/6700 [2:40:20<25:53,  1.61s/it]11/17/2022 01:34:29 - INFO - train.train_snli_ve - kd_loss is tensor(4.2040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:29 - INFO - train.train_snli_ve - loss is tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5739/6700 [2:40:22<25:43,  1.61s/it]11/17/2022 01:34:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:31 - INFO - train.train_snli_ve - loss is tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5740/6700 [2:40:24<25:51,  1.62s/it]11/17/2022 01:34:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.1136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:33 - INFO - train.train_snli_ve - loss is tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5741/6700 [2:40:25<25:49,  1.62s/it]11/17/2022 01:34:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.8389e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:34 - INFO - train.train_snli_ve - loss is tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5742/6700 [2:40:27<26:05,  1.63s/it]11/17/2022 01:34:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:36 - INFO - train.train_snli_ve - loss is tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5743/6700 [2:40:29<25:48,  1.62s/it]11/17/2022 01:34:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.5429e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:38 - INFO - train.train_snli_ve - loss is tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5744/6700 [2:40:30<25:49,  1.62s/it]11/17/2022 01:34:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.8143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:39 - INFO - train.train_snli_ve - loss is tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5745/6700 [2:40:32<25:35,  1.61s/it]11/17/2022 01:34:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.7561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:41 - INFO - train.train_snli_ve - loss is tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5746/6700 [2:40:33<25:29,  1.60s/it]11/17/2022 01:34:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.4812e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:42 - INFO - train.train_snli_ve - loss is tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5747/6700 [2:40:35<25:39,  1.61s/it]11/17/2022 01:34:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.5933e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:44 - INFO - train.train_snli_ve - loss is tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5748/6700 [2:40:37<25:38,  1.62s/it]11/17/2022 01:34:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.9529e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:46 - INFO - train.train_snli_ve - loss is tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5749/6700 [2:40:38<25:30,  1.61s/it]11/17/2022 01:34:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.4800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:47 - INFO - train.train_snli_ve - loss is tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5750/6700 [2:40:40<25:29,  1.61s/it]11/17/2022 01:34:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.1855e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:49 - INFO - train.train_snli_ve - loss is tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5751/6700 [2:40:41<25:34,  1.62s/it]11/17/2022 01:34:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.5857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:50 - INFO - train.train_snli_ve - loss is tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5752/6700 [2:40:43<25:34,  1.62s/it]11/17/2022 01:34:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:52 - INFO - train.train_snli_ve - loss is tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5753/6700 [2:40:45<25:43,  1.63s/it]11/17/2022 01:34:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5293e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:54 - INFO - train.train_snli_ve - loss is tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5754/6700 [2:40:46<25:35,  1.62s/it]11/17/2022 01:34:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.9973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:55 - INFO - train.train_snli_ve - loss is tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5755/6700 [2:40:48<25:32,  1.62s/it]11/17/2022 01:34:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1078e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:57 - INFO - train.train_snli_ve - loss is tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5756/6700 [2:40:50<25:27,  1.62s/it]11/17/2022 01:34:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0083e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:34:59 - INFO - train.train_snli_ve - loss is tensor(0.9844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5757/6700 [2:40:51<25:20,  1.61s/it]11/17/2022 01:35:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2029e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:00 - INFO - train.train_snli_ve - loss is tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5758/6700 [2:40:53<25:13,  1.61s/it]11/17/2022 01:35:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:02 - INFO - train.train_snli_ve - loss is tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5759/6700 [2:40:54<25:09,  1.60s/it]11/17/2022 01:35:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.5191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:03 - INFO - train.train_snli_ve - loss is tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5760/6700 [2:40:56<25:06,  1.60s/it]11/17/2022 01:35:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.0692e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:05 - INFO - train.train_snli_ve - loss is tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########5 | 5761/6700 [2:40:58<25:13,  1.61s/it]11/17/2022 01:35:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.2689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:07 - INFO - train.train_snli_ve - loss is tensor(0.8843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5762/6700 [2:40:59<25:10,  1.61s/it]11/17/2022 01:35:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:08 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5763/6700 [2:41:01<25:06,  1.61s/it]11/17/2022 01:35:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.4737e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:10 - INFO - train.train_snli_ve - loss is tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5764/6700 [2:41:02<25:12,  1.62s/it]11/17/2022 01:35:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3842e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:11 - INFO - train.train_snli_ve - loss is tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5765/6700 [2:41:04<25:10,  1.62s/it]11/17/2022 01:35:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.1310e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:13 - INFO - train.train_snli_ve - loss is tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5766/6700 [2:41:06<25:11,  1.62s/it]11/17/2022 01:35:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.6277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:15 - INFO - train.train_snli_ve - loss is tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5767/6700 [2:41:07<25:03,  1.61s/it]11/17/2022 01:35:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:16 - INFO - train.train_snli_ve - loss is tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5768/6700 [2:41:09<25:06,  1.62s/it]11/17/2022 01:35:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.7753e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:18 - INFO - train.train_snli_ve - loss is tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5769/6700 [2:41:11<24:56,  1.61s/it]11/17/2022 01:35:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.2593e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:19 - INFO - train.train_snli_ve - loss is tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5770/6700 [2:41:12<24:56,  1.61s/it]11/17/2022 01:35:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.4414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:21 - INFO - train.train_snli_ve - loss is tensor(0.8391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5771/6700 [2:41:14<24:50,  1.60s/it]11/17/2022 01:35:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.9116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:23 - INFO - train.train_snli_ve - loss is tensor(0.8487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5772/6700 [2:41:15<24:48,  1.60s/it]11/17/2022 01:35:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.8495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:24 - INFO - train.train_snli_ve - loss is tensor(0.4684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5773/6700 [2:41:17<24:38,  1.60s/it]11/17/2022 01:35:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.7545e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:26 - INFO - train.train_snli_ve - loss is tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5774/6700 [2:41:19<24:50,  1.61s/it]11/17/2022 01:35:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.8036e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:28 - INFO - train.train_snli_ve - loss is tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5775/6700 [2:41:20<25:01,  1.62s/it]11/17/2022 01:35:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.8675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:29 - INFO - train.train_snli_ve - loss is tensor(0.5182, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5776/6700 [2:41:22<24:54,  1.62s/it]11/17/2022 01:35:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.8604e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:31 - INFO - train.train_snli_ve - loss is tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5777/6700 [2:41:23<24:54,  1.62s/it]11/17/2022 01:35:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.2839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:32 - INFO - train.train_snli_ve - loss is tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5778/6700 [2:41:25<24:55,  1.62s/it]11/17/2022 01:35:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.4696e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:34 - INFO - train.train_snli_ve - loss is tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5779/6700 [2:41:27<24:58,  1.63s/it]11/17/2022 01:35:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:36 - INFO - train.train_snli_ve - loss is tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5780/6700 [2:41:28<24:57,  1.63s/it]11/17/2022 01:35:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.9128e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:37 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5781/6700 [2:41:30<24:52,  1.62s/it]11/17/2022 01:35:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.7541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:39 - INFO - train.train_snli_ve - loss is tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5782/6700 [2:41:32<24:51,  1.63s/it]11/17/2022 01:35:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.6703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:41 - INFO - train.train_snli_ve - loss is tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5783/6700 [2:41:33<24:56,  1.63s/it]11/17/2022 01:35:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.4822e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:42 - INFO - train.train_snli_ve - loss is tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5784/6700 [2:41:35<24:57,  1.63s/it]11/17/2022 01:35:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.6156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:44 - INFO - train.train_snli_ve - loss is tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5785/6700 [2:41:36<24:49,  1.63s/it]11/17/2022 01:35:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.3535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:45 - INFO - train.train_snli_ve - loss is tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5786/6700 [2:41:38<24:36,  1.62s/it]11/17/2022 01:35:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3328e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:47 - INFO - train.train_snli_ve - loss is tensor(0.9115, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5787/6700 [2:41:40<24:26,  1.61s/it]11/17/2022 01:35:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.8160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:49 - INFO - train.train_snli_ve - loss is tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5788/6700 [2:41:41<24:24,  1.61s/it]11/17/2022 01:35:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9864e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:50 - INFO - train.train_snli_ve - loss is tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5789/6700 [2:41:43<24:16,  1.60s/it]11/17/2022 01:35:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.2586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:52 - INFO - train.train_snli_ve - loss is tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5790/6700 [2:41:44<24:12,  1.60s/it]11/17/2022 01:35:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:53 - INFO - train.train_snli_ve - loss is tensor(0.5034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5791/6700 [2:41:46<24:09,  1.59s/it]11/17/2022 01:35:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8463e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:55 - INFO - train.train_snli_ve - loss is tensor(0.9499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5792/6700 [2:41:48<24:18,  1.61s/it]11/17/2022 01:35:57 - INFO - train.train_snli_ve - kd_loss is tensor(1.9601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:57 - INFO - train.train_snli_ve - loss is tensor(0.8546, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5793/6700 [2:41:49<24:15,  1.60s/it]11/17/2022 01:35:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.2488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:35:58 - INFO - train.train_snli_ve - loss is tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5794/6700 [2:41:51<24:14,  1.61s/it]11/17/2022 01:36:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.8294e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:00 - INFO - train.train_snli_ve - loss is tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  86%|########6 | 5795/6700 [2:41:52<24:17,  1.61s/it]11/17/2022 01:36:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.0454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:01 - INFO - train.train_snli_ve - loss is tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5796/6700 [2:41:54<24:09,  1.60s/it]11/17/2022 01:36:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:03 - INFO - train.train_snli_ve - loss is tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5797/6700 [2:41:56<24:09,  1.60s/it]11/17/2022 01:36:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.7999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:05 - INFO - train.train_snli_ve - loss is tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5798/6700 [2:41:57<24:02,  1.60s/it]11/17/2022 01:36:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.8541e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:06 - INFO - train.train_snli_ve - loss is tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5799/6700 [2:41:59<23:55,  1.59s/it]11/17/2022 01:36:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5296e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:08 - INFO - train.train_snli_ve - loss is tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5800/6700 [2:42:00<24:00,  1.60s/it]11/17/2022 01:36:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.3565e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:09 - INFO - train.train_snli_ve - loss is tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5801/6700 [2:42:02<24:01,  1.60s/it]11/17/2022 01:36:11 - INFO - train.train_snli_ve - kd_loss is tensor(1.5290e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:11 - INFO - train.train_snli_ve - loss is tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5802/6700 [2:42:04<24:01,  1.61s/it]11/17/2022 01:36:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:13 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5803/6700 [2:42:05<24:04,  1.61s/it]11/17/2022 01:36:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.4645e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:14 - INFO - train.train_snli_ve - loss is tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5804/6700 [2:42:07<24:04,  1.61s/it]11/17/2022 01:36:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.6939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:16 - INFO - train.train_snli_ve - loss is tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5805/6700 [2:42:08<23:55,  1.60s/it]11/17/2022 01:36:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:17 - INFO - train.train_snli_ve - loss is tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5806/6700 [2:42:10<24:02,  1.61s/it]11/17/2022 01:36:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.4745e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:19 - INFO - train.train_snli_ve - loss is tensor(0.9133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5807/6700 [2:42:12<23:50,  1.60s/it]11/17/2022 01:36:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1479e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:21 - INFO - train.train_snli_ve - loss is tensor(0.8739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5808/6700 [2:42:13<23:49,  1.60s/it]11/17/2022 01:36:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.4402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:22 - INFO - train.train_snli_ve - loss is tensor(0.8861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5809/6700 [2:42:15<24:00,  1.62s/it]11/17/2022 01:36:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.0062e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:24 - INFO - train.train_snli_ve - loss is tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5810/6700 [2:42:17<24:04,  1.62s/it]11/17/2022 01:36:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.7223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:26 - INFO - train.train_snli_ve - loss is tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5811/6700 [2:42:18<24:05,  1.63s/it]11/17/2022 01:36:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.1305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:27 - INFO - train.train_snli_ve - loss is tensor(0.7415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5812/6700 [2:42:20<24:08,  1.63s/it]11/17/2022 01:36:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.0838e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:29 - INFO - train.train_snli_ve - loss is tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5813/6700 [2:42:21<24:00,  1.62s/it]11/17/2022 01:36:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.5902e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:30 - INFO - train.train_snli_ve - loss is tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5814/6700 [2:42:23<23:51,  1.62s/it]11/17/2022 01:36:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:32 - INFO - train.train_snli_ve - loss is tensor(0.8801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5815/6700 [2:42:25<23:55,  1.62s/it]11/17/2022 01:36:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.7875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:34 - INFO - train.train_snli_ve - loss is tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5816/6700 [2:42:26<23:48,  1.62s/it]11/17/2022 01:36:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.8482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:35 - INFO - train.train_snli_ve - loss is tensor(0.4960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5817/6700 [2:42:28<23:39,  1.61s/it]11/17/2022 01:36:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.7361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:37 - INFO - train.train_snli_ve - loss is tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5818/6700 [2:42:29<23:33,  1.60s/it]11/17/2022 01:36:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.7195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:38 - INFO - train.train_snli_ve - loss is tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5819/6700 [2:42:31<23:37,  1.61s/it]11/17/2022 01:36:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.7962e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:40 - INFO - train.train_snli_ve - loss is tensor(0.7012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5820/6700 [2:42:33<23:46,  1.62s/it]11/17/2022 01:36:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8326e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:42 - INFO - train.train_snli_ve - loss is tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5821/6700 [2:42:34<23:49,  1.63s/it]11/17/2022 01:36:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:43 - INFO - train.train_snli_ve - loss is tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5822/6700 [2:42:36<23:47,  1.63s/it]11/17/2022 01:36:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.6506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:45 - INFO - train.train_snli_ve - loss is tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5823/6700 [2:42:38<23:47,  1.63s/it]11/17/2022 01:36:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:47 - INFO - train.train_snli_ve - loss is tensor(0.9281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5824/6700 [2:42:39<23:44,  1.63s/it]11/17/2022 01:36:48 - INFO - train.train_snli_ve - kd_loss is tensor(4.0320e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:48 - INFO - train.train_snli_ve - loss is tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5825/6700 [2:42:41<23:43,  1.63s/it]11/17/2022 01:36:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9745e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:50 - INFO - train.train_snli_ve - loss is tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5826/6700 [2:42:43<23:52,  1.64s/it]11/17/2022 01:36:52 - INFO - train.train_snli_ve - kd_loss is tensor(4.9570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:52 - INFO - train.train_snli_ve - loss is tensor(0.8464, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5827/6700 [2:42:44<23:46,  1.63s/it]11/17/2022 01:36:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.9931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:53 - INFO - train.train_snli_ve - loss is tensor(0.6055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########6 | 5828/6700 [2:42:46<23:41,  1.63s/it]11/17/2022 01:36:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.4402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:55 - INFO - train.train_snli_ve - loss is tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5829/6700 [2:42:47<23:40,  1.63s/it]11/17/2022 01:36:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.2839e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:56 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5830/6700 [2:42:49<23:26,  1.62s/it]11/17/2022 01:36:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.6724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:36:58 - INFO - train.train_snli_ve - loss is tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5831/6700 [2:42:51<23:28,  1.62s/it]11/17/2022 01:37:00 - INFO - train.train_snli_ve - kd_loss is tensor(1.2170e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:00 - INFO - train.train_snli_ve - loss is tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5832/6700 [2:42:52<23:33,  1.63s/it]11/17/2022 01:37:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.8664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:01 - INFO - train.train_snli_ve - loss is tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5833/6700 [2:42:54<23:38,  1.64s/it]11/17/2022 01:37:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.9176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:03 - INFO - train.train_snli_ve - loss is tensor(0.5034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5834/6700 [2:42:56<23:23,  1.62s/it]11/17/2022 01:37:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:04 - INFO - train.train_snli_ve - loss is tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5835/6700 [2:42:57<23:14,  1.61s/it]11/17/2022 01:37:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.1946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:06 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5836/6700 [2:42:59<23:13,  1.61s/it]11/17/2022 01:37:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:08 - INFO - train.train_snli_ve - loss is tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5837/6700 [2:43:00<23:09,  1.61s/it]11/17/2022 01:37:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.6618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:09 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5838/6700 [2:43:02<23:12,  1.61s/it]11/17/2022 01:37:11 - INFO - train.train_snli_ve - kd_loss is tensor(9.6429e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:11 - INFO - train.train_snli_ve - loss is tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5839/6700 [2:43:04<23:16,  1.62s/it]11/17/2022 01:37:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.4477e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:13 - INFO - train.train_snli_ve - loss is tensor(0.4657, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5840/6700 [2:43:05<23:07,  1.61s/it]11/17/2022 01:37:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:14 - INFO - train.train_snli_ve - loss is tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5841/6700 [2:43:07<22:57,  1.60s/it]11/17/2022 01:37:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:16 - INFO - train.train_snli_ve - loss is tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5842/6700 [2:43:08<22:56,  1.60s/it]11/17/2022 01:37:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.9085e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:17 - INFO - train.train_snli_ve - loss is tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5843/6700 [2:43:10<22:57,  1.61s/it]11/17/2022 01:37:19 - INFO - train.train_snli_ve - kd_loss is tensor(9.4524e-06, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:19 - INFO - train.train_snli_ve - loss is tensor(0.8866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5844/6700 [2:43:12<23:00,  1.61s/it]11/17/2022 01:37:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.2673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:21 - INFO - train.train_snli_ve - loss is tensor(0.7255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5845/6700 [2:43:13<22:58,  1.61s/it]11/17/2022 01:37:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.0652e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:22 - INFO - train.train_snli_ve - loss is tensor(0.8556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5846/6700 [2:43:15<22:53,  1.61s/it]11/17/2022 01:37:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.6309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:24 - INFO - train.train_snli_ve - loss is tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5847/6700 [2:43:16<22:58,  1.62s/it]11/17/2022 01:37:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:25 - INFO - train.train_snli_ve - loss is tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5848/6700 [2:43:18<23:10,  1.63s/it]11/17/2022 01:37:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.6774e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:27 - INFO - train.train_snli_ve - loss is tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5849/6700 [2:43:20<22:59,  1.62s/it]11/17/2022 01:37:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.3730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:29 - INFO - train.train_snli_ve - loss is tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5850/6700 [2:43:21<22:52,  1.61s/it]11/17/2022 01:37:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.0265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:30 - INFO - train.train_snli_ve - loss is tensor(0.8646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5851/6700 [2:43:23<22:57,  1.62s/it]11/17/2022 01:37:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.3752e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:32 - INFO - train.train_snli_ve - loss is tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5852/6700 [2:43:25<22:53,  1.62s/it]11/17/2022 01:37:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.9744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:34 - INFO - train.train_snli_ve - loss is tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5853/6700 [2:43:26<22:55,  1.62s/it]11/17/2022 01:37:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.5537e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:35 - INFO - train.train_snli_ve - loss is tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5854/6700 [2:43:28<23:01,  1.63s/it]11/17/2022 01:37:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.2102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:37 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5855/6700 [2:43:30<23:00,  1.63s/it]11/17/2022 01:37:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.8124e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:38 - INFO - train.train_snli_ve - loss is tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5856/6700 [2:43:31<22:58,  1.63s/it]11/17/2022 01:37:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0174e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:40 - INFO - train.train_snli_ve - loss is tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5857/6700 [2:43:33<22:43,  1.62s/it]11/17/2022 01:37:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.4887e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:42 - INFO - train.train_snli_ve - loss is tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5858/6700 [2:43:34<22:36,  1.61s/it]11/17/2022 01:37:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.6169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:43 - INFO - train.train_snli_ve - loss is tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5859/6700 [2:43:36<22:40,  1.62s/it]11/17/2022 01:37:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.2810e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:45 - INFO - train.train_snli_ve - loss is tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5860/6700 [2:43:38<22:45,  1.63s/it]11/17/2022 01:37:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.9677e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:47 - INFO - train.train_snli_ve - loss is tensor(0.4744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5861/6700 [2:43:39<22:42,  1.62s/it]11/17/2022 01:37:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8426e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:48 - INFO - train.train_snli_ve - loss is tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  87%|########7 | 5862/6700 [2:43:41<22:38,  1.62s/it]11/17/2022 01:37:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.6031e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:50 - INFO - train.train_snli_ve - loss is tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5863/6700 [2:43:42<22:29,  1.61s/it]11/17/2022 01:37:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4755e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:51 - INFO - train.train_snli_ve - loss is tensor(0.8338, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5864/6700 [2:43:44<22:25,  1.61s/it]11/17/2022 01:37:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.9804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:53 - INFO - train.train_snli_ve - loss is tensor(0.5472, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5865/6700 [2:43:46<22:21,  1.61s/it]11/17/2022 01:37:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.8648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:55 - INFO - train.train_snli_ve - loss is tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5866/6700 [2:43:47<22:13,  1.60s/it]11/17/2022 01:37:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6218e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:56 - INFO - train.train_snli_ve - loss is tensor(0.9523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5867/6700 [2:43:49<22:21,  1.61s/it]11/17/2022 01:37:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.5756e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:58 - INFO - train.train_snli_ve - loss is tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5868/6700 [2:43:50<22:21,  1.61s/it]11/17/2022 01:37:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.0729e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:37:59 - INFO - train.train_snli_ve - loss is tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5869/6700 [2:43:52<22:09,  1.60s/it]11/17/2022 01:38:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:01 - INFO - train.train_snli_ve - loss is tensor(0.4540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5870/6700 [2:43:54<22:08,  1.60s/it]11/17/2022 01:38:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.1943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:03 - INFO - train.train_snli_ve - loss is tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5871/6700 [2:43:55<22:18,  1.61s/it]11/17/2022 01:38:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6686e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:04 - INFO - train.train_snli_ve - loss is tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5872/6700 [2:43:57<22:35,  1.64s/it]11/17/2022 01:38:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.0931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:06 - INFO - train.train_snli_ve - loss is tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5873/6700 [2:43:59<22:19,  1.62s/it]11/17/2022 01:38:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.9641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:08 - INFO - train.train_snli_ve - loss is tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5874/6700 [2:44:00<22:15,  1.62s/it]11/17/2022 01:38:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.1910e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:09 - INFO - train.train_snli_ve - loss is tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5875/6700 [2:44:02<22:11,  1.61s/it]11/17/2022 01:38:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:11 - INFO - train.train_snli_ve - loss is tensor(0.7966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5876/6700 [2:44:03<22:16,  1.62s/it]11/17/2022 01:38:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:12 - INFO - train.train_snli_ve - loss is tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5877/6700 [2:44:05<22:11,  1.62s/it]11/17/2022 01:38:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.7321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:14 - INFO - train.train_snli_ve - loss is tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5878/6700 [2:44:07<22:18,  1.63s/it]11/17/2022 01:38:16 - INFO - train.train_snli_ve - kd_loss is tensor(1.5556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:16 - INFO - train.train_snli_ve - loss is tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5879/6700 [2:44:08<22:17,  1.63s/it]11/17/2022 01:38:17 - INFO - train.train_snli_ve - kd_loss is tensor(3.3360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:17 - INFO - train.train_snli_ve - loss is tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5880/6700 [2:44:10<22:20,  1.64s/it]11/17/2022 01:38:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.7445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:19 - INFO - train.train_snli_ve - loss is tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5881/6700 [2:44:12<22:11,  1.63s/it]11/17/2022 01:38:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:21 - INFO - train.train_snli_ve - loss is tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5882/6700 [2:44:13<22:08,  1.62s/it]11/17/2022 01:38:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4549e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:22 - INFO - train.train_snli_ve - loss is tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5883/6700 [2:44:15<22:04,  1.62s/it]11/17/2022 01:38:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.0926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:24 - INFO - train.train_snli_ve - loss is tensor(0.4902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5884/6700 [2:44:16<21:56,  1.61s/it]11/17/2022 01:38:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.7788e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:25 - INFO - train.train_snli_ve - loss is tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5885/6700 [2:44:18<21:49,  1.61s/it]11/17/2022 01:38:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8601e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:27 - INFO - train.train_snli_ve - loss is tensor(0.9178, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5886/6700 [2:44:20<21:53,  1.61s/it]11/17/2022 01:38:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.3040e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:29 - INFO - train.train_snli_ve - loss is tensor(0.5373, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5887/6700 [2:44:21<21:45,  1.61s/it]11/17/2022 01:38:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.6850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:30 - INFO - train.train_snli_ve - loss is tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5888/6700 [2:44:23<21:44,  1.61s/it]11/17/2022 01:38:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.2199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:32 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5889/6700 [2:44:24<21:57,  1.62s/it]11/17/2022 01:38:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4817e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:33 - INFO - train.train_snli_ve - loss is tensor(0.5638, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5890/6700 [2:44:26<21:47,  1.61s/it]11/17/2022 01:38:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:35 - INFO - train.train_snli_ve - loss is tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5891/6700 [2:44:28<21:47,  1.62s/it]11/17/2022 01:38:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.4010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:37 - INFO - train.train_snli_ve - loss is tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5892/6700 [2:44:29<21:49,  1.62s/it]11/17/2022 01:38:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9177e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:38 - INFO - train.train_snli_ve - loss is tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5893/6700 [2:44:31<21:39,  1.61s/it]11/17/2022 01:38:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.8060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:40 - INFO - train.train_snli_ve - loss is tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5894/6700 [2:44:33<21:34,  1.61s/it]11/17/2022 01:38:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.1301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:41 - INFO - train.train_snli_ve - loss is tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########7 | 5895/6700 [2:44:34<21:35,  1.61s/it]11/17/2022 01:38:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.2717e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:43 - INFO - train.train_snli_ve - loss is tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5896/6700 [2:44:36<21:27,  1.60s/it]11/17/2022 01:38:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:45 - INFO - train.train_snli_ve - loss is tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5897/6700 [2:44:37<21:32,  1.61s/it]11/17/2022 01:38:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.6484e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:46 - INFO - train.train_snli_ve - loss is tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5898/6700 [2:44:39<21:35,  1.62s/it]11/17/2022 01:38:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6981e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:48 - INFO - train.train_snli_ve - loss is tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5899/6700 [2:44:41<21:25,  1.61s/it]11/17/2022 01:38:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:50 - INFO - train.train_snli_ve - loss is tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5900/6700 [2:44:42<21:40,  1.63s/it]11/17/2022 01:38:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.7975e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:51 - INFO - train.train_snli_ve - loss is tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5901/6700 [2:44:44<21:33,  1.62s/it]11/17/2022 01:38:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.1051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:53 - INFO - train.train_snli_ve - loss is tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5902/6700 [2:44:45<21:32,  1.62s/it]11/17/2022 01:38:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:54 - INFO - train.train_snli_ve - loss is tensor(0.5273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5903/6700 [2:44:47<21:22,  1.61s/it]11/17/2022 01:38:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.7307e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:56 - INFO - train.train_snli_ve - loss is tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5904/6700 [2:44:49<21:14,  1.60s/it]11/17/2022 01:38:58 - INFO - train.train_snli_ve - kd_loss is tensor(3.4194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:58 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5905/6700 [2:44:50<21:06,  1.59s/it]11/17/2022 01:38:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.8404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:38:59 - INFO - train.train_snli_ve - loss is tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5906/6700 [2:44:52<21:09,  1.60s/it]11/17/2022 01:39:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.4380e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:01 - INFO - train.train_snli_ve - loss is tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5907/6700 [2:44:53<21:07,  1.60s/it]11/17/2022 01:39:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.5284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:02 - INFO - train.train_snli_ve - loss is tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5908/6700 [2:44:55<21:04,  1.60s/it]11/17/2022 01:39:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.4098e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:04 - INFO - train.train_snli_ve - loss is tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5909/6700 [2:44:57<21:13,  1.61s/it]11/17/2022 01:39:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.2117e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:06 - INFO - train.train_snli_ve - loss is tensor(0.7855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5910/6700 [2:44:58<21:13,  1.61s/it]11/17/2022 01:39:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:07 - INFO - train.train_snli_ve - loss is tensor(0.8415, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5911/6700 [2:45:00<21:05,  1.60s/it]11/17/2022 01:39:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0414e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:09 - INFO - train.train_snli_ve - loss is tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5912/6700 [2:45:01<21:12,  1.62s/it]11/17/2022 01:39:10 - INFO - train.train_snli_ve - kd_loss is tensor(5.1595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:10 - INFO - train.train_snli_ve - loss is tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5913/6700 [2:45:03<21:19,  1.63s/it]11/17/2022 01:39:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.7348e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:12 - INFO - train.train_snli_ve - loss is tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5914/6700 [2:45:05<21:20,  1.63s/it]11/17/2022 01:39:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.5976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:14 - INFO - train.train_snli_ve - loss is tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5915/6700 [2:45:06<21:14,  1.62s/it]11/17/2022 01:39:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.3672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:15 - INFO - train.train_snli_ve - loss is tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5916/6700 [2:45:08<21:16,  1.63s/it]11/17/2022 01:39:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3635e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:17 - INFO - train.train_snli_ve - loss is tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5917/6700 [2:45:10<21:14,  1.63s/it]11/17/2022 01:39:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:19 - INFO - train.train_snli_ve - loss is tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5918/6700 [2:45:11<21:11,  1.63s/it]11/17/2022 01:39:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.0349e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:20 - INFO - train.train_snli_ve - loss is tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5919/6700 [2:45:13<21:16,  1.63s/it]11/17/2022 01:39:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:22 - INFO - train.train_snli_ve - loss is tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5920/6700 [2:45:15<21:03,  1.62s/it]11/17/2022 01:39:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7359e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:23 - INFO - train.train_snli_ve - loss is tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5921/6700 [2:45:16<21:04,  1.62s/it]11/17/2022 01:39:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.8678e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:25 - INFO - train.train_snli_ve - loss is tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5922/6700 [2:45:18<20:58,  1.62s/it]11/17/2022 01:39:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.9970e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:27 - INFO - train.train_snli_ve - loss is tensor(0.8581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5923/6700 [2:45:19<20:49,  1.61s/it]11/17/2022 01:39:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:28 - INFO - train.train_snli_ve - loss is tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5924/6700 [2:45:21<20:45,  1.60s/it]11/17/2022 01:39:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7386e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:30 - INFO - train.train_snli_ve - loss is tensor(0.8969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5925/6700 [2:45:23<20:41,  1.60s/it]11/17/2022 01:39:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5495e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:31 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5926/6700 [2:45:24<20:39,  1.60s/it]11/17/2022 01:39:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9960e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:33 - INFO - train.train_snli_ve - loss is tensor(0.5635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5927/6700 [2:45:26<20:42,  1.61s/it]11/17/2022 01:39:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.1749e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:35 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5928/6700 [2:45:27<20:41,  1.61s/it]11/17/2022 01:39:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:36 - INFO - train.train_snli_ve - loss is tensor(0.7491, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  88%|########8 | 5929/6700 [2:45:29<20:42,  1.61s/it]11/17/2022 01:39:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.5433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:38 - INFO - train.train_snli_ve - loss is tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5930/6700 [2:45:31<20:39,  1.61s/it]11/17/2022 01:39:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.8015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:40 - INFO - train.train_snli_ve - loss is tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5931/6700 [2:45:32<20:41,  1.61s/it]11/17/2022 01:39:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5818e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:41 - INFO - train.train_snli_ve - loss is tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5932/6700 [2:45:34<20:45,  1.62s/it]11/17/2022 01:39:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.9488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:43 - INFO - train.train_snli_ve - loss is tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5933/6700 [2:45:35<20:50,  1.63s/it]11/17/2022 01:39:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.4830e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:44 - INFO - train.train_snli_ve - loss is tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5934/6700 [2:45:37<20:52,  1.64s/it]11/17/2022 01:39:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.8573e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:46 - INFO - train.train_snli_ve - loss is tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5935/6700 [2:45:39<20:43,  1.63s/it]11/17/2022 01:39:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.7639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:48 - INFO - train.train_snli_ve - loss is tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5936/6700 [2:45:40<20:44,  1.63s/it]11/17/2022 01:39:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:49 - INFO - train.train_snli_ve - loss is tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5937/6700 [2:45:42<20:43,  1.63s/it]11/17/2022 01:39:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.2329e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:51 - INFO - train.train_snli_ve - loss is tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5938/6700 [2:45:44<20:32,  1.62s/it]11/17/2022 01:39:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.8939e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:53 - INFO - train.train_snli_ve - loss is tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5939/6700 [2:45:45<20:29,  1.62s/it]11/17/2022 01:39:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.5391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:54 - INFO - train.train_snli_ve - loss is tensor(0.8288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5940/6700 [2:45:47<20:26,  1.61s/it]11/17/2022 01:39:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.5082e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:56 - INFO - train.train_snli_ve - loss is tensor(0.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5941/6700 [2:45:48<20:30,  1.62s/it]11/17/2022 01:39:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.8191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:57 - INFO - train.train_snli_ve - loss is tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5942/6700 [2:45:50<20:31,  1.63s/it]11/17/2022 01:39:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1500e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:39:59 - INFO - train.train_snli_ve - loss is tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5943/6700 [2:45:52<20:29,  1.62s/it]11/17/2022 01:40:01 - INFO - train.train_snli_ve - kd_loss is tensor(3.1943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:01 - INFO - train.train_snli_ve - loss is tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5944/6700 [2:45:53<20:20,  1.61s/it]11/17/2022 01:40:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.2931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:02 - INFO - train.train_snli_ve - loss is tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5945/6700 [2:45:55<20:29,  1.63s/it]11/17/2022 01:40:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.2824e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:04 - INFO - train.train_snli_ve - loss is tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5946/6700 [2:45:57<20:22,  1.62s/it]11/17/2022 01:40:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.6213e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:05 - INFO - train.train_snli_ve - loss is tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5947/6700 [2:45:58<20:13,  1.61s/it]11/17/2022 01:40:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:07 - INFO - train.train_snli_ve - loss is tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5948/6700 [2:46:00<20:19,  1.62s/it]11/17/2022 01:40:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.2156e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:09 - INFO - train.train_snli_ve - loss is tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5949/6700 [2:46:01<20:19,  1.62s/it]11/17/2022 01:40:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.8648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:10 - INFO - train.train_snli_ve - loss is tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5950/6700 [2:46:03<20:15,  1.62s/it]11/17/2022 01:40:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.2039e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:12 - INFO - train.train_snli_ve - loss is tensor(0.7911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5951/6700 [2:46:05<20:08,  1.61s/it]11/17/2022 01:40:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.1714e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:14 - INFO - train.train_snli_ve - loss is tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5952/6700 [2:46:06<20:18,  1.63s/it]11/17/2022 01:40:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.3634e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:15 - INFO - train.train_snli_ve - loss is tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5953/6700 [2:46:08<20:18,  1.63s/it]11/17/2022 01:40:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.7294e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:17 - INFO - train.train_snli_ve - loss is tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5954/6700 [2:46:10<20:15,  1.63s/it]11/17/2022 01:40:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.3019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:19 - INFO - train.train_snli_ve - loss is tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5955/6700 [2:46:11<20:12,  1.63s/it]11/17/2022 01:40:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:20 - INFO - train.train_snli_ve - loss is tensor(0.5117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5956/6700 [2:46:13<20:02,  1.62s/it]11/17/2022 01:40:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:22 - INFO - train.train_snli_ve - loss is tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5957/6700 [2:46:14<20:04,  1.62s/it]11/17/2022 01:40:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.2621e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:23 - INFO - train.train_snli_ve - loss is tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5958/6700 [2:46:16<19:52,  1.61s/it]11/17/2022 01:40:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.9544e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:25 - INFO - train.train_snli_ve - loss is tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5959/6700 [2:46:18<19:44,  1.60s/it]11/17/2022 01:40:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.2001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:26 - INFO - train.train_snli_ve - loss is tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5960/6700 [2:46:19<19:40,  1.59s/it]11/17/2022 01:40:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.7732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:28 - INFO - train.train_snli_ve - loss is tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5961/6700 [2:46:21<19:46,  1.61s/it]11/17/2022 01:40:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9311e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:30 - INFO - train.train_snli_ve - loss is tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########8 | 5962/6700 [2:46:22<19:47,  1.61s/it]11/17/2022 01:40:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.9381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:31 - INFO - train.train_snli_ve - loss is tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5963/6700 [2:46:24<19:57,  1.62s/it]11/17/2022 01:40:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.6154e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:33 - INFO - train.train_snli_ve - loss is tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5964/6700 [2:46:26<19:45,  1.61s/it]11/17/2022 01:40:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.9340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:35 - INFO - train.train_snli_ve - loss is tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5965/6700 [2:46:27<19:48,  1.62s/it]11/17/2022 01:40:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.0225e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:36 - INFO - train.train_snli_ve - loss is tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5966/6700 [2:46:29<19:52,  1.62s/it]11/17/2022 01:40:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1239e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:38 - INFO - train.train_snli_ve - loss is tensor(0.5716, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5967/6700 [2:46:31<19:51,  1.63s/it]11/17/2022 01:40:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.8207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:39 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5968/6700 [2:46:32<19:41,  1.61s/it]11/17/2022 01:40:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.9203e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:41 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5969/6700 [2:46:34<19:44,  1.62s/it]11/17/2022 01:40:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.7616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:43 - INFO - train.train_snli_ve - loss is tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5970/6700 [2:46:35<19:35,  1.61s/it]11/17/2022 01:40:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.6588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:44 - INFO - train.train_snli_ve - loss is tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5971/6700 [2:46:37<19:30,  1.61s/it]11/17/2022 01:40:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.4881e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:46 - INFO - train.train_snli_ve - loss is tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5972/6700 [2:46:39<19:29,  1.61s/it]11/17/2022 01:40:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.2295e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:47 - INFO - train.train_snli_ve - loss is tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5973/6700 [2:46:40<19:30,  1.61s/it]11/17/2022 01:40:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.4917e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:49 - INFO - train.train_snli_ve - loss is tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5974/6700 [2:46:42<19:23,  1.60s/it]11/17/2022 01:40:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4383e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:51 - INFO - train.train_snli_ve - loss is tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5975/6700 [2:46:43<19:16,  1.59s/it]11/17/2022 01:40:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.0987e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:52 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5976/6700 [2:46:45<19:17,  1.60s/it]11/17/2022 01:40:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.2597e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:54 - INFO - train.train_snli_ve - loss is tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5977/6700 [2:46:47<19:23,  1.61s/it]11/17/2022 01:40:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.5858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:55 - INFO - train.train_snli_ve - loss is tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5978/6700 [2:46:48<19:20,  1.61s/it]11/17/2022 01:40:57 - INFO - train.train_snli_ve - kd_loss is tensor(4.1104e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:57 - INFO - train.train_snli_ve - loss is tensor(0.7341, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5979/6700 [2:46:50<19:18,  1.61s/it]11/17/2022 01:40:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.4968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:40:59 - INFO - train.train_snli_ve - loss is tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5980/6700 [2:46:51<19:13,  1.60s/it]11/17/2022 01:41:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.3550e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:00 - INFO - train.train_snli_ve - loss is tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5981/6700 [2:46:53<19:09,  1.60s/it]11/17/2022 01:41:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9913e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:02 - INFO - train.train_snli_ve - loss is tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5982/6700 [2:46:55<19:09,  1.60s/it]11/17/2022 01:41:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.0417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:04 - INFO - train.train_snli_ve - loss is tensor(0.5656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5983/6700 [2:46:56<19:11,  1.61s/it]11/17/2022 01:41:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.3807e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:05 - INFO - train.train_snli_ve - loss is tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5984/6700 [2:46:58<19:13,  1.61s/it]11/17/2022 01:41:07 - INFO - train.train_snli_ve - kd_loss is tensor(1.7474e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:07 - INFO - train.train_snli_ve - loss is tensor(0.4372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5985/6700 [2:46:59<19:06,  1.60s/it]11/17/2022 01:41:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.0689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:08 - INFO - train.train_snli_ve - loss is tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5986/6700 [2:47:01<19:00,  1.60s/it]11/17/2022 01:41:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.9054e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:10 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5987/6700 [2:47:03<19:05,  1.61s/it]11/17/2022 01:41:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.6995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:12 - INFO - train.train_snli_ve - loss is tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5988/6700 [2:47:04<19:02,  1.60s/it]11/17/2022 01:41:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.4069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:13 - INFO - train.train_snli_ve - loss is tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5989/6700 [2:47:06<18:57,  1.60s/it]11/17/2022 01:41:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.4666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:15 - INFO - train.train_snli_ve - loss is tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5990/6700 [2:47:07<18:52,  1.60s/it]11/17/2022 01:41:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.7922e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:16 - INFO - train.train_snli_ve - loss is tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5991/6700 [2:47:09<18:49,  1.59s/it]11/17/2022 01:41:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.0333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:18 - INFO - train.train_snli_ve - loss is tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5992/6700 [2:47:11<18:55,  1.60s/it]11/17/2022 01:41:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.1442e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:20 - INFO - train.train_snli_ve - loss is tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5993/6700 [2:47:12<18:55,  1.61s/it]11/17/2022 01:41:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.6470e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:21 - INFO - train.train_snli_ve - loss is tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5994/6700 [2:47:14<18:53,  1.61s/it]11/17/2022 01:41:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.2096e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:23 - INFO - train.train_snli_ve - loss is tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5995/6700 [2:47:15<18:52,  1.61s/it]11/17/2022 01:41:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.7227e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:24 - INFO - train.train_snli_ve - loss is tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  89%|########9 | 5996/6700 [2:47:17<18:54,  1.61s/it]11/17/2022 01:41:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.3398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:26 - INFO - train.train_snli_ve - loss is tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 5997/6700 [2:47:19<18:51,  1.61s/it]11/17/2022 01:41:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.8871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:28 - INFO - train.train_snli_ve - loss is tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 5998/6700 [2:47:20<18:54,  1.62s/it]11/17/2022 01:41:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.9599e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:29 - INFO - train.train_snli_ve - loss is tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 5999/6700 [2:47:22<18:49,  1.61s/it]11/17/2022 01:41:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.5265e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:31 - INFO - train.train_snli_ve - loss is tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6000/6700 [2:47:24<18:57,  1.62s/it]11/17/2022 01:41:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:32 - INFO - train.train_snli_ve - loss is tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6001/6700 [2:47:25<18:48,  1.61s/it]11/17/2022 01:41:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.2073e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:34 - INFO - train.train_snli_ve - loss is tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6002/6700 [2:47:27<18:45,  1.61s/it]11/17/2022 01:41:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.6253e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:36 - INFO - train.train_snli_ve - loss is tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6003/6700 [2:47:28<18:44,  1.61s/it]11/17/2022 01:41:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:37 - INFO - train.train_snli_ve - loss is tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6004/6700 [2:47:30<18:39,  1.61s/it]11/17/2022 01:41:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:39 - INFO - train.train_snli_ve - loss is tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6005/6700 [2:47:32<18:34,  1.60s/it]11/17/2022 01:41:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1565e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:40 - INFO - train.train_snli_ve - loss is tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6006/6700 [2:47:33<18:28,  1.60s/it]11/17/2022 01:41:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1885e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:42 - INFO - train.train_snli_ve - loss is tensor(0.8138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6007/6700 [2:47:35<18:23,  1.59s/it]11/17/2022 01:41:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8515e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:44 - INFO - train.train_snli_ve - loss is tensor(0.7450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6008/6700 [2:47:36<18:29,  1.60s/it]11/17/2022 01:41:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:45 - INFO - train.train_snli_ve - loss is tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6009/6700 [2:47:38<18:25,  1.60s/it]11/17/2022 01:41:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:47 - INFO - train.train_snli_ve - loss is tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6010/6700 [2:47:40<18:23,  1.60s/it]11/17/2022 01:41:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1160e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:48 - INFO - train.train_snli_ve - loss is tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6011/6700 [2:47:41<18:26,  1.61s/it]11/17/2022 01:41:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.8315e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:50 - INFO - train.train_snli_ve - loss is tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6012/6700 [2:47:43<18:20,  1.60s/it]11/17/2022 01:41:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:52 - INFO - train.train_snli_ve - loss is tensor(0.8265, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6013/6700 [2:47:44<18:18,  1.60s/it]11/17/2022 01:41:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0656e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:53 - INFO - train.train_snli_ve - loss is tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6014/6700 [2:47:46<18:18,  1.60s/it]11/17/2022 01:41:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.3885e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:55 - INFO - train.train_snli_ve - loss is tensor(0.5138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6015/6700 [2:47:48<18:14,  1.60s/it]11/17/2022 01:41:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.1245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:56 - INFO - train.train_snli_ve - loss is tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6016/6700 [2:47:49<18:12,  1.60s/it]11/17/2022 01:41:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.6703e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:41:58 - INFO - train.train_snli_ve - loss is tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6017/6700 [2:47:51<18:15,  1.60s/it]11/17/2022 01:42:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.9162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:00 - INFO - train.train_snli_ve - loss is tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6018/6700 [2:47:52<18:21,  1.61s/it]11/17/2022 01:42:01 - INFO - train.train_snli_ve - kd_loss is tensor(4.3304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:01 - INFO - train.train_snli_ve - loss is tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6019/6700 [2:47:54<18:22,  1.62s/it]11/17/2022 01:42:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.6019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:03 - INFO - train.train_snli_ve - loss is tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6020/6700 [2:47:56<18:15,  1.61s/it]11/17/2022 01:42:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.9381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:05 - INFO - train.train_snli_ve - loss is tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6021/6700 [2:47:57<18:17,  1.62s/it]11/17/2022 01:42:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.0661e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:06 - INFO - train.train_snli_ve - loss is tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6022/6700 [2:47:59<18:15,  1.62s/it]11/17/2022 01:42:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.3411e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:08 - INFO - train.train_snli_ve - loss is tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6023/6700 [2:48:00<18:17,  1.62s/it]11/17/2022 01:42:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.4618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:09 - INFO - train.train_snli_ve - loss is tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6024/6700 [2:48:02<18:13,  1.62s/it]11/17/2022 01:42:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.3537e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:11 - INFO - train.train_snli_ve - loss is tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6025/6700 [2:48:04<18:15,  1.62s/it]11/17/2022 01:42:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.8297e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:13 - INFO - train.train_snli_ve - loss is tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6026/6700 [2:48:05<18:11,  1.62s/it]11/17/2022 01:42:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.3080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:14 - INFO - train.train_snli_ve - loss is tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6027/6700 [2:48:07<18:12,  1.62s/it]11/17/2022 01:42:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.6748e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:16 - INFO - train.train_snli_ve - loss is tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6028/6700 [2:48:09<18:14,  1.63s/it]11/17/2022 01:42:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8592e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:18 - INFO - train.train_snli_ve - loss is tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|########9 | 6029/6700 [2:48:10<18:12,  1.63s/it]11/17/2022 01:42:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.1740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:19 - INFO - train.train_snli_ve - loss is tensor(0.5874, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6030/6700 [2:48:12<18:24,  1.65s/it]11/17/2022 01:42:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.7859e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:21 - INFO - train.train_snli_ve - loss is tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6031/6700 [2:48:14<18:27,  1.66s/it]11/17/2022 01:42:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:23 - INFO - train.train_snli_ve - loss is tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6032/6700 [2:48:15<18:12,  1.64s/it]11/17/2022 01:42:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.8114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:24 - INFO - train.train_snli_ve - loss is tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6033/6700 [2:48:17<18:09,  1.63s/it]11/17/2022 01:42:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:26 - INFO - train.train_snli_ve - loss is tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6034/6700 [2:48:18<18:06,  1.63s/it]11/17/2022 01:42:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.0400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:27 - INFO - train.train_snli_ve - loss is tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6035/6700 [2:48:20<18:02,  1.63s/it]11/17/2022 01:42:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.7074e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:29 - INFO - train.train_snli_ve - loss is tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6036/6700 [2:48:22<18:00,  1.63s/it]11/17/2022 01:42:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.5966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:31 - INFO - train.train_snli_ve - loss is tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6037/6700 [2:48:23<17:55,  1.62s/it]11/17/2022 01:42:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.0946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:32 - INFO - train.train_snli_ve - loss is tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6038/6700 [2:48:25<17:55,  1.62s/it]11/17/2022 01:42:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:34 - INFO - train.train_snli_ve - loss is tensor(0.5278, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6039/6700 [2:48:27<17:52,  1.62s/it]11/17/2022 01:42:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.7561e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:35 - INFO - train.train_snli_ve - loss is tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6040/6700 [2:48:28<17:45,  1.61s/it]11/17/2022 01:42:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.3882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:37 - INFO - train.train_snli_ve - loss is tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6041/6700 [2:48:30<17:38,  1.61s/it]11/17/2022 01:42:39 - INFO - train.train_snli_ve - kd_loss is tensor(4.1494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:39 - INFO - train.train_snli_ve - loss is tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6042/6700 [2:48:31<17:43,  1.62s/it]11/17/2022 01:42:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.7682e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:40 - INFO - train.train_snli_ve - loss is tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6043/6700 [2:48:33<17:47,  1.63s/it]11/17/2022 01:42:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.9858e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:42 - INFO - train.train_snli_ve - loss is tensor(0.8251, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6044/6700 [2:48:35<17:53,  1.64s/it]11/17/2022 01:42:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:44 - INFO - train.train_snli_ve - loss is tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6045/6700 [2:48:36<17:46,  1.63s/it]11/17/2022 01:42:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.7071e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:45 - INFO - train.train_snli_ve - loss is tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6046/6700 [2:48:38<17:44,  1.63s/it]11/17/2022 01:42:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.4257e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:47 - INFO - train.train_snli_ve - loss is tensor(0.8169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6047/6700 [2:48:40<17:39,  1.62s/it]11/17/2022 01:42:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.3178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:48 - INFO - train.train_snli_ve - loss is tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6048/6700 [2:48:41<17:26,  1.61s/it]11/17/2022 01:42:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.1648e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:50 - INFO - train.train_snli_ve - loss is tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6049/6700 [2:48:43<17:30,  1.61s/it]11/17/2022 01:42:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5151e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:52 - INFO - train.train_snli_ve - loss is tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6050/6700 [2:48:44<17:24,  1.61s/it]11/17/2022 01:42:53 - INFO - train.train_snli_ve - kd_loss is tensor(5.6208e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:53 - INFO - train.train_snli_ve - loss is tensor(0.4914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6051/6700 [2:48:46<17:18,  1.60s/it]11/17/2022 01:42:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:55 - INFO - train.train_snli_ve - loss is tensor(0.7217, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6052/6700 [2:48:48<17:19,  1.60s/it]11/17/2022 01:42:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.2430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:56 - INFO - train.train_snli_ve - loss is tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6053/6700 [2:48:49<17:24,  1.61s/it]11/17/2022 01:42:58 - INFO - train.train_snli_ve - kd_loss is tensor(4.6649e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:42:58 - INFO - train.train_snli_ve - loss is tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6054/6700 [2:48:51<17:22,  1.61s/it]11/17/2022 01:43:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:00 - INFO - train.train_snli_ve - loss is tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6055/6700 [2:48:52<17:24,  1.62s/it]11/17/2022 01:43:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.9041e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:01 - INFO - train.train_snli_ve - loss is tensor(0.8572, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6056/6700 [2:48:54<17:19,  1.61s/it]11/17/2022 01:43:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.4212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:03 - INFO - train.train_snli_ve - loss is tensor(0.8623, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6057/6700 [2:48:56<17:14,  1.61s/it]11/17/2022 01:43:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.2655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:05 - INFO - train.train_snli_ve - loss is tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6058/6700 [2:48:57<17:15,  1.61s/it]11/17/2022 01:43:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.0396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:06 - INFO - train.train_snli_ve - loss is tensor(0.8547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6059/6700 [2:48:59<17:15,  1.62s/it]11/17/2022 01:43:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.4896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:08 - INFO - train.train_snli_ve - loss is tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6060/6700 [2:49:00<17:15,  1.62s/it]11/17/2022 01:43:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.8934e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:09 - INFO - train.train_snli_ve - loss is tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6061/6700 [2:49:02<17:14,  1.62s/it]11/17/2022 01:43:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.2088e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:11 - INFO - train.train_snli_ve - loss is tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6062/6700 [2:49:04<17:08,  1.61s/it]11/17/2022 01:43:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.7520e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:13 - INFO - train.train_snli_ve - loss is tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  90%|######### | 6063/6700 [2:49:05<17:01,  1.60s/it]11/17/2022 01:43:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.2702e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:14 - INFO - train.train_snli_ve - loss is tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6064/6700 [2:49:07<17:09,  1.62s/it]11/17/2022 01:43:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.1305e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:16 - INFO - train.train_snli_ve - loss is tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6065/6700 [2:49:09<17:01,  1.61s/it]11/17/2022 01:43:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.6731e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:17 - INFO - train.train_snli_ve - loss is tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6066/6700 [2:49:10<16:59,  1.61s/it]11/17/2022 01:43:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.7890e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:19 - INFO - train.train_snli_ve - loss is tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6067/6700 [2:49:12<16:57,  1.61s/it]11/17/2022 01:43:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.1142e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:21 - INFO - train.train_snli_ve - loss is tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6068/6700 [2:49:13<16:56,  1.61s/it]11/17/2022 01:43:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.7761e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:22 - INFO - train.train_snli_ve - loss is tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6069/6700 [2:49:15<16:49,  1.60s/it]11/17/2022 01:43:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.4840e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:24 - INFO - train.train_snli_ve - loss is tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6070/6700 [2:49:17<16:54,  1.61s/it]11/17/2022 01:43:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.6462e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:25 - INFO - train.train_snli_ve - loss is tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6071/6700 [2:49:18<16:47,  1.60s/it]11/17/2022 01:43:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.9189e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:27 - INFO - train.train_snli_ve - loss is tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6072/6700 [2:49:20<16:53,  1.61s/it]11/17/2022 01:43:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.0471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:29 - INFO - train.train_snli_ve - loss is tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6073/6700 [2:49:21<16:45,  1.60s/it]11/17/2022 01:43:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9427e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:30 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6074/6700 [2:49:23<16:49,  1.61s/it]11/17/2022 01:43:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.5992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:32 - INFO - train.train_snli_ve - loss is tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6075/6700 [2:49:25<16:41,  1.60s/it]11/17/2022 01:43:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.5143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:33 - INFO - train.train_snli_ve - loss is tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6076/6700 [2:49:26<16:39,  1.60s/it]11/17/2022 01:43:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.2384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:35 - INFO - train.train_snli_ve - loss is tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6077/6700 [2:49:28<16:39,  1.60s/it]11/17/2022 01:43:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.5378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:37 - INFO - train.train_snli_ve - loss is tensor(0.7450, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6078/6700 [2:49:29<16:41,  1.61s/it]11/17/2022 01:43:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.9528e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:38 - INFO - train.train_snli_ve - loss is tensor(0.8626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6079/6700 [2:49:31<16:35,  1.60s/it]11/17/2022 01:43:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.1725e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:40 - INFO - train.train_snli_ve - loss is tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6080/6700 [2:49:33<16:35,  1.61s/it]11/17/2022 01:43:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1764e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:42 - INFO - train.train_snli_ve - loss is tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6081/6700 [2:49:34<16:33,  1.60s/it]11/17/2022 01:43:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.0863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:43 - INFO - train.train_snli_ve - loss is tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6082/6700 [2:49:36<16:30,  1.60s/it]11/17/2022 01:43:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.4288e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:45 - INFO - train.train_snli_ve - loss is tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6083/6700 [2:49:37<16:39,  1.62s/it]11/17/2022 01:43:46 - INFO - train.train_snli_ve - kd_loss is tensor(5.1014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:46 - INFO - train.train_snli_ve - loss is tensor(0.4770, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6084/6700 [2:49:39<16:37,  1.62s/it]11/17/2022 01:43:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1527e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:48 - INFO - train.train_snli_ve - loss is tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6085/6700 [2:49:41<16:37,  1.62s/it]11/17/2022 01:43:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.9976e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:50 - INFO - train.train_snli_ve - loss is tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6086/6700 [2:49:42<16:31,  1.61s/it]11/17/2022 01:43:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.7502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:51 - INFO - train.train_snli_ve - loss is tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6087/6700 [2:49:44<16:26,  1.61s/it]11/17/2022 01:43:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0931e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:53 - INFO - train.train_snli_ve - loss is tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6088/6700 [2:49:46<16:28,  1.62s/it]11/17/2022 01:43:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.7060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:54 - INFO - train.train_snli_ve - loss is tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6089/6700 [2:49:47<16:27,  1.62s/it]11/17/2022 01:43:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6651e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:56 - INFO - train.train_snli_ve - loss is tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6090/6700 [2:49:49<16:29,  1.62s/it]11/17/2022 01:43:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.7718e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:58 - INFO - train.train_snli_ve - loss is tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6091/6700 [2:49:50<16:21,  1.61s/it]11/17/2022 01:43:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4155e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:43:59 - INFO - train.train_snli_ve - loss is tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6092/6700 [2:49:52<16:18,  1.61s/it]11/17/2022 01:44:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.2009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:01 - INFO - train.train_snli_ve - loss is tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6093/6700 [2:49:54<16:13,  1.60s/it]11/17/2022 01:44:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.7521e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:02 - INFO - train.train_snli_ve - loss is tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6094/6700 [2:49:55<16:07,  1.60s/it]11/17/2022 01:44:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.4291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:04 - INFO - train.train_snli_ve - loss is tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6095/6700 [2:49:57<16:12,  1.61s/it]11/17/2022 01:44:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.9542e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:06 - INFO - train.train_snli_ve - loss is tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|######### | 6096/6700 [2:49:58<16:06,  1.60s/it]11/17/2022 01:44:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1795e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:07 - INFO - train.train_snli_ve - loss is tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6097/6700 [2:50:00<16:03,  1.60s/it]11/17/2022 01:44:09 - INFO - train.train_snli_ve - kd_loss is tensor(1.8053e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:09 - INFO - train.train_snli_ve - loss is tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6098/6700 [2:50:02<16:04,  1.60s/it]11/17/2022 01:44:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.6808e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:10 - INFO - train.train_snli_ve - loss is tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6099/6700 [2:50:03<15:56,  1.59s/it]11/17/2022 01:44:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.8247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:12 - INFO - train.train_snli_ve - loss is tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6100/6700 [2:50:05<16:03,  1.61s/it]11/17/2022 01:44:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.3953e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:14 - INFO - train.train_snli_ve - loss is tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6101/6700 [2:50:06<16:05,  1.61s/it]11/17/2022 01:44:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.4492e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:15 - INFO - train.train_snli_ve - loss is tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6102/6700 [2:50:08<16:00,  1.61s/it]11/17/2022 01:44:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:17 - INFO - train.train_snli_ve - loss is tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6103/6700 [2:50:10<16:06,  1.62s/it]11/17/2022 01:44:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.4272e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:19 - INFO - train.train_snli_ve - loss is tensor(0.8006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6104/6700 [2:50:11<16:02,  1.62s/it]11/17/2022 01:44:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.4870e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:20 - INFO - train.train_snli_ve - loss is tensor(0.5333, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6105/6700 [2:50:13<16:02,  1.62s/it]11/17/2022 01:44:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.4978e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:22 - INFO - train.train_snli_ve - loss is tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6106/6700 [2:50:15<16:03,  1.62s/it]11/17/2022 01:44:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.0974e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:23 - INFO - train.train_snli_ve - loss is tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6107/6700 [2:50:16<16:00,  1.62s/it]11/17/2022 01:44:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.0441e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:25 - INFO - train.train_snli_ve - loss is tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6108/6700 [2:50:18<16:05,  1.63s/it]11/17/2022 01:44:27 - INFO - train.train_snli_ve - kd_loss is tensor(3.4596e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:27 - INFO - train.train_snli_ve - loss is tensor(0.4944, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6109/6700 [2:50:19<15:56,  1.62s/it]11/17/2022 01:44:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.2811e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:28 - INFO - train.train_snli_ve - loss is tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6110/6700 [2:50:21<15:51,  1.61s/it]11/17/2022 01:44:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.8569e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:30 - INFO - train.train_snli_ve - loss is tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6111/6700 [2:50:23<15:53,  1.62s/it]11/17/2022 01:44:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.4501e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:32 - INFO - train.train_snli_ve - loss is tensor(0.3262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6112/6700 [2:50:24<15:55,  1.63s/it]11/17/2022 01:44:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.4744e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:33 - INFO - train.train_snli_ve - loss is tensor(0.3463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6113/6700 [2:50:26<15:46,  1.61s/it]11/17/2022 01:44:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:35 - INFO - train.train_snli_ve - loss is tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6114/6700 [2:50:27<15:50,  1.62s/it]11/17/2022 01:44:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.1400e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:36 - INFO - train.train_snli_ve - loss is tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6115/6700 [2:50:29<15:45,  1.62s/it]11/17/2022 01:44:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.0471e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:38 - INFO - train.train_snli_ve - loss is tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6116/6700 [2:50:31<15:44,  1.62s/it]11/17/2022 01:44:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.0343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:40 - INFO - train.train_snli_ve - loss is tensor(0.5191, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6117/6700 [2:50:32<15:38,  1.61s/it]11/17/2022 01:44:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.9637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:41 - INFO - train.train_snli_ve - loss is tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6118/6700 [2:50:34<15:38,  1.61s/it]11/17/2022 01:44:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.0235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:43 - INFO - train.train_snli_ve - loss is tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6119/6700 [2:50:36<15:37,  1.61s/it]11/17/2022 01:44:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.0999e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:44 - INFO - train.train_snli_ve - loss is tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6120/6700 [2:50:37<15:39,  1.62s/it]11/17/2022 01:44:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:46 - INFO - train.train_snli_ve - loss is tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6121/6700 [2:50:39<15:37,  1.62s/it]11/17/2022 01:44:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.1945e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:48 - INFO - train.train_snli_ve - loss is tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6122/6700 [2:50:40<15:37,  1.62s/it]11/17/2022 01:44:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.7056e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:49 - INFO - train.train_snli_ve - loss is tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6123/6700 [2:50:42<15:36,  1.62s/it]11/17/2022 01:44:51 - INFO - train.train_snli_ve - kd_loss is tensor(1.9576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:51 - INFO - train.train_snli_ve - loss is tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6124/6700 [2:50:44<15:34,  1.62s/it]11/17/2022 01:44:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.8434e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:53 - INFO - train.train_snli_ve - loss is tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6125/6700 [2:50:45<15:33,  1.62s/it]11/17/2022 01:44:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.2623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:54 - INFO - train.train_snli_ve - loss is tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6126/6700 [2:50:47<15:23,  1.61s/it]11/17/2022 01:44:56 - INFO - train.train_snli_ve - kd_loss is tensor(5.1176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:56 - INFO - train.train_snli_ve - loss is tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6127/6700 [2:50:48<15:26,  1.62s/it]11/17/2022 01:44:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2579e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:57 - INFO - train.train_snli_ve - loss is tensor(0.9615, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6128/6700 [2:50:50<15:26,  1.62s/it]11/17/2022 01:44:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.3147e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:44:59 - INFO - train.train_snli_ve - loss is tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6129/6700 [2:50:52<15:23,  1.62s/it]11/17/2022 01:45:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.3785e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:01 - INFO - train.train_snli_ve - loss is tensor(1.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  91%|#########1| 6130/6700 [2:50:53<15:15,  1.61s/it]11/17/2022 01:45:02 - INFO - train.train_snli_ve - kd_loss is tensor(4.2149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:02 - INFO - train.train_snli_ve - loss is tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6131/6700 [2:50:55<15:17,  1.61s/it]11/17/2022 01:45:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.7879e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:04 - INFO - train.train_snli_ve - loss is tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6132/6700 [2:50:57<15:12,  1.61s/it]11/17/2022 01:45:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.7469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:05 - INFO - train.train_snli_ve - loss is tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6133/6700 [2:50:58<15:06,  1.60s/it]11/17/2022 01:45:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.7534e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:07 - INFO - train.train_snli_ve - loss is tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6134/6700 [2:51:00<15:06,  1.60s/it]11/17/2022 01:45:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.5228e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:09 - INFO - train.train_snli_ve - loss is tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6135/6700 [2:51:01<15:02,  1.60s/it]11/17/2022 01:45:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.4524e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:10 - INFO - train.train_snli_ve - loss is tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6136/6700 [2:51:03<15:04,  1.60s/it]11/17/2022 01:45:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.0014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:12 - INFO - train.train_snli_ve - loss is tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6137/6700 [2:51:05<15:04,  1.61s/it]11/17/2022 01:45:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.6625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:13 - INFO - train.train_snli_ve - loss is tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6138/6700 [2:51:06<15:05,  1.61s/it]11/17/2022 01:45:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.0518e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:15 - INFO - train.train_snli_ve - loss is tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6139/6700 [2:51:08<15:11,  1.62s/it]11/17/2022 01:45:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.3779e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:17 - INFO - train.train_snli_ve - loss is tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6140/6700 [2:51:09<15:04,  1.62s/it]11/17/2022 01:45:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.8217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:18 - INFO - train.train_snli_ve - loss is tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6141/6700 [2:51:11<15:01,  1.61s/it]11/17/2022 01:45:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6284e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:20 - INFO - train.train_snli_ve - loss is tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6142/6700 [2:51:13<15:04,  1.62s/it]11/17/2022 01:45:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.3595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:22 - INFO - train.train_snli_ve - loss is tensor(0.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6143/6700 [2:51:14<14:58,  1.61s/it]11/17/2022 01:45:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.9787e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:23 - INFO - train.train_snli_ve - loss is tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6144/6700 [2:51:16<14:57,  1.61s/it]11/17/2022 01:45:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5432e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:25 - INFO - train.train_snli_ve - loss is tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6145/6700 [2:51:17<14:53,  1.61s/it]11/17/2022 01:45:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.6589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:26 - INFO - train.train_snli_ve - loss is tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6146/6700 [2:51:19<14:51,  1.61s/it]11/17/2022 01:45:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1854e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:28 - INFO - train.train_snli_ve - loss is tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6147/6700 [2:51:21<14:48,  1.61s/it]11/17/2022 01:45:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.4202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:30 - INFO - train.train_snli_ve - loss is tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6148/6700 [2:51:22<14:49,  1.61s/it]11/17/2022 01:45:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.3191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:31 - INFO - train.train_snli_ve - loss is tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6149/6700 [2:51:24<14:44,  1.61s/it]11/17/2022 01:45:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:33 - INFO - train.train_snli_ve - loss is tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6150/6700 [2:51:25<14:42,  1.60s/it]11/17/2022 01:45:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:34 - INFO - train.train_snli_ve - loss is tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6151/6700 [2:51:27<14:39,  1.60s/it]11/17/2022 01:45:36 - INFO - train.train_snli_ve - kd_loss is tensor(3.9659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:36 - INFO - train.train_snli_ve - loss is tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6152/6700 [2:51:29<14:35,  1.60s/it]11/17/2022 01:45:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.6634e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:38 - INFO - train.train_snli_ve - loss is tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6153/6700 [2:51:30<14:36,  1.60s/it]11/17/2022 01:45:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4417e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:39 - INFO - train.train_snli_ve - loss is tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6154/6700 [2:51:32<14:35,  1.60s/it]11/17/2022 01:45:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9572e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:41 - INFO - train.train_snli_ve - loss is tensor(0.6069, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6155/6700 [2:51:33<14:34,  1.60s/it]11/17/2022 01:45:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.3269e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:42 - INFO - train.train_snli_ve - loss is tensor(0.5276, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6156/6700 [2:51:35<14:38,  1.62s/it]11/17/2022 01:45:44 - INFO - train.train_snli_ve - kd_loss is tensor(4.5330e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:44 - INFO - train.train_snli_ve - loss is tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6157/6700 [2:51:37<14:29,  1.60s/it]11/17/2022 01:45:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.2106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:46 - INFO - train.train_snli_ve - loss is tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6158/6700 [2:51:38<14:32,  1.61s/it]11/17/2022 01:45:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.2608e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:47 - INFO - train.train_snli_ve - loss is tensor(0.7643, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6159/6700 [2:51:40<14:32,  1.61s/it]11/17/2022 01:45:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.7557e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:49 - INFO - train.train_snli_ve - loss is tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6160/6700 [2:51:42<14:34,  1.62s/it]11/17/2022 01:45:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:51 - INFO - train.train_snli_ve - loss is tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6161/6700 [2:51:43<14:42,  1.64s/it]11/17/2022 01:45:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.7916e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:52 - INFO - train.train_snli_ve - loss is tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6162/6700 [2:51:45<14:39,  1.63s/it]11/17/2022 01:45:54 - INFO - train.train_snli_ve - kd_loss is tensor(6.2688e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:54 - INFO - train.train_snli_ve - loss is tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########1| 6163/6700 [2:51:46<14:34,  1.63s/it]11/17/2022 01:45:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.6025e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:55 - INFO - train.train_snli_ve - loss is tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6164/6700 [2:51:48<14:26,  1.62s/it]11/17/2022 01:45:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.1100e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:57 - INFO - train.train_snli_ve - loss is tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6165/6700 [2:51:50<14:16,  1.60s/it]11/17/2022 01:45:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8730e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:45:59 - INFO - train.train_snli_ve - loss is tensor(0.5190, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6166/6700 [2:51:51<14:11,  1.60s/it]11/17/2022 01:46:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.4199e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:00 - INFO - train.train_snli_ve - loss is tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6167/6700 [2:51:53<14:15,  1.61s/it]11/17/2022 01:46:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.7336e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:02 - INFO - train.train_snli_ve - loss is tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6168/6700 [2:51:54<14:13,  1.60s/it]11/17/2022 01:46:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.9711e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:03 - INFO - train.train_snli_ve - loss is tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6169/6700 [2:51:56<14:11,  1.60s/it]11/17/2022 01:46:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1961e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:05 - INFO - train.train_snli_ve - loss is tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6170/6700 [2:51:58<14:15,  1.61s/it]11/17/2022 01:46:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:07 - INFO - train.train_snli_ve - loss is tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6171/6700 [2:51:59<14:11,  1.61s/it]11/17/2022 01:46:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.6815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:08 - INFO - train.train_snli_ve - loss is tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6172/6700 [2:52:01<14:05,  1.60s/it]11/17/2022 01:46:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.9201e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:10 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6173/6700 [2:52:03<14:06,  1.61s/it]11/17/2022 01:46:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.1092e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:11 - INFO - train.train_snli_ve - loss is tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6174/6700 [2:52:04<14:07,  1.61s/it]11/17/2022 01:46:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.0490e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:13 - INFO - train.train_snli_ve - loss is tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6175/6700 [2:52:06<14:04,  1.61s/it]11/17/2022 01:46:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3882e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:15 - INFO - train.train_snli_ve - loss is tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6176/6700 [2:52:07<14:05,  1.61s/it]11/17/2022 01:46:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:16 - INFO - train.train_snli_ve - loss is tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6177/6700 [2:52:09<14:04,  1.62s/it]11/17/2022 01:46:18 - INFO - train.train_snli_ve - kd_loss is tensor(5.7134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:18 - INFO - train.train_snli_ve - loss is tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6178/6700 [2:52:11<13:59,  1.61s/it]11/17/2022 01:46:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.7941e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:20 - INFO - train.train_snli_ve - loss is tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6179/6700 [2:52:12<14:01,  1.62s/it]11/17/2022 01:46:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.4210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:21 - INFO - train.train_snli_ve - loss is tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6180/6700 [2:52:14<14:00,  1.62s/it]11/17/2022 01:46:23 - INFO - train.train_snli_ve - kd_loss is tensor(1.7891e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:23 - INFO - train.train_snli_ve - loss is tensor(0.7348, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6181/6700 [2:52:15<14:02,  1.62s/it]11/17/2022 01:46:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.8340e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:24 - INFO - train.train_snli_ve - loss is tensor(0.4715, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6182/6700 [2:52:17<13:59,  1.62s/it]11/17/2022 01:46:26 - INFO - train.train_snli_ve - kd_loss is tensor(4.2377e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:26 - INFO - train.train_snli_ve - loss is tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6183/6700 [2:52:19<13:51,  1.61s/it]11/17/2022 01:46:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9006e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:28 - INFO - train.train_snli_ve - loss is tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6184/6700 [2:52:20<13:51,  1.61s/it]11/17/2022 01:46:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.8851e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:29 - INFO - train.train_snli_ve - loss is tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6185/6700 [2:52:22<13:57,  1.63s/it]11/17/2022 01:46:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.5552e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:31 - INFO - train.train_snli_ve - loss is tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6186/6700 [2:52:24<14:01,  1.64s/it]11/17/2022 01:46:32 - INFO - train.train_snli_ve - kd_loss is tensor(3.8559e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:32 - INFO - train.train_snli_ve - loss is tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6187/6700 [2:52:25<13:51,  1.62s/it]11/17/2022 01:46:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.5935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:34 - INFO - train.train_snli_ve - loss is tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6188/6700 [2:52:27<13:51,  1.62s/it]11/17/2022 01:46:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.9953e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:36 - INFO - train.train_snli_ve - loss is tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6189/6700 [2:52:28<13:53,  1.63s/it]11/17/2022 01:46:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.5015e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:37 - INFO - train.train_snli_ve - loss is tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6190/6700 [2:52:30<13:49,  1.63s/it]11/17/2022 01:46:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.2590e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:39 - INFO - train.train_snli_ve - loss is tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6191/6700 [2:52:32<13:45,  1.62s/it]11/17/2022 01:46:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.5275e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:41 - INFO - train.train_snli_ve - loss is tensor(0.7496, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6192/6700 [2:52:33<13:50,  1.64s/it]11/17/2022 01:46:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:42 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6193/6700 [2:52:35<13:46,  1.63s/it]11/17/2022 01:46:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.2575e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:44 - INFO - train.train_snli_ve - loss is tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6194/6700 [2:52:37<13:42,  1.62s/it]11/17/2022 01:46:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.9819e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:45 - INFO - train.train_snli_ve - loss is tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6195/6700 [2:52:38<13:34,  1.61s/it]11/17/2022 01:46:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.4271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:47 - INFO - train.train_snli_ve - loss is tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6196/6700 [2:52:40<13:31,  1.61s/it]11/17/2022 01:46:49 - INFO - train.train_snli_ve - kd_loss is tensor(4.2234e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:49 - INFO - train.train_snli_ve - loss is tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  92%|#########2| 6197/6700 [2:52:41<13:29,  1.61s/it]11/17/2022 01:46:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.7421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:50 - INFO - train.train_snli_ve - loss is tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6198/6700 [2:52:43<13:29,  1.61s/it]11/17/2022 01:46:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5675e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:52 - INFO - train.train_snli_ve - loss is tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6199/6700 [2:52:45<13:28,  1.61s/it]11/17/2022 01:46:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.8014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:54 - INFO - train.train_snli_ve - loss is tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6200/6700 [2:52:46<13:35,  1.63s/it]11/17/2022 01:46:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9544e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:55 - INFO - train.train_snli_ve - loss is tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6201/6700 [2:52:48<13:33,  1.63s/it]11/17/2022 01:46:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5351e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:57 - INFO - train.train_snli_ve - loss is tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6202/6700 [2:52:50<13:27,  1.62s/it]11/17/2022 01:46:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6366e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:46:58 - INFO - train.train_snli_ve - loss is tensor(1.0560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6203/6700 [2:52:51<13:24,  1.62s/it]11/17/2022 01:47:00 - INFO - train.train_snli_ve - kd_loss is tensor(4.6510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:00 - INFO - train.train_snli_ve - loss is tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6204/6700 [2:52:53<13:23,  1.62s/it]11/17/2022 01:47:02 - INFO - train.train_snli_ve - kd_loss is tensor(3.2353e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:02 - INFO - train.train_snli_ve - loss is tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6205/6700 [2:52:54<13:19,  1.62s/it]11/17/2022 01:47:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.1136e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:03 - INFO - train.train_snli_ve - loss is tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6206/6700 [2:52:56<13:27,  1.63s/it]11/17/2022 01:47:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.9124e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:05 - INFO - train.train_snli_ve - loss is tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6207/6700 [2:52:58<13:20,  1.62s/it]11/17/2022 01:47:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.0894e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:07 - INFO - train.train_snli_ve - loss is tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6208/6700 [2:52:59<13:16,  1.62s/it]11/17/2022 01:47:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.7502e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:08 - INFO - train.train_snli_ve - loss is tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6209/6700 [2:53:01<13:17,  1.63s/it]11/17/2022 01:47:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.0539e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:10 - INFO - train.train_snli_ve - loss is tensor(0.8679, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6210/6700 [2:53:03<13:27,  1.65s/it]11/17/2022 01:47:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.3909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:11 - INFO - train.train_snli_ve - loss is tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6211/6700 [2:53:04<13:19,  1.64s/it]11/17/2022 01:47:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.2237e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:13 - INFO - train.train_snli_ve - loss is tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6212/6700 [2:53:06<13:15,  1.63s/it]11/17/2022 01:47:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.9667e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:15 - INFO - train.train_snli_ve - loss is tensor(0.4493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6213/6700 [2:53:07<13:07,  1.62s/it]11/17/2022 01:47:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.0408e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:16 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6214/6700 [2:53:09<13:12,  1.63s/it]11/17/2022 01:47:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.2058e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:18 - INFO - train.train_snli_ve - loss is tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6215/6700 [2:53:11<13:05,  1.62s/it]11/17/2022 01:47:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.4588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:20 - INFO - train.train_snli_ve - loss is tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6216/6700 [2:53:12<13:00,  1.61s/it]11/17/2022 01:47:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.9940e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:21 - INFO - train.train_snli_ve - loss is tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6217/6700 [2:53:14<12:58,  1.61s/it]11/17/2022 01:47:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.3722e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:23 - INFO - train.train_snli_ve - loss is tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6218/6700 [2:53:15<12:58,  1.62s/it]11/17/2022 01:47:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.9295e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:24 - INFO - train.train_snli_ve - loss is tensor(0.7915, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6219/6700 [2:53:17<12:54,  1.61s/it]11/17/2022 01:47:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.5863e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:26 - INFO - train.train_snli_ve - loss is tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6220/6700 [2:53:19<12:55,  1.61s/it]11/17/2022 01:47:28 - INFO - train.train_snli_ve - kd_loss is tensor(4.2735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:28 - INFO - train.train_snli_ve - loss is tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6221/6700 [2:53:20<12:53,  1.61s/it]11/17/2022 01:47:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.5641e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:29 - INFO - train.train_snli_ve - loss is tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6222/6700 [2:53:22<12:54,  1.62s/it]11/17/2022 01:47:31 - INFO - train.train_snli_ve - kd_loss is tensor(4.1379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:31 - INFO - train.train_snli_ve - loss is tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6223/6700 [2:53:24<12:54,  1.62s/it]11/17/2022 01:47:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.2988e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:33 - INFO - train.train_snli_ve - loss is tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6224/6700 [2:53:25<12:53,  1.63s/it]11/17/2022 01:47:34 - INFO - train.train_snli_ve - kd_loss is tensor(5.2602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:34 - INFO - train.train_snli_ve - loss is tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6225/6700 [2:53:27<12:46,  1.61s/it]11/17/2022 01:47:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3681e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:36 - INFO - train.train_snli_ve - loss is tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6226/6700 [2:53:28<12:44,  1.61s/it]11/17/2022 01:47:37 - INFO - train.train_snli_ve - kd_loss is tensor(3.8334e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:37 - INFO - train.train_snli_ve - loss is tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6227/6700 [2:53:30<12:40,  1.61s/it]11/17/2022 01:47:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.3404e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:39 - INFO - train.train_snli_ve - loss is tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6228/6700 [2:53:32<12:47,  1.63s/it]11/17/2022 01:47:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.8381e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:41 - INFO - train.train_snli_ve - loss is tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6229/6700 [2:53:33<12:43,  1.62s/it]11/17/2022 01:47:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.8200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:42 - INFO - train.train_snli_ve - loss is tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########2| 6230/6700 [2:53:35<12:36,  1.61s/it]11/17/2022 01:47:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8176e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:44 - INFO - train.train_snli_ve - loss is tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6231/6700 [2:53:36<12:30,  1.60s/it]11/17/2022 01:47:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0466e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:45 - INFO - train.train_snli_ve - loss is tensor(0.9999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6232/6700 [2:53:38<12:27,  1.60s/it]11/17/2022 01:47:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.8171e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:47 - INFO - train.train_snli_ve - loss is tensor(0.7602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6233/6700 [2:53:40<12:27,  1.60s/it]11/17/2022 01:47:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.5213e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:49 - INFO - train.train_snli_ve - loss is tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6234/6700 [2:53:41<12:23,  1.60s/it]11/17/2022 01:47:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.0805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:50 - INFO - train.train_snli_ve - loss is tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6235/6700 [2:53:43<12:20,  1.59s/it]11/17/2022 01:47:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5650e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:52 - INFO - train.train_snli_ve - loss is tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6236/6700 [2:53:44<12:18,  1.59s/it]11/17/2022 01:47:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.4689e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:53 - INFO - train.train_snli_ve - loss is tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6237/6700 [2:53:46<12:23,  1.61s/it]11/17/2022 01:47:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2630e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:55 - INFO - train.train_snli_ve - loss is tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6238/6700 [2:53:48<12:23,  1.61s/it]11/17/2022 01:47:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.3286e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:57 - INFO - train.train_snli_ve - loss is tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6239/6700 [2:53:49<12:19,  1.60s/it]11/17/2022 01:47:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.4777e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:47:58 - INFO - train.train_snli_ve - loss is tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6240/6700 [2:53:51<12:22,  1.62s/it]11/17/2022 01:48:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:00 - INFO - train.train_snli_ve - loss is tensor(0.8144, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6241/6700 [2:53:53<12:22,  1.62s/it]11/17/2022 01:48:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8081e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:01 - INFO - train.train_snli_ve - loss is tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6242/6700 [2:53:54<12:22,  1.62s/it]11/17/2022 01:48:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2952e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:03 - INFO - train.train_snli_ve - loss is tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6243/6700 [2:53:56<12:21,  1.62s/it]11/17/2022 01:48:05 - INFO - train.train_snli_ve - kd_loss is tensor(1.8701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:05 - INFO - train.train_snli_ve - loss is tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6244/6700 [2:53:57<12:18,  1.62s/it]11/17/2022 01:48:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.8977e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:06 - INFO - train.train_snli_ve - loss is tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6245/6700 [2:53:59<12:18,  1.62s/it]11/17/2022 01:48:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.6460e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:08 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6246/6700 [2:54:01<12:15,  1.62s/it]11/17/2022 01:48:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.9933e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:10 - INFO - train.train_snli_ve - loss is tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6247/6700 [2:54:02<12:15,  1.62s/it]11/17/2022 01:48:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.6849e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:11 - INFO - train.train_snli_ve - loss is tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6248/6700 [2:54:04<12:13,  1.62s/it]11/17/2022 01:48:13 - INFO - train.train_snli_ve - kd_loss is tensor(1.2805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:13 - INFO - train.train_snli_ve - loss is tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6249/6700 [2:54:06<12:15,  1.63s/it]11/17/2022 01:48:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.7574e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:14 - INFO - train.train_snli_ve - loss is tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6250/6700 [2:54:07<12:11,  1.62s/it]11/17/2022 01:48:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3734e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:16 - INFO - train.train_snli_ve - loss is tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6251/6700 [2:54:09<12:07,  1.62s/it]11/17/2022 01:48:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.5313e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:18 - INFO - train.train_snli_ve - loss is tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6252/6700 [2:54:10<12:03,  1.62s/it]11/17/2022 01:48:19 - INFO - train.train_snli_ve - kd_loss is tensor(5.4820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:19 - INFO - train.train_snli_ve - loss is tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6253/6700 [2:54:12<12:00,  1.61s/it]11/17/2022 01:48:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.5513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:21 - INFO - train.train_snli_ve - loss is tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6254/6700 [2:54:14<11:52,  1.60s/it]11/17/2022 01:48:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.5850e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:22 - INFO - train.train_snli_ve - loss is tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6255/6700 [2:54:15<11:50,  1.60s/it]11/17/2022 01:48:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.1028e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:24 - INFO - train.train_snli_ve - loss is tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6256/6700 [2:54:17<11:50,  1.60s/it]11/17/2022 01:48:26 - INFO - train.train_snli_ve - kd_loss is tensor(3.0639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:26 - INFO - train.train_snli_ve - loss is tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6257/6700 [2:54:18<11:50,  1.60s/it]11/17/2022 01:48:27 - INFO - train.train_snli_ve - kd_loss is tensor(4.3195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:27 - INFO - train.train_snli_ve - loss is tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6258/6700 [2:54:20<11:49,  1.61s/it]11/17/2022 01:48:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.7959e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:29 - INFO - train.train_snli_ve - loss is tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6259/6700 [2:54:22<11:49,  1.61s/it]11/17/2022 01:48:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.8045e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:30 - INFO - train.train_snli_ve - loss is tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6260/6700 [2:54:23<11:43,  1.60s/it]11/17/2022 01:48:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.8512e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:32 - INFO - train.train_snli_ve - loss is tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6261/6700 [2:54:25<11:46,  1.61s/it]11/17/2022 01:48:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.2805e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:34 - INFO - train.train_snli_ve - loss is tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6262/6700 [2:54:26<11:49,  1.62s/it]11/17/2022 01:48:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.3194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:35 - INFO - train.train_snli_ve - loss is tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6263/6700 [2:54:28<11:44,  1.61s/it]11/17/2022 01:48:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.7560e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:37 - INFO - train.train_snli_ve - loss is tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  93%|#########3| 6264/6700 [2:54:30<11:40,  1.61s/it]11/17/2022 01:48:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.5169e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:39 - INFO - train.train_snli_ve - loss is tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6265/6700 [2:54:31<11:39,  1.61s/it]11/17/2022 01:48:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.5231e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:40 - INFO - train.train_snli_ve - loss is tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6266/6700 [2:54:33<11:43,  1.62s/it]11/17/2022 01:48:42 - INFO - train.train_snli_ve - kd_loss is tensor(6.6396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:42 - INFO - train.train_snli_ve - loss is tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6267/6700 [2:54:35<11:45,  1.63s/it]11/17/2022 01:48:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:43 - INFO - train.train_snli_ve - loss is tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6268/6700 [2:54:36<11:47,  1.64s/it]11/17/2022 01:48:45 - INFO - train.train_snli_ve - kd_loss is tensor(3.0210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:45 - INFO - train.train_snli_ve - loss is tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6269/6700 [2:54:38<11:41,  1.63s/it]11/17/2022 01:48:47 - INFO - train.train_snli_ve - kd_loss is tensor(6.7701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:47 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6270/6700 [2:54:39<11:33,  1.61s/it]11/17/2022 01:48:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.8944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:48 - INFO - train.train_snli_ve - loss is tensor(0.8554, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6271/6700 [2:54:41<11:34,  1.62s/it]11/17/2022 01:48:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.8735e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:50 - INFO - train.train_snli_ve - loss is tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6272/6700 [2:54:43<11:31,  1.62s/it]11/17/2022 01:48:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.0024e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:51 - INFO - train.train_snli_ve - loss is tensor(0.8651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6273/6700 [2:54:44<11:25,  1.61s/it]11/17/2022 01:48:53 - INFO - train.train_snli_ve - kd_loss is tensor(4.2793e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:53 - INFO - train.train_snli_ve - loss is tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6274/6700 [2:54:46<11:26,  1.61s/it]11/17/2022 01:48:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.0423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:55 - INFO - train.train_snli_ve - loss is tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6275/6700 [2:54:47<11:28,  1.62s/it]11/17/2022 01:48:56 - INFO - train.train_snli_ve - kd_loss is tensor(1.6622e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:56 - INFO - train.train_snli_ve - loss is tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6276/6700 [2:54:49<11:29,  1.63s/it]11/17/2022 01:48:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.6710e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:48:58 - INFO - train.train_snli_ve - loss is tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6277/6700 [2:54:51<11:26,  1.62s/it]11/17/2022 01:49:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.8133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:00 - INFO - train.train_snli_ve - loss is tensor(0.5310, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6278/6700 [2:54:52<11:21,  1.62s/it]11/17/2022 01:49:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.6618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:01 - INFO - train.train_snli_ve - loss is tensor(0.5034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6279/6700 [2:54:54<11:20,  1.62s/it]11/17/2022 01:49:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.8998e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:03 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6280/6700 [2:54:56<11:15,  1.61s/it]11/17/2022 01:49:04 - INFO - train.train_snli_ve - kd_loss is tensor(4.7079e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:04 - INFO - train.train_snli_ve - loss is tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6281/6700 [2:54:57<11:19,  1.62s/it]11/17/2022 01:49:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.2973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:06 - INFO - train.train_snli_ve - loss is tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6282/6700 [2:54:59<11:22,  1.63s/it]11/17/2022 01:49:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.3694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:08 - INFO - train.train_snli_ve - loss is tensor(0.5841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6283/6700 [2:55:00<11:18,  1.63s/it]11/17/2022 01:49:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2236e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:09 - INFO - train.train_snli_ve - loss is tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6284/6700 [2:55:02<11:17,  1.63s/it]11/17/2022 01:49:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.9754e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:11 - INFO - train.train_snli_ve - loss is tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6285/6700 [2:55:04<11:16,  1.63s/it]11/17/2022 01:49:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.2707e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:13 - INFO - train.train_snli_ve - loss is tensor(0.3427, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6286/6700 [2:55:05<11:13,  1.63s/it]11/17/2022 01:49:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.0616e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:14 - INFO - train.train_snli_ve - loss is tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6287/6700 [2:55:07<11:08,  1.62s/it]11/17/2022 01:49:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3508e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:16 - INFO - train.train_snli_ve - loss is tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6288/6700 [2:55:09<11:07,  1.62s/it]11/17/2022 01:49:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.6235e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:17 - INFO - train.train_snli_ve - loss is tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6289/6700 [2:55:10<11:03,  1.61s/it]11/17/2022 01:49:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.0491e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:19 - INFO - train.train_snli_ve - loss is tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6290/6700 [2:55:12<11:03,  1.62s/it]11/17/2022 01:49:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.7659e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:21 - INFO - train.train_snli_ve - loss is tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6291/6700 [2:55:13<11:00,  1.62s/it]11/17/2022 01:49:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.2944e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:22 - INFO - train.train_snli_ve - loss is tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6292/6700 [2:55:15<10:57,  1.61s/it]11/17/2022 01:49:24 - INFO - train.train_snli_ve - kd_loss is tensor(5.5736e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:24 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6293/6700 [2:55:17<10:57,  1.62s/it]11/17/2022 01:49:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1178e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:26 - INFO - train.train_snli_ve - loss is tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6294/6700 [2:55:18<10:52,  1.61s/it]11/17/2022 01:49:27 - INFO - train.train_snli_ve - kd_loss is tensor(5.6111e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:27 - INFO - train.train_snli_ve - loss is tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6295/6700 [2:55:20<10:51,  1.61s/it]11/17/2022 01:49:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.4862e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:29 - INFO - train.train_snli_ve - loss is tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6296/6700 [2:55:21<10:51,  1.61s/it]11/17/2022 01:49:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.9856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:30 - INFO - train.train_snli_ve - loss is tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6297/6700 [2:55:23<10:52,  1.62s/it]11/17/2022 01:49:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.1197e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:32 - INFO - train.train_snli_ve - loss is tensor(0.5098, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########3| 6298/6700 [2:55:25<10:50,  1.62s/it]11/17/2022 01:49:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.1097e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:34 - INFO - train.train_snli_ve - loss is tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6299/6700 [2:55:26<10:48,  1.62s/it]11/17/2022 01:49:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.0672e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:35 - INFO - train.train_snli_ve - loss is tensor(0.9133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6300/6700 [2:55:28<10:49,  1.62s/it]11/17/2022 01:49:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.9110e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:37 - INFO - train.train_snli_ve - loss is tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6301/6700 [2:55:30<10:54,  1.64s/it]11/17/2022 01:49:39 - INFO - train.train_snli_ve - kd_loss is tensor(6.5001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:39 - INFO - train.train_snli_ve - loss is tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6302/6700 [2:55:31<10:46,  1.62s/it]11/17/2022 01:49:40 - INFO - train.train_snli_ve - kd_loss is tensor(3.4420e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:40 - INFO - train.train_snli_ve - loss is tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6303/6700 [2:55:33<10:45,  1.63s/it]11/17/2022 01:49:42 - INFO - train.train_snli_ve - kd_loss is tensor(3.7303e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:42 - INFO - train.train_snli_ve - loss is tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6304/6700 [2:55:34<10:40,  1.62s/it]11/17/2022 01:49:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.4883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:43 - INFO - train.train_snli_ve - loss is tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6305/6700 [2:55:36<10:36,  1.61s/it]11/17/2022 01:49:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.8739e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:45 - INFO - train.train_snli_ve - loss is tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6306/6700 [2:55:38<10:37,  1.62s/it]11/17/2022 01:49:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.4766e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:47 - INFO - train.train_snli_ve - loss is tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6307/6700 [2:55:39<10:36,  1.62s/it]11/17/2022 01:49:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.5639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:48 - INFO - train.train_snli_ve - loss is tensor(0.8493, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6308/6700 [2:55:41<10:35,  1.62s/it]11/17/2022 01:49:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.2291e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:50 - INFO - train.train_snli_ve - loss is tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6309/6700 [2:55:43<10:39,  1.64s/it]11/17/2022 01:49:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1720e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:52 - INFO - train.train_snli_ve - loss is tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6310/6700 [2:55:44<10:44,  1.65s/it]11/17/2022 01:49:53 - INFO - train.train_snli_ve - kd_loss is tensor(3.2112e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:53 - INFO - train.train_snli_ve - loss is tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6311/6700 [2:55:46<10:37,  1.64s/it]11/17/2022 01:49:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.0506e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:55 - INFO - train.train_snli_ve - loss is tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6312/6700 [2:55:47<10:30,  1.62s/it]11/17/2022 01:49:56 - INFO - train.train_snli_ve - kd_loss is tensor(4.2433e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:56 - INFO - train.train_snli_ve - loss is tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6313/6700 [2:55:49<10:27,  1.62s/it]11/17/2022 01:49:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:49:58 - INFO - train.train_snli_ve - loss is tensor(0.8971, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6314/6700 [2:55:51<10:23,  1.61s/it]11/17/2022 01:50:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.3133e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:00 - INFO - train.train_snli_ve - loss is tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6315/6700 [2:55:52<10:21,  1.61s/it]11/17/2022 01:50:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.5738e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:01 - INFO - train.train_snli_ve - loss is tensor(0.5345, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6316/6700 [2:55:54<10:28,  1.64s/it]11/17/2022 01:50:03 - INFO - train.train_snli_ve - kd_loss is tensor(4.0467e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:03 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6317/6700 [2:55:56<10:24,  1.63s/it]11/17/2022 01:50:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.9405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:04 - INFO - train.train_snli_ve - loss is tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6318/6700 [2:55:57<10:17,  1.62s/it]11/17/2022 01:50:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.3405e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:06 - INFO - train.train_snli_ve - loss is tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6319/6700 [2:55:59<10:17,  1.62s/it]11/17/2022 01:50:08 - INFO - train.train_snli_ve - kd_loss is tensor(3.5143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:08 - INFO - train.train_snli_ve - loss is tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6320/6700 [2:56:00<10:16,  1.62s/it]11/17/2022 01:50:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.8425e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:09 - INFO - train.train_snli_ve - loss is tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6321/6700 [2:56:02<10:16,  1.63s/it]11/17/2022 01:50:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.5162e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:11 - INFO - train.train_snli_ve - loss is tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6322/6700 [2:56:04<10:11,  1.62s/it]11/17/2022 01:50:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.8664e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:13 - INFO - train.train_snli_ve - loss is tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6323/6700 [2:56:05<10:08,  1.61s/it]11/17/2022 01:50:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.5191e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:14 - INFO - train.train_snli_ve - loss is tensor(0.9129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6324/6700 [2:56:07<10:13,  1.63s/it]11/17/2022 01:50:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.3161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:16 - INFO - train.train_snli_ve - loss is tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6325/6700 [2:56:09<10:06,  1.62s/it]11/17/2022 01:50:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.2494e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:17 - INFO - train.train_snli_ve - loss is tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6326/6700 [2:56:10<10:00,  1.61s/it]11/17/2022 01:50:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.6361e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:19 - INFO - train.train_snli_ve - loss is tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6327/6700 [2:56:12<09:59,  1.61s/it]11/17/2022 01:50:21 - INFO - train.train_snli_ve - kd_loss is tensor(5.1368e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:21 - INFO - train.train_snli_ve - loss is tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6328/6700 [2:56:13<10:00,  1.62s/it]11/17/2022 01:50:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9513e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:22 - INFO - train.train_snli_ve - loss is tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6329/6700 [2:56:15<09:59,  1.62s/it]11/17/2022 01:50:24 - INFO - train.train_snli_ve - kd_loss is tensor(2.3288e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:24 - INFO - train.train_snli_ve - loss is tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6330/6700 [2:56:17<09:58,  1.62s/it]11/17/2022 01:50:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1352e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:26 - INFO - train.train_snli_ve - loss is tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  94%|#########4| 6331/6700 [2:56:18<09:57,  1.62s/it]11/17/2022 01:50:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.5847e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:27 - INFO - train.train_snli_ve - loss is tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6332/6700 [2:56:20<09:52,  1.61s/it]11/17/2022 01:50:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.8493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:29 - INFO - train.train_snli_ve - loss is tensor(1.0211, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6333/6700 [2:56:21<09:53,  1.62s/it]11/17/2022 01:50:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.1157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:30 - INFO - train.train_snli_ve - loss is tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6334/6700 [2:56:23<09:53,  1.62s/it]11/17/2022 01:50:32 - INFO - train.train_snli_ve - kd_loss is tensor(5.0126e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:32 - INFO - train.train_snli_ve - loss is tensor(0.7818, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6335/6700 [2:56:25<09:48,  1.61s/it]11/17/2022 01:50:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.7995e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:34 - INFO - train.train_snli_ve - loss is tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6336/6700 [2:56:26<09:47,  1.61s/it]11/17/2022 01:50:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.4369e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:35 - INFO - train.train_snli_ve - loss is tensor(0.8082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6337/6700 [2:56:28<09:44,  1.61s/it]11/17/2022 01:50:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.8903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:37 - INFO - train.train_snli_ve - loss is tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6338/6700 [2:56:29<09:42,  1.61s/it]11/17/2022 01:50:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1984e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:38 - INFO - train.train_snli_ve - loss is tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6339/6700 [2:56:31<09:45,  1.62s/it]11/17/2022 01:50:40 - INFO - train.train_snli_ve - kd_loss is tensor(1.4418e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:40 - INFO - train.train_snli_ve - loss is tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6340/6700 [2:56:33<09:41,  1.61s/it]11/17/2022 01:50:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.1826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:42 - INFO - train.train_snli_ve - loss is tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6341/6700 [2:56:34<09:37,  1.61s/it]11/17/2022 01:50:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9968e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:43 - INFO - train.train_snli_ve - loss is tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6342/6700 [2:56:36<09:38,  1.62s/it]11/17/2022 01:50:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.1416e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:45 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6343/6700 [2:56:38<09:34,  1.61s/it]11/17/2022 01:50:46 - INFO - train.train_snli_ve - kd_loss is tensor(3.3139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:46 - INFO - train.train_snli_ve - loss is tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6344/6700 [2:56:39<09:35,  1.62s/it]11/17/2022 01:50:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.8980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:48 - INFO - train.train_snli_ve - loss is tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6345/6700 [2:56:41<09:30,  1.61s/it]11/17/2022 01:50:50 - INFO - train.train_snli_ve - kd_loss is tensor(5.4875e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:50 - INFO - train.train_snli_ve - loss is tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6346/6700 [2:56:42<09:32,  1.62s/it]11/17/2022 01:50:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.4626e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:51 - INFO - train.train_snli_ve - loss is tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6347/6700 [2:56:44<09:31,  1.62s/it]11/17/2022 01:50:53 - INFO - train.train_snli_ve - kd_loss is tensor(1.5666e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:53 - INFO - train.train_snli_ve - loss is tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6348/6700 [2:56:46<09:29,  1.62s/it]11/17/2022 01:50:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.2800e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:55 - INFO - train.train_snli_ve - loss is tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6349/6700 [2:56:47<09:29,  1.62s/it]11/17/2022 01:50:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.7209e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:56 - INFO - train.train_snli_ve - loss is tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6350/6700 [2:56:49<09:32,  1.63s/it]11/17/2022 01:50:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.0118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:58 - INFO - train.train_snli_ve - loss is tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6351/6700 [2:56:51<09:28,  1.63s/it]11/17/2022 01:50:59 - INFO - train.train_snli_ve - kd_loss is tensor(3.5705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:50:59 - INFO - train.train_snli_ve - loss is tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6352/6700 [2:56:52<09:25,  1.62s/it]11/17/2022 01:51:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.9180e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:01 - INFO - train.train_snli_ve - loss is tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6353/6700 [2:56:54<09:21,  1.62s/it]11/17/2022 01:51:03 - INFO - train.train_snli_ve - kd_loss is tensor(3.8726e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:03 - INFO - train.train_snli_ve - loss is tensor(0.5294, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6354/6700 [2:56:55<09:15,  1.61s/it]11/17/2022 01:51:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.6384e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:04 - INFO - train.train_snli_ve - loss is tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6355/6700 [2:56:57<09:15,  1.61s/it]11/17/2022 01:51:06 - INFO - train.train_snli_ve - kd_loss is tensor(1.4697e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:06 - INFO - train.train_snli_ve - loss is tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6356/6700 [2:56:59<09:15,  1.61s/it]11/17/2022 01:51:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.8803e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:08 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6357/6700 [2:57:00<09:13,  1.61s/it]11/17/2022 01:51:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.2372e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:09 - INFO - train.train_snli_ve - loss is tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6358/6700 [2:57:02<09:13,  1.62s/it]11/17/2022 01:51:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.4589e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:11 - INFO - train.train_snli_ve - loss is tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6359/6700 [2:57:03<09:10,  1.61s/it]11/17/2022 01:51:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.4122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:12 - INFO - train.train_snli_ve - loss is tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6360/6700 [2:57:05<09:10,  1.62s/it]11/17/2022 01:51:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.0567e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:14 - INFO - train.train_snli_ve - loss is tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6361/6700 [2:57:07<09:06,  1.61s/it]11/17/2022 01:51:16 - INFO - train.train_snli_ve - kd_loss is tensor(3.4623e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:16 - INFO - train.train_snli_ve - loss is tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6362/6700 [2:57:08<09:02,  1.61s/it]11/17/2022 01:51:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.7614e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:17 - INFO - train.train_snli_ve - loss is tensor(0.8692, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6363/6700 [2:57:10<08:59,  1.60s/it]11/17/2022 01:51:19 - INFO - train.train_snli_ve - kd_loss is tensor(2.9909e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:19 - INFO - train.train_snli_ve - loss is tensor(0.9336, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########4| 6364/6700 [2:57:11<09:01,  1.61s/it]11/17/2022 01:51:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:20 - INFO - train.train_snli_ve - loss is tensor(0.7614, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6365/6700 [2:57:13<08:56,  1.60s/it]11/17/2022 01:51:22 - INFO - train.train_snli_ve - kd_loss is tensor(4.8598e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:22 - INFO - train.train_snli_ve - loss is tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6366/6700 [2:57:15<08:56,  1.61s/it]11/17/2022 01:51:24 - INFO - train.train_snli_ve - kd_loss is tensor(1.7712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:24 - INFO - train.train_snli_ve - loss is tensor(0.5465, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6367/6700 [2:57:16<08:55,  1.61s/it]11/17/2022 01:51:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5413e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:25 - INFO - train.train_snli_ve - loss is tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6368/6700 [2:57:18<08:54,  1.61s/it]11/17/2022 01:51:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.5207e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:27 - INFO - train.train_snli_ve - loss is tensor(0.5921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6369/6700 [2:57:20<08:52,  1.61s/it]11/17/2022 01:51:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.6841e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:28 - INFO - train.train_snli_ve - loss is tensor(0.5049, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6370/6700 [2:57:21<08:52,  1.61s/it]11/17/2022 01:51:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.1153e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:30 - INFO - train.train_snli_ve - loss is tensor(0.8857, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6371/6700 [2:57:23<08:49,  1.61s/it]11/17/2022 01:51:32 - INFO - train.train_snli_ve - kd_loss is tensor(2.6751e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:32 - INFO - train.train_snli_ve - loss is tensor(0.9783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6372/6700 [2:57:24<08:45,  1.60s/it]11/17/2022 01:51:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.8918e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:33 - INFO - train.train_snli_ve - loss is tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6373/6700 [2:57:26<08:47,  1.61s/it]11/17/2022 01:51:35 - INFO - train.train_snli_ve - kd_loss is tensor(3.3447e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:35 - INFO - train.train_snli_ve - loss is tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6374/6700 [2:57:28<08:50,  1.63s/it]11/17/2022 01:51:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.5020e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:37 - INFO - train.train_snli_ve - loss is tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6375/6700 [2:57:29<08:48,  1.62s/it]11/17/2022 01:51:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9611e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:38 - INFO - train.train_snli_ve - loss is tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6376/6700 [2:57:31<08:45,  1.62s/it]11/17/2022 01:51:40 - INFO - train.train_snli_ve - kd_loss is tensor(4.1011e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:40 - INFO - train.train_snli_ve - loss is tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6377/6700 [2:57:33<08:44,  1.62s/it]11/17/2022 01:51:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.6452e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:41 - INFO - train.train_snli_ve - loss is tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6378/6700 [2:57:34<08:44,  1.63s/it]11/17/2022 01:51:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.8883e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:43 - INFO - train.train_snli_ve - loss is tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6379/6700 [2:57:36<08:43,  1.63s/it]11/17/2022 01:51:45 - INFO - train.train_snli_ve - kd_loss is tensor(2.0387e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:45 - INFO - train.train_snli_ve - loss is tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6380/6700 [2:57:37<08:41,  1.63s/it]11/17/2022 01:51:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.4122e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:46 - INFO - train.train_snli_ve - loss is tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6381/6700 [2:57:39<08:36,  1.62s/it]11/17/2022 01:51:48 - INFO - train.train_snli_ve - kd_loss is tensor(1.6517e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:48 - INFO - train.train_snli_ve - loss is tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6382/6700 [2:57:41<08:38,  1.63s/it]11/17/2022 01:51:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.6200e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:50 - INFO - train.train_snli_ve - loss is tensor(0.7841, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6383/6700 [2:57:42<08:32,  1.62s/it]11/17/2022 01:51:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.6205e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:51 - INFO - train.train_snli_ve - loss is tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6384/6700 [2:57:44<08:27,  1.61s/it]11/17/2022 01:51:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.5762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:53 - INFO - train.train_snli_ve - loss is tensor(0.5001, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6385/6700 [2:57:45<08:27,  1.61s/it]11/17/2022 01:51:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.8018e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:54 - INFO - train.train_snli_ve - loss is tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6386/6700 [2:57:47<08:23,  1.60s/it]11/17/2022 01:51:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.7392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:56 - INFO - train.train_snli_ve - loss is tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6387/6700 [2:57:49<08:25,  1.61s/it]11/17/2022 01:51:58 - INFO - train.train_snli_ve - kd_loss is tensor(1.6131e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:58 - INFO - train.train_snli_ve - loss is tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6388/6700 [2:57:50<08:24,  1.62s/it]11/17/2022 01:51:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.3732e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:51:59 - INFO - train.train_snli_ve - loss is tensor(0.7746, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6389/6700 [2:57:52<08:24,  1.62s/it]11/17/2022 01:52:01 - INFO - train.train_snli_ve - kd_loss is tensor(1.8639e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:01 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6390/6700 [2:57:54<08:21,  1.62s/it]11/17/2022 01:52:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.2276e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:02 - INFO - train.train_snli_ve - loss is tensor(0.5454, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6391/6700 [2:57:55<08:22,  1.63s/it]11/17/2022 01:52:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.0430e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:04 - INFO - train.train_snli_ve - loss is tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6392/6700 [2:57:57<08:21,  1.63s/it]11/17/2022 01:52:06 - INFO - train.train_snli_ve - kd_loss is tensor(2.3202e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:06 - INFO - train.train_snli_ve - loss is tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6393/6700 [2:57:58<08:16,  1.62s/it]11/17/2022 01:52:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.1259e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:07 - INFO - train.train_snli_ve - loss is tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6394/6700 [2:58:00<08:20,  1.64s/it]11/17/2022 01:52:09 - INFO - train.train_snli_ve - kd_loss is tensor(5.1607e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:09 - INFO - train.train_snli_ve - loss is tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6395/6700 [2:58:02<08:14,  1.62s/it]11/17/2022 01:52:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.2826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:11 - INFO - train.train_snli_ve - loss is tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6396/6700 [2:58:03<08:14,  1.63s/it]11/17/2022 01:52:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.9309e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:12 - INFO - train.train_snli_ve - loss is tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6397/6700 [2:58:05<08:12,  1.63s/it]11/17/2022 01:52:14 - INFO - train.train_snli_ve - kd_loss is tensor(2.2216e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:14 - INFO - train.train_snli_ve - loss is tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  95%|#########5| 6398/6700 [2:58:07<08:07,  1.61s/it]11/17/2022 01:52:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.9625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:15 - INFO - train.train_snli_ve - loss is tensor(0.5339, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6399/6700 [2:58:08<08:03,  1.61s/it]11/17/2022 01:52:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:17 - INFO - train.train_snli_ve - loss is tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6400/6700 [2:58:10<08:02,  1.61s/it]11/17/2022 01:52:19 - INFO - train.train_snli_ve - kd_loss is tensor(1.8724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:19 - INFO - train.train_snli_ve - loss is tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6401/6700 [2:58:11<08:00,  1.61s/it]11/17/2022 01:52:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.6712e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:20 - INFO - train.train_snli_ve - loss is tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6402/6700 [2:58:13<08:05,  1.63s/it]11/17/2022 01:52:22 - INFO - train.train_snli_ve - kd_loss is tensor(1.9758e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:22 - INFO - train.train_snli_ve - loss is tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6403/6700 [2:58:15<07:59,  1.62s/it]11/17/2022 01:52:24 - INFO - train.train_snli_ve - kd_loss is tensor(3.1240e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:24 - INFO - train.train_snli_ve - loss is tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6404/6700 [2:58:16<08:01,  1.63s/it]11/17/2022 01:52:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.6392e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:25 - INFO - train.train_snli_ve - loss is tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6405/6700 [2:58:18<08:02,  1.63s/it]11/17/2022 01:52:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.8116e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:27 - INFO - train.train_snli_ve - loss is tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6406/6700 [2:58:20<07:58,  1.63s/it]11/17/2022 01:52:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9337e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:28 - INFO - train.train_snli_ve - loss is tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6407/6700 [2:58:21<07:52,  1.61s/it]11/17/2022 01:52:30 - INFO - train.train_snli_ve - kd_loss is tensor(1.7830e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:30 - INFO - train.train_snli_ve - loss is tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6408/6700 [2:58:23<07:51,  1.61s/it]11/17/2022 01:52:32 - INFO - train.train_snli_ve - kd_loss is tensor(1.4165e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:32 - INFO - train.train_snli_ve - loss is tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6409/6700 [2:58:24<07:48,  1.61s/it]11/17/2022 01:52:33 - INFO - train.train_snli_ve - kd_loss is tensor(4.6233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:33 - INFO - train.train_snli_ve - loss is tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6410/6700 [2:58:26<07:50,  1.62s/it]11/17/2022 01:52:35 - INFO - train.train_snli_ve - kd_loss is tensor(2.8530e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:35 - INFO - train.train_snli_ve - loss is tensor(0.4789, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6411/6700 [2:58:28<07:46,  1.61s/it]11/17/2022 01:52:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:36 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6412/6700 [2:58:29<07:47,  1.62s/it]11/17/2022 01:52:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.8148e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:38 - INFO - train.train_snli_ve - loss is tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6413/6700 [2:58:31<07:43,  1.62s/it]11/17/2022 01:52:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.1301e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:40 - INFO - train.train_snli_ve - loss is tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6414/6700 [2:58:32<07:47,  1.63s/it]11/17/2022 01:52:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.2936e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:41 - INFO - train.train_snli_ve - loss is tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6415/6700 [2:58:34<07:42,  1.62s/it]11/17/2022 01:52:43 - INFO - train.train_snli_ve - kd_loss is tensor(1.9372e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:43 - INFO - train.train_snli_ve - loss is tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6416/6700 [2:58:36<07:39,  1.62s/it]11/17/2022 01:52:45 - INFO - train.train_snli_ve - kd_loss is tensor(1.9740e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:45 - INFO - train.train_snli_ve - loss is tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6417/6700 [2:58:37<07:40,  1.63s/it]11/17/2022 01:52:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:46 - INFO - train.train_snli_ve - loss is tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6418/6700 [2:58:39<07:35,  1.62s/it]11/17/2022 01:52:48 - INFO - train.train_snli_ve - kd_loss is tensor(3.1625e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:48 - INFO - train.train_snli_ve - loss is tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6419/6700 [2:58:41<07:32,  1.61s/it]11/17/2022 01:52:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.5896e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:49 - INFO - train.train_snli_ve - loss is tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6420/6700 [2:58:42<07:34,  1.62s/it]11/17/2022 01:52:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0333e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:51 - INFO - train.train_snli_ve - loss is tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6421/6700 [2:58:44<07:34,  1.63s/it]11/17/2022 01:52:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.1145e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:53 - INFO - train.train_snli_ve - loss is tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6422/6700 [2:58:45<07:32,  1.63s/it]11/17/2022 01:52:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.6196e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:54 - INFO - train.train_snli_ve - loss is tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6423/6700 [2:58:47<07:31,  1.63s/it]11/17/2022 01:52:56 - INFO - train.train_snli_ve - kd_loss is tensor(3.2456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:56 - INFO - train.train_snli_ve - loss is tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6424/6700 [2:58:49<07:28,  1.62s/it]11/17/2022 01:52:58 - INFO - train.train_snli_ve - kd_loss is tensor(2.4105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:58 - INFO - train.train_snli_ve - loss is tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6425/6700 [2:58:50<07:23,  1.61s/it]11/17/2022 01:52:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.2105e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:52:59 - INFO - train.train_snli_ve - loss is tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6426/6700 [2:58:52<07:23,  1.62s/it]11/17/2022 01:53:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.4554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:01 - INFO - train.train_snli_ve - loss is tensor(0.8910, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6427/6700 [2:58:54<07:21,  1.62s/it]11/17/2022 01:53:02 - INFO - train.train_snli_ve - kd_loss is tensor(1.9245e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:02 - INFO - train.train_snli_ve - loss is tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6428/6700 [2:58:55<07:20,  1.62s/it]11/17/2022 01:53:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.5379e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:04 - INFO - train.train_snli_ve - loss is tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6429/6700 [2:58:57<07:20,  1.63s/it]11/17/2022 01:53:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.0065e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:06 - INFO - train.train_snli_ve - loss is tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6430/6700 [2:58:58<07:15,  1.61s/it]11/17/2022 01:53:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.3409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:07 - INFO - train.train_snli_ve - loss is tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########5| 6431/6700 [2:59:00<07:16,  1.62s/it]11/17/2022 01:53:09 - INFO - train.train_snli_ve - kd_loss is tensor(4.0595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:09 - INFO - train.train_snli_ve - loss is tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6432/6700 [2:59:02<07:12,  1.62s/it]11/17/2022 01:53:11 - INFO - train.train_snli_ve - kd_loss is tensor(2.2605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:11 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6433/6700 [2:59:03<07:12,  1.62s/it]11/17/2022 01:53:12 - INFO - train.train_snli_ve - kd_loss is tensor(3.1558e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:12 - INFO - train.train_snli_ve - loss is tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6434/6700 [2:59:05<07:07,  1.61s/it]11/17/2022 01:53:14 - INFO - train.train_snli_ve - kd_loss is tensor(1.0972e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:14 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6435/6700 [2:59:06<07:05,  1.60s/it]11/17/2022 01:53:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.1042e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:15 - INFO - train.train_snli_ve - loss is tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6436/6700 [2:59:08<07:02,  1.60s/it]11/17/2022 01:53:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.5285e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:17 - INFO - train.train_snli_ve - loss is tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6437/6700 [2:59:10<07:03,  1.61s/it]11/17/2022 01:53:19 - INFO - train.train_snli_ve - kd_loss is tensor(3.2618e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:19 - INFO - train.train_snli_ve - loss is tensor(0.5787, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6438/6700 [2:59:11<07:00,  1.61s/it]11/17/2022 01:53:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.8953e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:20 - INFO - train.train_snli_ve - loss is tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6439/6700 [2:59:13<07:04,  1.63s/it]11/17/2022 01:53:22 - INFO - train.train_snli_ve - kd_loss is tensor(3.5971e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:22 - INFO - train.train_snli_ve - loss is tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6440/6700 [2:59:15<07:04,  1.63s/it]11/17/2022 01:53:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.1093e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:23 - INFO - train.train_snli_ve - loss is tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6441/6700 [2:59:16<07:00,  1.62s/it]11/17/2022 01:53:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.8595e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:25 - INFO - train.train_snli_ve - loss is tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6442/6700 [2:59:18<06:55,  1.61s/it]11/17/2022 01:53:27 - INFO - train.train_snli_ve - kd_loss is tensor(2.3223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:27 - INFO - train.train_snli_ve - loss is tensor(0.9394, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6443/6700 [2:59:19<06:53,  1.61s/it]11/17/2022 01:53:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.4804e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:28 - INFO - train.train_snli_ve - loss is tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6444/6700 [2:59:21<06:50,  1.61s/it]11/17/2022 01:53:30 - INFO - train.train_snli_ve - kd_loss is tensor(3.5482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:30 - INFO - train.train_snli_ve - loss is tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6445/6700 [2:59:23<06:48,  1.60s/it]11/17/2022 01:53:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:31 - INFO - train.train_snli_ve - loss is tensor(0.8654, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6446/6700 [2:59:24<06:47,  1.61s/it]11/17/2022 01:53:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.5728e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:33 - INFO - train.train_snli_ve - loss is tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6447/6700 [2:59:26<06:44,  1.60s/it]11/17/2022 01:53:35 - INFO - train.train_snli_ve - kd_loss is tensor(4.7992e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:35 - INFO - train.train_snli_ve - loss is tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6448/6700 [2:59:27<06:43,  1.60s/it]11/17/2022 01:53:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.6942e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:36 - INFO - train.train_snli_ve - loss is tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6449/6700 [2:59:29<06:45,  1.61s/it]11/17/2022 01:53:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.1654e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:38 - INFO - train.train_snli_ve - loss is tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6450/6700 [2:59:31<06:46,  1.63s/it]11/17/2022 01:53:40 - INFO - train.train_snli_ve - kd_loss is tensor(2.4164e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:40 - INFO - train.train_snli_ve - loss is tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6451/6700 [2:59:32<06:45,  1.63s/it]11/17/2022 01:53:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5485e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:41 - INFO - train.train_snli_ve - loss is tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6452/6700 [2:59:34<06:44,  1.63s/it]11/17/2022 01:53:43 - INFO - train.train_snli_ve - kd_loss is tensor(3.6226e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:43 - INFO - train.train_snli_ve - loss is tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6453/6700 [2:59:36<06:41,  1.63s/it]11/17/2022 01:53:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.7194e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:44 - INFO - train.train_snli_ve - loss is tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6454/6700 [2:59:37<06:38,  1.62s/it]11/17/2022 01:53:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.6694e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:46 - INFO - train.train_snli_ve - loss is tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6455/6700 [2:59:39<06:35,  1.61s/it]11/17/2022 01:53:48 - INFO - train.train_snli_ve - kd_loss is tensor(2.9695e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:48 - INFO - train.train_snli_ve - loss is tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6456/6700 [2:59:40<06:33,  1.61s/it]11/17/2022 01:53:49 - INFO - train.train_snli_ve - kd_loss is tensor(1.6346e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:49 - INFO - train.train_snli_ve - loss is tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6457/6700 [2:59:42<06:35,  1.63s/it]11/17/2022 01:53:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.0069e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:51 - INFO - train.train_snli_ve - loss is tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6458/6700 [2:59:44<06:36,  1.64s/it]11/17/2022 01:53:53 - INFO - train.train_snli_ve - kd_loss is tensor(2.0570e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:53 - INFO - train.train_snli_ve - loss is tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6459/6700 [2:59:45<06:31,  1.62s/it]11/17/2022 01:53:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.0994e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:54 - INFO - train.train_snli_ve - loss is tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6460/6700 [2:59:47<06:28,  1.62s/it]11/17/2022 01:53:56 - INFO - train.train_snli_ve - kd_loss is tensor(2.1423e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:56 - INFO - train.train_snli_ve - loss is tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6461/6700 [2:59:48<06:26,  1.62s/it]11/17/2022 01:53:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.2210e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:57 - INFO - train.train_snli_ve - loss is tensor(0.8005, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6462/6700 [2:59:50<06:27,  1.63s/it]11/17/2022 01:53:59 - INFO - train.train_snli_ve - kd_loss is tensor(5.8781e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:53:59 - INFO - train.train_snli_ve - loss is tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6463/6700 [2:59:52<06:27,  1.63s/it]11/17/2022 01:54:01 - INFO - train.train_snli_ve - kd_loss is tensor(2.5321e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:01 - INFO - train.train_snli_ve - loss is tensor(1.0199, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6464/6700 [2:59:53<06:22,  1.62s/it]11/17/2022 01:54:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.0396e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:02 - INFO - train.train_snli_ve - loss is tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  96%|#########6| 6465/6700 [2:59:55<06:19,  1.62s/it]11/17/2022 01:54:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.8271e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:04 - INFO - train.train_snli_ve - loss is tensor(0.8783, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6466/6700 [2:59:57<06:18,  1.62s/it]11/17/2022 01:54:06 - INFO - train.train_snli_ve - kd_loss is tensor(3.7844e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:06 - INFO - train.train_snli_ve - loss is tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6467/6700 [2:59:58<06:18,  1.62s/it]11/17/2022 01:54:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.9487e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:07 - INFO - train.train_snli_ve - loss is tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6468/6700 [3:00:00<06:16,  1.62s/it]11/17/2022 01:54:09 - INFO - train.train_snli_ve - kd_loss is tensor(3.4815e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:09 - INFO - train.train_snli_ve - loss is tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6469/6700 [3:00:01<06:13,  1.62s/it]11/17/2022 01:54:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.9983e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:10 - INFO - train.train_snli_ve - loss is tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6470/6700 [3:00:03<06:10,  1.61s/it]11/17/2022 01:54:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.5238e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:12 - INFO - train.train_snli_ve - loss is tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6471/6700 [3:00:05<06:08,  1.61s/it]11/17/2022 01:54:14 - INFO - train.train_snli_ve - kd_loss is tensor(3.0010e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:14 - INFO - train.train_snli_ve - loss is tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6472/6700 [3:00:06<06:05,  1.60s/it]11/17/2022 01:54:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.1832e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:15 - INFO - train.train_snli_ve - loss is tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6473/6700 [3:00:08<06:05,  1.61s/it]11/17/2022 01:54:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.1476e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:17 - INFO - train.train_snli_ve - loss is tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6474/6700 [3:00:10<06:07,  1.63s/it]11/17/2022 01:54:18 - INFO - train.train_snli_ve - kd_loss is tensor(1.9277e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:18 - INFO - train.train_snli_ve - loss is tensor(0.6863, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6475/6700 [3:00:11<06:04,  1.62s/it]11/17/2022 01:54:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.9576e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:20 - INFO - train.train_snli_ve - loss is tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6476/6700 [3:00:13<06:01,  1.61s/it]11/17/2022 01:54:22 - INFO - train.train_snli_ve - kd_loss is tensor(2.2179e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:22 - INFO - train.train_snli_ve - loss is tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6477/6700 [3:00:14<05:59,  1.61s/it]11/17/2022 01:54:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.8857e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:23 - INFO - train.train_snli_ve - loss is tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6478/6700 [3:00:16<06:02,  1.63s/it]11/17/2022 01:54:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.8244e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:25 - INFO - train.train_snli_ve - loss is tensor(0.5774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6479/6700 [3:00:18<05:57,  1.62s/it]11/17/2022 01:54:27 - INFO - train.train_snli_ve - kd_loss is tensor(1.8091e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:27 - INFO - train.train_snli_ve - loss is tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6480/6700 [3:00:19<05:55,  1.62s/it]11/17/2022 01:54:28 - INFO - train.train_snli_ve - kd_loss is tensor(3.7564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:28 - INFO - train.train_snli_ve - loss is tensor(0.8083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6481/6700 [3:00:21<05:55,  1.62s/it]11/17/2022 01:54:30 - INFO - train.train_snli_ve - kd_loss is tensor(2.4908e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:30 - INFO - train.train_snli_ve - loss is tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6482/6700 [3:00:22<05:52,  1.62s/it]11/17/2022 01:54:31 - INFO - train.train_snli_ve - kd_loss is tensor(3.3120e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:31 - INFO - train.train_snli_ve - loss is tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6483/6700 [3:00:24<05:52,  1.63s/it]11/17/2022 01:54:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.0304e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:33 - INFO - train.train_snli_ve - loss is tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6484/6700 [3:00:26<05:49,  1.62s/it]11/17/2022 01:54:35 - INFO - train.train_snli_ve - kd_loss is tensor(1.4806e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:35 - INFO - train.train_snli_ve - loss is tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6485/6700 [3:00:27<05:48,  1.62s/it]11/17/2022 01:54:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3161e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:36 - INFO - train.train_snli_ve - loss is tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6486/6700 [3:00:29<05:48,  1.63s/it]11/17/2022 01:54:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:38 - INFO - train.train_snli_ve - loss is tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6487/6700 [3:00:31<05:44,  1.62s/it]11/17/2022 01:54:39 - INFO - train.train_snli_ve - kd_loss is tensor(1.8436e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:39 - INFO - train.train_snli_ve - loss is tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6488/6700 [3:00:32<05:40,  1.61s/it]11/17/2022 01:54:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.6001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:41 - INFO - train.train_snli_ve - loss is tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6489/6700 [3:00:34<05:38,  1.60s/it]11/17/2022 01:54:43 - INFO - train.train_snli_ve - kd_loss is tensor(2.6023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:43 - INFO - train.train_snli_ve - loss is tensor(0.9485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6490/6700 [3:00:35<05:34,  1.59s/it]11/17/2022 01:54:44 - INFO - train.train_snli_ve - kd_loss is tensor(5.0759e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:44 - INFO - train.train_snli_ve - loss is tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6491/6700 [3:00:37<05:33,  1.60s/it]11/17/2022 01:54:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.1080e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:46 - INFO - train.train_snli_ve - loss is tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6492/6700 [3:00:38<05:31,  1.59s/it]11/17/2022 01:54:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.6820e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:47 - INFO - train.train_snli_ve - loss is tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6493/6700 [3:00:40<05:30,  1.60s/it]11/17/2022 01:54:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.6656e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:49 - INFO - train.train_snli_ve - loss is tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6494/6700 [3:00:42<05:31,  1.61s/it]11/17/2022 01:54:51 - INFO - train.train_snli_ve - kd_loss is tensor(2.2247e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:51 - INFO - train.train_snli_ve - loss is tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6495/6700 [3:00:43<05:29,  1.61s/it]11/17/2022 01:54:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.6935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:52 - INFO - train.train_snli_ve - loss is tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6496/6700 [3:00:45<05:28,  1.61s/it]11/17/2022 01:54:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.4332e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:54 - INFO - train.train_snli_ve - loss is tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6497/6700 [3:00:47<05:27,  1.61s/it]11/17/2022 01:54:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.8638e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:55 - INFO - train.train_snli_ve - loss is tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########6| 6498/6700 [3:00:48<05:24,  1.61s/it]11/17/2022 01:54:57 - INFO - train.train_snli_ve - kd_loss is tensor(3.3248e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:57 - INFO - train.train_snli_ve - loss is tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6499/6700 [3:00:50<05:21,  1.60s/it]11/17/2022 01:54:59 - INFO - train.train_snli_ve - kd_loss is tensor(4.2455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:54:59 - INFO - train.train_snli_ve - loss is tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6500/6700 [3:00:51<05:24,  1.62s/it]11/17/2022 01:55:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.4556e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:00 - INFO - train.train_snli_ve - loss is tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6501/6700 [3:00:53<05:25,  1.63s/it]11/17/2022 01:55:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.5052e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:02 - INFO - train.train_snli_ve - loss is tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6502/6700 [3:00:55<05:21,  1.63s/it]11/17/2022 01:55:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.9912e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:04 - INFO - train.train_snli_ve - loss is tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6503/6700 [3:00:56<05:18,  1.61s/it]11/17/2022 01:55:05 - INFO - train.train_snli_ve - kd_loss is tensor(5.5283e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:05 - INFO - train.train_snli_ve - loss is tensor(0.4687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6504/6700 [3:00:58<05:19,  1.63s/it]11/17/2022 01:55:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.2267e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:07 - INFO - train.train_snli_ve - loss is tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6505/6700 [3:01:00<05:16,  1.62s/it]11/17/2022 01:55:09 - INFO - train.train_snli_ve - kd_loss is tensor(2.0222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:09 - INFO - train.train_snli_ve - loss is tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6506/6700 [3:01:01<05:15,  1.63s/it]11/17/2022 01:55:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.3603e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:10 - INFO - train.train_snli_ve - loss is tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6507/6700 [3:01:03<05:12,  1.62s/it]11/17/2022 01:55:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.1762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:12 - INFO - train.train_snli_ve - loss is tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6508/6700 [3:01:04<05:12,  1.63s/it]11/17/2022 01:55:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.3791e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:13 - INFO - train.train_snli_ve - loss is tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6509/6700 [3:01:06<05:07,  1.61s/it]11/17/2022 01:55:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.1823e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:15 - INFO - train.train_snli_ve - loss is tensor(0.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6510/6700 [3:01:08<05:06,  1.61s/it]11/17/2022 01:55:17 - INFO - train.train_snli_ve - kd_loss is tensor(1.8479e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:17 - INFO - train.train_snli_ve - loss is tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6511/6700 [3:01:09<05:04,  1.61s/it]11/17/2022 01:55:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2378e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:18 - INFO - train.train_snli_ve - loss is tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6512/6700 [3:01:11<05:04,  1.62s/it]11/17/2022 01:55:20 - INFO - train.train_snli_ve - kd_loss is tensor(1.8451e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:20 - INFO - train.train_snli_ve - loss is tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6513/6700 [3:01:12<05:02,  1.62s/it]11/17/2022 01:55:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.1701e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:21 - INFO - train.train_snli_ve - loss is tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6514/6700 [3:01:14<05:01,  1.62s/it]11/17/2022 01:55:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.4533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:23 - INFO - train.train_snli_ve - loss is tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6515/6700 [3:01:16<04:58,  1.61s/it]11/17/2022 01:55:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.5656e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:25 - INFO - train.train_snli_ve - loss is tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6516/6700 [3:01:17<04:56,  1.61s/it]11/17/2022 01:55:26 - INFO - train.train_snli_ve - kd_loss is tensor(1.2775e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:26 - INFO - train.train_snli_ve - loss is tensor(0.5754, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6517/6700 [3:01:19<04:54,  1.61s/it]11/17/2022 01:55:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.6533e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:28 - INFO - train.train_snli_ve - loss is tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6518/6700 [3:01:21<04:54,  1.62s/it]11/17/2022 01:55:30 - INFO - train.train_snli_ve - kd_loss is tensor(4.1048e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:30 - INFO - train.train_snli_ve - loss is tensor(0.7255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6519/6700 [3:01:22<04:55,  1.63s/it]11/17/2022 01:55:31 - INFO - train.train_snli_ve - kd_loss is tensor(1.6157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:31 - INFO - train.train_snli_ve - loss is tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6520/6700 [3:01:24<04:51,  1.62s/it]11/17/2022 01:55:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.9760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:33 - INFO - train.train_snli_ve - loss is tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6521/6700 [3:01:25<04:49,  1.61s/it]11/17/2022 01:55:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.6289e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:34 - INFO - train.train_snli_ve - loss is tensor(0.5359, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6522/6700 [3:01:27<04:50,  1.63s/it]11/17/2022 01:55:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.8493e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:36 - INFO - train.train_snli_ve - loss is tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6523/6700 [3:01:29<04:45,  1.61s/it]11/17/2022 01:55:38 - INFO - train.train_snli_ve - kd_loss is tensor(2.9962e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:38 - INFO - train.train_snli_ve - loss is tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6524/6700 [3:01:30<04:44,  1.62s/it]11/17/2022 01:55:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.4043e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:39 - INFO - train.train_snli_ve - loss is tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6525/6700 [3:01:32<04:43,  1.62s/it]11/17/2022 01:55:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.3948e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:41 - INFO - train.train_snli_ve - loss is tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6526/6700 [3:01:34<04:42,  1.62s/it]11/17/2022 01:55:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.0869e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:42 - INFO - train.train_snli_ve - loss is tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6527/6700 [3:01:35<04:41,  1.63s/it]11/17/2022 01:55:44 - INFO - train.train_snli_ve - kd_loss is tensor(1.8705e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:44 - INFO - train.train_snli_ve - loss is tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6528/6700 [3:01:37<04:39,  1.62s/it]11/17/2022 01:55:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.7498e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:46 - INFO - train.train_snli_ve - loss is tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6529/6700 [3:01:38<04:36,  1.62s/it]11/17/2022 01:55:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.3723e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:47 - INFO - train.train_snli_ve - loss is tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6530/6700 [3:01:40<04:34,  1.62s/it]11/17/2022 01:55:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.5848e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:49 - INFO - train.train_snli_ve - loss is tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6531/6700 [3:01:42<04:32,  1.61s/it]11/17/2022 01:55:51 - INFO - train.train_snli_ve - kd_loss is tensor(3.0535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:51 - INFO - train.train_snli_ve - loss is tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  97%|#########7| 6532/6700 [3:01:43<04:31,  1.61s/it]11/17/2022 01:55:52 - INFO - train.train_snli_ve - kd_loss is tensor(1.5852e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:52 - INFO - train.train_snli_ve - loss is tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6533/6700 [3:01:45<04:29,  1.61s/it]11/17/2022 01:55:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.4564e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:54 - INFO - train.train_snli_ve - loss is tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6534/6700 [3:01:46<04:27,  1.61s/it]11/17/2022 01:55:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.3143e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:55 - INFO - train.train_snli_ve - loss is tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6535/6700 [3:01:48<04:25,  1.61s/it]11/17/2022 01:55:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.5316e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:57 - INFO - train.train_snli_ve - loss is tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6536/6700 [3:01:50<04:24,  1.61s/it]11/17/2022 01:55:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.1920e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:55:59 - INFO - train.train_snli_ve - loss is tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6537/6700 [3:01:51<04:23,  1.62s/it]11/17/2022 01:56:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.0946e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:00 - INFO - train.train_snli_ve - loss is tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6538/6700 [3:01:53<04:22,  1.62s/it]11/17/2022 01:56:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.5586e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:02 - INFO - train.train_snli_ve - loss is tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6539/6700 [3:01:55<04:19,  1.61s/it]11/17/2022 01:56:03 - INFO - train.train_snli_ve - kd_loss is tensor(2.2001e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:03 - INFO - train.train_snli_ve - loss is tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6540/6700 [3:01:56<04:18,  1.62s/it]11/17/2022 01:56:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.8403e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:05 - INFO - train.train_snli_ve - loss is tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6541/6700 [3:01:58<04:16,  1.61s/it]11/17/2022 01:56:07 - INFO - train.train_snli_ve - kd_loss is tensor(3.0049e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:07 - INFO - train.train_snli_ve - loss is tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6542/6700 [3:01:59<04:14,  1.61s/it]11/17/2022 01:56:08 - INFO - train.train_snli_ve - kd_loss is tensor(5.2050e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:08 - INFO - train.train_snli_ve - loss is tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6543/6700 [3:02:01<04:14,  1.62s/it]11/17/2022 01:56:10 - INFO - train.train_snli_ve - kd_loss is tensor(3.1973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:10 - INFO - train.train_snli_ve - loss is tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6544/6700 [3:02:03<04:14,  1.63s/it]11/17/2022 01:56:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.9535e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:12 - INFO - train.train_snli_ve - loss is tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6545/6700 [3:02:04<04:12,  1.63s/it]11/17/2022 01:56:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.5653e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:13 - INFO - train.train_snli_ve - loss is tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6546/6700 [3:02:06<04:09,  1.62s/it]11/17/2022 01:56:15 - INFO - train.train_snli_ve - kd_loss is tensor(1.8588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:15 - INFO - train.train_snli_ve - loss is tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6547/6700 [3:02:07<04:07,  1.62s/it]11/17/2022 01:56:16 - INFO - train.train_snli_ve - kd_loss is tensor(2.1401e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:16 - INFO - train.train_snli_ve - loss is tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6548/6700 [3:02:09<04:05,  1.61s/it]11/17/2022 01:56:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.3965e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:18 - INFO - train.train_snli_ve - loss is tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6549/6700 [3:02:11<04:04,  1.62s/it]11/17/2022 01:56:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.1476e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:20 - INFO - train.train_snli_ve - loss is tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6550/6700 [3:02:12<04:02,  1.62s/it]11/17/2022 01:56:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.1776e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:21 - INFO - train.train_snli_ve - loss is tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6551/6700 [3:02:14<03:59,  1.61s/it]11/17/2022 01:56:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.8319e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:23 - INFO - train.train_snli_ve - loss is tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6552/6700 [3:02:16<04:01,  1.63s/it]11/17/2022 01:56:25 - INFO - train.train_snli_ve - kd_loss is tensor(2.9019e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:25 - INFO - train.train_snli_ve - loss is tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6553/6700 [3:02:17<04:00,  1.63s/it]11/17/2022 01:56:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.6221e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:26 - INFO - train.train_snli_ve - loss is tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6554/6700 [3:02:19<03:58,  1.63s/it]11/17/2022 01:56:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.9762e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:28 - INFO - train.train_snli_ve - loss is tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6555/6700 [3:02:20<03:55,  1.62s/it]11/17/2022 01:56:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.0060e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:29 - INFO - train.train_snli_ve - loss is tensor(0.9255, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6556/6700 [3:02:22<03:53,  1.62s/it]11/17/2022 01:56:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.1724e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:31 - INFO - train.train_snli_ve - loss is tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6557/6700 [3:02:24<03:50,  1.61s/it]11/17/2022 01:56:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.7057e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:33 - INFO - train.train_snli_ve - loss is tensor(0.7759, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6558/6700 [3:02:25<03:50,  1.62s/it]11/17/2022 01:56:34 - INFO - train.train_snli_ve - kd_loss is tensor(2.0106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:34 - INFO - train.train_snli_ve - loss is tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6559/6700 [3:02:27<03:49,  1.62s/it]11/17/2022 01:56:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.8898e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:36 - INFO - train.train_snli_ve - loss is tensor(0.5469, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6560/6700 [3:02:29<03:46,  1.62s/it]11/17/2022 01:56:38 - INFO - train.train_snli_ve - kd_loss is tensor(1.5158e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:38 - INFO - train.train_snli_ve - loss is tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6561/6700 [3:02:30<03:45,  1.62s/it]11/17/2022 01:56:39 - INFO - train.train_snli_ve - kd_loss is tensor(3.2665e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:39 - INFO - train.train_snli_ve - loss is tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6562/6700 [3:02:32<03:44,  1.63s/it]11/17/2022 01:56:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.2988e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:41 - INFO - train.train_snli_ve - loss is tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6563/6700 [3:02:33<03:42,  1.62s/it]11/17/2022 01:56:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.8055e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:42 - INFO - train.train_snli_ve - loss is tensor(0.9794, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6564/6700 [3:02:35<03:41,  1.63s/it]11/17/2022 01:56:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.4591e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:44 - INFO - train.train_snli_ve - loss is tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########7| 6565/6700 [3:02:37<03:38,  1.62s/it]11/17/2022 01:56:46 - INFO - train.train_snli_ve - kd_loss is tensor(1.2613e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:46 - INFO - train.train_snli_ve - loss is tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6566/6700 [3:02:38<03:37,  1.62s/it]11/17/2022 01:56:47 - INFO - train.train_snli_ve - kd_loss is tensor(2.5009e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:47 - INFO - train.train_snli_ve - loss is tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6567/6700 [3:02:40<03:36,  1.63s/it]11/17/2022 01:56:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.3781e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:49 - INFO - train.train_snli_ve - loss is tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6568/6700 [3:02:42<03:33,  1.62s/it]11/17/2022 01:56:50 - INFO - train.train_snli_ve - kd_loss is tensor(3.4826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:50 - INFO - train.train_snli_ve - loss is tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6569/6700 [3:02:43<03:31,  1.61s/it]11/17/2022 01:56:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.2198e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:52 - INFO - train.train_snli_ve - loss is tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6570/6700 [3:02:45<03:32,  1.64s/it]11/17/2022 01:56:54 - INFO - train.train_snli_ve - kd_loss is tensor(3.3388e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:54 - INFO - train.train_snli_ve - loss is tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6571/6700 [3:02:47<03:31,  1.64s/it]11/17/2022 01:56:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.0903e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:55 - INFO - train.train_snli_ve - loss is tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6572/6700 [3:02:48<03:28,  1.63s/it]11/17/2022 01:56:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.4166e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:57 - INFO - train.train_snli_ve - loss is tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6573/6700 [3:02:50<03:27,  1.63s/it]11/17/2022 01:56:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.9982e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:56:59 - INFO - train.train_snli_ve - loss is tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6574/6700 [3:02:51<03:27,  1.64s/it]11/17/2022 01:57:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.6035e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:00 - INFO - train.train_snli_ve - loss is tensor(0.9305, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6575/6700 [3:02:53<03:25,  1.64s/it]11/17/2022 01:57:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4966e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:02 - INFO - train.train_snli_ve - loss is tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6576/6700 [3:02:55<03:22,  1.63s/it]11/17/2022 01:57:04 - INFO - train.train_snli_ve - kd_loss is tensor(3.9421e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:04 - INFO - train.train_snli_ve - loss is tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6577/6700 [3:02:56<03:20,  1.63s/it]11/17/2022 01:57:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.6114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:05 - INFO - train.train_snli_ve - loss is tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6578/6700 [3:02:58<03:19,  1.63s/it]11/17/2022 01:57:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.4146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:07 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6579/6700 [3:03:00<03:17,  1.64s/it]11/17/2022 01:57:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.5181e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:08 - INFO - train.train_snli_ve - loss is tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6580/6700 [3:03:01<03:15,  1.63s/it]11/17/2022 01:57:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.6856e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:10 - INFO - train.train_snli_ve - loss is tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6581/6700 [3:03:03<03:13,  1.62s/it]11/17/2022 01:57:12 - INFO - train.train_snli_ve - kd_loss is tensor(1.8481e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:12 - INFO - train.train_snli_ve - loss is tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6582/6700 [3:03:04<03:12,  1.63s/it]11/17/2022 01:57:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.8789e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:13 - INFO - train.train_snli_ve - loss is tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6583/6700 [3:03:06<03:09,  1.62s/it]11/17/2022 01:57:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.1269e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:15 - INFO - train.train_snli_ve - loss is tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6584/6700 [3:03:08<03:06,  1.61s/it]11/17/2022 01:57:17 - INFO - train.train_snli_ve - kd_loss is tensor(2.0757e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:17 - INFO - train.train_snli_ve - loss is tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6585/6700 [3:03:09<03:04,  1.61s/it]11/17/2022 01:57:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2323e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:18 - INFO - train.train_snli_ve - loss is tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6586/6700 [3:03:11<03:03,  1.61s/it]11/17/2022 01:57:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.4233e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:20 - INFO - train.train_snli_ve - loss is tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6587/6700 [3:03:12<03:01,  1.61s/it]11/17/2022 01:57:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0190e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:21 - INFO - train.train_snli_ve - loss is tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6588/6700 [3:03:14<03:00,  1.61s/it]11/17/2022 01:57:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.9445e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:23 - INFO - train.train_snli_ve - loss is tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6589/6700 [3:03:16<02:58,  1.61s/it]11/17/2022 01:57:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.2134e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:25 - INFO - train.train_snli_ve - loss is tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6590/6700 [3:03:17<02:56,  1.61s/it]11/17/2022 01:57:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.1108e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:26 - INFO - train.train_snli_ve - loss is tensor(1.0246, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6591/6700 [3:03:19<02:55,  1.61s/it]11/17/2022 01:57:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.1327e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:28 - INFO - train.train_snli_ve - loss is tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6592/6700 [3:03:20<02:53,  1.61s/it]11/17/2022 01:57:29 - INFO - train.train_snli_ve - kd_loss is tensor(2.3217e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:29 - INFO - train.train_snli_ve - loss is tensor(0.7086, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6593/6700 [3:03:22<02:53,  1.62s/it]11/17/2022 01:57:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.2792e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:31 - INFO - train.train_snli_ve - loss is tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6594/6700 [3:03:24<02:51,  1.62s/it]11/17/2022 01:57:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.9620e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:33 - INFO - train.train_snli_ve - loss is tensor(0.5410, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6595/6700 [3:03:25<02:49,  1.61s/it]11/17/2022 01:57:34 - INFO - train.train_snli_ve - kd_loss is tensor(1.8114e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:34 - INFO - train.train_snli_ve - loss is tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6596/6700 [3:03:27<02:48,  1.62s/it]11/17/2022 01:57:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.7443e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:36 - INFO - train.train_snli_ve - loss is tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6597/6700 [3:03:29<02:46,  1.61s/it]11/17/2022 01:57:37 - INFO - train.train_snli_ve - kd_loss is tensor(2.6375e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:37 - INFO - train.train_snli_ve - loss is tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6598/6700 [3:03:30<02:44,  1.61s/it]11/17/2022 01:57:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.1437e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:39 - INFO - train.train_snli_ve - loss is tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  98%|#########8| 6599/6700 [3:03:32<02:42,  1.61s/it]11/17/2022 01:57:41 - INFO - train.train_snli_ve - kd_loss is tensor(3.3033e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:41 - INFO - train.train_snli_ve - loss is tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6600/6700 [3:03:33<02:41,  1.61s/it]11/17/2022 01:57:42 - INFO - train.train_snli_ve - kd_loss is tensor(2.6241e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:42 - INFO - train.train_snli_ve - loss is tensor(0.7712, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6601/6700 [3:03:35<02:40,  1.62s/it]11/17/2022 01:57:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.1367e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:44 - INFO - train.train_snli_ve - loss is tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6602/6700 [3:03:37<02:37,  1.61s/it]11/17/2022 01:57:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.6888e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:46 - INFO - train.train_snli_ve - loss is tensor(0.8042, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6603/6700 [3:03:38<02:36,  1.61s/it]11/17/2022 01:57:47 - INFO - train.train_snli_ve - kd_loss is tensor(1.8605e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:47 - INFO - train.train_snli_ve - loss is tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6604/6700 [3:03:40<02:37,  1.64s/it]11/17/2022 01:57:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.9469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:49 - INFO - train.train_snli_ve - loss is tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6605/6700 [3:03:42<02:35,  1.63s/it]11/17/2022 01:57:51 - INFO - train.train_snli_ve - kd_loss is tensor(4.1640e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:51 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6606/6700 [3:03:43<02:33,  1.64s/it]11/17/2022 01:57:52 - INFO - train.train_snli_ve - kd_loss is tensor(2.1655e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:52 - INFO - train.train_snli_ve - loss is tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6607/6700 [3:03:45<02:32,  1.64s/it]11/17/2022 01:57:54 - INFO - train.train_snli_ve - kd_loss is tensor(1.7874e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:54 - INFO - train.train_snli_ve - loss is tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6608/6700 [3:03:47<02:31,  1.64s/it]11/17/2022 01:57:55 - INFO - train.train_snli_ve - kd_loss is tensor(2.8905e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:55 - INFO - train.train_snli_ve - loss is tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6609/6700 [3:03:48<02:29,  1.64s/it]11/17/2022 01:57:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.6826e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:57 - INFO - train.train_snli_ve - loss is tensor(0.5656, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6610/6700 [3:03:50<02:27,  1.64s/it]11/17/2022 01:57:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.4973e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:57:59 - INFO - train.train_snli_ve - loss is tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6611/6700 [3:03:51<02:24,  1.63s/it]11/17/2022 01:58:00 - INFO - train.train_snli_ve - kd_loss is tensor(3.1016e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:00 - INFO - train.train_snli_ve - loss is tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6612/6700 [3:03:53<02:23,  1.63s/it]11/17/2022 01:58:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.9383e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:02 - INFO - train.train_snli_ve - loss is tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6613/6700 [3:03:55<02:21,  1.63s/it]11/17/2022 01:58:04 - INFO - train.train_snli_ve - kd_loss is tensor(2.3770e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:04 - INFO - train.train_snli_ve - loss is tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6614/6700 [3:03:56<02:20,  1.63s/it]11/17/2022 01:58:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.3482e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:05 - INFO - train.train_snli_ve - loss is tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6615/6700 [3:03:58<02:17,  1.62s/it]11/17/2022 01:58:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.6371e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:07 - INFO - train.train_snli_ve - loss is tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6616/6700 [3:03:59<02:15,  1.61s/it]11/17/2022 01:58:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.3047e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:08 - INFO - train.train_snli_ve - loss is tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6617/6700 [3:04:01<02:14,  1.62s/it]11/17/2022 01:58:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.5538e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:10 - INFO - train.train_snli_ve - loss is tensor(0.9652, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6618/6700 [3:04:03<02:12,  1.62s/it]11/17/2022 01:58:12 - INFO - train.train_snli_ve - kd_loss is tensor(4.5568e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:12 - INFO - train.train_snli_ve - loss is tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6619/6700 [3:04:04<02:11,  1.62s/it]11/17/2022 01:58:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0679e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:13 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6620/6700 [3:04:06<02:09,  1.62s/it]11/17/2022 01:58:15 - INFO - train.train_snli_ve - kd_loss is tensor(2.3222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:15 - INFO - train.train_snli_ve - loss is tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6621/6700 [3:04:08<02:07,  1.62s/it]11/17/2022 01:58:17 - INFO - train.train_snli_ve - kd_loss is tensor(4.5129e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:17 - INFO - train.train_snli_ve - loss is tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6622/6700 [3:04:09<02:07,  1.63s/it]11/17/2022 01:58:18 - INFO - train.train_snli_ve - kd_loss is tensor(3.4454e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:18 - INFO - train.train_snli_ve - loss is tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6623/6700 [3:04:11<02:05,  1.63s/it]11/17/2022 01:58:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7005e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:20 - INFO - train.train_snli_ve - loss is tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6624/6700 [3:04:12<02:02,  1.61s/it]11/17/2022 01:58:21 - INFO - train.train_snli_ve - kd_loss is tensor(4.0167e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:21 - INFO - train.train_snli_ve - loss is tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6625/6700 [3:04:14<02:01,  1.62s/it]11/17/2022 01:58:23 - INFO - train.train_snli_ve - kd_loss is tensor(2.9343e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:23 - INFO - train.train_snli_ve - loss is tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6626/6700 [3:04:16<01:59,  1.62s/it]11/17/2022 01:58:25 - INFO - train.train_snli_ve - kd_loss is tensor(1.2298e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:25 - INFO - train.train_snli_ve - loss is tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6627/6700 [3:04:17<01:58,  1.62s/it]11/17/2022 01:58:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.6935e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:26 - INFO - train.train_snli_ve - loss is tensor(0.5651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6628/6700 [3:04:19<01:56,  1.62s/it]11/17/2022 01:58:28 - INFO - train.train_snli_ve - kd_loss is tensor(1.9676e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:28 - INFO - train.train_snli_ve - loss is tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6629/6700 [3:04:21<01:54,  1.61s/it]11/17/2022 01:58:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.4924e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:29 - INFO - train.train_snli_ve - loss is tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6630/6700 [3:04:22<01:53,  1.62s/it]11/17/2022 01:58:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.9068e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:31 - INFO - train.train_snli_ve - loss is tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6631/6700 [3:04:24<01:51,  1.61s/it]11/17/2022 01:58:33 - INFO - train.train_snli_ve - kd_loss is tensor(2.2473e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:33 - INFO - train.train_snli_ve - loss is tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########8| 6632/6700 [3:04:25<01:49,  1.61s/it]11/17/2022 01:58:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.9118e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:34 - INFO - train.train_snli_ve - loss is tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6633/6700 [3:04:27<01:47,  1.61s/it]11/17/2022 01:58:36 - INFO - train.train_snli_ve - kd_loss is tensor(1.6222e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:36 - INFO - train.train_snli_ve - loss is tensor(1.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6634/6700 [3:04:29<01:46,  1.61s/it]11/17/2022 01:58:38 - INFO - train.train_snli_ve - kd_loss is tensor(3.1438e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:38 - INFO - train.train_snli_ve - loss is tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6635/6700 [3:04:30<01:44,  1.62s/it]11/17/2022 01:58:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.0360e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:39 - INFO - train.train_snli_ve - loss is tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6636/6700 [3:04:32<01:44,  1.63s/it]11/17/2022 01:58:41 - INFO - train.train_snli_ve - kd_loss is tensor(2.5456e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:41 - INFO - train.train_snli_ve - loss is tensor(0.4693, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6637/6700 [3:04:33<01:42,  1.62s/it]11/17/2022 01:58:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.7374e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:42 - INFO - train.train_snli_ve - loss is tensor(0.8508, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6638/6700 [3:04:35<01:39,  1.61s/it]11/17/2022 01:58:44 - INFO - train.train_snli_ve - kd_loss is tensor(2.8157e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:44 - INFO - train.train_snli_ve - loss is tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6639/6700 [3:04:37<01:38,  1.61s/it]11/17/2022 01:58:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.5861e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:46 - INFO - train.train_snli_ve - loss is tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6640/6700 [3:04:38<01:36,  1.61s/it]11/17/2022 01:58:47 - INFO - train.train_snli_ve - kd_loss is tensor(3.1510e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:47 - INFO - train.train_snli_ve - loss is tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6641/6700 [3:04:40<01:35,  1.61s/it]11/17/2022 01:58:49 - INFO - train.train_snli_ve - kd_loss is tensor(3.6488e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:49 - INFO - train.train_snli_ve - loss is tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6642/6700 [3:04:42<01:33,  1.62s/it]11/17/2022 01:58:50 - INFO - train.train_snli_ve - kd_loss is tensor(2.9048e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:50 - INFO - train.train_snli_ve - loss is tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6643/6700 [3:04:43<01:32,  1.62s/it]11/17/2022 01:58:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.1799e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:52 - INFO - train.train_snli_ve - loss is tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6644/6700 [3:04:45<01:30,  1.62s/it]11/17/2022 01:58:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.8263e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:54 - INFO - train.train_snli_ve - loss is tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6645/6700 [3:04:46<01:29,  1.63s/it]11/17/2022 01:58:55 - INFO - train.train_snli_ve - kd_loss is tensor(1.9499e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:55 - INFO - train.train_snli_ve - loss is tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6646/6700 [3:04:48<01:28,  1.64s/it]11/17/2022 01:58:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.4721e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:57 - INFO - train.train_snli_ve - loss is tensor(0.4765, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6647/6700 [3:04:50<01:26,  1.64s/it]11/17/2022 01:58:59 - INFO - train.train_snli_ve - kd_loss is tensor(1.8673e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:58:59 - INFO - train.train_snli_ve - loss is tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6648/6700 [3:04:51<01:24,  1.63s/it]11/17/2022 01:59:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2926e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:00 - INFO - train.train_snli_ve - loss is tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6649/6700 [3:04:53<01:22,  1.62s/it]11/17/2022 01:59:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.4212e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:02 - INFO - train.train_snli_ve - loss is tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6650/6700 [3:04:55<01:21,  1.64s/it]11/17/2022 01:59:04 - INFO - train.train_snli_ve - kd_loss is tensor(1.9602e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:04 - INFO - train.train_snli_ve - loss is tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6651/6700 [3:04:56<01:19,  1.63s/it]11/17/2022 01:59:05 - INFO - train.train_snli_ve - kd_loss is tensor(2.1531e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:05 - INFO - train.train_snli_ve - loss is tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6652/6700 [3:04:58<01:18,  1.63s/it]11/17/2022 01:59:07 - INFO - train.train_snli_ve - kd_loss is tensor(2.5765e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:07 - INFO - train.train_snli_ve - loss is tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6653/6700 [3:04:59<01:16,  1.63s/it]11/17/2022 01:59:08 - INFO - train.train_snli_ve - kd_loss is tensor(2.7223e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:08 - INFO - train.train_snli_ve - loss is tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6654/6700 [3:05:01<01:14,  1.62s/it]11/17/2022 01:59:10 - INFO - train.train_snli_ve - kd_loss is tensor(1.8871e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:10 - INFO - train.train_snli_ve - loss is tensor(0.8127, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6655/6700 [3:05:03<01:12,  1.61s/it]11/17/2022 01:59:12 - INFO - train.train_snli_ve - kd_loss is tensor(2.0943e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:12 - INFO - train.train_snli_ve - loss is tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6656/6700 [3:05:04<01:11,  1.62s/it]11/17/2022 01:59:13 - INFO - train.train_snli_ve - kd_loss is tensor(3.6563e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:13 - INFO - train.train_snli_ve - loss is tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6657/6700 [3:05:06<01:09,  1.62s/it]11/17/2022 01:59:15 - INFO - train.train_snli_ve - kd_loss is tensor(4.0149e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:15 - INFO - train.train_snli_ve - loss is tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6658/6700 [3:05:08<01:07,  1.61s/it]11/17/2022 01:59:16 - INFO - train.train_snli_ve - kd_loss is tensor(5.5637e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:16 - INFO - train.train_snli_ve - loss is tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6659/6700 [3:05:09<01:06,  1.61s/it]11/17/2022 01:59:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2089e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:18 - INFO - train.train_snli_ve - loss is tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6660/6700 [3:05:11<01:04,  1.61s/it]11/17/2022 01:59:20 - INFO - train.train_snli_ve - kd_loss is tensor(3.2023e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:20 - INFO - train.train_snli_ve - loss is tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6661/6700 [3:05:12<01:02,  1.61s/it]11/17/2022 01:59:21 - INFO - train.train_snli_ve - kd_loss is tensor(3.6302e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:21 - INFO - train.train_snli_ve - loss is tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6662/6700 [3:05:14<01:01,  1.62s/it]11/17/2022 01:59:23 - INFO - train.train_snli_ve - kd_loss is tensor(3.6554e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:23 - INFO - train.train_snli_ve - loss is tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6663/6700 [3:05:16<00:59,  1.61s/it]11/17/2022 01:59:25 - INFO - train.train_snli_ve - kd_loss is tensor(3.0455e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:25 - INFO - train.train_snli_ve - loss is tensor(0.5934, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6664/6700 [3:05:17<00:58,  1.61s/it]11/17/2022 01:59:26 - INFO - train.train_snli_ve - kd_loss is tensor(2.4786e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:26 - INFO - train.train_snli_ve - loss is tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6665/6700 [3:05:19<00:55,  1.60s/it]11/17/2022 01:59:28 - INFO - train.train_snli_ve - kd_loss is tensor(2.6195e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:28 - INFO - train.train_snli_ve - loss is tensor(0.6609, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1:  99%|#########9| 6666/6700 [3:05:20<00:54,  1.60s/it]11/17/2022 01:59:29 - INFO - train.train_snli_ve - kd_loss is tensor(3.4409e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:29 - INFO - train.train_snli_ve - loss is tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6667/6700 [3:05:22<00:52,  1.60s/it]11/17/2022 01:59:31 - INFO - train.train_snli_ve - kd_loss is tensor(2.6716e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:31 - INFO - train.train_snli_ve - loss is tensor(0.9053, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6668/6700 [3:05:24<00:51,  1.61s/it]11/17/2022 01:59:33 - INFO - train.train_snli_ve - kd_loss is tensor(1.6733e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:33 - INFO - train.train_snli_ve - loss is tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6669/6700 [3:05:25<00:50,  1.62s/it]11/17/2022 01:59:34 - INFO - train.train_snli_ve - kd_loss is tensor(3.0540e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:34 - INFO - train.train_snli_ve - loss is tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6670/6700 [3:05:27<00:48,  1.62s/it]11/17/2022 01:59:36 - INFO - train.train_snli_ve - kd_loss is tensor(2.3102e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:36 - INFO - train.train_snli_ve - loss is tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6671/6700 [3:05:28<00:46,  1.62s/it]11/17/2022 01:59:37 - INFO - train.train_snli_ve - kd_loss is tensor(1.8582e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:37 - INFO - train.train_snli_ve - loss is tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6672/6700 [3:05:30<00:45,  1.62s/it]11/17/2022 01:59:39 - INFO - train.train_snli_ve - kd_loss is tensor(2.3629e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:39 - INFO - train.train_snli_ve - loss is tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6673/6700 [3:05:32<00:43,  1.62s/it]11/17/2022 01:59:41 - INFO - train.train_snli_ve - kd_loss is tensor(1.9146e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:41 - INFO - train.train_snli_ve - loss is tensor(0.8651, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6674/6700 [3:05:33<00:42,  1.65s/it]11/17/2022 01:59:42 - INFO - train.train_snli_ve - kd_loss is tensor(1.6907e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:42 - INFO - train.train_snli_ve - loss is tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6675/6700 [3:05:35<00:41,  1.64s/it]11/17/2022 01:59:44 - INFO - train.train_snli_ve - kd_loss is tensor(3.4612e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:44 - INFO - train.train_snli_ve - loss is tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6676/6700 [3:05:37<00:39,  1.63s/it]11/17/2022 01:59:46 - INFO - train.train_snli_ve - kd_loss is tensor(2.4955e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:46 - INFO - train.train_snli_ve - loss is tensor(1.1103, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6677/6700 [3:05:38<00:37,  1.62s/it]11/17/2022 01:59:47 - INFO - train.train_snli_ve - kd_loss is tensor(4.8274e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:47 - INFO - train.train_snli_ve - loss is tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6678/6700 [3:05:40<00:35,  1.61s/it]11/17/2022 01:59:49 - INFO - train.train_snli_ve - kd_loss is tensor(2.6342e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:49 - INFO - train.train_snli_ve - loss is tensor(0.5150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6679/6700 [3:05:42<00:34,  1.62s/it]11/17/2022 01:59:50 - INFO - train.train_snli_ve - kd_loss is tensor(1.5921e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:50 - INFO - train.train_snli_ve - loss is tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6680/6700 [3:05:43<00:32,  1.62s/it]11/17/2022 01:59:52 - INFO - train.train_snli_ve - kd_loss is tensor(3.4051e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:52 - INFO - train.train_snli_ve - loss is tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6681/6700 [3:05:45<00:30,  1.62s/it]11/17/2022 01:59:54 - INFO - train.train_snli_ve - kd_loss is tensor(2.5469e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:54 - INFO - train.train_snli_ve - loss is tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6682/6700 [3:05:46<00:28,  1.61s/it]11/17/2022 01:59:55 - INFO - train.train_snli_ve - kd_loss is tensor(3.0867e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:55 - INFO - train.train_snli_ve - loss is tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6683/6700 [3:05:48<00:27,  1.64s/it]11/17/2022 01:59:57 - INFO - train.train_snli_ve - kd_loss is tensor(2.8087e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:57 - INFO - train.train_snli_ve - loss is tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6684/6700 [3:05:50<00:26,  1.64s/it]11/17/2022 01:59:59 - INFO - train.train_snli_ve - kd_loss is tensor(2.5760e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 01:59:59 - INFO - train.train_snli_ve - loss is tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6685/6700 [3:05:51<00:24,  1.63s/it]11/17/2022 02:00:00 - INFO - train.train_snli_ve - kd_loss is tensor(2.2402e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:00 - INFO - train.train_snli_ve - loss is tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6686/6700 [3:05:53<00:22,  1.62s/it]11/17/2022 02:00:02 - INFO - train.train_snli_ve - kd_loss is tensor(2.1398e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:02 - INFO - train.train_snli_ve - loss is tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6687/6700 [3:05:54<00:20,  1.61s/it]11/17/2022 02:00:03 - INFO - train.train_snli_ve - kd_loss is tensor(1.5273e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:03 - INFO - train.train_snli_ve - loss is tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6688/6700 [3:05:56<00:19,  1.61s/it]11/17/2022 02:00:05 - INFO - train.train_snli_ve - kd_loss is tensor(3.3610e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:05 - INFO - train.train_snli_ve - loss is tensor(0.7500, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6689/6700 [3:05:58<00:17,  1.61s/it]11/17/2022 02:00:07 - INFO - train.train_snli_ve - kd_loss is tensor(4.3980e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:07 - INFO - train.train_snli_ve - loss is tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6690/6700 [3:05:59<00:16,  1.61s/it]11/17/2022 02:00:08 - INFO - train.train_snli_ve - kd_loss is tensor(1.6363e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:08 - INFO - train.train_snli_ve - loss is tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6691/6700 [3:06:01<00:14,  1.61s/it]11/17/2022 02:00:10 - INFO - train.train_snli_ve - kd_loss is tensor(2.6390e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:10 - INFO - train.train_snli_ve - loss is tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6692/6700 [3:06:03<00:12,  1.61s/it]11/17/2022 02:00:11 - INFO - train.train_snli_ve - kd_loss is tensor(3.1215e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:11 - INFO - train.train_snli_ve - loss is tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6693/6700 [3:06:04<00:11,  1.62s/it]11/17/2022 02:00:13 - INFO - train.train_snli_ve - kd_loss is tensor(2.0014e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:13 - INFO - train.train_snli_ve - loss is tensor(0.5150, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6694/6700 [3:06:06<00:09,  1.63s/it]11/17/2022 02:00:15 - INFO - train.train_snli_ve - kd_loss is tensor(3.6420e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:15 - INFO - train.train_snli_ve - loss is tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6695/6700 [3:06:07<00:08,  1.62s/it]11/17/2022 02:00:16 - INFO - train.train_snli_ve - kd_loss is tensor(4.0588e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:16 - INFO - train.train_snli_ve - loss is tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6696/6700 [3:06:09<00:06,  1.63s/it]11/17/2022 02:00:18 - INFO - train.train_snli_ve - kd_loss is tensor(2.2139e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:18 - INFO - train.train_snli_ve - loss is tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6697/6700 [3:06:11<00:04,  1.61s/it]11/17/2022 02:00:20 - INFO - train.train_snli_ve - kd_loss is tensor(2.7106e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:20 - INFO - train.train_snli_ve - loss is tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6698/6700 [3:06:12<00:03,  1.60s/it]11/17/2022 02:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(2.0624e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:21 - INFO - train.train_snli_ve - loss is tensor(0.5507, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|#########9| 6699/6700 [3:06:14<00:01,  1.62s/it]11/17/2022 02:00:21 - INFO - train.train_snli_ve - kd_loss is tensor(1.2391e-05, device='cuda:0', grad_fn=<AddBackward0>)
11/17/2022 02:00:21 - INFO - train.train_snli_ve - loss is tensor(1.2828, device='cuda:0', grad_fn=<NllLossBackward0>)
Training epoch 1: 100%|##########| 6700/6700 [3:06:14<00:00,  1.18s/it]Training epoch 1: 100%|##########| 6700/6700 [3:06:14<00:00,  1.67s/it]
Evaluating on SNLI-VE val set:   0%|          | 0/215 [00:00<?, ?it/s]Evaluating on SNLI-VE val set:   0%|          | 1/215 [00:01<04:19,  1.21s/it]Evaluating on SNLI-VE val set:   1%|          | 2/215 [00:01<03:13,  1.10it/s]Evaluating on SNLI-VE val set:   1%|1         | 3/215 [00:02<02:45,  1.28it/s]Evaluating on SNLI-VE val set:   2%|1         | 4/215 [00:03<02:39,  1.32it/s]Evaluating on SNLI-VE val set:   2%|2         | 5/215 [00:03<02:35,  1.35it/s]Evaluating on SNLI-VE val set:   3%|2         | 6/215 [00:04<02:32,  1.37it/s]Evaluating on SNLI-VE val set:   3%|3         | 7/215 [00:05<02:32,  1.36it/s]Evaluating on SNLI-VE val set:   4%|3         | 8/215 [00:06<02:34,  1.34it/s]Evaluating on SNLI-VE val set:   4%|4         | 9/215 [00:06<02:35,  1.33it/s]Evaluating on SNLI-VE val set:   5%|4         | 10/215 [00:07<02:32,  1.34it/s]Evaluating on SNLI-VE val set:   5%|5         | 11/215 [00:08<02:23,  1.42it/s]Evaluating on SNLI-VE val set:   6%|5         | 12/215 [00:09<02:25,  1.40it/s]Evaluating on SNLI-VE val set:   6%|6         | 13/215 [00:09<02:24,  1.39it/s]Evaluating on SNLI-VE val set:   7%|6         | 14/215 [00:10<02:22,  1.41it/s]Evaluating on SNLI-VE val set:   7%|6         | 15/215 [00:11<02:24,  1.39it/s]Evaluating on SNLI-VE val set:   7%|7         | 16/215 [00:11<02:24,  1.37it/s]Evaluating on SNLI-VE val set:   8%|7         | 17/215 [00:12<02:24,  1.37it/s]Evaluating on SNLI-VE val set:   8%|8         | 18/215 [00:13<02:21,  1.39it/s]Evaluating on SNLI-VE val set:   9%|8         | 19/215 [00:14<02:21,  1.39it/s]Evaluating on SNLI-VE val set:   9%|9         | 20/215 [00:14<02:17,  1.42it/s]Evaluating on SNLI-VE val set:  10%|9         | 21/215 [00:15<02:12,  1.47it/s]Evaluating on SNLI-VE val set:  10%|#         | 22/215 [00:16<02:12,  1.46it/s]Evaluating on SNLI-VE val set:  11%|#         | 23/215 [00:16<02:16,  1.41it/s]Evaluating on SNLI-VE val set:  11%|#1        | 24/215 [00:17<02:17,  1.39it/s]Evaluating on SNLI-VE val set:  12%|#1        | 25/215 [00:18<02:14,  1.41it/s]Evaluating on SNLI-VE val set:  12%|#2        | 26/215 [00:19<02:17,  1.38it/s]Evaluating on SNLI-VE val set:  13%|#2        | 27/215 [00:19<02:14,  1.40it/s]Evaluating on SNLI-VE val set:  13%|#3        | 28/215 [00:20<02:10,  1.43it/s]Evaluating on SNLI-VE val set:  13%|#3        | 29/215 [00:21<02:10,  1.43it/s]Evaluating on SNLI-VE val set:  14%|#3        | 30/215 [00:21<02:11,  1.40it/s]Evaluating on SNLI-VE val set:  14%|#4        | 31/215 [00:22<02:14,  1.36it/s]Evaluating on SNLI-VE val set:  15%|#4        | 32/215 [00:23<02:16,  1.34it/s]Evaluating on SNLI-VE val set:  15%|#5        | 33/215 [00:24<02:14,  1.36it/s]Evaluating on SNLI-VE val set:  16%|#5        | 34/215 [00:24<02:13,  1.35it/s]Evaluating on SNLI-VE val set:  16%|#6        | 35/215 [00:25<02:12,  1.36it/s]Evaluating on SNLI-VE val set:  17%|#6        | 36/215 [00:26<02:08,  1.39it/s]Evaluating on SNLI-VE val set:  17%|#7        | 37/215 [00:26<02:07,  1.40it/s]Evaluating on SNLI-VE val set:  18%|#7        | 38/215 [00:27<02:09,  1.36it/s]Evaluating on SNLI-VE val set:  18%|#8        | 39/215 [00:28<02:06,  1.39it/s]Evaluating on SNLI-VE val set:  19%|#8        | 40/215 [00:29<02:07,  1.38it/s]Evaluating on SNLI-VE val set:  19%|#9        | 41/215 [00:29<02:06,  1.38it/s]Evaluating on SNLI-VE val set:  20%|#9        | 42/215 [00:30<02:04,  1.39it/s]Evaluating on SNLI-VE val set:  20%|##        | 43/215 [00:31<01:59,  1.43it/s]Evaluating on SNLI-VE val set:  20%|##        | 44/215 [00:31<01:59,  1.43it/s]Evaluating on SNLI-VE val set:  21%|##        | 45/215 [00:32<02:01,  1.40it/s]Evaluating on SNLI-VE val set:  21%|##1       | 46/215 [00:33<01:59,  1.41it/s]Evaluating on SNLI-VE val set:  22%|##1       | 47/215 [00:34<01:59,  1.41it/s]Evaluating on SNLI-VE val set:  22%|##2       | 48/215 [00:34<01:57,  1.43it/s]Evaluating on SNLI-VE val set:  23%|##2       | 49/215 [00:35<01:53,  1.47it/s]Evaluating on SNLI-VE val set:  23%|##3       | 50/215 [00:36<01:51,  1.48it/s]Evaluating on SNLI-VE val set:  24%|##3       | 51/215 [00:36<01:52,  1.46it/s]Evaluating on SNLI-VE val set:  24%|##4       | 52/215 [00:37<01:55,  1.41it/s]Evaluating on SNLI-VE val set:  25%|##4       | 53/215 [00:38<01:57,  1.38it/s]Evaluating on SNLI-VE val set:  25%|##5       | 54/215 [00:39<01:57,  1.37it/s]Evaluating on SNLI-VE val set:  26%|##5       | 55/215 [00:39<01:54,  1.39it/s]Evaluating on SNLI-VE val set:  26%|##6       | 56/215 [00:40<01:52,  1.41it/s]Evaluating on SNLI-VE val set:  27%|##6       | 57/215 [00:41<01:51,  1.42it/s]Evaluating on SNLI-VE val set:  27%|##6       | 58/215 [00:41<01:53,  1.39it/s]Evaluating on SNLI-VE val set:  27%|##7       | 59/215 [00:42<01:55,  1.35it/s]Evaluating on SNLI-VE val set:  28%|##7       | 60/215 [00:43<01:52,  1.37it/s]Evaluating on SNLI-VE val set:  28%|##8       | 61/215 [00:44<01:52,  1.36it/s]Evaluating on SNLI-VE val set:  29%|##8       | 62/215 [00:44<01:51,  1.37it/s]Evaluating on SNLI-VE val set:  29%|##9       | 63/215 [00:45<01:49,  1.39it/s]Evaluating on SNLI-VE val set:  30%|##9       | 64/215 [00:46<01:46,  1.42it/s]Evaluating on SNLI-VE val set:  30%|###       | 65/215 [00:47<01:50,  1.36it/s]Evaluating on SNLI-VE val set:  31%|###       | 66/215 [00:47<01:46,  1.39it/s]Evaluating on SNLI-VE val set:  31%|###1      | 67/215 [00:48<01:46,  1.39it/s]Evaluating on SNLI-VE val set:  32%|###1      | 68/215 [00:49<01:48,  1.36it/s]Evaluating on SNLI-VE val set:  32%|###2      | 69/215 [00:49<01:43,  1.42it/s]Evaluating on SNLI-VE val set:  33%|###2      | 70/215 [00:50<01:43,  1.40it/s]Evaluating on SNLI-VE val set:  33%|###3      | 71/215 [00:51<01:42,  1.41it/s]Evaluating on SNLI-VE val set:  33%|###3      | 72/215 [00:51<01:41,  1.41it/s]Evaluating on SNLI-VE val set:  34%|###3      | 73/215 [00:52<01:37,  1.46it/s]Evaluating on SNLI-VE val set:  34%|###4      | 74/215 [00:53<01:34,  1.50it/s]Evaluating on SNLI-VE val set:  35%|###4      | 75/215 [00:54<01:37,  1.44it/s]Evaluating on SNLI-VE val set:  35%|###5      | 76/215 [00:54<01:40,  1.39it/s]Evaluating on SNLI-VE val set:  36%|###5      | 77/215 [00:55<01:39,  1.39it/s]Evaluating on SNLI-VE val set:  36%|###6      | 78/215 [00:56<01:39,  1.38it/s]Evaluating on SNLI-VE val set:  37%|###6      | 79/215 [00:57<01:40,  1.35it/s]Evaluating on SNLI-VE val set:  37%|###7      | 80/215 [00:57<01:37,  1.39it/s]Evaluating on SNLI-VE val set:  38%|###7      | 81/215 [00:58<01:34,  1.42it/s]Evaluating on SNLI-VE val set:  38%|###8      | 82/215 [00:59<01:36,  1.39it/s]Evaluating on SNLI-VE val set:  39%|###8      | 83/215 [00:59<01:34,  1.39it/s]Evaluating on SNLI-VE val set:  39%|###9      | 84/215 [01:00<01:34,  1.38it/s]Evaluating on SNLI-VE val set:  40%|###9      | 85/215 [01:01<01:33,  1.39it/s]Evaluating on SNLI-VE val set:  40%|####      | 86/215 [01:01<01:31,  1.40it/s]Evaluating on SNLI-VE val set:  40%|####      | 87/215 [01:02<01:31,  1.40it/s]Evaluating on SNLI-VE val set:  41%|####      | 88/215 [01:03<01:33,  1.36it/s]Evaluating on SNLI-VE val set:  41%|####1     | 89/215 [01:04<01:33,  1.34it/s]Evaluating on SNLI-VE val set:  42%|####1     | 90/215 [01:04<01:27,  1.42it/s]Evaluating on SNLI-VE val set:  42%|####2     | 91/215 [01:05<01:26,  1.43it/s]Evaluating on SNLI-VE val set:  43%|####2     | 92/215 [01:06<01:26,  1.42it/s]Evaluating on SNLI-VE val set:  43%|####3     | 93/215 [01:06<01:27,  1.40it/s]Evaluating on SNLI-VE val set:  44%|####3     | 94/215 [01:07<01:25,  1.42it/s]Evaluating on SNLI-VE val set:  44%|####4     | 95/215 [01:08<01:25,  1.41it/s]Evaluating on SNLI-VE val set:  45%|####4     | 96/215 [01:09<01:24,  1.41it/s]Evaluating on SNLI-VE val set:  45%|####5     | 97/215 [01:09<01:26,  1.36it/s]Evaluating on SNLI-VE val set:  46%|####5     | 98/215 [01:10<01:26,  1.36it/s]Evaluating on SNLI-VE val set:  46%|####6     | 99/215 [01:11<01:23,  1.38it/s]Evaluating on SNLI-VE val set:  47%|####6     | 100/215 [01:12<01:22,  1.39it/s]Evaluating on SNLI-VE val set:  47%|####6     | 101/215 [01:12<01:22,  1.37it/s]Evaluating on SNLI-VE val set:  47%|####7     | 102/215 [01:13<01:22,  1.37it/s]Evaluating on SNLI-VE val set:  48%|####7     | 103/215 [01:14<01:21,  1.37it/s]Evaluating on SNLI-VE val set:  48%|####8     | 104/215 [01:14<01:20,  1.38it/s]Evaluating on SNLI-VE val set:  49%|####8     | 105/215 [01:15<01:20,  1.37it/s]Evaluating on SNLI-VE val set:  49%|####9     | 106/215 [01:16<01:19,  1.37it/s]Evaluating on SNLI-VE val set:  50%|####9     | 107/215 [01:17<01:16,  1.42it/s]Evaluating on SNLI-VE val set:  50%|#####     | 108/215 [01:17<01:16,  1.41it/s]Evaluating on SNLI-VE val set:  51%|#####     | 109/215 [01:18<01:16,  1.39it/s]Evaluating on SNLI-VE val set:  51%|#####1    | 110/215 [01:19<01:15,  1.39it/s]Evaluating on SNLI-VE val set:  52%|#####1    | 111/215 [01:20<01:16,  1.36it/s]Evaluating on SNLI-VE val set:  52%|#####2    | 112/215 [01:20<01:16,  1.34it/s]Evaluating on SNLI-VE val set:  53%|#####2    | 113/215 [01:21<01:15,  1.35it/s]Evaluating on SNLI-VE val set:  53%|#####3    | 114/215 [01:22<01:12,  1.39it/s]Evaluating on SNLI-VE val set:  53%|#####3    | 115/215 [01:22<01:11,  1.40it/s]Evaluating on SNLI-VE val set:  54%|#####3    | 116/215 [01:23<01:11,  1.39it/s]Evaluating on SNLI-VE val set:  54%|#####4    | 117/215 [01:24<01:09,  1.41it/s]Evaluating on SNLI-VE val set:  55%|#####4    | 118/215 [01:24<01:07,  1.44it/s]Evaluating on SNLI-VE val set:  55%|#####5    | 119/215 [01:25<01:07,  1.42it/s]Evaluating on SNLI-VE val set:  56%|#####5    | 120/215 [01:26<01:08,  1.39it/s]Evaluating on SNLI-VE val set:  56%|#####6    | 121/215 [01:27<01:08,  1.38it/s]Evaluating on SNLI-VE val set:  57%|#####6    | 122/215 [01:27<01:07,  1.37it/s]Evaluating on SNLI-VE val set:  57%|#####7    | 123/215 [01:28<01:05,  1.40it/s]Evaluating on SNLI-VE val set:  58%|#####7    | 124/215 [01:29<01:04,  1.40it/s]Evaluating on SNLI-VE val set:  58%|#####8    | 125/215 [01:30<01:03,  1.41it/s]Evaluating on SNLI-VE val set:  59%|#####8    | 126/215 [01:30<01:01,  1.44it/s]Evaluating on SNLI-VE val set:  59%|#####9    | 127/215 [01:31<01:01,  1.44it/s]Evaluating on SNLI-VE val set:  60%|#####9    | 128/215 [01:32<01:00,  1.44it/s]Evaluating on SNLI-VE val set:  60%|######    | 129/215 [01:32<01:01,  1.41it/s]Evaluating on SNLI-VE val set:  60%|######    | 130/215 [01:33<01:01,  1.38it/s]Evaluating on SNLI-VE val set:  61%|######    | 131/215 [01:34<01:00,  1.38it/s]Evaluating on SNLI-VE val set:  61%|######1   | 132/215 [01:35<01:00,  1.37it/s]Evaluating on SNLI-VE val set:  62%|######1   | 133/215 [01:35<01:00,  1.36it/s]Evaluating on SNLI-VE val set:  62%|######2   | 134/215 [01:36<00:59,  1.36it/s]Evaluating on SNLI-VE val set:  63%|######2   | 135/215 [01:37<00:58,  1.36it/s]Evaluating on SNLI-VE val set:  63%|######3   | 136/215 [01:38<00:59,  1.34it/s]Evaluating on SNLI-VE val set:  64%|######3   | 137/215 [01:38<00:57,  1.36it/s]Evaluating on SNLI-VE val set:  64%|######4   | 138/215 [01:39<00:55,  1.39it/s]Evaluating on SNLI-VE val set:  65%|######4   | 139/215 [01:40<00:52,  1.45it/s]Evaluating on SNLI-VE val set:  65%|######5   | 140/215 [01:40<00:52,  1.44it/s]Evaluating on SNLI-VE val set:  66%|######5   | 141/215 [01:41<00:50,  1.45it/s]Evaluating on SNLI-VE val set:  66%|######6   | 142/215 [01:42<00:51,  1.42it/s]Evaluating on SNLI-VE val set:  67%|######6   | 143/215 [01:42<00:50,  1.43it/s]Evaluating on SNLI-VE val set:  67%|######6   | 144/215 [01:43<00:50,  1.40it/s]Evaluating on SNLI-VE val set:  67%|######7   | 145/215 [01:44<00:49,  1.41it/s]Evaluating on SNLI-VE val set:  68%|######7   | 146/215 [01:45<00:50,  1.37it/s]Evaluating on SNLI-VE val set:  68%|######8   | 147/215 [01:45<00:49,  1.36it/s]Evaluating on SNLI-VE val set:  69%|######8   | 148/215 [01:46<00:48,  1.39it/s]Evaluating on SNLI-VE val set:  69%|######9   | 149/215 [01:47<00:47,  1.40it/s]Evaluating on SNLI-VE val set:  70%|######9   | 150/215 [01:47<00:47,  1.37it/s]Evaluating on SNLI-VE val set:  70%|#######   | 151/215 [01:48<00:45,  1.40it/s]Evaluating on SNLI-VE val set:  71%|#######   | 152/215 [01:49<00:45,  1.39it/s]Evaluating on SNLI-VE val set:  71%|#######1  | 153/215 [01:50<00:44,  1.39it/s]Evaluating on SNLI-VE val set:  72%|#######1  | 154/215 [01:50<00:42,  1.43it/s]Evaluating on SNLI-VE val set:  72%|#######2  | 155/215 [01:51<00:41,  1.45it/s]Evaluating on SNLI-VE val set:  73%|#######2  | 156/215 [01:52<00:40,  1.44it/s]Evaluating on SNLI-VE val set:  73%|#######3  | 157/215 [01:52<00:40,  1.44it/s]Evaluating on SNLI-VE val set:  73%|#######3  | 158/215 [01:53<00:40,  1.41it/s]Evaluating on SNLI-VE val set:  74%|#######3  | 159/215 [01:54<00:40,  1.38it/s]Evaluating on SNLI-VE val set:  74%|#######4  | 160/215 [01:55<00:40,  1.36it/s]Evaluating on SNLI-VE val set:  75%|#######4  | 161/215 [01:55<00:38,  1.39it/s]Evaluating on SNLI-VE val set:  75%|#######5  | 162/215 [01:56<00:38,  1.38it/s]Evaluating on SNLI-VE val set:  76%|#######5  | 163/215 [01:57<00:38,  1.36it/s]Evaluating on SNLI-VE val set:  76%|#######6  | 164/215 [01:57<00:37,  1.38it/s]Evaluating on SNLI-VE val set:  77%|#######6  | 165/215 [01:58<00:36,  1.39it/s]Evaluating on SNLI-VE val set:  77%|#######7  | 166/215 [01:59<00:34,  1.41it/s]Evaluating on SNLI-VE val set:  78%|#######7  | 167/215 [02:00<00:34,  1.41it/s]Evaluating on SNLI-VE val set:  78%|#######8  | 168/215 [02:00<00:34,  1.36it/s]Evaluating on SNLI-VE val set:  79%|#######8  | 169/215 [02:01<00:33,  1.37it/s]Evaluating on SNLI-VE val set:  79%|#######9  | 170/215 [02:02<00:33,  1.36it/s]Evaluating on SNLI-VE val set:  80%|#######9  | 171/215 [02:03<00:32,  1.36it/s]Evaluating on SNLI-VE val set:  80%|########  | 172/215 [02:03<00:31,  1.38it/s]Evaluating on SNLI-VE val set:  80%|########  | 173/215 [02:04<00:29,  1.43it/s]Evaluating on SNLI-VE val set:  81%|########  | 174/215 [02:05<00:29,  1.38it/s]Evaluating on SNLI-VE val set:  81%|########1 | 175/215 [02:05<00:29,  1.36it/s]Evaluating on SNLI-VE val set:  82%|########1 | 176/215 [02:06<00:28,  1.36it/s]Evaluating on SNLI-VE val set:  82%|########2 | 177/215 [02:07<00:27,  1.36it/s]Evaluating on SNLI-VE val set:  83%|########2 | 178/215 [02:08<00:27,  1.35it/s]Evaluating on SNLI-VE val set:  83%|########3 | 179/215 [02:08<00:26,  1.36it/s]Evaluating on SNLI-VE val set:  84%|########3 | 180/215 [02:09<00:25,  1.39it/s]Evaluating on SNLI-VE val set:  84%|########4 | 181/215 [02:10<00:25,  1.35it/s]Evaluating on SNLI-VE val set:  85%|########4 | 182/215 [02:11<00:24,  1.32it/s]Evaluating on SNLI-VE val set:  85%|########5 | 183/215 [02:11<00:24,  1.33it/s]Evaluating on SNLI-VE val set:  86%|########5 | 184/215 [02:12<00:22,  1.35it/s]Evaluating on SNLI-VE val set:  86%|########6 | 185/215 [02:13<00:22,  1.35it/s]Evaluating on SNLI-VE val set:  87%|########6 | 186/215 [02:14<00:20,  1.38it/s]Evaluating on SNLI-VE val set:  87%|########6 | 187/215 [02:14<00:19,  1.42it/s]Evaluating on SNLI-VE val set:  87%|########7 | 188/215 [02:15<00:19,  1.39it/s]Evaluating on SNLI-VE val set:  88%|########7 | 189/215 [02:16<00:19,  1.36it/s]Evaluating on SNLI-VE val set:  88%|########8 | 190/215 [02:17<00:18,  1.36it/s]Evaluating on SNLI-VE val set:  89%|########8 | 191/215 [02:17<00:17,  1.38it/s]Evaluating on SNLI-VE val set:  89%|########9 | 192/215 [02:18<00:16,  1.40it/s]Evaluating on SNLI-VE val set:  90%|########9 | 193/215 [02:19<00:15,  1.44it/s]Evaluating on SNLI-VE val set:  90%|######### | 194/215 [02:19<00:14,  1.48it/s]Evaluating on SNLI-VE val set:  91%|######### | 195/215 [02:20<00:13,  1.46it/s]Evaluating on SNLI-VE val set:  91%|#########1| 196/215 [02:21<00:13,  1.42it/s]Evaluating on SNLI-VE val set:  92%|#########1| 197/215 [02:21<00:12,  1.43it/s]Evaluating on SNLI-VE val set:  92%|#########2| 198/215 [02:22<00:11,  1.42it/s]Evaluating on SNLI-VE val set:  93%|#########2| 199/215 [02:23<00:11,  1.39it/s]Evaluating on SNLI-VE val set:  93%|#########3| 200/215 [02:24<00:10,  1.39it/s]Evaluating on SNLI-VE val set:  93%|#########3| 201/215 [02:24<00:10,  1.40it/s]Evaluating on SNLI-VE val set:  94%|#########3| 202/215 [02:25<00:09,  1.40it/s]Evaluating on SNLI-VE val set:  94%|#########4| 203/215 [02:26<00:08,  1.40it/s]Evaluating on SNLI-VE val set:  95%|#########4| 204/215 [02:26<00:08,  1.36it/s]Evaluating on SNLI-VE val set:  95%|#########5| 205/215 [02:27<00:07,  1.36it/s]Evaluating on SNLI-VE val set:  96%|#########5| 206/215 [02:28<00:06,  1.38it/s]Evaluating on SNLI-VE val set:  96%|#########6| 207/215 [02:29<00:05,  1.38it/s]Evaluating on SNLI-VE val set:  97%|#########6| 208/215 [02:29<00:04,  1.40it/s]Evaluating on SNLI-VE val set:  97%|#########7| 209/215 [02:30<00:04,  1.42it/s]Evaluating on SNLI-VE val set:  98%|#########7| 210/215 [02:31<00:03,  1.37it/s]Evaluating on SNLI-VE val set:  98%|#########8| 211/215 [02:31<00:02,  1.41it/s]Evaluating on SNLI-VE val set:  99%|#########8| 212/215 [02:32<00:02,  1.42it/s]Evaluating on SNLI-VE val set:  99%|#########9| 213/215 [02:33<00:01,  1.46it/s]Evaluating on SNLI-VE val set: 100%|#########9| 214/215 [02:33<00:00,  1.45it/s]Evaluating on SNLI-VE val set: 100%|##########| 215/215 [02:34<00:00,  1.90it/s]Evaluating on SNLI-VE val set: 100%|##########| 215/215 [02:34<00:00,  1.39it/s]
11/17/2022 02:02:56 - INFO - train.train_snli_ve - Evaluation after epoch 1: 73.11
11/17/2022 02:02:56 - INFO - train.train_snli_ve - New best evaluation score: 73.11
11/17/2022 02:02:56 - INFO - __main__ - Best SNLI-VE evaluation score = 73.11, after epoch 1
11/17/2022 02:02:56 - INFO - __main__ - Saving best model and encoder checkpoint after SNLI-VE training
11/17/2022 02:02:58 - INFO - __main__ - Saved checkpoint!
11/17/2022 02:02:58 - INFO - __main__ - Saved continual learning results so far!
11/17/2022 02:02:58 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/17/2022 02:02:58 - INFO - __main__ - Evaluating FORWARD TRANSFER of vilt model on vqa -> snli-ve
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - ----------------------------------------------------------------------------------------------------
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - Absolute performance on task #0, VQAv2 = 58.18%
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - Relative Gain for task #0, VQAv2 = -13.43%
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - Absolute performance on task #1, SNLI-VE = 73.11%
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - Relative Gain for task #1, SNLI-VE = 17.47%
11/17/2022 02:02:58 - INFO - __main__ - Average forward transfer gain = 2.02%
11/17/2022 02:02:58 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
11/17/2022 02:02:58 - INFO - __main__ - Evaluating CATASTROPHIC FORGETTING of vilt model on vqa -> snli-ve
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - ----------------------------------------------------------------------------------------------------
11/17/2022 02:02:58 - INFO - evaluate_cl_algorithm - Evaluating ViLT model using checkpoint after SNLI-VE training, on previously-seen tasks vqa
11/17/2022 02:02:58 - INFO - train.train_vqa - Loaded model checkpoint from /project/rostamim_919/caiyulia/Multi-Dytox/output/vilt-sequential_ft_vqa_snli_1_1_halfdata_noft_dytox/checkpoints/task1_snli-ve/model
Evaluating on VQA val set:   0%|          | 0/2671 [00:00<?, ?it/s]Evaluating on VQA val set:   0%|          | 1/2671 [00:01<54:01,  1.21s/it]Evaluating on VQA val set:   0%|          | 2/2671 [00:01<39:58,  1.11it/s]Evaluating on VQA val set:   0%|          | 3/2671 [00:02<33:34,  1.32it/s]Evaluating on VQA val set:   0%|          | 4/2671 [00:03<32:22,  1.37it/s]Evaluating on VQA val set:   0%|          | 5/2671 [00:03<30:54,  1.44it/s]Evaluating on VQA val set:   0%|          | 6/2671 [00:04<30:02,  1.48it/s]Evaluating on VQA val set:   0%|          | 7/2671 [00:05<29:58,  1.48it/s]Evaluating on VQA val set:   0%|          | 8/2671 [00:05<29:01,  1.53it/s]Evaluating on VQA val set:   0%|          | 9/2671 [00:06<29:15,  1.52it/s]Evaluating on VQA val set:   0%|          | 10/2671 [00:06<28:26,  1.56it/s]Evaluating on VQA val set:   0%|          | 11/2671 [00:07<28:17,  1.57it/s]Evaluating on VQA val set:   0%|          | 12/2671 [00:08<28:39,  1.55it/s]Evaluating on VQA val set:   0%|          | 13/2671 [00:08<28:39,  1.55it/s]Evaluating on VQA val set:   1%|          | 14/2671 [00:09<28:03,  1.58it/s]Evaluating on VQA val set:   1%|          | 15/2671 [00:10<28:49,  1.54it/s]Evaluating on VQA val set:   1%|          | 16/2671 [00:10<28:40,  1.54it/s]Evaluating on VQA val set:   1%|          | 17/2671 [00:11<29:05,  1.52it/s]Evaluating on VQA val set:   1%|          | 18/2671 [00:12<29:34,  1.49it/s]Evaluating on VQA val set:   1%|          | 19/2671 [00:12<29:21,  1.51it/s]Evaluating on VQA val set:   1%|          | 20/2671 [00:13<27:52,  1.59it/s]Evaluating on VQA val set:   1%|          | 21/2671 [00:14<28:14,  1.56it/s]Evaluating on VQA val set:   1%|          | 22/2671 [00:14<29:16,  1.51it/s]Evaluating on VQA val set:   1%|          | 23/2671 [00:15<29:35,  1.49it/s]Evaluating on VQA val set:   1%|          | 24/2671 [00:16<28:36,  1.54it/s]Evaluating on VQA val set:   1%|          | 25/2671 [00:16<29:19,  1.50it/s]Evaluating on VQA val set:   1%|          | 26/2671 [00:17<29:29,  1.49it/s]Evaluating on VQA val set:   1%|1         | 27/2671 [00:18<29:23,  1.50it/s]Evaluating on VQA val set:   1%|1         | 28/2671 [00:18<29:57,  1.47it/s]Evaluating on VQA val set:   1%|1         | 29/2671 [00:19<29:55,  1.47it/s]Evaluating on VQA val set:   1%|1         | 30/2671 [00:20<30:04,  1.46it/s]Evaluating on VQA val set:   1%|1         | 31/2671 [00:20<29:55,  1.47it/s]Evaluating on VQA val set:   1%|1         | 32/2671 [00:21<28:50,  1.53it/s]Evaluating on VQA val set:   1%|1         | 33/2671 [00:22<28:21,  1.55it/s]Evaluating on VQA val set:   1%|1         | 34/2671 [00:22<28:54,  1.52it/s]Evaluating on VQA val set:   1%|1         | 35/2671 [00:23<29:19,  1.50it/s]Evaluating on VQA val set:   1%|1         | 36/2671 [00:24<29:45,  1.48it/s]Evaluating on VQA val set:   1%|1         | 37/2671 [00:24<29:26,  1.49it/s]Evaluating on VQA val set:   1%|1         | 38/2671 [00:25<29:36,  1.48it/s]Evaluating on VQA val set:   1%|1         | 39/2671 [00:26<28:31,  1.54it/s]Evaluating on VQA val set:   1%|1         | 40/2671 [00:26<28:35,  1.53it/s]Evaluating on VQA val set:   2%|1         | 41/2671 [00:27<28:08,  1.56it/s]Evaluating on VQA val set:   2%|1         | 42/2671 [00:27<26:24,  1.66it/s]Evaluating on VQA val set:   2%|1         | 43/2671 [00:28<25:10,  1.74it/s]Evaluating on VQA val set:   2%|1         | 44/2671 [00:29<26:34,  1.65it/s]Evaluating on VQA val set:   2%|1         | 45/2671 [00:29<27:20,  1.60it/s]Evaluating on VQA val set:   2%|1         | 46/2671 [00:30<28:01,  1.56it/s]Evaluating on VQA val set:   2%|1         | 47/2671 [00:31<28:37,  1.53it/s]Evaluating on VQA val set:   2%|1         | 48/2671 [00:31<28:37,  1.53it/s]Evaluating on VQA val set:   2%|1         | 49/2671 [00:32<28:56,  1.51it/s]Evaluating on VQA val set:   2%|1         | 50/2671 [00:33<28:43,  1.52it/s]Evaluating on VQA val set:   2%|1         | 51/2671 [00:33<29:13,  1.49it/s]Evaluating on VQA val set:   2%|1         | 52/2671 [00:34<28:45,  1.52it/s]Evaluating on VQA val set:   2%|1         | 53/2671 [00:35<28:27,  1.53it/s]Evaluating on VQA val set:   2%|2         | 54/2671 [00:35<28:42,  1.52it/s]Evaluating on VQA val set:   2%|2         | 55/2671 [00:36<26:33,  1.64it/s]Evaluating on VQA val set:   2%|2         | 56/2671 [00:36<27:57,  1.56it/s]Evaluating on VQA val set:   2%|2         | 57/2671 [00:37<29:03,  1.50it/s]Evaluating on VQA val set:   2%|2         | 58/2671 [00:38<29:27,  1.48it/s]Evaluating on VQA val set:   2%|2         | 59/2671 [00:39<28:18,  1.54it/s]Evaluating on VQA val set:   2%|2         | 60/2671 [00:39<28:54,  1.51it/s]Evaluating on VQA val set:   2%|2         | 61/2671 [00:40<28:55,  1.50it/s]Evaluating on VQA val set:   2%|2         | 62/2671 [00:41<28:43,  1.51it/s]Evaluating on VQA val set:   2%|2         | 63/2671 [00:41<28:58,  1.50it/s]Evaluating on VQA val set:   2%|2         | 64/2671 [00:42<29:33,  1.47it/s]Evaluating on VQA val set:   2%|2         | 65/2671 [00:43<29:27,  1.47it/s]Evaluating on VQA val set:   2%|2         | 66/2671 [00:43<28:35,  1.52it/s]Evaluating on VQA val set:   3%|2         | 67/2671 [00:44<28:43,  1.51it/s]Evaluating on VQA val set:   3%|2         | 68/2671 [00:45<28:47,  1.51it/s]Evaluating on VQA val set:   3%|2         | 69/2671 [00:45<28:47,  1.51it/s]Evaluating on VQA val set:   3%|2         | 70/2671 [00:46<29:20,  1.48it/s]Evaluating on VQA val set:   3%|2         | 71/2671 [00:46<28:11,  1.54it/s]Evaluating on VQA val set:   3%|2         | 72/2671 [00:47<28:59,  1.49it/s]Evaluating on VQA val set:   3%|2         | 73/2671 [00:48<29:05,  1.49it/s]Evaluating on VQA val set:   3%|2         | 74/2671 [00:49<28:56,  1.50it/s]Evaluating on VQA val set:   3%|2         | 75/2671 [00:49<28:41,  1.51it/s]Evaluating on VQA val set:   3%|2         | 76/2671 [00:50<28:27,  1.52it/s]Evaluating on VQA val set:   3%|2         | 77/2671 [00:50<28:05,  1.54it/s]Evaluating on VQA val set:   3%|2         | 78/2671 [00:51<28:29,  1.52it/s]Evaluating on VQA val set:   3%|2         | 79/2671 [00:52<29:00,  1.49it/s]Evaluating on VQA val set:   3%|2         | 80/2671 [00:52<28:07,  1.54it/s]Evaluating on VQA val set:   3%|3         | 81/2671 [00:53<28:07,  1.54it/s]Evaluating on VQA val set:   3%|3         | 82/2671 [00:54<28:27,  1.52it/s]Evaluating on VQA val set:   3%|3         | 83/2671 [00:54<28:25,  1.52it/s]Evaluating on VQA val set:   3%|3         | 84/2671 [00:55<28:15,  1.53it/s]Evaluating on VQA val set:   3%|3         | 85/2671 [00:56<28:46,  1.50it/s]Evaluating on VQA val set:   3%|3         | 86/2671 [00:56<28:50,  1.49it/s]Evaluating on VQA val set:   3%|3         | 87/2671 [00:57<28:52,  1.49it/s]Evaluating on VQA val set:   3%|3         | 88/2671 [00:58<29:00,  1.48it/s]Evaluating on VQA val set:   3%|3         | 89/2671 [00:58<28:41,  1.50it/s]Evaluating on VQA val set:   3%|3         | 90/2671 [00:59<29:29,  1.46it/s]Evaluating on VQA val set:   3%|3         | 91/2671 [01:00<28:53,  1.49it/s]Evaluating on VQA val set:   3%|3         | 92/2671 [01:00<28:05,  1.53it/s]Evaluating on VQA val set:   3%|3         | 93/2671 [01:01<28:24,  1.51it/s]Evaluating on VQA val set:   4%|3         | 94/2671 [01:02<28:32,  1.51it/s]Evaluating on VQA val set:   4%|3         | 95/2671 [01:02<27:48,  1.54it/s]Evaluating on VQA val set:   4%|3         | 96/2671 [01:03<28:05,  1.53it/s]Evaluating on VQA val set:   4%|3         | 97/2671 [01:04<27:44,  1.55it/s]Evaluating on VQA val set:   4%|3         | 98/2671 [01:04<26:30,  1.62it/s]Evaluating on VQA val set:   4%|3         | 99/2671 [01:05<25:28,  1.68it/s]Evaluating on VQA val set:   4%|3         | 100/2671 [01:06<27:24,  1.56it/s]Evaluating on VQA val set:   4%|3         | 101/2671 [01:06<27:29,  1.56it/s]Evaluating on VQA val set:   4%|3         | 102/2671 [01:07<27:30,  1.56it/s]Evaluating on VQA val set:   4%|3         | 103/2671 [01:08<28:20,  1.51it/s]Evaluating on VQA val set:   4%|3         | 104/2671 [01:08<28:04,  1.52it/s]Evaluating on VQA val set:   4%|3         | 105/2671 [01:09<29:16,  1.46it/s]Evaluating on VQA val set:   4%|3         | 106/2671 [01:10<29:28,  1.45it/s]Evaluating on VQA val set:   4%|4         | 107/2671 [01:10<29:35,  1.44it/s]Evaluating on VQA val set:   4%|4         | 108/2671 [01:11<29:46,  1.44it/s]Evaluating on VQA val set:   4%|4         | 109/2671 [01:12<30:17,  1.41it/s]Evaluating on VQA val set:   4%|4         | 110/2671 [01:12<29:10,  1.46it/s]Evaluating on VQA val set:   4%|4         | 111/2671 [01:13<28:40,  1.49it/s]Evaluating on VQA val set:   4%|4         | 112/2671 [01:14<28:24,  1.50it/s]Evaluating on VQA val set:   4%|4         | 113/2671 [01:14<28:41,  1.49it/s]Evaluating on VQA val set:   4%|4         | 114/2671 [01:15<29:32,  1.44it/s]Evaluating on VQA val set:   4%|4         | 115/2671 [01:16<29:53,  1.43it/s]Evaluating on VQA val set:   4%|4         | 116/2671 [01:17<29:59,  1.42it/s]Evaluating on VQA val set:   4%|4         | 117/2671 [01:17<30:36,  1.39it/s]Evaluating on VQA val set:   4%|4         | 118/2671 [01:18<30:00,  1.42it/s]Evaluating on VQA val set:   4%|4         | 119/2671 [01:19<29:40,  1.43it/s]Evaluating on VQA val set:   4%|4         | 120/2671 [01:19<30:29,  1.39it/s]Evaluating on VQA val set:   5%|4         | 121/2671 [01:20<30:25,  1.40it/s]Evaluating on VQA val set:   5%|4         | 122/2671 [01:21<30:47,  1.38it/s]Evaluating on VQA val set:   5%|4         | 123/2671 [01:22<30:51,  1.38it/s]Evaluating on VQA val set:   5%|4         | 124/2671 [01:22<31:14,  1.36it/s]Evaluating on VQA val set:   5%|4         | 125/2671 [01:23<30:29,  1.39it/s]Evaluating on VQA val set:   5%|4         | 126/2671 [01:24<29:51,  1.42it/s]Evaluating on VQA val set:   5%|4         | 127/2671 [01:24<30:25,  1.39it/s]Evaluating on VQA val set:   5%|4         | 128/2671 [01:25<30:49,  1.38it/s]Evaluating on VQA val set:   5%|4         | 129/2671 [01:26<29:57,  1.41it/s]Evaluating on VQA val set:   5%|4         | 130/2671 [01:27<30:38,  1.38it/s]Evaluating on VQA val set:   5%|4         | 131/2671 [01:27<29:31,  1.43it/s]Evaluating on VQA val set:   5%|4         | 132/2671 [01:28<29:06,  1.45it/s]Evaluating on VQA val set:   5%|4         | 133/2671 [01:29<30:17,  1.40it/s]Evaluating on VQA val set:   5%|5         | 134/2671 [01:29<30:04,  1.41it/s]Evaluating on VQA val set:   5%|5         | 135/2671 [01:30<30:32,  1.38it/s]Evaluating on VQA val set:   5%|5         | 136/2671 [01:31<30:40,  1.38it/s]Evaluating on VQA val set:   5%|5         | 137/2671 [01:32<29:26,  1.43it/s]Evaluating on VQA val set:   5%|5         | 138/2671 [01:32<29:46,  1.42it/s]Evaluating on VQA val set:   5%|5         | 139/2671 [01:33<30:27,  1.39it/s]Evaluating on VQA val set:   5%|5         | 140/2671 [01:34<30:10,  1.40it/s]Evaluating on VQA val set:   5%|5         | 141/2671 [01:34<30:40,  1.37it/s]Evaluating on VQA val set:   5%|5         | 142/2671 [01:35<30:31,  1.38it/s]Evaluating on VQA val set:   5%|5         | 143/2671 [01:36<30:42,  1.37it/s]Evaluating on VQA val set:   5%|5         | 144/2671 [01:37<30:41,  1.37it/s]Evaluating on VQA val set:   5%|5         | 145/2671 [01:37<29:45,  1.41it/s]Evaluating on VQA val set:   5%|5         | 146/2671 [01:38<28:02,  1.50it/s]Evaluating on VQA val set:   6%|5         | 147/2671 [01:39<27:54,  1.51it/s]Evaluating on VQA val set:   6%|5         | 148/2671 [01:39<27:56,  1.51it/s]Evaluating on VQA val set:   6%|5         | 149/2671 [01:40<29:41,  1.42it/s]Evaluating on VQA val set:   6%|5         | 150/2671 [01:41<29:17,  1.43it/s]Evaluating on VQA val set:   6%|5         | 151/2671 [01:41<28:45,  1.46it/s]Evaluating on VQA val set:   6%|5         | 152/2671 [01:42<28:30,  1.47it/s]Evaluating on VQA val set:   6%|5         | 153/2671 [01:43<29:10,  1.44it/s]Evaluating on VQA val set:   6%|5         | 154/2671 [01:43<29:06,  1.44it/s]Evaluating on VQA val set:   6%|5         | 155/2671 [01:44<28:22,  1.48it/s]Evaluating on VQA val set:   6%|5         | 156/2671 [01:45<28:51,  1.45it/s]Evaluating on VQA val set:   6%|5         | 157/2671 [01:45<28:03,  1.49it/s]Evaluating on VQA val set:   6%|5         | 158/2671 [01:46<29:01,  1.44it/s]Evaluating on VQA val set:   6%|5         | 159/2671 [01:47<30:42,  1.36it/s]Evaluating on VQA val set:   6%|5         | 160/2671 [01:48<31:20,  1.34it/s]Evaluating on VQA val set:   6%|6         | 161/2671 [01:49<31:19,  1.34it/s]Evaluating on VQA val set:   6%|6         | 162/2671 [01:49<30:33,  1.37it/s]Evaluating on VQA val set:   6%|6         | 163/2671 [01:50<30:53,  1.35it/s]Evaluating on VQA val set:   6%|6         | 164/2671 [01:51<30:53,  1.35it/s]Evaluating on VQA val set:   6%|6         | 165/2671 [01:51<31:08,  1.34it/s]Evaluating on VQA val set:   6%|6         | 166/2671 [01:52<28:57,  1.44it/s]Evaluating on VQA val set:   6%|6         | 167/2671 [01:53<29:43,  1.40it/s]Evaluating on VQA val set:   6%|6         | 168/2671 [01:54<30:13,  1.38it/s]Evaluating on VQA val set:   6%|6         | 169/2671 [01:54<30:38,  1.36it/s]Evaluating on VQA val set:   6%|6         | 170/2671 [01:55<30:53,  1.35it/s]Evaluating on VQA val set:   6%|6         | 171/2671 [01:56<31:21,  1.33it/s]Evaluating on VQA val set:   6%|6         | 172/2671 [01:57<30:56,  1.35it/s]Evaluating on VQA val set:   6%|6         | 173/2671 [01:57<30:47,  1.35it/s]Evaluating on VQA val set:   7%|6         | 174/2671 [01:58<29:55,  1.39it/s]Evaluating on VQA val set:   7%|6         | 175/2671 [01:59<27:39,  1.50it/s]Evaluating on VQA val set:   7%|6         | 176/2671 [01:59<28:23,  1.46it/s]Evaluating on VQA val set:   7%|6         | 177/2671 [02:00<28:04,  1.48it/s]Evaluating on VQA val set:   7%|6         | 178/2671 [02:01<29:19,  1.42it/s]Evaluating on VQA val set:   7%|6         | 179/2671 [02:01<29:29,  1.41it/s]Evaluating on VQA val set:   7%|6         | 180/2671 [02:02<30:01,  1.38it/s]Evaluating on VQA val set:   7%|6         | 181/2671 [02:03<29:46,  1.39it/s]Evaluating on VQA val set:   7%|6         | 182/2671 [02:04<29:45,  1.39it/s]Evaluating on VQA val set:   7%|6         | 183/2671 [02:04<29:35,  1.40it/s]Evaluating on VQA val set:   7%|6         | 184/2671 [02:05<30:15,  1.37it/s]Evaluating on VQA val set:   7%|6         | 185/2671 [02:06<29:08,  1.42it/s]Evaluating on VQA val set:   7%|6         | 186/2671 [02:06<29:26,  1.41it/s]Evaluating on VQA val set:   7%|7         | 187/2671 [02:07<29:30,  1.40it/s]Evaluating on VQA val set:   7%|7         | 188/2671 [02:08<28:16,  1.46it/s]Evaluating on VQA val set:   7%|7         | 189/2671 [02:08<29:07,  1.42it/s]Evaluating on VQA val set:   7%|7         | 190/2671 [02:09<30:17,  1.36it/s]Evaluating on VQA val set:   7%|7         | 191/2671 [02:10<30:41,  1.35it/s]Evaluating on VQA val set:   7%|7         | 192/2671 [02:11<30:24,  1.36it/s]Evaluating on VQA val set:   7%|7         | 193/2671 [02:11<29:42,  1.39it/s]Evaluating on VQA val set:   7%|7         | 194/2671 [02:12<28:43,  1.44it/s]Evaluating on VQA val set:   7%|7         | 195/2671 [02:13<29:10,  1.41it/s]Evaluating on VQA val set:   7%|7         | 196/2671 [02:14<29:20,  1.41it/s]Evaluating on VQA val set:   7%|7         | 197/2671 [02:14<28:52,  1.43it/s]Evaluating on VQA val set:   7%|7         | 198/2671 [02:15<27:55,  1.48it/s]Evaluating on VQA val set:   7%|7         | 199/2671 [02:16<28:10,  1.46it/s]Evaluating on VQA val set:   7%|7         | 200/2671 [02:16<28:56,  1.42it/s]Evaluating on VQA val set:   8%|7         | 201/2671 [02:17<27:26,  1.50it/s]Evaluating on VQA val set:   8%|7         | 202/2671 [02:18<27:27,  1.50it/s]Evaluating on VQA val set:   8%|7         | 203/2671 [02:18<27:08,  1.52it/s]Evaluating on VQA val set:   8%|7         | 204/2671 [02:19<27:57,  1.47it/s]Evaluating on VQA val set:   8%|7         | 205/2671 [02:20<28:46,  1.43it/s]Evaluating on VQA val set:   8%|7         | 206/2671 [02:20<28:40,  1.43it/s]Evaluating on VQA val set:   8%|7         | 207/2671 [02:21<27:13,  1.51it/s]Evaluating on VQA val set:   8%|7         | 208/2671 [02:22<26:37,  1.54it/s]Evaluating on VQA val set:   8%|7         | 209/2671 [02:22<26:23,  1.55it/s]Evaluating on VQA val set:   8%|7         | 210/2671 [02:23<27:28,  1.49it/s]Evaluating on VQA val set:   8%|7         | 211/2671 [02:24<27:40,  1.48it/s]Evaluating on VQA val set:   8%|7         | 212/2671 [02:24<28:28,  1.44it/s]Evaluating on VQA val set:   8%|7         | 213/2671 [02:25<28:40,  1.43it/s]Evaluating on VQA val set:   8%|8         | 214/2671 [02:26<29:13,  1.40it/s]Evaluating on VQA val set:   8%|8         | 215/2671 [02:27<28:59,  1.41it/s]Evaluating on VQA val set:   8%|8         | 216/2671 [02:27<28:10,  1.45it/s]Evaluating on VQA val set:   8%|8         | 217/2671 [02:28<29:06,  1.40it/s]Evaluating on VQA val set:   8%|8         | 218/2671 [02:29<29:23,  1.39it/s]Evaluating on VQA val set:   8%|8         | 219/2671 [02:29<28:12,  1.45it/s]Evaluating on VQA val set:   8%|8         | 220/2671 [02:30<28:46,  1.42it/s]Evaluating on VQA val set:   8%|8         | 221/2671 [02:31<29:32,  1.38it/s]Evaluating on VQA val set:   8%|8         | 222/2671 [02:31<28:58,  1.41it/s]Evaluating on VQA val set:   8%|8         | 223/2671 [02:32<29:09,  1.40it/s]Evaluating on VQA val set:   8%|8         | 224/2671 [02:33<29:40,  1.37it/s]Evaluating on VQA val set:   8%|8         | 225/2671 [02:34<27:55,  1.46it/s]Evaluating on VQA val set:   8%|8         | 226/2671 [02:34<28:06,  1.45it/s]Evaluating on VQA val set:   8%|8         | 227/2671 [02:35<28:00,  1.45it/s]Evaluating on VQA val set:   9%|8         | 228/2671 [02:36<27:18,  1.49it/s]Evaluating on VQA val set:   9%|8         | 229/2671 [02:36<28:44,  1.42it/s]Evaluating on VQA val set:   9%|8         | 230/2671 [02:37<30:10,  1.35it/s]Evaluating on VQA val set:   9%|8         | 231/2671 [02:38<29:57,  1.36it/s]Evaluating on VQA val set:   9%|8         | 232/2671 [02:39<29:59,  1.36it/s]Evaluating on VQA val set:   9%|8         | 233/2671 [02:39<29:52,  1.36it/s]Evaluating on VQA val set:   9%|8         | 234/2671 [02:40<28:36,  1.42it/s]Evaluating on VQA val set:   9%|8         | 235/2671 [02:41<28:17,  1.43it/s]Evaluating on VQA val set:   9%|8         | 236/2671 [02:41<28:32,  1.42it/s]Evaluating on VQA val set:   9%|8         | 237/2671 [02:42<28:18,  1.43it/s]Evaluating on VQA val set:   9%|8         | 238/2671 [02:43<27:16,  1.49it/s]Evaluating on VQA val set:   9%|8         | 239/2671 [02:43<28:28,  1.42it/s]Evaluating on VQA val set:   9%|8         | 240/2671 [02:44<27:56,  1.45it/s]Evaluating on VQA val set:   9%|9         | 241/2671 [02:45<27:43,  1.46it/s]Evaluating on VQA val set:   9%|9         | 242/2671 [02:45<28:06,  1.44it/s]Evaluating on VQA val set:   9%|9         | 243/2671 [02:46<28:38,  1.41it/s]Evaluating on VQA val set:   9%|9         | 244/2671 [02:47<27:48,  1.45it/s]Evaluating on VQA val set:   9%|9         | 245/2671 [02:48<28:27,  1.42it/s]Evaluating on VQA val set:   9%|9         | 246/2671 [02:48<28:26,  1.42it/s]Evaluating on VQA val set:   9%|9         | 247/2671 [02:49<28:02,  1.44it/s]Evaluating on VQA val set:   9%|9         | 248/2671 [02:50<28:38,  1.41it/s]Evaluating on VQA val set:   9%|9         | 249/2671 [02:50<28:30,  1.42it/s]Evaluating on VQA val set:   9%|9         | 250/2671 [02:51<29:05,  1.39it/s]Evaluating on VQA val set:   9%|9         | 251/2671 [02:52<29:49,  1.35it/s]Evaluating on VQA val set:   9%|9         | 252/2671 [02:53<29:03,  1.39it/s]Evaluating on VQA val set:   9%|9         | 253/2671 [02:53<29:13,  1.38it/s]Evaluating on VQA val set:  10%|9         | 254/2671 [02:54<28:45,  1.40it/s]Evaluating on VQA val set:  10%|9         | 255/2671 [02:55<27:29,  1.46it/s]Evaluating on VQA val set:  10%|9         | 256/2671 [02:55<28:18,  1.42it/s]Evaluating on VQA val set:  10%|9         | 257/2671 [02:56<27:55,  1.44it/s]Evaluating on VQA val set:  10%|9         | 258/2671 [02:57<27:39,  1.45it/s]Evaluating on VQA val set:  10%|9         | 259/2671 [02:57<27:26,  1.46it/s]Evaluating on VQA val set:  10%|9         | 260/2671 [02:58<27:32,  1.46it/s]Evaluating on VQA val set:  10%|9         | 261/2671 [02:59<27:07,  1.48it/s]Evaluating on VQA val set:  10%|9         | 262/2671 [02:59<26:16,  1.53it/s]Evaluating on VQA val set:  10%|9         | 263/2671 [03:00<27:39,  1.45it/s]Evaluating on VQA val set:  10%|9         | 264/2671 [03:01<27:19,  1.47it/s]Evaluating on VQA val set:  10%|9         | 265/2671 [03:02<28:20,  1.42it/s]Evaluating on VQA val set:  10%|9         | 266/2671 [03:02<28:45,  1.39it/s]Evaluating on VQA val set:  10%|9         | 267/2671 [03:03<29:41,  1.35it/s]Evaluating on VQA val set:  10%|#         | 268/2671 [03:04<29:07,  1.37it/s]Evaluating on VQA val set:  10%|#         | 269/2671 [03:05<29:21,  1.36it/s]Evaluating on VQA val set:  10%|#         | 270/2671 [03:05<28:49,  1.39it/s]Evaluating on VQA val set:  10%|#         | 271/2671 [03:06<28:24,  1.41it/s]Evaluating on VQA val set:  10%|#         | 272/2671 [03:07<27:19,  1.46it/s]Evaluating on VQA val set:  10%|#         | 273/2671 [03:07<27:29,  1.45it/s]Evaluating on VQA val set:  10%|#         | 274/2671 [03:08<28:33,  1.40it/s]Evaluating on VQA val set:  10%|#         | 275/2671 [03:09<28:38,  1.39it/s]Evaluating on VQA val set:  10%|#         | 276/2671 [03:09<28:31,  1.40it/s]Evaluating on VQA val set:  10%|#         | 277/2671 [03:10<29:39,  1.35it/s]Evaluating on VQA val set:  10%|#         | 278/2671 [03:11<28:56,  1.38it/s]Evaluating on VQA val set:  10%|#         | 279/2671 [03:12<29:10,  1.37it/s]Evaluating on VQA val set:  10%|#         | 280/2671 [03:13<29:47,  1.34it/s]Evaluating on VQA val set:  11%|#         | 281/2671 [03:13<29:14,  1.36it/s]Evaluating on VQA val set:  11%|#         | 282/2671 [03:14<29:05,  1.37it/s]Evaluating on VQA val set:  11%|#         | 283/2671 [03:15<29:05,  1.37it/s]Evaluating on VQA val set:  11%|#         | 284/2671 [03:15<28:53,  1.38it/s]Evaluating on VQA val set:  11%|#         | 285/2671 [03:16<28:22,  1.40it/s]Evaluating on VQA val set:  11%|#         | 286/2671 [03:17<27:08,  1.46it/s]Evaluating on VQA val set:  11%|#         | 287/2671 [03:17<26:59,  1.47it/s]Evaluating on VQA val set:  11%|#         | 288/2671 [03:18<26:56,  1.47it/s]Evaluating on VQA val set:  11%|#         | 289/2671 [03:19<27:36,  1.44it/s]Evaluating on VQA val set:  11%|#         | 290/2671 [03:19<27:51,  1.42it/s]Evaluating on VQA val set:  11%|#         | 291/2671 [03:20<27:27,  1.44it/s]Evaluating on VQA val set:  11%|#         | 292/2671 [03:21<26:07,  1.52it/s]Evaluating on VQA val set:  11%|#         | 293/2671 [03:21<26:49,  1.48it/s]Evaluating on VQA val set:  11%|#1        | 294/2671 [03:22<25:59,  1.52it/s]Evaluating on VQA val set:  11%|#1        | 295/2671 [03:23<27:16,  1.45it/s]Evaluating on VQA val set:  11%|#1        | 296/2671 [03:24<27:42,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 297/2671 [03:24<28:02,  1.41it/s]Evaluating on VQA val set:  11%|#1        | 298/2671 [03:25<28:14,  1.40it/s]Evaluating on VQA val set:  11%|#1        | 299/2671 [03:26<28:04,  1.41it/s]Evaluating on VQA val set:  11%|#1        | 300/2671 [03:26<27:32,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 301/2671 [03:27<27:04,  1.46it/s]Evaluating on VQA val set:  11%|#1        | 302/2671 [03:28<27:24,  1.44it/s]Evaluating on VQA val set:  11%|#1        | 303/2671 [03:28<27:36,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 304/2671 [03:29<27:22,  1.44it/s]Evaluating on VQA val set:  11%|#1        | 305/2671 [03:30<27:35,  1.43it/s]Evaluating on VQA val set:  11%|#1        | 306/2671 [03:30<26:26,  1.49it/s]Evaluating on VQA val set:  11%|#1        | 307/2671 [03:31<26:35,  1.48it/s]Evaluating on VQA val set:  12%|#1        | 308/2671 [03:32<27:20,  1.44it/s]Evaluating on VQA val set:  12%|#1        | 309/2671 [03:33<27:23,  1.44it/s]Evaluating on VQA val set:  12%|#1        | 310/2671 [03:33<27:17,  1.44it/s]Evaluating on VQA val set:  12%|#1        | 311/2671 [03:34<27:11,  1.45it/s]Evaluating on VQA val set:  12%|#1        | 312/2671 [03:35<27:51,  1.41it/s]Evaluating on VQA val set:  12%|#1        | 313/2671 [03:35<28:31,  1.38it/s]Evaluating on VQA val set:  12%|#1        | 314/2671 [03:36<27:46,  1.41it/s]Evaluating on VQA val set:  12%|#1        | 315/2671 [03:37<27:42,  1.42it/s]Evaluating on VQA val set:  12%|#1        | 316/2671 [03:38<28:00,  1.40it/s]Evaluating on VQA val set:  12%|#1        | 317/2671 [03:38<28:13,  1.39it/s]Evaluating on VQA val set:  12%|#1        | 318/2671 [03:39<28:39,  1.37it/s]Evaluating on VQA val set:  12%|#1        | 319/2671 [03:40<29:14,  1.34it/s]Evaluating on VQA val set:  12%|#1        | 320/2671 [03:41<28:46,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 321/2671 [03:41<28:35,  1.37it/s]Evaluating on VQA val set:  12%|#2        | 322/2671 [03:42<28:21,  1.38it/s]Evaluating on VQA val set:  12%|#2        | 323/2671 [03:43<28:47,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 324/2671 [03:43<28:04,  1.39it/s]Evaluating on VQA val set:  12%|#2        | 325/2671 [03:44<25:42,  1.52it/s]Evaluating on VQA val set:  12%|#2        | 326/2671 [03:45<26:35,  1.47it/s]Evaluating on VQA val set:  12%|#2        | 327/2671 [03:45<27:33,  1.42it/s]Evaluating on VQA val set:  12%|#2        | 328/2671 [03:46<28:01,  1.39it/s]Evaluating on VQA val set:  12%|#2        | 329/2671 [03:47<27:58,  1.39it/s]Evaluating on VQA val set:  12%|#2        | 330/2671 [03:48<28:37,  1.36it/s]Evaluating on VQA val set:  12%|#2        | 331/2671 [03:48<27:56,  1.40it/s]Evaluating on VQA val set:  12%|#2        | 332/2671 [03:49<27:46,  1.40it/s]Evaluating on VQA val set:  12%|#2        | 333/2671 [03:50<27:34,  1.41it/s]Evaluating on VQA val set:  13%|#2        | 334/2671 [03:50<27:12,  1.43it/s]Evaluating on VQA val set:  13%|#2        | 335/2671 [03:51<25:50,  1.51it/s]Evaluating on VQA val set:  13%|#2        | 336/2671 [03:52<26:35,  1.46it/s]Evaluating on VQA val set:  13%|#2        | 337/2671 [03:52<26:00,  1.50it/s]Evaluating on VQA val set:  13%|#2        | 338/2671 [03:53<26:41,  1.46it/s]Evaluating on VQA val set:  13%|#2        | 339/2671 [03:54<26:36,  1.46it/s]Evaluating on VQA val set:  13%|#2        | 340/2671 [03:54<25:38,  1.52it/s]Evaluating on VQA val set:  13%|#2        | 341/2671 [03:55<26:36,  1.46it/s]Evaluating on VQA val set:  13%|#2        | 342/2671 [03:56<27:01,  1.44it/s]Evaluating on VQA val set:  13%|#2        | 343/2671 [03:57<27:16,  1.42it/s]Evaluating on VQA val set:  13%|#2        | 344/2671 [03:57<26:57,  1.44it/s]Evaluating on VQA val set:  13%|#2        | 345/2671 [03:58<27:05,  1.43it/s]Evaluating on VQA val set:  13%|#2        | 346/2671 [03:59<27:06,  1.43it/s]Evaluating on VQA val set:  13%|#2        | 347/2671 [03:59<27:27,  1.41it/s]Evaluating on VQA val set:  13%|#3        | 348/2671 [04:00<27:20,  1.42it/s]Evaluating on VQA val set:  13%|#3        | 349/2671 [04:01<27:07,  1.43it/s]Evaluating on VQA val set:  13%|#3        | 350/2671 [04:01<26:47,  1.44it/s]Evaluating on VQA val set:  13%|#3        | 351/2671 [04:02<27:17,  1.42it/s]Evaluating on VQA val set:  13%|#3        | 352/2671 [04:03<28:08,  1.37it/s]Evaluating on VQA val set:  13%|#3        | 353/2671 [04:04<27:56,  1.38it/s]Evaluating on VQA val set:  13%|#3        | 354/2671 [04:04<27:30,  1.40it/s]Evaluating on VQA val set:  13%|#3        | 355/2671 [04:05<25:32,  1.51it/s]Evaluating on VQA val set:  13%|#3        | 356/2671 [04:06<26:12,  1.47it/s]Evaluating on VQA val set:  13%|#3        | 357/2671 [04:06<27:06,  1.42it/s]Evaluating on VQA val set:  13%|#3        | 358/2671 [04:07<27:28,  1.40it/s]Evaluating on VQA val set:  13%|#3        | 359/2671 [04:08<28:19,  1.36it/s]Evaluating on VQA val set:  13%|#3        | 360/2671 [04:09<27:16,  1.41it/s]Evaluating on VQA val set:  14%|#3        | 361/2671 [04:09<27:34,  1.40it/s]Evaluating on VQA val set:  14%|#3        | 362/2671 [04:10<27:50,  1.38it/s]Evaluating on VQA val set:  14%|#3        | 363/2671 [04:11<28:00,  1.37it/s]Evaluating on VQA val set:  14%|#3        | 364/2671 [04:11<27:32,  1.40it/s]Evaluating on VQA val set:  14%|#3        | 365/2671 [04:12<27:37,  1.39it/s]Evaluating on VQA val set:  14%|#3        | 366/2671 [04:13<28:15,  1.36it/s]Evaluating on VQA val set:  14%|#3        | 367/2671 [04:14<28:10,  1.36it/s]Evaluating on VQA val set:  14%|#3        | 368/2671 [04:14<28:04,  1.37it/s]Evaluating on VQA val set:  14%|#3        | 369/2671 [04:15<28:07,  1.36it/s]Evaluating on VQA val set:  14%|#3        | 370/2671 [04:16<28:15,  1.36it/s]Evaluating on VQA val set:  14%|#3        | 371/2671 [04:17<28:00,  1.37it/s]Evaluating on VQA val set:  14%|#3        | 372/2671 [04:17<27:34,  1.39it/s]Evaluating on VQA val set:  14%|#3        | 373/2671 [04:18<27:48,  1.38it/s]Evaluating on VQA val set:  14%|#4        | 374/2671 [04:19<27:47,  1.38it/s]Evaluating on VQA val set:  14%|#4        | 375/2671 [04:19<27:58,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 376/2671 [04:20<27:41,  1.38it/s]Evaluating on VQA val set:  14%|#4        | 377/2671 [04:21<27:18,  1.40it/s]Evaluating on VQA val set:  14%|#4        | 378/2671 [04:22<28:34,  1.34it/s]Evaluating on VQA val set:  14%|#4        | 379/2671 [04:22<28:14,  1.35it/s]Evaluating on VQA val set:  14%|#4        | 380/2671 [04:23<27:51,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 381/2671 [04:24<27:09,  1.41it/s]Evaluating on VQA val set:  14%|#4        | 382/2671 [04:25<27:30,  1.39it/s]Evaluating on VQA val set:  14%|#4        | 383/2671 [04:25<27:54,  1.37it/s]Evaluating on VQA val set:  14%|#4        | 384/2671 [04:26<27:18,  1.40it/s]Evaluating on VQA val set:  14%|#4        | 385/2671 [04:27<27:20,  1.39it/s]Evaluating on VQA val set:  14%|#4        | 386/2671 [04:27<27:01,  1.41it/s]Evaluating on VQA val set:  14%|#4        | 387/2671 [04:28<27:06,  1.40it/s]Evaluating on VQA val set:  15%|#4        | 388/2671 [04:29<27:04,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 389/2671 [04:30<26:56,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 390/2671 [04:30<27:11,  1.40it/s]Evaluating on VQA val set:  15%|#4        | 391/2671 [04:31<27:14,  1.39it/s]Evaluating on VQA val set:  15%|#4        | 392/2671 [04:32<27:56,  1.36it/s]Evaluating on VQA val set:  15%|#4        | 393/2671 [04:32<27:50,  1.36it/s]Evaluating on VQA val set:  15%|#4        | 394/2671 [04:33<26:52,  1.41it/s]Evaluating on VQA val set:  15%|#4        | 395/2671 [04:34<27:21,  1.39it/s]Evaluating on VQA val set:  15%|#4        | 396/2671 [04:35<26:43,  1.42it/s]Evaluating on VQA val set:  15%|#4        | 397/2671 [04:35<26:15,  1.44it/s]Evaluating on VQA val set:  15%|#4        | 398/2671 [04:36<25:50,  1.47it/s]Evaluating on VQA val set:  15%|#4        | 399/2671 [04:37<26:41,  1.42it/s]Evaluating on VQA val set:  15%|#4        | 400/2671 [04:37<27:05,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 401/2671 [04:38<27:04,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 402/2671 [04:39<27:01,  1.40it/s]Evaluating on VQA val set:  15%|#5        | 403/2671 [04:40<27:45,  1.36it/s]Evaluating on VQA val set:  15%|#5        | 404/2671 [04:40<27:15,  1.39it/s]Evaluating on VQA val set:  15%|#5        | 405/2671 [04:41<26:11,  1.44it/s]Evaluating on VQA val set:  15%|#5        | 406/2671 [04:42<25:07,  1.50it/s]Evaluating on VQA val set:  15%|#5        | 407/2671 [04:42<26:06,  1.45it/s]Evaluating on VQA val set:  15%|#5        | 408/2671 [04:43<26:41,  1.41it/s]Evaluating on VQA val set:  15%|#5        | 409/2671 [04:44<26:41,  1.41it/s]Evaluating on VQA val set:  15%|#5        | 410/2671 [04:44<26:33,  1.42it/s]Evaluating on VQA val set:  15%|#5        | 411/2671 [04:45<26:25,  1.43it/s]Evaluating on VQA val set:  15%|#5        | 412/2671 [04:46<26:22,  1.43it/s]Evaluating on VQA val set:  15%|#5        | 413/2671 [04:46<25:44,  1.46it/s]Evaluating on VQA val set:  15%|#5        | 414/2671 [04:47<26:25,  1.42it/s]Evaluating on VQA val set:  16%|#5        | 415/2671 [04:48<24:55,  1.51it/s]Evaluating on VQA val set:  16%|#5        | 416/2671 [04:48<24:29,  1.53it/s]Evaluating on VQA val set:  16%|#5        | 417/2671 [04:49<25:09,  1.49it/s]Evaluating on VQA val set:  16%|#5        | 418/2671 [04:50<25:30,  1.47it/s]Evaluating on VQA val set:  16%|#5        | 419/2671 [04:51<25:55,  1.45it/s]Evaluating on VQA val set:  16%|#5        | 420/2671 [04:51<25:29,  1.47it/s]Evaluating on VQA val set:  16%|#5        | 421/2671 [04:52<26:03,  1.44it/s]Evaluating on VQA val set:  16%|#5        | 422/2671 [04:53<26:54,  1.39it/s]Evaluating on VQA val set:  16%|#5        | 423/2671 [04:53<27:24,  1.37it/s]Evaluating on VQA val set:  16%|#5        | 424/2671 [04:54<27:27,  1.36it/s]Evaluating on VQA val set:  16%|#5        | 425/2671 [04:55<27:11,  1.38it/s]Evaluating on VQA val set:  16%|#5        | 426/2671 [04:56<27:39,  1.35it/s]Evaluating on VQA val set:  16%|#5        | 427/2671 [04:56<28:00,  1.34it/s]Evaluating on VQA val set:  16%|#6        | 428/2671 [04:57<26:49,  1.39it/s]Evaluating on VQA val set:  16%|#6        | 429/2671 [04:58<26:30,  1.41it/s]Evaluating on VQA val set:  16%|#6        | 430/2671 [04:58<25:14,  1.48it/s]Evaluating on VQA val set:  16%|#6        | 431/2671 [04:59<25:23,  1.47it/s]Evaluating on VQA val set:  16%|#6        | 432/2671 [05:00<23:58,  1.56it/s]Evaluating on VQA val set:  16%|#6        | 433/2671 [05:00<23:28,  1.59it/s]Evaluating on VQA val set:  16%|#6        | 434/2671 [05:01<24:21,  1.53it/s]Evaluating on VQA val set:  16%|#6        | 435/2671 [05:02<25:07,  1.48it/s]Evaluating on VQA val set:  16%|#6        | 436/2671 [05:02<25:20,  1.47it/s]Evaluating on VQA val set:  16%|#6        | 437/2671 [05:03<25:35,  1.46it/s]Evaluating on VQA val set:  16%|#6        | 438/2671 [05:04<26:07,  1.42it/s]Evaluating on VQA val set:  16%|#6        | 439/2671 [05:05<26:34,  1.40it/s]Evaluating on VQA val set:  16%|#6        | 440/2671 [05:05<27:03,  1.37it/s]Evaluating on VQA val set:  17%|#6        | 441/2671 [05:06<26:50,  1.38it/s]Evaluating on VQA val set:  17%|#6        | 442/2671 [05:07<26:04,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 443/2671 [05:07<26:11,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 444/2671 [05:08<26:02,  1.43it/s]Evaluating on VQA val set:  17%|#6        | 445/2671 [05:09<26:02,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 446/2671 [05:09<25:19,  1.46it/s]Evaluating on VQA val set:  17%|#6        | 447/2671 [05:10<25:27,  1.46it/s]Evaluating on VQA val set:  17%|#6        | 448/2671 [05:11<25:31,  1.45it/s]Evaluating on VQA val set:  17%|#6        | 449/2671 [05:12<25:52,  1.43it/s]Evaluating on VQA val set:  17%|#6        | 450/2671 [05:12<26:06,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 451/2671 [05:13<26:00,  1.42it/s]Evaluating on VQA val set:  17%|#6        | 452/2671 [05:14<25:51,  1.43it/s]Evaluating on VQA val set:  17%|#6        | 453/2671 [05:14<25:50,  1.43it/s]Evaluating on VQA val set:  17%|#6        | 454/2671 [05:15<26:11,  1.41it/s]Evaluating on VQA val set:  17%|#7        | 455/2671 [05:16<25:10,  1.47it/s]Evaluating on VQA val set:  17%|#7        | 456/2671 [05:16<24:26,  1.51it/s]Evaluating on VQA val set:  17%|#7        | 457/2671 [05:17<25:52,  1.43it/s]Evaluating on VQA val set:  17%|#7        | 458/2671 [05:18<25:35,  1.44it/s]Evaluating on VQA val set:  17%|#7        | 459/2671 [05:19<26:37,  1.38it/s]Evaluating on VQA val set:  17%|#7        | 460/2671 [05:19<27:26,  1.34it/s]Evaluating on VQA val set:  17%|#7        | 461/2671 [05:20<27:17,  1.35it/s]Evaluating on VQA val set:  17%|#7        | 462/2671 [05:21<27:34,  1.34it/s]Evaluating on VQA val set:  17%|#7        | 463/2671 [05:22<27:30,  1.34it/s]Evaluating on VQA val set:  17%|#7        | 464/2671 [05:22<27:22,  1.34it/s]Evaluating on VQA val set:  17%|#7        | 465/2671 [05:23<26:33,  1.38it/s]Evaluating on VQA val set:  17%|#7        | 466/2671 [05:24<25:51,  1.42it/s]Evaluating on VQA val set:  17%|#7        | 467/2671 [05:24<25:34,  1.44it/s]Evaluating on VQA val set:  18%|#7        | 468/2671 [05:25<26:46,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 469/2671 [05:26<26:35,  1.38it/s]Evaluating on VQA val set:  18%|#7        | 470/2671 [05:27<26:48,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 471/2671 [05:27<27:15,  1.34it/s]Evaluating on VQA val set:  18%|#7        | 472/2671 [05:28<26:44,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 473/2671 [05:29<27:04,  1.35it/s]Evaluating on VQA val set:  18%|#7        | 474/2671 [05:30<26:40,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 475/2671 [05:30<26:20,  1.39it/s]Evaluating on VQA val set:  18%|#7        | 476/2671 [05:31<26:39,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 477/2671 [05:32<27:00,  1.35it/s]Evaluating on VQA val set:  18%|#7        | 478/2671 [05:32<26:42,  1.37it/s]Evaluating on VQA val set:  18%|#7        | 479/2671 [05:33<26:53,  1.36it/s]Evaluating on VQA val set:  18%|#7        | 480/2671 [05:34<25:46,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 481/2671 [05:35<25:23,  1.44it/s]Evaluating on VQA val set:  18%|#8        | 482/2671 [05:35<24:25,  1.49it/s]Evaluating on VQA val set:  18%|#8        | 483/2671 [05:36<23:28,  1.55it/s]Evaluating on VQA val set:  18%|#8        | 484/2671 [05:36<24:07,  1.51it/s]Evaluating on VQA val set:  18%|#8        | 485/2671 [05:37<23:27,  1.55it/s]Evaluating on VQA val set:  18%|#8        | 486/2671 [05:38<24:13,  1.50it/s]Evaluating on VQA val set:  18%|#8        | 487/2671 [05:38<25:06,  1.45it/s]Evaluating on VQA val set:  18%|#8        | 488/2671 [05:39<25:13,  1.44it/s]Evaluating on VQA val set:  18%|#8        | 489/2671 [05:40<25:05,  1.45it/s]Evaluating on VQA val set:  18%|#8        | 490/2671 [05:41<24:57,  1.46it/s]Evaluating on VQA val set:  18%|#8        | 491/2671 [05:41<25:40,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 492/2671 [05:42<25:48,  1.41it/s]Evaluating on VQA val set:  18%|#8        | 493/2671 [05:43<25:31,  1.42it/s]Evaluating on VQA val set:  18%|#8        | 494/2671 [05:43<26:19,  1.38it/s]Evaluating on VQA val set:  19%|#8        | 495/2671 [05:44<26:28,  1.37it/s]Evaluating on VQA val set:  19%|#8        | 496/2671 [05:45<26:33,  1.37it/s]Evaluating on VQA val set:  19%|#8        | 497/2671 [05:46<26:01,  1.39it/s]Evaluating on VQA val set:  19%|#8        | 498/2671 [05:46<26:24,  1.37it/s]Evaluating on VQA val set:  19%|#8        | 499/2671 [05:47<27:03,  1.34it/s]Evaluating on VQA val set:  19%|#8        | 500/2671 [05:48<27:10,  1.33it/s]Evaluating on VQA val set:  19%|#8        | 501/2671 [05:49<26:55,  1.34it/s]Evaluating on VQA val set:  19%|#8        | 502/2671 [05:49<26:03,  1.39it/s]Evaluating on VQA val set:  19%|#8        | 503/2671 [05:50<26:37,  1.36it/s]Evaluating on VQA val set:  19%|#8        | 504/2671 [05:51<26:57,  1.34it/s]Evaluating on VQA val set:  19%|#8        | 505/2671 [05:52<27:31,  1.31it/s]Evaluating on VQA val set:  19%|#8        | 506/2671 [05:52<26:42,  1.35it/s]Evaluating on VQA val set:  19%|#8        | 507/2671 [05:53<25:04,  1.44it/s]Evaluating on VQA val set:  19%|#9        | 508/2671 [05:54<24:36,  1.47it/s]Evaluating on VQA val set:  19%|#9        | 509/2671 [05:54<25:27,  1.42it/s]Evaluating on VQA val set:  19%|#9        | 510/2671 [05:55<25:10,  1.43it/s]Evaluating on VQA val set:  19%|#9        | 511/2671 [05:56<25:58,  1.39it/s]Evaluating on VQA val set:  19%|#9        | 512/2671 [05:57<26:08,  1.38it/s]Evaluating on VQA val set:  19%|#9        | 513/2671 [05:57<25:46,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 514/2671 [05:58<26:18,  1.37it/s]Evaluating on VQA val set:  19%|#9        | 515/2671 [05:59<25:23,  1.42it/s]Evaluating on VQA val set:  19%|#9        | 516/2671 [05:59<24:53,  1.44it/s]Evaluating on VQA val set:  19%|#9        | 517/2671 [06:00<25:12,  1.42it/s]Evaluating on VQA val set:  19%|#9        | 518/2671 [06:01<25:35,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 519/2671 [06:02<25:40,  1.40it/s]Evaluating on VQA val set:  19%|#9        | 520/2671 [06:02<26:25,  1.36it/s]Evaluating on VQA val set:  20%|#9        | 521/2671 [06:03<26:00,  1.38it/s]Evaluating on VQA val set:  20%|#9        | 522/2671 [06:04<25:14,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 523/2671 [06:04<25:33,  1.40it/s]Evaluating on VQA val set:  20%|#9        | 524/2671 [06:05<25:26,  1.41it/s]Evaluating on VQA val set:  20%|#9        | 525/2671 [06:06<25:43,  1.39it/s]Evaluating on VQA val set:  20%|#9        | 526/2671 [06:07<26:06,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 527/2671 [06:07<26:06,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 528/2671 [06:08<26:21,  1.35it/s]Evaluating on VQA val set:  20%|#9        | 529/2671 [06:09<26:03,  1.37it/s]Evaluating on VQA val set:  20%|#9        | 530/2671 [06:09<25:25,  1.40it/s]Evaluating on VQA val set:  20%|#9        | 531/2671 [06:10<25:42,  1.39it/s]Evaluating on VQA val set:  20%|#9        | 532/2671 [06:11<25:04,  1.42it/s]Evaluating on VQA val set:  20%|#9        | 533/2671 [06:12<24:36,  1.45it/s]Evaluating on VQA val set:  20%|#9        | 534/2671 [06:12<24:00,  1.48it/s]Evaluating on VQA val set:  20%|##        | 535/2671 [06:13<23:31,  1.51it/s]Evaluating on VQA val set:  20%|##        | 536/2671 [06:14<24:26,  1.46it/s]Evaluating on VQA val set:  20%|##        | 537/2671 [06:14<24:53,  1.43it/s]Evaluating on VQA val set:  20%|##        | 538/2671 [06:15<25:16,  1.41it/s]Evaluating on VQA val set:  20%|##        | 539/2671 [06:16<25:18,  1.40it/s]Evaluating on VQA val set:  20%|##        | 540/2671 [06:16<25:53,  1.37it/s]Evaluating on VQA val set:  20%|##        | 541/2671 [06:17<25:36,  1.39it/s]Evaluating on VQA val set:  20%|##        | 542/2671 [06:18<24:08,  1.47it/s]Evaluating on VQA val set:  20%|##        | 543/2671 [06:18<24:37,  1.44it/s]Evaluating on VQA val set:  20%|##        | 544/2671 [06:19<24:54,  1.42it/s]Evaluating on VQA val set:  20%|##        | 545/2671 [06:20<24:42,  1.43it/s]Evaluating on VQA val set:  20%|##        | 546/2671 [06:21<25:32,  1.39it/s]Evaluating on VQA val set:  20%|##        | 547/2671 [06:21<25:36,  1.38it/s]Evaluating on VQA val set:  21%|##        | 548/2671 [06:22<25:45,  1.37it/s]Evaluating on VQA val set:  21%|##        | 549/2671 [06:23<25:46,  1.37it/s]Evaluating on VQA val set:  21%|##        | 550/2671 [06:24<26:26,  1.34it/s]Evaluating on VQA val set:  21%|##        | 551/2671 [06:24<25:36,  1.38it/s]Evaluating on VQA val set:  21%|##        | 552/2671 [06:25<25:20,  1.39it/s]Evaluating on VQA val set:  21%|##        | 553/2671 [06:26<24:31,  1.44it/s]Evaluating on VQA val set:  21%|##        | 554/2671 [06:26<25:17,  1.40it/s]Evaluating on VQA val set:  21%|##        | 555/2671 [06:27<25:20,  1.39it/s]Evaluating on VQA val set:  21%|##        | 556/2671 [06:28<25:18,  1.39it/s]Evaluating on VQA val set:  21%|##        | 557/2671 [06:29<26:06,  1.35it/s]Evaluating on VQA val set:  21%|##        | 558/2671 [06:29<25:42,  1.37it/s]Evaluating on VQA val set:  21%|##        | 559/2671 [06:30<24:32,  1.43it/s]Evaluating on VQA val set:  21%|##        | 560/2671 [06:31<25:07,  1.40it/s]Evaluating on VQA val set:  21%|##1       | 561/2671 [06:31<24:43,  1.42it/s]Evaluating on VQA val set:  21%|##1       | 562/2671 [06:32<25:08,  1.40it/s]Evaluating on VQA val set:  21%|##1       | 563/2671 [06:33<25:37,  1.37it/s]Evaluating on VQA val set:  21%|##1       | 564/2671 [06:34<26:19,  1.33it/s]Evaluating on VQA val set:  21%|##1       | 565/2671 [06:34<25:17,  1.39it/s]Evaluating on VQA val set:  21%|##1       | 566/2671 [06:35<25:07,  1.40it/s]Evaluating on VQA val set:  21%|##1       | 567/2671 [06:36<23:40,  1.48it/s]Evaluating on VQA val set:  21%|##1       | 568/2671 [06:36<24:27,  1.43it/s]Evaluating on VQA val set:  21%|##1       | 569/2671 [06:37<24:06,  1.45it/s]Evaluating on VQA val set:  21%|##1       | 570/2671 [06:38<24:50,  1.41it/s]Evaluating on VQA val set:  21%|##1       | 571/2671 [06:39<24:44,  1.41it/s]Evaluating on VQA val set:  21%|##1       | 572/2671 [06:39<24:47,  1.41it/s]Evaluating on VQA val set:  21%|##1       | 573/2671 [06:40<24:49,  1.41it/s]Evaluating on VQA val set:  21%|##1       | 574/2671 [06:41<23:20,  1.50it/s]Evaluating on VQA val set:  22%|##1       | 575/2671 [06:41<23:57,  1.46it/s]Evaluating on VQA val set:  22%|##1       | 576/2671 [06:42<24:46,  1.41it/s]Evaluating on VQA val set:  22%|##1       | 577/2671 [06:43<25:11,  1.39it/s]Evaluating on VQA val set:  22%|##1       | 578/2671 [06:44<25:38,  1.36it/s]Evaluating on VQA val set:  22%|##1       | 579/2671 [06:44<25:26,  1.37it/s]Evaluating on VQA val set:  22%|##1       | 580/2671 [06:45<25:18,  1.38it/s]Evaluating on VQA val set:  22%|##1       | 581/2671 [06:46<24:38,  1.41it/s]Evaluating on VQA val set:  22%|##1       | 582/2671 [06:46<24:52,  1.40it/s]Evaluating on VQA val set:  22%|##1       | 583/2671 [06:47<25:13,  1.38it/s]Evaluating on VQA val set:  22%|##1       | 584/2671 [06:48<24:30,  1.42it/s]Evaluating on VQA val set:  22%|##1       | 585/2671 [06:49<24:25,  1.42it/s]Evaluating on VQA val set:  22%|##1       | 586/2671 [06:49<24:21,  1.43it/s]Evaluating on VQA val set:  22%|##1       | 587/2671 [06:50<23:50,  1.46it/s]Evaluating on VQA val set:  22%|##2       | 588/2671 [06:51<24:04,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 589/2671 [06:51<24:04,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 590/2671 [06:52<24:51,  1.40it/s]Evaluating on VQA val set:  22%|##2       | 591/2671 [06:53<24:24,  1.42it/s]Evaluating on VQA val set:  22%|##2       | 592/2671 [06:53<23:41,  1.46it/s]Evaluating on VQA val set:  22%|##2       | 593/2671 [06:54<23:19,  1.48it/s]Evaluating on VQA val set:  22%|##2       | 594/2671 [06:55<23:30,  1.47it/s]Evaluating on VQA val set:  22%|##2       | 595/2671 [06:55<24:00,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 596/2671 [06:56<23:57,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 597/2671 [06:57<24:00,  1.44it/s]Evaluating on VQA val set:  22%|##2       | 598/2671 [06:58<24:14,  1.43it/s]Evaluating on VQA val set:  22%|##2       | 599/2671 [06:58<24:45,  1.40it/s]Evaluating on VQA val set:  22%|##2       | 600/2671 [06:59<23:15,  1.48it/s]Evaluating on VQA val set:  23%|##2       | 601/2671 [07:00<23:43,  1.45it/s]Evaluating on VQA val set:  23%|##2       | 602/2671 [07:00<23:55,  1.44it/s]Evaluating on VQA val set:  23%|##2       | 603/2671 [07:01<24:05,  1.43it/s]Evaluating on VQA val set:  23%|##2       | 604/2671 [07:02<23:37,  1.46it/s]Evaluating on VQA val set:  23%|##2       | 605/2671 [07:02<24:13,  1.42it/s]Evaluating on VQA val set:  23%|##2       | 606/2671 [07:03<24:30,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 607/2671 [07:04<24:15,  1.42it/s]Evaluating on VQA val set:  23%|##2       | 608/2671 [07:04<23:54,  1.44it/s]Evaluating on VQA val set:  23%|##2       | 609/2671 [07:05<23:11,  1.48it/s]Evaluating on VQA val set:  23%|##2       | 610/2671 [07:06<23:56,  1.43it/s]Evaluating on VQA val set:  23%|##2       | 611/2671 [07:07<24:01,  1.43it/s]Evaluating on VQA val set:  23%|##2       | 612/2671 [07:07<24:31,  1.40it/s]Evaluating on VQA val set:  23%|##2       | 613/2671 [07:08<24:46,  1.38it/s]Evaluating on VQA val set:  23%|##2       | 614/2671 [07:09<25:02,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 615/2671 [07:09<24:37,  1.39it/s]Evaluating on VQA val set:  23%|##3       | 616/2671 [07:10<24:23,  1.40it/s]Evaluating on VQA val set:  23%|##3       | 617/2671 [07:11<24:37,  1.39it/s]Evaluating on VQA val set:  23%|##3       | 618/2671 [07:12<23:46,  1.44it/s]Evaluating on VQA val set:  23%|##3       | 619/2671 [07:12<23:29,  1.46it/s]Evaluating on VQA val set:  23%|##3       | 620/2671 [07:13<24:01,  1.42it/s]Evaluating on VQA val set:  23%|##3       | 621/2671 [07:14<24:54,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 622/2671 [07:15<25:04,  1.36it/s]Evaluating on VQA val set:  23%|##3       | 623/2671 [07:15<24:54,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 624/2671 [07:16<24:17,  1.40it/s]Evaluating on VQA val set:  23%|##3       | 625/2671 [07:17<24:51,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 626/2671 [07:17<24:48,  1.37it/s]Evaluating on VQA val set:  23%|##3       | 627/2671 [07:18<24:30,  1.39it/s]Evaluating on VQA val set:  24%|##3       | 628/2671 [07:19<24:58,  1.36it/s]Evaluating on VQA val set:  24%|##3       | 629/2671 [07:20<24:23,  1.40it/s]Evaluating on VQA val set:  24%|##3       | 630/2671 [07:20<23:37,  1.44it/s]Evaluating on VQA val set:  24%|##3       | 631/2671 [07:21<23:51,  1.43it/s]Evaluating on VQA val set:  24%|##3       | 632/2671 [07:22<23:28,  1.45it/s]Evaluating on VQA val set:  24%|##3       | 633/2671 [07:22<22:11,  1.53it/s]Evaluating on VQA val set:  24%|##3       | 634/2671 [07:23<22:50,  1.49it/s]Evaluating on VQA val set:  24%|##3       | 635/2671 [07:24<23:59,  1.41it/s]Evaluating on VQA val set:  24%|##3       | 636/2671 [07:24<23:45,  1.43it/s]Evaluating on VQA val set:  24%|##3       | 637/2671 [07:25<23:38,  1.43it/s]Evaluating on VQA val set:  24%|##3       | 638/2671 [07:26<23:49,  1.42it/s]Evaluating on VQA val set:  24%|##3       | 639/2671 [07:26<24:09,  1.40it/s]Evaluating on VQA val set:  24%|##3       | 640/2671 [07:27<24:10,  1.40it/s]Evaluating on VQA val set:  24%|##3       | 641/2671 [07:28<24:35,  1.38it/s]Evaluating on VQA val set:  24%|##4       | 642/2671 [07:29<24:02,  1.41it/s]Evaluating on VQA val set:  24%|##4       | 643/2671 [07:29<23:18,  1.45it/s]Evaluating on VQA val set:  24%|##4       | 644/2671 [07:30<23:48,  1.42it/s]Evaluating on VQA val set:  24%|##4       | 645/2671 [07:31<22:53,  1.47it/s]Evaluating on VQA val set:  24%|##4       | 646/2671 [07:31<21:42,  1.55it/s]Evaluating on VQA val set:  24%|##4       | 647/2671 [07:32<22:52,  1.47it/s]Evaluating on VQA val set:  24%|##4       | 648/2671 [07:33<23:36,  1.43it/s]Evaluating on VQA val set:  24%|##4       | 649/2671 [07:33<23:28,  1.44it/s]Evaluating on VQA val set:  24%|##4       | 650/2671 [07:34<24:06,  1.40it/s]Evaluating on VQA val set:  24%|##4       | 651/2671 [07:35<24:38,  1.37it/s]Evaluating on VQA val set:  24%|##4       | 652/2671 [07:36<24:23,  1.38it/s]Evaluating on VQA val set:  24%|##4       | 653/2671 [07:36<24:26,  1.38it/s]Evaluating on VQA val set:  24%|##4       | 654/2671 [07:37<23:54,  1.41it/s]Evaluating on VQA val set:  25%|##4       | 655/2671 [07:38<24:30,  1.37it/s]Evaluating on VQA val set:  25%|##4       | 656/2671 [07:39<24:45,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 657/2671 [07:39<24:52,  1.35it/s]Evaluating on VQA val set:  25%|##4       | 658/2671 [07:40<24:39,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 659/2671 [07:41<24:36,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 660/2671 [07:41<24:42,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 661/2671 [07:42<24:38,  1.36it/s]Evaluating on VQA val set:  25%|##4       | 662/2671 [07:43<23:53,  1.40it/s]Evaluating on VQA val set:  25%|##4       | 663/2671 [07:44<23:08,  1.45it/s]Evaluating on VQA val set:  25%|##4       | 664/2671 [07:44<23:04,  1.45it/s]Evaluating on VQA val set:  25%|##4       | 665/2671 [07:45<23:07,  1.45it/s]Evaluating on VQA val set:  25%|##4       | 666/2671 [07:46<23:51,  1.40it/s]Evaluating on VQA val set:  25%|##4       | 667/2671 [07:46<23:54,  1.40it/s]Evaluating on VQA val set:  25%|##5       | 668/2671 [07:47<24:06,  1.38it/s]Evaluating on VQA val set:  25%|##5       | 669/2671 [07:48<24:07,  1.38it/s]Evaluating on VQA val set:  25%|##5       | 670/2671 [07:49<24:28,  1.36it/s]Evaluating on VQA val set:  25%|##5       | 671/2671 [07:49<23:59,  1.39it/s]Evaluating on VQA val set:  25%|##5       | 672/2671 [07:50<23:48,  1.40it/s]Evaluating on VQA val set:  25%|##5       | 673/2671 [07:51<23:25,  1.42it/s]Evaluating on VQA val set:  25%|##5       | 674/2671 [07:51<23:25,  1.42it/s]Evaluating on VQA val set:  25%|##5       | 675/2671 [07:52<23:28,  1.42it/s]Evaluating on VQA val set:  25%|##5       | 676/2671 [07:53<22:34,  1.47it/s]Evaluating on VQA val set:  25%|##5       | 677/2671 [07:53<22:41,  1.46it/s]Evaluating on VQA val set:  25%|##5       | 678/2671 [07:54<22:48,  1.46it/s]Evaluating on VQA val set:  25%|##5       | 679/2671 [07:55<22:25,  1.48it/s]Evaluating on VQA val set:  25%|##5       | 680/2671 [07:55<23:03,  1.44it/s]Evaluating on VQA val set:  25%|##5       | 681/2671 [07:56<22:30,  1.47it/s]Evaluating on VQA val set:  26%|##5       | 682/2671 [07:57<23:05,  1.44it/s]Evaluating on VQA val set:  26%|##5       | 683/2671 [07:58<24:10,  1.37it/s]Evaluating on VQA val set:  26%|##5       | 684/2671 [07:58<23:37,  1.40it/s]Evaluating on VQA val set:  26%|##5       | 685/2671 [07:59<23:06,  1.43it/s]Evaluating on VQA val set:  26%|##5       | 686/2671 [08:00<23:36,  1.40it/s]Evaluating on VQA val set:  26%|##5       | 687/2671 [08:00<23:25,  1.41it/s]Evaluating on VQA val set:  26%|##5       | 688/2671 [08:01<23:12,  1.42it/s]Evaluating on VQA val set:  26%|##5       | 689/2671 [08:02<23:59,  1.38it/s]Evaluating on VQA val set:  26%|##5       | 690/2671 [08:03<23:47,  1.39it/s]Evaluating on VQA val set:  26%|##5       | 691/2671 [08:03<23:48,  1.39it/s]Evaluating on VQA val set:  26%|##5       | 692/2671 [08:04<23:03,  1.43it/s]Evaluating on VQA val set:  26%|##5       | 693/2671 [08:05<22:19,  1.48it/s]Evaluating on VQA val set:  26%|##5       | 694/2671 [08:05<21:44,  1.52it/s]Evaluating on VQA val set:  26%|##6       | 695/2671 [08:06<22:13,  1.48it/s]Evaluating on VQA val set:  26%|##6       | 696/2671 [08:07<22:11,  1.48it/s]Evaluating on VQA val set:  26%|##6       | 697/2671 [08:07<22:30,  1.46it/s]Evaluating on VQA val set:  26%|##6       | 698/2671 [08:08<22:57,  1.43it/s]Evaluating on VQA val set:  26%|##6       | 699/2671 [08:09<22:58,  1.43it/s]Evaluating on VQA val set:  26%|##6       | 700/2671 [08:09<23:17,  1.41it/s]Evaluating on VQA val set:  26%|##6       | 701/2671 [08:10<23:26,  1.40it/s]Evaluating on VQA val set:  26%|##6       | 702/2671 [08:11<23:21,  1.40it/s]Evaluating on VQA val set:  26%|##6       | 703/2671 [08:12<22:36,  1.45it/s]Evaluating on VQA val set:  26%|##6       | 704/2671 [08:12<23:10,  1.42it/s]Evaluating on VQA val set:  26%|##6       | 705/2671 [08:13<22:36,  1.45it/s]Evaluating on VQA val set:  26%|##6       | 706/2671 [08:14<22:27,  1.46it/s]Evaluating on VQA val set:  26%|##6       | 707/2671 [08:14<22:45,  1.44it/s]Evaluating on VQA val set:  27%|##6       | 708/2671 [08:15<23:03,  1.42it/s]Evaluating on VQA val set:  27%|##6       | 709/2671 [08:16<22:58,  1.42it/s]Evaluating on VQA val set:  27%|##6       | 710/2671 [08:16<23:07,  1.41it/s]Evaluating on VQA val set:  27%|##6       | 711/2671 [08:17<23:50,  1.37it/s]Evaluating on VQA val set:  27%|##6       | 712/2671 [08:18<22:54,  1.42it/s]Evaluating on VQA val set:  27%|##6       | 713/2671 [08:19<23:25,  1.39it/s]Evaluating on VQA val set:  27%|##6       | 714/2671 [08:19<23:54,  1.36it/s]Evaluating on VQA val set:  27%|##6       | 715/2671 [08:20<23:38,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 716/2671 [08:21<23:32,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 717/2671 [08:22<23:43,  1.37it/s]Evaluating on VQA val set:  27%|##6       | 718/2671 [08:22<23:35,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 719/2671 [08:23<23:37,  1.38it/s]Evaluating on VQA val set:  27%|##6       | 720/2671 [08:24<23:52,  1.36it/s]Evaluating on VQA val set:  27%|##6       | 721/2671 [08:25<23:48,  1.36it/s]Evaluating on VQA val set:  27%|##7       | 722/2671 [08:25<23:37,  1.38it/s]Evaluating on VQA val set:  27%|##7       | 723/2671 [08:26<23:01,  1.41it/s]Evaluating on VQA val set:  27%|##7       | 724/2671 [08:27<22:56,  1.41it/s]Evaluating on VQA val set:  27%|##7       | 725/2671 [08:27<22:23,  1.45it/s]Evaluating on VQA val set:  27%|##7       | 726/2671 [08:28<22:52,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 727/2671 [08:29<22:26,  1.44it/s]Evaluating on VQA val set:  27%|##7       | 728/2671 [08:29<22:43,  1.43it/s]Evaluating on VQA val set:  27%|##7       | 729/2671 [08:30<22:06,  1.46it/s]Evaluating on VQA val set:  27%|##7       | 730/2671 [08:31<22:17,  1.45it/s]Evaluating on VQA val set:  27%|##7       | 731/2671 [08:31<22:44,  1.42it/s]Evaluating on VQA val set:  27%|##7       | 732/2671 [08:32<22:56,  1.41it/s]Evaluating on VQA val set:  27%|##7       | 733/2671 [08:33<23:00,  1.40it/s]Evaluating on VQA val set:  27%|##7       | 734/2671 [08:34<23:16,  1.39it/s]Evaluating on VQA val set:  28%|##7       | 735/2671 [08:34<23:20,  1.38it/s]Evaluating on VQA val set:  28%|##7       | 736/2671 [08:35<22:21,  1.44it/s]Evaluating on VQA val set:  28%|##7       | 737/2671 [08:36<22:49,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 738/2671 [08:36<22:15,  1.45it/s]Evaluating on VQA val set:  28%|##7       | 739/2671 [08:37<22:35,  1.43it/s]Evaluating on VQA val set:  28%|##7       | 740/2671 [08:38<22:47,  1.41it/s]Evaluating on VQA val set:  28%|##7       | 741/2671 [08:39<22:36,  1.42it/s]Evaluating on VQA val set:  28%|##7       | 742/2671 [08:39<23:02,  1.40it/s]Evaluating on VQA val set:  28%|##7       | 743/2671 [08:40<23:33,  1.36it/s]Evaluating on VQA val set:  28%|##7       | 744/2671 [08:41<22:19,  1.44it/s]Evaluating on VQA val set:  28%|##7       | 745/2671 [08:41<22:08,  1.45it/s]Evaluating on VQA val set:  28%|##7       | 746/2671 [08:42<21:28,  1.49it/s]Evaluating on VQA val set:  28%|##7       | 747/2671 [08:43<22:03,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 748/2671 [08:43<21:54,  1.46it/s]Evaluating on VQA val set:  28%|##8       | 749/2671 [08:44<21:36,  1.48it/s]Evaluating on VQA val set:  28%|##8       | 750/2671 [08:45<22:07,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 751/2671 [08:45<22:06,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 752/2671 [08:46<22:40,  1.41it/s]Evaluating on VQA val set:  28%|##8       | 753/2671 [08:47<22:53,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 754/2671 [08:48<22:07,  1.44it/s]Evaluating on VQA val set:  28%|##8       | 755/2671 [08:48<22:05,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 756/2671 [08:49<22:02,  1.45it/s]Evaluating on VQA val set:  28%|##8       | 757/2671 [08:50<22:20,  1.43it/s]Evaluating on VQA val set:  28%|##8       | 758/2671 [08:50<23:06,  1.38it/s]Evaluating on VQA val set:  28%|##8       | 759/2671 [08:51<22:44,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 760/2671 [08:52<22:40,  1.40it/s]Evaluating on VQA val set:  28%|##8       | 761/2671 [08:53<22:31,  1.41it/s]Evaluating on VQA val set:  29%|##8       | 762/2671 [08:53<22:38,  1.41it/s]Evaluating on VQA val set:  29%|##8       | 763/2671 [08:54<22:23,  1.42it/s]Evaluating on VQA val set:  29%|##8       | 764/2671 [08:55<22:35,  1.41it/s]Evaluating on VQA val set:  29%|##8       | 765/2671 [08:55<22:37,  1.40it/s]Evaluating on VQA val set:  29%|##8       | 766/2671 [08:56<22:18,  1.42it/s]Evaluating on VQA val set:  29%|##8       | 767/2671 [08:57<22:19,  1.42it/s]Evaluating on VQA val set:  29%|##8       | 768/2671 [08:58<23:18,  1.36it/s]Evaluating on VQA val set:  29%|##8       | 769/2671 [08:58<23:28,  1.35it/s]Evaluating on VQA val set:  29%|##8       | 770/2671 [08:59<23:40,  1.34it/s]Evaluating on VQA val set:  29%|##8       | 771/2671 [09:00<23:40,  1.34it/s]Evaluating on VQA val set:  29%|##8       | 772/2671 [09:01<23:10,  1.37it/s]Evaluating on VQA val set:  29%|##8       | 773/2671 [09:01<22:54,  1.38it/s]Evaluating on VQA val set:  29%|##8       | 774/2671 [09:02<22:57,  1.38it/s]Evaluating on VQA val set:  29%|##9       | 775/2671 [09:03<23:08,  1.37it/s]Evaluating on VQA val set:  29%|##9       | 776/2671 [09:03<22:47,  1.39it/s]Evaluating on VQA val set:  29%|##9       | 777/2671 [09:04<22:00,  1.43it/s]Evaluating on VQA val set:  29%|##9       | 778/2671 [09:05<22:25,  1.41it/s]Evaluating on VQA val set:  29%|##9       | 779/2671 [09:06<22:31,  1.40it/s]Evaluating on VQA val set:  29%|##9       | 780/2671 [09:06<22:33,  1.40it/s]Evaluating on VQA val set:  29%|##9       | 781/2671 [09:07<22:23,  1.41it/s]Evaluating on VQA val set:  29%|##9       | 782/2671 [09:08<22:19,  1.41it/s]Evaluating on VQA val set:  29%|##9       | 783/2671 [09:08<23:00,  1.37it/s]Evaluating on VQA val set:  29%|##9       | 784/2671 [09:09<22:53,  1.37it/s]Evaluating on VQA val set:  29%|##9       | 785/2671 [09:10<22:23,  1.40it/s]Evaluating on VQA val set:  29%|##9       | 786/2671 [09:11<22:34,  1.39it/s]Evaluating on VQA val set:  29%|##9       | 787/2671 [09:11<22:11,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 788/2671 [09:12<21:58,  1.43it/s]Evaluating on VQA val set:  30%|##9       | 789/2671 [09:13<23:11,  1.35it/s]Evaluating on VQA val set:  30%|##9       | 790/2671 [09:14<23:21,  1.34it/s]Evaluating on VQA val set:  30%|##9       | 791/2671 [09:14<22:35,  1.39it/s]Evaluating on VQA val set:  30%|##9       | 792/2671 [09:15<22:30,  1.39it/s]Evaluating on VQA val set:  30%|##9       | 793/2671 [09:16<23:00,  1.36it/s]Evaluating on VQA val set:  30%|##9       | 794/2671 [09:16<21:51,  1.43it/s]Evaluating on VQA val set:  30%|##9       | 795/2671 [09:17<21:50,  1.43it/s]Evaluating on VQA val set:  30%|##9       | 796/2671 [09:18<21:16,  1.47it/s]Evaluating on VQA val set:  30%|##9       | 797/2671 [09:18<21:48,  1.43it/s]Evaluating on VQA val set:  30%|##9       | 798/2671 [09:19<22:00,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 799/2671 [09:20<22:05,  1.41it/s]Evaluating on VQA val set:  30%|##9       | 800/2671 [09:21<22:01,  1.42it/s]Evaluating on VQA val set:  30%|##9       | 801/2671 [09:21<22:22,  1.39it/s]Evaluating on VQA val set:  30%|###       | 802/2671 [09:22<22:38,  1.38it/s]Evaluating on VQA val set:  30%|###       | 803/2671 [09:23<22:09,  1.41it/s]Evaluating on VQA val set:  30%|###       | 804/2671 [09:23<22:38,  1.37it/s]Evaluating on VQA val set:  30%|###       | 805/2671 [09:24<22:31,  1.38it/s]Evaluating on VQA val set:  30%|###       | 806/2671 [09:25<22:42,  1.37it/s]Evaluating on VQA val set:  30%|###       | 807/2671 [09:26<22:46,  1.36it/s]Evaluating on VQA val set:  30%|###       | 808/2671 [09:26<22:01,  1.41it/s]Evaluating on VQA val set:  30%|###       | 809/2671 [09:27<22:12,  1.40it/s]Evaluating on VQA val set:  30%|###       | 810/2671 [09:28<22:32,  1.38it/s]Evaluating on VQA val set:  30%|###       | 811/2671 [09:29<22:45,  1.36it/s]Evaluating on VQA val set:  30%|###       | 812/2671 [09:29<21:37,  1.43it/s]Evaluating on VQA val set:  30%|###       | 813/2671 [09:30<21:30,  1.44it/s]Evaluating on VQA val set:  30%|###       | 814/2671 [09:30<20:22,  1.52it/s]Evaluating on VQA val set:  31%|###       | 815/2671 [09:31<21:11,  1.46it/s]Evaluating on VQA val set:  31%|###       | 816/2671 [09:32<21:25,  1.44it/s]Evaluating on VQA val set:  31%|###       | 817/2671 [09:33<21:41,  1.43it/s]Evaluating on VQA val set:  31%|###       | 818/2671 [09:33<22:36,  1.37it/s]Evaluating on VQA val set:  31%|###       | 819/2671 [09:34<22:09,  1.39it/s]Evaluating on VQA val set:  31%|###       | 820/2671 [09:35<21:57,  1.41it/s]Evaluating on VQA val set:  31%|###       | 821/2671 [09:35<21:19,  1.45it/s]Evaluating on VQA val set:  31%|###       | 822/2671 [09:36<21:51,  1.41it/s]Evaluating on VQA val set:  31%|###       | 823/2671 [09:37<21:37,  1.42it/s]Evaluating on VQA val set:  31%|###       | 824/2671 [09:38<21:45,  1.41it/s]Evaluating on VQA val set:  31%|###       | 825/2671 [09:38<21:53,  1.41it/s]Evaluating on VQA val set:  31%|###       | 826/2671 [09:39<22:23,  1.37it/s]Evaluating on VQA val set:  31%|###       | 827/2671 [09:40<22:11,  1.38it/s]Evaluating on VQA val set:  31%|###       | 828/2671 [09:40<22:19,  1.38it/s]Evaluating on VQA val set:  31%|###1      | 829/2671 [09:41<22:43,  1.35it/s]Evaluating on VQA val set:  31%|###1      | 830/2671 [09:42<22:11,  1.38it/s]Evaluating on VQA val set:  31%|###1      | 831/2671 [09:43<22:20,  1.37it/s]Evaluating on VQA val set:  31%|###1      | 832/2671 [09:43<22:04,  1.39it/s]Evaluating on VQA val set:  31%|###1      | 833/2671 [09:44<22:32,  1.36it/s]Evaluating on VQA val set:  31%|###1      | 834/2671 [09:45<22:57,  1.33it/s]Evaluating on VQA val set:  31%|###1      | 835/2671 [09:46<22:53,  1.34it/s]Evaluating on VQA val set:  31%|###1      | 836/2671 [09:46<22:34,  1.35it/s]Evaluating on VQA val set:  31%|###1      | 837/2671 [09:47<22:11,  1.38it/s]Evaluating on VQA val set:  31%|###1      | 838/2671 [09:48<22:29,  1.36it/s]Evaluating on VQA val set:  31%|###1      | 839/2671 [09:49<21:48,  1.40it/s]Evaluating on VQA val set:  31%|###1      | 840/2671 [09:49<21:29,  1.42it/s]Evaluating on VQA val set:  31%|###1      | 841/2671 [09:50<21:51,  1.40it/s]Evaluating on VQA val set:  32%|###1      | 842/2671 [09:51<21:51,  1.39it/s]Evaluating on VQA val set:  32%|###1      | 843/2671 [09:51<21:13,  1.44it/s]Evaluating on VQA val set:  32%|###1      | 844/2671 [09:52<19:43,  1.54it/s]Evaluating on VQA val set:  32%|###1      | 845/2671 [09:53<19:51,  1.53it/s]Evaluating on VQA val set:  32%|###1      | 846/2671 [09:53<20:24,  1.49it/s]Evaluating on VQA val set:  32%|###1      | 847/2671 [09:54<20:03,  1.52it/s]Evaluating on VQA val set:  32%|###1      | 848/2671 [09:54<19:41,  1.54it/s]Evaluating on VQA val set:  32%|###1      | 849/2671 [09:55<20:41,  1.47it/s]Evaluating on VQA val set:  32%|###1      | 850/2671 [09:56<21:09,  1.43it/s]Evaluating on VQA val set:  32%|###1      | 851/2671 [09:57<21:22,  1.42it/s]Evaluating on VQA val set:  32%|###1      | 852/2671 [09:57<21:41,  1.40it/s]Evaluating on VQA val set:  32%|###1      | 853/2671 [09:58<21:29,  1.41it/s]Evaluating on VQA val set:  32%|###1      | 854/2671 [09:59<21:17,  1.42it/s]Evaluating on VQA val set:  32%|###2      | 855/2671 [10:00<21:38,  1.40it/s]Evaluating on VQA val set:  32%|###2      | 856/2671 [10:00<21:14,  1.42it/s]Evaluating on VQA val set:  32%|###2      | 857/2671 [10:01<21:09,  1.43it/s]Evaluating on VQA val set:  32%|###2      | 858/2671 [10:02<22:02,  1.37it/s]Evaluating on VQA val set:  32%|###2      | 859/2671 [10:02<22:06,  1.37it/s]Evaluating on VQA val set:  32%|###2      | 860/2671 [10:03<21:54,  1.38it/s]Evaluating on VQA val set:  32%|###2      | 861/2671 [10:04<21:54,  1.38it/s]Evaluating on VQA val set:  32%|###2      | 862/2671 [10:05<20:58,  1.44it/s]Evaluating on VQA val set:  32%|###2      | 863/2671 [10:05<21:42,  1.39it/s]Evaluating on VQA val set:  32%|###2      | 864/2671 [10:06<21:45,  1.38it/s]Evaluating on VQA val set:  32%|###2      | 865/2671 [10:07<21:13,  1.42it/s]Evaluating on VQA val set:  32%|###2      | 866/2671 [10:07<21:00,  1.43it/s]Evaluating on VQA val set:  32%|###2      | 867/2671 [10:08<21:02,  1.43it/s]Evaluating on VQA val set:  32%|###2      | 868/2671 [10:09<20:31,  1.46it/s]Evaluating on VQA val set:  33%|###2      | 869/2671 [10:09<20:29,  1.47it/s]Evaluating on VQA val set:  33%|###2      | 870/2671 [10:10<20:16,  1.48it/s]Evaluating on VQA val set:  33%|###2      | 871/2671 [10:11<20:47,  1.44it/s]Evaluating on VQA val set:  33%|###2      | 872/2671 [10:11<20:19,  1.48it/s]Evaluating on VQA val set:  33%|###2      | 873/2671 [10:12<20:53,  1.43it/s]Evaluating on VQA val set:  33%|###2      | 874/2671 [10:13<20:23,  1.47it/s]Evaluating on VQA val set:  33%|###2      | 875/2671 [10:14<20:31,  1.46it/s]Evaluating on VQA val set:  33%|###2      | 876/2671 [10:14<21:16,  1.41it/s]Evaluating on VQA val set:  33%|###2      | 877/2671 [10:15<21:17,  1.40it/s]Evaluating on VQA val set:  33%|###2      | 878/2671 [10:16<21:20,  1.40it/s]Evaluating on VQA val set:  33%|###2      | 879/2671 [10:16<21:19,  1.40it/s]Evaluating on VQA val set:  33%|###2      | 880/2671 [10:17<21:23,  1.40it/s]Evaluating on VQA val set:  33%|###2      | 881/2671 [10:18<20:59,  1.42it/s]Evaluating on VQA val set:  33%|###3      | 882/2671 [10:19<21:02,  1.42it/s]Evaluating on VQA val set:  33%|###3      | 883/2671 [10:19<20:54,  1.43it/s]Evaluating on VQA val set:  33%|###3      | 884/2671 [10:20<19:55,  1.49it/s]Evaluating on VQA val set:  33%|###3      | 885/2671 [10:21<20:20,  1.46it/s]Evaluating on VQA val set:  33%|###3      | 886/2671 [10:21<20:38,  1.44it/s]Evaluating on VQA val set:  33%|###3      | 887/2671 [10:22<19:55,  1.49it/s]Evaluating on VQA val set:  33%|###3      | 888/2671 [10:23<20:44,  1.43it/s]Evaluating on VQA val set:  33%|###3      | 889/2671 [10:23<20:28,  1.45it/s]Evaluating on VQA val set:  33%|###3      | 890/2671 [10:24<20:30,  1.45it/s]Evaluating on VQA val set:  33%|###3      | 891/2671 [10:25<21:31,  1.38it/s]Evaluating on VQA val set:  33%|###3      | 892/2671 [10:25<21:01,  1.41it/s]Evaluating on VQA val set:  33%|###3      | 893/2671 [10:26<21:02,  1.41it/s]Evaluating on VQA val set:  33%|###3      | 894/2671 [10:27<19:54,  1.49it/s]Evaluating on VQA val set:  34%|###3      | 895/2671 [10:27<20:10,  1.47it/s]Evaluating on VQA val set:  34%|###3      | 896/2671 [10:28<20:20,  1.45it/s]Evaluating on VQA val set:  34%|###3      | 897/2671 [10:29<21:10,  1.40it/s]Evaluating on VQA val set:  34%|###3      | 898/2671 [10:30<21:27,  1.38it/s]Evaluating on VQA val set:  34%|###3      | 899/2671 [10:30<21:28,  1.37it/s]Evaluating on VQA val set:  34%|###3      | 900/2671 [10:31<20:42,  1.43it/s]Evaluating on VQA val set:  34%|###3      | 901/2671 [10:32<20:57,  1.41it/s]Evaluating on VQA val set:  34%|###3      | 902/2671 [10:33<21:23,  1.38it/s]Evaluating on VQA val set:  34%|###3      | 903/2671 [10:33<21:34,  1.37it/s]Evaluating on VQA val set:  34%|###3      | 904/2671 [10:34<21:30,  1.37it/s]Evaluating on VQA val set:  34%|###3      | 905/2671 [10:35<19:36,  1.50it/s]Evaluating on VQA val set:  34%|###3      | 906/2671 [10:35<19:29,  1.51it/s]Evaluating on VQA val set:  34%|###3      | 907/2671 [10:36<20:06,  1.46it/s]Evaluating on VQA val set:  34%|###3      | 908/2671 [10:37<20:04,  1.46it/s]Evaluating on VQA val set:  34%|###4      | 909/2671 [10:37<19:46,  1.48it/s]Evaluating on VQA val set:  34%|###4      | 910/2671 [10:38<19:42,  1.49it/s]Evaluating on VQA val set:  34%|###4      | 911/2671 [10:39<19:19,  1.52it/s]Evaluating on VQA val set:  34%|###4      | 912/2671 [10:39<19:53,  1.47it/s]Evaluating on VQA val set:  34%|###4      | 913/2671 [10:40<20:08,  1.46it/s]Evaluating on VQA val set:  34%|###4      | 914/2671 [10:41<20:19,  1.44it/s]Evaluating on VQA val set:  34%|###4      | 915/2671 [10:41<19:49,  1.48it/s]Evaluating on VQA val set:  34%|###4      | 916/2671 [10:42<19:52,  1.47it/s]Evaluating on VQA val set:  34%|###4      | 917/2671 [10:43<20:12,  1.45it/s]Evaluating on VQA val set:  34%|###4      | 918/2671 [10:44<21:06,  1.38it/s]Evaluating on VQA val set:  34%|###4      | 919/2671 [10:44<21:45,  1.34it/s]Evaluating on VQA val set:  34%|###4      | 920/2671 [10:45<21:18,  1.37it/s]Evaluating on VQA val set:  34%|###4      | 921/2671 [10:46<21:30,  1.36it/s]Evaluating on VQA val set:  35%|###4      | 922/2671 [10:47<21:44,  1.34it/s]Evaluating on VQA val set:  35%|###4      | 923/2671 [10:47<21:22,  1.36it/s]Evaluating on VQA val set:  35%|###4      | 924/2671 [10:48<21:23,  1.36it/s]Evaluating on VQA val set:  35%|###4      | 925/2671 [10:49<21:21,  1.36it/s]Evaluating on VQA val set:  35%|###4      | 926/2671 [10:49<20:49,  1.40it/s]Evaluating on VQA val set:  35%|###4      | 927/2671 [10:50<20:51,  1.39it/s]Evaluating on VQA val set:  35%|###4      | 928/2671 [10:51<20:16,  1.43it/s]Evaluating on VQA val set:  35%|###4      | 929/2671 [10:51<19:43,  1.47it/s]Evaluating on VQA val set:  35%|###4      | 930/2671 [10:52<19:44,  1.47it/s]Evaluating on VQA val set:  35%|###4      | 931/2671 [10:53<19:21,  1.50it/s]Evaluating on VQA val set:  35%|###4      | 932/2671 [10:53<19:48,  1.46it/s]Evaluating on VQA val set:  35%|###4      | 933/2671 [10:54<20:18,  1.43it/s]Evaluating on VQA val set:  35%|###4      | 934/2671 [10:55<20:16,  1.43it/s]Evaluating on VQA val set:  35%|###5      | 935/2671 [10:56<20:09,  1.44it/s]Evaluating on VQA val set:  35%|###5      | 936/2671 [10:56<20:34,  1.41it/s]Evaluating on VQA val set:  35%|###5      | 937/2671 [10:57<21:10,  1.36it/s]Evaluating on VQA val set:  35%|###5      | 938/2671 [10:58<20:13,  1.43it/s]Evaluating on VQA val set:  35%|###5      | 939/2671 [10:58<20:22,  1.42it/s]Evaluating on VQA val set:  35%|###5      | 940/2671 [10:59<19:36,  1.47it/s]Evaluating on VQA val set:  35%|###5      | 941/2671 [11:00<19:58,  1.44it/s]Evaluating on VQA val set:  35%|###5      | 942/2671 [11:01<20:16,  1.42it/s]Evaluating on VQA val set:  35%|###5      | 943/2671 [11:01<19:56,  1.44it/s]Evaluating on VQA val set:  35%|###5      | 944/2671 [11:02<19:04,  1.51it/s]Evaluating on VQA val set:  35%|###5      | 945/2671 [11:02<19:07,  1.50it/s]Evaluating on VQA val set:  35%|###5      | 946/2671 [11:03<19:16,  1.49it/s]Evaluating on VQA val set:  35%|###5      | 947/2671 [11:04<19:01,  1.51it/s]Evaluating on VQA val set:  35%|###5      | 948/2671 [11:05<19:12,  1.50it/s]Evaluating on VQA val set:  36%|###5      | 949/2671 [11:05<19:16,  1.49it/s]Evaluating on VQA val set:  36%|###5      | 950/2671 [11:06<19:40,  1.46it/s]Evaluating on VQA val set:  36%|###5      | 951/2671 [11:07<19:50,  1.44it/s]Evaluating on VQA val set:  36%|###5      | 952/2671 [11:07<20:03,  1.43it/s]Evaluating on VQA val set:  36%|###5      | 953/2671 [11:08<20:12,  1.42it/s]Evaluating on VQA val set:  36%|###5      | 954/2671 [11:09<20:03,  1.43it/s]Evaluating on VQA val set:  36%|###5      | 955/2671 [11:09<20:29,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 956/2671 [11:10<20:28,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 957/2671 [11:11<20:25,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 958/2671 [11:12<21:12,  1.35it/s]Evaluating on VQA val set:  36%|###5      | 959/2671 [11:12<20:24,  1.40it/s]Evaluating on VQA val set:  36%|###5      | 960/2671 [11:13<20:33,  1.39it/s]Evaluating on VQA val set:  36%|###5      | 961/2671 [11:14<20:16,  1.41it/s]Evaluating on VQA val set:  36%|###6      | 962/2671 [11:15<20:47,  1.37it/s]Evaluating on VQA val set:  36%|###6      | 963/2671 [11:15<20:05,  1.42it/s]Evaluating on VQA val set:  36%|###6      | 964/2671 [11:16<19:05,  1.49it/s]Evaluating on VQA val set:  36%|###6      | 965/2671 [11:17<19:28,  1.46it/s]Evaluating on VQA val set:  36%|###6      | 966/2671 [11:17<19:58,  1.42it/s]Evaluating on VQA val set:  36%|###6      | 967/2671 [11:18<20:23,  1.39it/s]Evaluating on VQA val set:  36%|###6      | 968/2671 [11:19<20:24,  1.39it/s]Evaluating on VQA val set:  36%|###6      | 969/2671 [11:20<20:58,  1.35it/s]Evaluating on VQA val set:  36%|###6      | 970/2671 [11:20<20:46,  1.36it/s]Evaluating on VQA val set:  36%|###6      | 971/2671 [11:21<20:18,  1.40it/s]Evaluating on VQA val set:  36%|###6      | 972/2671 [11:22<20:04,  1.41it/s]Evaluating on VQA val set:  36%|###6      | 973/2671 [11:22<19:58,  1.42it/s]Evaluating on VQA val set:  36%|###6      | 974/2671 [11:23<19:49,  1.43it/s]Evaluating on VQA val set:  37%|###6      | 975/2671 [11:24<20:07,  1.40it/s]Evaluating on VQA val set:  37%|###6      | 976/2671 [11:24<19:54,  1.42it/s]Evaluating on VQA val set:  37%|###6      | 977/2671 [11:25<19:35,  1.44it/s]Evaluating on VQA val set:  37%|###6      | 978/2671 [11:26<19:37,  1.44it/s]Evaluating on VQA val set:  37%|###6      | 979/2671 [11:27<19:52,  1.42it/s]Evaluating on VQA val set:  37%|###6      | 980/2671 [11:27<19:59,  1.41it/s]Evaluating on VQA val set:  37%|###6      | 981/2671 [11:28<19:54,  1.42it/s]Evaluating on VQA val set:  37%|###6      | 982/2671 [11:29<18:34,  1.52it/s]Evaluating on VQA val set:  37%|###6      | 983/2671 [11:29<18:33,  1.52it/s]Evaluating on VQA val set:  37%|###6      | 984/2671 [11:30<19:16,  1.46it/s]Evaluating on VQA val set:  37%|###6      | 985/2671 [11:31<19:19,  1.45it/s]Evaluating on VQA val set:  37%|###6      | 986/2671 [11:31<19:17,  1.46it/s]Evaluating on VQA val set:  37%|###6      | 987/2671 [11:32<19:15,  1.46it/s]Evaluating on VQA val set:  37%|###6      | 988/2671 [11:33<19:16,  1.46it/s]Evaluating on VQA val set:  37%|###7      | 989/2671 [11:33<19:38,  1.43it/s]Evaluating on VQA val set:  37%|###7      | 990/2671 [11:34<19:25,  1.44it/s]Evaluating on VQA val set:  37%|###7      | 991/2671 [11:35<19:32,  1.43it/s]Evaluating on VQA val set:  37%|###7      | 992/2671 [11:36<19:46,  1.41it/s]Evaluating on VQA val set:  37%|###7      | 993/2671 [11:36<18:59,  1.47it/s]Evaluating on VQA val set:  37%|###7      | 994/2671 [11:37<19:09,  1.46it/s]Evaluating on VQA val set:  37%|###7      | 995/2671 [11:37<19:06,  1.46it/s]Evaluating on VQA val set:  37%|###7      | 996/2671 [11:38<18:52,  1.48it/s]Evaluating on VQA val set:  37%|###7      | 997/2671 [11:39<18:45,  1.49it/s]Evaluating on VQA val set:  37%|###7      | 998/2671 [11:39<18:39,  1.49it/s]Evaluating on VQA val set:  37%|###7      | 999/2671 [11:40<18:05,  1.54it/s]Evaluating on VQA val set:  37%|###7      | 1000/2671 [11:41<17:51,  1.56it/s]Evaluating on VQA val set:  37%|###7      | 1001/2671 [11:41<18:18,  1.52it/s]Evaluating on VQA val set:  38%|###7      | 1002/2671 [11:42<18:35,  1.50it/s]Evaluating on VQA val set:  38%|###7      | 1003/2671 [11:43<19:08,  1.45it/s]Evaluating on VQA val set:  38%|###7      | 1004/2671 [11:44<19:13,  1.45it/s]Evaluating on VQA val set:  38%|###7      | 1005/2671 [11:44<19:37,  1.41it/s]Evaluating on VQA val set:  38%|###7      | 1006/2671 [11:45<19:35,  1.42it/s]Evaluating on VQA val set:  38%|###7      | 1007/2671 [11:46<20:01,  1.38it/s]Evaluating on VQA val set:  38%|###7      | 1008/2671 [11:47<20:39,  1.34it/s]Evaluating on VQA val set:  38%|###7      | 1009/2671 [11:47<20:43,  1.34it/s]Evaluating on VQA val set:  38%|###7      | 1010/2671 [11:48<20:32,  1.35it/s]Evaluating on VQA val set:  38%|###7      | 1011/2671 [11:49<20:22,  1.36it/s]Evaluating on VQA val set:  38%|###7      | 1012/2671 [11:49<19:44,  1.40it/s]Evaluating on VQA val set:  38%|###7      | 1013/2671 [11:50<19:58,  1.38it/s]Evaluating on VQA val set:  38%|###7      | 1014/2671 [11:51<19:50,  1.39it/s]Evaluating on VQA val set:  38%|###8      | 1015/2671 [11:52<20:13,  1.37it/s]Evaluating on VQA val set:  38%|###8      | 1016/2671 [11:52<20:13,  1.36it/s]Evaluating on VQA val set:  38%|###8      | 1017/2671 [11:53<19:45,  1.40it/s]Evaluating on VQA val set:  38%|###8      | 1018/2671 [11:54<19:55,  1.38it/s]Evaluating on VQA val set:  38%|###8      | 1019/2671 [11:55<20:13,  1.36it/s]Evaluating on VQA val set:  38%|###8      | 1020/2671 [11:55<19:25,  1.42it/s]Evaluating on VQA val set:  38%|###8      | 1021/2671 [11:56<19:22,  1.42it/s]Evaluating on VQA val set:  38%|###8      | 1022/2671 [11:57<19:11,  1.43it/s]Evaluating on VQA val set:  38%|###8      | 1023/2671 [11:57<19:10,  1.43it/s]Evaluating on VQA val set:  38%|###8      | 1024/2671 [11:58<18:05,  1.52it/s]Evaluating on VQA val set:  38%|###8      | 1025/2671 [11:59<18:29,  1.48it/s]Evaluating on VQA val set:  38%|###8      | 1026/2671 [11:59<18:51,  1.45it/s]Evaluating on VQA val set:  38%|###8      | 1027/2671 [12:00<19:11,  1.43it/s]Evaluating on VQA val set:  38%|###8      | 1028/2671 [12:01<19:28,  1.41it/s]Evaluating on VQA val set:  39%|###8      | 1029/2671 [12:01<19:05,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1030/2671 [12:02<19:03,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1031/2671 [12:03<19:47,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1032/2671 [12:04<19:47,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1033/2671 [12:04<18:56,  1.44it/s]Evaluating on VQA val set:  39%|###8      | 1034/2671 [12:05<19:06,  1.43it/s]Evaluating on VQA val set:  39%|###8      | 1035/2671 [12:06<19:14,  1.42it/s]Evaluating on VQA val set:  39%|###8      | 1036/2671 [12:06<19:37,  1.39it/s]Evaluating on VQA val set:  39%|###8      | 1037/2671 [12:07<19:42,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1038/2671 [12:08<19:27,  1.40it/s]Evaluating on VQA val set:  39%|###8      | 1039/2671 [12:09<19:40,  1.38it/s]Evaluating on VQA val set:  39%|###8      | 1040/2671 [12:09<20:05,  1.35it/s]Evaluating on VQA val set:  39%|###8      | 1041/2671 [12:10<20:01,  1.36it/s]Evaluating on VQA val set:  39%|###9      | 1042/2671 [12:11<19:15,  1.41it/s]Evaluating on VQA val set:  39%|###9      | 1043/2671 [12:11<18:46,  1.44it/s]Evaluating on VQA val set:  39%|###9      | 1044/2671 [12:12<19:17,  1.41it/s]Evaluating on VQA val set:  39%|###9      | 1045/2671 [12:13<19:34,  1.38it/s]Evaluating on VQA val set:  39%|###9      | 1046/2671 [12:14<18:46,  1.44it/s]Evaluating on VQA val set:  39%|###9      | 1047/2671 [12:14<18:51,  1.44it/s]Evaluating on VQA val set:  39%|###9      | 1048/2671 [12:15<19:01,  1.42it/s]Evaluating on VQA val set:  39%|###9      | 1049/2671 [12:16<18:58,  1.42it/s]Evaluating on VQA val set:  39%|###9      | 1050/2671 [12:16<19:28,  1.39it/s]Evaluating on VQA val set:  39%|###9      | 1051/2671 [12:17<19:15,  1.40it/s]Evaluating on VQA val set:  39%|###9      | 1052/2671 [12:18<19:29,  1.38it/s]Evaluating on VQA val set:  39%|###9      | 1053/2671 [12:19<19:04,  1.41it/s]Evaluating on VQA val set:  39%|###9      | 1054/2671 [12:19<18:54,  1.43it/s]Evaluating on VQA val set:  39%|###9      | 1055/2671 [12:20<19:20,  1.39it/s]Evaluating on VQA val set:  40%|###9      | 1056/2671 [12:21<18:47,  1.43it/s]Evaluating on VQA val set:  40%|###9      | 1057/2671 [12:21<19:02,  1.41it/s]Evaluating on VQA val set:  40%|###9      | 1058/2671 [12:22<18:55,  1.42it/s]Evaluating on VQA val set:  40%|###9      | 1059/2671 [12:23<19:10,  1.40it/s]Evaluating on VQA val set:  40%|###9      | 1060/2671 [12:24<19:23,  1.38it/s]Evaluating on VQA val set:  40%|###9      | 1061/2671 [12:24<19:09,  1.40it/s]Evaluating on VQA val set:  40%|###9      | 1062/2671 [12:25<19:08,  1.40it/s]Evaluating on VQA val set:  40%|###9      | 1063/2671 [12:26<18:38,  1.44it/s]Evaluating on VQA val set:  40%|###9      | 1064/2671 [12:26<18:49,  1.42it/s]Evaluating on VQA val set:  40%|###9      | 1065/2671 [12:27<18:23,  1.46it/s]Evaluating on VQA val set:  40%|###9      | 1066/2671 [12:28<18:10,  1.47it/s]Evaluating on VQA val set:  40%|###9      | 1067/2671 [12:28<18:33,  1.44it/s]Evaluating on VQA val set:  40%|###9      | 1068/2671 [12:29<18:54,  1.41it/s]Evaluating on VQA val set:  40%|####      | 1069/2671 [12:30<18:46,  1.42it/s]Evaluating on VQA val set:  40%|####      | 1070/2671 [12:31<19:04,  1.40it/s]Evaluating on VQA val set:  40%|####      | 1071/2671 [12:31<19:14,  1.39it/s]Evaluating on VQA val set:  40%|####      | 1072/2671 [12:32<18:56,  1.41it/s]Evaluating on VQA val set:  40%|####      | 1073/2671 [12:33<19:20,  1.38it/s]Evaluating on VQA val set:  40%|####      | 1074/2671 [12:33<19:49,  1.34it/s]Evaluating on VQA val set:  40%|####      | 1075/2671 [12:34<19:40,  1.35it/s]Evaluating on VQA val set:  40%|####      | 1076/2671 [12:35<19:26,  1.37it/s]Evaluating on VQA val set:  40%|####      | 1077/2671 [12:36<18:44,  1.42it/s]Evaluating on VQA val set:  40%|####      | 1078/2671 [12:36<17:42,  1.50it/s]Evaluating on VQA val set:  40%|####      | 1079/2671 [12:37<17:56,  1.48it/s]Evaluating on VQA val set:  40%|####      | 1080/2671 [12:38<18:39,  1.42it/s]Evaluating on VQA val set:  40%|####      | 1081/2671 [12:38<18:14,  1.45it/s]Evaluating on VQA val set:  41%|####      | 1082/2671 [12:39<18:26,  1.44it/s]Evaluating on VQA val set:  41%|####      | 1083/2671 [12:40<18:59,  1.39it/s]Evaluating on VQA val set:  41%|####      | 1084/2671 [12:40<19:07,  1.38it/s]Evaluating on VQA val set:  41%|####      | 1085/2671 [12:41<18:46,  1.41it/s]Evaluating on VQA val set:  41%|####      | 1086/2671 [12:42<19:01,  1.39it/s]Evaluating on VQA val set:  41%|####      | 1087/2671 [12:43<18:18,  1.44it/s]Evaluating on VQA val set:  41%|####      | 1088/2671 [12:43<18:42,  1.41it/s]Evaluating on VQA val set:  41%|####      | 1089/2671 [12:44<18:42,  1.41it/s]Evaluating on VQA val set:  41%|####      | 1090/2671 [12:45<19:21,  1.36it/s]Evaluating on VQA val set:  41%|####      | 1091/2671 [12:45<18:58,  1.39it/s]Evaluating on VQA val set:  41%|####      | 1092/2671 [12:46<18:51,  1.40it/s]Evaluating on VQA val set:  41%|####      | 1093/2671 [12:47<18:46,  1.40it/s]Evaluating on VQA val set:  41%|####      | 1094/2671 [12:48<18:55,  1.39it/s]Evaluating on VQA val set:  41%|####      | 1095/2671 [12:48<17:44,  1.48it/s]Evaluating on VQA val set:  41%|####1     | 1096/2671 [12:49<17:53,  1.47it/s]Evaluating on VQA val set:  41%|####1     | 1097/2671 [12:50<18:17,  1.43it/s]Evaluating on VQA val set:  41%|####1     | 1098/2671 [12:50<18:46,  1.40it/s]Evaluating on VQA val set:  41%|####1     | 1099/2671 [12:51<17:47,  1.47it/s]Evaluating on VQA val set:  41%|####1     | 1100/2671 [12:52<18:14,  1.44it/s]Evaluating on VQA val set:  41%|####1     | 1101/2671 [12:52<18:27,  1.42it/s]Evaluating on VQA val set:  41%|####1     | 1102/2671 [12:53<17:56,  1.46it/s]Evaluating on VQA val set:  41%|####1     | 1103/2671 [12:54<18:30,  1.41it/s]Evaluating on VQA val set:  41%|####1     | 1104/2671 [12:55<18:37,  1.40it/s]Evaluating on VQA val set:  41%|####1     | 1105/2671 [12:55<18:30,  1.41it/s]Evaluating on VQA val set:  41%|####1     | 1106/2671 [12:56<18:39,  1.40it/s]Evaluating on VQA val set:  41%|####1     | 1107/2671 [12:57<18:14,  1.43it/s]Evaluating on VQA val set:  41%|####1     | 1108/2671 [12:57<17:54,  1.45it/s]Evaluating on VQA val set:  42%|####1     | 1109/2671 [12:58<18:10,  1.43it/s]Evaluating on VQA val set:  42%|####1     | 1110/2671 [12:59<18:39,  1.39it/s]Evaluating on VQA val set:  42%|####1     | 1111/2671 [12:59<18:23,  1.41it/s]Evaluating on VQA val set:  42%|####1     | 1112/2671 [13:00<18:48,  1.38it/s]Evaluating on VQA val set:  42%|####1     | 1113/2671 [13:01<18:29,  1.40it/s]Evaluating on VQA val set:  42%|####1     | 1114/2671 [13:02<17:57,  1.45it/s]Evaluating on VQA val set:  42%|####1     | 1115/2671 [13:02<18:21,  1.41it/s]Evaluating on VQA val set:  42%|####1     | 1116/2671 [13:03<18:10,  1.43it/s]Evaluating on VQA val set:  42%|####1     | 1117/2671 [13:04<17:31,  1.48it/s]Evaluating on VQA val set:  42%|####1     | 1118/2671 [13:04<17:46,  1.46it/s]Evaluating on VQA val set:  42%|####1     | 1119/2671 [13:05<17:59,  1.44it/s]Evaluating on VQA val set:  42%|####1     | 1120/2671 [13:06<18:15,  1.42it/s]Evaluating on VQA val set:  42%|####1     | 1121/2671 [13:06<18:15,  1.41it/s]Evaluating on VQA val set:  42%|####2     | 1122/2671 [13:07<18:24,  1.40it/s]Evaluating on VQA val set:  42%|####2     | 1123/2671 [13:08<17:28,  1.48it/s]Evaluating on VQA val set:  42%|####2     | 1124/2671 [13:08<17:03,  1.51it/s]Evaluating on VQA val set:  42%|####2     | 1125/2671 [13:09<17:09,  1.50it/s]Evaluating on VQA val set:  42%|####2     | 1126/2671 [13:10<16:52,  1.53it/s]Evaluating on VQA val set:  42%|####2     | 1127/2671 [13:10<16:51,  1.53it/s]Evaluating on VQA val set:  42%|####2     | 1128/2671 [13:11<17:15,  1.49it/s]Evaluating on VQA val set:  42%|####2     | 1129/2671 [13:12<17:17,  1.49it/s]Evaluating on VQA val set:  42%|####2     | 1130/2671 [13:12<17:32,  1.46it/s]Evaluating on VQA val set:  42%|####2     | 1131/2671 [13:13<18:01,  1.42it/s]Evaluating on VQA val set:  42%|####2     | 1132/2671 [13:14<18:02,  1.42it/s]Evaluating on VQA val set:  42%|####2     | 1133/2671 [13:15<18:04,  1.42it/s]Evaluating on VQA val set:  42%|####2     | 1134/2671 [13:15<17:47,  1.44it/s]Evaluating on VQA val set:  42%|####2     | 1135/2671 [13:16<18:25,  1.39it/s]Evaluating on VQA val set:  43%|####2     | 1136/2671 [13:17<18:26,  1.39it/s]Evaluating on VQA val set:  43%|####2     | 1137/2671 [13:18<18:51,  1.36it/s]Evaluating on VQA val set:  43%|####2     | 1138/2671 [13:18<18:13,  1.40it/s]Evaluating on VQA val set:  43%|####2     | 1139/2671 [13:19<18:15,  1.40it/s]Evaluating on VQA val set:  43%|####2     | 1140/2671 [13:20<18:26,  1.38it/s]Evaluating on VQA val set:  43%|####2     | 1141/2671 [13:20<18:15,  1.40it/s]Evaluating on VQA val set:  43%|####2     | 1142/2671 [13:21<17:34,  1.45it/s]Evaluating on VQA val set:  43%|####2     | 1143/2671 [13:22<17:25,  1.46it/s]Evaluating on VQA val set:  43%|####2     | 1144/2671 [13:22<17:48,  1.43it/s]Evaluating on VQA val set:  43%|####2     | 1145/2671 [13:23<17:32,  1.45it/s]Evaluating on VQA val set:  43%|####2     | 1146/2671 [13:24<17:41,  1.44it/s]Evaluating on VQA val set:  43%|####2     | 1147/2671 [13:25<17:35,  1.44it/s]Evaluating on VQA val set:  43%|####2     | 1148/2671 [13:25<17:42,  1.43it/s]Evaluating on VQA val set:  43%|####3     | 1149/2671 [13:26<18:08,  1.40it/s]Evaluating on VQA val set:  43%|####3     | 1150/2671 [13:27<17:51,  1.42it/s]Evaluating on VQA val set:  43%|####3     | 1151/2671 [13:27<17:52,  1.42it/s]Evaluating on VQA val set:  43%|####3     | 1152/2671 [13:28<17:28,  1.45it/s]Evaluating on VQA val set:  43%|####3     | 1153/2671 [13:29<16:52,  1.50it/s]Evaluating on VQA val set:  43%|####3     | 1154/2671 [13:29<17:39,  1.43it/s]Evaluating on VQA val set:  43%|####3     | 1155/2671 [13:30<18:03,  1.40it/s]Evaluating on VQA val set:  43%|####3     | 1156/2671 [13:31<18:09,  1.39it/s]Evaluating on VQA val set:  43%|####3     | 1157/2671 [13:32<17:57,  1.41it/s]Evaluating on VQA val set:  43%|####3     | 1158/2671 [13:32<17:50,  1.41it/s]Evaluating on VQA val set:  43%|####3     | 1159/2671 [13:33<17:29,  1.44it/s]Evaluating on VQA val set:  43%|####3     | 1160/2671 [13:34<17:17,  1.46it/s]Evaluating on VQA val set:  43%|####3     | 1161/2671 [13:34<17:28,  1.44it/s]Evaluating on VQA val set:  44%|####3     | 1162/2671 [13:35<17:48,  1.41it/s]Evaluating on VQA val set:  44%|####3     | 1163/2671 [13:36<17:59,  1.40it/s]Evaluating on VQA val set:  44%|####3     | 1164/2671 [13:37<18:11,  1.38it/s]Evaluating on VQA val set:  44%|####3     | 1165/2671 [13:37<18:34,  1.35it/s]Evaluating on VQA val set:  44%|####3     | 1166/2671 [13:38<18:11,  1.38it/s]Evaluating on VQA val set:  44%|####3     | 1167/2671 [13:39<18:13,  1.37it/s]Evaluating on VQA val set:  44%|####3     | 1168/2671 [13:39<17:20,  1.44it/s]Evaluating on VQA val set:  44%|####3     | 1169/2671 [13:40<16:53,  1.48it/s]Evaluating on VQA val set:  44%|####3     | 1170/2671 [13:41<17:32,  1.43it/s]Evaluating on VQA val set:  44%|####3     | 1171/2671 [13:41<17:53,  1.40it/s]Evaluating on VQA val set:  44%|####3     | 1172/2671 [13:42<18:07,  1.38it/s]Evaluating on VQA val set:  44%|####3     | 1173/2671 [13:43<18:02,  1.38it/s]Evaluating on VQA val set:  44%|####3     | 1174/2671 [13:44<17:53,  1.39it/s]Evaluating on VQA val set:  44%|####3     | 1175/2671 [13:44<17:56,  1.39it/s]Evaluating on VQA val set:  44%|####4     | 1176/2671 [13:45<18:04,  1.38it/s]Evaluating on VQA val set:  44%|####4     | 1177/2671 [13:46<17:21,  1.43it/s]Evaluating on VQA val set:  44%|####4     | 1178/2671 [13:46<17:18,  1.44it/s]Evaluating on VQA val set:  44%|####4     | 1179/2671 [13:47<16:55,  1.47it/s]Evaluating on VQA val set:  44%|####4     | 1180/2671 [13:48<16:47,  1.48it/s]Evaluating on VQA val set:  44%|####4     | 1181/2671 [13:49<17:16,  1.44it/s]Evaluating on VQA val set:  44%|####4     | 1182/2671 [13:49<16:57,  1.46it/s]Evaluating on VQA val set:  44%|####4     | 1183/2671 [13:50<16:26,  1.51it/s]Evaluating on VQA val set:  44%|####4     | 1184/2671 [13:50<16:51,  1.47it/s]Evaluating on VQA val set:  44%|####4     | 1185/2671 [13:51<17:29,  1.42it/s]Evaluating on VQA val set:  44%|####4     | 1186/2671 [13:52<17:18,  1.43it/s]Evaluating on VQA val set:  44%|####4     | 1187/2671 [13:53<16:52,  1.47it/s]Evaluating on VQA val set:  44%|####4     | 1188/2671 [13:53<17:12,  1.44it/s]Evaluating on VQA val set:  45%|####4     | 1189/2671 [13:54<16:42,  1.48it/s]Evaluating on VQA val set:  45%|####4     | 1190/2671 [13:55<16:59,  1.45it/s]Evaluating on VQA val set:  45%|####4     | 1191/2671 [13:55<17:20,  1.42it/s]Evaluating on VQA val set:  45%|####4     | 1192/2671 [13:56<16:49,  1.47it/s]Evaluating on VQA val set:  45%|####4     | 1193/2671 [13:57<16:45,  1.47it/s]Evaluating on VQA val set:  45%|####4     | 1194/2671 [13:57<16:36,  1.48it/s]Evaluating on VQA val set:  45%|####4     | 1195/2671 [13:58<16:19,  1.51it/s]Evaluating on VQA val set:  45%|####4     | 1196/2671 [13:59<16:26,  1.50it/s]Evaluating on VQA val set:  45%|####4     | 1197/2671 [13:59<17:01,  1.44it/s]Evaluating on VQA val set:  45%|####4     | 1198/2671 [14:00<17:19,  1.42it/s]Evaluating on VQA val set:  45%|####4     | 1199/2671 [14:01<17:21,  1.41it/s]Evaluating on VQA val set:  45%|####4     | 1200/2671 [14:02<17:43,  1.38it/s]Evaluating on VQA val set:  45%|####4     | 1201/2671 [14:02<17:46,  1.38it/s]Evaluating on VQA val set:  45%|####5     | 1202/2671 [14:03<17:50,  1.37it/s]Evaluating on VQA val set:  45%|####5     | 1203/2671 [14:04<17:52,  1.37it/s]Evaluating on VQA val set:  45%|####5     | 1204/2671 [14:04<17:13,  1.42it/s]Evaluating on VQA val set:  45%|####5     | 1205/2671 [14:05<17:06,  1.43it/s]Evaluating on VQA val set:  45%|####5     | 1206/2671 [14:06<17:07,  1.43it/s]Evaluating on VQA val set:  45%|####5     | 1207/2671 [14:07<17:44,  1.38it/s]Evaluating on VQA val set:  45%|####5     | 1208/2671 [14:07<18:06,  1.35it/s]Evaluating on VQA val set:  45%|####5     | 1209/2671 [14:08<18:18,  1.33it/s]Evaluating on VQA val set:  45%|####5     | 1210/2671 [14:09<17:48,  1.37it/s]Evaluating on VQA val set:  45%|####5     | 1211/2671 [14:10<17:11,  1.42it/s]Evaluating on VQA val set:  45%|####5     | 1212/2671 [14:10<16:46,  1.45it/s]Evaluating on VQA val set:  45%|####5     | 1213/2671 [14:11<16:57,  1.43it/s]Evaluating on VQA val set:  45%|####5     | 1214/2671 [14:12<16:47,  1.45it/s]Evaluating on VQA val set:  45%|####5     | 1215/2671 [14:12<17:16,  1.41it/s]Evaluating on VQA val set:  46%|####5     | 1216/2671 [14:13<17:05,  1.42it/s]Evaluating on VQA val set:  46%|####5     | 1217/2671 [14:14<17:04,  1.42it/s]Evaluating on VQA val set:  46%|####5     | 1218/2671 [14:14<16:51,  1.44it/s]Evaluating on VQA val set:  46%|####5     | 1219/2671 [14:15<17:06,  1.41it/s]Evaluating on VQA val set:  46%|####5     | 1220/2671 [14:16<17:03,  1.42it/s]Evaluating on VQA val set:  46%|####5     | 1221/2671 [14:17<17:10,  1.41it/s]Evaluating on VQA val set:  46%|####5     | 1222/2671 [14:17<17:12,  1.40it/s]Evaluating on VQA val set:  46%|####5     | 1223/2671 [14:18<17:26,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1224/2671 [14:19<17:31,  1.38it/s]Evaluating on VQA val set:  46%|####5     | 1225/2671 [14:19<17:05,  1.41it/s]Evaluating on VQA val set:  46%|####5     | 1226/2671 [14:20<17:02,  1.41it/s]Evaluating on VQA val set:  46%|####5     | 1227/2671 [14:21<17:09,  1.40it/s]Evaluating on VQA val set:  46%|####5     | 1228/2671 [14:22<17:17,  1.39it/s]Evaluating on VQA val set:  46%|####6     | 1229/2671 [14:22<16:45,  1.43it/s]Evaluating on VQA val set:  46%|####6     | 1230/2671 [14:23<16:31,  1.45it/s]Evaluating on VQA val set:  46%|####6     | 1231/2671 [14:24<17:09,  1.40it/s]Evaluating on VQA val set:  46%|####6     | 1232/2671 [14:24<16:39,  1.44it/s]Evaluating on VQA val set:  46%|####6     | 1233/2671 [14:25<16:39,  1.44it/s]Evaluating on VQA val set:  46%|####6     | 1234/2671 [14:26<17:09,  1.40it/s]Evaluating on VQA val set:  46%|####6     | 1235/2671 [14:26<16:38,  1.44it/s]Evaluating on VQA val set:  46%|####6     | 1236/2671 [14:27<17:02,  1.40it/s]Evaluating on VQA val set:  46%|####6     | 1237/2671 [14:28<16:37,  1.44it/s]Evaluating on VQA val set:  46%|####6     | 1238/2671 [14:29<16:41,  1.43it/s]Evaluating on VQA val set:  46%|####6     | 1239/2671 [14:29<17:12,  1.39it/s]Evaluating on VQA val set:  46%|####6     | 1240/2671 [14:30<17:07,  1.39it/s]Evaluating on VQA val set:  46%|####6     | 1241/2671 [14:31<17:06,  1.39it/s]Evaluating on VQA val set:  46%|####6     | 1242/2671 [14:31<16:31,  1.44it/s]Evaluating on VQA val set:  47%|####6     | 1243/2671 [14:32<16:55,  1.41it/s]Evaluating on VQA val set:  47%|####6     | 1244/2671 [14:33<16:58,  1.40it/s]Evaluating on VQA val set:  47%|####6     | 1245/2671 [14:34<16:39,  1.43it/s]Evaluating on VQA val set:  47%|####6     | 1246/2671 [14:34<17:10,  1.38it/s]Evaluating on VQA val set:  47%|####6     | 1247/2671 [14:35<16:53,  1.41it/s]Evaluating on VQA val set:  47%|####6     | 1248/2671 [14:36<16:57,  1.40it/s]Evaluating on VQA val set:  47%|####6     | 1249/2671 [14:36<16:37,  1.43it/s]Evaluating on VQA val set:  47%|####6     | 1250/2671 [14:37<16:05,  1.47it/s]Evaluating on VQA val set:  47%|####6     | 1251/2671 [14:38<16:25,  1.44it/s]Evaluating on VQA val set:  47%|####6     | 1252/2671 [14:39<16:54,  1.40it/s]Evaluating on VQA val set:  47%|####6     | 1253/2671 [14:39<17:16,  1.37it/s]Evaluating on VQA val set:  47%|####6     | 1254/2671 [14:40<17:22,  1.36it/s]Evaluating on VQA val set:  47%|####6     | 1255/2671 [14:41<16:50,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1256/2671 [14:41<16:40,  1.41it/s]Evaluating on VQA val set:  47%|####7     | 1257/2671 [14:42<16:30,  1.43it/s]Evaluating on VQA val set:  47%|####7     | 1258/2671 [14:43<16:36,  1.42it/s]Evaluating on VQA val set:  47%|####7     | 1259/2671 [14:44<16:35,  1.42it/s]Evaluating on VQA val set:  47%|####7     | 1260/2671 [14:44<17:13,  1.36it/s]Evaluating on VQA val set:  47%|####7     | 1261/2671 [14:45<17:23,  1.35it/s]Evaluating on VQA val set:  47%|####7     | 1262/2671 [14:46<16:57,  1.38it/s]Evaluating on VQA val set:  47%|####7     | 1263/2671 [14:46<16:35,  1.41it/s]Evaluating on VQA val set:  47%|####7     | 1264/2671 [14:47<16:42,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1265/2671 [14:48<16:45,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1266/2671 [14:49<16:40,  1.40it/s]Evaluating on VQA val set:  47%|####7     | 1267/2671 [14:49<16:30,  1.42it/s]Evaluating on VQA val set:  47%|####7     | 1268/2671 [14:50<15:37,  1.50it/s]Evaluating on VQA val set:  48%|####7     | 1269/2671 [14:50<14:45,  1.58it/s]Evaluating on VQA val set:  48%|####7     | 1270/2671 [14:51<15:39,  1.49it/s]Evaluating on VQA val set:  48%|####7     | 1271/2671 [14:52<15:25,  1.51it/s]Evaluating on VQA val set:  48%|####7     | 1272/2671 [14:53<16:32,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1273/2671 [14:53<16:31,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1274/2671 [14:54<16:12,  1.44it/s]Evaluating on VQA val set:  48%|####7     | 1275/2671 [14:55<16:27,  1.41it/s]Evaluating on VQA val set:  48%|####7     | 1276/2671 [14:55<16:44,  1.39it/s]Evaluating on VQA val set:  48%|####7     | 1277/2671 [14:56<17:08,  1.36it/s]Evaluating on VQA val set:  48%|####7     | 1278/2671 [14:57<16:35,  1.40it/s]Evaluating on VQA val set:  48%|####7     | 1279/2671 [14:58<16:07,  1.44it/s]Evaluating on VQA val set:  48%|####7     | 1280/2671 [14:58<16:05,  1.44it/s]Evaluating on VQA val set:  48%|####7     | 1281/2671 [14:59<16:18,  1.42it/s]Evaluating on VQA val set:  48%|####7     | 1282/2671 [15:00<16:47,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1283/2671 [15:00<16:43,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1284/2671 [15:01<16:49,  1.37it/s]Evaluating on VQA val set:  48%|####8     | 1285/2671 [15:02<17:05,  1.35it/s]Evaluating on VQA val set:  48%|####8     | 1286/2671 [15:03<16:41,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1287/2671 [15:03<16:45,  1.38it/s]Evaluating on VQA val set:  48%|####8     | 1288/2671 [15:04<16:32,  1.39it/s]Evaluating on VQA val set:  48%|####8     | 1289/2671 [15:05<16:15,  1.42it/s]Evaluating on VQA val set:  48%|####8     | 1290/2671 [15:05<16:14,  1.42it/s]Evaluating on VQA val set:  48%|####8     | 1291/2671 [15:06<16:28,  1.40it/s]Evaluating on VQA val set:  48%|####8     | 1292/2671 [15:07<16:46,  1.37it/s]Evaluating on VQA val set:  48%|####8     | 1293/2671 [15:08<16:18,  1.41it/s]Evaluating on VQA val set:  48%|####8     | 1294/2671 [15:08<16:13,  1.41it/s]Evaluating on VQA val set:  48%|####8     | 1295/2671 [15:09<16:08,  1.42it/s]Evaluating on VQA val set:  49%|####8     | 1296/2671 [15:10<15:35,  1.47it/s]Evaluating on VQA val set:  49%|####8     | 1297/2671 [15:10<15:24,  1.49it/s]Evaluating on VQA val set:  49%|####8     | 1298/2671 [15:11<15:24,  1.49it/s]Evaluating on VQA val set:  49%|####8     | 1299/2671 [15:12<15:34,  1.47it/s]Evaluating on VQA val set:  49%|####8     | 1300/2671 [15:12<15:43,  1.45it/s]Evaluating on VQA val set:  49%|####8     | 1301/2671 [15:13<16:22,  1.39it/s]Evaluating on VQA val set:  49%|####8     | 1302/2671 [15:14<16:37,  1.37it/s]Evaluating on VQA val set:  49%|####8     | 1303/2671 [15:15<16:12,  1.41it/s]Evaluating on VQA val set:  49%|####8     | 1304/2671 [15:15<16:34,  1.38it/s]Evaluating on VQA val set:  49%|####8     | 1305/2671 [15:16<15:14,  1.49it/s]Evaluating on VQA val set:  49%|####8     | 1306/2671 [15:17<14:47,  1.54it/s]Evaluating on VQA val set:  49%|####8     | 1307/2671 [15:17<15:37,  1.46it/s]Evaluating on VQA val set:  49%|####8     | 1308/2671 [15:18<15:57,  1.42it/s]Evaluating on VQA val set:  49%|####9     | 1309/2671 [15:19<15:48,  1.44it/s]Evaluating on VQA val set:  49%|####9     | 1310/2671 [15:19<14:50,  1.53it/s]Evaluating on VQA val set:  49%|####9     | 1311/2671 [15:20<14:56,  1.52it/s]Evaluating on VQA val set:  49%|####9     | 1312/2671 [15:21<15:03,  1.50it/s]Evaluating on VQA val set:  49%|####9     | 1313/2671 [15:21<16:00,  1.41it/s]Evaluating on VQA val set:  49%|####9     | 1314/2671 [15:22<16:34,  1.36it/s]Evaluating on VQA val set:  49%|####9     | 1315/2671 [15:23<16:11,  1.40it/s]Evaluating on VQA val set:  49%|####9     | 1316/2671 [15:24<16:48,  1.34it/s]Evaluating on VQA val set:  49%|####9     | 1317/2671 [15:24<16:22,  1.38it/s]Evaluating on VQA val set:  49%|####9     | 1318/2671 [15:25<16:07,  1.40it/s]Evaluating on VQA val set:  49%|####9     | 1319/2671 [15:26<16:11,  1.39it/s]Evaluating on VQA val set:  49%|####9     | 1320/2671 [15:27<16:09,  1.39it/s]Evaluating on VQA val set:  49%|####9     | 1321/2671 [15:27<15:59,  1.41it/s]Evaluating on VQA val set:  49%|####9     | 1322/2671 [15:28<15:18,  1.47it/s]Evaluating on VQA val set:  50%|####9     | 1323/2671 [15:29<15:25,  1.46it/s]Evaluating on VQA val set:  50%|####9     | 1324/2671 [15:29<15:13,  1.48it/s]Evaluating on VQA val set:  50%|####9     | 1325/2671 [15:30<15:10,  1.48it/s]Evaluating on VQA val set:  50%|####9     | 1326/2671 [15:31<15:03,  1.49it/s]Evaluating on VQA val set:  50%|####9     | 1327/2671 [15:31<15:16,  1.47it/s]Evaluating on VQA val set:  50%|####9     | 1328/2671 [15:32<15:21,  1.46it/s]Evaluating on VQA val set:  50%|####9     | 1329/2671 [15:33<15:37,  1.43it/s]Evaluating on VQA val set:  50%|####9     | 1330/2671 [15:33<15:56,  1.40it/s]Evaluating on VQA val set:  50%|####9     | 1331/2671 [15:34<15:38,  1.43it/s]Evaluating on VQA val set:  50%|####9     | 1332/2671 [15:35<15:55,  1.40it/s]Evaluating on VQA val set:  50%|####9     | 1333/2671 [15:36<15:58,  1.40it/s]Evaluating on VQA val set:  50%|####9     | 1334/2671 [15:36<16:14,  1.37it/s]Evaluating on VQA val set:  50%|####9     | 1335/2671 [15:37<16:19,  1.36it/s]Evaluating on VQA val set:  50%|#####     | 1336/2671 [15:38<15:45,  1.41it/s]Evaluating on VQA val set:  50%|#####     | 1337/2671 [15:38<16:06,  1.38it/s]Evaluating on VQA val set:  50%|#####     | 1338/2671 [15:39<15:48,  1.40it/s]Evaluating on VQA val set:  50%|#####     | 1339/2671 [15:40<15:06,  1.47it/s]Evaluating on VQA val set:  50%|#####     | 1340/2671 [15:40<14:54,  1.49it/s]Evaluating on VQA val set:  50%|#####     | 1341/2671 [15:41<15:00,  1.48it/s]Evaluating on VQA val set:  50%|#####     | 1342/2671 [15:42<15:31,  1.43it/s]Evaluating on VQA val set:  50%|#####     | 1343/2671 [15:42<15:14,  1.45it/s]Evaluating on VQA val set:  50%|#####     | 1344/2671 [15:43<15:11,  1.46it/s]Evaluating on VQA val set:  50%|#####     | 1345/2671 [15:44<15:07,  1.46it/s]Evaluating on VQA val set:  50%|#####     | 1346/2671 [15:44<14:51,  1.49it/s]Evaluating on VQA val set:  50%|#####     | 1347/2671 [15:45<14:37,  1.51it/s]Evaluating on VQA val set:  50%|#####     | 1348/2671 [15:46<14:50,  1.49it/s]Evaluating on VQA val set:  51%|#####     | 1349/2671 [15:47<15:18,  1.44it/s]Evaluating on VQA val set:  51%|#####     | 1350/2671 [15:47<15:18,  1.44it/s]Evaluating on VQA val set:  51%|#####     | 1351/2671 [15:48<15:06,  1.46it/s]Evaluating on VQA val set:  51%|#####     | 1352/2671 [15:49<15:08,  1.45it/s]Evaluating on VQA val set:  51%|#####     | 1353/2671 [15:49<15:06,  1.45it/s]Evaluating on VQA val set:  51%|#####     | 1354/2671 [15:50<14:53,  1.47it/s]Evaluating on VQA val set:  51%|#####     | 1355/2671 [15:51<15:06,  1.45it/s]Evaluating on VQA val set:  51%|#####     | 1356/2671 [15:51<15:06,  1.45it/s]Evaluating on VQA val set:  51%|#####     | 1357/2671 [15:52<15:12,  1.44it/s]Evaluating on VQA val set:  51%|#####     | 1358/2671 [15:53<15:28,  1.41it/s]Evaluating on VQA val set:  51%|#####     | 1359/2671 [15:54<15:26,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1360/2671 [15:54<15:03,  1.45it/s]Evaluating on VQA val set:  51%|#####     | 1361/2671 [15:55<15:25,  1.42it/s]Evaluating on VQA val set:  51%|#####     | 1362/2671 [15:56<14:47,  1.47it/s]Evaluating on VQA val set:  51%|#####1    | 1363/2671 [15:56<14:20,  1.52it/s]Evaluating on VQA val set:  51%|#####1    | 1364/2671 [15:57<14:48,  1.47it/s]Evaluating on VQA val set:  51%|#####1    | 1365/2671 [15:58<15:01,  1.45it/s]Evaluating on VQA val set:  51%|#####1    | 1366/2671 [15:58<15:07,  1.44it/s]Evaluating on VQA val set:  51%|#####1    | 1367/2671 [15:59<15:08,  1.43it/s]Evaluating on VQA val set:  51%|#####1    | 1368/2671 [16:00<15:22,  1.41it/s]Evaluating on VQA val set:  51%|#####1    | 1369/2671 [16:01<15:50,  1.37it/s]Evaluating on VQA val set:  51%|#####1    | 1370/2671 [16:01<15:08,  1.43it/s]Evaluating on VQA val set:  51%|#####1    | 1371/2671 [16:02<15:16,  1.42it/s]Evaluating on VQA val set:  51%|#####1    | 1372/2671 [16:03<15:16,  1.42it/s]Evaluating on VQA val set:  51%|#####1    | 1373/2671 [16:03<15:00,  1.44it/s]Evaluating on VQA val set:  51%|#####1    | 1374/2671 [16:04<14:24,  1.50it/s]Evaluating on VQA val set:  51%|#####1    | 1375/2671 [16:05<14:26,  1.50it/s]Evaluating on VQA val set:  52%|#####1    | 1376/2671 [16:05<14:35,  1.48it/s]Evaluating on VQA val set:  52%|#####1    | 1377/2671 [16:06<14:40,  1.47it/s]Evaluating on VQA val set:  52%|#####1    | 1378/2671 [16:07<15:09,  1.42it/s]Evaluating on VQA val set:  52%|#####1    | 1379/2671 [16:07<15:15,  1.41it/s]Evaluating on VQA val set:  52%|#####1    | 1380/2671 [16:08<15:12,  1.42it/s]Evaluating on VQA val set:  52%|#####1    | 1381/2671 [16:09<15:30,  1.39it/s]Evaluating on VQA val set:  52%|#####1    | 1382/2671 [16:10<15:15,  1.41it/s]Evaluating on VQA val set:  52%|#####1    | 1383/2671 [16:10<14:59,  1.43it/s]Evaluating on VQA val set:  52%|#####1    | 1384/2671 [16:11<15:32,  1.38it/s]Evaluating on VQA val set:  52%|#####1    | 1385/2671 [16:12<15:49,  1.35it/s]Evaluating on VQA val set:  52%|#####1    | 1386/2671 [16:12<15:44,  1.36it/s]Evaluating on VQA val set:  52%|#####1    | 1387/2671 [16:13<16:03,  1.33it/s]Evaluating on VQA val set:  52%|#####1    | 1388/2671 [16:14<15:28,  1.38it/s]Evaluating on VQA val set:  52%|#####2    | 1389/2671 [16:15<15:33,  1.37it/s]Evaluating on VQA val set:  52%|#####2    | 1390/2671 [16:15<15:23,  1.39it/s]Evaluating on VQA val set:  52%|#####2    | 1391/2671 [16:16<15:13,  1.40it/s]Evaluating on VQA val set:  52%|#####2    | 1392/2671 [16:17<14:55,  1.43it/s]Evaluating on VQA val set:  52%|#####2    | 1393/2671 [16:17<14:42,  1.45it/s]Evaluating on VQA val set:  52%|#####2    | 1394/2671 [16:18<15:03,  1.41it/s]Evaluating on VQA val set:  52%|#####2    | 1395/2671 [16:19<15:21,  1.38it/s]Evaluating on VQA val set:  52%|#####2    | 1396/2671 [16:20<14:47,  1.44it/s]Evaluating on VQA val set:  52%|#####2    | 1397/2671 [16:20<14:34,  1.46it/s]Evaluating on VQA val set:  52%|#####2    | 1398/2671 [16:21<14:31,  1.46it/s]Evaluating on VQA val set:  52%|#####2    | 1399/2671 [16:21<14:09,  1.50it/s]Evaluating on VQA val set:  52%|#####2    | 1400/2671 [16:22<13:50,  1.53it/s]Evaluating on VQA val set:  52%|#####2    | 1401/2671 [16:23<13:57,  1.52it/s]Evaluating on VQA val set:  52%|#####2    | 1402/2671 [16:24<14:39,  1.44it/s]Evaluating on VQA val set:  53%|#####2    | 1403/2671 [16:24<14:52,  1.42it/s]Evaluating on VQA val set:  53%|#####2    | 1404/2671 [16:25<14:57,  1.41it/s]Evaluating on VQA val set:  53%|#####2    | 1405/2671 [16:26<14:22,  1.47it/s]Evaluating on VQA val set:  53%|#####2    | 1406/2671 [16:26<14:43,  1.43it/s]Evaluating on VQA val set:  53%|#####2    | 1407/2671 [16:27<14:22,  1.47it/s]Evaluating on VQA val set:  53%|#####2    | 1408/2671 [16:28<14:45,  1.43it/s]Evaluating on VQA val set:  53%|#####2    | 1409/2671 [16:28<15:01,  1.40it/s]Evaluating on VQA val set:  53%|#####2    | 1410/2671 [16:29<14:46,  1.42it/s]Evaluating on VQA val set:  53%|#####2    | 1411/2671 [16:30<14:54,  1.41it/s]Evaluating on VQA val set:  53%|#####2    | 1412/2671 [16:31<15:11,  1.38it/s]Evaluating on VQA val set:  53%|#####2    | 1413/2671 [16:31<15:06,  1.39it/s]Evaluating on VQA val set:  53%|#####2    | 1414/2671 [16:32<15:24,  1.36it/s]Evaluating on VQA val set:  53%|#####2    | 1415/2671 [16:33<14:47,  1.42it/s]Evaluating on VQA val set:  53%|#####3    | 1416/2671 [16:33<14:27,  1.45it/s]Evaluating on VQA val set:  53%|#####3    | 1417/2671 [16:34<14:33,  1.44it/s]Evaluating on VQA val set:  53%|#####3    | 1418/2671 [16:35<14:53,  1.40it/s]Evaluating on VQA val set:  53%|#####3    | 1419/2671 [16:36<14:25,  1.45it/s]Evaluating on VQA val set:  53%|#####3    | 1420/2671 [16:36<14:24,  1.45it/s]Evaluating on VQA val set:  53%|#####3    | 1421/2671 [16:37<14:39,  1.42it/s]Evaluating on VQA val set:  53%|#####3    | 1422/2671 [16:38<14:50,  1.40it/s]Evaluating on VQA val set:  53%|#####3    | 1423/2671 [16:38<14:33,  1.43it/s]Evaluating on VQA val set:  53%|#####3    | 1424/2671 [16:39<14:47,  1.41it/s]Evaluating on VQA val set:  53%|#####3    | 1425/2671 [16:40<14:20,  1.45it/s]Evaluating on VQA val set:  53%|#####3    | 1426/2671 [16:40<14:30,  1.43it/s]Evaluating on VQA val set:  53%|#####3    | 1427/2671 [16:41<14:19,  1.45it/s]Evaluating on VQA val set:  53%|#####3    | 1428/2671 [16:42<14:13,  1.46it/s]Evaluating on VQA val set:  54%|#####3    | 1429/2671 [16:43<14:31,  1.42it/s]Evaluating on VQA val set:  54%|#####3    | 1430/2671 [16:43<14:25,  1.43it/s]Evaluating on VQA val set:  54%|#####3    | 1431/2671 [16:44<13:54,  1.49it/s]Evaluating on VQA val set:  54%|#####3    | 1432/2671 [16:44<13:18,  1.55it/s]Evaluating on VQA val set:  54%|#####3    | 1433/2671 [16:45<14:08,  1.46it/s]Evaluating on VQA val set:  54%|#####3    | 1434/2671 [16:46<14:04,  1.47it/s]Evaluating on VQA val set:  54%|#####3    | 1435/2671 [16:47<14:23,  1.43it/s]Evaluating on VQA val set:  54%|#####3    | 1436/2671 [16:47<14:24,  1.43it/s]Evaluating on VQA val set:  54%|#####3    | 1437/2671 [16:48<14:45,  1.39it/s]Evaluating on VQA val set:  54%|#####3    | 1438/2671 [16:49<14:41,  1.40it/s]Evaluating on VQA val set:  54%|#####3    | 1439/2671 [16:50<15:01,  1.37it/s]Evaluating on VQA val set:  54%|#####3    | 1440/2671 [16:50<14:39,  1.40it/s]Evaluating on VQA val set:  54%|#####3    | 1441/2671 [16:51<15:02,  1.36it/s]Evaluating on VQA val set:  54%|#####3    | 1442/2671 [16:52<14:25,  1.42it/s]Evaluating on VQA val set:  54%|#####4    | 1443/2671 [16:52<14:26,  1.42it/s]Evaluating on VQA val set:  54%|#####4    | 1444/2671 [16:53<14:29,  1.41it/s]Evaluating on VQA val set:  54%|#####4    | 1445/2671 [16:54<14:32,  1.41it/s]Evaluating on VQA val set:  54%|#####4    | 1446/2671 [16:55<14:35,  1.40it/s]Evaluating on VQA val set:  54%|#####4    | 1447/2671 [16:55<14:28,  1.41it/s]Evaluating on VQA val set:  54%|#####4    | 1448/2671 [16:56<14:57,  1.36it/s]Evaluating on VQA val set:  54%|#####4    | 1449/2671 [16:57<14:54,  1.37it/s]Evaluating on VQA val set:  54%|#####4    | 1450/2671 [16:57<14:19,  1.42it/s]Evaluating on VQA val set:  54%|#####4    | 1451/2671 [16:58<14:03,  1.45it/s]Evaluating on VQA val set:  54%|#####4    | 1452/2671 [16:59<14:03,  1.44it/s]Evaluating on VQA val set:  54%|#####4    | 1453/2671 [16:59<14:01,  1.45it/s]Evaluating on VQA val set:  54%|#####4    | 1454/2671 [17:00<14:17,  1.42it/s]Evaluating on VQA val set:  54%|#####4    | 1455/2671 [17:01<14:31,  1.39it/s]Evaluating on VQA val set:  55%|#####4    | 1456/2671 [17:02<14:39,  1.38it/s]Evaluating on VQA val set:  55%|#####4    | 1457/2671 [17:02<14:37,  1.38it/s]Evaluating on VQA val set:  55%|#####4    | 1458/2671 [17:03<14:41,  1.38it/s]Evaluating on VQA val set:  55%|#####4    | 1459/2671 [17:04<13:59,  1.44it/s]Evaluating on VQA val set:  55%|#####4    | 1460/2671 [17:04<14:18,  1.41it/s]Evaluating on VQA val set:  55%|#####4    | 1461/2671 [17:05<14:46,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1462/2671 [17:06<14:37,  1.38it/s]Evaluating on VQA val set:  55%|#####4    | 1463/2671 [17:07<14:45,  1.36it/s]Evaluating on VQA val set:  55%|#####4    | 1464/2671 [17:07<13:56,  1.44it/s]Evaluating on VQA val set:  55%|#####4    | 1465/2671 [17:08<13:59,  1.44it/s]Evaluating on VQA val set:  55%|#####4    | 1466/2671 [17:09<14:14,  1.41it/s]Evaluating on VQA val set:  55%|#####4    | 1467/2671 [17:09<14:15,  1.41it/s]Evaluating on VQA val set:  55%|#####4    | 1468/2671 [17:10<14:16,  1.40it/s]Evaluating on VQA val set:  55%|#####4    | 1469/2671 [17:11<14:08,  1.42it/s]Evaluating on VQA val set:  55%|#####5    | 1470/2671 [17:12<14:32,  1.38it/s]Evaluating on VQA val set:  55%|#####5    | 1471/2671 [17:12<14:12,  1.41it/s]Evaluating on VQA val set:  55%|#####5    | 1472/2671 [17:13<13:35,  1.47it/s]Evaluating on VQA val set:  55%|#####5    | 1473/2671 [17:14<13:00,  1.53it/s]Evaluating on VQA val set:  55%|#####5    | 1474/2671 [17:14<12:31,  1.59it/s]Evaluating on VQA val set:  55%|#####5    | 1475/2671 [17:15<12:57,  1.54it/s]Evaluating on VQA val set:  55%|#####5    | 1476/2671 [17:15<13:10,  1.51it/s]Evaluating on VQA val set:  55%|#####5    | 1477/2671 [17:16<12:59,  1.53it/s]Evaluating on VQA val set:  55%|#####5    | 1478/2671 [17:17<13:12,  1.50it/s]Evaluating on VQA val set:  55%|#####5    | 1479/2671 [17:17<12:36,  1.58it/s]Evaluating on VQA val set:  55%|#####5    | 1480/2671 [17:18<12:40,  1.57it/s]Evaluating on VQA val set:  55%|#####5    | 1481/2671 [17:19<13:11,  1.50it/s]Evaluating on VQA val set:  55%|#####5    | 1482/2671 [17:19<13:23,  1.48it/s]Evaluating on VQA val set:  56%|#####5    | 1483/2671 [17:20<13:44,  1.44it/s]Evaluating on VQA val set:  56%|#####5    | 1484/2671 [17:21<13:42,  1.44it/s]Evaluating on VQA val set:  56%|#####5    | 1485/2671 [17:22<14:03,  1.41it/s]Evaluating on VQA val set:  56%|#####5    | 1486/2671 [17:22<13:52,  1.42it/s]Evaluating on VQA val set:  56%|#####5    | 1487/2671 [17:23<14:08,  1.40it/s]Evaluating on VQA val set:  56%|#####5    | 1488/2671 [17:24<13:31,  1.46it/s]Evaluating on VQA val set:  56%|#####5    | 1489/2671 [17:24<13:38,  1.44it/s]Evaluating on VQA val set:  56%|#####5    | 1490/2671 [17:25<13:51,  1.42it/s]Evaluating on VQA val set:  56%|#####5    | 1491/2671 [17:26<14:18,  1.37it/s]Evaluating on VQA val set:  56%|#####5    | 1492/2671 [17:27<14:24,  1.36it/s]Evaluating on VQA val set:  56%|#####5    | 1493/2671 [17:27<14:07,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1494/2671 [17:28<14:04,  1.39it/s]Evaluating on VQA val set:  56%|#####5    | 1495/2671 [17:29<13:50,  1.42it/s]Evaluating on VQA val set:  56%|#####6    | 1496/2671 [17:29<13:22,  1.46it/s]Evaluating on VQA val set:  56%|#####6    | 1497/2671 [17:30<13:20,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1498/2671 [17:31<13:31,  1.44it/s]Evaluating on VQA val set:  56%|#####6    | 1499/2671 [17:31<13:51,  1.41it/s]Evaluating on VQA val set:  56%|#####6    | 1500/2671 [17:32<13:39,  1.43it/s]Evaluating on VQA val set:  56%|#####6    | 1501/2671 [17:33<13:17,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1502/2671 [17:33<13:12,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1503/2671 [17:34<13:25,  1.45it/s]Evaluating on VQA val set:  56%|#####6    | 1504/2671 [17:35<13:00,  1.50it/s]Evaluating on VQA val set:  56%|#####6    | 1505/2671 [17:36<13:14,  1.47it/s]Evaluating on VQA val set:  56%|#####6    | 1506/2671 [17:36<13:33,  1.43it/s]Evaluating on VQA val set:  56%|#####6    | 1507/2671 [17:37<13:33,  1.43it/s]Evaluating on VQA val set:  56%|#####6    | 1508/2671 [17:38<13:40,  1.42it/s]Evaluating on VQA val set:  56%|#####6    | 1509/2671 [17:38<13:51,  1.40it/s]Evaluating on VQA val set:  57%|#####6    | 1510/2671 [17:39<13:59,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1511/2671 [17:40<14:03,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1512/2671 [17:40<13:18,  1.45it/s]Evaluating on VQA val set:  57%|#####6    | 1513/2671 [17:41<13:16,  1.45it/s]Evaluating on VQA val set:  57%|#####6    | 1514/2671 [17:42<13:32,  1.42it/s]Evaluating on VQA val set:  57%|#####6    | 1515/2671 [17:43<13:40,  1.41it/s]Evaluating on VQA val set:  57%|#####6    | 1516/2671 [17:43<13:56,  1.38it/s]Evaluating on VQA val set:  57%|#####6    | 1517/2671 [17:44<13:41,  1.41it/s]Evaluating on VQA val set:  57%|#####6    | 1518/2671 [17:45<13:29,  1.42it/s]Evaluating on VQA val set:  57%|#####6    | 1519/2671 [17:45<13:31,  1.42it/s]Evaluating on VQA val set:  57%|#####6    | 1520/2671 [17:46<13:41,  1.40it/s]Evaluating on VQA val set:  57%|#####6    | 1521/2671 [17:47<13:47,  1.39it/s]Evaluating on VQA val set:  57%|#####6    | 1522/2671 [17:48<13:33,  1.41it/s]Evaluating on VQA val set:  57%|#####7    | 1523/2671 [17:48<13:36,  1.41it/s]Evaluating on VQA val set:  57%|#####7    | 1524/2671 [17:49<13:40,  1.40it/s]Evaluating on VQA val set:  57%|#####7    | 1525/2671 [17:50<13:15,  1.44it/s]Evaluating on VQA val set:  57%|#####7    | 1526/2671 [17:50<13:15,  1.44it/s]Evaluating on VQA val set:  57%|#####7    | 1527/2671 [17:51<13:33,  1.41it/s]Evaluating on VQA val set:  57%|#####7    | 1528/2671 [17:52<13:08,  1.45it/s]Evaluating on VQA val set:  57%|#####7    | 1529/2671 [17:52<12:49,  1.48it/s]Evaluating on VQA val set:  57%|#####7    | 1530/2671 [17:53<12:58,  1.46it/s]Evaluating on VQA val set:  57%|#####7    | 1531/2671 [17:54<13:10,  1.44it/s]Evaluating on VQA val set:  57%|#####7    | 1532/2671 [17:55<13:24,  1.42it/s]Evaluating on VQA val set:  57%|#####7    | 1533/2671 [17:55<13:40,  1.39it/s]Evaluating on VQA val set:  57%|#####7    | 1534/2671 [17:56<13:18,  1.42it/s]Evaluating on VQA val set:  57%|#####7    | 1535/2671 [17:57<13:19,  1.42it/s]Evaluating on VQA val set:  58%|#####7    | 1536/2671 [17:57<13:30,  1.40it/s]Evaluating on VQA val set:  58%|#####7    | 1537/2671 [17:58<13:11,  1.43it/s]Evaluating on VQA val set:  58%|#####7    | 1538/2671 [17:59<13:39,  1.38it/s]Evaluating on VQA val set:  58%|#####7    | 1539/2671 [18:00<13:53,  1.36it/s]Evaluating on VQA val set:  58%|#####7    | 1540/2671 [18:00<13:02,  1.44it/s]Evaluating on VQA val set:  58%|#####7    | 1541/2671 [18:01<12:38,  1.49it/s]Evaluating on VQA val set:  58%|#####7    | 1542/2671 [18:02<12:57,  1.45it/s]Evaluating on VQA val set:  58%|#####7    | 1543/2671 [18:02<13:00,  1.44it/s]Evaluating on VQA val set:  58%|#####7    | 1544/2671 [18:03<13:10,  1.43it/s]Evaluating on VQA val set:  58%|#####7    | 1545/2671 [18:04<13:41,  1.37it/s]Evaluating on VQA val set:  58%|#####7    | 1546/2671 [18:05<13:36,  1.38it/s]Evaluating on VQA val set:  58%|#####7    | 1547/2671 [18:05<13:28,  1.39it/s]Evaluating on VQA val set:  58%|#####7    | 1548/2671 [18:06<13:25,  1.39it/s]Evaluating on VQA val set:  58%|#####7    | 1549/2671 [18:07<13:22,  1.40it/s]Evaluating on VQA val set:  58%|#####8    | 1550/2671 [18:07<13:36,  1.37it/s]Evaluating on VQA val set:  58%|#####8    | 1551/2671 [18:08<14:02,  1.33it/s]Evaluating on VQA val set:  58%|#####8    | 1552/2671 [18:09<13:38,  1.37it/s]Evaluating on VQA val set:  58%|#####8    | 1553/2671 [18:10<13:22,  1.39it/s]Evaluating on VQA val set:  58%|#####8    | 1554/2671 [18:10<13:22,  1.39it/s]Evaluating on VQA val set:  58%|#####8    | 1555/2671 [18:11<13:03,  1.42it/s]Evaluating on VQA val set:  58%|#####8    | 1556/2671 [18:12<13:01,  1.43it/s]Evaluating on VQA val set:  58%|#####8    | 1557/2671 [18:12<13:13,  1.40it/s]Evaluating on VQA val set:  58%|#####8    | 1558/2671 [18:13<13:25,  1.38it/s]Evaluating on VQA val set:  58%|#####8    | 1559/2671 [18:14<13:33,  1.37it/s]Evaluating on VQA val set:  58%|#####8    | 1560/2671 [18:15<13:37,  1.36it/s]Evaluating on VQA val set:  58%|#####8    | 1561/2671 [18:15<13:34,  1.36it/s]Evaluating on VQA val set:  58%|#####8    | 1562/2671 [18:16<13:23,  1.38it/s]Evaluating on VQA val set:  59%|#####8    | 1563/2671 [18:17<13:15,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1564/2671 [18:17<13:09,  1.40it/s]Evaluating on VQA val set:  59%|#####8    | 1565/2671 [18:18<13:32,  1.36it/s]Evaluating on VQA val set:  59%|#####8    | 1566/2671 [18:19<13:19,  1.38it/s]Evaluating on VQA val set:  59%|#####8    | 1567/2671 [18:20<12:55,  1.42it/s]Evaluating on VQA val set:  59%|#####8    | 1568/2671 [18:20<13:23,  1.37it/s]Evaluating on VQA val set:  59%|#####8    | 1569/2671 [18:21<13:12,  1.39it/s]Evaluating on VQA val set:  59%|#####8    | 1570/2671 [18:22<13:02,  1.41it/s]Evaluating on VQA val set:  59%|#####8    | 1571/2671 [18:23<13:04,  1.40it/s]Evaluating on VQA val set:  59%|#####8    | 1572/2671 [18:23<12:58,  1.41it/s]Evaluating on VQA val set:  59%|#####8    | 1573/2671 [18:24<12:48,  1.43it/s]Evaluating on VQA val set:  59%|#####8    | 1574/2671 [18:25<12:56,  1.41it/s]Evaluating on VQA val set:  59%|#####8    | 1575/2671 [18:25<12:57,  1.41it/s]Evaluating on VQA val set:  59%|#####9    | 1576/2671 [18:26<13:13,  1.38it/s]Evaluating on VQA val set:  59%|#####9    | 1577/2671 [18:27<13:12,  1.38it/s]Evaluating on VQA val set:  59%|#####9    | 1578/2671 [18:28<12:55,  1.41it/s]Evaluating on VQA val set:  59%|#####9    | 1579/2671 [18:28<12:58,  1.40it/s]Evaluating on VQA val set:  59%|#####9    | 1580/2671 [18:29<13:03,  1.39it/s]Evaluating on VQA val set:  59%|#####9    | 1581/2671 [18:30<13:27,  1.35it/s]Evaluating on VQA val set:  59%|#####9    | 1582/2671 [18:30<13:28,  1.35it/s]Evaluating on VQA val set:  59%|#####9    | 1583/2671 [18:31<13:27,  1.35it/s]Evaluating on VQA val set:  59%|#####9    | 1584/2671 [18:32<13:11,  1.37it/s]Evaluating on VQA val set:  59%|#####9    | 1585/2671 [18:32<12:13,  1.48it/s]Evaluating on VQA val set:  59%|#####9    | 1586/2671 [18:33<12:10,  1.49it/s]Evaluating on VQA val set:  59%|#####9    | 1587/2671 [18:34<12:26,  1.45it/s]Evaluating on VQA val set:  59%|#####9    | 1588/2671 [18:35<12:26,  1.45it/s]Evaluating on VQA val set:  59%|#####9    | 1589/2671 [18:35<11:50,  1.52it/s]Evaluating on VQA val set:  60%|#####9    | 1590/2671 [18:36<12:04,  1.49it/s]Evaluating on VQA val set:  60%|#####9    | 1591/2671 [18:37<12:09,  1.48it/s]Evaluating on VQA val set:  60%|#####9    | 1592/2671 [18:37<12:26,  1.45it/s]Evaluating on VQA val set:  60%|#####9    | 1593/2671 [18:38<12:41,  1.42it/s]Evaluating on VQA val set:  60%|#####9    | 1594/2671 [18:39<12:14,  1.47it/s]Evaluating on VQA val set:  60%|#####9    | 1595/2671 [18:39<11:33,  1.55it/s]Evaluating on VQA val set:  60%|#####9    | 1596/2671 [18:40<11:30,  1.56it/s]Evaluating on VQA val set:  60%|#####9    | 1597/2671 [18:41<11:53,  1.51it/s]Evaluating on VQA val set:  60%|#####9    | 1598/2671 [18:41<12:02,  1.49it/s]Evaluating on VQA val set:  60%|#####9    | 1599/2671 [18:42<12:23,  1.44it/s]Evaluating on VQA val set:  60%|#####9    | 1600/2671 [18:43<12:50,  1.39it/s]Evaluating on VQA val set:  60%|#####9    | 1601/2671 [18:43<12:28,  1.43it/s]Evaluating on VQA val set:  60%|#####9    | 1602/2671 [18:44<11:34,  1.54it/s]Evaluating on VQA val set:  60%|######    | 1603/2671 [18:45<12:17,  1.45it/s]Evaluating on VQA val set:  60%|######    | 1604/2671 [18:45<12:15,  1.45it/s]Evaluating on VQA val set:  60%|######    | 1605/2671 [18:46<12:21,  1.44it/s]Evaluating on VQA val set:  60%|######    | 1606/2671 [18:47<12:18,  1.44it/s]Evaluating on VQA val set:  60%|######    | 1607/2671 [18:48<12:38,  1.40it/s]Evaluating on VQA val set:  60%|######    | 1608/2671 [18:48<12:20,  1.44it/s]Evaluating on VQA val set:  60%|######    | 1609/2671 [18:49<12:11,  1.45it/s]Evaluating on VQA val set:  60%|######    | 1610/2671 [18:50<12:44,  1.39it/s]Evaluating on VQA val set:  60%|######    | 1611/2671 [18:50<13:07,  1.35it/s]Evaluating on VQA val set:  60%|######    | 1612/2671 [18:51<12:43,  1.39it/s]Evaluating on VQA val set:  60%|######    | 1613/2671 [18:52<12:29,  1.41it/s]Evaluating on VQA val set:  60%|######    | 1614/2671 [18:53<12:19,  1.43it/s]Evaluating on VQA val set:  60%|######    | 1615/2671 [18:53<12:28,  1.41it/s]Evaluating on VQA val set:  61%|######    | 1616/2671 [18:54<12:39,  1.39it/s]Evaluating on VQA val set:  61%|######    | 1617/2671 [18:55<12:29,  1.41it/s]Evaluating on VQA val set:  61%|######    | 1618/2671 [18:55<12:22,  1.42it/s]Evaluating on VQA val set:  61%|######    | 1619/2671 [18:56<11:35,  1.51it/s]Evaluating on VQA val set:  61%|######    | 1620/2671 [18:57<12:01,  1.46it/s]Evaluating on VQA val set:  61%|######    | 1621/2671 [18:57<12:15,  1.43it/s]Evaluating on VQA val set:  61%|######    | 1622/2671 [18:58<12:27,  1.40it/s]Evaluating on VQA val set:  61%|######    | 1623/2671 [18:59<12:41,  1.38it/s]Evaluating on VQA val set:  61%|######    | 1624/2671 [19:00<12:58,  1.34it/s]Evaluating on VQA val set:  61%|######    | 1625/2671 [19:00<12:49,  1.36it/s]Evaluating on VQA val set:  61%|######    | 1626/2671 [19:01<12:48,  1.36it/s]Evaluating on VQA val set:  61%|######    | 1627/2671 [19:02<12:32,  1.39it/s]Evaluating on VQA val set:  61%|######    | 1628/2671 [19:02<12:06,  1.44it/s]Evaluating on VQA val set:  61%|######    | 1629/2671 [19:03<12:37,  1.38it/s]Evaluating on VQA val set:  61%|######1   | 1630/2671 [19:04<12:32,  1.38it/s]Evaluating on VQA val set:  61%|######1   | 1631/2671 [19:05<12:08,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1632/2671 [19:05<12:11,  1.42it/s]Evaluating on VQA val set:  61%|######1   | 1633/2671 [19:06<12:28,  1.39it/s]Evaluating on VQA val set:  61%|######1   | 1634/2671 [19:07<12:24,  1.39it/s]Evaluating on VQA val set:  61%|######1   | 1635/2671 [19:07<12:02,  1.43it/s]Evaluating on VQA val set:  61%|######1   | 1636/2671 [19:08<11:57,  1.44it/s]Evaluating on VQA val set:  61%|######1   | 1637/2671 [19:09<11:31,  1.49it/s]Evaluating on VQA val set:  61%|######1   | 1638/2671 [19:09<11:38,  1.48it/s]Evaluating on VQA val set:  61%|######1   | 1639/2671 [19:10<11:55,  1.44it/s]Evaluating on VQA val set:  61%|######1   | 1640/2671 [19:11<11:58,  1.44it/s]Evaluating on VQA val set:  61%|######1   | 1641/2671 [19:12<12:18,  1.39it/s]Evaluating on VQA val set:  61%|######1   | 1642/2671 [19:12<11:27,  1.50it/s]Evaluating on VQA val set:  62%|######1   | 1643/2671 [19:13<11:44,  1.46it/s]Evaluating on VQA val set:  62%|######1   | 1644/2671 [19:14<11:26,  1.50it/s]Evaluating on VQA val set:  62%|######1   | 1645/2671 [19:14<11:29,  1.49it/s]Evaluating on VQA val set:  62%|######1   | 1646/2671 [19:15<11:33,  1.48it/s]Evaluating on VQA val set:  62%|######1   | 1647/2671 [19:15<10:52,  1.57it/s]Evaluating on VQA val set:  62%|######1   | 1648/2671 [19:16<11:12,  1.52it/s]Evaluating on VQA val set:  62%|######1   | 1649/2671 [19:17<11:34,  1.47it/s]Evaluating on VQA val set:  62%|######1   | 1650/2671 [19:18<11:32,  1.47it/s]Evaluating on VQA val set:  62%|######1   | 1651/2671 [19:18<11:47,  1.44it/s]Evaluating on VQA val set:  62%|######1   | 1652/2671 [19:19<11:30,  1.48it/s]Evaluating on VQA val set:  62%|######1   | 1653/2671 [19:20<11:30,  1.47it/s]Evaluating on VQA val set:  62%|######1   | 1654/2671 [19:20<11:25,  1.48it/s]Evaluating on VQA val set:  62%|######1   | 1655/2671 [19:21<11:21,  1.49it/s]Evaluating on VQA val set:  62%|######1   | 1656/2671 [19:22<11:35,  1.46it/s]Evaluating on VQA val set:  62%|######2   | 1657/2671 [19:22<11:45,  1.44it/s]Evaluating on VQA val set:  62%|######2   | 1658/2671 [19:23<11:29,  1.47it/s]Evaluating on VQA val set:  62%|######2   | 1659/2671 [19:24<10:28,  1.61it/s]Evaluating on VQA val set:  62%|######2   | 1660/2671 [19:24<10:57,  1.54it/s]Evaluating on VQA val set:  62%|######2   | 1661/2671 [19:25<10:53,  1.55it/s]Evaluating on VQA val set:  62%|######2   | 1662/2671 [19:26<10:43,  1.57it/s]Evaluating on VQA val set:  62%|######2   | 1663/2671 [19:26<11:21,  1.48it/s]Evaluating on VQA val set:  62%|######2   | 1664/2671 [19:27<11:46,  1.43it/s]Evaluating on VQA val set:  62%|######2   | 1665/2671 [19:28<11:52,  1.41it/s]Evaluating on VQA val set:  62%|######2   | 1666/2671 [19:29<12:05,  1.39it/s]Evaluating on VQA val set:  62%|######2   | 1667/2671 [19:29<12:04,  1.39it/s]Evaluating on VQA val set:  62%|######2   | 1668/2671 [19:30<12:09,  1.38it/s]Evaluating on VQA val set:  62%|######2   | 1669/2671 [19:31<12:08,  1.38it/s]Evaluating on VQA val set:  63%|######2   | 1670/2671 [19:31<12:03,  1.38it/s]Evaluating on VQA val set:  63%|######2   | 1671/2671 [19:32<11:54,  1.40it/s]Evaluating on VQA val set:  63%|######2   | 1672/2671 [19:33<11:28,  1.45it/s]Evaluating on VQA val set:  63%|######2   | 1673/2671 [19:33<11:27,  1.45it/s]Evaluating on VQA val set:  63%|######2   | 1674/2671 [19:34<11:53,  1.40it/s]Evaluating on VQA val set:  63%|######2   | 1675/2671 [19:35<11:42,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1676/2671 [19:36<11:31,  1.44it/s]Evaluating on VQA val set:  63%|######2   | 1677/2671 [19:36<11:28,  1.44it/s]Evaluating on VQA val set:  63%|######2   | 1678/2671 [19:37<11:36,  1.42it/s]Evaluating on VQA val set:  63%|######2   | 1679/2671 [19:38<11:25,  1.45it/s]Evaluating on VQA val set:  63%|######2   | 1680/2671 [19:38<11:33,  1.43it/s]Evaluating on VQA val set:  63%|######2   | 1681/2671 [19:39<11:18,  1.46it/s]Evaluating on VQA val set:  63%|######2   | 1682/2671 [19:40<11:23,  1.45it/s]Evaluating on VQA val set:  63%|######3   | 1683/2671 [19:40<11:15,  1.46it/s]Evaluating on VQA val set:  63%|######3   | 1684/2671 [19:41<11:33,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1685/2671 [19:42<11:35,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1686/2671 [19:43<11:32,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1687/2671 [19:43<11:17,  1.45it/s]Evaluating on VQA val set:  63%|######3   | 1688/2671 [19:44<11:24,  1.44it/s]Evaluating on VQA val set:  63%|######3   | 1689/2671 [19:45<11:39,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1690/2671 [19:45<11:36,  1.41it/s]Evaluating on VQA val set:  63%|######3   | 1691/2671 [19:46<11:37,  1.40it/s]Evaluating on VQA val set:  63%|######3   | 1692/2671 [19:47<11:30,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1693/2671 [19:47<11:30,  1.42it/s]Evaluating on VQA val set:  63%|######3   | 1694/2671 [19:48<11:49,  1.38it/s]Evaluating on VQA val set:  63%|######3   | 1695/2671 [19:49<11:31,  1.41it/s]Evaluating on VQA val set:  63%|######3   | 1696/2671 [19:50<11:35,  1.40it/s]Evaluating on VQA val set:  64%|######3   | 1697/2671 [19:50<11:06,  1.46it/s]Evaluating on VQA val set:  64%|######3   | 1698/2671 [19:51<11:20,  1.43it/s]Evaluating on VQA val set:  64%|######3   | 1699/2671 [19:52<11:32,  1.40it/s]Evaluating on VQA val set:  64%|######3   | 1700/2671 [19:52<11:40,  1.39it/s]Evaluating on VQA val set:  64%|######3   | 1701/2671 [19:53<11:27,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1702/2671 [19:54<11:40,  1.38it/s]Evaluating on VQA val set:  64%|######3   | 1703/2671 [19:55<11:23,  1.42it/s]Evaluating on VQA val set:  64%|######3   | 1704/2671 [19:55<11:23,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1705/2671 [19:56<11:14,  1.43it/s]Evaluating on VQA val set:  64%|######3   | 1706/2671 [19:57<11:22,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1707/2671 [19:57<11:22,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1708/2671 [19:58<11:21,  1.41it/s]Evaluating on VQA val set:  64%|######3   | 1709/2671 [19:59<11:43,  1.37it/s]Evaluating on VQA val set:  64%|######4   | 1710/2671 [20:00<11:54,  1.35it/s]Evaluating on VQA val set:  64%|######4   | 1711/2671 [20:00<11:52,  1.35it/s]Evaluating on VQA val set:  64%|######4   | 1712/2671 [20:01<11:17,  1.42it/s]Evaluating on VQA val set:  64%|######4   | 1713/2671 [20:02<11:21,  1.41it/s]Evaluating on VQA val set:  64%|######4   | 1714/2671 [20:03<11:32,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1715/2671 [20:03<11:31,  1.38it/s]Evaluating on VQA val set:  64%|######4   | 1716/2671 [20:04<11:21,  1.40it/s]Evaluating on VQA val set:  64%|######4   | 1717/2671 [20:05<11:20,  1.40it/s]Evaluating on VQA val set:  64%|######4   | 1718/2671 [20:05<11:24,  1.39it/s]Evaluating on VQA val set:  64%|######4   | 1719/2671 [20:06<11:23,  1.39it/s]Evaluating on VQA val set:  64%|######4   | 1720/2671 [20:07<10:56,  1.45it/s]Evaluating on VQA val set:  64%|######4   | 1721/2671 [20:07<11:16,  1.40it/s]Evaluating on VQA val set:  64%|######4   | 1722/2671 [20:08<11:24,  1.39it/s]Evaluating on VQA val set:  65%|######4   | 1723/2671 [20:09<10:50,  1.46it/s]Evaluating on VQA val set:  65%|######4   | 1724/2671 [20:09<10:38,  1.48it/s]Evaluating on VQA val set:  65%|######4   | 1725/2671 [20:10<10:46,  1.46it/s]Evaluating on VQA val set:  65%|######4   | 1726/2671 [20:11<11:06,  1.42it/s]Evaluating on VQA val set:  65%|######4   | 1727/2671 [20:12<11:03,  1.42it/s]Evaluating on VQA val set:  65%|######4   | 1728/2671 [20:12<10:55,  1.44it/s]Evaluating on VQA val set:  65%|######4   | 1729/2671 [20:13<11:06,  1.41it/s]Evaluating on VQA val set:  65%|######4   | 1730/2671 [20:14<10:56,  1.43it/s]Evaluating on VQA val set:  65%|######4   | 1731/2671 [20:14<11:03,  1.42it/s]Evaluating on VQA val set:  65%|######4   | 1732/2671 [20:15<11:04,  1.41it/s]Evaluating on VQA val set:  65%|######4   | 1733/2671 [20:16<10:46,  1.45it/s]Evaluating on VQA val set:  65%|######4   | 1734/2671 [20:16<10:51,  1.44it/s]Evaluating on VQA val set:  65%|######4   | 1735/2671 [20:17<11:08,  1.40it/s]Evaluating on VQA val set:  65%|######4   | 1736/2671 [20:18<10:57,  1.42it/s]Evaluating on VQA val set:  65%|######5   | 1737/2671 [20:19<10:50,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1738/2671 [20:19<10:54,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1739/2671 [20:20<10:54,  1.42it/s]Evaluating on VQA val set:  65%|######5   | 1740/2671 [20:21<10:59,  1.41it/s]Evaluating on VQA val set:  65%|######5   | 1741/2671 [20:21<10:54,  1.42it/s]Evaluating on VQA val set:  65%|######5   | 1742/2671 [20:22<11:06,  1.39it/s]Evaluating on VQA val set:  65%|######5   | 1743/2671 [20:23<10:50,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1744/2671 [20:23<09:28,  1.63it/s]Evaluating on VQA val set:  65%|######5   | 1745/2671 [20:24<09:47,  1.58it/s]Evaluating on VQA val set:  65%|######5   | 1746/2671 [20:25<10:21,  1.49it/s]Evaluating on VQA val set:  65%|######5   | 1747/2671 [20:25<10:26,  1.48it/s]Evaluating on VQA val set:  65%|######5   | 1748/2671 [20:26<10:45,  1.43it/s]Evaluating on VQA val set:  65%|######5   | 1749/2671 [20:27<10:28,  1.47it/s]Evaluating on VQA val set:  66%|######5   | 1750/2671 [20:27<10:21,  1.48it/s]Evaluating on VQA val set:  66%|######5   | 1751/2671 [20:28<10:23,  1.48it/s]Evaluating on VQA val set:  66%|######5   | 1752/2671 [20:29<09:58,  1.53it/s]Evaluating on VQA val set:  66%|######5   | 1753/2671 [20:29<10:15,  1.49it/s]Evaluating on VQA val set:  66%|######5   | 1754/2671 [20:30<10:24,  1.47it/s]Evaluating on VQA val set:  66%|######5   | 1755/2671 [20:31<10:35,  1.44it/s]Evaluating on VQA val set:  66%|######5   | 1756/2671 [20:32<10:38,  1.43it/s]Evaluating on VQA val set:  66%|######5   | 1757/2671 [20:32<10:55,  1.39it/s]Evaluating on VQA val set:  66%|######5   | 1758/2671 [20:33<10:49,  1.41it/s]Evaluating on VQA val set:  66%|######5   | 1759/2671 [20:34<10:45,  1.41it/s]Evaluating on VQA val set:  66%|######5   | 1760/2671 [20:34<10:52,  1.40it/s]Evaluating on VQA val set:  66%|######5   | 1761/2671 [20:35<10:45,  1.41it/s]Evaluating on VQA val set:  66%|######5   | 1762/2671 [20:36<10:43,  1.41it/s]Evaluating on VQA val set:  66%|######6   | 1763/2671 [20:36<09:57,  1.52it/s]Evaluating on VQA val set:  66%|######6   | 1764/2671 [20:37<09:51,  1.53it/s]Evaluating on VQA val set:  66%|######6   | 1765/2671 [20:38<10:06,  1.49it/s]Evaluating on VQA val set:  66%|######6   | 1766/2671 [20:38<10:10,  1.48it/s]Evaluating on VQA val set:  66%|######6   | 1767/2671 [20:39<10:23,  1.45it/s]Evaluating on VQA val set:  66%|######6   | 1768/2671 [20:40<10:35,  1.42it/s]Evaluating on VQA val set:  66%|######6   | 1769/2671 [20:41<10:41,  1.41it/s]Evaluating on VQA val set:  66%|######6   | 1770/2671 [20:41<10:41,  1.40it/s]Evaluating on VQA val set:  66%|######6   | 1771/2671 [20:42<10:45,  1.39it/s]Evaluating on VQA val set:  66%|######6   | 1772/2671 [20:43<11:02,  1.36it/s]Evaluating on VQA val set:  66%|######6   | 1773/2671 [20:44<10:59,  1.36it/s]Evaluating on VQA val set:  66%|######6   | 1774/2671 [20:44<10:53,  1.37it/s]Evaluating on VQA val set:  66%|######6   | 1775/2671 [20:45<10:53,  1.37it/s]Evaluating on VQA val set:  66%|######6   | 1776/2671 [20:46<10:50,  1.38it/s]Evaluating on VQA val set:  67%|######6   | 1777/2671 [20:46<10:38,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1778/2671 [20:47<10:36,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1779/2671 [20:48<10:41,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1780/2671 [20:49<10:43,  1.38it/s]Evaluating on VQA val set:  67%|######6   | 1781/2671 [20:49<10:51,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1782/2671 [20:50<10:23,  1.42it/s]Evaluating on VQA val set:  67%|######6   | 1783/2671 [20:51<10:35,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1784/2671 [20:51<10:32,  1.40it/s]Evaluating on VQA val set:  67%|######6   | 1785/2671 [20:52<10:29,  1.41it/s]Evaluating on VQA val set:  67%|######6   | 1786/2671 [20:53<10:45,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1787/2671 [20:54<10:44,  1.37it/s]Evaluating on VQA val set:  67%|######6   | 1788/2671 [20:54<10:33,  1.39it/s]Evaluating on VQA val set:  67%|######6   | 1789/2671 [20:55<10:48,  1.36it/s]Evaluating on VQA val set:  67%|######7   | 1790/2671 [20:56<10:37,  1.38it/s]Evaluating on VQA val set:  67%|######7   | 1791/2671 [20:57<10:26,  1.40it/s]Evaluating on VQA val set:  67%|######7   | 1792/2671 [20:57<10:27,  1.40it/s]Evaluating on VQA val set:  67%|######7   | 1793/2671 [20:58<10:30,  1.39it/s]Evaluating on VQA val set:  67%|######7   | 1794/2671 [20:59<10:12,  1.43it/s]Evaluating on VQA val set:  67%|######7   | 1795/2671 [20:59<10:20,  1.41it/s]Evaluating on VQA val set:  67%|######7   | 1796/2671 [21:00<10:15,  1.42it/s]Evaluating on VQA val set:  67%|######7   | 1797/2671 [21:01<10:21,  1.41it/s]Evaluating on VQA val set:  67%|######7   | 1798/2671 [21:02<10:41,  1.36it/s]Evaluating on VQA val set:  67%|######7   | 1799/2671 [21:02<10:52,  1.34it/s]Evaluating on VQA val set:  67%|######7   | 1800/2671 [21:03<10:39,  1.36it/s]Evaluating on VQA val set:  67%|######7   | 1801/2671 [21:04<10:21,  1.40it/s]Evaluating on VQA val set:  67%|######7   | 1802/2671 [21:04<09:52,  1.47it/s]Evaluating on VQA val set:  68%|######7   | 1803/2671 [21:05<09:43,  1.49it/s]Evaluating on VQA val set:  68%|######7   | 1804/2671 [21:06<09:30,  1.52it/s]Evaluating on VQA val set:  68%|######7   | 1805/2671 [21:06<09:43,  1.48it/s]Evaluating on VQA val set:  68%|######7   | 1806/2671 [21:07<09:49,  1.47it/s]Evaluating on VQA val set:  68%|######7   | 1807/2671 [21:08<10:03,  1.43it/s]Evaluating on VQA val set:  68%|######7   | 1808/2671 [21:08<09:56,  1.45it/s]Evaluating on VQA val set:  68%|######7   | 1809/2671 [21:09<10:06,  1.42it/s]Evaluating on VQA val set:  68%|######7   | 1810/2671 [21:10<10:15,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1811/2671 [21:11<10:04,  1.42it/s]Evaluating on VQA val set:  68%|######7   | 1812/2671 [21:11<10:15,  1.40it/s]Evaluating on VQA val set:  68%|######7   | 1813/2671 [21:12<10:19,  1.39it/s]Evaluating on VQA val set:  68%|######7   | 1814/2671 [21:13<10:15,  1.39it/s]Evaluating on VQA val set:  68%|######7   | 1815/2671 [21:13<09:59,  1.43it/s]Evaluating on VQA val set:  68%|######7   | 1816/2671 [21:14<09:54,  1.44it/s]Evaluating on VQA val set:  68%|######8   | 1817/2671 [21:15<09:59,  1.43it/s]Evaluating on VQA val set:  68%|######8   | 1818/2671 [21:15<09:50,  1.44it/s]Evaluating on VQA val set:  68%|######8   | 1819/2671 [21:16<09:54,  1.43it/s]Evaluating on VQA val set:  68%|######8   | 1820/2671 [21:17<09:56,  1.43it/s]Evaluating on VQA val set:  68%|######8   | 1821/2671 [21:18<09:34,  1.48it/s]Evaluating on VQA val set:  68%|######8   | 1822/2671 [21:18<09:46,  1.45it/s]Evaluating on VQA val set:  68%|######8   | 1823/2671 [21:19<09:46,  1.45it/s]Evaluating on VQA val set:  68%|######8   | 1824/2671 [21:20<09:54,  1.42it/s]Evaluating on VQA val set:  68%|######8   | 1825/2671 [21:20<09:53,  1.43it/s]Evaluating on VQA val set:  68%|######8   | 1826/2671 [21:21<10:00,  1.41it/s]Evaluating on VQA val set:  68%|######8   | 1827/2671 [21:22<09:52,  1.43it/s]Evaluating on VQA val set:  68%|######8   | 1828/2671 [21:22<09:57,  1.41it/s]Evaluating on VQA val set:  68%|######8   | 1829/2671 [21:23<09:44,  1.44it/s]Evaluating on VQA val set:  69%|######8   | 1830/2671 [21:24<09:29,  1.48it/s]Evaluating on VQA val set:  69%|######8   | 1831/2671 [21:24<09:27,  1.48it/s]Evaluating on VQA val set:  69%|######8   | 1832/2671 [21:25<09:40,  1.44it/s]Evaluating on VQA val set:  69%|######8   | 1833/2671 [21:26<09:38,  1.45it/s]Evaluating on VQA val set:  69%|######8   | 1834/2671 [21:27<10:05,  1.38it/s]Evaluating on VQA val set:  69%|######8   | 1835/2671 [21:27<10:01,  1.39it/s]Evaluating on VQA val set:  69%|######8   | 1836/2671 [21:28<10:06,  1.38it/s]Evaluating on VQA val set:  69%|######8   | 1837/2671 [21:29<10:07,  1.37it/s]Evaluating on VQA val set:  69%|######8   | 1838/2671 [21:30<10:12,  1.36it/s]Evaluating on VQA val set:  69%|######8   | 1839/2671 [21:30<10:13,  1.36it/s]Evaluating on VQA val set:  69%|######8   | 1840/2671 [21:31<09:59,  1.39it/s]Evaluating on VQA val set:  69%|######8   | 1841/2671 [21:32<10:03,  1.37it/s]Evaluating on VQA val set:  69%|######8   | 1842/2671 [21:32<09:46,  1.41it/s]Evaluating on VQA val set:  69%|######9   | 1843/2671 [21:33<09:54,  1.39it/s]Evaluating on VQA val set:  69%|######9   | 1844/2671 [21:34<10:02,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1845/2671 [21:35<09:19,  1.48it/s]Evaluating on VQA val set:  69%|######9   | 1846/2671 [21:35<08:44,  1.57it/s]Evaluating on VQA val set:  69%|######9   | 1847/2671 [21:36<08:54,  1.54it/s]Evaluating on VQA val set:  69%|######9   | 1848/2671 [21:36<09:19,  1.47it/s]Evaluating on VQA val set:  69%|######9   | 1849/2671 [21:37<09:11,  1.49it/s]Evaluating on VQA val set:  69%|######9   | 1850/2671 [21:38<09:24,  1.46it/s]Evaluating on VQA val set:  69%|######9   | 1851/2671 [21:39<09:27,  1.44it/s]Evaluating on VQA val set:  69%|######9   | 1852/2671 [21:39<09:44,  1.40it/s]Evaluating on VQA val set:  69%|######9   | 1853/2671 [21:40<09:50,  1.38it/s]Evaluating on VQA val set:  69%|######9   | 1854/2671 [21:41<09:55,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1855/2671 [21:42<09:55,  1.37it/s]Evaluating on VQA val set:  69%|######9   | 1856/2671 [21:42<09:57,  1.36it/s]Evaluating on VQA val set:  70%|######9   | 1857/2671 [21:43<09:53,  1.37it/s]Evaluating on VQA val set:  70%|######9   | 1858/2671 [21:44<09:29,  1.43it/s]Evaluating on VQA val set:  70%|######9   | 1859/2671 [21:44<09:22,  1.44it/s]Evaluating on VQA val set:  70%|######9   | 1860/2671 [21:45<08:47,  1.54it/s]Evaluating on VQA val set:  70%|######9   | 1861/2671 [21:46<08:53,  1.52it/s]Evaluating on VQA val set:  70%|######9   | 1862/2671 [21:46<08:49,  1.53it/s]Evaluating on VQA val set:  70%|######9   | 1863/2671 [21:47<09:03,  1.49it/s]Evaluating on VQA val set:  70%|######9   | 1864/2671 [21:48<08:58,  1.50it/s]Evaluating on VQA val set:  70%|######9   | 1865/2671 [21:48<09:07,  1.47it/s]Evaluating on VQA val set:  70%|######9   | 1866/2671 [21:49<09:23,  1.43it/s]Evaluating on VQA val set:  70%|######9   | 1867/2671 [21:50<09:25,  1.42it/s]Evaluating on VQA val set:  70%|######9   | 1868/2671 [21:50<09:29,  1.41it/s]Evaluating on VQA val set:  70%|######9   | 1869/2671 [21:51<09:29,  1.41it/s]Evaluating on VQA val set:  70%|#######   | 1870/2671 [21:52<09:21,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1871/2671 [21:53<09:21,  1.42it/s]Evaluating on VQA val set:  70%|#######   | 1872/2671 [21:53<09:18,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1873/2671 [21:54<08:56,  1.49it/s]Evaluating on VQA val set:  70%|#######   | 1874/2671 [21:55<09:24,  1.41it/s]Evaluating on VQA val set:  70%|#######   | 1875/2671 [21:55<09:00,  1.47it/s]Evaluating on VQA val set:  70%|#######   | 1876/2671 [21:56<08:48,  1.50it/s]Evaluating on VQA val set:  70%|#######   | 1877/2671 [21:57<08:44,  1.51it/s]Evaluating on VQA val set:  70%|#######   | 1878/2671 [21:57<08:41,  1.52it/s]Evaluating on VQA val set:  70%|#######   | 1879/2671 [21:58<08:54,  1.48it/s]Evaluating on VQA val set:  70%|#######   | 1880/2671 [21:59<09:04,  1.45it/s]Evaluating on VQA val set:  70%|#######   | 1881/2671 [21:59<09:14,  1.43it/s]Evaluating on VQA val set:  70%|#######   | 1882/2671 [22:00<09:29,  1.38it/s]Evaluating on VQA val set:  70%|#######   | 1883/2671 [22:01<09:24,  1.39it/s]Evaluating on VQA val set:  71%|#######   | 1884/2671 [22:01<09:11,  1.43it/s]Evaluating on VQA val set:  71%|#######   | 1885/2671 [22:02<09:23,  1.40it/s]Evaluating on VQA val set:  71%|#######   | 1886/2671 [22:03<09:33,  1.37it/s]Evaluating on VQA val set:  71%|#######   | 1887/2671 [22:04<09:27,  1.38it/s]Evaluating on VQA val set:  71%|#######   | 1888/2671 [22:04<09:32,  1.37it/s]Evaluating on VQA val set:  71%|#######   | 1889/2671 [22:05<09:17,  1.40it/s]Evaluating on VQA val set:  71%|#######   | 1890/2671 [22:06<08:58,  1.45it/s]Evaluating on VQA val set:  71%|#######   | 1891/2671 [22:06<09:00,  1.44it/s]Evaluating on VQA val set:  71%|#######   | 1892/2671 [22:07<09:01,  1.44it/s]Evaluating on VQA val set:  71%|#######   | 1893/2671 [22:08<09:20,  1.39it/s]Evaluating on VQA val set:  71%|#######   | 1894/2671 [22:09<09:27,  1.37it/s]Evaluating on VQA val set:  71%|#######   | 1895/2671 [22:09<09:21,  1.38it/s]Evaluating on VQA val set:  71%|#######   | 1896/2671 [22:10<09:22,  1.38it/s]Evaluating on VQA val set:  71%|#######1  | 1897/2671 [22:11<09:29,  1.36it/s]Evaluating on VQA val set:  71%|#######1  | 1898/2671 [22:12<09:22,  1.37it/s]Evaluating on VQA val set:  71%|#######1  | 1899/2671 [22:12<09:18,  1.38it/s]Evaluating on VQA val set:  71%|#######1  | 1900/2671 [22:13<09:20,  1.38it/s]Evaluating on VQA val set:  71%|#######1  | 1901/2671 [22:14<09:28,  1.35it/s]Evaluating on VQA val set:  71%|#######1  | 1902/2671 [22:15<09:29,  1.35it/s]Evaluating on VQA val set:  71%|#######1  | 1903/2671 [22:15<09:29,  1.35it/s]Evaluating on VQA val set:  71%|#######1  | 1904/2671 [22:16<09:31,  1.34it/s]Evaluating on VQA val set:  71%|#######1  | 1905/2671 [22:17<09:34,  1.33it/s]Evaluating on VQA val set:  71%|#######1  | 1906/2671 [22:18<09:26,  1.35it/s]Evaluating on VQA val set:  71%|#######1  | 1907/2671 [22:18<09:03,  1.41it/s]Evaluating on VQA val set:  71%|#######1  | 1908/2671 [22:19<09:01,  1.41it/s]Evaluating on VQA val set:  71%|#######1  | 1909/2671 [22:20<08:47,  1.44it/s]Evaluating on VQA val set:  72%|#######1  | 1910/2671 [22:20<08:54,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1911/2671 [22:21<09:11,  1.38it/s]Evaluating on VQA val set:  72%|#######1  | 1912/2671 [22:22<08:45,  1.45it/s]Evaluating on VQA val set:  72%|#######1  | 1913/2671 [22:22<08:55,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1914/2671 [22:23<08:57,  1.41it/s]Evaluating on VQA val set:  72%|#######1  | 1915/2671 [22:24<08:50,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1916/2671 [22:25<09:00,  1.40it/s]Evaluating on VQA val set:  72%|#######1  | 1917/2671 [22:25<09:16,  1.36it/s]Evaluating on VQA val set:  72%|#######1  | 1918/2671 [22:26<08:49,  1.42it/s]Evaluating on VQA val set:  72%|#######1  | 1919/2671 [22:27<08:42,  1.44it/s]Evaluating on VQA val set:  72%|#######1  | 1920/2671 [22:27<08:40,  1.44it/s]Evaluating on VQA val set:  72%|#######1  | 1921/2671 [22:28<08:52,  1.41it/s]Evaluating on VQA val set:  72%|#######1  | 1922/2671 [22:29<09:03,  1.38it/s]Evaluating on VQA val set:  72%|#######1  | 1923/2671 [22:30<09:04,  1.37it/s]Evaluating on VQA val set:  72%|#######2  | 1924/2671 [22:30<08:41,  1.43it/s]Evaluating on VQA val set:  72%|#######2  | 1925/2671 [22:31<08:56,  1.39it/s]Evaluating on VQA val set:  72%|#######2  | 1926/2671 [22:32<08:56,  1.39it/s]Evaluating on VQA val set:  72%|#######2  | 1927/2671 [22:32<08:34,  1.45it/s]Evaluating on VQA val set:  72%|#######2  | 1928/2671 [22:33<08:37,  1.44it/s]Evaluating on VQA val set:  72%|#######2  | 1929/2671 [22:34<09:01,  1.37it/s]Evaluating on VQA val set:  72%|#######2  | 1930/2671 [22:35<08:52,  1.39it/s]Evaluating on VQA val set:  72%|#######2  | 1931/2671 [22:35<08:55,  1.38it/s]Evaluating on VQA val set:  72%|#######2  | 1932/2671 [22:36<09:06,  1.35it/s]Evaluating on VQA val set:  72%|#######2  | 1933/2671 [22:37<08:55,  1.38it/s]Evaluating on VQA val set:  72%|#######2  | 1934/2671 [22:37<09:04,  1.35it/s]Evaluating on VQA val set:  72%|#######2  | 1935/2671 [22:38<09:21,  1.31it/s]Evaluating on VQA val set:  72%|#######2  | 1936/2671 [22:39<09:02,  1.35it/s]Evaluating on VQA val set:  73%|#######2  | 1937/2671 [22:40<08:59,  1.36it/s]Evaluating on VQA val set:  73%|#######2  | 1938/2671 [22:40<08:52,  1.38it/s]Evaluating on VQA val set:  73%|#######2  | 1939/2671 [22:41<08:52,  1.37it/s]Evaluating on VQA val set:  73%|#######2  | 1940/2671 [22:42<08:58,  1.36it/s]Evaluating on VQA val set:  73%|#######2  | 1941/2671 [22:43<08:44,  1.39it/s]Evaluating on VQA val set:  73%|#######2  | 1942/2671 [22:43<08:38,  1.41it/s]Evaluating on VQA val set:  73%|#######2  | 1943/2671 [22:44<08:26,  1.44it/s]Evaluating on VQA val set:  73%|#######2  | 1944/2671 [22:45<08:46,  1.38it/s]Evaluating on VQA val set:  73%|#######2  | 1945/2671 [22:45<08:48,  1.37it/s]Evaluating on VQA val set:  73%|#######2  | 1946/2671 [22:46<08:42,  1.39it/s]Evaluating on VQA val set:  73%|#######2  | 1947/2671 [22:47<08:17,  1.46it/s]Evaluating on VQA val set:  73%|#######2  | 1948/2671 [22:47<08:23,  1.44it/s]Evaluating on VQA val set:  73%|#######2  | 1949/2671 [22:48<08:39,  1.39it/s]Evaluating on VQA val set:  73%|#######3  | 1950/2671 [22:49<08:36,  1.40it/s]Evaluating on VQA val set:  73%|#######3  | 1951/2671 [22:50<08:43,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1952/2671 [22:50<08:49,  1.36it/s]Evaluating on VQA val set:  73%|#######3  | 1953/2671 [22:51<08:55,  1.34it/s]Evaluating on VQA val set:  73%|#######3  | 1954/2671 [22:52<08:55,  1.34it/s]Evaluating on VQA val set:  73%|#######3  | 1955/2671 [22:53<08:49,  1.35it/s]Evaluating on VQA val set:  73%|#######3  | 1956/2671 [22:53<08:41,  1.37it/s]Evaluating on VQA val set:  73%|#######3  | 1957/2671 [22:54<08:38,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1958/2671 [22:55<08:31,  1.39it/s]Evaluating on VQA val set:  73%|#######3  | 1959/2671 [22:56<08:39,  1.37it/s]Evaluating on VQA val set:  73%|#######3  | 1960/2671 [22:56<08:34,  1.38it/s]Evaluating on VQA val set:  73%|#######3  | 1961/2671 [22:57<08:13,  1.44it/s]Evaluating on VQA val set:  73%|#######3  | 1962/2671 [22:58<08:19,  1.42it/s]Evaluating on VQA val set:  73%|#######3  | 1963/2671 [22:58<08:32,  1.38it/s]Evaluating on VQA val set:  74%|#######3  | 1964/2671 [22:59<08:15,  1.43it/s]Evaluating on VQA val set:  74%|#######3  | 1965/2671 [23:00<08:22,  1.41it/s]Evaluating on VQA val set:  74%|#######3  | 1966/2671 [23:01<08:17,  1.42it/s]Evaluating on VQA val set:  74%|#######3  | 1967/2671 [23:01<08:24,  1.39it/s]Evaluating on VQA val set:  74%|#######3  | 1968/2671 [23:02<08:35,  1.36it/s]Evaluating on VQA val set:  74%|#######3  | 1969/2671 [23:03<08:45,  1.34it/s]Evaluating on VQA val set:  74%|#######3  | 1970/2671 [23:03<08:26,  1.38it/s]Evaluating on VQA val set:  74%|#######3  | 1971/2671 [23:04<08:24,  1.39it/s]Evaluating on VQA val set:  74%|#######3  | 1972/2671 [23:05<08:23,  1.39it/s]Evaluating on VQA val set:  74%|#######3  | 1973/2671 [23:06<08:22,  1.39it/s]Evaluating on VQA val set:  74%|#######3  | 1974/2671 [23:06<08:21,  1.39it/s]Evaluating on VQA val set:  74%|#######3  | 1975/2671 [23:07<08:31,  1.36it/s]Evaluating on VQA val set:  74%|#######3  | 1976/2671 [23:08<08:15,  1.40it/s]Evaluating on VQA val set:  74%|#######4  | 1977/2671 [23:08<08:00,  1.44it/s]Evaluating on VQA val set:  74%|#######4  | 1978/2671 [23:09<08:05,  1.43it/s]Evaluating on VQA val set:  74%|#######4  | 1979/2671 [23:10<08:18,  1.39it/s]Evaluating on VQA val set:  74%|#######4  | 1980/2671 [23:11<08:17,  1.39it/s]Evaluating on VQA val set:  74%|#######4  | 1981/2671 [23:11<08:16,  1.39it/s]Evaluating on VQA val set:  74%|#######4  | 1982/2671 [23:12<08:09,  1.41it/s]Evaluating on VQA val set:  74%|#######4  | 1983/2671 [23:13<08:28,  1.35it/s]Evaluating on VQA val set:  74%|#######4  | 1984/2671 [23:14<08:09,  1.40it/s]Evaluating on VQA val set:  74%|#######4  | 1985/2671 [23:14<08:05,  1.41it/s]Evaluating on VQA val set:  74%|#######4  | 1986/2671 [23:15<08:01,  1.42it/s]Evaluating on VQA val set:  74%|#######4  | 1987/2671 [23:15<07:33,  1.51it/s]Evaluating on VQA val set:  74%|#######4  | 1988/2671 [23:16<07:25,  1.53it/s]Evaluating on VQA val set:  74%|#######4  | 1989/2671 [23:17<07:45,  1.46it/s]Evaluating on VQA val set:  75%|#######4  | 1990/2671 [23:18<07:41,  1.48it/s]Evaluating on VQA val set:  75%|#######4  | 1991/2671 [23:18<07:28,  1.51it/s]Evaluating on VQA val set:  75%|#######4  | 1992/2671 [23:19<07:53,  1.43it/s]Evaluating on VQA val set:  75%|#######4  | 1993/2671 [23:20<07:53,  1.43it/s]Evaluating on VQA val set:  75%|#######4  | 1994/2671 [23:20<07:45,  1.45it/s]Evaluating on VQA val set:  75%|#######4  | 1995/2671 [23:21<07:57,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 1996/2671 [23:22<08:01,  1.40it/s]Evaluating on VQA val set:  75%|#######4  | 1997/2671 [23:22<07:56,  1.42it/s]Evaluating on VQA val set:  75%|#######4  | 1998/2671 [23:23<08:10,  1.37it/s]Evaluating on VQA val set:  75%|#######4  | 1999/2671 [23:24<08:15,  1.36it/s]Evaluating on VQA val set:  75%|#######4  | 2000/2671 [23:25<08:07,  1.38it/s]Evaluating on VQA val set:  75%|#######4  | 2001/2671 [23:25<07:54,  1.41it/s]Evaluating on VQA val set:  75%|#######4  | 2002/2671 [23:26<07:48,  1.43it/s]Evaluating on VQA val set:  75%|#######4  | 2003/2671 [23:27<07:44,  1.44it/s]Evaluating on VQA val set:  75%|#######5  | 2004/2671 [23:27<07:52,  1.41it/s]Evaluating on VQA val set:  75%|#######5  | 2005/2671 [23:28<08:00,  1.39it/s]Evaluating on VQA val set:  75%|#######5  | 2006/2671 [23:29<08:11,  1.35it/s]Evaluating on VQA val set:  75%|#######5  | 2007/2671 [23:30<08:14,  1.34it/s]Evaluating on VQA val set:  75%|#######5  | 2008/2671 [23:30<08:05,  1.37it/s]Evaluating on VQA val set:  75%|#######5  | 2009/2671 [23:31<07:54,  1.39it/s]Evaluating on VQA val set:  75%|#######5  | 2010/2671 [23:32<07:34,  1.45it/s]Evaluating on VQA val set:  75%|#######5  | 2011/2671 [23:32<07:29,  1.47it/s]Evaluating on VQA val set:  75%|#######5  | 2012/2671 [23:33<07:39,  1.43it/s]Evaluating on VQA val set:  75%|#######5  | 2013/2671 [23:34<07:49,  1.40it/s]Evaluating on VQA val set:  75%|#######5  | 2014/2671 [23:35<07:33,  1.45it/s]Evaluating on VQA val set:  75%|#######5  | 2015/2671 [23:35<07:31,  1.45it/s]Evaluating on VQA val set:  75%|#######5  | 2016/2671 [23:36<07:35,  1.44it/s]Evaluating on VQA val set:  76%|#######5  | 2017/2671 [23:37<07:38,  1.43it/s]Evaluating on VQA val set:  76%|#######5  | 2018/2671 [23:37<07:27,  1.46it/s]Evaluating on VQA val set:  76%|#######5  | 2019/2671 [23:38<07:30,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2020/2671 [23:39<07:36,  1.43it/s]Evaluating on VQA val set:  76%|#######5  | 2021/2671 [23:39<07:28,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2022/2671 [23:40<07:02,  1.54it/s]Evaluating on VQA val set:  76%|#######5  | 2023/2671 [23:41<07:10,  1.51it/s]Evaluating on VQA val set:  76%|#######5  | 2024/2671 [23:41<07:27,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2025/2671 [23:42<07:39,  1.41it/s]Evaluating on VQA val set:  76%|#######5  | 2026/2671 [23:43<07:45,  1.39it/s]Evaluating on VQA val set:  76%|#######5  | 2027/2671 [23:44<07:24,  1.45it/s]Evaluating on VQA val set:  76%|#######5  | 2028/2671 [23:44<07:19,  1.46it/s]Evaluating on VQA val set:  76%|#######5  | 2029/2671 [23:45<07:24,  1.45it/s]Evaluating on VQA val set:  76%|#######6  | 2030/2671 [23:46<07:11,  1.49it/s]Evaluating on VQA val set:  76%|#######6  | 2031/2671 [23:46<07:09,  1.49it/s]Evaluating on VQA val set:  76%|#######6  | 2032/2671 [23:47<07:18,  1.46it/s]Evaluating on VQA val set:  76%|#######6  | 2033/2671 [23:48<07:22,  1.44it/s]Evaluating on VQA val set:  76%|#######6  | 2034/2671 [23:48<07:13,  1.47it/s]Evaluating on VQA val set:  76%|#######6  | 2035/2671 [23:49<07:01,  1.51it/s]Evaluating on VQA val set:  76%|#######6  | 2036/2671 [23:50<07:04,  1.50it/s]Evaluating on VQA val set:  76%|#######6  | 2037/2671 [23:50<06:54,  1.53it/s]Evaluating on VQA val set:  76%|#######6  | 2038/2671 [23:51<07:02,  1.50it/s]Evaluating on VQA val set:  76%|#######6  | 2039/2671 [23:52<07:28,  1.41it/s]Evaluating on VQA val set:  76%|#######6  | 2040/2671 [23:52<07:37,  1.38it/s]Evaluating on VQA val set:  76%|#######6  | 2041/2671 [23:53<07:15,  1.45it/s]Evaluating on VQA val set:  76%|#######6  | 2042/2671 [23:54<07:02,  1.49it/s]Evaluating on VQA val set:  76%|#######6  | 2043/2671 [23:54<07:10,  1.46it/s]Evaluating on VQA val set:  77%|#######6  | 2044/2671 [23:55<07:10,  1.46it/s]Evaluating on VQA val set:  77%|#######6  | 2045/2671 [23:56<07:19,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2046/2671 [23:57<07:21,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2047/2671 [23:57<07:20,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2048/2671 [23:58<07:19,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2049/2671 [23:59<07:05,  1.46it/s]Evaluating on VQA val set:  77%|#######6  | 2050/2671 [23:59<06:55,  1.50it/s]Evaluating on VQA val set:  77%|#######6  | 2051/2671 [24:00<07:00,  1.47it/s]Evaluating on VQA val set:  77%|#######6  | 2052/2671 [24:01<07:08,  1.44it/s]Evaluating on VQA val set:  77%|#######6  | 2053/2671 [24:01<07:00,  1.47it/s]Evaluating on VQA val set:  77%|#######6  | 2054/2671 [24:02<07:14,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2055/2671 [24:03<07:12,  1.42it/s]Evaluating on VQA val set:  77%|#######6  | 2056/2671 [24:03<07:08,  1.44it/s]Evaluating on VQA val set:  77%|#######7  | 2057/2671 [24:04<06:53,  1.48it/s]Evaluating on VQA val set:  77%|#######7  | 2058/2671 [24:05<06:51,  1.49it/s]Evaluating on VQA val set:  77%|#######7  | 2059/2671 [24:05<07:02,  1.45it/s]Evaluating on VQA val set:  77%|#######7  | 2060/2671 [24:06<06:49,  1.49it/s]Evaluating on VQA val set:  77%|#######7  | 2061/2671 [24:07<06:50,  1.49it/s]Evaluating on VQA val set:  77%|#######7  | 2062/2671 [24:08<07:06,  1.43it/s]Evaluating on VQA val set:  77%|#######7  | 2063/2671 [24:08<07:18,  1.39it/s]Evaluating on VQA val set:  77%|#######7  | 2064/2671 [24:09<07:14,  1.40it/s]Evaluating on VQA val set:  77%|#######7  | 2065/2671 [24:10<07:20,  1.38it/s]Evaluating on VQA val set:  77%|#######7  | 2066/2671 [24:11<07:24,  1.36it/s]Evaluating on VQA val set:  77%|#######7  | 2067/2671 [24:11<07:20,  1.37it/s]Evaluating on VQA val set:  77%|#######7  | 2068/2671 [24:12<07:31,  1.34it/s]Evaluating on VQA val set:  77%|#######7  | 2069/2671 [24:13<07:28,  1.34it/s]Evaluating on VQA val set:  77%|#######7  | 2070/2671 [24:13<07:23,  1.35it/s]Evaluating on VQA val set:  78%|#######7  | 2071/2671 [24:14<07:22,  1.35it/s]Evaluating on VQA val set:  78%|#######7  | 2072/2671 [24:15<07:19,  1.36it/s]Evaluating on VQA val set:  78%|#######7  | 2073/2671 [24:16<07:09,  1.39it/s]Evaluating on VQA val set:  78%|#######7  | 2074/2671 [24:16<07:12,  1.38it/s]Evaluating on VQA val set:  78%|#######7  | 2075/2671 [24:17<07:16,  1.37it/s]Evaluating on VQA val set:  78%|#######7  | 2076/2671 [24:18<07:05,  1.40it/s]Evaluating on VQA val set:  78%|#######7  | 2077/2671 [24:18<06:50,  1.45it/s]Evaluating on VQA val set:  78%|#######7  | 2078/2671 [24:19<06:49,  1.45it/s]Evaluating on VQA val set:  78%|#######7  | 2079/2671 [24:20<06:57,  1.42it/s]Evaluating on VQA val set:  78%|#######7  | 2080/2671 [24:21<07:05,  1.39it/s]Evaluating on VQA val set:  78%|#######7  | 2081/2671 [24:21<07:05,  1.39it/s]Evaluating on VQA val set:  78%|#######7  | 2082/2671 [24:22<07:04,  1.39it/s]Evaluating on VQA val set:  78%|#######7  | 2083/2671 [24:23<07:11,  1.36it/s]Evaluating on VQA val set:  78%|#######8  | 2084/2671 [24:24<07:08,  1.37it/s]Evaluating on VQA val set:  78%|#######8  | 2085/2671 [24:24<07:15,  1.35it/s]Evaluating on VQA val set:  78%|#######8  | 2086/2671 [24:25<07:03,  1.38it/s]Evaluating on VQA val set:  78%|#######8  | 2087/2671 [24:26<07:05,  1.37it/s]Evaluating on VQA val set:  78%|#######8  | 2088/2671 [24:27<07:15,  1.34it/s]Evaluating on VQA val set:  78%|#######8  | 2089/2671 [24:27<07:18,  1.33it/s]Evaluating on VQA val set:  78%|#######8  | 2090/2671 [24:28<07:15,  1.33it/s]Evaluating on VQA val set:  78%|#######8  | 2091/2671 [24:29<06:47,  1.42it/s]Evaluating on VQA val set:  78%|#######8  | 2092/2671 [24:29<06:51,  1.41it/s]Evaluating on VQA val set:  78%|#######8  | 2093/2671 [24:30<06:43,  1.43it/s]Evaluating on VQA val set:  78%|#######8  | 2094/2671 [24:31<06:45,  1.42it/s]Evaluating on VQA val set:  78%|#######8  | 2095/2671 [24:31<06:41,  1.43it/s]Evaluating on VQA val set:  78%|#######8  | 2096/2671 [24:32<06:32,  1.47it/s]Evaluating on VQA val set:  79%|#######8  | 2097/2671 [24:33<06:41,  1.43it/s]Evaluating on VQA val set:  79%|#######8  | 2098/2671 [24:33<06:31,  1.46it/s]Evaluating on VQA val set:  79%|#######8  | 2099/2671 [24:34<06:30,  1.47it/s]Evaluating on VQA val set:  79%|#######8  | 2100/2671 [24:35<06:34,  1.45it/s]Evaluating on VQA val set:  79%|#######8  | 2101/2671 [24:36<06:40,  1.42it/s]Evaluating on VQA val set:  79%|#######8  | 2102/2671 [24:36<06:47,  1.40it/s]Evaluating on VQA val set:  79%|#######8  | 2103/2671 [24:37<06:43,  1.41it/s]Evaluating on VQA val set:  79%|#######8  | 2104/2671 [24:38<06:39,  1.42it/s]Evaluating on VQA val set:  79%|#######8  | 2105/2671 [24:39<06:58,  1.35it/s]Evaluating on VQA val set:  79%|#######8  | 2106/2671 [24:39<07:01,  1.34it/s]Evaluating on VQA val set:  79%|#######8  | 2107/2671 [24:40<06:49,  1.38it/s]Evaluating on VQA val set:  79%|#######8  | 2108/2671 [24:41<06:58,  1.34it/s]Evaluating on VQA val set:  79%|#######8  | 2109/2671 [24:41<06:54,  1.36it/s]Evaluating on VQA val set:  79%|#######8  | 2110/2671 [24:42<06:36,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2111/2671 [24:43<06:34,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2112/2671 [24:44<06:32,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2113/2671 [24:44<06:36,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2114/2671 [24:45<06:42,  1.38it/s]Evaluating on VQA val set:  79%|#######9  | 2115/2671 [24:46<06:39,  1.39it/s]Evaluating on VQA val set:  79%|#######9  | 2116/2671 [24:46<06:43,  1.38it/s]Evaluating on VQA val set:  79%|#######9  | 2117/2671 [24:47<06:31,  1.41it/s]Evaluating on VQA val set:  79%|#######9  | 2118/2671 [24:48<06:29,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2119/2671 [24:49<06:28,  1.42it/s]Evaluating on VQA val set:  79%|#######9  | 2120/2671 [24:49<06:40,  1.37it/s]Evaluating on VQA val set:  79%|#######9  | 2121/2671 [24:50<06:37,  1.38it/s]Evaluating on VQA val set:  79%|#######9  | 2122/2671 [24:51<06:35,  1.39it/s]Evaluating on VQA val set:  79%|#######9  | 2123/2671 [24:51<06:38,  1.37it/s]Evaluating on VQA val set:  80%|#######9  | 2124/2671 [24:52<06:36,  1.38it/s]Evaluating on VQA val set:  80%|#######9  | 2125/2671 [24:53<06:24,  1.42it/s]Evaluating on VQA val set:  80%|#######9  | 2126/2671 [24:54<06:19,  1.44it/s]Evaluating on VQA val set:  80%|#######9  | 2127/2671 [24:54<06:27,  1.40it/s]Evaluating on VQA val set:  80%|#######9  | 2128/2671 [24:55<06:26,  1.40it/s]Evaluating on VQA val set:  80%|#######9  | 2129/2671 [24:56<06:37,  1.36it/s]Evaluating on VQA val set:  80%|#######9  | 2130/2671 [24:56<06:22,  1.42it/s]Evaluating on VQA val set:  80%|#######9  | 2131/2671 [24:57<06:29,  1.39it/s]Evaluating on VQA val set:  80%|#######9  | 2132/2671 [24:58<06:31,  1.38it/s]Evaluating on VQA val set:  80%|#######9  | 2133/2671 [24:59<06:18,  1.42it/s]Evaluating on VQA val set:  80%|#######9  | 2134/2671 [24:59<06:11,  1.45it/s]Evaluating on VQA val set:  80%|#######9  | 2135/2671 [25:00<06:11,  1.44it/s]Evaluating on VQA val set:  80%|#######9  | 2136/2671 [25:01<05:54,  1.51it/s]Evaluating on VQA val set:  80%|########  | 2137/2671 [25:01<06:05,  1.46it/s]Evaluating on VQA val set:  80%|########  | 2138/2671 [25:02<06:10,  1.44it/s]Evaluating on VQA val set:  80%|########  | 2139/2671 [25:03<06:17,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2140/2671 [25:03<06:17,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2141/2671 [25:04<06:05,  1.45it/s]Evaluating on VQA val set:  80%|########  | 2142/2671 [25:05<06:18,  1.40it/s]Evaluating on VQA val set:  80%|########  | 2143/2671 [25:06<06:15,  1.40it/s]Evaluating on VQA val set:  80%|########  | 2144/2671 [25:06<06:13,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2145/2671 [25:07<06:18,  1.39it/s]Evaluating on VQA val set:  80%|########  | 2146/2671 [25:08<06:25,  1.36it/s]Evaluating on VQA val set:  80%|########  | 2147/2671 [25:08<06:24,  1.36it/s]Evaluating on VQA val set:  80%|########  | 2148/2671 [25:09<06:12,  1.41it/s]Evaluating on VQA val set:  80%|########  | 2149/2671 [25:10<06:02,  1.44it/s]Evaluating on VQA val set:  80%|########  | 2150/2671 [25:11<06:05,  1.43it/s]Evaluating on VQA val set:  81%|########  | 2151/2671 [25:11<06:07,  1.42it/s]Evaluating on VQA val set:  81%|########  | 2152/2671 [25:12<06:09,  1.41it/s]Evaluating on VQA val set:  81%|########  | 2153/2671 [25:13<05:59,  1.44it/s]Evaluating on VQA val set:  81%|########  | 2154/2671 [25:13<05:42,  1.51it/s]Evaluating on VQA val set:  81%|########  | 2155/2671 [25:14<05:44,  1.50it/s]Evaluating on VQA val set:  81%|########  | 2156/2671 [25:15<05:52,  1.46it/s]Evaluating on VQA val set:  81%|########  | 2157/2671 [25:15<05:56,  1.44it/s]Evaluating on VQA val set:  81%|########  | 2158/2671 [25:16<05:53,  1.45it/s]Evaluating on VQA val set:  81%|########  | 2159/2671 [25:17<05:49,  1.46it/s]Evaluating on VQA val set:  81%|########  | 2160/2671 [25:17<06:09,  1.38it/s]Evaluating on VQA val set:  81%|########  | 2161/2671 [25:18<06:08,  1.38it/s]Evaluating on VQA val set:  81%|########  | 2162/2671 [25:19<05:59,  1.41it/s]Evaluating on VQA val set:  81%|########  | 2163/2671 [25:20<06:03,  1.40it/s]Evaluating on VQA val set:  81%|########1 | 2164/2671 [25:20<05:58,  1.41it/s]Evaluating on VQA val set:  81%|########1 | 2165/2671 [25:21<05:56,  1.42it/s]Evaluating on VQA val set:  81%|########1 | 2166/2671 [25:22<06:07,  1.37it/s]Evaluating on VQA val set:  81%|########1 | 2167/2671 [25:22<06:00,  1.40it/s]Evaluating on VQA val set:  81%|########1 | 2168/2671 [25:23<06:04,  1.38it/s]Evaluating on VQA val set:  81%|########1 | 2169/2671 [25:24<05:59,  1.40it/s]Evaluating on VQA val set:  81%|########1 | 2170/2671 [25:25<06:05,  1.37it/s]Evaluating on VQA val set:  81%|########1 | 2171/2671 [25:25<05:59,  1.39it/s]Evaluating on VQA val set:  81%|########1 | 2172/2671 [25:26<06:08,  1.35it/s]Evaluating on VQA val set:  81%|########1 | 2173/2671 [25:27<06:01,  1.38it/s]Evaluating on VQA val set:  81%|########1 | 2174/2671 [25:28<05:57,  1.39it/s]Evaluating on VQA val set:  81%|########1 | 2175/2671 [25:28<05:52,  1.41it/s]Evaluating on VQA val set:  81%|########1 | 2176/2671 [25:29<05:43,  1.44it/s]Evaluating on VQA val set:  82%|########1 | 2177/2671 [25:30<05:43,  1.44it/s]Evaluating on VQA val set:  82%|########1 | 2178/2671 [25:30<05:46,  1.42it/s]Evaluating on VQA val set:  82%|########1 | 2179/2671 [25:31<05:35,  1.47it/s]Evaluating on VQA val set:  82%|########1 | 2180/2671 [25:32<05:27,  1.50it/s]Evaluating on VQA val set:  82%|########1 | 2181/2671 [25:32<05:35,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2182/2671 [25:33<05:35,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2183/2671 [25:34<05:42,  1.42it/s]Evaluating on VQA val set:  82%|########1 | 2184/2671 [25:34<05:32,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2185/2671 [25:35<05:21,  1.51it/s]Evaluating on VQA val set:  82%|########1 | 2186/2671 [25:36<05:12,  1.55it/s]Evaluating on VQA val set:  82%|########1 | 2187/2671 [25:36<05:25,  1.49it/s]Evaluating on VQA val set:  82%|########1 | 2188/2671 [25:37<05:30,  1.46it/s]Evaluating on VQA val set:  82%|########1 | 2189/2671 [25:38<05:22,  1.50it/s]Evaluating on VQA val set:  82%|########1 | 2190/2671 [25:38<05:23,  1.49it/s]Evaluating on VQA val set:  82%|########2 | 2191/2671 [25:39<05:27,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2192/2671 [25:40<05:41,  1.40it/s]Evaluating on VQA val set:  82%|########2 | 2193/2671 [25:41<05:38,  1.41it/s]Evaluating on VQA val set:  82%|########2 | 2194/2671 [25:41<05:28,  1.45it/s]Evaluating on VQA val set:  82%|########2 | 2195/2671 [25:42<05:28,  1.45it/s]Evaluating on VQA val set:  82%|########2 | 2196/2671 [25:43<05:32,  1.43it/s]Evaluating on VQA val set:  82%|########2 | 2197/2671 [25:43<05:24,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2198/2671 [25:44<05:33,  1.42it/s]Evaluating on VQA val set:  82%|########2 | 2199/2671 [25:45<05:34,  1.41it/s]Evaluating on VQA val set:  82%|########2 | 2200/2671 [25:45<05:38,  1.39it/s]Evaluating on VQA val set:  82%|########2 | 2201/2671 [25:46<05:48,  1.35it/s]Evaluating on VQA val set:  82%|########2 | 2202/2671 [25:47<05:21,  1.46it/s]Evaluating on VQA val set:  82%|########2 | 2203/2671 [25:48<05:24,  1.44it/s]Evaluating on VQA val set:  83%|########2 | 2204/2671 [25:48<05:33,  1.40it/s]Evaluating on VQA val set:  83%|########2 | 2205/2671 [25:49<05:35,  1.39it/s]Evaluating on VQA val set:  83%|########2 | 2206/2671 [25:50<05:36,  1.38it/s]Evaluating on VQA val set:  83%|########2 | 2207/2671 [25:51<05:41,  1.36it/s]Evaluating on VQA val set:  83%|########2 | 2208/2671 [25:51<05:40,  1.36it/s]Evaluating on VQA val set:  83%|########2 | 2209/2671 [25:52<05:40,  1.36it/s]Evaluating on VQA val set:  83%|########2 | 2210/2671 [25:53<05:35,  1.37it/s]Evaluating on VQA val set:  83%|########2 | 2211/2671 [25:53<05:30,  1.39it/s]Evaluating on VQA val set:  83%|########2 | 2212/2671 [25:54<05:22,  1.43it/s]Evaluating on VQA val set:  83%|########2 | 2213/2671 [25:55<05:27,  1.40it/s]Evaluating on VQA val set:  83%|########2 | 2214/2671 [25:55<05:17,  1.44it/s]Evaluating on VQA val set:  83%|########2 | 2215/2671 [25:56<05:22,  1.41it/s]Evaluating on VQA val set:  83%|########2 | 2216/2671 [25:57<05:14,  1.44it/s]Evaluating on VQA val set:  83%|########3 | 2217/2671 [25:58<05:25,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2218/2671 [25:58<05:22,  1.41it/s]Evaluating on VQA val set:  83%|########3 | 2219/2671 [25:59<05:15,  1.43it/s]Evaluating on VQA val set:  83%|########3 | 2220/2671 [26:00<05:21,  1.40it/s]Evaluating on VQA val set:  83%|########3 | 2221/2671 [26:01<05:29,  1.36it/s]Evaluating on VQA val set:  83%|########3 | 2222/2671 [26:01<05:23,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2223/2671 [26:02<05:28,  1.36it/s]Evaluating on VQA val set:  83%|########3 | 2224/2671 [26:03<05:28,  1.36it/s]Evaluating on VQA val set:  83%|########3 | 2225/2671 [26:03<05:20,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2226/2671 [26:04<05:28,  1.35it/s]Evaluating on VQA val set:  83%|########3 | 2227/2671 [26:05<05:21,  1.38it/s]Evaluating on VQA val set:  83%|########3 | 2228/2671 [26:06<05:13,  1.41it/s]Evaluating on VQA val set:  83%|########3 | 2229/2671 [26:06<05:17,  1.39it/s]Evaluating on VQA val set:  83%|########3 | 2230/2671 [26:07<05:14,  1.40it/s]Evaluating on VQA val set:  84%|########3 | 2231/2671 [26:08<05:21,  1.37it/s]Evaluating on VQA val set:  84%|########3 | 2232/2671 [26:09<05:25,  1.35it/s]Evaluating on VQA val set:  84%|########3 | 2233/2671 [26:09<05:19,  1.37it/s]Evaluating on VQA val set:  84%|########3 | 2234/2671 [26:10<05:10,  1.41it/s]Evaluating on VQA val set:  84%|########3 | 2235/2671 [26:11<05:16,  1.38it/s]Evaluating on VQA val set:  84%|########3 | 2236/2671 [26:11<05:13,  1.39it/s]Evaluating on VQA val set:  84%|########3 | 2237/2671 [26:12<05:18,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2238/2671 [26:13<05:05,  1.42it/s]Evaluating on VQA val set:  84%|########3 | 2239/2671 [26:13<05:09,  1.40it/s]Evaluating on VQA val set:  84%|########3 | 2240/2671 [26:14<05:07,  1.40it/s]Evaluating on VQA val set:  84%|########3 | 2241/2671 [26:15<05:16,  1.36it/s]Evaluating on VQA val set:  84%|########3 | 2242/2671 [26:16<05:18,  1.35it/s]Evaluating on VQA val set:  84%|########3 | 2243/2671 [26:16<05:12,  1.37it/s]Evaluating on VQA val set:  84%|########4 | 2244/2671 [26:17<05:12,  1.37it/s]Evaluating on VQA val set:  84%|########4 | 2245/2671 [26:18<05:11,  1.37it/s]Evaluating on VQA val set:  84%|########4 | 2246/2671 [26:19<05:04,  1.40it/s]Evaluating on VQA val set:  84%|########4 | 2247/2671 [26:19<05:04,  1.39it/s]Evaluating on VQA val set:  84%|########4 | 2248/2671 [26:20<04:53,  1.44it/s]Evaluating on VQA val set:  84%|########4 | 2249/2671 [26:21<04:46,  1.47it/s]Evaluating on VQA val set:  84%|########4 | 2250/2671 [26:21<04:50,  1.45it/s]Evaluating on VQA val set:  84%|########4 | 2251/2671 [26:22<04:44,  1.47it/s]Evaluating on VQA val set:  84%|########4 | 2252/2671 [26:23<04:57,  1.41it/s]Evaluating on VQA val set:  84%|########4 | 2253/2671 [26:24<05:01,  1.39it/s]Evaluating on VQA val set:  84%|########4 | 2254/2671 [26:24<05:05,  1.36it/s]Evaluating on VQA val set:  84%|########4 | 2255/2671 [26:25<05:01,  1.38it/s]Evaluating on VQA val set:  84%|########4 | 2256/2671 [26:26<04:46,  1.45it/s]Evaluating on VQA val set:  85%|########4 | 2257/2671 [26:26<04:46,  1.45it/s]Evaluating on VQA val set:  85%|########4 | 2258/2671 [26:27<04:43,  1.46it/s]Evaluating on VQA val set:  85%|########4 | 2259/2671 [26:28<04:50,  1.42it/s]Evaluating on VQA val set:  85%|########4 | 2260/2671 [26:28<04:45,  1.44it/s]Evaluating on VQA val set:  85%|########4 | 2261/2671 [26:29<04:44,  1.44it/s]Evaluating on VQA val set:  85%|########4 | 2262/2671 [26:30<04:48,  1.42it/s]Evaluating on VQA val set:  85%|########4 | 2263/2671 [26:30<04:44,  1.43it/s]Evaluating on VQA val set:  85%|########4 | 2264/2671 [26:31<04:49,  1.40it/s]Evaluating on VQA val set:  85%|########4 | 2265/2671 [26:32<04:55,  1.37it/s]Evaluating on VQA val set:  85%|########4 | 2266/2671 [26:33<04:51,  1.39it/s]Evaluating on VQA val set:  85%|########4 | 2267/2671 [26:33<04:52,  1.38it/s]Evaluating on VQA val set:  85%|########4 | 2268/2671 [26:34<04:55,  1.36it/s]Evaluating on VQA val set:  85%|########4 | 2269/2671 [26:35<04:49,  1.39it/s]Evaluating on VQA val set:  85%|########4 | 2270/2671 [26:36<04:44,  1.41it/s]Evaluating on VQA val set:  85%|########5 | 2271/2671 [26:36<04:45,  1.40it/s]Evaluating on VQA val set:  85%|########5 | 2272/2671 [26:37<04:47,  1.39it/s]Evaluating on VQA val set:  85%|########5 | 2273/2671 [26:38<04:40,  1.42it/s]Evaluating on VQA val set:  85%|########5 | 2274/2671 [26:38<04:43,  1.40it/s]Evaluating on VQA val set:  85%|########5 | 2275/2671 [26:39<04:40,  1.41it/s]Evaluating on VQA val set:  85%|########5 | 2276/2671 [26:40<04:42,  1.40it/s]Evaluating on VQA val set:  85%|########5 | 2277/2671 [26:41<04:44,  1.39it/s]Evaluating on VQA val set:  85%|########5 | 2278/2671 [26:41<04:32,  1.44it/s]Evaluating on VQA val set:  85%|########5 | 2279/2671 [26:42<04:14,  1.54it/s]Evaluating on VQA val set:  85%|########5 | 2280/2671 [26:42<04:19,  1.51it/s]Evaluating on VQA val set:  85%|########5 | 2281/2671 [26:43<04:29,  1.45it/s]Evaluating on VQA val set:  85%|########5 | 2282/2671 [26:44<04:31,  1.43it/s]Evaluating on VQA val set:  85%|########5 | 2283/2671 [26:45<04:32,  1.42it/s]Evaluating on VQA val set:  86%|########5 | 2284/2671 [26:45<04:34,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2285/2671 [26:46<04:35,  1.40it/s]Evaluating on VQA val set:  86%|########5 | 2286/2671 [26:47<04:30,  1.42it/s]Evaluating on VQA val set:  86%|########5 | 2287/2671 [26:47<04:28,  1.43it/s]Evaluating on VQA val set:  86%|########5 | 2288/2671 [26:48<04:20,  1.47it/s]Evaluating on VQA val set:  86%|########5 | 2289/2671 [26:49<04:29,  1.42it/s]Evaluating on VQA val set:  86%|########5 | 2290/2671 [26:50<04:30,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2291/2671 [26:50<04:19,  1.47it/s]Evaluating on VQA val set:  86%|########5 | 2292/2671 [26:51<04:27,  1.42it/s]Evaluating on VQA val set:  86%|########5 | 2293/2671 [26:52<04:32,  1.39it/s]Evaluating on VQA val set:  86%|########5 | 2294/2671 [26:52<04:33,  1.38it/s]Evaluating on VQA val set:  86%|########5 | 2295/2671 [26:53<04:36,  1.36it/s]Evaluating on VQA val set:  86%|########5 | 2296/2671 [26:54<04:25,  1.41it/s]Evaluating on VQA val set:  86%|########5 | 2297/2671 [26:55<04:30,  1.38it/s]Evaluating on VQA val set:  86%|########6 | 2298/2671 [26:55<04:31,  1.37it/s]Evaluating on VQA val set:  86%|########6 | 2299/2671 [26:56<04:20,  1.43it/s]Evaluating on VQA val set:  86%|########6 | 2300/2671 [26:57<04:22,  1.41it/s]Evaluating on VQA val set:  86%|########6 | 2301/2671 [26:57<04:25,  1.39it/s]Evaluating on VQA val set:  86%|########6 | 2302/2671 [26:58<04:20,  1.41it/s]Evaluating on VQA val set:  86%|########6 | 2303/2671 [26:59<04:22,  1.40it/s]Evaluating on VQA val set:  86%|########6 | 2304/2671 [27:00<04:15,  1.44it/s]Evaluating on VQA val set:  86%|########6 | 2305/2671 [27:00<04:12,  1.45it/s]Evaluating on VQA val set:  86%|########6 | 2306/2671 [27:01<04:13,  1.44it/s]Evaluating on VQA val set:  86%|########6 | 2307/2671 [27:02<04:09,  1.46it/s]Evaluating on VQA val set:  86%|########6 | 2308/2671 [27:02<04:11,  1.44it/s]Evaluating on VQA val set:  86%|########6 | 2309/2671 [27:03<04:12,  1.44it/s]Evaluating on VQA val set:  86%|########6 | 2310/2671 [27:04<04:12,  1.43it/s]Evaluating on VQA val set:  87%|########6 | 2311/2671 [27:04<04:09,  1.44it/s]Evaluating on VQA val set:  87%|########6 | 2312/2671 [27:05<04:09,  1.44it/s]Evaluating on VQA val set:  87%|########6 | 2313/2671 [27:06<04:06,  1.45it/s]Evaluating on VQA val set:  87%|########6 | 2314/2671 [27:06<04:02,  1.47it/s]Evaluating on VQA val set:  87%|########6 | 2315/2671 [27:07<03:58,  1.49it/s]Evaluating on VQA val set:  87%|########6 | 2316/2671 [27:08<04:08,  1.43it/s]Evaluating on VQA val set:  87%|########6 | 2317/2671 [27:09<04:09,  1.42it/s]Evaluating on VQA val set:  87%|########6 | 2318/2671 [27:09<04:09,  1.41it/s]Evaluating on VQA val set:  87%|########6 | 2319/2671 [27:10<04:18,  1.36it/s]Evaluating on VQA val set:  87%|########6 | 2320/2671 [27:11<04:11,  1.39it/s]Evaluating on VQA val set:  87%|########6 | 2321/2671 [27:11<04:17,  1.36it/s]Evaluating on VQA val set:  87%|########6 | 2322/2671 [27:12<04:03,  1.43it/s]Evaluating on VQA val set:  87%|########6 | 2323/2671 [27:13<04:05,  1.42it/s]Evaluating on VQA val set:  87%|########7 | 2324/2671 [27:13<04:01,  1.44it/s]Evaluating on VQA val set:  87%|########7 | 2325/2671 [27:14<03:55,  1.47it/s]Evaluating on VQA val set:  87%|########7 | 2326/2671 [27:15<04:00,  1.43it/s]Evaluating on VQA val set:  87%|########7 | 2327/2671 [27:16<04:03,  1.41it/s]Evaluating on VQA val set:  87%|########7 | 2328/2671 [27:16<04:08,  1.38it/s]Evaluating on VQA val set:  87%|########7 | 2329/2671 [27:17<04:05,  1.39it/s]Evaluating on VQA val set:  87%|########7 | 2330/2671 [27:18<04:03,  1.40it/s]Evaluating on VQA val set:  87%|########7 | 2331/2671 [27:18<04:02,  1.40it/s]Evaluating on VQA val set:  87%|########7 | 2332/2671 [27:19<04:02,  1.40it/s]Evaluating on VQA val set:  87%|########7 | 2333/2671 [27:20<04:06,  1.37it/s]Evaluating on VQA val set:  87%|########7 | 2334/2671 [27:21<04:05,  1.37it/s]Evaluating on VQA val set:  87%|########7 | 2335/2671 [27:21<04:02,  1.38it/s]Evaluating on VQA val set:  87%|########7 | 2336/2671 [27:22<03:53,  1.43it/s]Evaluating on VQA val set:  87%|########7 | 2337/2671 [27:23<03:52,  1.44it/s]Evaluating on VQA val set:  88%|########7 | 2338/2671 [27:23<03:52,  1.43it/s]Evaluating on VQA val set:  88%|########7 | 2339/2671 [27:24<03:55,  1.41it/s]Evaluating on VQA val set:  88%|########7 | 2340/2671 [27:25<04:05,  1.35it/s]Evaluating on VQA val set:  88%|########7 | 2341/2671 [27:26<04:10,  1.32it/s]Evaluating on VQA val set:  88%|########7 | 2342/2671 [27:26<03:58,  1.38it/s]Evaluating on VQA val set:  88%|########7 | 2343/2671 [27:27<03:55,  1.40it/s]Evaluating on VQA val set:  88%|########7 | 2344/2671 [27:28<03:44,  1.46it/s]Evaluating on VQA val set:  88%|########7 | 2345/2671 [27:28<03:45,  1.45it/s]Evaluating on VQA val set:  88%|########7 | 2346/2671 [27:29<03:44,  1.45it/s]Evaluating on VQA val set:  88%|########7 | 2347/2671 [27:30<03:51,  1.40it/s]Evaluating on VQA val set:  88%|########7 | 2348/2671 [27:31<03:55,  1.37it/s]Evaluating on VQA val set:  88%|########7 | 2349/2671 [27:31<03:59,  1.35it/s]Evaluating on VQA val set:  88%|########7 | 2350/2671 [27:32<04:01,  1.33it/s]Evaluating on VQA val set:  88%|########8 | 2351/2671 [27:33<03:48,  1.40it/s]Evaluating on VQA val set:  88%|########8 | 2352/2671 [27:33<03:40,  1.45it/s]Evaluating on VQA val set:  88%|########8 | 2353/2671 [27:34<03:48,  1.39it/s]Evaluating on VQA val set:  88%|########8 | 2354/2671 [27:35<03:50,  1.38it/s]Evaluating on VQA val set:  88%|########8 | 2355/2671 [27:36<03:50,  1.37it/s]Evaluating on VQA val set:  88%|########8 | 2356/2671 [27:36<03:49,  1.37it/s]Evaluating on VQA val set:  88%|########8 | 2357/2671 [27:37<03:46,  1.38it/s]Evaluating on VQA val set:  88%|########8 | 2358/2671 [27:38<03:41,  1.41it/s]Evaluating on VQA val set:  88%|########8 | 2359/2671 [27:38<03:32,  1.47it/s]Evaluating on VQA val set:  88%|########8 | 2360/2671 [27:39<03:28,  1.49it/s]Evaluating on VQA val set:  88%|########8 | 2361/2671 [27:40<03:33,  1.45it/s]Evaluating on VQA val set:  88%|########8 | 2362/2671 [27:41<03:38,  1.41it/s]Evaluating on VQA val set:  88%|########8 | 2363/2671 [27:41<03:37,  1.41it/s]Evaluating on VQA val set:  89%|########8 | 2364/2671 [27:42<03:33,  1.44it/s]Evaluating on VQA val set:  89%|########8 | 2365/2671 [27:43<03:38,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2366/2671 [27:43<03:35,  1.41it/s]Evaluating on VQA val set:  89%|########8 | 2367/2671 [27:44<03:33,  1.42it/s]Evaluating on VQA val set:  89%|########8 | 2368/2671 [27:45<03:31,  1.43it/s]Evaluating on VQA val set:  89%|########8 | 2369/2671 [27:45<03:18,  1.52it/s]Evaluating on VQA val set:  89%|########8 | 2370/2671 [27:46<03:25,  1.46it/s]Evaluating on VQA val set:  89%|########8 | 2371/2671 [27:47<03:31,  1.42it/s]Evaluating on VQA val set:  89%|########8 | 2372/2671 [27:48<03:28,  1.43it/s]Evaluating on VQA val set:  89%|########8 | 2373/2671 [27:48<03:39,  1.36it/s]Evaluating on VQA val set:  89%|########8 | 2374/2671 [27:49<03:33,  1.39it/s]Evaluating on VQA val set:  89%|########8 | 2375/2671 [27:50<03:33,  1.39it/s]Evaluating on VQA val set:  89%|########8 | 2376/2671 [27:50<03:31,  1.40it/s]Evaluating on VQA val set:  89%|########8 | 2377/2671 [27:51<03:31,  1.39it/s]Evaluating on VQA val set:  89%|########9 | 2378/2671 [27:52<03:32,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2379/2671 [27:53<03:30,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2380/2671 [27:53<03:30,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2381/2671 [27:54<03:31,  1.37it/s]Evaluating on VQA val set:  89%|########9 | 2382/2671 [27:55<03:32,  1.36it/s]Evaluating on VQA val set:  89%|########9 | 2383/2671 [27:56<03:32,  1.36it/s]Evaluating on VQA val set:  89%|########9 | 2384/2671 [27:56<03:24,  1.41it/s]Evaluating on VQA val set:  89%|########9 | 2385/2671 [27:57<03:23,  1.41it/s]Evaluating on VQA val set:  89%|########9 | 2386/2671 [27:58<03:26,  1.38it/s]Evaluating on VQA val set:  89%|########9 | 2387/2671 [27:58<03:19,  1.42it/s]Evaluating on VQA val set:  89%|########9 | 2388/2671 [27:59<03:22,  1.40it/s]Evaluating on VQA val set:  89%|########9 | 2389/2671 [28:00<03:28,  1.35it/s]Evaluating on VQA val set:  89%|########9 | 2390/2671 [28:01<03:28,  1.35it/s]Evaluating on VQA val set:  90%|########9 | 2391/2671 [28:01<03:27,  1.35it/s]Evaluating on VQA val set:  90%|########9 | 2392/2671 [28:02<03:29,  1.33it/s]Evaluating on VQA val set:  90%|########9 | 2393/2671 [28:03<03:22,  1.37it/s]Evaluating on VQA val set:  90%|########9 | 2394/2671 [28:04<03:19,  1.39it/s]Evaluating on VQA val set:  90%|########9 | 2395/2671 [28:04<03:21,  1.37it/s]Evaluating on VQA val set:  90%|########9 | 2396/2671 [28:05<03:19,  1.38it/s]Evaluating on VQA val set:  90%|########9 | 2397/2671 [28:06<03:19,  1.38it/s]Evaluating on VQA val set:  90%|########9 | 2398/2671 [28:07<03:21,  1.35it/s]Evaluating on VQA val set:  90%|########9 | 2399/2671 [28:07<03:20,  1.36it/s]Evaluating on VQA val set:  90%|########9 | 2400/2671 [28:08<03:12,  1.40it/s]Evaluating on VQA val set:  90%|########9 | 2401/2671 [28:08<02:48,  1.61it/s]Evaluating on VQA val set:  90%|########9 | 2402/2671 [28:09<02:33,  1.75it/s]Evaluating on VQA val set:  90%|########9 | 2403/2671 [28:09<02:40,  1.67it/s]Evaluating on VQA val set:  90%|######### | 2404/2671 [28:10<02:51,  1.56it/s]Evaluating on VQA val set:  90%|######### | 2405/2671 [28:11<02:57,  1.50it/s]Evaluating on VQA val set:  90%|######### | 2406/2671 [28:12<02:55,  1.51it/s]Evaluating on VQA val set:  90%|######### | 2407/2671 [28:12<02:59,  1.47it/s]Evaluating on VQA val set:  90%|######### | 2408/2671 [28:13<03:02,  1.44it/s]Evaluating on VQA val set:  90%|######### | 2409/2671 [28:14<02:59,  1.46it/s]Evaluating on VQA val set:  90%|######### | 2410/2671 [28:14<02:50,  1.53it/s]Evaluating on VQA val set:  90%|######### | 2411/2671 [28:15<02:55,  1.48it/s]Evaluating on VQA val set:  90%|######### | 2412/2671 [28:16<02:56,  1.47it/s]Evaluating on VQA val set:  90%|######### | 2413/2671 [28:17<03:07,  1.38it/s]Evaluating on VQA val set:  90%|######### | 2414/2671 [28:17<03:09,  1.36it/s]Evaluating on VQA val set:  90%|######### | 2415/2671 [28:18<03:10,  1.34it/s]Evaluating on VQA val set:  90%|######### | 2416/2671 [28:19<02:59,  1.42it/s]Evaluating on VQA val set:  90%|######### | 2417/2671 [28:19<03:01,  1.40it/s]Evaluating on VQA val set:  91%|######### | 2418/2671 [28:20<03:00,  1.40it/s]Evaluating on VQA val set:  91%|######### | 2419/2671 [28:21<02:57,  1.42it/s]Evaluating on VQA val set:  91%|######### | 2420/2671 [28:21<02:53,  1.45it/s]Evaluating on VQA val set:  91%|######### | 2421/2671 [28:22<02:53,  1.44it/s]Evaluating on VQA val set:  91%|######### | 2422/2671 [28:23<02:56,  1.41it/s]Evaluating on VQA val set:  91%|######### | 2423/2671 [28:24<02:54,  1.42it/s]Evaluating on VQA val set:  91%|######### | 2424/2671 [28:24<02:53,  1.42it/s]Evaluating on VQA val set:  91%|######### | 2425/2671 [28:25<02:51,  1.44it/s]Evaluating on VQA val set:  91%|######### | 2426/2671 [28:26<02:47,  1.47it/s]Evaluating on VQA val set:  91%|######### | 2427/2671 [28:26<02:54,  1.40it/s]Evaluating on VQA val set:  91%|######### | 2428/2671 [28:27<02:56,  1.38it/s]Evaluating on VQA val set:  91%|######### | 2429/2671 [28:28<02:53,  1.39it/s]Evaluating on VQA val set:  91%|######### | 2430/2671 [28:29<02:51,  1.40it/s]Evaluating on VQA val set:  91%|#########1| 2431/2671 [28:29<02:52,  1.39it/s]Evaluating on VQA val set:  91%|#########1| 2432/2671 [28:30<02:50,  1.40it/s]Evaluating on VQA val set:  91%|#########1| 2433/2671 [28:31<02:49,  1.41it/s]Evaluating on VQA val set:  91%|#########1| 2434/2671 [28:31<02:51,  1.38it/s]Evaluating on VQA val set:  91%|#########1| 2435/2671 [28:32<02:47,  1.41it/s]Evaluating on VQA val set:  91%|#########1| 2436/2671 [28:33<02:47,  1.41it/s]Evaluating on VQA val set:  91%|#########1| 2437/2671 [28:33<02:41,  1.45it/s]Evaluating on VQA val set:  91%|#########1| 2438/2671 [28:34<02:39,  1.46it/s]Evaluating on VQA val set:  91%|#########1| 2439/2671 [28:35<02:42,  1.43it/s]Evaluating on VQA val set:  91%|#########1| 2440/2671 [28:36<02:48,  1.37it/s]Evaluating on VQA val set:  91%|#########1| 2441/2671 [28:36<02:49,  1.35it/s]Evaluating on VQA val set:  91%|#########1| 2442/2671 [28:37<02:46,  1.38it/s]Evaluating on VQA val set:  91%|#########1| 2443/2671 [28:38<02:47,  1.36it/s]Evaluating on VQA val set:  92%|#########1| 2444/2671 [28:39<02:40,  1.41it/s]Evaluating on VQA val set:  92%|#########1| 2445/2671 [28:39<02:37,  1.44it/s]Evaluating on VQA val set:  92%|#########1| 2446/2671 [28:40<02:36,  1.44it/s]Evaluating on VQA val set:  92%|#########1| 2447/2671 [28:41<02:34,  1.45it/s]Evaluating on VQA val set:  92%|#########1| 2448/2671 [28:41<02:33,  1.45it/s]Evaluating on VQA val set:  92%|#########1| 2449/2671 [28:42<02:28,  1.49it/s]Evaluating on VQA val set:  92%|#########1| 2450/2671 [28:42<02:22,  1.55it/s]Evaluating on VQA val set:  92%|#########1| 2451/2671 [28:43<02:27,  1.50it/s]Evaluating on VQA val set:  92%|#########1| 2452/2671 [28:44<02:27,  1.49it/s]Evaluating on VQA val set:  92%|#########1| 2453/2671 [28:45<02:25,  1.50it/s]Evaluating on VQA val set:  92%|#########1| 2454/2671 [28:45<02:27,  1.47it/s]Evaluating on VQA val set:  92%|#########1| 2455/2671 [28:46<02:24,  1.49it/s]Evaluating on VQA val set:  92%|#########1| 2456/2671 [28:47<02:24,  1.48it/s]Evaluating on VQA val set:  92%|#########1| 2457/2671 [28:47<02:26,  1.46it/s]Evaluating on VQA val set:  92%|#########2| 2458/2671 [28:48<02:30,  1.42it/s]Evaluating on VQA val set:  92%|#########2| 2459/2671 [28:49<02:26,  1.45it/s]Evaluating on VQA val set:  92%|#########2| 2460/2671 [28:49<02:26,  1.44it/s]Evaluating on VQA val set:  92%|#########2| 2461/2671 [28:50<02:30,  1.39it/s]Evaluating on VQA val set:  92%|#########2| 2462/2671 [28:51<02:30,  1.39it/s]Evaluating on VQA val set:  92%|#########2| 2463/2671 [28:52<02:33,  1.36it/s]Evaluating on VQA val set:  92%|#########2| 2464/2671 [28:52<02:31,  1.37it/s]Evaluating on VQA val set:  92%|#########2| 2465/2671 [28:53<02:31,  1.36it/s]Evaluating on VQA val set:  92%|#########2| 2466/2671 [28:54<02:29,  1.37it/s]Evaluating on VQA val set:  92%|#########2| 2467/2671 [28:55<02:32,  1.34it/s]Evaluating on VQA val set:  92%|#########2| 2468/2671 [28:55<02:23,  1.41it/s]Evaluating on VQA val set:  92%|#########2| 2469/2671 [28:56<02:19,  1.45it/s]Evaluating on VQA val set:  92%|#########2| 2470/2671 [28:57<02:20,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2471/2671 [28:57<02:22,  1.40it/s]Evaluating on VQA val set:  93%|#########2| 2472/2671 [28:58<02:21,  1.40it/s]Evaluating on VQA val set:  93%|#########2| 2473/2671 [28:59<02:18,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2474/2671 [28:59<02:16,  1.44it/s]Evaluating on VQA val set:  93%|#########2| 2475/2671 [29:00<02:16,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2476/2671 [29:01<02:10,  1.50it/s]Evaluating on VQA val set:  93%|#########2| 2477/2671 [29:02<02:15,  1.43it/s]Evaluating on VQA val set:  93%|#########2| 2478/2671 [29:02<02:13,  1.45it/s]Evaluating on VQA val set:  93%|#########2| 2479/2671 [29:03<02:16,  1.41it/s]Evaluating on VQA val set:  93%|#########2| 2480/2671 [29:04<02:10,  1.46it/s]Evaluating on VQA val set:  93%|#########2| 2481/2671 [29:04<02:11,  1.45it/s]Evaluating on VQA val set:  93%|#########2| 2482/2671 [29:05<02:13,  1.42it/s]Evaluating on VQA val set:  93%|#########2| 2483/2671 [29:06<02:10,  1.44it/s]Evaluating on VQA val set:  93%|#########2| 2484/2671 [29:06<02:11,  1.42it/s]Evaluating on VQA val set:  93%|#########3| 2485/2671 [29:07<02:16,  1.36it/s]Evaluating on VQA val set:  93%|#########3| 2486/2671 [29:08<02:14,  1.37it/s]Evaluating on VQA val set:  93%|#########3| 2487/2671 [29:09<02:08,  1.43it/s]Evaluating on VQA val set:  93%|#########3| 2488/2671 [29:09<02:05,  1.46it/s]Evaluating on VQA val set:  93%|#########3| 2489/2671 [29:10<02:02,  1.49it/s]Evaluating on VQA val set:  93%|#########3| 2490/2671 [29:10<01:55,  1.57it/s]Evaluating on VQA val set:  93%|#########3| 2491/2671 [29:11<01:52,  1.60it/s]Evaluating on VQA val set:  93%|#########3| 2492/2671 [29:12<01:50,  1.62it/s]Evaluating on VQA val set:  93%|#########3| 2493/2671 [29:12<01:55,  1.54it/s]Evaluating on VQA val set:  93%|#########3| 2494/2671 [29:13<01:59,  1.49it/s]Evaluating on VQA val set:  93%|#########3| 2495/2671 [29:14<01:55,  1.53it/s]Evaluating on VQA val set:  93%|#########3| 2496/2671 [29:14<01:57,  1.48it/s]Evaluating on VQA val set:  93%|#########3| 2497/2671 [29:15<01:53,  1.53it/s]Evaluating on VQA val set:  94%|#########3| 2498/2671 [29:16<01:53,  1.53it/s]Evaluating on VQA val set:  94%|#########3| 2499/2671 [29:16<01:55,  1.50it/s]Evaluating on VQA val set:  94%|#########3| 2500/2671 [29:17<01:52,  1.52it/s]Evaluating on VQA val set:  94%|#########3| 2501/2671 [29:18<01:55,  1.47it/s]Evaluating on VQA val set:  94%|#########3| 2502/2671 [29:18<01:57,  1.44it/s]Evaluating on VQA val set:  94%|#########3| 2503/2671 [29:19<02:00,  1.40it/s]Evaluating on VQA val set:  94%|#########3| 2504/2671 [29:20<02:00,  1.39it/s]Evaluating on VQA val set:  94%|#########3| 2505/2671 [29:21<01:57,  1.41it/s]Evaluating on VQA val set:  94%|#########3| 2506/2671 [29:21<01:54,  1.44it/s]Evaluating on VQA val set:  94%|#########3| 2507/2671 [29:22<01:56,  1.41it/s]Evaluating on VQA val set:  94%|#########3| 2508/2671 [29:23<01:49,  1.48it/s]Evaluating on VQA val set:  94%|#########3| 2509/2671 [29:23<01:40,  1.61it/s]Evaluating on VQA val set:  94%|#########3| 2510/2671 [29:24<01:41,  1.58it/s]Evaluating on VQA val set:  94%|#########4| 2511/2671 [29:24<01:41,  1.57it/s]Evaluating on VQA val set:  94%|#########4| 2512/2671 [29:25<01:46,  1.50it/s]Evaluating on VQA val set:  94%|#########4| 2513/2671 [29:26<01:48,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2514/2671 [29:27<01:47,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2515/2671 [29:27<01:46,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2516/2671 [29:28<01:47,  1.44it/s]Evaluating on VQA val set:  94%|#########4| 2517/2671 [29:29<01:46,  1.45it/s]Evaluating on VQA val set:  94%|#########4| 2518/2671 [29:29<01:47,  1.42it/s]Evaluating on VQA val set:  94%|#########4| 2519/2671 [29:30<01:48,  1.40it/s]Evaluating on VQA val set:  94%|#########4| 2520/2671 [29:31<01:47,  1.41it/s]Evaluating on VQA val set:  94%|#########4| 2521/2671 [29:31<01:43,  1.45it/s]Evaluating on VQA val set:  94%|#########4| 2522/2671 [29:32<01:41,  1.47it/s]Evaluating on VQA val set:  94%|#########4| 2523/2671 [29:33<01:41,  1.46it/s]Evaluating on VQA val set:  94%|#########4| 2524/2671 [29:34<01:42,  1.43it/s]Evaluating on VQA val set:  95%|#########4| 2525/2671 [29:34<01:41,  1.44it/s]Evaluating on VQA val set:  95%|#########4| 2526/2671 [29:35<01:33,  1.55it/s]Evaluating on VQA val set:  95%|#########4| 2527/2671 [29:35<01:35,  1.51it/s]Evaluating on VQA val set:  95%|#########4| 2528/2671 [29:36<01:39,  1.43it/s]Evaluating on VQA val set:  95%|#########4| 2529/2671 [29:37<01:40,  1.41it/s]Evaluating on VQA val set:  95%|#########4| 2530/2671 [29:38<01:38,  1.43it/s]Evaluating on VQA val set:  95%|#########4| 2531/2671 [29:38<01:34,  1.48it/s]Evaluating on VQA val set:  95%|#########4| 2532/2671 [29:39<01:34,  1.47it/s]Evaluating on VQA val set:  95%|#########4| 2533/2671 [29:40<01:33,  1.47it/s]Evaluating on VQA val set:  95%|#########4| 2534/2671 [29:40<01:33,  1.46it/s]Evaluating on VQA val set:  95%|#########4| 2535/2671 [29:41<01:31,  1.49it/s]Evaluating on VQA val set:  95%|#########4| 2536/2671 [29:42<01:33,  1.44it/s]Evaluating on VQA val set:  95%|#########4| 2537/2671 [29:42<01:34,  1.42it/s]Evaluating on VQA val set:  95%|#########5| 2538/2671 [29:43<01:31,  1.45it/s]Evaluating on VQA val set:  95%|#########5| 2539/2671 [29:44<01:29,  1.48it/s]Evaluating on VQA val set:  95%|#########5| 2540/2671 [29:44<01:25,  1.54it/s]Evaluating on VQA val set:  95%|#########5| 2541/2671 [29:45<01:27,  1.49it/s]Evaluating on VQA val set:  95%|#########5| 2542/2671 [29:46<01:29,  1.43it/s]Evaluating on VQA val set:  95%|#########5| 2543/2671 [29:47<01:33,  1.36it/s]Evaluating on VQA val set:  95%|#########5| 2544/2671 [29:47<01:30,  1.40it/s]Evaluating on VQA val set:  95%|#########5| 2545/2671 [29:48<01:28,  1.42it/s]Evaluating on VQA val set:  95%|#########5| 2546/2671 [29:49<01:26,  1.45it/s]Evaluating on VQA val set:  95%|#########5| 2547/2671 [29:49<01:25,  1.44it/s]Evaluating on VQA val set:  95%|#########5| 2548/2671 [29:50<01:25,  1.43it/s]Evaluating on VQA val set:  95%|#########5| 2549/2671 [29:51<01:27,  1.39it/s]Evaluating on VQA val set:  95%|#########5| 2550/2671 [29:52<01:27,  1.39it/s]Evaluating on VQA val set:  96%|#########5| 2551/2671 [29:52<01:28,  1.36it/s]Evaluating on VQA val set:  96%|#########5| 2552/2671 [29:53<01:28,  1.35it/s]Evaluating on VQA val set:  96%|#########5| 2553/2671 [29:54<01:25,  1.38it/s]Evaluating on VQA val set:  96%|#########5| 2554/2671 [29:54<01:23,  1.40it/s]Evaluating on VQA val set:  96%|#########5| 2555/2671 [29:55<01:22,  1.40it/s]Evaluating on VQA val set:  96%|#########5| 2556/2671 [29:56<01:20,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2557/2671 [29:57<01:20,  1.42it/s]Evaluating on VQA val set:  96%|#########5| 2558/2671 [29:57<01:20,  1.40it/s]Evaluating on VQA val set:  96%|#########5| 2559/2671 [29:58<01:18,  1.42it/s]Evaluating on VQA val set:  96%|#########5| 2560/2671 [29:59<01:16,  1.45it/s]Evaluating on VQA val set:  96%|#########5| 2561/2671 [29:59<01:17,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2562/2671 [30:00<01:15,  1.44it/s]Evaluating on VQA val set:  96%|#########5| 2563/2671 [30:01<01:15,  1.43it/s]Evaluating on VQA val set:  96%|#########5| 2564/2671 [30:01<01:15,  1.42it/s]Evaluating on VQA val set:  96%|#########6| 2565/2671 [30:02<01:12,  1.46it/s]Evaluating on VQA val set:  96%|#########6| 2566/2671 [30:03<01:10,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2567/2671 [30:03<01:12,  1.44it/s]Evaluating on VQA val set:  96%|#########6| 2568/2671 [30:04<01:09,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2569/2671 [30:05<01:07,  1.50it/s]Evaluating on VQA val set:  96%|#########6| 2570/2671 [30:05<01:09,  1.46it/s]Evaluating on VQA val set:  96%|#########6| 2571/2671 [30:06<01:07,  1.49it/s]Evaluating on VQA val set:  96%|#########6| 2572/2671 [30:07<01:07,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2573/2671 [30:07<01:06,  1.48it/s]Evaluating on VQA val set:  96%|#########6| 2574/2671 [30:08<01:04,  1.51it/s]Evaluating on VQA val set:  96%|#########6| 2575/2671 [30:09<01:06,  1.44it/s]Evaluating on VQA val set:  96%|#########6| 2576/2671 [30:10<01:06,  1.43it/s]Evaluating on VQA val set:  96%|#########6| 2577/2671 [30:10<01:05,  1.44it/s]Evaluating on VQA val set:  97%|#########6| 2578/2671 [30:11<01:05,  1.42it/s]Evaluating on VQA val set:  97%|#########6| 2579/2671 [30:12<01:05,  1.41it/s]Evaluating on VQA val set:  97%|#########6| 2580/2671 [30:12<01:03,  1.44it/s]Evaluating on VQA val set:  97%|#########6| 2581/2671 [30:13<01:04,  1.40it/s]Evaluating on VQA val set:  97%|#########6| 2582/2671 [30:14<01:03,  1.41it/s]Evaluating on VQA val set:  97%|#########6| 2583/2671 [30:15<01:04,  1.36it/s]Evaluating on VQA val set:  97%|#########6| 2584/2671 [30:15<01:02,  1.39it/s]Evaluating on VQA val set:  97%|#########6| 2585/2671 [30:16<01:02,  1.38it/s]Evaluating on VQA val set:  97%|#########6| 2586/2671 [30:17<01:02,  1.35it/s]Evaluating on VQA val set:  97%|#########6| 2587/2671 [30:18<01:01,  1.37it/s]Evaluating on VQA val set:  97%|#########6| 2588/2671 [30:18<01:01,  1.35it/s]Evaluating on VQA val set:  97%|#########6| 2589/2671 [30:19<01:00,  1.35it/s]Evaluating on VQA val set:  97%|#########6| 2590/2671 [30:20<00:59,  1.35it/s]Evaluating on VQA val set:  97%|#########7| 2591/2671 [30:20<00:57,  1.38it/s]Evaluating on VQA val set:  97%|#########7| 2592/2671 [30:21<00:56,  1.39it/s]Evaluating on VQA val set:  97%|#########7| 2593/2671 [30:22<00:55,  1.41it/s]Evaluating on VQA val set:  97%|#########7| 2594/2671 [30:23<00:54,  1.42it/s]Evaluating on VQA val set:  97%|#########7| 2595/2671 [30:23<00:52,  1.46it/s]Evaluating on VQA val set:  97%|#########7| 2596/2671 [30:24<00:51,  1.45it/s]Evaluating on VQA val set:  97%|#########7| 2597/2671 [30:25<00:52,  1.42it/s]Evaluating on VQA val set:  97%|#########7| 2598/2671 [30:25<00:50,  1.44it/s]Evaluating on VQA val set:  97%|#########7| 2599/2671 [30:26<00:50,  1.43it/s]Evaluating on VQA val set:  97%|#########7| 2600/2671 [30:27<00:50,  1.41it/s]Evaluating on VQA val set:  97%|#########7| 2601/2671 [30:27<00:50,  1.40it/s]Evaluating on VQA val set:  97%|#########7| 2602/2671 [30:28<00:49,  1.38it/s]Evaluating on VQA val set:  97%|#########7| 2603/2671 [30:29<00:48,  1.40it/s]Evaluating on VQA val set:  97%|#########7| 2604/2671 [30:30<00:46,  1.43it/s]Evaluating on VQA val set:  98%|#########7| 2605/2671 [30:30<00:42,  1.55it/s]Evaluating on VQA val set:  98%|#########7| 2606/2671 [30:31<00:43,  1.49it/s]Evaluating on VQA val set:  98%|#########7| 2607/2671 [30:32<00:44,  1.45it/s]Evaluating on VQA val set:  98%|#########7| 2608/2671 [30:32<00:44,  1.43it/s]Evaluating on VQA val set:  98%|#########7| 2609/2671 [30:33<00:43,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2610/2671 [30:34<00:42,  1.43it/s]Evaluating on VQA val set:  98%|#########7| 2611/2671 [30:34<00:41,  1.44it/s]Evaluating on VQA val set:  98%|#########7| 2612/2671 [30:35<00:41,  1.41it/s]Evaluating on VQA val set:  98%|#########7| 2613/2671 [30:36<00:41,  1.40it/s]Evaluating on VQA val set:  98%|#########7| 2614/2671 [30:37<00:41,  1.38it/s]Evaluating on VQA val set:  98%|#########7| 2615/2671 [30:37<00:39,  1.43it/s]Evaluating on VQA val set:  98%|#########7| 2616/2671 [30:38<00:39,  1.40it/s]Evaluating on VQA val set:  98%|#########7| 2617/2671 [30:39<00:38,  1.39it/s]Evaluating on VQA val set:  98%|#########8| 2618/2671 [30:39<00:37,  1.42it/s]Evaluating on VQA val set:  98%|#########8| 2619/2671 [30:40<00:36,  1.44it/s]Evaluating on VQA val set:  98%|#########8| 2620/2671 [30:41<00:34,  1.47it/s]Evaluating on VQA val set:  98%|#########8| 2621/2671 [30:41<00:33,  1.48it/s]Evaluating on VQA val set:  98%|#########8| 2622/2671 [30:42<00:33,  1.48it/s]Evaluating on VQA val set:  98%|#########8| 2623/2671 [30:43<00:31,  1.52it/s]Evaluating on VQA val set:  98%|#########8| 2624/2671 [30:43<00:30,  1.52it/s]Evaluating on VQA val set:  98%|#########8| 2625/2671 [30:44<00:30,  1.52it/s]Evaluating on VQA val set:  98%|#########8| 2626/2671 [30:45<00:30,  1.47it/s]Evaluating on VQA val set:  98%|#########8| 2627/2671 [30:45<00:30,  1.44it/s]Evaluating on VQA val set:  98%|#########8| 2628/2671 [30:46<00:31,  1.38it/s]Evaluating on VQA val set:  98%|#########8| 2629/2671 [30:47<00:29,  1.43it/s]Evaluating on VQA val set:  98%|#########8| 2630/2671 [30:48<00:28,  1.42it/s]Evaluating on VQA val set:  99%|#########8| 2631/2671 [30:48<00:28,  1.39it/s]Evaluating on VQA val set:  99%|#########8| 2632/2671 [30:49<00:28,  1.38it/s]Evaluating on VQA val set:  99%|#########8| 2633/2671 [30:50<00:27,  1.36it/s]Evaluating on VQA val set:  99%|#########8| 2634/2671 [30:51<00:26,  1.40it/s]Evaluating on VQA val set:  99%|#########8| 2635/2671 [30:51<00:25,  1.39it/s]Evaluating on VQA val set:  99%|#########8| 2636/2671 [30:52<00:25,  1.39it/s]Evaluating on VQA val set:  99%|#########8| 2637/2671 [30:53<00:24,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2638/2671 [30:53<00:22,  1.44it/s]Evaluating on VQA val set:  99%|#########8| 2639/2671 [30:54<00:22,  1.43it/s]Evaluating on VQA val set:  99%|#########8| 2640/2671 [30:55<00:21,  1.47it/s]Evaluating on VQA val set:  99%|#########8| 2641/2671 [30:55<00:21,  1.41it/s]Evaluating on VQA val set:  99%|#########8| 2642/2671 [30:56<00:20,  1.40it/s]Evaluating on VQA val set:  99%|#########8| 2643/2671 [30:57<00:19,  1.44it/s]Evaluating on VQA val set:  99%|#########8| 2644/2671 [30:57<00:17,  1.52it/s]Evaluating on VQA val set:  99%|#########9| 2645/2671 [30:58<00:17,  1.48it/s]Evaluating on VQA val set:  99%|#########9| 2646/2671 [30:59<00:17,  1.47it/s]Evaluating on VQA val set:  99%|#########9| 2647/2671 [31:00<00:16,  1.44it/s]Evaluating on VQA val set:  99%|#########9| 2648/2671 [31:00<00:16,  1.42it/s]Evaluating on VQA val set:  99%|#########9| 2649/2671 [31:01<00:15,  1.39it/s]Evaluating on VQA val set:  99%|#########9| 2650/2671 [31:02<00:14,  1.46it/s]Evaluating on VQA val set:  99%|#########9| 2651/2671 [31:02<00:14,  1.40it/s]Evaluating on VQA val set:  99%|#########9| 2652/2671 [31:03<00:13,  1.40it/s]Evaluating on VQA val set:  99%|#########9| 2653/2671 [31:04<00:12,  1.43it/s]Evaluating on VQA val set:  99%|#########9| 2654/2671 [31:05<00:12,  1.41it/s]Evaluating on VQA val set:  99%|#########9| 2655/2671 [31:05<00:11,  1.43it/s]Evaluating on VQA val set:  99%|#########9| 2656/2671 [31:06<00:10,  1.43it/s]Evaluating on VQA val set:  99%|#########9| 2657/2671 [31:06<00:09,  1.48it/s]Evaluating on VQA val set: 100%|#########9| 2658/2671 [31:07<00:09,  1.44it/s]Evaluating on VQA val set: 100%|#########9| 2659/2671 [31:08<00:08,  1.45it/s]Evaluating on VQA val set: 100%|#########9| 2660/2671 [31:09<00:07,  1.43it/s]Evaluating on VQA val set: 100%|#########9| 2661/2671 [31:09<00:06,  1.45it/s]Evaluating on VQA val set: 100%|#########9| 2662/2671 [31:10<00:06,  1.50it/s]Evaluating on VQA val set: 100%|#########9| 2663/2671 [31:11<00:05,  1.48it/s]Evaluating on VQA val set: 100%|#########9| 2664/2671 [31:11<00:04,  1.47it/s]Evaluating on VQA val set: 100%|#########9| 2665/2671 [31:12<00:04,  1.48it/s]Evaluating on VQA val set: 100%|#########9| 2666/2671 [31:13<00:03,  1.45it/s]Evaluating on VQA val set: 100%|#########9| 2667/2671 [31:13<00:02,  1.42it/s]Evaluating on VQA val set: 100%|#########9| 2668/2671 [31:14<00:02,  1.42it/s]Evaluating on VQA val set: 100%|#########9| 2669/2671 [31:15<00:01,  1.44it/s]Evaluating on VQA val set: 100%|#########9| 2670/2671 [31:15<00:00,  1.44it/s]Evaluating on VQA val set: 100%|##########| 2671/2671 [31:16<00:00,  1.67it/s]Evaluating on VQA val set: 100%|##########| 2671/2671 [31:16<00:00,  1.42it/s]
11/17/2022 02:34:15 - INFO - evaluate_cl_algorithm - Evaluation score of vilt model on VQAv2, using checkpoint after SNLI-VE training: 51.46
11/17/2022 02:34:15 - INFO - evaluate_cl_algorithm - Forgetting of VQAv2, after training on SNLI-VE = 11.55%
11/17/2022 02:34:15 - INFO - __main__ - ----------------------------------------------------------------------------------------------------
