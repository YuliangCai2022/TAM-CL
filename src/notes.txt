1. current vilt+dytox cannot freeze previous task_token if new task come

2. the encoder output of vilt is [32,768]

3. current loss is only vilt loss, haven't add Dytox loss, should add loss in train_vqa.py

4. For dytox loss, we need kl loss and div loss, but only for CL scenario, if train on one task, no need

5. need to seperate the oringal vilt and dytox-vilt to do comparison

6. applying loss functions are in train_<dataset>.py,

7. need function to freeze bottom k layers to accelerate training. 

8. added flag --task_attention to tell if the model to use is original vilt or vilt + task attention block


11711783 is with TAB but fixed the Q
11712463 is without TAB freeze 11 layers
11712504 is without TAB freeze 1 layer

11. in train step, there's no different in logits

12. vilt without TAB without fine tune on VQA : 45
    vilt+TAB wihout find tune on VQA: 14.7

    now: 57.7/62.82/63.39


13. the output shape of classifier of Dytox is incremental every new task

14. during the training, after task 1, the batch data will go to every current and previous header and get result and concatenate them together as logits.

15. Remember to run experiment of Dytox on CIFAR100

16. Dytox get classifier result only from current task's classifier

17. modify the dytox.py 

18. the classifier output num is correct

19. 63 no dytox 69 normal dytox 47 dytox no ft 45 dytox no ft no kd 60 dytox with div no ft 63 dytox with div alter no ft, 11 dytox with div with ftï¼Œ 41
dytox with div with 2 times kd 77 dytox with div with 2 times div 71 dytox with div freeze teacher model tab 62 69 freeze tab at all after first task
21 dytox with freeze tab and head 30% training dataset 55 freeze tab and head both teacher model and model after first task 30% dataset 
29 30% dataset non dytox 71 30% dataset dytox freeze all 30 30% data dytox freeze vilt and head 51 30% data dytox freeze vilt and tab
65 30% data freeze tab and head
46 30% data no freeze dytox
362 30% data no freeze dytox freeze 11 (max) layers of vilt
348 30% data no freeze dytox freeze 6 layers of vilt
0362 30% data no freeze dytox freze 10 layers of vilt
174 30% data freeze vilt freeze 10 layers of vilt (a double check)

362 30% data freeze 10 layers of vilt, dytox no kdl-loss ratio correction, no vilt kd loss                   [53.08, 70.86, 7.93% (48.87)]
549 30% data freeze 10 layers of vilt, non-dytox                                                             [55.00, 72.13, 5.14% (52.17)]
654 30% data freeze 10 layers of vilt, correct the ratio of kd_loss and loss, no vilt kd loss                [51.50, 70.45, 5.24% (48.80)]
005 30% data freeze 10 layers of vilt, correct the ratio of kd_loss and loss, add vilt kd loss * 10000       [54.30, 69.62, 2.90% (52.73)]
224 30% data freeze 10 layers of vilt, correct the ratio of kd_loss and loss, add vilt kd loss * 1000000     [53.47, 66.24, 2.42% (52.18)]
296 30% data freeze 10 layers of vilt, non-dytox, ewc                                                        [54.70, 71.40, 8.82% (49.87)]

157 full data freeze 10 layers of vilt, dytox with kd_loss, vilt kd loss * 10000       [62.06, 72.30, 3.17% (60.10)]
177 full data freeze 10 layers of vilt, non-dytox                                      [62.14, 73.78, 10.56% (55.58)]
296 full data freeze 10 layers of vilt, ewc                                            [61.93, 73.93, 8.35% (56.76)]
847 full data freeze 10 layers of vilt, dytox wihout vilt_kd_loss                      [61.99, 72.45, 3.25% (59.97)]
849 full data freeze 10 layers of vilt, dytox with ewc without vilt_kd_loss            [62.08, 72.56, 4.09% (59.54)]    

379 full data freeze 6 layers of vilt, dytox with kd_loss, vKD * 10000                 [64.78, 74.27, 2.52% (63.15)]
380 full data freeze 8 layers of vilt, dytox with kd_loss, vKD * 10000                 [64.21, 73.58, 2.92% (62.34)]
019 full data freeze 0 layers of vilt, dytox with kd_loss, vKD * 10000, 16 batch       [61.08, 75.16, 3.80% (58.76)]
884 full data freeze 6 layers of vilt, dytox with kd_loss, vKD * 10000, 16 batch       [63.87, 74.45, 3.67% (61.53)]
614 full data freeze 0 layers of vilt, dytox with kd_loss, vKD * 20000, 16 batch       [61.56, 73.88, 6.35% (57.65)]

666 / 782 full data freeze 6 layers of vilt, dytox with vKD, 3 tasks     [63.09, 74.26, ]
405 full data freeze 6 layres of vilt, non-dytox with vKD, 3 tasks [64.33, 76.07, 58.35, 16.70%, 63.14%/87.78%]

147/258 0.1 data freeze 6 layer 3 tasks, non dytox                                                  [41,19, 71.78, 47.67, 6.34%, 32.82%, 45.91%]
863 0.1 data freeze 6 layes 3 tasks, dytox with 0.5 kd weight for vcr KD * 1000                     [36.25, 71.20, 25.72, Nan, 8.43%, 20.79%]
958 0.1 data freeze 6 layer 3 tasks, dytox with 0.5 kd, KD * 2000, vKD * 5000 (originally 10000)    [39.14, 67.80, 24.88, Nan, 4.00%, 15.47%]
777 0.1 data freeze 6 layers 3 tasks, dytox but comment KD and vKD for VCR                          [42.48, 70.35, 25.18, Nan, 43.37%, 98.89%]
353 0.1 data freeze 6 layers 3 tasks, dytox without KD and VKD

915 0.1 data freeze 6 layers 3 tasks, seperate answers with kd loss of seperate answers
920 0.1 data freeze 6 layers 3 tasks, sepearte answers with kd loss of one of seperate answers (1 of 4)
923 0.1 data freeze 6 layers 3 tasks, seperate answers with kd loss of mixed answers

20. it is the vilt part that make the forgetting rate high, after freeze the vilt after training the VQA, the SNLI become low but forgetting rate become low!